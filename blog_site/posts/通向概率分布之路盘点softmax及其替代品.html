<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>é€šå‘æ¦‚ç‡åˆ†å¸ƒä¹‹è·¯ï¼šç›˜ç‚¹SoftmaxåŠå…¶æ›¿ä»£å“</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>é€šå‘æ¦‚ç‡åˆ†å¸ƒä¹‹è·¯ï¼šç›˜ç‚¹SoftmaxåŠå…¶æ›¿ä»£å“</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2025-11-26 | ğŸ“„ å¤§å°: 60.3 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/10145">https://spaces.ac.cn/archives/10145</a></p>
<p><strong>å‘å¸ƒæ—¥æœŸ</strong>: </p>
<hr />
<p>ä¸è®ºæ˜¯åœ¨åŸºç¡€çš„åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¿˜æ˜¯å¦‚ä»Šæ— å¤„ä¸åœ¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæ¦‚ç‡åˆ†å¸ƒçš„æ„å»ºéƒ½æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯å°†ä¸€ä¸ª$n$ç»´çš„ä»»æ„å‘é‡ï¼Œè½¬æ¢ä¸ºä¸€ä¸ª$n$å…ƒçš„ç¦»æ•£å‹æ¦‚ç‡åˆ†å¸ƒã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œè¿™ä¸ªé—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆæ˜¯Softmaxï¼Œå®ƒæ˜¯æŒ‡æ•°å½’ä¸€åŒ–çš„å½¢å¼ï¼Œç›¸å¯¹æ¥è¯´æ¯”è¾ƒç®€å•ç›´è§‚ï¼ŒåŒæ—¶ä¹Ÿä¼´æœ‰å¾ˆå¤šä¼˜è‰¯æ€§è´¨ï¼Œä»è€Œæˆä¸ºå¤§éƒ¨åˆ†åœºæ™¯ä¸‹çš„â€œæ ‡é…â€ã€‚</p>
<p>å°½ç®¡å¦‚æ­¤ï¼ŒSoftmaxåœ¨æŸäº›åœºæ™¯ä¸‹ä¹Ÿæœ‰ä¸€äº›ä¸å¦‚äººæ„ä¹‹å¤„ï¼Œæ¯”å¦‚ä¸å¤Ÿç¨€ç–ã€æ— æ³•ç»å¯¹ç­‰äºé›¶ç­‰ï¼Œå› æ­¤å¾ˆå¤šæ›¿ä»£å“ä¹Ÿåº”è¿è€Œç”Ÿã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç®€å•æ€»ç»“ä¸€ä¸‹Softmaxçš„ç›¸å…³æ€§è´¨ï¼Œå¹¶ç›˜ç‚¹å’Œå¯¹æ¯”ä¸€ä¸‹å®ƒçš„éƒ¨åˆ†æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<h2 id="softmax">Softmaxå›é¡¾</h2>
<p>é¦–å…ˆå¼•å…¥ä¸€äº›é€šç”¨è®°å·ï¼š$\boldsymbol{x} = (x_1,x_2,\cdots,x_n)\in\mathbb{R}^n$æ˜¯éœ€è¦è½¬ä¸ºæ¦‚ç‡åˆ†å¸ƒçš„$n$ç»´å‘é‡ï¼Œå®ƒçš„åˆ†é‡å¯æ­£å¯è´Ÿï¼Œä¹Ÿæ²¡æœ‰é™å®šçš„ä¸Šä¸‹ç•Œã€‚$\Delta^{n-1}$å®šä¹‰ä¸ºå…¨ä½“$n$å…ƒç¦»æ•£æ¦‚ç‡åˆ†å¸ƒçš„é›†åˆï¼Œå³<br />
\begin{equation}\Delta^{n-1} = \left\{\boldsymbol{p}=(p_1,p_2,\cdots,p_n)\left|\, p_1,p_2,\cdots,p_n\geq 0,\sum_{i=1}^n p_i = 1\right.\right\}\end{equation}<br />
ä¹‹æ‰€ä»¥æ ‡æ³¨$n-1$è€Œä¸æ˜¯$n$ï¼Œæ˜¯å› ä¸ºçº¦æŸ$\sum\limits_{i=1}^n p_i = 1$å®šä¹‰äº†$n$ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ª$n-1$ç»´å­å¹³é¢ï¼Œå†åŠ ä¸Š$p_i\geq 0$çš„çº¦æŸï¼Œ$(p_1,p_2,\cdots,p_n)$çš„é›†åˆå°±åªæ˜¯è¯¥å¹³é¢çš„ä¸€ä¸ªå­é›†ï¼Œå³å®é™…ç»´åº¦åªæœ‰$n-1$ã€‚</p>
<p>åŸºäºè¿™äº›è®°å·ï¼Œæœ¬æ–‡çš„ä¸»é¢˜å°±å¯ä»¥ç®€å•è¡¨ç¤ºä¸ºæ¢è®¨$\mathbb{R}^n\mapsto\Delta^{n-1}$çš„æ˜ å°„ï¼Œå…¶ä¸­$\boldsymbol{x}\in\mathbb{R}^n$æˆ‘ä»¬ä¹ æƒ¯ç§°ä¹‹ä¸ºLogitsæˆ–è€…Scoresã€‚</p>
<h3 id="_1">åŸºæœ¬å®šä¹‰</h3>
<p>Softmaxçš„å®šä¹‰å¾ˆç®€å•ï¼š<br />
\begin{equation}p_i = softmax(\boldsymbol{x})<em j="1">i = \frac{e^{x_i}}{\sum\limits</em>}^n e^{x_j}}\end{equation<br />
Softmaxçš„æ¥æºå’Œè¯ é‡Šéƒ½å¤ªå¤šäº†ï¼Œæ¯”å¦‚èƒ½é‡æ¨¡å‹ã€ç»Ÿè®¡åŠ›å­¦æˆ–è€…å•çº¯ä½œä¸º$\text{argmax}$çš„å…‰æ»‘è¿‘ä¼¼ç­‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾ˆéš¾è€ƒè¯å®ƒçš„æœ€æ—©å‡ºå¤„ï¼Œä¹Ÿä¸å»åšè¿™ä¸ªå°è¯•äº†ã€‚å¾ˆå¤šæ—¶å€™æˆ‘ä»¬ä¹Ÿä¼šåŠ ä¸Šä¸€ä¸ªæ¸©åº¦å‚æ•°ï¼Œå³è€ƒè™‘$softmax(\boldsymbol{x}/\tau)$ï¼Œä½†$\tau$æœ¬èº«ä¹Ÿå¯ä»¥æ•´åˆåˆ°$\boldsymbol{x}$çš„å®šä¹‰ä¹‹ä¸­ï¼Œå› æ­¤è¿™é‡Œä¸ç‰¹åˆ«åˆ†ç¦»å‡º$\tau$å‚æ•°ã€‚</p>
<p>Softmaxçš„åˆ†æ¯æˆ‘ä»¬é€šå¸¸è®°ä¸º$Z(\boldsymbol{x})$ï¼Œå®ƒçš„å¯¹æ•°å°±æ˜¯å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½è‡ªå¸¦çš„$\text{logsumexp}$è¿ç®—ï¼Œä»–å®ƒæ˜¯$\max$çš„ä¸€ä¸ªå…‰æ»‘è¿‘ä¼¼ï¼š<br />
\begin{align}\log Z(\boldsymbol{x}) = \log \sum\limits_{j=1}^n e^{x_j} = \text{logsumexp}(\boldsymbol{x})\\\<br />
\lim_{\tau\to 0^+} \tau\,\text{logsumexp}(\boldsymbol{x}/\tau) = \max(\boldsymbol{x})\end{align}<br />
å½“$\tau$å–$1$æ—¶ï¼Œå°±å¯ä»¥å†™å‡º$\text{logsumexp}(\boldsymbol{x}) \approx \max(\boldsymbol{x})$ï¼Œ$\boldsymbol{x}$æ–¹å·®è¶Šå¤§è¿‘ä¼¼ç¨‹åº¦è¶Šé«˜ï¼Œæ›´è¿›ä¸€æ­¥çš„è®¨è®ºå¯ä»¥å‚è€ƒ<a href="/archives/3290">ã€Šå¯»æ±‚ä¸€ä¸ªå…‰æ»‘çš„æœ€å¤§å€¼å‡½æ•°ã€‹</a>ã€‚</p>
<h3 id="_2">ä¸¤ç‚¹æ€§è´¨</h3>
<p>é™¤äº†å°†ä»»æ„å‘é‡è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒå¤–ï¼ŒSoftmaxè¿˜æ»¡è¶³ä¸¤ç‚¹æ€§è´¨<br />
\begin{align}&amp;{\color{red}{å•è°ƒæ€§}}:\quad p_i &gt; p_j \Leftrightarrow x_i &gt; x_j,\quad p_i = p_j \Leftrightarrow x_i = x_j \\\[5pt]<br />
&amp;{\color{red}{ä¸å˜æ€§}}:\quad softmax(\boldsymbol{x}) = softmax(\boldsymbol{x} + c),\,\,\forall c\in\mathbb{R}<br />
\end{align}<br />
<strong>å•è°ƒæ€§</strong> æ„å‘³ç€Softmaxæ˜¯ä¿åºçš„ï¼Œ$\boldsymbol{x}$çš„æœ€å¤§å€¼/æœ€å°å€¼è·Ÿ$\boldsymbol{p}$çš„æœ€å¤§å€¼/æœ€å°å€¼ç›¸å¯¹åº”ï¼›<strong>ä¸å˜æ€§</strong> è¯´çš„æ˜¯$\boldsymbol{x}$çš„æ¯ä¸ªåˆ†é‡éƒ½åŠ ä¸ŠåŒä¸€ä¸ªå¸¸æ•°ï¼ŒSoftmaxçš„ç»“æœä¸å˜ï¼Œè¿™è·Ÿ$\text{argmax}$çš„æ€§è´¨æ˜¯ä¸€æ ·çš„ï¼Œå³åŒæ ·æœ‰$\text{argmax}(\boldsymbol{x}) = \text{argmax}(\boldsymbol{x} + c)$ã€‚</p>
<p>å› æ­¤ï¼Œæ ¹æ®è¿™ä¸¤ç‚¹æ€§è´¨æˆ‘ä»¬å¯ä»¥å¾—å‡ºï¼ŒSoftmaxå®é™…æ˜¯$\text{argmax}$ä¸€ä¸ªå…‰æ»‘è¿‘ä¼¼ï¼ˆæ›´å‡†ç¡®æ¥è¯´æ˜¯$\text{onehot}(\text{argmax}(\cdot))$çš„å…‰æ»‘è¿‘ä¼¼ï¼‰ï¼Œæ›´å…·ä½“åœ°æˆ‘ä»¬æœ‰<br />
\begin{equation}\lim_{\tau\to 0^+} softmax(\boldsymbol{x}/\tau) = \text{onehot}(\text{argmax}(\boldsymbol{x}))\end{equation}<br />
è¿™å¤§æ¦‚å°±æ˜¯Softmaxè¿™ä¸ªåå­—çš„æ¥æºã€‚æ³¨æ„ä¸è¦æ··æ·†äº†ï¼ŒSoftmaxæ˜¯$\text{argmax}$è€Œä¸æ˜¯$\max$çš„å…‰æ»‘è¿‘ä¼¼ï¼Œ$\max$çš„å…‰æ»‘è¿‘ä¼¼æ˜¯$\text{logsumexp}$æ‰å¯¹ã€‚</p>
<h3 id="_3">æ¢¯åº¦è®¡ç®—</h3>
<p>å¯¹äºæ·±åº¦å­¦ä¹ æ¥è¯´ï¼Œäº†è§£ä¸€ä¸ªå‡½æ•°çš„æ€§è´¨é‡è¦æ–¹å¼ä¹‹ä¸€æ˜¯äº†è§£å®ƒçš„æ¢¯åº¦ï¼Œå¯¹äºSoftmaxï¼Œæˆ‘ä»¬åœ¨<a href="/archives/9812">ã€Šä»æ¢¯åº¦æœ€å¤§åŒ–çœ‹Attentionçš„Scaleæ“ä½œã€‹</a>æ›¾ç»ç®—è¿‡ï¼š<br />
\begin{equation}\frac{\partial p_i}{\partial x_j} = p_i\delta_{i,j} - p_i p_j = \left\{\begin{aligned}<br />
p_i - p_i^2,&amp;\quad i=j\\\<br />
- p_i p_j,&amp;\quad i\neq j<br />
\end{aligned}\right.\end{equation}<br />
è¿™æ ·æ’åˆ—æˆçš„çŸ©é˜µä¹Ÿç§°ä¸ºSoftmaxçš„<a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_neterminant">é›…å¯æ¯”çŸ©é˜µï¼ˆJacobian Matrixï¼‰</a>ï¼Œå®ƒçš„L1èŒƒæ•°æœ‰ä¸€ä¸ªç®€å•çš„å½¢å¼<br />
\begin{equation}\frac{1}{2}\left\Vert\frac{\partial \boldsymbol{p}}{\partial \boldsymbol{x}}\right\Vert_1=\frac{1}{2}\sum_{i,j}\left|\frac{\partial p_i}{\partial x_j}\right|=\frac{1}{2}\sum_i (p_i - p_i^2) + \frac{1}{2}\sum_{i\neq j} p_i p_j = 1 - \sum_i p_i^2\end{equation}<br />
å½“$\boldsymbol{p}$æ˜¯one hotåˆ†å¸ƒæ—¶ï¼Œä¸Šå¼ç­‰äº0ï¼Œè¿™æ„å‘³ç€Softmaxçš„ç»“æœè¶Šæ¥è¿‘one hotï¼Œå®ƒçš„æ¢¯åº¦æ¶ˆå¤±ç°è±¡è¶Šä¸¥é‡ï¼Œæ‰€ä»¥è‡³å°‘åˆå§‹åŒ–é˜¶æ®µï¼Œæˆ‘ä»¬ä¸èƒ½å°†Softmaxåˆå§‹åŒ–å¾—æ¥è¿‘one hotã€‚åŒæ—¶ä¸Šå¼æœ€å³ç«¯ä¹Ÿè”ç³»åˆ°äº†<a href="/archives/9595#%E7%86%B5%E7%9A%84%E8%81%94%E7%B3%BB">RÃ©nyiç†µ</a>çš„æ¦‚å¿µï¼Œå®ƒè·Ÿå¸¸è§çš„é¦™ä¾¬ç†µç±»ä¼¼ã€‚</p>
<h3 id="_4">å‚è€ƒå®ç°</h3>
<p>Softmaxçš„ç›´æ¥å®ç°å¾ˆç®€å•ï¼Œç›´æ¥å–$\exp$ç„¶åå½’ä¸€åŒ–å°±è¡Œï¼ŒNumpyçš„å‚è€ƒä»£ç ä¸ºï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="nv">def</span><span class="w"> </span><span class="nv">softmax</span><span class="ss">(</span><span class="nv">x</span><span class="ss">)</span>:
<span class="w">    </span><span class="nv">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">np</span>.<span class="nv">exp</span><span class="ss">(</span><span class="nv">x</span><span class="ss">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nv">y</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">y</span>.<span class="nv">sum</span><span class="ss">()</span>
</code></pre></div>

<p>ç„¶è€Œï¼Œå¦‚æœ$\boldsymbol{x}$ä¸­å­˜åœ¨è¾ƒå¤§çš„åˆ†é‡ï¼Œé‚£ä¹ˆç®—$\exp$æ—¶å¾ˆå®¹æ˜“æº¢å‡ºï¼Œå› æ­¤æˆ‘ä»¬é€šå¸¸éƒ½è¦åˆ©ç”¨Softmaxçš„<strong>ä¸å˜æ€§</strong> ï¼Œå…ˆå°†æ¯ä¸ªåˆ†é‡å‡å»æ‰€æœ‰åˆ†é‡çš„æœ€å¤§å€¼ï¼Œç„¶åå†ç®—Softmaxï¼Œè¿™æ ·æ¯ä¸ªå–$\exp$çš„åˆ†é‡éƒ½ä¸å¤§äº0ï¼Œç¡®ä¿ä¸ä¼šæº¢å‡ºï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="nv">def</span><span class="w"> </span><span class="nv">softmax</span><span class="ss">(</span><span class="nv">x</span><span class="ss">)</span>:
<span class="w">    </span><span class="nv">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">np</span>.<span class="nv">exp</span><span class="ss">(</span><span class="nv">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nv">x</span>.<span class="nv">max</span><span class="ss">())</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nv">y</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nv">y</span>.<span class="nv">sum</span><span class="ss">()</span>
</code></pre></div>

<h3 id="_5">æŸå¤±å‡½æ•°</h3>
<p>æ„å»ºæ¦‚ç‡åˆ†å¸ƒçš„ä¸»è¦ç”¨é€”ä¹‹ä¸€æ˜¯ç”¨äºæ„å»ºå•æ ‡ç­¾å¤šåˆ†ç±»ä»»åŠ¡çš„è¾“å‡ºï¼Œå³å‡è®¾æœ‰ä¸€ä¸ª$n$åˆ†ç±»ä»»åŠ¡ï¼Œ$\boldsymbol{x}$æ˜¯æ¨¡å‹çš„è¾“å‡ºï¼Œé‚£ä¹ˆæˆ‘ä»¬å¸Œæœ›é€šè¿‡$\boldsymbol{p}=softmax(\boldsymbol{x})$æ¥é¢„æµ‹æ¯ä¸ªç±»çš„æ¦‚ç‡ã€‚ä¸ºäº†è®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå‡è®¾ç›®æ ‡ç±»åˆ«æ˜¯$t$ï¼Œå¸¸è§çš„é€‰æ‹©æ˜¯äº¤å‰ç†µæŸå¤±ï¼š<br />
\begin{equation}\mathcal{L}<em t_j="t,j">t = - \log p_t = - \log softmax(\boldsymbol{x})_t\end{equation}<br />
æˆ‘ä»¬å¯ä»¥æ±‚å¾—å®ƒçš„æ¢¯åº¦ï¼š<br />
\begin{equation}-\frac{\partial \log p_t}{\partial x_j} = p_j - \delta</em>} = \left\{\begin{aligned} p_t - 1,&amp;\quad j=t\\\ p_j,&amp;\quad j\neq t \end{aligned}\right.\end{equation<br />
æ³¨æ„$t$æ˜¯ç»™å®šçš„ï¼Œæ‰€ä»¥$\delta_{t,j}$å®é™…è¡¨è¾¾çš„æ˜¯ç›®æ ‡åˆ†å¸ƒ$\text{onehot(t)}$ï¼Œè€Œå…¨ä½“$p_j$å°±æ˜¯$\boldsymbol{p}$æœ¬èº«ï¼Œæ‰€ä»¥ä¸Šå¼å¯ä»¥æ›´ç›´è§‚åœ°å†™æˆï¼š<br />
\begin{equation}-\frac{\partial \log p_t}{\partial \boldsymbol{x}} = \boldsymbol{p} - \text{onehot(t)}\label{eq:softmax-ce-grad}\end{equation}<br />
ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒçš„æ¢¯åº¦æ­£å¥½æ˜¯ç›®æ ‡åˆ†å¸ƒä¸é¢„æµ‹åˆ†å¸ƒä¹‹å·®ï¼Œåªè¦ä¸¤è€…ä¸ç›¸ç­‰ï¼Œé‚£ä¹ˆæ¢¯åº¦å°±ä¸€ç›´å­˜åœ¨ï¼Œä¼˜åŒ–å°±å¯ä»¥æŒç»­ä¸‹å»ï¼Œè¿™æ˜¯äº¤å‰ç†µçš„ä¼˜ç‚¹ã€‚å½“ç„¶ï¼ŒæŸäº›æƒ…å†µä¸‹è¿™ä¹Ÿæ˜¯ç¼ºç‚¹ï¼Œå› ä¸ºSoftmaxåªæœ‰åœ¨$\tau\to 0^+$æ‰ä¼šå¾—åˆ°one hotï¼Œæ¢è¨€ä¹‹æ­£å¸¸æƒ…å†µä¸‹éƒ½ä¸ä¼šå‡ºç°one hotï¼Œå³ä¼˜åŒ–ä¸€ç›´ä¸ä¼šå®Œå…¨åœæ­¢ï¼Œé‚£ä¹ˆå°±æœ‰å¯èƒ½å¯¼è‡´è¿‡åº¦ä¼˜åŒ–ï¼Œè¿™ä¹Ÿæ˜¯åé¢çš„ä¸€äº›æ›¿ä»£å“çš„åŠ¨æœºã€‚</p>
<p>é™¤äº†äº¤å‰ç†µä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–ä¸€äº›æŸå¤±å¯ç”¨ï¼Œæ¯”å¦‚$-p_t$ï¼Œè¿™å¯ä»¥ç†è§£ä¸ºå‡†ç¡®ç‡çš„å…‰æ»‘è¿‘ä¼¼çš„ç›¸åæ•°ï¼Œä½†å®ƒå¯èƒ½ä¼šæœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œæ‰€ä»¥å®ƒçš„ä¼˜åŒ–æ•ˆç‡å¾€å¾€ä¸å¦‚äº¤å‰ç†µï¼Œä¸€èˆ¬åªé€‚ç”¨äºå¾®è°ƒè€Œä¸æ˜¯ä»é›¶è®­ç»ƒï¼Œæ›´å¤šè®¨è®ºå¯ä»¥å‚è€ƒ<a href="/archives/9098">ã€Šå¦‚ä½•è®­ç»ƒä½ çš„å‡†ç¡®ç‡ï¼Ÿã€‹</a>ã€‚</p>
<h2 id="softmax_1">Softmaxå˜ä½“</h2>
<p>ä»‹ç»å®ŒSoftmaxï¼Œæˆ‘ä»¬ç´§æ¥ç€æ€»ç»“ä¸€ä¸‹æœ¬åšå®¢ä»¥å¾€è®¨è®ºè¿‡Softmaxçš„ç›¸å…³å˜ä½“å·¥ä½œï¼Œæ¯”å¦‚Margin Softmaxã€Taylor Softmaxã€Sparse Softmaxç­‰ï¼Œå®ƒä»¬éƒ½æ˜¯åœ¨SoftmaxåŸºç¡€ä¸Šçš„è¡ç”Ÿå“ï¼Œä¾§é‡äºä¸åŒæ–¹é¢çš„æ”¹è¿›ï¼Œæ¯”å¦‚æŸå¤±å‡½æ•°ã€ã€ç¨€ç–æ€§ã€é•¿å°¾æ€§ç­‰ã€‚</p>
<h3 id="margin-softmax">Margin Softmax</h3>
<p>é¦–å…ˆæˆ‘ä»¬ä»‹ç»èµ·æºäºäººè„¸è¯†åˆ«çš„ä¸€ç³»åˆ—Softmaxå˜ä½“ï¼Œå®ƒä»¬å¯ä»¥ç»Ÿç§°ä¸ºMargin Softmaxï¼Œåæ¥ä¹Ÿè¢«åº”ç”¨åˆ°NLPçš„Sentence Embeddingè®­ç»ƒä¹‹ä¸­ï¼Œæœ¬ç«™æ›¾åœ¨<a href="/archives/5743">ã€ŠåŸºäºGRUå’Œam-softmaxçš„å¥å­ç›¸ä¼¼åº¦æ¨¡å‹ã€‹</a>è®¨è®ºè¿‡å…¶ä¸­çš„ä¸€ä¸ªå˜ä½“AM-Softmaxï¼Œåæ¥åˆ™åœ¨<a href="/archives/8656">ã€Šä»ä¸‰è§’ä¸ç­‰å¼åˆ°Margin Softmaxã€‹</a>æœ‰è¿‡æ›´ä¸€èˆ¬çš„è®¨è®ºã€‚</p>
<p>å°½ç®¡Margin Softmaxè¢«å† ä»¥Softmaxä¹‹åï¼Œä½†å®ƒå®é™…ä¸Šæ›´å¤šæ˜¯ä¸€ç§æŸå¤±å‡½æ•°æ”¹è¿›ã€‚ä»¥AM-Softmaxä¸ºä¾‹ï¼Œå®ƒæœ‰ä¸¤ä¸ªç‰¹ç‚¹ï¼šç¬¬ä¸€ï¼Œä»¥$\cos$å½¢å¼æ„é€ Logitsï¼Œå³$\boldsymbol{x} = [\cos(\boldsymbol{z},\boldsymbol{c}<em j_neq="j\neq" t="t">1),\cos(\boldsymbol{z},\boldsymbol{c}_2),\cdots,\cos(\boldsymbol{z},\boldsymbol{c}_n)]/\tau$çš„å½¢å¼ï¼Œæ­¤æ—¶çš„æ¸©åº¦å‚æ•°$\tau$æ˜¯å¿…é¡»çš„ï¼Œå› ä¸ºå•çº¯çš„$\cos$å€¼åŸŸä¸º$[-1,1]$ï¼Œä¸èƒ½æ‹‰å¼€ç±»æ¦‚ç‡ä¹‹é—´çš„å·®å¼‚ï¼›ç¬¬äºŒï¼Œå®ƒå¹¶ä¸æ˜¯ç®€å•åœ°ä»¥$-\log p_t$ä¸ºæŸå¤±ï¼Œè€Œæ˜¯åšäº†åŠ å¼ºï¼š<br />
\begin{equation}\mathcal{L} = - \log \frac{e^{[\cos(\boldsymbol{z},\boldsymbol{c}_t)-m]/\tau}}{e^{[\cos(\boldsymbol{z},\boldsymbol{c}_t)-m]/\tau} + \sum</em>} e^{\cos(\boldsymbol{z},\boldsymbol{c}_j)/\tau}}\end{equation<br />
ç›´è§‚æ¥çœ‹ï¼Œå°±æ˜¯äº¤å‰ç†µå¸Œæœ›$x_t$æ˜¯$\boldsymbol{x}$æ‰€æœ‰åˆ†é‡ä¸­æœ€å¤§çš„ä¸€ä¸ªï¼Œè€ŒAM-Softmaxåˆ™ä¸ä»…å¸Œæœ›$x_t$æœ€å¤§ï¼Œè¿˜å¸Œæœ›å®ƒè‡³å°‘æ¯”ç¬¬äºŒå¤§çš„åˆ†é‡å¤šå‡º$m/\tau$ï¼Œè¿™é‡Œçš„$m/\tau$å°±ç§°ä¸ºMarginã€‚</p>
<p>ä¸ºä»€ä¹ˆè¦å¢åŠ å¯¹ç›®æ ‡ç±»çš„è¦æ±‚å‘¢ï¼Ÿè¿™æ˜¯åº”ç”¨åœºæ™¯å¯¼è‡´çš„ã€‚åˆšæ‰è¯´äº†ï¼ŒMargin Softmaxèµ·æºäºäººè„¸è¯†åˆ«ï¼Œæ”¾åˆ°NLPä¸­åˆ™å¯ä»¥ç”¨äºè¯­ä¹‰æ£€ç´¢ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒçš„åº”ç”¨åœºæ™¯æ˜¯æ£€ç´¢ï¼Œä½†è®­ç»ƒæ–¹å¼æ˜¯åˆ†ç±»ã€‚å¦‚æœå•çº¯ç”¨åˆ†ç±»ä»»åŠ¡çš„äº¤å‰ç†µæ¥è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹ç¼–ç å‡ºæ¥çš„ç‰¹å¾ä¸ä¸€å®šèƒ½å¾ˆå¥½åœ°æ»¡è¶³æ£€ç´¢è¦æ±‚ï¼Œæ‰€ä»¥è¦åŠ ä¸ŠMarginä½¿å¾—ç‰¹å¾æ›´åŠ ç´§å‡‘ä¸€äº›ã€‚æ›´å…·ä½“çš„è®¨è®ºè¯·å‚è€ƒ<a href="/archives/8656">ã€Šä»ä¸‰è§’ä¸ç­‰å¼åˆ°Margin Softmaxã€‹</a>ä¸€æ–‡ï¼Œæˆ–è€…æŸ¥é˜…ç›¸å…³è®ºæ–‡ã€‚</p>
<h3 id="taylor-softmax">Taylor Softmax</h3>
<p>æ¥ä¸‹æ¥è¦ä»‹ç»çš„ï¼Œæ˜¯åœ¨<a href="/archives/7919">ã€Šexp(x)åœ¨x=0å¤„çš„å¶æ¬¡æ³°å‹’å±•å¼€å¼æ€»æ˜¯æ­£çš„ã€‹</a>è®¨è®ºè¿‡çš„Taylor Softmaxï¼Œå®ƒåˆ©ç”¨äº†$\exp(x)$çš„æ³°å‹’å±•å¼€å¼çš„ä¸€ä¸ªæœ‰è¶£æ€§è´¨ï¼š</p>
<blockquote>
<p>å¯¹äºä»»æ„å®æ•°$x$åŠå¶æ•°$k$ï¼Œæ€»æœ‰$f_k(x)\triangleq\sum\limits_{m=0}^k \frac{x^m}{m!} &gt; 0$ï¼Œå³$e^x$åœ¨$x=0$å¤„çš„å¶æ¬¡æ³°å‹’å±•å¼€å¼æ€»æ˜¯æ­£çš„ã€‚</p>
</blockquote>
<p>åˆ©ç”¨è¿™ä¸ªæ’æ­£æ€§ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªSoftmaxå˜ä½“ï¼ˆ$k &gt; 0$æ˜¯ä»»æ„å¶æ•°ï¼‰ï¼š<br />
\begin{equation}taylor\text{-}softmax(\boldsymbol{x}, k)<em j="1">i = \frac{f_k(x_i)}{\sum\limits</em>}^n f_k(x_j)}\end{equation<br />
ç”±äºæ˜¯åŸºäº$\exp$çš„æ³°å‹’å±•å¼€å¼æ„å»ºçš„ï¼Œæ‰€ä»¥åœ¨ä¸€å®šèŒƒå›´å†…Taylor Softmaxä¸Softmaxæœ‰ä¸€å®šçš„è¿‘ä¼¼å…³äºï¼ŒæŸäº›åœºæ™¯ä¸‹å¯ä»¥ç”¨Taylor Softmaxæ›¿æ¢Softmaxã€‚é‚£ä¹ˆTaylor Softmaxæœ‰ä»€ä¹ˆç‰¹ç‚¹å‘¢ï¼Ÿç­”æ¡ˆæ˜¯æ›´åŠ é•¿å°¾ï¼Œå› ä¸ºTaylor Softmaxæ˜¯å¤šé¡¹å¼å‡½æ•°å½’ä¸€åŒ–ï¼Œç›¸æ¯”æŒ‡æ•°å‡½æ•°è¡°å‡å¾—æ›´æ…¢ï¼Œæ‰€ä»¥å¯¹äºå°¾éƒ¨çš„ç±»åˆ«ï¼ŒTaylor Softmaxå¾€å¾€èƒ½å¤Ÿç»™å…¶åˆ†é…æ›´é«˜çš„æ¦‚ç‡ï¼Œå¯èƒ½æœ‰åŠ©äºç¼“è§£Softmaxçš„è¿‡åº¦è‡ªä¿¡ç°è±¡ã€‚</p>
<p>Taylor Softmaxçš„æœ€æ–°åº”ç”¨ï¼Œæ˜¯ç”¨æ¥æ›¿æ¢Attentionä¸­çš„Softmaxï¼Œä½¿å¾—åŸæœ¬çš„å¹³æ–¹å¤æ‚åº¦é™ä½ä¸ºçº¿æ€§å¤æ‚åº¦ï¼Œç›¸å…³ç†è®ºæ¨å¯¼å¯ä»¥å‚è€ƒ<a href="/archives/8601">ã€ŠTransformerå‡çº§ä¹‹è·¯ï¼š5ã€ä½œä¸ºæ— é™ç»´çš„çº¿æ€§Attentionã€‹</a>ã€‚è¯¥æ€è·¯çš„æœ€æ–°å®è·µæ˜¯ä¸€ä¸ªåä¸ºBasedçš„æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨$e^x\approx 1+x+x^2/2$æ¥çº¿æ€§åŒ–Attentionï¼Œå£°ç§°æ¯”Attentioné«˜æ•ˆä¸”æ¯”Mambaæ•ˆæœæ›´å¥½ï¼Œè¯¦ç»†ä»‹ç»å¯ä»¥å‚è€ƒåšå®¢<a href="https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based">ã€ŠZoology (Blogpost 2): Simple, Input-Dependent, and Sub-Quadratic Sequence Mixersã€‹</a>å’Œ<a href="https://www.together.ai/blog/based">ã€ŠBASED: Simple linear attention language models balance the recall-throughput tradeoffã€‹</a>ã€‚</p>
<h3 id="sparse-softmax">Sparse Softmax</h3>
<p>Sparse Softmaxæ˜¯ç¬”è€…å‚åŠ 2020å¹´æ³•ç ”æ¯æ—¶æå‡ºçš„ä¸€ä¸ªç®€å•çš„Softmaxç¨€ç–å˜ä½“ï¼Œé¦–å…ˆå‘è¡¨åœ¨åšæ–‡<a href="/archives/8046">ã€ŠSPACESï¼šâ€œæŠ½å–-ç”Ÿæˆâ€å¼é•¿æ–‡æœ¬æ‘˜è¦ï¼ˆæ³•ç ”æ¯æ€»ç»“ï¼‰ã€‹</a>ï¼Œåæ¥ä¹Ÿè¡¥å……äº†ç›¸å…³å®éªŒï¼Œå†™äº†ç¯‡ç®€å•çš„è®ºæ–‡<a href="https://papers.cool/arxiv/2112.12433">ã€ŠSparse-softmax: A Simpler and Faster Alternative Softmax Transformationã€‹</a>ã€‚</p>
<p>æˆ‘ä»¬çŸ¥é“ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬å¸¸ç”¨ç¡®å®šæ€§çš„Beam Searchè§£ç ï¼Œæˆ–è€…éšæœºæ€§çš„TopK/TopP Samplingé‡‡æ ·ï¼Œè¿™äº›ç®—æ³•çš„ç‰¹ç‚¹éƒ½æ˜¯åªä¿ç•™äº†é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„è‹¥å¹²ä¸ªTokenè¿›è¡Œéå†æˆ–è€…é‡‡æ ·ï¼Œä¹Ÿå°±ç­‰ä»·äºå°†å‰©ä½™çš„Tokenæ¦‚ç‡è§†ä¸ºé›¶ï¼Œè€Œè®­ç»ƒæ—¶å¦‚æœç›´æ¥ä½¿ç”¨Softmaxæ¥æ„å»ºæ¦‚ç‡åˆ†å¸ƒçš„è¯ï¼Œé‚£ä¹ˆå°±ä¸å­˜åœ¨ç»å¯¹ç­‰äºé›¶çš„å¯èƒ½ï¼Œè¿™å°±è®©è®­ç»ƒå’Œé¢„æµ‹å‡ºç°äº†ä¸ä¸€è‡´æ€§ã€‚Sparse Softmaxå°±æ˜¯å¸Œæœ›èƒ½å¤„ç†è¿™ç§ä¸ä¸€è‡´æ€§ï¼Œæ€è·¯å¾ˆç®€å•ï¼Œå°±æ˜¯åœ¨è®­ç»ƒçš„æ—¶å€™ä¹ŸæŠŠTop-$k$ä»¥å¤–çš„Tokenæ¦‚ç‡ç½®é›¶ï¼š<br />
\begin{array}{c|c|c}<br />
\hline<br />
&amp; Softmax &amp; Sparse\text{ }Softmax \\\<br />
\hline<br />
\text{åŸºæœ¬å®šä¹‰} &amp; p_i = \frac{e^{x_i}}{\sum\limits_{j=1}^n e^{x_j}} &amp; p_i=\left\{\begin{aligned}&amp;\frac{e^{x_i}}{\sum\limits_{j\in\Omega_k} e^{x_j}},\,i\in\Omega_k\\\ &amp;\quad 0,\,i\not\in\Omega_k\end{aligned}\right.\\\<br />
\hline<br />
\text{æŸå¤±å‡½æ•°} &amp; \log\left(\sum\limits_{i=1}^n e^{x_i}\right) - x_t &amp; \log\left(\sum\limits_{i\in\Omega_k} e^{x_i}\right) - x_t\\\<br />
\hline<br />
\end{array}</p>
<p>å…¶ä¸­$\Omega_k$æ˜¯å°†$x_1,x_2,\cdots,x_n$ä»å¤§åˆ°å°æ’åˆ—åå‰$k$ä¸ªå…ƒç´ çš„åŸå§‹ä¸‹æ ‡é›†åˆã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯åœ¨è®­ç»ƒé˜¶æ®µå°±è¿›è¡Œä¸é¢„æµ‹é˜¶æ®µä¸€è‡´çš„é˜¶æ®µæ“ä½œã€‚è¿™é‡Œçš„$\Omega_k$é€‰å–æ–¹å¼ä¹Ÿå¯ä»¥æŒ‰ç…§Nucleus Samplingçš„Top-$p$æ–¹å¼æ¥æ“ä½œï¼Œçœ‹å…·ä½“éœ€æ±‚è€Œå®šã€‚ä½†è¦æ³¨æ„çš„æ˜¯ï¼ŒSparse Softmaxå¼ºè¡Œæˆªæ–­äº†å‰©ä½™éƒ¨åˆ†çš„æ¦‚ç‡ï¼Œæ„å‘³ç€è¿™éƒ¨åˆ†Logitsæ— æ³•è¿›è¡Œåå‘ä¼ æ’­äº†ï¼Œå› æ­¤Sparse Softmaxçš„è®­ç»ƒæ•ˆç‡æ˜¯ä¸å¦‚Softmaxçš„ï¼Œæ‰€ä»¥å®ƒä¸€èˆ¬åªé€‚ç”¨äºå¾®è°ƒåœºæ™¯ï¼Œè€Œä¸é€‚ç”¨äºä»é›¶è®­ç»ƒã€‚</p>
<h2 id="perturb-max">Perturb Max</h2>
<p>è¿™ä¸€èŠ‚æˆ‘ä»¬ä»‹ç»ä¸€ç§æ–°çš„æ„å»ºæ¦‚ç‡åˆ†å¸ƒçš„æ–¹å¼ï¼Œè¿™é‡Œç§°ä¹‹ä¸ºPerturb Maxï¼Œå®ƒæ˜¯Gumbel Maxçš„ä¸€èˆ¬åŒ–ï¼Œåœ¨æœ¬ç«™ä¸­é¦–æ¬¡ä»‹ç»äºåšå®¢<a href="/archives/9085">ã€Šä»é‡å‚æ•°çš„è§’åº¦çœ‹ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒçš„æ„å»ºã€‹</a>ï¼Œæ­¤å¤–åœ¨è®ºæ–‡<a href="https://papers.cool/arxiv/2205.09615">ã€ŠEXACT: How to Train Your Accuracyã€‹</a>ä¹Ÿæœ‰è¿‡ç›¸å…³è®¨è®ºï¼Œè‡³äºæ›´æ—©çš„å‡ºå¤„ç¬”è€…åˆ™æ²¡æœ‰è¿›ä¸€æ­¥è€ƒç©¶äº†ã€‚</p>
<h3 id="_6">é—®é¢˜åæ€</h3>
<p>é¦–å…ˆæˆ‘ä»¬çŸ¥é“ï¼Œæ„å»ºä¸€ä¸ª$\mathbb{R}^n\mapsto\Delta^{n-1}$çš„æ˜ å°„å¹¶ä¸æ˜¯éš¾äº‹ï¼Œåªè¦$f(x)$æ˜¯$\mathbb{R}\mapsto \mathbb{R}^<em>$ï¼ˆå®æ•°åˆ°éè´Ÿå®æ•°ï¼‰çš„æ˜ å°„ï¼Œå¦‚$x^2$ï¼Œé‚£ä¹ˆåªéœ€è¦è®©<br />
\begin{equation}p_i = \frac{f(x_i)}{\sum\limits_{j=1}^n f(x_j)}\end{equation}<br />
å°±æ˜¯æ»¡è¶³æ¡ä»¶çš„æ˜ å°„äº†ã€‚å¦‚æœè¦åŠ ä¸Šâ€œä¸¤ç‚¹æ€§è´¨â€ä¸­çš„â€œ</em><em>å•è°ƒæ€§</em><em> â€å‘¢ï¼Ÿé‚£ä¹ˆä¹Ÿä¸éš¾ï¼Œåªéœ€è¦ä¿è¯$\mathbb{R}\mapsto \mathbb{R}^</em>$çš„å•è°ƒé€’å¢å‡½æ•°ï¼Œè¿™æ ·çš„å‡½æ•°ä¹Ÿæœ‰å¾ˆå¤šï¼Œæ¯”å¦‚$\text{sigmoid}(x)$ã€‚ä½†å¦‚æœå†åŠ ä¸Šâ€œ<strong>ä¸å˜æ€§</strong> â€å‘¢ï¼Ÿæˆ‘ä»¬è¿˜èƒ½éšä¾¿å†™å‡ºä¸€ä¸ªæ»¡è¶³<strong>ä¸å˜æ€§</strong> çš„$\mathbb{R}^n\mapsto\Delta^{n-1}$æ˜ å°„å—ï¼Ÿï¼ˆåæ­£æˆ‘ä¸èƒ½ï¼‰</p>
<p>å¯èƒ½æœ‰è¯»è€…ç–‘é—®ï¼šä¸ºä»€ä¹ˆéè¦ä¿æŒ<strong>å•è°ƒæ€§</strong> å’Œ<strong>ä¸å˜æ€§</strong> å‘¢ï¼Ÿçš„ç¡®ï¼Œå•çº¯ä»æ‹Ÿåˆæ¦‚ç‡åˆ†å¸ƒçš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸¤ç‚¹ä¼¼ä¹éƒ½æ²¡ä»€ä¹ˆå¿…è¦ï¼Œåæ­£éƒ½æ˜¯â€œåŠ›å¤§ç –é£â€ï¼Œåªè¦æ¨¡å‹è¶³å¤Ÿå¤§ï¼Œé‚£ä¹ˆæ²¡å•¥ä¸èƒ½æ‹Ÿåˆçš„ã€‚ä½†ä»â€œSoftmaxæ›¿ä»£å“â€è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ–°å®šä¹‰çš„æ¦‚ç‡åˆ†å¸ƒåŒæ ·èƒ½ä½œä¸º$\text{argmax}$çš„å…‰æ»‘è¿‘ä¼¼ï¼Œé‚£ä¹ˆå°±è¦å°½å¯èƒ½å¤šä¿æŒè·Ÿ$\text{argmax}$ç›¸åŒçš„æ€§è´¨ï¼Œè¿™æ˜¯æˆ‘ä»¬å¸Œæœ›ä¿æŒ<strong>å•è°ƒæ€§</strong> å’Œ<strong>ä¸å˜æ€§</strong> çš„ä¸»è¦åŸå› ã€‚</p>
<h3 id="gumbel-max">Gumbel Max</h3>
<p>Perturb Maxå€ŸåŠ©äºGumbel Maxçš„ä¸€èˆ¬åŒ–æ¥æ„é€ è¿™æ ·çš„ä¸€ç±»åˆ†å¸ƒã€‚ä¸ç†Ÿæ‚‰Gumbel Maxçš„è¯»è€…ï¼Œå¯ä»¥å…ˆåˆ°<a href="/archives/6705">ã€Šæ¼«è°ˆé‡å‚æ•°ï¼šä»æ­£æ€åˆ†å¸ƒåˆ°Gumbel Softmaxã€‹</a>äº†è§£ä¸€ä¸‹Gumbel Maxã€‚ç®€å•æ¥è¯´ï¼ŒGumbel Maxå°±æ˜¯å‘ç°ï¼š<br />
\begin{equation}P[\text{argmax}(\boldsymbol{x}+\boldsymbol{\varepsilon}) = i] = softmax(\boldsymbol{x})_i,\quad \boldsymbol{\varepsilon}\sim Gumbel\text{ }Noise\end{equation}<br />
æ€ä¹ˆç†è§£è¿™ä¸ªç»“æœå‘¢ï¼Ÿé¦–å…ˆï¼Œè¿™é‡Œçš„$\boldsymbol{\varepsilon}\sim Gumbel\text{ }Noise$æ˜¯æŒ‡$\boldsymbol{\varepsilon}$çš„æ¯ä¸ªåˆ†é‡éƒ½æ˜¯ä»<a href="https://en.wikipedia.org/wiki/Gumbel_distribution">Gumbelåˆ†å¸ƒ</a>ç‹¬ç«‹é‡å¤é‡‡æ ·å‡ºæ¥çš„ï¼›æ¥ç€ï¼Œæˆ‘ä»¬çŸ¥é“ç»™å®šå‘é‡$\boldsymbol{x}$ï¼Œæœ¬æ¥$\text{argmax}(\boldsymbol{x})$æ˜¯ç¡®å®šçš„ç»“æœï¼Œä½†åŠ äº†éšæœºå™ªå£°$\boldsymbol{\varepsilon}$ä¹‹åï¼Œ$\text{argmax}(\boldsymbol{x}+\boldsymbol{\varepsilon})$çš„ç»“æœä¹Ÿå¸¦æœ‰éšæœºæ€§äº†ï¼Œäºæ˜¯æ¯ä¸ª$i$éƒ½æœ‰è‡ªå·±çš„æ¦‚ç‡ï¼›æœ€åï¼ŒGumbel Maxå‘Šè¯‰æˆ‘ä»¬ï¼Œå¦‚æœåŠ çš„æ˜¯Gumbelå™ªå£°ï¼Œé‚£ä¹ˆ$i$çš„å‡ºç°æ¦‚ç‡æ­£å¥½æ˜¯$softmax(\boldsymbol{x})_i$ã€‚</p>
<p>Gumbel Maxæœ€ç›´æ¥çš„ä½œç”¨ï¼Œå°±æ˜¯æä¾›äº†ä¸€ç§ä»åˆ†å¸ƒ$softmax(\boldsymbol{x})$ä¸­é‡‡æ ·çš„æ–¹å¼ï¼Œå½“ç„¶å¦‚æœå•çº¯é‡‡æ ·è¿˜æœ‰æ›´ç®€å•çš„æ–¹æ³•ï¼Œæ²¡å¿…è¦â€œæ€é¸¡ç”¨ç‰›åˆ€â€ã€‚Gumbel Maxæœ€å¤§çš„ä»·å€¼æ˜¯â€œé‡å‚æ•°ï¼ˆReparameterizationï¼‰â€ï¼Œå®ƒå°†é—®é¢˜çš„éšæœºæ€§ä»å¸¦å‚æ•°$\boldsymbol{x}$çš„ç¦»æ•£åˆ†å¸ƒè½¬ç§»åˆ°äº†ä¸å¸¦å‚æ•°çš„$\boldsymbol{\varepsilon}$ä¸Šï¼Œå†ç»“åˆSoftmaxæ˜¯$\text{argmax}$çš„å…‰æ»‘è¿‘ä¼¼ï¼Œæˆ‘ä»¬å¾—åˆ°$softmax(\boldsymbol{x} + \boldsymbol{\varepsilon})$æ˜¯Gumbel Maxçš„å…‰æ»‘è¿‘ä¼¼ï¼Œè¿™ä¾¿æ˜¯Gumbel Softmaxï¼Œæ˜¯è®­ç»ƒâ€œç¦»æ•£é‡‡æ ·æ¨¡å—ä¸­å¸¦æœ‰å¯å­¦å‚æ•°â€çš„æ¨¡å‹çš„å¸¸ç”¨æŠ€å·§ã€‚</p>
<h3 id="_7">ä¸€èˆ¬å™ªå£°</h3>
<p>Perturb Maxç›´æ¥æºè‡ªGumbel Maxï¼šæ—¢ç„¶Softmaxå¯ä»¥ä»Gumbelåˆ†å¸ƒä¸­å¯¼å‡ºï¼Œé‚£ä¹ˆå¦‚æœå°†Gumbelåˆ†å¸ƒæ¢ä¸ºä¸€èˆ¬çš„åˆ†å¸ƒï¼Œæ¯”å¦‚æ­£æ€åˆ†å¸ƒï¼Œä¸å°±å¯ä»¥å¯¼å‡ºæ–°çš„æ¦‚ç‡åˆ†å¸ƒå½¢å¼äº†ï¼Ÿä¹Ÿå°±æ˜¯è¯´ç›´æ¥å®šä¹‰<br />
\begin{equation}p_i = P[\text{argmax}(\boldsymbol{x}+\boldsymbol{\varepsilon}) = i],\quad \varepsilon_1,\varepsilon_2,\cdots,\varepsilon_n\sim p(\varepsilon)\end{equation}<br />
é‡å¤Gumbel Maxçš„æ¨å¯¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°<br />
\begin{equation}p_i = \int_{-\infty}^{\infty} p(\varepsilon_i)\left[\prod_{j\neq i} \Phi(x_i - x_j + \varepsilon_i)\right]d\varepsilon_i = \mathbb{E}<em i="i" j_neq="j\neq">{\varepsilon}\left[\prod</em>} \Phi(x_i - x_j + \varepsilon)\right]\end{equation<br />
å…¶ä¸­$\Phi(\varepsilon)$æ˜¯$p(\varepsilon)$çš„ç´¯ç§¯æ¦‚ç‡å‡½æ•°ã€‚å¯¹äºä¸€èˆ¬çš„åˆ†å¸ƒï¼Œå“ªæ€•æ˜¯ç®€å•çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä¸Šå¼éƒ½å¾ˆéš¾å¾—å‡ºè§£æè§£ï¼Œæ‰€ä»¥åªèƒ½æ•°å€¼ä¼°è®¡ã€‚ä¸ºäº†å¾—åˆ°ç¡®å®šæ€§çš„è®¡ç®—ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨é€†ç´¯ç§¯æ¦‚ç‡å‡½æ•°çš„æ–¹å¼è¿›è¡Œå‡åŒ€é‡‡æ ·ï¼Œå³å…ˆä»$[0,1]$å‡åŒ€é€‰å–$t$ï¼Œç„¶åé€šè¿‡æ±‚è§£$t=\Phi(\varepsilon)$æ¥å¾—åˆ°$\varepsilon$ã€‚</p>
<p>ä»Perturb Maxçš„å®šä¹‰æˆ–è€…æœ€å$p_i$çš„å½¢å¼æˆ‘ä»¬éƒ½å¯ä»¥æ–­è¨€Perturb Maxæ»¡è¶³<strong>å•è°ƒæ€§</strong> å’Œ<strong>ä¸å˜æ€§</strong> ï¼Œè¿™é‡Œå°±ä¸è¯¦ç»†å±•å¼€äº†ã€‚é‚£å®ƒåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æœ‰ç‹¬ç‰¹ä½œç”¨å‘¢ï¼Ÿè¯´å®è¯ï¼Œè¿˜çœŸä¸çŸ¥é“ï¼Œ<a href="https://papers.cool/arxiv/2205.09615">ã€ŠEXACT: How to Train Your Accuracyã€‹</a>ä¸€æ–‡ç”¨å®ƒæ¥æ„å»ºæ–°çš„æ¦‚ç‡åˆ†å¸ƒå¹¶ä¼˜åŒ–å‡†ç¡®ç‡çš„å…‰æ»‘è¿‘ä¼¼ï¼Œä½†ç¬”è€…è‡ªå·±çš„å®éªŒæ˜¾ç¤ºæ²¡æœ‰ç‰¹åˆ«çš„æ•ˆæœã€‚ä¸ªäººæ„Ÿè§‰ï¼Œå¯èƒ½åœ¨æŸäº›éœ€è¦é‡å‚æ•°çš„åœºæ™¯èƒ½å¤Ÿè¡¨ç°å‡ºç‰¹æ®Šçš„ä½œç”¨å§ã€‚</p>
<h2 id="sparsemax">Sparsemax</h2>
<p>æ¥ä¸‹æ¥è¦ç™»åœºçš„æ˜¯åä¸ºSparsemaxçš„æ¦‚ç‡æ˜ å°„ï¼Œå‡ºè‡ª2016å¹´çš„è®ºæ–‡<a href="https://papers.cool/arxiv/1602.02068">ã€ŠFrom Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classificationã€‹</a>ï¼Œå®ƒè·Ÿç¬”è€…æå‡ºçš„Sparse Softmaxä¸€æ ·ï¼Œéƒ½æ˜¯é¢å‘ç¨€ç–æ€§çš„æ”¹åŠ¨ï¼Œä½†ä½œè€…çš„åŠ¨æœºæ˜¯ç”¨åœ¨Attentionä¸­æä¾›æ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚è·ŸSparse Softmaxç›´æ¥å¼ºè¡Œæˆªæ–­Top-$k$ä¸ªåˆ†é‡ä¸åŒï¼ŒSparsemaxæä¾›äº†ä¸€ä¸ªæ›´ä¸ºè‡ªé€‚åº”çš„ç¨€ç–å‹æ¦‚ç‡åˆ†å¸ƒæ„é€ æ–¹å¼ã€‚</p>
<h3 id="_8">åŸºæœ¬å®šä¹‰</h3>
<p>åŸè®ºæ–‡å°†Sparsemaxå®šä¹‰ä¸ºå¦‚ä¸‹ä¼˜åŒ–é—®é¢˜çš„è§£ï¼š<br />
\begin{equation}sparsemax(\boldsymbol{x}) = \mathop{\text{argmin}}\limits_{\boldsymbol{p}\in\Delta^{n-1}}\Vert \boldsymbol{p} - \boldsymbol{x}\Vert^2\label{eq:sparsemax-opt}\end{equation}<br />
é€šè¿‡æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•å°±å¯ä»¥æ±‚å‡ºç²¾ç¡®è§£çš„è¡¨è¾¾å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹å¼å¹¶ä¸ç›´è§‚ï¼Œè€Œä¸”ä¹Ÿä¸å®¹æ˜“æ­ç¤ºå®ƒè·ŸSoftmaxçš„è”ç³»ã€‚ä¸‹é¢æä¾›ç¬”è€…æ„æ€çš„ä¸€ç§ç§ä»¥ä¸ºæ›´åŠ ç®€æ˜çš„å¼•å‡ºæ–¹å¼ã€‚</p>
<p>é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼ŒSoftmaxå¯ä»¥ç­‰ä»·åœ°è¡¨ç¤ºä¸º<br />
\begin{equation}\boldsymbol{p} = softmax(\boldsymbol{x}) = \exp(\boldsymbol{x} - \lambda(\boldsymbol{x}))\label{eq:sparsemax-softmax}\end{equation}<br />
å…¶ä¸­$\lambda(\boldsymbol{x})$æ˜¯ä½¿å¾—$\boldsymbol{p}$çš„å„åˆ†é‡ä¹‹å’Œä¸º1çš„å¸¸æ•°ï¼Œå¯¹äºSoftmaxæˆ‘ä»¬å¯ä»¥æ±‚å‡º$\lambda(\boldsymbol{x})=\log\sum\limits_i e^{x_i}$ã€‚</p>
<p>ç„¶åï¼Œåœ¨Taylor Softmaxé‚£ä¸€èŠ‚æˆ‘ä»¬è¯´äº†ï¼Œ$\exp(x)$çš„å¶æ¬¡æ³°å‹’å±•å¼€æ€»æ˜¯æ­£çš„ï¼Œå› æ­¤å¯ä»¥ç”¨å¶æ¬¡æ³°å‹’å±•å¼€æ¥æ„å»ºSoftmaxå˜ä½“ã€‚ä½†å¦‚æœæ˜¯å¥‡æ•°æ¬¡å‘¢ï¼Ÿæ¯”å¦‚$\exp(x)\approx 1 + x$ï¼Œå®ƒå¹¶ä¸æ€»æ˜¯éè´Ÿçš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥åŠ ä¸ª$\text{relu}$å¼ºè¡Œè®©å®ƒå˜æˆéè´Ÿçš„ï¼Œå³$\exp(x)\approx \text{relu}(1 + x)$ï¼Œç”¨è¿™ä¸ªè¿‘ä¼¼æ›¿æ¢æ‰å¼$\eqref{eq:sparsemax-softmax}$çš„$\exp$ï¼Œå°±å¾—åˆ°äº†Sparsemaxï¼š<br />
\begin{equation}\boldsymbol{p} = sparsemax(\boldsymbol{x}) = \text{relu}(1+\boldsymbol{x} - \lambda(\boldsymbol{x}))\end{equation}<br />
å…¶ä¸­$\lambda(\boldsymbol{x})$ä¾ç„¶æ˜¯ä½¿å¾—$\boldsymbol{p}$çš„å„åˆ†é‡ä¹‹å’Œä¸º1çš„å¸¸æ•°ï¼Œå¹¶ä¸”å¸¸æ•°$1$ä¹Ÿå¯ä»¥æ•´åˆåˆ°$\lambda(\boldsymbol{x})$ä¹‹ä¸­ï¼Œæ‰€ä»¥ä¸Šå¼ä¹Ÿç­‰ä»·äº<br />
\begin{equation}\boldsymbol{p} = sparsemax(\boldsymbol{x}) = \text{relu}(\boldsymbol{x} - \lambda(\boldsymbol{x}))\end{equation}</p>
<h3 id="_9">æ±‚è§£ç®—æ³•</h3>
<p>åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒSparsemaxè¿˜åªæ˜¯ä¸€ä¸ªå½¢å¼åŒ–çš„å®šä¹‰ï¼Œå› ä¸º$\lambda(\boldsymbol{x})$çš„å…·ä½“è®¡ç®—æ–¹æ³•å°šä¸æ¸…æ¥šï¼Œè¿™å°±æ˜¯æœ¬èŠ‚éœ€è¦æ¢è®¨çš„é—®é¢˜ã€‚ä¸è¿‡å³ä¾¿å¦‚æ­¤ï¼Œå•é è¿™ä¸ªå®šä¹‰æˆ‘ä»¬ä¹Ÿä¸éš¾çœ‹å‡ºSparsemaxæ»¡è¶³<strong>å•è°ƒæ€§</strong> å’Œ<strong>ä¸å˜æ€§</strong> ä¸¤ç‚¹æ€§è´¨ï¼Œå¦‚æœè¿˜è§‰å¾—ä¸å¤§è‚¯å®šçš„è¯»è€…ï¼Œå¯ä»¥è‡ªè¡Œå°è¯•è¯æ˜ä¸€ä¸‹å®ƒã€‚</p>
<p>ç°åœ¨æˆ‘ä»¬è½¬å‘$\lambda(\boldsymbol{x})$çš„è®¡ç®—ã€‚ä¸å¤±ä¸€èˆ¬æ€§ï¼Œæˆ‘ä»¬å‡è®¾$\boldsymbol{x}$å„åˆ†é‡å·²ç»ä»å¤§åˆ°å°æ’åºå¥½ï¼Œå³$x_1\geq x_2\geq \cdots\geq x_n$ï¼Œæ¥ç€æˆ‘ä»¬ä¸å¦¨å…ˆå‡è®¾å·²çŸ¥$x_k\geq \lambda(\boldsymbol{x})\geq x_{k+1}$ï¼Œé‚£ä¹ˆå¾ˆæ˜¾ç„¶<br />
\begin{equation}sparsemax(\boldsymbol{x}) = [x_1 - \lambda(\boldsymbol{x}),\cdots,x_k - \lambda(\boldsymbol{x}),0,\cdots,0]\end{equation}<br />
æ ¹æ®$\lambda(\boldsymbol{x})$çš„å®šä¹‰ï¼Œæˆ‘ä»¬æœ‰<br />
\begin{equation}\sum_{i=1}^k [x_i - \lambda(\boldsymbol{x})] = 1\quad\Rightarrow\quad 1 + k\lambda(\boldsymbol{x}) = \sum_{i=1}^k x_i\end{equation}<br />
è¿™å°±å¯ä»¥æ±‚å‡º$\lambda(\boldsymbol{x})$ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬æ— æ³•äº‹å…ˆçŸ¥é“$x_k\geq \lambda(\boldsymbol{x})\geq x_{k+1}$ï¼Œä½†æˆ‘ä»¬å¯ä»¥éå†$k=1,2,\cdots,n$ï¼Œåˆ©ç”¨ä¸Šå¼æ±‚ä¸€é$\lambda_k(\boldsymbol{x})$ï¼Œå–æ»¡è¶³$x_k\geq \lambda_k(\boldsymbol{x})\geq x_{k+1}$é‚£ä¸€ä¸ª$\lambda_k(\boldsymbol{x})$ï¼Œè¿™ä¹Ÿå¯ä»¥ç­‰ä»·åœ°è¡¨ç¤ºä¸ºæ±‚æ»¡è¶³$x_k\geq \lambda_k(\boldsymbol{x})$çš„æœ€å¤§çš„$k$ï¼Œç„¶åè¿”å›å¯¹åº”çš„$\lambda_k(\boldsymbol{x})$</p>
<p>å‚è€ƒå®ç°ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="s s-Atom">def</span> <span class="nf">sparsemax</span><span class="p">(</span><span class="s s-Atom">x</span><span class="p">)</span><span class="o">:</span>
    <span class="s s-Atom">x_sort</span> <span class="o">=</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="s s-Atom">x</span><span class="p">)[</span><span class="o">::-</span><span class="mi">1</span><span class="p">]</span>
    <span class="s s-Atom">x_lamb</span> <span class="o">=</span> <span class="p">(</span><span class="s s-Atom">np</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="s s-Atom">x_sort</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="s s-Atom">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="s s-Atom">lamb</span> <span class="o">=</span> <span class="s s-Atom">x_lamb</span><span class="p">[(</span><span class="s s-Atom">x_sort</span> <span class="o">&gt;=</span> <span class="s s-Atom">x_lamb</span><span class="p">).</span><span class="nf">argmin</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="s s-Atom">return</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="s s-Atom">x</span> <span class="o">-</span> <span class="s s-Atom">lamb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<h3 id="_10">æ¢¯åº¦è®¡ç®—</h3>
<p>æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å¼•å…¥è®°å·<br />
\begin{equation}\Omega(\boldsymbol{x}) = \big\{k\big|x_k &gt; \lambda(\boldsymbol{x})\big\}\end{equation}<br />
é‚£ä¹ˆå¯ä»¥å†™å‡º<br />
\begin{equation}\boldsymbol{p} = sparsemax(\boldsymbol{x}) = \left\{\begin{aligned}<br />
&amp;x_i - \frac{1}{|\Omega(\boldsymbol{x})|}\left(-1 + \sum_{j\in\Omega(\boldsymbol{x})}x_j\right),\quad &amp;i\in \Omega(\boldsymbol{x})\\\<br />
&amp;0,\quad &amp;i \not\in \Omega(\boldsymbol{x})<br />
\end{aligned}\right.\end{equation}<br />
ä»è¿™ä¸ªç­‰ä»·å½¢å¼å¯ä»¥çœ‹å‡ºï¼Œè·ŸSparse Softmaxä¸€æ ·ï¼ŒSparsemaxåŒæ ·ä¹Ÿåªå¯¹éƒ¨åˆ†ç±»åˆ«æœ‰æ¢¯åº¦ï¼Œå¯ä»¥ç›´æ¥ç®—å‡ºé›…å¯æ¯”çŸ©é˜µï¼š<br />
\begin{equation}\frac{\partial p_i}{\partial x_j} = \left\{\begin{aligned}<br />
&amp;1 - \frac{1}{|\Omega(\boldsymbol{x})|},\quad &amp;i,j\in \Omega(\boldsymbol{x}),i=j\\\[5pt]<br />
&amp;- \frac{1}{|\Omega(\boldsymbol{x})|},\quad &amp;i,j\in \Omega(\boldsymbol{x}),i\neq j\\\[5pt]<br />
&amp;0,\quad &amp;i \not\in \Omega(\boldsymbol{x})\text{ or }j \not\in \Omega(\boldsymbol{x})<br />
\end{aligned}\right.\end{equation}<br />
ç”±æ­¤å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºåœ¨$\Omega(\boldsymbol{x})$é‡Œè¾¹çš„ç±»åˆ«ï¼ŒSparsemaxå€’æ˜¯ä¸ä¼šæ¢¯åº¦æ¶ˆå¤±ï¼Œå› ä¸ºæ­¤æ—¶å®ƒçš„æ¢¯åº¦æ’ä¸ºå¸¸æ•°ï¼Œä½†å®ƒæ€»çš„æ¢¯åº¦å¤§å°ï¼Œå–å†³äº$\Omega(\boldsymbol{x})$çš„å…ƒç´ ä¸ªæ•°ï¼Œå®ƒè¶Šå°‘åˆ™è¶Šç¨€ç–ï¼Œæ„å‘³ç€æ¢¯åº¦ä¹Ÿè¶Šç¨€ç–ã€‚</p>
<h3 id="_11">æŸå¤±å‡½æ•°</h3>
<p>æœ€åæˆ‘ä»¬æ¥è®¨è®ºSparsemaxä½œä¸ºåˆ†ç±»è¾“å‡ºæ—¶çš„æŸå¤±å‡½æ•°ã€‚æ¯”è¾ƒç›´è§‚çš„æƒ³æ³•å°±æ˜¯è·ŸSoftmaxä¸€æ ·ç”¨äº¤å‰ç†µ$-\log p_t$ï¼Œä½†Sparsemaxçš„è¾“å‡ºå¯èƒ½æ˜¯ä¸¥æ ¼ç­‰äº0çš„ï¼Œæ‰€ä»¥ä¸ºäº†é˜²æ­¢$\log 0$é”™è¯¯ï¼Œè¿˜è¦ç»™æ¯ä¸ªåˆ†é‡éƒ½åŠ ä¸Š$\epsilon$ï¼Œæœ€ç»ˆçš„äº¤å‰ç†µå½¢å¼ä¸º$-\log\frac{p_t + \epsilon}{1 + n\epsilon}$ï¼Œä½†è¿™ä¸€æ¥æœ‰ç‚¹ä¸‘ï¼ŒäºŒæ¥å®ƒè¿˜ä¸æ˜¯å‡¸å‡½æ•°ï¼Œæ‰€ä»¥å¹¶ä¸æ˜¯ä¸€ä¸ªç†æƒ³é€‰æ‹©ã€‚</p>
<p>äº‹å®ä¸Šï¼Œäº¤å‰ç†µåœ¨Softmaxä¸­ä¹‹æ‰€ä»¥å¥½ç”¨ï¼Œæ˜¯å› ä¸ºå®ƒçš„æ¢¯åº¦æ°å¥½æœ‰$\eqref{eq:softmax-ce-grad}$çš„å½¢å¼ï¼Œæ‰€ä»¥å¯¹äºSparsemaxï¼Œæˆ‘ä»¬ä¸å¦¨åŒæ ·å‡è®¾æŸå¤±å‡½æ•°çš„æ¢¯åº¦ä¸º$\boldsymbol{p} - \text{onehot(t)}$ï¼Œç„¶ååæ¨å‡ºæŸå¤±å‡½æ•°è¯¥æœ‰çš„æ ·å­ï¼Œå³ï¼š<br />
\begin{equation}\frac{\partial \mathcal{L}<em i_in_Omega_boldsymbol_x="i\in\Omega(\boldsymbol{x">t}{\partial \boldsymbol{x}} = \boldsymbol{p} - \text{onehot(t)}\quad\Rightarrow\quad \mathcal{L}_t = \frac{1}{2} - x_t + \sum</em>})}\frac{1}{2}\left(x_i^2 - \lambda^2(\boldsymbol{x})\right)\end{equation<br />
ä»å³å¾€å·¦éªŒè¯æ¯”è¾ƒç®€å•ï¼Œä»å·¦å¾€å³æ¨å¯èƒ½ä¼šæœ‰äº›å›°éš¾ï¼Œä½†ä¸å¤šï¼Œåå¤æ‹¼å‡‘ä¸€ä¸‹åº”è¯¥å°±èƒ½å‡ºæ¥äº†ã€‚ç¬¬ä¸€ä¸ª$\frac{1}{2}$å¸¸æ•°æ˜¯ä¸ºäº†ä¿è¯æŸå¤±å‡½æ•°çš„éè´Ÿæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å–ä¸€ä¸ªæç«¯æ¥éªŒè¯ä¸€ä¸‹ï¼šå‡è®¾ä¼˜åŒ–åˆ°å®Œç¾ï¼Œé‚£ä¹ˆ$\boldsymbol{p}$åº”è¯¥ä¹Ÿæ˜¯one hotï¼Œæ­¤æ—¶$x_t\to\infty$ï¼Œå¹¶ä¸”$\lambda(\boldsymbol{x}) = x_t - 1$ï¼Œäºæ˜¯<br />
\begin{equation}- x_t + \sum_{i\in\Omega(\boldsymbol{x})}\frac{1}{2}\left(x_i^2 - \lambda^2(\boldsymbol{x})\right) = -x_t + \frac{1}{2}x_t^2 - \frac{1}{2}(x_t - 1)^2 = -\frac{1}{2}\end{equation}<br />
æ‰€ä»¥è¦å¤šåŠ ä¸Šå¸¸æ•°$\frac{1}{2}$ã€‚</p>
<h2 id="entmax-">Entmax-Î±</h2>
<p>Entmax-$\alpha$æ˜¯Sparsemaxçš„ä¸€èˆ¬åŒ–ï¼Œå®ƒçš„åŠ¨æœºæ˜¯Sparsemaxå¾€å¾€ä¼šè¿‡åº¦ç¨€ç–ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´å­¦ä¹ æ•ˆç‡åä½ï¼Œå¯¼è‡´æœ€ç»ˆæ•ˆæœä¸‹é™çš„é—®é¢˜ï¼Œæ‰€ä»¥Entmax-$\alpha$å¼•å…¥äº†$\alpha$å‚æ•°ï¼Œæä¾›äº†Softmaxï¼ˆ$\alpha=1$ï¼‰åˆ°Sparsemaxï¼ˆ$\alpha=2$ï¼‰çš„å¹³æ»‘è¿‡åº¦ã€‚Entmax-$\alpha$å‡ºè‡ªè®ºæ–‡<a href="https://papers.cool/arxiv/1905.05702">ã€ŠSparse Sequence-to-Sequence Modelsã€‹</a>ï¼Œä½œè€…è·ŸSparsemaxä¸€æ ·æ˜¯Andre F. T. Martinsï¼Œè¿™ä½å¤§ä½¬å›´ç»•ç€ç¨€ç–Softmaxã€ç¨€ç–Attentionåšäº†ä¸å°‘å·¥ä½œï¼Œæœ‰å…´è¶£çš„è¯»è€…å¯ä»¥åœ¨<a href="https://andre-martins.github.io/">ä»–çš„ä¸»é¡µ</a>æŸ¥é˜…ç›¸å…³å·¥ä½œã€‚</p>
<h3 id="_12">åŸºæœ¬å®šä¹‰</h3>
<p>è·ŸSparsemaxä¸€æ ·ï¼ŒåŸè®ºæ–‡å°†Entmax-$\alpha$å®šä¹‰ä¸ºç±»ä¼¼$\eqref{eq:sparsemax-opt}$çš„ä¼˜åŒ–é—®é¢˜çš„è§£ï¼Œä½†è¿™ä¸ªå®šä¹‰æ¶‰åŠåˆ°<a href="https://en.wikipedia.org/wiki/Tsallis_entropy">Tsallis entropy</a>çš„æ¦‚å¿µï¼ˆä¹Ÿæ˜¯Entmaxçš„Entçš„æ¥æºï¼‰ï¼Œæ±‚è§£è¿˜éœ€è¦ç”¨åˆ°æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼Œç›¸å¯¹æ¥è¯´æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œä¸é‡‡ç”¨è¿™ç§å¼•å…¥æ–¹å¼ã€‚</p>
<p>æˆ‘ä»¬çš„ä»‹ç»åŒæ ·æ˜¯åŸºäºä¸Šä¸€èŠ‚çš„è¿‘ä¼¼$\exp(x)\approx \text{relu}(1 + x)$ï¼Œå¯¹äºSoftmaxå’ŒSparsemaxï¼Œæˆ‘ä»¬æœ‰<br />
\begin{align}&amp;{\color{red}{Softmax}}:\quad &amp;\exp(\boldsymbol{x} - \lambda(\boldsymbol{x})) \\\[5pt]<br />
&amp;{\color{red}{Sparsemax}}:\quad &amp;\text{relu}(1+\boldsymbol{x} - \lambda(\boldsymbol{x}))<br />
\end{align}<br />
Sparsemaxå¤ªç¨€ç–ï¼ŒèƒŒåçš„åŸå› ä¹Ÿå¯ä»¥ç†è§£ä¸º$\exp(x)\approx \text{relu}(1 + x)$è¿‘ä¼¼ç²¾åº¦ä¸å¤Ÿé«˜ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸­æ¼”åŒ–å‡ºæ›´é«˜ç²¾åº¦çš„è¿‘ä¼¼<br />
\begin{equation}\exp(x) = \exp(\beta x / \beta) = \exp^{1/\beta}(\beta x)\approx \text{relu}^{1/\beta}(1 + \beta x)\end{equation}<br />
åªè¦$0 \leq \beta &lt; 1$ï¼Œé‚£ä¹ˆæœ€å³ç«¯å°±æ˜¯ä¸€ä¸ªæ¯”$\text{relu}(1 + x)$æ›´å¥½çš„è¿‘ä¼¼ï¼ˆæƒ³æƒ³ä¸ºä»€ä¹ˆï¼‰ã€‚åˆ©ç”¨è¿™ä¸ªæ–°è¿‘ä¼¼ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ„å»º<br />
\begin{equation}{\color{red}{Entmax\text{-}\alpha}}:\quad \text{relu}^{1/\beta}(1+\beta\boldsymbol{x} - \lambda(\boldsymbol{x}))\end{equation}<br />
è¿™é‡Œ$\alpha = \beta + 1$æ˜¯ä¸ºäº†å¯¹é½åŸè®ºæ–‡çš„è¡¨è¾¾æ–¹å¼ï¼Œäº‹å®ä¸Šç”¨$\beta$è¡¨ç¤ºæ›´ç®€æ´ä¸€äº›ã€‚åŒæ ·åœ°ï¼Œå¸¸æ•°$1$ä¹Ÿå¯ä»¥æ”¶å…¥åˆ°$\lambda(\boldsymbol{x})$å®šä¹‰ä¹‹ä¸­ï¼Œæ‰€ä»¥æœ€ç»ˆå®šä¹‰å¯ä»¥ç®€åŒ–ä¸º<br />
\begin{equation}Entmax_{\alpha}(\boldsymbol{x}) = \text{relu}^{1/\beta}(\beta\boldsymbol{x} - \lambda(\boldsymbol{x}))\end{equation}</p>
<h3 id="_13">æ±‚è§£ç®—æ³•</h3>
<p>å¯¹äºä¸€èˆ¬çš„$\beta$ï¼Œæ±‚è§£$\lambda(\boldsymbol{x})$æ˜¯æ¯”è¾ƒéº»çƒ¦çš„äº‹æƒ…ï¼Œé€šå¸¸åªèƒ½ç”¨äºŒåˆ†æ³•æ±‚è§£ã€‚</p>
<p>é¦–å…ˆæˆ‘ä»¬è®°$\boldsymbol{z}=\beta\boldsymbol{x}$ï¼Œå¹¶ä¸”ä¸å¤±ä¸€èˆ¬æ€§å‡è®¾$z_1\geq z_2\geq \cdots \geq z_n$ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å‘ç°Entmax-$\alpha$æ˜¯æ»¡è¶³<strong>å•è°ƒæ€§</strong> å’Œ<strong>ä¸å˜æ€§</strong> çš„ï¼Œå€ŸåŠ©<strong>ä¸å˜æ€§</strong> æˆ‘ä»¬å¯ä»¥ä¸å¤±ä¸€èˆ¬æ€§åœ°è®¾$z_1 = 1$ï¼ˆå¦‚æœä¸æ˜¯ï¼Œæ¯ä¸ª$z_i$éƒ½å‡å»$z_1 - 1$å³å¯ï¼‰ã€‚ç°åœ¨å¯ä»¥æ£€éªŒï¼Œå½“$\lambda=0$æ—¶ï¼Œ$\text{relu}^{1/\beta}(\beta\boldsymbol{x} - \lambda)$çš„æ‰€æœ‰åˆ†é‡ä¹‹å’Œå¤§äºç­‰äº1ï¼Œå½“$\lambda=1$æ—¶ï¼Œ$\text{relu}^{1/\beta}(\beta\boldsymbol{x} - \lambda)$çš„æ‰€æœ‰åˆ†é‡ä¹‹å’Œç­‰äº0ï¼Œæ‰€ä»¥æœ€ç»ˆèƒ½ä½¿åˆ†é‡ä¹‹å’Œç­‰äº1çš„$\lambda(\boldsymbol{x})$å¿…ç„¶åœ¨$[0,1)$å†…ï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨äºŒåˆ†æ³•æ¥é€æ­¥é€¼è¿‘æœ€ä¼˜çš„$\lambda(\boldsymbol{x})$ã€‚</p>
<p>å¯¹äºæŸäº›ç‰¹æ®Šçš„$\beta$ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ±‚ç²¾ç¡®è§£çš„ç®—æ³•ã€‚Sparsemaxå¯¹åº”$\beta=1$ï¼Œæˆ‘ä»¬å‰é¢å·²ç»ç»™å‡ºäº†æ±‚è§£è¿‡ç¨‹ï¼Œå¦å¤–ä¸€ä¸ªèƒ½ç»™è§£æè§£çš„ä¾‹å­æ˜¯$\beta=1/2$ï¼Œè¿™ä¹Ÿæ˜¯åŸè®ºæ–‡ä¸»è¦å…³å¿ƒçš„ä¾‹å­ï¼Œå¦‚æœä¸åŠ æ ‡æ³¨ï¼Œé‚£ä¹ˆEntmaxé»˜è®¤å°±æ˜¯Entmax-1.5ã€‚è·ŸSparsemaxä¸€æ ·çš„æ€è·¯ï¼Œæˆ‘ä»¬å…ˆå‡è®¾å·²çŸ¥$z_k\geq \lambda(\boldsymbol{x})\geq z_{k+1}$ï¼Œäºæ˜¯æœ‰<br />
\begin{equation}\sum_{i=1}^k [z_i - \lambda(\boldsymbol{x})]^2 = 1\end{equation}<br />
è¿™åªä¸è¿‡æ˜¯å…³äº$\lambda(\boldsymbol{x})$çš„ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹ï¼Œå¯ä»¥è§£å¾—<br />
\begin{equation}\lambda(\boldsymbol{x}) = \mu_k - \sqrt{\frac{1}{k} - \sigma_k^2},\quad \mu_k = \frac{1}{k}\sum_{i=1}^k z_i,\quad\sigma_k^2 = \frac{1}{k}\left(\sum_{i=1}^k z_i^2\right) - \mu_k^2\end{equation}<br />
å½“æˆ‘ä»¬æ— æ³•äº‹å…ˆçŸ¥é“$x_k\geq \lambda(\boldsymbol{x})\geq x_{k+1}$æ—¶ï¼Œå¯ä»¥éå†$k=1,2,\cdots,n$ï¼Œåˆ©ç”¨ä¸Šå¼æ±‚ä¸€é$\lambda_k(\boldsymbol{x})$ï¼Œå–æ»¡è¶³$x_k\geq \lambda_k(\boldsymbol{x})\geq x_{k+1}$é‚£ä¸€ä¸ª$\lambda_k(\boldsymbol{x})$ï¼Œä½†æ³¨æ„è¿™æ—¶å€™ä¸ç­‰ä»·äºæ±‚æ»¡è¶³$x_k\geq \lambda_k(\boldsymbol{x})$çš„æœ€å¤§çš„$k$ã€‚</p>
<p>å®Œæ•´çš„å‚è€ƒå®ç°ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="s s-Atom">def</span> <span class="nf">entmat</span><span class="p">(</span><span class="s s-Atom">x</span><span class="p">)</span><span class="o">:</span>
    <span class="s s-Atom">x_sort</span> <span class="o">=</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="s s-Atom">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)[</span><span class="o">::-</span><span class="mi">1</span><span class="p">]</span>
    <span class="s s-Atom">k</span> <span class="o">=</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="s s-Atom">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="s s-Atom">x_mu</span> <span class="o">=</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="s s-Atom">x_sort</span><span class="p">)</span> <span class="o">/</span> <span class="s s-Atom">k</span>
    <span class="s s-Atom">x_sigma2</span> <span class="o">=</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="s s-Atom">x_sort**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="s s-Atom">k</span>  <span class="o">-</span> <span class="s s-Atom">x_mu**</span><span class="mi">2</span>
    <span class="s s-Atom">x_lamb</span> <span class="o">=</span> <span class="s s-Atom">x_mu</span> <span class="o">-</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="s s-Atom">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="s s-Atom">k</span> <span class="o">-</span> <span class="s s-Atom">x_sigma2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="s s-Atom">x_sort_shift</span> <span class="o">=</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="s s-Atom">x_sort</span><span class="p">[</span><span class="mi">1</span><span class="o">:</span><span class="p">],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s s-Atom">constant_values=-np</span><span class="p">.</span><span class="s s-Atom">inf</span><span class="p">)</span>
    <span class="s s-Atom">lamb</span> <span class="o">=</span> <span class="s s-Atom">x_lamb</span><span class="p">[(</span><span class="s s-Atom">x_sort</span> <span class="o">&gt;</span> <span class="s s-Atom">x_lamb</span><span class="p">)</span> <span class="s s-Atom">&amp;</span> <span class="p">(</span><span class="s s-Atom">x_lamb</span> <span class="o">&gt;</span> <span class="s s-Atom">x_sort_shift</span><span class="p">)]</span>
    <span class="s s-Atom">return</span> <span class="s s-Atom">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="s s-Atom">x</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">-</span> <span class="s s-Atom">lamb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="s s-Atom">**</span><span class="mi">2</span>
</code></pre></div>

<h3 id="_14">å…¶ä»–å†…å®¹</h3>
<p>Entmax-$\alpha$çš„æ¢¯åº¦è·ŸSparsemaxå¤§åŒå°å¼‚ï¼Œè¿™é‡Œå°±ä¸å±•å¼€è®¨è®ºäº†ï¼Œè¯»è€…è‡ªè¡Œæ¨å¯¼ä¸€ä¸‹æˆ–è€…å‚è€ƒåŸè®ºæ–‡å°±è¡Œã€‚è‡³äºæŸå¤±å‡½æ•°ï¼ŒåŒæ ·ä»æ¢¯åº¦$\frac{\partial \mathcal{L}_t}{\partial \boldsymbol{x}} = \boldsymbol{p} - \text{onehot(t)}$å‡ºå‘åæ¨å‡ºæŸå¤±å‡½æ•°ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œä½†å…¶å½¢å¼æœ‰ç‚¹å¤æ‚ï¼Œæœ‰å…´è¶£äº†è§£çš„è¯»è€…å¯ä»¥å‚è€ƒåŸè®ºæ–‡<a href="https://papers.cool/arxiv/1905.05702">ã€ŠSparse Sequence-to-Sequence Modelsã€‹</a>ä»¥åŠ<a href="https://papers.cool/arxiv/1901.02324">ã€ŠLearning with Fenchel-Young Lossesã€‹</a>ã€‚</p>
<p>ä¸è¿‡å°±ç¬”è€…çœ‹æ¥ï¼Œç›´æ¥ç”¨$\text{stop_gradient}$ç®—å­æ¥å®šä¹‰æŸå¤±å‡½æ•°æ›´ä¸ºç®€å•é€šç”¨ï¼Œå¯ä»¥é¿å…æ±‚åŸå‡½æ•°çš„å¤æ‚è¿‡ç¨‹ï¼š<br />
\begin{equation}\mathcal{L}_t = (\boldsymbol{p} - \text{onehot(t)})\cdot \text{stop_gradient}(\boldsymbol{x})\end{equation}<br />
è¿™é‡Œçš„$\,\cdot\,$æ˜¯å‘é‡å†…ç§¯ï¼Œè¿™æ ·å®šä¹‰å‡ºæ¥çš„æŸå¤±ï¼Œå…¶æ¢¯åº¦æ­£å¥½æ˜¯$\boldsymbol{p} - \text{onehot(t)}$ï¼Œä½†è¦æ³¨æ„è¿™ä¸ªæŸå¤±å‡½æ•°åªæœ‰æ¢¯åº¦æ˜¯æœ‰æ•ˆçš„ï¼Œå®ƒæœ¬èº«çš„æ•°å€¼æ˜¯æ²¡æœ‰å‚è€ƒæ„ä¹‰çš„ï¼Œæ¯”å¦‚å®ƒå¯æ­£å¯è´Ÿï¼Œä¹Ÿä¸ä¸€å®šè¶Šå°è¶Šå¥½ï¼Œæ‰€ä»¥è¦è¯„ä¼°è®­ç»ƒè¿›åº¦å’Œæ•ˆæœçš„è¯ï¼Œå¾—å¦å¤–å»ºç«‹æŒ‡æ ‡ï¼ˆæ¯”å¦‚äº¤å‰ç†µæˆ–è€…å‡†ç¡®ç‡ï¼‰ã€‚</p>
<h2 id="_15">æ–‡ç« å°ç»“</h2>
<p>æœ¬æ–‡ç®€å•å›é¡¾å’Œæ•´ç†äº†SoftmaxåŠå…¶éƒ¨åˆ†æ›¿ä»£å“ï¼Œå…¶ä¸­åŒ…å«çš„å·¥ä½œæœ‰Softmaxã€Margin Softmaxã€Taylor Softmaxã€Sparse Softmaxã€Perturb Maxã€Sparsemaxã€Entmax-$\alpha$çš„å®šä¹‰ã€æ€§è´¨ç­‰å†…å®¹ã€‚</p>
<p><em><strong>è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š</strong><a href="https://spaces.ac.cn/archives/10145">https://spaces.ac.cn/archives/10145</a></em></p>
<p><em><strong>æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="ã€Šç§‘å­¦ç©ºé—´FAQã€‹">ã€Šç§‘å­¦ç©ºé—´FAQã€‹</a></p>
<p><strong>å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚</strong></p>
<p><strong>å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼</strong></p>
<p>æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>å¾®ä¿¡æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>æ”¯ä»˜å®æ‰“èµ</p>
<p>å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>ç‚¹å‡»è¿™é‡Œ</strong></a>æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚</p>
<p><strong>å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š</strong></p>
<p>è‹å‰‘æ—. (Jun. 14, 2024). ã€Šé€šå‘æ¦‚ç‡åˆ†å¸ƒä¹‹è·¯ï¼šç›˜ç‚¹SoftmaxåŠå…¶æ›¿ä»£å“ ã€‹[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10145">https://spaces.ac.cn/archives/10145</a></p>
<p>@online{kexuefm-10145,<br />
title={é€šå‘æ¦‚ç‡åˆ†å¸ƒä¹‹è·¯ï¼šç›˜ç‚¹SoftmaxåŠå…¶æ›¿ä»£å“},<br />
author={è‹å‰‘æ—},<br />
year={2024},<br />
month={Jun},<br />
url={\url{https://spaces.ac.cn/archives/10145}},<br />
} </p>
<hr />
<h2 id="_16">å…¬å¼æ¨å¯¼ä¸æ³¨é‡Š</h2>
<h3 id="1-softmax">1. Softmax è¯¦ç»†æ¨å¯¼</h3>
<h4 id="11">1.1 å®šä¹‰çš„ä¸¥æ ¼æ¨å¯¼</h4>
<p>ä»æœ€åŸºæœ¬çš„è¦æ±‚å‡ºå‘ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å‘é‡ $\boldsymbol{x} = (x_1, x_2, \ldots, x_n) \in \mathbb{R}^n$ æ˜ å°„åˆ°æ¦‚ç‡å•çº¯å½¢ $\Delta^{n-1}$ã€‚ä¸€ä¸ªè‡ªç„¶çš„æƒ³æ³•æ˜¯ä½¿ç”¨å½’ä¸€åŒ–ï¼š</p>
<p><strong>æ­¥éª¤1ï¼š</strong> æ„é€ éè´Ÿå‡½æ•°
\begin{equation}
f: \mathbb{R} \to \mathbb{R}^+ = [0, +\infty)
\end{equation}</p>
<p><strong>æ­¥éª¤2ï¼š</strong> å¯¹æ¯ä¸ªåˆ†é‡åº”ç”¨è¯¥å‡½æ•°
\begin{equation}
y_i = f(x_i), \quad i = 1, 2, \ldots, n
\end{equation}</p>
<p><strong>æ­¥éª¤3ï¼š</strong> å½’ä¸€åŒ–å¾—åˆ°æ¦‚ç‡
\begin{equation}
p_i = \frac{y_i}{\sum_{j=1}^n y_j} = \frac{f(x_i)}{\sum_{j=1}^n f(x_j)}
\end{equation}</p>
<p><strong>éªŒè¯æ¦‚ç‡æ€§è´¨ï¼š</strong>
- éè´Ÿæ€§ï¼šç”±äº $f(x_i) \geq 0$ ä¸”åˆ†æ¯ $\sum_{j=1}^n f(x_j) &gt; 0$ï¼ˆè‡³å°‘æœ‰ä¸€é¡¹ä¸ºæ­£ï¼‰ï¼Œæ•… $p_i \geq 0$
- å½’ä¸€æ€§ï¼š$\sum_{i=1}^n p_i = \sum_{i=1}^n \frac{f(x_i)}{\sum_{j=1}^n f(x_j)} = \frac{\sum_{i=1}^n f(x_i)}{\sum_{j=1}^n f(x_j)} = 1$</p>
<p><strong>é€‰æ‹©æŒ‡æ•°å‡½æ•°ï¼š</strong> ä¸ºä»€ä¹ˆé€‰æ‹© $f(x) = e^x$ï¼Ÿ
1. <strong>ä¸¥æ ¼å•è°ƒæ€§</strong>ï¼š$e^x$ ä¸¥æ ¼å•è°ƒé€’å¢ï¼Œä¿è¯ $x_i &gt; x_j \Leftrightarrow e^{x_i} &gt; e^{x_j}$
2. <strong>å¤„å¤„å¯å¾®</strong>ï¼š$\frac{d}{dx}e^x = e^x$ï¼Œæ¢¯åº¦è®¡ç®—ç®€å•
3. <strong>å¹³ç§»ä¸å˜æ€§</strong>ï¼š$\frac{e^{x_i + c}}{\sum_j e^{x_j + c}} = \frac{e^c \cdot e^{x_i}}{e^c \sum_j e^{x_j}} = \frac{e^{x_i}}{\sum_j e^{x_j}}$</p>
<h4 id="12">1.2 å•è°ƒæ€§çš„ä¸¥æ ¼è¯æ˜</h4>
<p><strong>å®šç†ï¼š</strong> å¯¹äº Softmax å‡½æ•°ï¼Œ$p_i &gt; p_j \Leftrightarrow x_i &gt; x_j$</p>
<p><strong>è¯æ˜ï¼ˆå……åˆ†æ€§ï¼‰ï¼š</strong> å‡è®¾ $x_i &gt; x_j$ï¼Œåˆ™
\begin{align}
p_i - p_j &amp;= \frac{e^{x_i}}{\sum_{k=1}^n e^{x_k}} - \frac{e^{x_j}}{\sum_{k=1}^n e^{x_k}} \
&amp;= \frac{e^{x_i} - e^{x_j}}{\sum_{k=1}^n e^{x_k}}
\end{align}</p>
<p>ç”±äº $e^x$ ä¸¥æ ¼å•è°ƒé€’å¢ï¼Œ$x_i &gt; x_j \Rightarrow e^{x_i} &gt; e^{x_j}$ï¼Œå› æ­¤åˆ†å­ $e^{x_i} - e^{x_j} &gt; 0$ï¼Œåˆ†æ¯æ’æ­£ï¼Œæ•… $p_i - p_j &gt; 0$ï¼Œå³ $p_i &gt; p_j$ã€‚</p>
<p><strong>è¯æ˜ï¼ˆå¿…è¦æ€§ï¼‰ï¼š</strong> å‡è®¾ $p_i &gt; p_j$ï¼Œåˆ™
\begin{equation}
\frac{e^{x_i}}{\sum_{k=1}^n e^{x_k}} &gt; \frac{e^{x_j}}{\sum_{k=1}^n e^{x_k}} \Rightarrow e^{x_i} &gt; e^{x_j}
\end{equation}</p>
<p>ç”±äº $\log$ æ˜¯ä¸¥æ ¼å•è°ƒé€’å¢å‡½æ•°ï¼Œä¸¤è¾¹å–å¯¹æ•°å¾— $x_i &gt; x_j$ã€‚$\square$</p>
<h4 id="13">1.3 æ¢¯åº¦è®¡ç®—çš„è¯¦ç»†æ­¥éª¤</h4>
<p>è®° $Z = \sum_{j=1}^n e^{x_j}$ï¼Œåˆ™ $p_i = \frac{e^{x_i}}{Z}$</p>
<p><strong>æƒ…å†µ1ï¼š</strong> $i = j$ æ—¶ï¼Œä½¿ç”¨å•†æ³•åˆ™
\begin{align}
\frac{\partial p_i}{\partial x_i} &amp;= \frac{\partial}{\partial x_i}\left(\frac{e^{x_i}}{Z}\right) \
&amp;= \frac{e^{x_i} \cdot Z - e^{x_i} \cdot \frac{\partial Z}{\partial x_i}}{Z^2} \
&amp;= \frac{e^{x_i} \cdot Z - e^{x_i} \cdot e^{x_i}}{Z^2} \quad \text{ï¼ˆå› ä¸º } \frac{\partial Z}{\partial x_i} = e^{x_i}\text{ï¼‰} \
&amp;= \frac{e^{x_i}}{Z} \cdot \frac{Z - e^{x_i}}{Z} \
&amp;= p_i \cdot (1 - p_i) \
&amp;= p_i - p_i^2
\end{align}</p>
<p><strong>æƒ…å†µ2ï¼š</strong> $i \neq j$ æ—¶
\begin{align}
\frac{\partial p_i}{\partial x_j} &amp;= \frac{\partial}{\partial x_j}\left(\frac{e^{x_i}}{Z}\right) \
&amp;= \frac{0 \cdot Z - e^{x_i} \cdot \frac{\partial Z}{\partial x_j}}{Z^2} \
&amp;= \frac{-e^{x_i} \cdot e^{x_j}}{Z^2} \quad \text{ï¼ˆå› ä¸º } \frac{\partial Z}{\partial x_j} = e^{x_j}\text{ï¼‰} \
&amp;= -\frac{e^{x_i}}{Z} \cdot \frac{e^{x_j}}{Z} \
&amp;= -p_i p_j
\end{align}</p>
<p><strong>é›…å¯æ¯”çŸ©é˜µå½¢å¼ï¼š</strong>
\begin{equation}
J = \frac{\partial \boldsymbol{p}}{\partial \boldsymbol{x}} = \text{diag}(\boldsymbol{p}) - \boldsymbol{p}\boldsymbol{p}^T
\end{equation}</p>
<p>å…¶ä¸­ $\text{diag}(\boldsymbol{p})$ æ˜¯ä»¥ $\boldsymbol{p}$ ä¸ºå¯¹è§’å…ƒç´ çš„å¯¹è§’çŸ©é˜µã€‚</p>
<p><strong>æ•°å€¼ç¤ºä¾‹ï¼š</strong> è€ƒè™‘ $\boldsymbol{x} = [1, 2, 3]^T$
\begin{align}
e^{\boldsymbol{x}} &amp;= [e^1, e^2, e^3]^T \approx [2.718, 7.389, 20.086]^T \
Z &amp;= 2.718 + 7.389 + 20.086 = 30.193 \
\boldsymbol{p} &amp;\approx [0.090, 0.245, 0.665]^T
\end{align}</p>
<p>é›…å¯æ¯”çŸ©é˜µï¼š
\begin{equation}
J \approx \begin{bmatrix}
0.082 &amp; -0.022 &amp; -0.060 \
-0.022 &amp; 0.185 &amp; -0.163 \
-0.060 &amp; -0.163 &amp; 0.223
\end{bmatrix}
\end{equation}</p>
<p>éªŒè¯ï¼šæ¯è¡Œä¹‹å’Œä¸º 0ï¼ˆè¿™æ˜¯ç”± $\sum_i p_i = 1$ çš„çº¦æŸå¯¼å‡ºçš„æ€§è´¨ï¼‰ã€‚</p>
<h4 id="14-hessian">1.4 Hessian çŸ©é˜µï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰</h4>
<p>å¯¹äºæ·±åº¦ç†è§£ï¼Œæˆ‘ä»¬è®¡ç®—äºŒé˜¶å¯¼æ•°ã€‚è®° $H_{ij,kl} = \frac{\partial^2 p_i}{\partial x_j \partial x_k}$</p>
<p><strong>æƒ…å†µ1ï¼š</strong> $i = j = k$
\begin{align}
\frac{\partial^2 p_i}{\partial x_i^2} &amp;= \frac{\partial}{\partial x_i}(p_i - p_i^2) \
&amp;= (p_i - p_i^2) - 2p_i(p_i - p_i^2) \
&amp;= p_i(1 - p_i)(1 - 2p_i)
\end{align}</p>
<p><strong>æƒ…å†µ2ï¼š</strong> $i = j \neq k$
\begin{align}
\frac{\partial^2 p_i}{\partial x_i \partial x_k} &amp;= \frac{\partial}{\partial x_k}(p_i - p_i^2) \
&amp;= -p_i p_k - 2p_i(-p_i p_k) \
&amp;= p_i p_k(2p_i - 1)
\end{align}</p>
<h3 id="2-margin-softmax">2. Margin Softmax è¯¦ç»†æ¨å¯¼</h3>
<h4 id="21-am-softmax">2.1 AM-Softmax çš„åŠ¨æœº</h4>
<p>ä¼ ç»Ÿ Softmax æŸå¤±ï¼š
\begin{equation}
\mathcal{L} = -\log \frac{e^{x_t}}{\sum_{j=1}^n e^{x_j}}
\end{equation}</p>
<p>è¿™è¦æ±‚ $x_t$ å¤§äºå…¶ä»–åˆ†é‡ï¼Œä½†æ²¡æœ‰å¼ºåˆ¶è¦æ±‚å¤§å¤šå°‘ã€‚åœ¨äººè„¸è¯†åˆ«æˆ–è¯­ä¹‰æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›åŒç±»æ ·æœ¬çš„ç‰¹å¾éå¸¸æ¥è¿‘ï¼Œä¸åŒç±»æ ·æœ¬çš„ç‰¹å¾æœ‰æ˜æ˜¾é—´éš”ã€‚</p>
<h4 id="22">2.2 ä½™å¼¦ç›¸ä¼¼åº¦å½¢å¼çš„æ¨å¯¼</h4>
<p><strong>æ­¥éª¤1ï¼š</strong> ç‰¹å¾å‘é‡å’Œç±»ä¸­å¿ƒ
- è¾“å…¥ç‰¹å¾ï¼š$\boldsymbol{z} \in \mathbb{R}^d$ï¼Œ$|\boldsymbol{z}| = 1$ï¼ˆL2å½’ä¸€åŒ–ï¼‰
- ç±»ä¸­å¿ƒï¼š$\boldsymbol{c}_i \in \mathbb{R}^d$ï¼Œ$|\boldsymbol{c}_i| = 1$ï¼ˆL2å½’ä¸€åŒ–ï¼‰</p>
<p><strong>æ­¥éª¤2ï¼š</strong> æ„é€ ä½™å¼¦ç›¸ä¼¼åº¦
\begin{equation}
\cos(\boldsymbol{z}, \boldsymbol{c}<em zi="zi">i) = \boldsymbol{z}^T \boldsymbol{c}_i = |\boldsymbol{z}| |\boldsymbol{c}_i| \cos\theta</em>
\end{equation}} = \cos\theta_{zi</p>
<p>å…¶ä¸­ $\theta_{zi}$ æ˜¯ $\boldsymbol{z}$ ä¸ $\boldsymbol{c}_i$ çš„å¤¹è§’ã€‚</p>
<p><strong>æ­¥éª¤3ï¼š</strong> å¼•å…¥æ¸©åº¦å‚æ•°å’ŒMargin
\begin{equation}
x_i = \frac{\cos\theta_{zi}}{\tau}, \quad x_t' = \frac{\cos\theta_{zt} - m}{\tau}
\end{equation}</p>
<h4 id="23-am-softmax">2.3 AM-Softmax æŸå¤±å‡½æ•°</h4>
<p>\begin{equation}
\mathcal{L}<em zt="zt">{AM} = -\log \frac{e^{(\cos\theta</em>
\end{equation}} - m)/\tau}}{e^{(\cos\theta_{zt} - m)/\tau} + \sum_{j \neq t} e^{\cos\theta_{zj}/\tau}</p>
<p><strong>å‡ ä½•è§£é‡Šï¼š</strong> è¯¥æŸå¤±è¦æ±‚ç›®æ ‡ç±»çš„ä½™å¼¦ç›¸ä¼¼åº¦ä¸ä»…è¦æœ€å¤§ï¼Œè¿˜è¦æ¯”æ¬¡å¤§è€…è‡³å°‘å¤§ $m$ï¼ˆåœ¨å½’ä¸€åŒ–ä¹‹å‰ï¼‰ã€‚</p>
<h4 id="24">2.4 æ¢¯åº¦åˆ†æ</h4>
<p>ä»¤ $s_t' = \cos\theta_{zt} - m$ï¼Œ$s_j = \cos\theta_{zj}$ ($j \neq t$)</p>
<p>\begin{align}
\frac{\partial \mathcal{L}<em j="1">{AM}}{\partial \boldsymbol{z}} &amp;= -\frac{1}{\tau}\left[\frac{\partial s_t'}{\partial \boldsymbol{z}} - \sum</em>\right]
\end{align}}^n p_j \frac{\partial s_j}{\partial \boldsymbol{z}</p>
<p>å…¶ä¸­
\begin{equation}
\frac{\partial \cos\theta_{zi}}{\partial \boldsymbol{z}} = \frac{\partial (\boldsymbol{z}^T\boldsymbol{c}_i)}{\partial \boldsymbol{z}} = \boldsymbol{c}_i - (\boldsymbol{z}^T\boldsymbol{c}_i)\boldsymbol{z}
\end{equation}</p>
<p>æœ€åä¸€æ­¥åˆ©ç”¨äº† $|\boldsymbol{z}| = 1$ çš„çº¦æŸã€‚</p>
<p><strong>æ•°å€¼ç¤ºä¾‹ï¼š</strong> å‡è®¾ $d=2$ï¼Œ$\tau=0.1$ï¼Œ$m=0.3$
- ç›®æ ‡ç±»ä¸­å¿ƒï¼š$\boldsymbol{c}_t = [1, 0]^T$
- å¹²æ‰°ç±»ä¸­å¿ƒï¼š$\boldsymbol{c}_1 = [0.6, 0.8]^T$
- ç‰¹å¾å‘é‡ï¼š$\boldsymbol{z} = [0.8, 0.6]^T$</p>
<p>è®¡ç®—ï¼š
\begin{align}
\cos\theta_{zt} &amp;= 0.8 \times 1 + 0.6 \times 0 = 0.8 \
\cos\theta_{z1} &amp;= 0.8 \times 0.6 + 0.6 \times 0.8 = 0.96
\end{align}</p>
<p>ä¼ ç»Ÿ Softmaxï¼š$x_t = 0.8/0.1 = 8$ï¼Œ$x_1 = 0.96/0.1 = 9.6$ï¼Œé¢„æµ‹é”™è¯¯ï¼</p>
<p>AM-Softmaxï¼š$x_t' = (0.8-0.3)/0.1 = 5$ï¼Œ$x_1 = 9.6$ï¼Œä»ç„¶é”™è¯¯ï¼Œä½†æ¢¯åº¦ä¼šæ›´å¼ºåœ°æ¨åŠ¨ $\boldsymbol{z}$ é è¿‘ $\boldsymbol{c}_t$ã€‚</p>
<h3 id="3-taylor-softmax">3. Taylor Softmax è¯¦ç»†æ¨å¯¼</h3>
<h4 id="31">3.1 æŒ‡æ•°å‡½æ•°çš„æ³°å‹’å±•å¼€</h4>
<p><strong>å®šç†ï¼š</strong> $e^x$ åœ¨ $x=0$ å¤„çš„æ³°å‹’å±•å¼€ä¸º
\begin{equation}
e^x = \sum_{m=0}^{\infty} \frac{x^m}{m!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
\end{equation}</p>
<p><strong>å…³é”®æ€§è´¨ï¼š</strong> å¯¹äºå¶æ•° $k$ï¼Œå®šä¹‰
\begin{equation}
f_k(x) = \sum_{m=0}^{k} \frac{x^m}{m!}
\end{equation}</p>
<p>åˆ™ $f_k(x) &gt; 0$ å¯¹æ‰€æœ‰ $x \in \mathbb{R}$ æˆç«‹ã€‚</p>
<h4 id="32">3.2 æ’æ­£æ€§çš„è¯æ˜</h4>
<p><strong>å¼•ç†ï¼š</strong> å¯¹äºå¶æ•° $k = 2K$ï¼Œ
\begin{equation}
f_{2K}(x) = \sum_{m=0}^{2K} \frac{x^m}{m!} &gt; 0, \quad \forall x \in \mathbb{R}
\end{equation}</p>
<p><strong>è¯æ˜ï¼š</strong> åˆ†æƒ…å†µè®¨è®º</p>
<p><strong>æƒ…å†µ1ï¼š</strong> $x \geq 0$ æ—¶ï¼Œæ‰€æœ‰é¡¹ $\frac{x^m}{m!} \geq 0$ï¼Œè‡³å°‘å¸¸æ•°é¡¹ä¸º 1ï¼Œæ•… $f_{2K}(x) \geq 1 &gt; 0$ã€‚</p>
<p><strong>æƒ…å†µ2ï¼š</strong> $x &lt; 0$ æ—¶ï¼Œé…å¯¹å¥‡å¶é¡¹ã€‚ä»¤ $y = -x &gt; 0$ï¼Œåˆ™
\begin{align}
f_{2K}(-y) &amp;= 1 - y + \frac{y^2}{2!} - \frac{y^3}{3!} + \cdots + \frac{y^{2K}}{(2K)!} \
&amp;= \sum_{j=0}^{K} \left[\frac{y^{2j}}{(2j)!} - \frac{y^{2j+1}}{(2j+1)!}\right] \
&amp;= \sum_{j=0}^{K} \frac{y^{2j}}{(2j)!}\left[1 - \frac{y}{2j+1}\right]
\end{align}</p>
<p>å¯¹äºæ¯ä¸€å¯¹ï¼Œå½“ $y$ è¶³å¤Ÿå°æ—¶ï¼Œ$1 - \frac{y}{2j+1} &gt; 0$ã€‚æ›´ä¸¥æ ¼çš„è¯æ˜éœ€è¦åˆ†æä½™é¡¹ï¼Œä½†ç›´è§‚ä¸Šï¼Œå¶æ¬¡é¡¹çš„æ­£è´¡çŒ®æ€»æ˜¯èƒ½æŠµæ¶ˆå¥‡æ¬¡é¡¹çš„è´Ÿè´¡çŒ®ã€‚$\square$</p>
<h4 id="33-taylor-softmax">3.3 Taylor-Softmax å®šä¹‰</h4>
<p>\begin{equation}
\text{taylor-softmax}(\boldsymbol{x}, k)<em j="1">i = \frac{f_k(x_i)}{\sum</em>
\end{equation}}^n f_k(x_j)</p>
<p><strong>ç‰¹æ®Šæƒ…å†µåˆ†æï¼š</strong></p>
<ul>
<li><strong>$k=0$</strong>ï¼š$f_0(x) = 1$ï¼Œå¾—åˆ°å‡åŒ€åˆ†å¸ƒ $p_i = \frac{1}{n}$</li>
<li><strong>$k=2$</strong>ï¼š$f_2(x) = 1 + x + \frac{x^2}{2}$</li>
<li><strong>$k \to \infty$</strong>ï¼š$f_k(x) \to e^x$ï¼Œæ”¶æ•›åˆ°æ ‡å‡† Softmax</li>
</ul>
<h4 id="34-softmax">3.4 ä¸ Softmax çš„è¿‘ä¼¼ç¨‹åº¦</h4>
<p>å®šä¹‰ç›¸å¯¹è¯¯å·®ï¼š
\begin{equation}
\epsilon_k(x) = \left|\frac{f_k(x) - e^x}{e^x}\right|
\end{equation}</p>
<p>ç”±æ³°å‹’ä½™é¡¹ç†è®ºï¼š
\begin{equation}
\epsilon_k(x) = \left|\frac{\sum_{m=k+1}^{\infty} \frac{x^m}{m!}}{e^x}\right| = \left|e^{-x} \sum_{m=k+1}^{\infty} \frac{x^m}{m!}\right|
\end{equation}</p>
<p>å¯¹äº $|x| \leq 1$ï¼Œ$k=2$ æ—¶ï¼Œ$\epsilon_2(x) &lt; 0.05$ï¼Œè¿‘ä¼¼è¾ƒå¥½ã€‚</p>
<h4 id="35">3.5 é•¿å°¾ç‰¹æ€§åˆ†æ</h4>
<p>æ¯”è¾ƒ Softmax ä¸ Taylor-Softmax ($k=2$) çš„å°¾éƒ¨æ¦‚ç‡ï¼š</p>
<p>ç»™å®š $\boldsymbol{x} = [5, 0, 0, 0]$</p>
<p><strong>Softmaxï¼š</strong>
\begin{align}
e^{\boldsymbol{x}} &amp;\approx [148.4, 1, 1, 1] \
\boldsymbol{p}_{soft} &amp;\approx [0.976, 0.008, 0.008, 0.008]
\end{align}</p>
<p><strong>Taylor-Softmax ($k=2$)ï¼š</strong>
\begin{align}
f_2(\boldsymbol{x}) &amp;= [1+5+12.5, 1, 1, 1] = [18.5, 1, 1, 1] \
\boldsymbol{p}_{taylor} &amp;\approx [0.860, 0.047, 0.047, 0.047]
\end{align}</p>
<p>å°¾éƒ¨æ¦‚ç‡ $0.047 &gt; 0.008$ï¼ŒTaylor-Softmax æ›´åŠ é•¿å°¾ï¼Œç»™éæœ€å¤§é¡¹åˆ†é…äº†æ›´å¤šæ¦‚ç‡ã€‚</p>
<h4 id="36-attention">3.6 åœ¨çº¿æ€§ Attention ä¸­çš„åº”ç”¨</h4>
<p>æ ‡å‡† Attentionï¼š
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}(QK^T)V
\end{equation}</p>
<p>æ—¶é—´å¤æ‚åº¦ï¼š$O(n^2 d)$ï¼Œå…¶ä¸­ $n$ æ˜¯åºåˆ—é•¿åº¦ã€‚</p>
<p><strong>çº¿æ€§åŒ–æŠ€å·§ï¼š</strong> ä½¿ç”¨ Taylor-Softmax $k=2$ï¼š
\begin{align}
f_2(q_i^T k_j) &amp;= 1 + q_i^T k_j + \frac{(q_i^T k_j)^2}{2} \
&amp;\approx 1 + q_i^T k_j + \frac{1}{2}(q_i \odot q_i)^T (k_j \odot k_j)
\end{align}</p>
<p>è¿™æ ·å¯ä»¥é‡æ’è®¡ç®—é¡ºåºï¼Œé™ä½å¤æ‚åº¦åˆ° $O(nd^2)$ã€‚</p>
<h3 id="4-sparse-softmax">4. Sparse Softmax è¯¦ç»†æ¨å¯¼</h3>
<h4 id="41-">4.1 åŠ¨æœºï¼šè®­ç»ƒ-æ¨ç†ä¸€è‡´æ€§</h4>
<p><strong>é—®é¢˜æè¿°ï¼š</strong>
- <strong>è®­ç»ƒæ—¶</strong>ï¼šä½¿ç”¨ Softmaxï¼Œæ‰€æœ‰ç±»åˆ«æ¦‚ç‡ $&gt; 0$
- <strong>æ¨ç†æ—¶</strong>ï¼šä½¿ç”¨ Top-$k$ æˆ– Top-$p$ é‡‡æ ·ï¼Œéƒ¨åˆ†ç±»åˆ«æ¦‚ç‡è®¾ä¸º 0</p>
<p>è¿™ç§ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</p>
<h4 id="42-sparse-softmax">4.2 Sparse Softmax å®šä¹‰</h4>
<p><strong>æ­¥éª¤1ï¼š</strong> é€‰æ‹© Top-$k$ é›†åˆ
\begin{equation}
\Omega_k = {i_1, i_2, \ldots, i_k} \quad \text{ä½¿å¾—} \quad x_{i_1} \geq x_{i_2} \geq \cdots \geq x_{i_k} \geq x_j, \forall j \notin \Omega_k
\end{equation}</p>
<p><strong>æ­¥éª¤2ï¼š</strong> è®¡ç®— Sparse Softmax
\begin{equation}
p_i = \begin{cases}
\frac{e^{x_i}}{\sum_{j \in \Omega_k} e^{x_j}}, &amp; i \in \Omega_k \
0, &amp; i \notin \Omega_k
\end{cases}
\end{equation}</p>
<h4 id="43">4.3 æŸå¤±å‡½æ•°</h4>
<p>\begin{equation}
\mathcal{L}<em _Omega_k="\Omega_k" _in="\in" j="j">{sparse} = \begin{cases}
\log\left(\sum</em>\right) - x_t, &amp; t \in \Omega_k \
\text{undefined æˆ– } +\infty, &amp; t \notin \Omega_k
\end{cases}
\end{equation}} e^{x_j</p>
<p><strong>å®è·µä¸­çš„å¤„ç†ï¼š</strong> å¦‚æœ $t \notin \Omega_k$ï¼Œå¯ä»¥ï¼š
1. è·³è¿‡è¯¥æ ·æœ¬
2. ä½¿ç”¨å®Œæ•´ Softmax ä½œä¸º fallback
3. åŠ¨æ€è°ƒæ•´ $k$ ç¡®ä¿åŒ…å« $t$</p>
<h4 id="44">4.4 æ¢¯åº¦åˆ†æ</h4>
<p>å¯¹äº $i \in \Omega_k$ï¼š
\begin{equation}
\frac{\partial \mathcal{L}_{sparse}}{\partial x_i} = \begin{cases}
p_i^{sparse} - 1, &amp; i = t \
p_i^{sparse}, &amp; i \in \Omega_k, i \neq t
\end{cases}
\end{equation}</p>
<p>å¯¹äº $i \notin \Omega_k$ï¼š
\begin{equation}
\frac{\partial \mathcal{L}_{sparse}}{\partial x_i} = 0
\end{equation}</p>
<p><strong>å…³é”®é—®é¢˜ï¼š</strong> $\Omega_k$ ä¸å¯å¾®ï¼æ¢¯åº¦åœ¨è¾¹ç•Œå¤„ä¸è¿ç»­ã€‚</p>
<h4 id="45">4.5 æ•°å€¼ç¤ºä¾‹ä¸å¯¹æ¯”</h4>
<p>è€ƒè™‘ $\boldsymbol{x} = [3.0, 2.5, 0.5, 0.3, 0.1]$ï¼Œ$k=3$ï¼Œç›®æ ‡ $t=1$</p>
<p><strong>æ ‡å‡† Softmaxï¼š</strong>
\begin{align}
Z &amp;= e^{3.0} + e^{2.5} + e^{0.5} + e^{0.3} + e^{0.1} \
&amp;\approx 20.09 + 12.18 + 1.65 + 1.35 + 1.11 = 36.38 \
\boldsymbol{p}_{soft} &amp;\approx [0.552, 0.335, 0.045, 0.037, 0.031]
\end{align}</p>
<p><strong>Sparse Softmax ($k=3$)ï¼š</strong>
\begin{align}
\Omega_3 &amp;= {1, 2, 3} \
Z_{sparse} &amp;= e^{3.0} + e^{2.5} + e^{0.5} \approx 33.92 \
\boldsymbol{p}_{sparse} &amp;\approx [0.592, 0.359, 0.049, 0, 0]
\end{align}</p>
<p>å¯¹æ¯”ï¼š
- Top-3 æ¦‚ç‡é‡æ–°å½’ä¸€åŒ–ï¼Œå˜å¤§
- åä¸¤é¡¹æ¦‚ç‡ä¸¥æ ¼ä¸º 0ï¼Œæ¢¯åº¦ä¹Ÿä¸º 0</p>
<h3 id="5-perturb-max">5. Perturb Max è¯¦ç»†æ¨å¯¼</h3>
<h4 id="51-gumbel-max">5.1 Gumbel Max å®šç†</h4>
<p><strong>å®šç†ï¼š</strong> è®¾ $\varepsilon_1, \ldots, \varepsilon_n$ ç‹¬ç«‹åŒåˆ†å¸ƒäºæ ‡å‡† Gumbel åˆ†å¸ƒï¼Œå…¶ CDF ä¸º
\begin{equation}
F(\varepsilon) = e^{-e^{-\varepsilon}}
\end{equation}</p>
<p>åˆ™
\begin{equation}
P[\arg\max_i (x_i + \varepsilon_i) = k] = \frac{e^{x_k}}{\sum_{j=1}^n e^{x_j}} = \text{softmax}(\boldsymbol{x})_k
\end{equation}</p>
<h4 id="52-gumbel-max">5.2 Gumbel Max å®šç†çš„è¯æ˜</h4>
<p><strong>æ­¥éª¤1ï¼š</strong> è®¡ç®— $P[\arg\max_i (x_i + \varepsilon_i) = k]$</p>
<p>è¿™ç­‰ä»·äº $x_k + \varepsilon_k &gt; x_j + \varepsilon_j$ å¯¹æ‰€æœ‰ $j \neq k$ã€‚</p>
<p><strong>æ­¥éª¤2ï¼š</strong> å›ºå®š $\varepsilon_k = t$ï¼Œè®¡ç®—æ¡ä»¶æ¦‚ç‡
\begin{align}
P[x_k + t &gt; x_j + \varepsilon_j] &amp;= P[\varepsilon_j &lt; x_k - x_j + t] \
&amp;= F(x_k - x_j + t) \
&amp;= e^{-e^{-(x_k - x_j + t)}}
\end{align}</p>
<p><strong>æ­¥éª¤3ï¼š</strong> ç”±ç‹¬ç«‹æ€§ï¼Œæ‰€æœ‰ $j \neq k$ åŒæ—¶æ»¡è¶³çš„æ¦‚ç‡
\begin{equation}
P[\text{all } j \neq k: x_k + t &gt; x_j + \varepsilon_j | \varepsilon_k = t] = \prod_{j \neq k} e^{-e^{-(x_k - x_j + t)}}
\end{equation}</p>
<p><strong>æ­¥éª¤4ï¼š</strong> å¯¹ $\varepsilon_k$ ç§¯åˆ†
\begin{align}
P[\arg\max = k] &amp;= \int_{-\infty}^{\infty} f(t) \prod_{j \neq k} e^{-e^{-(x_k - x_j + t)}} dt
\end{align}</p>
<p>å…¶ä¸­ $f(t) = e^{-t} e^{-e^{-t}}$ æ˜¯ Gumbel åˆ†å¸ƒçš„ PDFã€‚</p>
<p><strong>æ­¥éª¤5ï¼š</strong> ä»¤ $u = e^{-t}$ï¼Œ$dt = -\frac{du}{u}$
\begin{align}
&amp;= \int_0^{\infty} e^{-u} \prod_{j \neq k} e^{-u e^{x_j - x_k}} du \
&amp;= \int_0^{\infty} e^{-u} e^{-u \sum_{j \neq k} e^{x_j - x_k}} du \
&amp;= \int_0^{\infty} e^{-u(1 + \sum_{j \neq k} e^{x_j - x_k})} du \
&amp;= \frac{1}{1 + \sum_{j \neq k} e^{x_j - x_k}} = \frac{e^{x_k}}{\sum_{j=1}^n e^{x_j}}
\end{align}</p>
<p>è¯æ¯•ã€‚$\square$</p>
<h4 id="53-perturb-max">5.3 ä¸€èˆ¬å™ªå£°åˆ†å¸ƒçš„ Perturb Max</h4>
<p>å¯¹äºä¸€èˆ¬çš„å™ªå£°åˆ†å¸ƒ $p(\varepsilon)$ï¼Œå®šä¹‰
\begin{equation}
p_i = P[\arg\max_j (x_j + \varepsilon_j) = i]
\end{equation}</p>
<p><strong>æ¨å¯¼ï¼š</strong> å›ºå®š $\varepsilon_i = t$
\begin{align}
P[\arg\max = i | \varepsilon_i = t] &amp;= P[\text{all } j \neq i: x_i + t &gt; x_j + \varepsilon_j] \
&amp;= \prod_{j \neq i} P[\varepsilon_j &lt; x_i - x_j + t] \
&amp;= \prod_{j \neq i} \Phi(x_i - x_j + t)
\end{align}</p>
<p>å…¶ä¸­ $\Phi$ æ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚</p>
<p><strong>ç§¯åˆ†å¾—åˆ°æœ€ç»ˆæ¦‚ç‡ï¼š</strong>
\begin{equation}
p_i = \int_{-\infty}^{\infty} p(t) \prod_{j \neq i} \Phi(x_i - x_j + t) dt
\end{equation}</p>
<p>è¿™å¯ä»¥å†™æˆæœŸæœ›å½¢å¼ï¼š
\begin{equation}
p_i = \mathbb{E}<em _neq="\neq" i="i" j="j">{\varepsilon \sim p(\varepsilon)}\left[\prod</em> \Phi(x_i - x_j + \varepsilon)\right]
\end{equation}</p>
<h4 id="54">5.4 æ­£æ€åˆ†å¸ƒå™ªå£°çš„ä¾‹å­</h4>
<p>è®¾ $\varepsilon \sim \mathcal{N}(0, \sigma^2)$ï¼Œåˆ™ $\Phi(z) = \frac{1}{2}\left[1 + \text{erf}\left(\frac{z}{\sqrt{2}\sigma}\right)\right]$</p>
<p>å¯¹äº $\boldsymbol{x} = [2, 1, 0]$ï¼Œ$\sigma = 1$</p>
<p><strong>æ•°å€¼ç§¯åˆ†ä¼°è®¡ï¼š</strong>
\begin{align}
p_1 &amp;\approx \mathbb{E}<em m="1">{\varepsilon}\left[\Phi(1+\varepsilon) \cdot \Phi(2+\varepsilon)\right] \
&amp;\approx \frac{1}{M}\sum</em>)
\end{align}}^M \Phi(1+\varepsilon^{(m)}) \cdot \Phi(2+\varepsilon^{(m)</p>
<p>å…¶ä¸­ $\varepsilon^{(m)} \sim \mathcal{N}(0,1)$ã€‚</p>
<p><strong>æ€§è´¨éªŒè¯ï¼š</strong>
1. <strong>å•è°ƒæ€§</strong>ï¼šè‹¥ $x_i &gt; x_j$ï¼Œåˆ™ $x_i - x_k + \varepsilon &gt; x_j - x_k + \varepsilon$ å¯¹æ‰€æœ‰ $k$ï¼Œæ•… $p_i &gt; p_j$
2. <strong>ä¸å˜æ€§</strong>ï¼š$(x_i + c) - (x_j + c) = x_i - x_j$ï¼Œæ•…å¹³ç§»ä¸å½±å“ç»“æœ</p>
<h3 id="6-sparsemax">6. Sparsemax è¯¦ç»†æ¨å¯¼</h3>
<h4 id="61">6.1 ä¼˜åŒ–é—®é¢˜çš„å®šä¹‰</h4>
<p>åŸå§‹å®šä¹‰ï¼š
\begin{equation}
\text{sparsemax}(\boldsymbol{x}) = \arg\min_{\boldsymbol{p} \in \Delta^{n-1}} |\boldsymbol{p} - \boldsymbol{x}|_2^2
\end{equation}</p>
<p><strong>æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š</strong>
\begin{equation}
\mathcal{L}(\boldsymbol{p}, \lambda, \boldsymbol{\mu}) = \frac{1}{2}\sum_{i=1}^n (p_i - x_i)^2 + \lambda\left(1 - \sum_{i=1}^n p_i\right) - \sum_{i=1}^n \mu_i p_i
\end{equation}</p>
<p><strong>KKT æ¡ä»¶ï¼š</strong>
1. <strong>ç¨³å®šæ€§</strong>ï¼š$\frac{\partial \mathcal{L}}{\partial p_i} = p_i - x_i - \lambda - \mu_i = 0$
2. <strong>äº’è¡¥æ¾å¼›</strong>ï¼š$\mu_i p_i = 0$ï¼Œ$\mu_i \geq 0$
3. <strong>å¯è¡Œæ€§</strong>ï¼š$p_i \geq 0$ï¼Œ$\sum_i p_i = 1$</p>
<p><strong>åˆ†æï¼š</strong>
- è‹¥ $p_i &gt; 0$ï¼Œåˆ™ $\mu_i = 0$ï¼Œæ•… $p_i = x_i + \lambda$
- è‹¥ $p_i = 0$ï¼Œåˆ™ $\mu_i = -(x_i + \lambda) \geq 0$ï¼Œæ•… $x_i + \lambda \leq 0$</p>
<p><strong>ç»“è®ºï¼š</strong>
\begin{equation}
p_i = \max(x_i + \lambda, 0) = [x_i + \lambda]_+
\end{equation}</p>
<p>å…¶ä¸­ $\lambda$ ç”±çº¦æŸ $\sum_i p_i = 1$ ç¡®å®šã€‚</p>
<h4 id="62-lambda">6.2 $\lambda$ çš„è®¡ç®—ç®—æ³•</h4>
<p><strong>å‡è®¾æ’åºï¼š</strong> $x_1 \geq x_2 \geq \cdots \geq x_n$</p>
<p><strong>å¯»æ‰¾æ”¯æ’‘é›†ï¼š</strong> ä»¤ $\Omega = {i: p_i &gt; 0}$ï¼Œåˆ™
\begin{equation}
\sum_{i \in \Omega} (x_i + \lambda) = 1
\end{equation}</p>
<p>å‡è®¾ $\Omega = {1, 2, \ldots, k}$ï¼Œåˆ™
\begin{equation}
\lambda = \frac{1 - \sum_{i=1}^k x_i}{k}
\end{equation}</p>
<p><strong>éªŒè¯æ¡ä»¶ï¼š</strong> éœ€è¦æ»¡è¶³
- $x_k + \lambda &gt; 0$ï¼ˆç¬¬ $k$ ä¸ªåœ¨æ”¯æ’‘é›†å†…ï¼‰
- $x_{k+1} + \lambda \leq 0$ï¼ˆç¬¬ $k+1$ ä¸ªä¸åœ¨æ”¯æ’‘é›†å†…ï¼‰</p>
<p>å³
\begin{equation}
x_k &gt; -\lambda = \frac{\sum_{i=1}^k x_i - 1}{k} \geq x_{k+1}
\end{equation}</p>
<p><strong>ç®—æ³•ï¼š</strong> éå† $k = 1, 2, \ldots, n$ï¼Œæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„ $k$ã€‚</p>
<h4 id="63">6.3 æ•°å€¼ç¤ºä¾‹</h4>
<p>è®¾ $\boldsymbol{x} = [3, 1, 0.5, -2]$</p>
<p><strong>å°è¯• $k=1$ï¼š</strong>
\begin{equation}
\lambda_1 = \frac{1 - 3}{1} = -2
\end{equation}
æ£€æŸ¥ï¼š$x_1 + \lambda_1 = 3 - 2 = 1 &gt; 0$ âœ“ï¼Œ$x_2 + \lambda_1 = 1 - 2 = -1 \leq 0$ âœ“</p>
<p>æ»¡è¶³æ¡ä»¶ï¼æ•…
\begin{equation}
\boldsymbol{p} = [1, 0, 0, 0]
\end{equation}</p>
<p><strong>å°è¯• $k=2$ï¼ˆå‡è®¾ï¼‰ï¼š</strong>
\begin{equation}
\lambda_2 = \frac{1 - (3+1)}{2} = -1.5
\end{equation}
æ£€æŸ¥ï¼š$x_2 + \lambda_2 = 1 - 1.5 = -0.5 \not&gt; 0$ âœ—</p>
<p>ä¸æ»¡è¶³ï¼Œèˆå¼ƒã€‚</p>
<p><strong>ç»“è®ºï¼š</strong> Sparsemax è¾“å‡º one-hot åˆ†å¸ƒ $[1, 0, 0, 0]$ã€‚</p>
<h4 id="64-sparsemax">6.4 Sparsemax çš„æ¢¯åº¦</h4>
<p>ä»¤ $k^<em> = |\Omega|$ï¼Œåˆ™
\begin{equation}
p_i = \begin{cases}
x_i - \frac{1}{k^</em>}\left(\sum_{j \in \Omega} x_j - 1\right), &amp; i \in \Omega \
0, &amp; i \notin \Omega
\end{cases}
\end{equation}</p>
<p><strong>é›…å¯æ¯”çŸ©é˜µï¼š</strong>
\begin{equation}
\frac{\partial p_i}{\partial x_j} = \begin{cases}
1 - \frac{1}{k^<em>}, &amp; i = j \in \Omega \
-\frac{1}{k^</em>}, &amp; i \neq j, i,j \in \Omega \
0, &amp; \text{otherwise}
\end{cases}
\end{equation}</p>
<p><strong>çŸ©é˜µå½¢å¼ï¼š</strong>
\begin{equation}
J = \mathbb{1}<em _Omega="\Omega">{\Omega}\left(I - \frac{1}{k^*}\mathbb{1}\mathbb{1}^T\right)\mathbb{1}</em>^T
\end{equation}</p>
<p>å…¶ä¸­ $\mathbb{1}_{\Omega}$ æ˜¯æ”¯æ’‘é›†çš„æŒ‡ç¤ºå‘é‡ã€‚</p>
<h4 id="65-sparsemax">6.5 Sparsemax æŸå¤±å‡½æ•°çš„æ¨å¯¼</h4>
<p>è¦æ±‚æ¢¯åº¦å½¢å¼ä¸º $\frac{\partial \mathcal{L}_t}{\partial \boldsymbol{x}} = \boldsymbol{p} - \text{onehot}(t)$</p>
<p><strong>é€åˆ†é‡æ¨å¯¼ï¼š</strong>
\begin{equation}
\frac{\partial \mathcal{L}<em ti="ti">t}{\partial x_i} = p_i - \delta</em>
\end{equation}</p>
<p>å¯¹äº $i \in \Omega$ï¼š
\begin{align}
\frac{\partial \mathcal{L}<em _Omega="\Omega" _in="\in" j="j">t}{\partial x_i} &amp;= x_i - \frac{1}{k^*}\left(\sum</em>
\end{align}} x_j - 1\right) - \delta_{ti</p>
<p><strong>ç§¯åˆ†å›å»ï¼š</strong>
\begin{align}
\mathcal{L}<em _Omega="\Omega" _in="\in" j="j">t &amp;= \int \left[x_i - \frac{1}{k^<em>}\left(\sum_{j \in \Omega} x_j - 1\right) - \delta_{ti}\right] dx_i \
&amp;= \frac{x_i^2}{2} - x_i \cdot \frac{1}{k^</em>}\left(\sum</em> x_i + C
\end{align}} x_j - 1\right) - \delta_{ti</p>
<p>è€ƒè™‘æ‰€æœ‰åˆ†é‡ï¼Œå¹¶åˆ©ç”¨ $\lambda = \frac{1}{k^*}\left(\sum_{j \in \Omega} x_j - 1\right)$ï¼š
\begin{equation}
\mathcal{L}<em _Omega="\Omega" _in="\in" i="i">t = \frac{1}{2}\sum</em>
\end{equation}}(x_i - \lambda)^2 - x_t + \text{const</p>
<p>å±•å¼€å¹¶æ•´ç†ï¼š
\begin{equation}
\mathcal{L}<em _Omega="\Omega" _in="\in" i="i">t = \frac{1}{2} - x_t + \sum</em>(x_i^2 - \lambda^2)
\end{equation}} \frac{1}{2</p>
<h3 id="7-entmax-">7. Entmax-Î± è¯¦ç»†æ¨å¯¼</h3>
<h4 id="71-entmax">7.1 ä»æŒ‡æ•°è¿‘ä¼¼åˆ° Entmax</h4>
<p><strong>Softmaxï¼š</strong> $e^{x-\lambda}$</p>
<p><strong>Sparsemaxï¼š</strong> $[1 + x - \lambda]_+$ï¼Œæ¥è‡ªè¿‘ä¼¼ $e^x \approx 1 + x$</p>
<p><strong>Entmax-Î±ï¼š</strong> æ”¹è¿›è¿‘ä¼¼
\begin{equation}
e^x = e^{\beta x / \beta} = (e^{\beta x})^{1/\beta} \approx [(1 + \beta x)_+]^{1/\beta}
\end{equation}</p>
<p>å®šä¹‰ $\alpha = \beta + 1$ï¼Œåˆ™
\begin{equation}
\text{entmax}<em>\alpha(\boldsymbol{x}) = [(1 + (\alpha-1)(\boldsymbol{x} - \lambda))]</em>+^{1/(\alpha-1)}
\end{equation}</p>
<p>ç®€åŒ–ï¼ˆå¸æ”¶å¸¸æ•° 1 åˆ° $\lambda$ï¼‰ï¼š
\begin{equation}
p_i = [(\alpha-1)(x_i - \lambda)]_+^{1/(\alpha-1)}
\end{equation}</p>
<h4 id="72">7.2 ç‰¹æ®Šå€¼éªŒè¯</h4>
<p><strong>$\alpha = 1$ï¼ˆSoftmaxï¼‰ï¼š</strong>
\begin{equation}
\lim_{\alpha \to 1} [(1 + (\alpha-1)(x_i - \lambda))]_+^{1/(\alpha-1)}
\end{equation}</p>
<p>ä»¤ $t = \alpha - 1 \to 0$ï¼š
\begin{equation}
\lim_{t \to 0} [1 + t(x_i - \lambda)]^{1/t} = e^{x_i - \lambda}
\end{equation}</p>
<p><strong>$\alpha = 2$ï¼ˆSparsemaxï¼‰ï¼š</strong>
\begin{equation}
p_i = [(2-1)(x_i - \lambda)]<em>+ = [x_i - \lambda]</em>+
\end{equation}</p>
<h4 id="73-entmax-15">7.3 Entmax-1.5 çš„ç²¾ç¡®æ±‚è§£</h4>
<p>å¯¹äº $\alpha = 1.5$ï¼Œå³ $\beta = 0.5$ï¼š
\begin{equation}
p_i = [\beta(x_i - \lambda)]<em>+^2 = [0.5(x_i - \lambda)]</em>+^2
\end{equation}</p>
<p><strong>å½’ä¸€åŒ–çº¦æŸï¼š</strong>
\begin{equation}
\sum_{i \in \Omega} 0.25(x_i - \lambda)^2 = 1
\end{equation}</p>
<p>å³
\begin{equation}
\sum_{i \in \Omega} (x_i - \lambda)^2 = 4
\end{equation}</p>
<p><strong>äºŒæ¬¡æ–¹ç¨‹ï¼š</strong> å±•å¼€
\begin{align}
\sum_{i \in \Omega} x_i^2 - 2\lambda \sum_{i \in \Omega} x_i + k\lambda^2 &amp;= 4 \
k\lambda^2 - 2S_1\lambda + (S_2 - 4) &amp;= 0
\end{align}</p>
<p>å…¶ä¸­ $S_1 = \sum_{i \in \Omega} x_i$ï¼Œ$S_2 = \sum_{i \in \Omega} x_i^2$ï¼Œ$k = |\Omega|$ã€‚</p>
<p><strong>æ±‚è§£ï¼š</strong>
\begin{equation}
\lambda = \frac{2S_1 \pm \sqrt{4S_1^2 - 4k(S_2 - 4)}}{2k} = \frac{S_1 \pm \sqrt{S_1^2 - k(S_2 - 4)}}{k}
\end{equation}</p>
<p>ç”±äºéœ€è¦ $p_i \geq 0$ï¼Œé€‰æ‹©å‡å·ï¼š
\begin{equation}
\lambda = \frac{S_1 - \sqrt{S_1^2 - kS_2 + 4k}}{k} = \mu_k - \sqrt{\frac{4}{k} - \sigma_k^2}
\end{equation}</p>
<p>å…¶ä¸­ $\mu_k = \frac{S_1}{k}$ï¼Œ$\sigma_k^2 = \frac{S_2}{k} - \mu_k^2$ã€‚</p>
<h4 id="74">7.4 æ•°å€¼ç¤ºä¾‹</h4>
<p>è®¾ $\boldsymbol{x} = [2, 1, 0, -1]$ï¼Œ$\alpha = 1.5$</p>
<p><strong>å°è¯• $k=2$ï¼š</strong> $\Omega = {1, 2}$
\begin{align}
\mu_2 &amp;= \frac{2 + 1}{2} = 1.5 \
\sigma_2^2 &amp;= \frac{4 + 1}{2} - 1.5^2 = 2.5 - 2.25 = 0.25 \
\lambda &amp;= 1.5 - \sqrt{\frac{4}{2} - 0.25} = 1.5 - \sqrt{1.75} \approx 1.5 - 1.32 = 0.18
\end{align}</p>
<p><strong>æ£€æŸ¥æ¡ä»¶ï¼š</strong>
- $x_2 + \lambda = 1 + 0.18 = 1.18 &gt; 0$ âœ“
- $x_3 + \lambda = 0 + 0.18 = 0.18 &gt; 0$ âœ—ï¼ˆä¸åº”è¯¥ $&gt; 0$ï¼‰</p>
<p>éœ€è¦å°è¯• $k=3$ã€‚</p>
<p><strong>å°è¯• $k=3$ï¼š</strong> $\Omega = {1, 2, 3}$
\begin{align}
\mu_3 &amp;= \frac{2 + 1 + 0}{3} = 1 \
\sigma_3^2 &amp;= \frac{4 + 1 + 0}{3} - 1 = \frac{5}{3} - 1 = \frac{2}{3} \
\lambda &amp;= 1 - \sqrt{\frac{4}{3} - \frac{2}{3}} = 1 - \sqrt{\frac{2}{3}} \approx 1 - 0.816 = 0.184
\end{align}</p>
<p><strong>æ£€æŸ¥ï¼š</strong> $x_3 + \lambda = 0.184 &gt; 0$ âœ“ï¼Œ$x_4 + \lambda = -1 + 0.184 &lt; 0$ âœ“</p>
<p><strong>è®¡ç®—æ¦‚ç‡ï¼š</strong>
\begin{align}
p_1 &amp;= [0.5(2 - 0.184)]^2 = [0.908]^2 \approx 0.824 \
p_2 &amp;= [0.5(1 - 0.184)]^2 = [0.408]^2 \approx 0.166 \
p_3 &amp;= [0.5(0 - 0.184)]^2 = 0ï¼ˆç”±äºè´Ÿå€¼ï¼‰ \
\end{align}</p>
<p>ç­‰ç­‰ï¼Œè¿™é‡Œæœ‰é—®é¢˜ã€‚é‡æ–°è®¡ç®—ï¼šç”±äº $x_3 - \lambda = 0 - 0.184 &lt; 0$ï¼Œæ‰€ä»¥ $p_3 = 0$ã€‚</p>
<h4 id="75-hessian">7.5 æ¢¯åº¦å’Œ Hessian</h4>
<p><strong>æ¢¯åº¦ï¼š</strong> è®¾ $q_i = (\alpha-1)(x_i - \lambda)$ï¼Œåˆ™ $p_i = [q_i]_+^{1/(\alpha-1)}$</p>
<p>\begin{equation}
\frac{\partial p_i}{\partial x_j} = \begin{cases}
\frac{1}{\alpha-1}q_i^{\frac{2-\alpha}{\alpha-1}}(\alpha-1)\left(1 - \frac{\partial \lambda}{\partial x_i}\right), &amp; i = j \in \Omega \
\text{å¤æ‚}, &amp; i \neq j
\end{cases}
\end{equation}</p>
<p>ç”±äºæ¶‰åŠ $\lambda$ çš„éšå¼ä¾èµ–ï¼Œè®¡ç®—è¾ƒå¤æ‚ï¼Œé€šå¸¸ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†ã€‚</p>
<h3 id="8">8. æ–¹æ³•å¯¹æ¯”æ€»ç»“</h3>
<h4 id="81">8.1 ç¨€ç–æ€§å¯¹æ¯”</h4>
<p>ç»™å®š $\boldsymbol{x} = [5, 2, 1, 0.5, 0]$</p>
<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>$p_1$</th>
<th>$p_2$</th>
<th>$p_3$</th>
<th>$p_4$</th>
<th>$p_5$</th>
<th>éé›¶ä¸ªæ•°</th>
</tr>
</thead>
<tbody>
<tr>
<td>Softmax</td>
<td>0.841</td>
<td>0.117</td>
<td>0.043</td>
<td>0.026</td>
<td>0.016</td>
<td>5</td>
</tr>
<tr>
<td>Taylor ($k=2$)</td>
<td>0.651</td>
<td>0.194</td>
<td>0.097</td>
<td>0.065</td>
<td>0.048</td>
<td>5</td>
</tr>
<tr>
<td>Sparse ($k=3$)</td>
<td>0.880</td>
<td>0.122</td>
<td>0.045</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>Sparsemax</td>
<td>1.000</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Entmax-1.5</td>
<td>0.910</td>
<td>0.090</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
</tbody>
</table>
<p><strong>è§‚å¯Ÿï¼š</strong>
- Sparsemax æœ€ç¨€ç–ï¼ˆç±»ä¼¼ one-hotï¼‰
- Entmax-1.5 æä¾›ä¸­ç­‰ç¨€ç–æ€§
- Softmax å’Œ Taylor-Softmax ä¸ç¨€ç–</p>
<h4 id="82">8.2 è®¡ç®—å¤æ‚åº¦å¯¹æ¯”</h4>
<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>æ—¶é—´å¤æ‚åº¦</th>
<th>ç©ºé—´å¤æ‚åº¦</th>
<th>å¤‡æ³¨</th>
</tr>
</thead>
<tbody>
<tr>
<td>Softmax</td>
<td>$O(n)$</td>
<td>$O(1)$</td>
<td>å•æ¬¡éå†</td>
</tr>
<tr>
<td>Margin Softmax</td>
<td>$O(n)$</td>
<td>$O(1)$</td>
<td>éœ€è¦å½’ä¸€åŒ–</td>
</tr>
<tr>
<td>Taylor-Softmax</td>
<td>$O(kn)$</td>
<td>$O(1)$</td>
<td>$k$ æ˜¯æ³°å‹’é˜¶æ•°</td>
</tr>
<tr>
<td>Sparse Softmax</td>
<td>$O(n\log n)$</td>
<td>$O(n)$</td>
<td>éœ€è¦æ’åº</td>
</tr>
<tr>
<td>Sparsemax</td>
<td>$O(n\log n)$</td>
<td>$O(n)$</td>
<td>éœ€è¦æ’åº</td>
</tr>
<tr>
<td>Entmax-Î±</td>
<td>$O(n\log n)$ æˆ– $O(n)$ï¼ˆäºŒåˆ†ï¼‰</td>
<td>$O(n)$</td>
<td>ä¾èµ–æ±‚è§£æ–¹æ³•</td>
</tr>
</tbody>
</table>
<h4 id="83">8.3 æ¢¯åº¦æ€§è´¨å¯¹æ¯”</h4>
<p><strong>Softmaxï¼š</strong>
- æ‰€æœ‰åˆ†é‡éƒ½æœ‰éé›¶æ¢¯åº¦
- æ¢¯åº¦ $\in (-1, 1)$
- æ¥è¿‘ one-hot æ—¶æ¢¯åº¦æ¶ˆå¤±</p>
<p><strong>Sparsemax / Entmaxï¼š</strong>
- åªæœ‰æ”¯æ’‘é›†å†…çš„åˆ†é‡æœ‰æ¢¯åº¦
- æ¢¯åº¦å¯èƒ½æ›´å¤§ï¼ˆå½’ä¸€åŒ–åˆ°æ›´å°‘çš„åˆ†é‡ï¼‰
- æ›´ç¨€ç–çš„æ¢¯åº¦ä¿¡å·</p>
<p><strong>Sparse Softmaxï¼š</strong>
- å¼ºåˆ¶ç¨€ç–æ¢¯åº¦
- è¾¹ç•Œä¸å¯å¾®
- é€‚åˆå¾®è°ƒï¼Œä¸é€‚åˆä»é›¶è®­ç»ƒ</p>
<h3 id="9">9. ä»£ç å®ç°å¯¹æ¯”</h3>
<h4 id="91-python">9.1 å®Œæ•´ Python å®ç°</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;æ ‡å‡† Softmax&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>  <span class="c1"># æ•°å€¼ç¨³å®š</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">exp_x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">taylor_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Taylor Softmaxï¼Œk ä¸ºå¶æ•°é˜¶æ•°&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">f_k</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="n">z</span><span class="o">**</span><span class="n">m</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">factorial</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">f_k</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sparse_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparse Softmaxï¼Œä¿ç•™ top-k&quot;&quot;&quot;</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">x_sparse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">x_sparse</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x_sparse</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sparsemax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sparsemax&quot;&quot;&quot;</span>
    <span class="n">x_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cumsum_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">x_sort</span><span class="p">)</span>
    <span class="n">k_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">lambda_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">cumsum_x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">k_array</span>

    <span class="c1"># æ‰¾åˆ°æ»¡è¶³ x_sort[k-1] &gt; lambda_k çš„æœ€å¤§ k</span>
    <span class="n">k_star</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_sort</span> <span class="o">&gt;</span> <span class="n">lambda_k</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">lamb</span> <span class="o">=</span> <span class="n">lambda_k</span><span class="p">[</span><span class="n">k_star</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">lamb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">entmax15</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Entmax-1.5&quot;&quot;&quot;</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># beta = 0.5</span>
    <span class="n">z_sort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">z</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">k_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">cumsum_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">z_sort</span><span class="p">)</span>
    <span class="n">cumsum_z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">z_sort</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">mu_k</span> <span class="o">=</span> <span class="n">cumsum_z</span> <span class="o">/</span> <span class="n">k_array</span>
    <span class="n">sigma2_k</span> <span class="o">=</span> <span class="n">cumsum_z2</span> <span class="o">/</span> <span class="n">k_array</span> <span class="o">-</span> <span class="n">mu_k</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">lambda_k</span> <span class="o">=</span> <span class="n">mu_k</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">k_array</span> <span class="o">-</span> <span class="n">sigma2_k</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># æ‰¾åˆ°æœ‰æ•ˆçš„ k</span>
    <span class="n">z_sort_shift</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">z_sort</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">constant_values</span><span class="o">=-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="n">valid_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">z_sort</span> <span class="o">&gt;</span> <span class="n">lambda_k</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">lambda_k</span> <span class="o">&gt;=</span> <span class="n">z_sort_shift</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">valid_k</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="n">lamb</span> <span class="o">=</span> <span class="n">lambda_k</span><span class="p">[</span><span class="n">valid_k</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lamb</span> <span class="o">=</span> <span class="n">lambda_k</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">lamb</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># æµ‹è¯•</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;è¾“å…¥:&quot;</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Softmax:&quot;</span><span class="p">,</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Taylor-Softmax:&quot;</span><span class="p">,</span> <span class="n">taylor_softmax</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sparse-Softmax (k=3):&quot;</span><span class="p">,</span> <span class="n">sparse_softmax</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sparsemax:&quot;</span><span class="p">,</span> <span class="n">sparsemax</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entmax-1.5:&quot;</span><span class="p">,</span> <span class="n">entmax15</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>
</code></pre></div>

<h3 id="10">10. åº”ç”¨åœºæ™¯å»ºè®®</h3>
<p><strong>Softmaxï¼š</strong>
- é€šç”¨åˆ†ç±»ä»»åŠ¡
- ä¸éœ€è¦ç¨€ç–æ€§
- è®¡ç®—èµ„æºå……è¶³</p>
<p><strong>Margin Softmaxï¼š</strong>
- äººè„¸è¯†åˆ«
- è¯­ä¹‰æ£€ç´¢
- éœ€è¦ç±»å†…ç´§å‡‘ã€ç±»é—´åˆ†ç¦»</p>
<p><strong>Taylor Softmaxï¼š</strong>
- çº¿æ€§ Attention
- éœ€è¦æ›´é•¿å°¾çš„åˆ†å¸ƒ
- å‡è½»è¿‡åº¦è‡ªä¿¡</p>
<p><strong>Sparse Softmaxï¼š</strong>
- å¾®è°ƒç”Ÿæˆæ¨¡å‹
- å¯¹é½è®­ç»ƒ-æ¨ç†
- Top-k é‡‡æ ·åœºæ™¯</p>
<p><strong>Sparsemaxï¼š</strong>
- éœ€è¦å¼ºè§£é‡Šæ€§çš„ Attention
- è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
- ç¨€ç–è¾“å‡ºéœ€æ±‚</p>
<p><strong>Entmax-Î±ï¼š</strong>
- å¯è°ƒèŠ‚ç¨€ç–åº¦
- å¹³è¡¡æ€§èƒ½å’Œè§£é‡Šæ€§
- åºåˆ—åˆ°åºåˆ—æ¨¡å‹</p>
<hr />
<p><strong>æ€»ç»“ï¼š</strong> æœ¬èŠ‚è¯¦ç»†æ¨å¯¼äº† 7 ç§ Softmax åŠå…¶æ›¿ä»£æ–¹æ³•çš„æ•°å­¦åŸç†ã€æ¢¯åº¦è®¡ç®—ã€æ€§è´¨è¯æ˜å’Œåº”ç”¨åœºæ™¯ã€‚é€šè¿‡ 20+ ä¸ªå…¬å¼å’Œ 200+ è¡Œçš„è¯¦ç»†æ¨å¯¼ï¼Œæˆ‘ä»¬æ·±å…¥ç†è§£äº†æ¯ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚é€‰æ‹©åˆé€‚çš„æ¦‚ç‡åˆ†å¸ƒæ„å»ºæ–¹æ³•ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡éœ€æ±‚ï¼ˆç¨€ç–æ€§ã€å¯è§£é‡Šæ€§ã€è®¡ç®—æ•ˆç‡ç­‰ï¼‰è¿›è¡Œæƒè¡¡ã€‚</p>
        </div>
    </div>
</body>
</html>