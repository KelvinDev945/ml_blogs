<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MoE环游记：1、从几何意义出发</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">← 返回首页</a>
        <header>
            <h1>MoE环游记：1、从几何意义出发</h1>
            <div class="meta">📅 最后更新: 2025-11-24 | 📄 大小: 50.1 KB</div>
        </header>
        <div class="content">
            <p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10699">https://spaces.ac.cn/archives/10699</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>前两年福至心灵之下，开了一个“<a href="/search/Transformer%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF/">Transformer升级之路</a>”系列，陆续分享了主流Transformer架构的一些改进工作和个人思考，得到了部份读者的认可。这篇文章开始，我们沿着同样的风格，介绍当前另一个主流架构MoE（Mixture of Experts）。</p>
<p>MoE的流行自不必多说，近来火出圈的<a href="https://papers.cool/arxiv/2412.19437">DeepSeek-V3</a>便是MoE架构，传言GPT-4也是MoE架构，国内最近出的一些模型也有不少用上了MoE。然而，虽然MoE的研究由来已久，但其应用长时间内都不愠不火，大致上是从去年初的<a href="https://papers.cool/arxiv/2401.04088">《Mixtral of Experts》</a>开始，MoE才逐渐吸引大家的注意力，其显著优点是参数量大，但训练和推理成本都显著低。</p>
<p>但同时MoE也有一些难题，如训练不稳定、负载不均衡、效果不够好等，这也是它早年没有流行起来的主要原因。不过随着这两年关注度的提升，这些问题在很大程度上已经得到解决，我们在接下来的介绍中会逐一谈到这些内容。</p>
<h2 id="_1">问题定义</h2>
<p>首先要指出的是，这里会用笔者自己的一种理解思路来介绍MoE，在必要的地方会附上相应的参考文献，但不会对MoE架构进行系统的追根溯源，还请读者见谅。</p>
<p>我们知道，Transformer模型由Attention层和MLP层组成，MoE替换的是模型中MLP层。MLP层又分FFN（FeedForward Network）和GLU（Gated Linear Unit）两种，主流的是GLU，但简单起见我们还是以FFN为例<br />
\begin{equation}\boldsymbol{y} = f(\boldsymbol{x}\boldsymbol{W}^{(A)})\boldsymbol{W}^{(B)}\end{equation}<br />
其中$\boldsymbol{x}\in\mathbb{R}^{d}$是输入向量（行向量），$\boldsymbol{W}^{(A)}\in\mathbb{R}^{d\times D},\boldsymbol{W}^{(B)}\in\mathbb{R}^{D\times d}$是两个参数矩阵，$f$是Element-wise的激活函数。设$n$是一个能整除$D$的整数，那么上述可以等价地用分块矩阵写成<br />
\begin{equation}\boldsymbol{y} = f\big(\boldsymbol{x}\begin{bmatrix}\boldsymbol{W}^{(A)}<em i="1">1 &amp; \boldsymbol{W}^{(A)}_2 &amp; \cdots &amp; \boldsymbol{W}^{(A)}_n\end{bmatrix}\big)\begin{bmatrix}\boldsymbol{W}^{(B)}_1 \\\ \boldsymbol{W}^{(B)}_2 \\\ \vdots \\\ \boldsymbol{W}^{(B)}_n\end{bmatrix} = \sum</em>}^n \underbrace{f(\boldsymbol{x}\boldsymbol{W}^{(A)<em _boldsymbol_v="\boldsymbol{v">i)\boldsymbol{W}^{(B)}_i}</em><em _:_i-1_c:ic_="[:,(i-1)c:ic]">i}\end{equation}<br />
其中$\boldsymbol{W}^{(A)}_i = \boldsymbol{W}^{(A)}</em>}, \boldsymbol{W}^{(B)<em _i-1_c:ic_:_="[(i-1)c:ic,:]">i = \boldsymbol{W}^{(B)}</em>_i$的输出，每个小模型计算量相同，这些小模型就是MoE中的“Expert”。},c= D/n$，这里的切片按照Python规则来。由此可见，FFN可以等价表示成$n$个向量$\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_n$之和，每个向量代表了一个小模型$f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i)\boldsymbol{W}^{(B)</p>
<p>MoE提出的问题是：</p>
<blockquote>
<p>能否只挑$k$个向量的和来逼近$n$个向量的和呢？这样就可以将计算量降低到$k/n$了。</p>
</blockquote>
<h2 id="_2">模长排序</h2>
<p>这个问题其实我们在<a href="/archives/10427">《低秩近似之路（三）：CR》</a>已经探究过，写成数学公式是<br />
\begin{equation}\mathop{\text{argmin}}<em i="1">{\lambda_1,\lambda_2,\cdots,\lambda_n\in\{0,1\}}\left\Vert\sum</em>}^n \lambda_i \boldsymbol{v<em i="1">i - \sum</em>}^n\boldsymbol{v<em i="1">i\right\Vert^2\quad\text{s.t.}\quad \sum</em>}^n \lambda_i = k\end{equation<br />
记$\gamma_i = 1 - \lambda_i$，那么它又可以写成<br />
\begin{equation}\mathop{\text{argmin}}<em i="1">{\gamma_1,\gamma_2,\cdots,\gamma_n\in\{0,1\}}\left\Vert\sum</em>}^n \gamma_i \boldsymbol{v<em i="1">i\right\Vert^2\quad\text{s.t.}\quad \sum</em>}^n \gamma_i = n - k\end{equation<br />
这个问题的精确求解是比较困难的，但有一个简单的近似解：当$\boldsymbol{v}<em i="1">i$两两正交时，我们有<br />
\begin{equation}\left\Vert\sum</em>}^n \gamma_i \boldsymbol{v<em i="1">i\right\Vert^2 = \sum</em>}^n \gamma_i^2 \Vert\boldsymbol{v<em i="1">i\Vert^2 = \sum</em>}^n \gamma_i \Vert\boldsymbol{v}_i\Vert^2\end{equation<br />
上式最优解显然就是让模长$\Vert\boldsymbol{v}_i\Vert$最小的$n-k$个$\gamma_i$等于1，这又等价于说挑出模长最大的$k$个向量来逼近$n$个向量之和。当$\boldsymbol{v}_i$不满足两两正交的条件时，我们依然用它来作为一个近似解。它的几何意义也很直观，模长越大的向量，在求和过程中越不容易被抵消，从而作用越突出。</p>
<p>此外，在<a href="/archives/10427">《低秩近似之路（三）：CR》</a>中我们还讨论了一种依概率采样的逼近过程，在方差最小的假设下得到的最优采样概率同样有正比于模长的特点，所以总的来说按向量模长排序是一个简单但不失有效的策略。</p>
<h2 id="moe">MoE初现</h2>
<p>现在策略已经有了——“挑模长最大的$k$个向量”——可是细想之下我们会发现它并不实用：要挑模长最大的$k$个向量，就得把所有向量的模长都算出来，这又意味着要把所有的$\boldsymbol{v}_i$先算出来，可我们的原本目的却是减少$\boldsymbol{v}_i$的计算量！</p>
<p>为了解决这个矛盾，我们需要重新设计每个Expert模型，使得它的模长可以低成本地计算出来。什么意思呢？首先我们将$\boldsymbol{v}<em _boldsymbol_rho="\boldsymbol{\rho">i$归一化得到$\boldsymbol{e}_i = \boldsymbol{v}_i/\Vert\boldsymbol{v}_i\Vert$，这样每个$\boldsymbol{e}_i$的模长都相同了。接着我们定义<br />
\begin{equation}\underbrace{[\rho_1,\rho_2,\cdots,\rho_n]}</em>}} = h(\boldsymbol{x}\boldsymbol{W}^{(R)})\quad\in\mathbb{R<em 0="0" _geq="\geq">{\geq 0}^n\end{equation}<br />
其中$\boldsymbol{W}^{(R)}\in\mathbb{R}^{d\times n}$是参数矩阵，$h(\cdot)$是一个$\mathbb{R}\to\mathbb{R}</em>$的激活函数，说白了这就是一个$d$维到$n$维的线性变换加激活函数，所以计算量是比较小的，这部分模型在MoE中被称为“Router”。</p>
<p>$\boldsymbol{\rho}$的作用是什么呢？预测每个Expert的模长！换言之，我们将$\rho_i$作为第$i$个Expert的模长，$\rho_i \boldsymbol{e}<em _mathop_text_argtop="\mathop{\text{argtop" i_in="i\in">i$才是完整的Expert，它被分解为两部分：计算量比较小的模长$\rho_i$以及计算量比较大的方向$\boldsymbol{e}_i$。为了减少计算量，我们先计算出$\boldsymbol{\rho}$，挑出最大的$k$个后才去计算相应的$\boldsymbol{e}_i$，最后乘上$\rho_i$并求和：<br />
\begin{equation}\boldsymbol{y} = \sum</em>}}_k \boldsymbol{\rho}} \rho_i \boldsymbol{e}_i\end{equation<br />
这便是MoE模型的基本公式。由于计算中只保留了Top-$k$部分，所以它本质上属于一种Sparse模型，而原本的FFN或者$k=n$时的模型，通常称为对应的Dense模型。</p>
<h2 id="_3">思路概括</h2>
<p>不管是熟悉MoE还是不熟悉MoE的读者，可能都会对上述过程有点陌生，因为这是笔者自己闭门造车的一种MoE理解路线，但因为其几何意义更明确，所以本质上应该是更好理解的。</p>
<p>我们再来整理一下整个思路：</p>
<blockquote>
<p>1、一个常规的Dense模型FFN，可以等价改写为$n$个Expert向量$\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_n$之和；</p>
<p>2、为了节省计算量，我们试图挑出$k$个向量求和来逼近原本的$n$个向量之和；</p>
<p>3、转化为数学问题求解后，我们发现挑选规则是模长最大的$k$个向量；</p>
<p>4、直接去算$n$个Expert的模长然后选$k$个实际上是不省计算量的，所以要重新设计Expert；</p>
<p>5、将$\boldsymbol{v}_i$归一化得到$\boldsymbol{e}_i$，然后用另外的小模型（Router）预测模长$\rho_i$，最终的Expert为$\rho_i \boldsymbol{e}_i$；</p>
<p>6、此时，我们就可以先算全体$\rho_i$，挑出$k$个后才去计算$\boldsymbol{e}_i$，达到节省计算量的目的。</p>
</blockquote>
<h2 id="_4">为何如此</h2>
<p>可能有些读者疑问，为什么要做这个看似复杂的过程？原本的MoE不是挺好理解的吗？一般的MoE形式为<br />
\begin{equation}\boldsymbol{y} = \sum_{i\in \mathop{\text{argtop}}_k \boldsymbol{\rho}} \rho_i \boldsymbol{v}_i\end{equation}<br />
也就是求和前少了对$\boldsymbol{v}_i$的归一化，此时$\rho_i$也没有模长的意义，它纯粹是一个用来对Expert排序的打分模型（即Router）。可为什么将$\rho_i$乘到Expert上去就能让Router学会正确排序Expert呢？笔者发现只有<a href="https://papers.cool/arxiv/2310.00811">《Sparse Backpropagation for MoE Training》</a>对此给出了一个解释，但还是稍欠直观。</p>
<p>而在本文的几何视角下，我们会发现很多问题就“豁然开朗”了。我们将Expert重新参数化为$\rho_i \boldsymbol{e}_i$后，Dense模型对应于全体$\rho_i \boldsymbol{e}_i$求和，而MoE对应于$\rho_i$选Top-$k$后求和，这是Dense模型的一个有理论保证的逼近。我们没有去考虑Router如何选择Expert，只是每一步都尽可能逼近Dense模型，这可以说是<strong>既要</strong> 大参数、<strong>又要</strong> 小计算量的最佳选择。</p>
<p>现在$\rho_i$的几何意义是模长而不是概率，所以激活函数$h(\cdot)$就没有归一化的要求了，除了Softmax外，像Sigmoid、ReLU都可以考虑使用，也可以考虑我们在<a href="/archives/10373">《Softmax后传：寻找Top-K的光滑近似》</a>介绍的Top-$k$光滑近似。Router使用非归一化的激活函数，有助于避免$k &gt; 1$时Expert之间的恶性竞争，有时候能取得更好的效果。</p>
<p>最后补充一点，我们前面定义$\boldsymbol{e}_i = \boldsymbol{v}_i/ \Vert\boldsymbol{v}_i\Vert$，目的是让所有$\boldsymbol{e}_i$模长相同，实际操作中不是一定要L2 Normalize，也可以是其他等价操作，比如gamma参数恒等于1的RMS Norm，它更符合我们的输出习惯。</p>
<h2 id="_5">文章小结</h2>
<p>本文从Dense模型的最佳逼近出发来推导和理解MoE，得到了一种特定的MoE形式，它比现有MoE多了一个Normalize步骤，但能让MoE的几何意义更加明显。当然，不管Normalize与否，MoE之路都只是刚刚开始，更多的困难还在路上。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10699">https://spaces.ac.cn/archives/10699</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Feb. 08, 2025). 《MoE环游记：1、从几何意义出发 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10699">https://spaces.ac.cn/archives/10699</a></p>
<p>@online{kexuefm-10699,<br />
title={MoE环游记：1、从几何意义出发},<br />
author={苏剑林},<br />
year={2025},<br />
month={Feb},<br />
url={\url{https://spaces.ac.cn/archives/10699}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释</h2>
<h3 id="1">第1部分：核心理论、公理与历史基础</h3>
<h4 id="11">1.1 理论起源与历史发展</h4>
<p><strong>MoE的理论根源</strong>可追溯到多个研究领域的交叉：</p>
<div class="theorem-box">

**多来源融合**：
- **集成学习** (1990s)：通过组合多个弱学习器提升性能
- **专家系统** (1980s)：不同专家处理不同类型的问题
- **稀疏激活神经网络** (2000s)：只激活网络的一部分以提高效率
- **条件计算** (2013, Bengio et al.)：根据输入动态选择计算路径

</div>

<p><strong>关键里程碑</strong>：</p>
<ol>
<li><strong>1991 - Jacobs等人</strong>：首次提出"混合专家"概念，用于多任务学习</li>
<li><strong>2017 - Shazeer等人（Google Brain）</strong>：在LSTM中引入MoE，处理1000亿参数</li>
<li><strong>2021 - Switch Transformer</strong>：简化为Top-1路由，在大规模预训练中验证有效性</li>
<li><strong>2023 - Mixtral 8x7B</strong>：开源MoE模型，证明在资源受限下的优越性</li>
<li><strong>2024 - DeepSeek-V3</strong>：685B总参数，37B激活参数，突破性能与效率平衡</li>
</ol>
<h4 id="12">1.2 数学公理与基础假设</h4>
<div class="theorem-box">

### 公理1：可分解性假设

深度神经网络的表示可以分解为多个子空间的组合：

$$\mathcal{F}(\boldsymbol{x}) = \sum_{i=1}^{n} g_i(\boldsymbol{x}) \cdot f_i(\boldsymbol{x})$$

其中：
- $f_i(\cdot)$：第$i$个专家函数（Expert）
- $g_i(\cdot)$：第$i$个门控函数（Gating），满足$g_i(\boldsymbol{x}) \geq 0$

</div>

<div class="theorem-box">

### 公理2：稀疏性原则

**Sparse Activation Hypothesis**：对于给定输入$\boldsymbol{x}$，只需激活少数几个专家即可获得接近全激活的性能：

$$\left\|\sum_{i=1}^{n} g_i(\boldsymbol{x}) f_i(\boldsymbol{x}) - \sum_{i \in \mathcal{S}_k} g_i(\boldsymbol{x}) f_i(\boldsymbol{x})\right\| \leq \epsilon$$

其中$|\mathcal{S}_k| = k \ll n$，$\epsilon$是容忍误差。

</div>

<div class="theorem-box">

### 公理3：专家特化假设

不同专家在训练过程中会自然地对不同类型的输入产生特化（Specialization）：

$$\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}_i}[g_i(\boldsymbol{x})] > \mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}_j}[g_i(\boldsymbol{x})], \quad i \neq j$$

其中$\mathcal{D}_i$是第$i$个专家擅长处理的数据分布。

</div>

<h4 id="13">1.3 设计哲学</h4>
<p>MoE的核心设计哲学体现为<strong>"分而治之"</strong>与<strong>"按需计算"</strong>的结合：</p>
<p><strong>分而治之（Divide and Conquer）</strong>：
- 将复杂的函数逼近任务分解为$n$个子任务
- 每个Expert专注于处理特定类型的输入
- 类比：如同一个公司的不同部门，各司其职</p>
<p><strong>按需计算（Conditional Computation）</strong>：
- 根据输入特征动态选择激活哪些Expert
- 避免全部计算，实现<strong>计算量与参数量的解耦</strong>
- 类比：如同按需咨询专家，而非召集所有人开会</p>
<p><strong>与Dense模型的本质区别</strong>：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Dense FFN</th>
<th>MoE</th>
</tr>
</thead>
<tbody>
<tr>
<td>参数量</td>
<td>$2dD$</td>
<td>$2dD \cdot n$（$n$倍）</td>
</tr>
<tr>
<td>计算量（FLOPs）</td>
<td>$2dD$</td>
<td>$2dD \cdot k$（$k$倍）</td>
</tr>
<tr>
<td>激活路径</td>
<td>固定</td>
<td>动态（输入相关）</td>
</tr>
<tr>
<td>表达能力</td>
<td>受限于参数量</td>
<td>理论上可无限扩展</td>
</tr>
</tbody>
</table>
<p><strong>核心思想</strong>：
- Dense模型：<strong>参数量 = 计算量</strong>
- MoE模型：<strong>参数量 ≫ 计算量</strong>（当$n \gg k$时）</p>
<hr />
<h3 id="2">第2部分：严谨的核心数学推导</h3>
<h4 id="21-ffnmoe">2.1 从FFN到MoE的完整推导</h4>
<div class="derivation-box">

### 推导目标：证明FFN可以分解为多个Expert的加权和

**步骤1：标准FFN定义**

给定输入$\boldsymbol{x} \in \mathbb{R}^{d}$（行向量），标准的Feed-Forward Network定义为：

$$\boldsymbol{y} = f(\boldsymbol{x}\boldsymbol{W}^{(A)})\boldsymbol{W}^{(B)}$$

其中：
- $\boldsymbol{W}^{(A)} \in \mathbb{R}^{d \times D}$：第一层权重矩阵
- $\boldsymbol{W}^{(B)} \in \mathbb{R}^{D \times d}$：第二层权重矩阵
- $f(\cdot)$：逐元素激活函数（如ReLU、GELU）
- $D$：中间维度（通常$D = 4d$）

**步骤2：矩阵分块**

选择整数$n$使得$n | D$（$n$整除$D$），定义$c = D/n$为每块大小。将权重矩阵按列/行分块：

$$\boldsymbol{W}^{(A)} = \begin{bmatrix} \boldsymbol{W}^{(A)}_1 & \boldsymbol{W}^{(A)}_2 & \cdots & \boldsymbol{W}^{(A)}_n \end{bmatrix}$$

$$\boldsymbol{W}^{(B)} = \begin{bmatrix} \boldsymbol{W}^{(B)}_1 \\ \boldsymbol{W}^{(B)}_2 \\ \vdots \\ \boldsymbol{W}^{(B)}_n \end{bmatrix}$$

其中：
- $\boldsymbol{W}^{(A)}_i \in \mathbb{R}^{d \times c}$：第$i$块第一层权重
- $\boldsymbol{W}^{(B)}_i \in \mathbb{R}^{c \times d}$：第$i$块第二层权重

**步骤3：分块矩阵乘法**

$$\boldsymbol{x}\boldsymbol{W}^{(A)} = \boldsymbol{x} \begin{bmatrix} \boldsymbol{W}^{(A)}_1 & \boldsymbol{W}^{(A)}_2 & \cdots & \boldsymbol{W}^{(A)}_n \end{bmatrix} = \begin{bmatrix} \boldsymbol{x}\boldsymbol{W}^{(A)}_1 & \boldsymbol{x}\boldsymbol{W}^{(A)}_2 & \cdots & \boldsymbol{x}\boldsymbol{W}^{(A)}_n \end{bmatrix}$$

这是一个$1 \times D$的行向量，分为$n$块，每块大小为$c$。

**步骤4：激活函数作用**

由于$f(\cdot)$是逐元素激活，对每块独立作用：

$$f(\boldsymbol{x}\boldsymbol{W}^{(A)}) = \begin{bmatrix} f(\boldsymbol{x}\boldsymbol{W}^{(A)}_1) & f(\boldsymbol{x}\boldsymbol{W}^{(A)}_2) & \cdots & f(\boldsymbol{x}\boldsymbol{W}^{(A)}_n) \end{bmatrix}$$

**步骤5：第二层矩阵乘法**

$$\boldsymbol{y} = f(\boldsymbol{x}\boldsymbol{W}^{(A)})\boldsymbol{W}^{(B)} = \begin{bmatrix} f(\boldsymbol{x}\boldsymbol{W}^{(A)}_1) & \cdots & f(\boldsymbol{x}\boldsymbol{W}^{(A)}_n) \end{bmatrix} \begin{bmatrix} \boldsymbol{W}^{(B)}_1 \\ \vdots \\ \boldsymbol{W}^{(B)}_n \end{bmatrix}$$

利用分块矩阵乘法公式：

$$\boldsymbol{y} = \sum_{i=1}^{n} f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i) \boldsymbol{W}^{(B)}_i$$

**步骤6：定义Expert向量**

定义第$i$个Expert的输出向量为：

$$\boldsymbol{v}_i \triangleq f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i) \boldsymbol{W}^{(B)}_i \in \mathbb{R}^{d}$$

**结论**：

$$\boxed{\boldsymbol{y} = \sum_{i=1}^{n} \boldsymbol{v}_i}$$

FFN的输出等价于$n$个Expert向量的和！

</div>

<h4 id="22">2.2 最优子集选择问题</h4>
<div class="derivation-box">

### 推导目标：找到最优的$k$个Expert来逼近全部$n$个Expert

**问题定义**：

给定$n$个向量$\{\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_n\}$，找到$k$个向量的子集$\mathcal{S}_k \subset \{1,2,\ldots,n\}$使得：

$$\min_{\mathcal{S}_k: |\mathcal{S}_k| = k} \left\| \sum_{i \in \mathcal{S}_k} \boldsymbol{v}_i - \sum_{i=1}^{n} \boldsymbol{v}_i \right\|^2$$

**步骤1：引入0-1变量**

定义指示变量$\lambda_i \in \{0, 1\}$：

$$\lambda_i = \begin{cases} 1, & \text{如果Expert } i \text{ 被选中} \\ 0, & \text{否则} \end{cases}$$

则优化问题改写为：

$$\min_{\lambda_1, \ldots, \lambda_n \in \{0,1\}} \left\| \sum_{i=1}^{n} \lambda_i \boldsymbol{v}_i - \sum_{i=1}^{n} \boldsymbol{v}_i \right\|^2 \quad \text{s.t.} \quad \sum_{i=1}^{n} \lambda_i = k$$

**步骤2：变量替换**

令$\gamma_i = 1 - \lambda_i$（即选择"不要"的Expert），则$\sum_{i=1}^{n} \gamma_i = n - k$，问题等价于：

$$\min_{\gamma_1, \ldots, \gamma_n \in \{0,1\}} \left\| \sum_{i=1}^{n} \gamma_i \boldsymbol{v}_i \right\|^2 \quad \text{s.t.} \quad \sum_{i=1}^{n} \gamma_i = n - k$$

**解释**：最小化"被丢弃的Expert"的总贡献。

**步骤3：展开平方项**

$$\left\| \sum_{i=1}^{n} \gamma_i \boldsymbol{v}_i \right\|^2 = \left( \sum_{i=1}^{n} \gamma_i \boldsymbol{v}_i \right)^T \left( \sum_{j=1}^{n} \gamma_j \boldsymbol{v}_j \right) = \sum_{i=1}^{n} \sum_{j=1}^{n} \gamma_i \gamma_j \boldsymbol{v}_i^T \boldsymbol{v}_j$$

这是一个关于$\{\gamma_i\}$的二次型。

**步骤4：正交假设下的简化**

**假设**：$\boldsymbol{v}_i$两两正交，即$\boldsymbol{v}_i^T \boldsymbol{v}_j = 0, \forall i \neq j$

则交叉项消失：

$$\left\| \sum_{i=1}^{n} \gamma_i \boldsymbol{v}_i \right\|^2 = \sum_{i=1}^{n} \gamma_i^2 \|\boldsymbol{v}_i\|^2$$

由于$\gamma_i \in \{0, 1\}$，有$\gamma_i^2 = \gamma_i$，故：

$$\left\| \sum_{i=1}^{n} \gamma_i \boldsymbol{v}_i \right\|^2 = \sum_{i=1}^{n} \gamma_i \|\boldsymbol{v}_i\|^2$$

**步骤5：求解最优策略**

目标是最小化$\sum_{i=1}^{n} \gamma_i \|\boldsymbol{v}_i\|^2$，约束是$\sum_{i=1}^{n} \gamma_i = n - k$。

显然，应该选择$\|\boldsymbol{v}_i\|^2$最小的$n-k$个指标令$\gamma_i = 1$。

等价地，**选择$\|\boldsymbol{v}_i\|$最大的$k$个Expert**。

**结论**：

$$\boxed{\mathcal{S}_k^* = \text{argtop}_k \left\{ \|\boldsymbol{v}_1\|, \|\boldsymbol{v}_2\|, \ldots, \|\boldsymbol{v}_n\| \right\}}$$

</div>

<h4 id="23">2.3 非正交情况下的近似分析</h4>
<div class="formula-explanation">

<div class="formula-step">
<div class="step-label">实际情况：Expert向量并非两两正交</div>

在真实神经网络中，$\boldsymbol{v}_i$之间存在相关性。定义相关矩阵：

$$\boldsymbol{C} = [\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_n]^T [\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_n] \in \mathbb{R}^{n \times n}$$

其中$C_{ij} = \boldsymbol{v}_i^T \boldsymbol{v}_j$。

<div class="step-explanation">
如果$\boldsymbol{C}$接近对角矩阵（即非对角元素较小），则"按模长排序"仍是良好近似。
</div>
</div>

<div class="formula-step">
<div class="step-label">误差界分析</div>

定义近似误差为：

$$E_{\text{approx}} = \left\| \sum_{i=1}^{n} \boldsymbol{v}_i - \sum_{i \in \mathcal{S}_k} \boldsymbol{v}_i \right\|^2$$

可以证明（详见[Compressed Routing, 2024]）：

$$E_{\text{approx}} \leq \sum_{i \notin \mathcal{S}_k} \|\boldsymbol{v}_i\|^2 + 2 \sum_{i \in \mathcal{S}_k} \sum_{j \notin \mathcal{S}_k} |\boldsymbol{v}_i^T \boldsymbol{v}_j|$$

<div class="step-explanation">
第一项：丢弃的Expert的能量；第二项：保留与丢弃的Expert之间的相关性。
</div>
</div>

</div>

<h4 id="24-router">2.4 Router机制的数学建模</h4>
<div class="derivation-box">

### 推导目标：设计可微分的Router来预测Expert重要性

**问题**：直接计算$\|\boldsymbol{v}_i\|$需要先计算$\boldsymbol{v}_i$，这违背了省计算量的初衷。

**解决方案**：将Expert分解为"模长"和"方向"两部分。

**步骤1：归一化Expert**

定义归一化的Expert方向：

$$\boldsymbol{e}_i = \frac{\boldsymbol{v}_i}{\|\boldsymbol{v}_i\|} \in \mathbb{R}^{d}, \quad \|\boldsymbol{e}_i\| = 1$$

则原始Expert可以写成：

$$\boldsymbol{v}_i = \|\boldsymbol{v}_i\| \cdot \boldsymbol{e}_i$$

**步骤2：设计Router预测模长**

引入一个轻量级模型（Router）来预测每个Expert的模长：

$$\boldsymbol{\rho} = [\rho_1, \rho_2, \ldots, \rho_n] = h(\boldsymbol{x} \boldsymbol{W}^{(R)})$$

其中：
- $\boldsymbol{W}^{(R)} \in \mathbb{R}^{d \times n}$：Router权重矩阵
- $h: \mathbb{R} \to \mathbb{R}_{\geq 0}$：激活函数（如Softplus、ReLU、Sigmoid）

**计算量分析**：
- Router计算：$O(dn)$（通常$n \ll D$）
- Expert计算：$O(kdc)$（只计算Top-$k$）
- Dense FFN计算：$O(dD) = O(dnc)$

当$k \ll n$时，总计算量显著降低。

**步骤3：重构Expert输出**

$$\boldsymbol{v}_i \approx \rho_i \cdot \boldsymbol{e}_i$$

其中$\rho_i$是Router预测的模长，$\boldsymbol{e}_i$是归一化后的Expert方向。

**步骤4：MoE最终公式**

$$\boxed{\boldsymbol{y} = \sum_{i \in \mathcal{S}_k} \rho_i \boldsymbol{e}_i, \quad \mathcal{S}_k = \text{argtop}_k(\boldsymbol{\rho})}$$

</div>

<div class="step-by-step">

<div class="step">
**关键洞察1**：Router只需要$O(dn)$的计算就能预测所有Expert的重要性。
</div>

<div class="step">
**关键洞察2**：只有被选中的Top-$k$个Expert才需要计算$\boldsymbol{e}_i$，节省了$(n-k)/n$的计算量。
</div>

<div class="step">
**关键洞察3**：归一化（如RMSNorm）确保所有$\boldsymbol{e}_i$的模长相同，使得$\rho_i$真正反映重要性。
</div>

</div>

<h4 id="25-top-k">2.5 Top-k选择的梯度传播</h4>
<div class="derivation-box">

### 推导目标：分析Top-k操作的梯度

**问题**：Top-k是离散操作，不可微，如何反向传播？

**步骤1：定义Top-k掩码**

$$m_i = \mathbb{1}\{\rho_i \in \text{Top-}k(\boldsymbol{\rho})\} \in \{0, 1\}$$

则MoE输出为：

$$\boldsymbol{y} = \sum_{i=1}^{n} m_i \rho_i \boldsymbol{e}_i$$

**步骤2：Straight-Through Estimator（STE）**

前向传播：使用离散的$m_i$

$$\boldsymbol{y} = \sum_{i=1}^{n} m_i \rho_i \boldsymbol{e}_i$$

反向传播：**假装$m_i$是连续的**，计算梯度时忽略$m_i$对$\rho_i$的依赖：

$$\frac{\partial \mathcal{L}}{\partial \rho_i} = m_i \frac{\partial \mathcal{L}}{\partial \boldsymbol{y}} \cdot \boldsymbol{e}_i$$

**步骤3：更精细的梯度估计**

一些工作（如《Sparse Backpropagation for MoE》）提出：

$$\frac{\partial \mathcal{L}}{\partial \rho_i} = m_i \frac{\partial \mathcal{L}}{\partial \boldsymbol{y}} \cdot \boldsymbol{e}_i + \lambda \frac{\partial \mathcal{L}_{\text{balance}}}{\partial \rho_i}$$

其中$\mathcal{L}_{\text{balance}}$是负载均衡损失（后文详述）。

</div>

<h4 id="26">2.6 不同激活函数的对比</h4>
<div class="derivation-box">

### Router激活函数$h(\cdot)$的选择

**选项1：Softmax（归一化）**

$$\rho_i = \frac{\exp(z_i)}{\sum_{j=1}^{n} \exp(z_j)}, \quad z_i = (\boldsymbol{x} \boldsymbol{W}^{(R)})_i$$

- **优点**：$\sum_{i=1}^{n} \rho_i = 1$，概率解释
- **缺点**：Expert之间存在竞争（一个变大，其他必然变小），可能导致负载不均

**选项2：Sigmoid（独立激活）**

$$\rho_i = \sigma(z_i) = \frac{1}{1 + \exp(-z_i)}$$

- **优点**：每个Expert独立评分，减少竞争
- **缺点**：$\rho_i$范围有限$[0, 1]$，表达能力受限

**选项3：Softplus（非负无界）**

$$\rho_i = \log(1 + \exp(z_i))$$

- **优点**：非负、无上界、平滑可微
- **缺点**：可能导致数值不稳定（某些$\rho_i$过大）

**选项4：ReLU（简单有效）**

$$\rho_i = \max(0, z_i)$$

- **优点**：计算简单，稀疏性强
- **缺点**：不可微于0点

</div>

<h4 id="27">2.7 计算复杂度的精确分析</h4>
<div class="formula-explanation">

<div class="formula-step">
<div class="step-label">Dense FFN的FLOPs</div>

$$\text{FLOPs}_{\text{Dense}} = 2dD + 2Dd = 4dD$$

其中：
- $\boldsymbol{x} \boldsymbol{W}^{(A)}$：$dD$次乘法 + $(d-1)D$次加法 ≈ $2dD$
- $f(\boldsymbol{x}\boldsymbol{W}^{(A)}) \boldsymbol{W}^{(B)}$：$2dD$

<div class="step-explanation">
假设$D = 4d$（常见设置），则FLOPs = $16d^2$。
</div>
</div>

<div class="formula-step">
<div class="step-label">MoE的FLOPs（$n$个Expert，Top-$k$激活）</div>

$$\text{FLOPs}_{\text{MoE}} = 2dn + 2k \cdot dc + 2k \cdot cd$$

其中$c = D/n$，分解为：
- **Router**：$\boldsymbol{x} \boldsymbol{W}^{(R)}$ → $2dn$
- **Top-k Expert（第一层）**：$k$个$\boldsymbol{x} \boldsymbol{W}^{(A)}_i$ → $2kdc$
- **Top-k Expert（第二层）**：$k$个$f(\boldsymbol{x}\boldsymbol{W}^{(A)}_i) \boldsymbol{W}^{(B)}_i$ → $2kcd$

总计：

$$\text{FLOPs}_{\text{MoE}} = 2dn + 4kdc = 2dn + 4kdD/n$$

<div class="step-explanation">
假设$n = 8, k = 2, D = 4d$，则FLOPs = $16d + 4d^2$，相比Dense的$16d^2$显著降低。
</div>
</div>

<div class="formula-step">
<div class="step-label">加速比</div>

$$\text{Speedup} = \frac{\text{FLOPs}_{\text{Dense}}}{\text{FLOPs}_{\text{MoE}}} = \frac{4dD}{2dn + 4kdD/n} \approx \frac{n}{k} \quad (\text{当} D \gg n \text{时})$$

<div class="step-explanation">
例如$n=8, k=2$时，理论加速比约为4倍！
</div>
</div>

</div>

<hr />
<h3 id="3">第3部分：数学直觉、多角度解释与类比</h3>
<h4 id="31">3.1 生活化类比</h4>
<div class="intuition-box">

### 🧠 直觉理解1：专家咨询团队

**场景**：你是一家公司的CEO，面临复杂决策。

**Dense模型（传统FFN）**：
- 每次决策都要召集公司**所有部门**开会（财务、技术、市场、法务...）
- 每个部门都要发言，最后综合所有意见
- **缺点**：效率低下，很多时候大部分部门的意见不重要

**MoE模型**：
- 你有一个"智能助手"（Router）快速判断这个问题属于什么类型
- 只召集**最相关的2-3个部门**（Top-k）来讨论
- **优点**：速度快，决策质量不降低

**例子**：
- 财务问题 → 只找财务部 + 法务部
- 技术问题 → 只找技术部 + 产品部
- 市场问题 → 只找市场部 + 销售部

**关键**：Router（助手）必须准确识别问题类型，否则可能找错专家！

</div>

<div class="intuition-box">

### 🧠 直觉理解2：向量求和的几何图景

**场景**：在3维空间中，要用$k$个向量逼近$n$个向量的和。

假设有5个向量：
- $\boldsymbol{v}_1$：长度10，指向东
- $\boldsymbol{v}_2$：长度8，指向北
- $\boldsymbol{v}_3$：长度2，指向西
- $\boldsymbol{v}_4$：长度1，指向南
- $\boldsymbol{v}_5$：长度0.5，指向上

**问题**：只选2个向量（$k=2$）来逼近5个向量的和？

**策略1（按模长）**：选$\boldsymbol{v}_1 + \boldsymbol{v}_2$
- 误差 = 丢弃的$\boldsymbol{v}_3 + \boldsymbol{v}_4 + \boldsymbol{v}_5$
- 丢弃的总长度 ≈ 3.5

**策略2（随机选）**：假设选$\boldsymbol{v}_3 + \boldsymbol{v}_4$
- 误差 = 丢弃的$\boldsymbol{v}_1 + \boldsymbol{v}_2 + \boldsymbol{v}_5$
- 丢弃的总长度 ≈ 18.5

**结论**：选长向量是更好的策略！

</div>

<div class="intuition-box">

### 🧠 直觉理解3：信息压缩类比

**MoE ≈ 有损压缩**

- **原始数据**：$n$个Expert的输出（$n$张高清照片）
- **压缩目标**：只保留$k$个Expert（只保留$k$张照片）
- **压缩策略**：保留"信息量最大"的那些（最清晰、最关键的照片）

**Router的作用**：
- 类似"图像重要性评分器"
- 快速扫描所有照片，给每张打分
- 保留分数最高的$k$张

**归一化的作用**：
- 确保所有照片都缩放到同样大小
- 这样"评分"才能真正反映重要性，而非受大小影响

</div>

<h4 id="32">3.2 几何意义</h4>
<p><strong>几何视角1：子空间分解</strong></p>
<div class="intuition-box">

将$d$维输出空间$\mathbb{R}^d$分解为$n$个子空间的并：

$$\mathbb{R}^d = \text{span}(\boldsymbol{v}_1) \oplus \text{span}(\boldsymbol{v}_2) \oplus \cdots \oplus \text{span}(\boldsymbol{v}_n)$$

- **Dense FFN**：使用全部$n$个子空间
- **MoE**：动态选择其中$k$个子空间

**可视化**（2维情况）：
- 假设$d=2, n=4$，每个Expert对应一个方向
- Dense：输出是4个方向向量的和（菱形）
- MoE (k=2)：输出是2个方向向量的和（部分菱形的顶点）

</div>

<p><strong>几何视角2：流形假设</strong></p>
<div class="intuition-box">

**假设**：不同类型的输入对应数据流形上的不同区域。

- **Expert特化**：每个Expert学习处理流形上的某个局部区域
- **Router选择**：根据输入在流形上的位置，选择对应区域的Expert
- **稀疏激活**：大多数区域只需少数几个Expert覆盖

**类比**：
- 数据流形 = 地球表面
- Expert = 不同地区的专家
- 输入 = 某个位置的任务
- Router = GPS定位系统

</div>

<h4 id="33">3.3 多角度理解</h4>
<p><strong>📊 概率论视角</strong></p>
<div class="intuition-box">

MoE可以看作**混合模型（Mixture Model）**：

$$p(\boldsymbol{y}|\boldsymbol{x}) = \sum_{i=1}^{n} \underbrace{p(i|\boldsymbol{x})}_{\text{Router}} \cdot \underbrace{p(\boldsymbol{y}|\boldsymbol{x}, i)}_{\text{Expert}}$$

- $p(i|\boldsymbol{x})$：Router预测选择第$i$个Expert的概率
- $p(\boldsymbol{y}|\boldsymbol{x}, i)$：第$i$个Expert的输出分布

**Top-k近似**：只保留概率最大的$k$项（Importance Sampling）

</div>

<p><strong>📡 信息论视角</strong></p>
<div class="intuition-box">

**信息分解**：

$$I(\boldsymbol{y}; \boldsymbol{x}) = \sum_{i=1}^{n} I(\boldsymbol{y}; \boldsymbol{x} | \text{Expert}_i) \cdot p(\text{Expert}_i | \boldsymbol{x})$$

- **Router的目标**：最大化Top-$k$ Expert的互信息贡献
- **稀疏性**：只保留高互信息的Expert

**直觉**：丢弃那些对输出贡献信息量少的Expert。

</div>

<p><strong>🎯 优化视角</strong></p>
<div class="intuition-box">

MoE可以看作**受限优化问题**：

$$\max_{\boldsymbol{y}} \mathcal{L}(\boldsymbol{y}, \boldsymbol{x}) \quad \text{s.t.} \quad \|\boldsymbol{y}\|_0 \leq k$$

其中$\|\cdot\|_0$是"激活的Expert数量"。

**等价形式**（Lagrange松弛）：

$$\max_{\boldsymbol{y}} \mathcal{L}(\boldsymbol{y}, \boldsymbol{x}) - \lambda \|\boldsymbol{y}\|_0$$

这类似于**L0正则化**，鼓励稀疏激活。

</div>

<p><strong>🔄 动态计算图视角</strong></p>
<div class="intuition-box">

- **Static Graph（Dense）**：每次前向传播的计算图固定
- **Dynamic Graph（MoE）**：根据输入动态生成计算图

**类比编程**：
- Dense = 顺序执行所有代码
- MoE = if-else条件分支（只执行相关分支）

</div>

<hr />
<h3 id="4">第4部分：方法论变体、批判性比较与优化</h3>
<h4 id="41-moe">4.1 主流MoE变体对比表</h4>
<table>
<thead>
<tr>
<th>方法</th>
<th>核心思想</th>
<th>优点</th>
<th><strong>缺点</strong></th>
<th><strong>优化方向</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>原始MoE (Shazeer 2017)</strong></td>
<td>Softmax Router + Top-k</td>
<td>✅ 首次在大规模验证<br>✅ 理论简洁</td>
<td>❌ <strong>负载不均衡严重</strong><br>❌ 训练不稳定<br>❌ 通信开销大</td>
<td>✅ 负载均衡损失<br>✅ Expert Capacity限制<br>✅ 噪声注入</td>
</tr>
<tr>
<td><strong>Switch Transformer (2021)</strong></td>
<td>Top-1路由（$k=1$）</td>
<td>✅ 计算最简单<br>✅ 参数效率高</td>
<td>❌ <strong>单Expert容易过拟合</strong><br>❌ 表达能力受限<br>❌ 对噪声敏感</td>
<td>✅ 增大Expert数量$n$<br>✅ Expert Dropout<br>✅ 精度混合训练</td>
</tr>
<tr>
<td><strong>Mixtral (2024)</strong></td>
<td>Top-2 + 归一化Router</td>
<td>✅ 开源易用<br>✅ 训练稳定</td>
<td>❌ <strong>仍需负载均衡调优</strong><br>❌ 推理时内存占用大<br>❌ Expert利用率不均</td>
<td>✅ 动态Expert容量<br>✅ 自适应路由<br>✅ 知识蒸馏压缩</td>
</tr>
<tr>
<td><strong>本文方法（归一化Expert）</strong></td>
<td>$\rho_i \boldsymbol{e}_i$分解 + 非归一化Router</td>
<td>✅ 几何意义明确<br>✅ 减少Expert竞争<br>✅ 理论保证更强</td>
<td>❌ <strong>归一化增加计算</strong><br>❌ 实现复杂度高<br>❌ 缺乏大规模验证</td>
<td>✅ RMSNorm优化<br>✅ 融合算子加速<br>✅ 更多实证研究</td>
</tr>
</tbody>
</table>
<h4 id="42-moe-">4.2 原始MoE - 批判性分析</h4>
<div class="analysis-box">

### **核心缺陷**

**缺陷1：负载不均衡（Load Imbalance）**

**问题描述**：
- 某些Expert被频繁选择（"热门Expert"），另一些几乎不被使用（"冷门Expert"）
- 极端情况：只有1-2个Expert承担大部分计算

**根本原因**：
1. **Rich-get-richer效应**：一旦某Expert在初始阶段表现好，Router会更频繁选择它
2. **梯度稀疏性**：未被选中的Expert没有梯度，无法学习进步
3. **Softmax的竞争性**：$\sum \rho_i = 1$导致Expert之间零和博弈

**定量影响**：
- 文献报告：Top-10% Expert承担50%-80%的tokens
- 计算资源浪费：大量Expert参数实际未被利用
- 训练不稳定：某些Expert的BatchNorm统计量不准确（样本太少）

**示例数据**（Shazeer 2017论文）：
| Expert ID | 负载比例 | 是否有效利用 |
|-----------|---------|-------------|
| Expert 0 | 35.2% | ✅ |
| Expert 1 | 28.7% | ✅ |
| Expert 2 | 15.1% | ⚠️ |
| Expert 3-7 | 21.0% (总计) | ❌ |

---

**缺陷2：Expert容量问题（Capacity Overflow）**

**问题描述**：
- 在分布式训练中，每个Expert有固定容量（可处理的token数）
- 当某Expert被分配过多tokens时，超出部分被丢弃（dropped tokens）
- 这导致信息损失和训练不稳定

**根本原因**：
- 批次内的负载分布是随机的（某批次可能大量样本都选同一个Expert）
- 固定容量无法适应动态负载

**定量影响**：
- Switch Transformer报告：5%-15% tokens被丢弃（容量因子=1.25时）
- 丢弃的tokens没有梯度，导致训练损失不连续

**数学表达**：

设Expert $i$的容量为$C_i$，实际分配的token数为$N_i$，则丢弃率为：

$$\text{Drop Rate}_i = \max\left(0, \frac{N_i - C_i}{N_i}\right)$$

---

**缺陷3：通信开销大（Communication Overhead）**

**问题描述**：
- 在分布式训练（如Megatron）中，不同Expert可能在不同GPU上
- 需要All-to-All通信将tokens路由到对应Expert
- 通信延迟成为瓶颈

**根本原因**：
- Expert数量$n$通常等于GPU数量（如8或16），需要跨设备通信
- 通信量 ∝ $k \times \text{batch_size} \times d$

**定量影响**：
- 文献报告：通信时间占总训练时间的20%-40%
- 在网络带宽受限环境下，加速比从理论$n/k$降至1.5-2倍

---

### **优化方向**

**优化1：负载均衡损失（Load Balancing Loss）**

**策略**：在损失函数中加入正则项，惩罚负载不均衡。

**公式**（辅助损失）：

$$\mathcal{L}_{\text{balance}} = \alpha \cdot n \sum_{i=1}^{n} f_i \cdot P_i$$

其中：
- $f_i = \frac{1}{T} \sum_{t=1}^{T} \mathbb{1}\{i \in \text{Top-}k(\boldsymbol{\rho}_t)\}$：Expert $i$被选中的频率
- $P_i = \frac{1}{T} \sum_{t=1}^{T} \rho_i^{(t)}$：Expert $i$的平均Router得分
- $T$：批次内token总数
- $\alpha$：平衡系数（通常0.01-0.1）

**直觉**：
- 当所有Expert均匀负载时，$f_i = k/n, P_i = 1/n$
- 此时$\mathcal{L}_{\text{balance}} = n \cdot (k/n) \cdot (1/n) \cdot n = k$
- 任何不均衡都会增大损失

**效果**（Switch Transformer论文）：
- 标准差从45%降至12%（负载分布）
- 训练稳定性显著提升

---

**优化2：Expert Capacity与动态调整**

**策略1：设置容量上限**

$$C_i = \text{capacity_factor} \times \frac{T \cdot k}{n}$$

- $\text{capacity_factor} = 1.25$：允许25%的buffer
- 超出部分丢弃或分配给次优Expert

**策略2：动态容量（DeepSeek-V3）**

根据实时负载动态调整：

$$C_i^{(t+1)} = C_i^{(t)} + \eta (N_i^{(t)} - C_i^{(t)})$$

其中$\eta = 0.1$是更新率。

**效果**：
- Token丢弃率从15%降至<3%
- 训练吞吐提升10%-15%

---

**优化3：专家并行（Expert Parallelism）+ 通信优化**

**策略1：局部Expert**
- 每个GPU维护部分Expert，减少通信
- 使用层次化路由（先选GPU，再选Expert）

**策略2：异步通信**
- 与计算overlap
- 使用NCCL优化All-to-All

**策略3：Expert缓存**
- 缓存最常用的Expert在每个GPU本地

**效果**（Mixtral论文）：
- 通信时间从40%降至15%
- 整体加速比从2.5x提升至3.8x

---

**优化4：非归一化Router（本文方法）**

**策略**：使用Sigmoid/Softplus替代Softmax

$$\rho_i = \text{Sigmoid}(z_i) \quad \text{或} \quad \rho_i = \text{Softplus}(z_i)$$

**优点**：
- Expert之间不再竞争（一个变大不影响其他）
- 缓解负载不均衡
- 结合归一化Expert，理论保证更强

**缺点**：
- 需要额外的负载均衡约束
- 可能导致所有$\rho_i$都很大或很小（需要适当正则化）

**实验效果**（初步）：
- 负载标准差降低15%-20%
- 训练损失曲线更平滑

</div>

<h4 id="43-switch-transformer-">4.3 Switch Transformer - 批判性分析</h4>
<div class="analysis-box">

### **核心缺陷**

**缺陷1：单Expert容易过拟合**

**问题**：Top-1意味着每个token只由一个Expert处理，容易对特定模式过拟合。

**根本原因**：缺乏集成效应（Ensemble Effect）

**定量影响**：
- 泛化能力下降5%-10%（vs Top-2）
- 对分布外数据敏感

**优化方向**：
- 增大Expert数量（$n$从8增至128）
- Expert Dropout（训练时随机丢弃）

---

**缺陷2：表达能力受限**

**问题**：$k=1$限制了模型在复杂任务上的表达。

**优化方向**：
- 混合Top-1和Top-2（如某些层用Top-2）
- 自适应$k$（简单样本用Top-1，复杂样本用Top-2）

</div>

<h4 id="44-expert-">4.4 归一化Expert方法 - 批判性分析</h4>
<div class="analysis-box">

### **核心缺陷**

**缺陷1：归一化增加计算开销**

**问题**：每个Expert都需要RMSNorm或LayerNorm

**根本原因**：额外的归一化操作

**定量影响**：
- 增加5%-10%的计算时间（取决于实现）

**优化方向**：
- 融合算子（fuse normalization into GEMM）
- 只在推理时归一化，训练时近似

---

**缺陷2：实现复杂度高**

**问题**：需要修改标准MoE实现

**优化方向**：
- 开源参考实现
- 集成到主流框架（如DeepSpeed, Megatron）

---

**缺陷3：缺乏大规模验证**

**问题**：目前只有理论推导，缺乏10B+规模的实证

**优化方向**：
- 在多个任务上验证（语言模型、多模态）
- 与Mixtral等对比实验

</div>

<hr />
<h3 id="5">第5部分：学习路线图与未来展望</h3>
<h4 id="51">5.1 学习路线图</h4>
<p><strong>必备前置知识</strong></p>
<p><strong>数学基础</strong>：
- 线性代数：矩阵分块、范数、正交性
- 概率论：条件概率、混合模型
- 优化理论：约束优化、Lagrange乘子</p>
<p><strong>机器学习基础</strong>：
- 深度学习：反向传播、梯度下降
- Transformer架构：Attention、FFN、Layer Norm
- 分布式训练：数据并行、模型并行</p>
<p><strong>推荐学习顺序</strong>：</p>
<ol>
<li><strong>理解Dense FFN</strong>（本文"问题定义"部分）</li>
<li><strong>向量逼近问题</strong>（线性代数视角）</li>
<li><strong>Router设计</strong>（条件计算）</li>
<li><strong>负载均衡</strong>（优化问题）</li>
<li><strong>分布式实现</strong>（工程实践）</li>
</ol>
<hr />
<p><strong>核心论文列表（按时间顺序）</strong></p>
<p><strong>理论奠基</strong>：
1. Jacobs et al. (1991) - "Adaptive Mixtures of Local Experts"
2. Jordan &amp; Jacobs (1994) - "Hierarchical Mixtures of Experts"</p>
<p><strong>深度学习时代</strong>：
3. Bengio et al. (2013) - "Estimating or Propagating Gradients Through Stochastic Neurons" (条件计算)
4. Shazeer et al. (2017) - "Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer" ⭐</p>
<p><strong>Transformer集成</strong>：
5. Fedus et al. (2021) - "Switch Transformers" ⭐
6. Lepikhin et al. (2021) - "GShard: Scaling Giant Models with Conditional Computation"</p>
<p><strong>最新进展</strong>：
7. Jiang et al. (2024) - "Mixtral of Experts" ⭐
8. Liu et al. (2024) - "DeepSeek-V3" ⭐
9. Dai et al. (2024) - "Compressed Routing for Scalable MoE"</p>
<hr />
<h4 id="52">5.2 研究空白与未来方向</h4>
<h4 id="1-moe"><strong>方向1：理论层面 - MoE的泛化理论</strong></h4>
<p><strong>研究空白</strong>：
- 当前MoE缺乏严格的泛化界（Generalization Bound）
- Top-k近似误差如何影响最终模型性能？
- 稀疏激活与Dense模型的表达能力差距有多大？</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li><strong>问题</strong>：MoE的样本复杂度是多少？</li>
<li><strong>挑战</strong>：Router引入额外的学习任务，可能需要更多数据</li>
<li><strong>潜在方法</strong>：<ul>
<li>建立PAC学习框架分析MoE</li>
<li>推导Router准确率与下游任务性能的关系</li>
<li>分析Expert数量$n$、激活数$k$与泛化的trade-off</li>
</ul>
</li>
<li>
<p><strong>潜在意义</strong>：指导模型设计（何时用MoE，何时用Dense）</p>
</li>
<li>
<p><strong>问题</strong>：稀疏激活引入的偏差（Bias）如何量化？</p>
</li>
<li><strong>已知</strong>：Top-k是对全激活的有偏估计</li>
<li><strong>未知</strong>：偏差的上界、方差、与$k$的关系</li>
<li>
<p><strong>潜在意义</strong>：设计无偏或低偏差的采样策略</p>
</li>
<li>
<p><strong>问题</strong>：MoE在不同任务上的适用性？</p>
</li>
<li><strong>现状</strong>：语言模型效果好，视觉任务不明显</li>
<li><strong>探索方向</strong>：分析任务特性（如数据多样性、输入分布）与MoE收益的关系</li>
</ol>
<p><strong>优化方向</strong>：
- 借鉴统计学习理论（VC维、Rademacher复杂度）
- 分析Router的决策边界
- 研究Expert特化与数据流形结构的关系</p>
<p><strong>量化目标</strong>：
- 推导形如$\mathcal{O}(\sqrt{(nk \log n) / m})$的泛化界（$m$为样本数）
- 证明在何种条件下MoE与Dense等价
- 建立偏差-方差分解公式</p>
<hr />
<h4 id="2-"><strong>方向2：效率层面 - 极致加速与资源优化</strong></h4>
<p><strong>研究空白</strong>：
- Router计算仍占5%-10%总时间，能否进一步压缩？
- 动态$k$选择（自适应Top-k）缺乏高效实现
- 推理时内存占用大（需要加载所有Expert）</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li><strong>问题</strong>：能否设计O(1)复杂度的Router？</li>
<li><strong>现有方案</strong>：当前Router是$O(dn)$</li>
<li><strong>优化方向</strong>：<ul>
<li>哈希路由（Hash Routing）：用局部敏感哈希直接映射到Expert</li>
<li>学习二叉决策树：$O(\log n)$选择</li>
<li>神经架构搜索（NAS）自动设计轻量Router</li>
</ul>
</li>
<li>
<p><strong>挑战</strong>：保证路由准确率</p>
</li>
<li>
<p><strong>问题</strong>：如何在推理时动态加载Expert（Expert on Demand）？</p>
</li>
<li><strong>现状</strong>：所有Expert都需要加载到GPU内存</li>
<li><strong>优化方向</strong>：<ul>
<li>预测下一批Token会用到哪些Expert，提前加载</li>
<li>Expert缓存策略（LRU、LFU）</li>
<li>CPU-GPU异构存储（冷Expert放CPU）</li>
</ul>
</li>
<li>
<p><strong>量化目标</strong>：将内存占用从$n \times \text{expert_size}$降至$k \times \text{expert_size}$</p>
</li>
<li>
<p><strong>问题</strong>：如何实现自适应Top-k？</p>
</li>
<li><strong>需求</strong>：简单Token用Top-1，复杂Token用Top-3</li>
<li><strong>挑战</strong>：如何评估Token复杂度？如何动态调整计算图？</li>
<li><strong>潜在方法</strong>：<ul>
<li>引入"难度预测器"（Difficulty Predictor）</li>
<li>使用强化学习优化$k$的选择策略</li>
<li>基于Router置信度（如Top-1与Top-2的得分差）</li>
</ul>
</li>
</ol>
<p><strong>优化方向</strong>：
- 研究稀疏矩阵乘法加速（对Top-k激活）
- 探索模型量化（INT8 Expert，FP16 Router）
- 开发定制化硬件加速器（类似TPU对MoE的优化）</p>
<p><strong>量化目标</strong>：
- Router时间降至总时间&lt;2%
- 推理内存降至Dense模型的1.5倍（vs 当前3-5倍）
- 实现动态Top-k，平均$k$从2降至1.5（节省25%计算）
- 在移动端（如iPhone）实现实时推理（&lt;100ms/token for 7B MoE）</p>
<hr />
<h4 id="3-"><strong>方向3：应用层面 - 多模态与长上下文</strong></h4>
<p><strong>研究空白</strong>：
- MoE在多模态（图像+文本）中的应用不充分
- 长上下文场景下的Expert选择策略
- Fine-tuning MoE模型的最佳实践</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li><strong>问题</strong>：如何为不同模态设计Expert？</li>
<li><strong>现状</strong>：大多数工作只在语言模型中使用MoE</li>
<li><strong>优化方向</strong>：<ul>
<li>模态特定Expert（Image Expert、Text Expert）</li>
<li>跨模态融合Expert（处理图文对齐）</li>
<li>层次化MoE（低层模态特定，高层跨模态）</li>
</ul>
</li>
<li>
<p><strong>挑战</strong>：不同模态的数据量不均衡（如图像少、文本多）</p>
</li>
<li>
<p><strong>问题</strong>：长上下文下的Expert复用？</p>
</li>
<li><strong>观察</strong>：在长文档中，相邻Token可能选择相同Expert</li>
<li>
<p><strong>优化方向</strong>：</p>
<ul>
<li>Expert缓存机制（避免重复计算）</li>
<li>滑动窗口内共享Expert选择</li>
<li>分层路由（粗粒度选择Expert组，细粒度选择具体Expert）</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：如何高效Fine-tune MoE？</p>
</li>
<li><strong>挑战</strong>：全参数微调开销大，LoRA在MoE中效果不明显</li>
<li><strong>优化方向</strong>：<ul>
<li>只微调Router（冻结Expert）</li>
<li>只微调部分Expert（如Top-使用频率的2个）</li>
<li>引入Adapter层（在Expert之间插入）</li>
</ul>
</li>
<li><strong>量化目标</strong>：微调参数量&lt;5%总参数，性能保持95%+</li>
</ol>
<p><strong>优化方向</strong>：
- 研究视觉MoE（如ViT-MoE）
- 探索音频、视频领域的MoE
- 开发通用多模态MoE框架</p>
<p><strong>量化目标</strong>：
- 多模态MoE在图像描述任务上超越Dense CLIP 10%
- 长上下文（128k tokens）下Expert缓存命中率&gt;80%
- Fine-tuning只调整2%参数，性能降低&lt;3%</p>
<hr />
<h4 id="4-"><strong>方向4：鲁棒性层面 - 对抗攻击与失效模式</strong></h4>
<p><strong>研究空白</strong>：
- MoE对对抗样本的鲁棒性未知
- Router可能成为攻击目标（如诱导选择错误Expert）
- Expert失效（如硬件故障）的容错机制</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li><strong>问题</strong>：Router是否容易被对抗攻击？</li>
<li><strong>攻击场景</strong>：微小扰动$\boldsymbol{x} + \delta$导致选择错误Expert</li>
<li>
<p><strong>潜在防御</strong>：</p>
<ul>
<li>对抗训练（Adversarial Training）Router</li>
<li>多样性路由（随机选择Top-k中的一个）</li>
<li>鲁棒Router架构（如使用鲁棒激活函数）</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：如何处理Expert失效？</p>
</li>
<li><strong>场景</strong>：分布式推理中某GPU故障，对应Expert不可用</li>
<li>
<p><strong>优化方向</strong>：</p>
<ul>
<li>冗余设计（每个Expert有备份）</li>
<li>动态替代（用相似Expert替代失效Expert）</li>
<li>降级策略（用Dense层临时替代）</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：Router的可解释性？</p>
</li>
<li><strong>现状</strong>：很难理解为什么某Expert被选择</li>
<li><strong>探索方向</strong>：<ul>
<li>可视化Router学到的决策边界</li>
<li>分析Expert的"专长领域"</li>
<li>引入注意力机制增强可解释性</li>
</ul>
</li>
</ol>
<p><strong>优化方向</strong>：
- 借鉴鲁棒优化理论
- 开发MoE的形式化验证方法
- 研究不确定性量化（Router置信度估计）</p>
<p><strong>量化目标</strong>：
- 对抗鲁棒性：在PGD攻击下准确率下降&lt;10%（Dense模型下降20%）
- 容错性：1个Expert失效，性能下降&lt;5%
- 可解释性：至少70%的Expert具有明确的"专长标签"</p>
<hr />
<h4 id="5-moe"><strong>方向5：新型架构 - 层次化与动态MoE</strong></h4>
<p><strong>研究空白</strong>：
- 当前MoE大多是扁平结构（所有Expert平等）
- Expert数量$n$固定，无法动态调整
- 缺乏跨层Expert共享机制</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li><strong>问题</strong>：层次化MoE（Hierarchical MoE）？</li>
<li><strong>设计</strong>：<ul>
<li>第一层Router选择Expert组（如8组）</li>
<li>第二层Router在组内选择具体Expert（如每组4个，共32个Expert）</li>
</ul>
</li>
<li><strong>优点</strong>：更强表达能力，更细粒度控制</li>
<li>
<p><strong>挑战</strong>：训练复杂度增加</p>
</li>
<li>
<p><strong>问题</strong>：动态Expert增长（Growing MoE）？</p>
</li>
<li><strong>思路</strong>：训练初期用少量Expert，逐渐增加</li>
<li><strong>优点</strong>：避免冷启动时的负载不均衡</li>
<li>
<p><strong>挑战</strong>：如何初始化新Expert？何时增加？</p>
</li>
<li>
<p><strong>问题</strong>：跨层Expert共享？</p>
</li>
<li><strong>观察</strong>：某些Expert可能在多层都有用</li>
<li><strong>优化</strong>：允许不同层的Router选择同一个Expert池</li>
<li><strong>优点</strong>：减少参数量，增强一致性</li>
</ol>
<p><strong>优化方向</strong>：
- 借鉴神经架构搜索（NAS）
- 研究元学习（Meta-Learning）自动设计MoE结构
- 探索可微分架构搜索（DARTS）for MoE</p>
<p><strong>量化目标</strong>：
- 层次化MoE性能提升10%，参数增加&lt;20%
- 动态增长策略：最终$n=32$，但训练前期只用$n=8$，节省30%训练时间
- 跨层共享：参数量降低40%，性能保持95%</p>
<hr />
<h4 id="_7"><strong>潜在应用场景</strong></h4>
<p><strong>语言模型</strong>：
- 超大规模预训练（万亿参数）
- 多语言模型（每个Expert负责一种语言）
- 代码生成（不同Expert处理不同编程语言）</p>
<p><strong>多模态</strong>：
- 图文生成（Stable Diffusion + MoE）
- 视频理解（时空MoE）
- 机器人控制（不同Expert处理不同传感器）</p>
<p><strong>科学计算</strong>：
- PDE求解（不同Expert处理不同方程类型）
- 蛋白质折叠（氨基酸类型特定Expert）
- 气候模拟（地理区域特定Expert）</p>
<hr />
<h3 id="_8">总结</h3>
<p>本文从几何意义出发，推导了MoE的核心思想：通过选择"模长最大"的Top-k向量来逼近全部向量的和。这一视角不仅提供了清晰的数学直觉，还引出了"归一化Expert + 非归一化Router"的新设计。</p>
<p><strong>核心要点</strong>：
1. FFN可分解为$n$个Expert向量的和
2. 按模长选择Top-k是理论最优策略
3. Router预测模长，避免计算所有Expert
4. 归一化确保几何意义，减少Expert竞争
5. 负载均衡、通信优化是工程关键</p>
<p><strong>未来值得关注</strong>：
- 理论：泛化界、收敛性分析
- 效率：Router加速、动态Top-k
- 应用：多模态、长上下文、Fine-tuning
- 鲁棒性：对抗攻击、容错机制
- 新架构：层次化、动态增长、跨层共享</p>
        </div>
    </div>
</body>
</html>