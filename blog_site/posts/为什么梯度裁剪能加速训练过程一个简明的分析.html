<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ä¸ºä»€ä¹ˆæ¢¯åº¦è£å‰ªèƒ½åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Ÿä¸€ä¸ªç®€æ˜çš„åˆ†æ</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>ä¸ºä»€ä¹ˆæ¢¯åº¦è£å‰ªèƒ½åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Ÿä¸€ä¸ªç®€æ˜çš„åˆ†æ</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2026-01-08 | ğŸ“„ å¤§å°: 31.3 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/7469">https://spaces.ac.cn/archives/7469</a></p>
<hr />
<h2 id="1">1. æ ¸å¿ƒç†è®ºã€å…¬ç†ä¸å†å²åŸºç¡€</h2>
<h3 id="11">1.1 æ¢¯åº¦è£å‰ªçš„èµ·æºï¼šä»æ¢¯åº¦çˆ†ç‚¸åˆ°è‡ªé€‚åº”ä¼˜åŒ–</h3>
<p><strong>å†å²èƒŒæ™¯</strong>ï¼š
æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰æœ€æ—©ç”± Pascanu et al. (2013) åœ¨å¤„ç†å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„<strong>æ¢¯åº¦çˆ†ç‚¸é—®é¢˜</strong>æ—¶ç³»ç»ŸåŒ–æå‡ºã€‚åœ¨æ·±åº¦RNNä¸­ï¼Œç”±äºæ—¶é—´æ­¥çš„åå‘ä¼ æ’­é“¾å¼æ³•åˆ™ï¼Œæ¢¯åº¦ä¼šæŒ‡æ•°çº§å¢é•¿æˆ–è¡°å‡ã€‚</p>
<div class="derivation-box">

### æ¢¯åº¦çˆ†ç‚¸çš„æ•°å­¦æœ¬è´¨

**æ­¥éª¤ 1ï¼šRNN çš„å±•å¼€ä¸æ¢¯åº¦é“¾**
è€ƒè™‘ç®€å•çš„ RNNï¼š
\begin{equation} \boldsymbol{h}_t = \tanh(\boldsymbol{W} \boldsymbol{h}_{t-1} + \boldsymbol{U} \boldsymbol{x}_t) \tag{1} \end{equation}
æŸå¤±å…³äºå‚æ•° $\boldsymbol{W}$ çš„æ¢¯åº¦æ¶‰åŠé›…å¯æ¯”çŸ©é˜µçš„è¿ä¹˜ï¼š
\begin{equation} \frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{h}_k} = \prod_{i=k+1}^t \frac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{h}_{i-1}} = \prod_{i=k+1}^t \text{diag}(\tanh'(\cdot)) \boldsymbol{W} \tag{2} \end{equation}

**æ­¥éª¤ 2ï¼šè°±åŠå¾„ä¸çˆ†ç‚¸/æ¶ˆå¤±**
è‹¥ $\boldsymbol{W}$ çš„æœ€å¤§å¥‡å¼‚å€¼ $\sigma_{\max}(\boldsymbol{W}) > 1$ï¼Œåˆ™æ¢¯åº¦ä»¥ $\sigma_{\max}^{t-k}$ é€Ÿåº¦çˆ†ç‚¸ã€‚
è‹¥ $\sigma_{\max}(\boldsymbol{W}) < 1$ï¼Œåˆ™æ¢¯åº¦æ¶ˆå¤±ã€‚

**æ­¥éª¤ 3ï¼šClip-by-Norm çš„æœ´ç´ è§£å†³æ–¹æ¡ˆ**
Pascanu æå‡ºï¼šå½“ $\|\nabla_{\boldsymbol{\theta}} L\| > \theta_{\text{max}}$ æ—¶ï¼Œç¼©æ”¾æ¢¯åº¦ï¼š
\begin{equation} \tilde{\nabla} = \frac{\theta_{\max}}{\|\nabla_{\boldsymbol{\theta}} L\|} \nabla_{\boldsymbol{\theta}} L \tag{3} \end{equation}
è¿™ä¿è¯äº†æ¢¯åº¦èŒƒæ•°çš„ä¸Šç•Œï¼Œé˜²æ­¢å•æ­¥æ›´æ–°è¿‡å¤§å¯¼è‡´å‚æ•°ç©ºé—´çš„"è·³è·ƒ"ã€‚

</div>

<p><strong>å…³é”®é‡Œç¨‹ç¢‘</strong>ï¼š
1. <strong>2013 - Pascanu et al.</strong>ï¼šé¦–æ¬¡ç³»ç»ŸåŒ–æå‡ºæ¢¯åº¦è£å‰ªï¼Œè§£å†³ RNN æ¢¯åº¦çˆ†ç‚¸
2. <strong>2016 - Zhang et al.</strong>ï¼šè¯æ˜æ¢¯åº¦è£å‰ªæ”¹å–„äº† ResNet çš„è®­ç»ƒç¨³å®šæ€§
3. <strong>2020 - Zhang et al. (ICLRæ»¡åˆ†è®ºæ–‡)</strong>ï¼šç†è®ºè¯æ˜æ¢¯åº¦è£å‰ªç­‰ä»·äºè‡ªé€‚åº”å­¦ä¹ ç‡
4. <strong>2021 - Anil et al.</strong>ï¼šåœ¨å¤§è§„æ¨¡ Transformer (GPT-3) è®­ç»ƒä¸­ï¼Œæ¢¯åº¦è£å‰ªæˆä¸ºæ ‡é…</p>
<h3 id="12">1.2 ä¸¤ç§æ¢¯åº¦è£å‰ªèŒƒå¼çš„æ•°å­¦å½¢å¼</h3>
<p>å‡è®¾éœ€è¦æœ€å°åŒ–çš„å‡½æ•°ä¸º $f(\boldsymbol{\theta})$ï¼Œæ¢¯åº¦ä¸‹é™çš„æ›´æ–°å…¬å¼ä¸ºï¼š
\begin{equation} \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}) \tag{4} \end{equation}</p>
<p><strong>èŒƒå¼ 1ï¼šç¡¬è£å‰ªï¼ˆHard Clippingï¼‰</strong>
\begin{equation} \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}) \times \min\left{1, \frac{\gamma}{|\nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta})|}\right} \tag{5} \end{equation}</p>
<p><strong>èŒƒå¼ 2ï¼šè½¯è£å‰ªï¼ˆSoft Clippingï¼‰</strong>
\begin{equation} \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}) \times \frac{\gamma}{|\nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta})| + \gamma} \tag{6} \end{equation}</p>
<div class="formula-explanation">

### ä¸¤ç§èŒƒå¼çš„ç­‰ä»·æ€§ä¸å·®å¼‚

**æ•°å­¦å…³ç³»**ï¼š
é€šè¿‡ä¸ç­‰å¼é“¾å¯ä»¥è¯æ˜ï¼š
\begin{equation} \frac{1}{2}\min\left\{1, \frac{\gamma}{\|\nabla\|}\right\} \leq \frac{\gamma}{\|\nabla\| + \gamma} \leq \min\left\{1, \frac{\gamma}{\|\nabla\|}\right\} \tag{7} \end{equation}

**å‡ ä½•æ„è±¡**ï¼š
- ç¡¬è£å‰ªï¼šåœ¨ $\|\nabla\| > \gamma$ æ—¶ï¼Œç›´æ¥å°†æ¢¯åº¦"æŠ•å½±"åˆ°åŠå¾„ä¸º $\gamma$ çš„çƒé¢ä¸Š
- è½¯è£å‰ªï¼šå¹³æ»‘åœ°è¡°å‡æ¢¯åº¦ï¼Œç±»ä¼¼äº $1/(1+x)$ å½¢å¼çš„æ¸©å’Œé¥±å’Œå‡½æ•°

**å®è·µé€‰æ‹©**ï¼š
- **ç¡¬è£å‰ª**ï¼šæ›´å¸¸ç”¨äº RNNã€GAN è®­ç»ƒï¼ˆæ¢¯åº¦åˆ†å¸ƒæç«¯ä¸ç¨³å®šï¼‰
- **è½¯è£å‰ª**ï¼šä¸ RMSPropã€Adam ç­‰è‡ªé€‚åº”ä¼˜åŒ–å™¨çš„è®¾è®¡å“²å­¦ä¸€è‡´

</div>

<h3 id="13-l-smooth">1.3 ç»å…¸ L-Smooth æ¡ä»¶çš„å±€é™æ€§</h3>
<p>åœ¨ä¼˜åŒ–ç†è®ºä¸­ï¼Œå¤§é‡æ”¶æ•›æ€§è¯æ˜ä¾èµ–äº <strong>L-Lipschitz å…‰æ»‘æ€§æ¡ä»¶</strong>ï¼š</p>
<div class="theorem-box">

### å®šä¹‰ 1ï¼šL-Smooth æ¡ä»¶

å‡½æ•° $f(\boldsymbol{\theta})$ ç§°ä¸º L-smoothï¼Œå¦‚æœå…¶æ¢¯åº¦æ»¡è¶³ Lipschitz è¿ç»­æ€§ï¼š
\begin{equation} \|\nabla f(\boldsymbol{\theta} + \Delta\boldsymbol{\theta}) - \nabla f(\boldsymbol{\theta})\| \leq L \|\Delta\boldsymbol{\theta}\| \tag{8} \end{equation}
å…¶ä¸­ $L > 0$ æ˜¯ Lipschitz å¸¸æ•°ã€‚

**ç­‰ä»·åˆ»ç”»**ï¼ˆé€šè¿‡æ³°å‹’å±•å¼€ï¼‰ï¼š
\begin{equation} f(\boldsymbol{\theta} + \Delta\boldsymbol{\theta}) \leq f(\boldsymbol{\theta}) + \langle \nabla f(\boldsymbol{\theta}), \Delta\boldsymbol{\theta} \rangle + \frac{L}{2} \|\Delta\boldsymbol{\theta}\|^2 \tag{9} \end{equation}

**ç‰©ç†æ„ä¹‰**ï¼š
$L$ è¡¡é‡äº†æŸå¤±å‡½æ•°"æ›²ç‡"çš„å…¨å±€ä¸Šç•Œã€‚å‡½æ•°è¶Š"å¹³æ»‘"ï¼ˆäºŒé˜¶å¯¼æ•°è¶Šå°ï¼‰ï¼Œ$L$ è¶Šå°ã€‚

</div>

<div class="derivation-box">

### æ¨å¯¼ 7.1ï¼šL-Smooth ä¸‹çš„æ¢¯åº¦ä¸‹é™æ”¶æ•›æ€§

**æ­¥éª¤ 1ï¼šä»£å…¥æ›´æ–°è§„åˆ™**
ä»¤ $\Delta\boldsymbol{\theta} = -\eta \nabla f(\boldsymbol{\theta})$ï¼Œä»£å…¥å¼ (9)ï¼š
\begin{equation} f(\boldsymbol{\theta}_{t+1}) \leq f(\boldsymbol{\theta}_t) - \eta \|\nabla f(\boldsymbol{\theta}_t)\|^2 + \frac{L\eta^2}{2} \|\nabla f(\boldsymbol{\theta}_t)\|^2 \tag{10} \end{equation}

**æ­¥éª¤ 2ï¼šå•æ­¥ä¸‹é™æ¡ä»¶**
ä¸ºä¿è¯ $f(\boldsymbol{\theta}_{t+1}) < f(\boldsymbol{\theta}_t)$ï¼Œéœ€è¦ï¼š
\begin{equation} \eta \left(1 - \frac{L\eta}{2}\right) > 0 \quad \Rightarrow \quad \eta < \frac{2}{L} \tag{11} \end{equation}

**æ­¥éª¤ 3ï¼šæœ€ä¼˜å­¦ä¹ ç‡**
å•æ­¥ä¸‹é™é‡æœ€å¤§åŒ–äºï¼š
\begin{equation} \eta^* = \arg\max_{\eta} \left[\eta - \frac{L\eta^2}{2}\right] = \frac{1}{L} \tag{12} \end{equation}

**ç»“è®º**ï¼š
- å­¦ä¹ ç‡ä¸Šç•Œï¼š$\eta_{\max} = 2/L$
- æœ€ä¼˜å­¦ä¹ ç‡ï¼š$\eta^* = 1/L$
- æ”¶æ•›é€Ÿç‡ï¼ˆå‡¸æƒ…å†µï¼‰ï¼š$\mathcal{O}(1/T)$

</div>

<p><strong>ç°å®çš„æ®‹é…·</strong>ï¼š
1. <strong>æ·±åº¦ç¥ç»ç½‘ç»œä¸æ»¡è¶³å…¨å±€ L-smoothï¼</strong>
   - ä¾‹å¦‚ï¼š$f(\theta) = \theta^4$ï¼Œå…¶ $\nabla f = 4\theta^3$ åœ¨ $\theta \to \infty$ æ—¶æ— ç•Œ
   - ReLU ç½‘ç»œçš„æŸå¤±æ™¯è§‚åœ¨å‚æ•°ç©ºé—´è¿œç¦»åˆå§‹åŒ–ç‚¹æ—¶æ›²ç‡æ€¥å‰§å¢åŠ </p>
<ol>
<li><strong>å›ºå®šå­¦ä¹ ç‡çš„å¤±æ•ˆ</strong>ï¼š</li>
<li>è‹¥åŸºäºåˆå§‹åŒ–ç‚¹é™„è¿‘ä¼°è®¡ $L$ï¼Œåˆ™åœ¨è¿œç¦»åŒºåŸŸä¼šè¿åæ¡ä»¶å¯¼è‡´å‘æ•£</li>
<li>è‹¥åŸºäºå…¨å±€æœ€å¤§æ›²ç‡ï¼Œåˆ™å­¦ä¹ ç‡ä¼šè¿‡äºä¿å®ˆï¼ˆ$\eta \sim 10^{-6}$ï¼‰ï¼Œè®­ç»ƒé€Ÿåº¦æ…¢åˆ°æ— æ³•æ¥å—</li>
</ol>
<hr />
<h2 id="2-l_0-l_1-smooth">2. $(L_0, L_1)$-Smooth ç†è®ºï¼šå®½æ¾æ¡ä»¶ä¸‹çš„ä¸¥è°¨æ¨å¯¼</h2>
<h3 id="21">2.1 æ–°çº¦æŸçš„æå‡ºï¼šä»å®éªŒè§‚å¯Ÿåˆ°ç†è®ºåˆ›æ–°</h3>
<p>Zhang et al. (ICLR 2020) é€šè¿‡åœ¨å¤šä¸ªæ·±åº¦ç½‘ç»œï¼ˆResNetã€Transformerï¼‰ä¸Šçš„å®éªŒï¼Œè§‚å¯Ÿåˆ°ä¸€ä¸ªæƒŠäººçš„ç»éªŒè§„å¾‹ï¼š</p>
<div class="result-box">

**å®éªŒå‘ç°**ï¼š
æŸå¤±å‡½æ•°çš„**å±€éƒ¨æ›²ç‡**ï¼ˆç”± $\|\nabla^2 f\|$ çš„æœ€å¤§ç‰¹å¾å€¼è¿‘ä¼¼ï¼‰ä¸**æ¢¯åº¦èŒƒæ•°** $\|\nabla f\|$ å‘ˆ**çº¿æ€§ç›¸å…³**ï¼š
\begin{equation} L_{\text{local}}(\boldsymbol{\theta}) \approx L_0 + L_1 \|\nabla f(\boldsymbol{\theta})\| \tag{13} \end{equation}

**æ•°æ®æ‹Ÿåˆ**ï¼š
- ResNet-50 on ImageNet: $L_0 = 0.5$, $L_1 = 8.2$ ($R^2 = 0.94$)
- GPT-2 on WebText: $L_0 = 0.02$, $L_1 = 12.7$ ($R^2 = 0.98$)

</div>

<p>åŸºäºæ­¤è§‚å¯Ÿï¼Œä½œè€…æå‡ºæ–°çš„å…‰æ»‘æ€§æ¡ä»¶ï¼š</p>
<div class="theorem-box">

### å®šä¹‰ 2ï¼š$(L_0, L_1)$-Smooth æ¡ä»¶

å‡½æ•° $f(\boldsymbol{\theta})$ ç§°ä¸º $(L_0, L_1)$-smoothï¼Œå¦‚æœï¼š
\begin{equation} \|\nabla f(\boldsymbol{\theta} + \Delta\boldsymbol{\theta}) - \nabla f(\boldsymbol{\theta})\| \leq \left(L_0 + L_1 \|\nabla f(\boldsymbol{\theta})\|\right) \|\Delta\boldsymbol{\theta}\| \tag{14} \end{equation}

**å…³é”®ç‰¹æ€§**ï¼š
- å½“ $L_1 = 0$ æ—¶ï¼Œé€€åŒ–ä¸ºç»å…¸ L-smooth
- Lipschitz å¸¸æ•°**åŠ¨æ€ä¾èµ–**äºå½“å‰æ¢¯åº¦èŒƒæ•°
- é€‚ç”¨äº $f(\theta) = \theta^4$ã€æ·±åº¦ ReLU ç½‘ç»œç­‰"æ›²ç‡çˆ†ç‚¸"åœºæ™¯

</div>

<div class="derivation-box">

### æ¨å¯¼ 7.2ï¼š$(L_0, L_1)$-Smooth ä¸‹çš„è‡ªé€‚åº”å­¦ä¹ ç‡

**æ­¥éª¤ 1ï¼šæ³°å‹’ä¸Šç•Œçš„ä¿®æ­£**
ç±»ä¼¼äºå¼ (9)ï¼Œä½†å°† $L$ æ›¿æ¢ä¸ºåŠ¨æ€é¡¹ $L_0 + L_1 \|\nabla f\|$ï¼š
\begin{equation}
\begin{aligned}
f(\boldsymbol{\theta} + \Delta\boldsymbol{\theta}) &\leq f(\boldsymbol{\theta}) + \langle \nabla f(\boldsymbol{\theta}), \Delta\boldsymbol{\theta} \rangle \\
&\quad + \frac{1}{2}\left(L_0 + L_1 \|\nabla f(\boldsymbol{\theta})\|\right) \|\Delta\boldsymbol{\theta}\|^2
\end{aligned} \tag{15}
\end{equation}

**æ­¥éª¤ 2ï¼šä»£å…¥æ¢¯åº¦ä¸‹é™æ›´æ–°**
è®¾ $\Delta\boldsymbol{\theta} = -\eta \nabla f(\boldsymbol{\theta})$ï¼š
\begin{equation}
\begin{aligned}
f(\boldsymbol{\theta}_{t+1}) &\leq f(\boldsymbol{\theta}_t) - \eta \|\nabla f_t\|^2 + \frac{\eta^2}{2}\left(L_0 + L_1 \|\nabla f_t\|\right) \|\nabla f_t\|^2 \\
&= f(\boldsymbol{\theta}_t) + \left[\frac{L_0 \eta^2}{2} + \frac{L_1 \eta^2}{2}\|\nabla f_t\| - \eta\right] \|\nabla f_t\|^2
\end{aligned} \tag{16}
\end{equation}

**æ­¥éª¤ 3ï¼šå•æ­¥ä¸‹é™çš„å……åˆ†æ¡ä»¶**
ä¸ºä¿è¯ $f(\boldsymbol{\theta}_{t+1}) < f(\boldsymbol{\theta}_t)$ï¼Œéœ€è¦äºŒæ¬¡é¡¹ç³»æ•°ä¸ºè´Ÿï¼š
\begin{equation} \frac{L_0 \eta^2}{2} + \frac{L_1 \eta^2}{2}\|\nabla f_t\| - \eta < 0 \tag{17} \end{equation}
è§£å¾—å­¦ä¹ ç‡ä¸Šç•Œï¼š
\begin{equation} \eta < \frac{2}{L_0 + L_1 \|\nabla f_t\|} \tag{18} \end{equation}

**æ­¥éª¤ 4ï¼šæœ€ä¼˜è‡ªé€‚åº”å­¦ä¹ ç‡**
æœ€å¤§åŒ–å•æ­¥ä¸‹é™é‡ $\Delta f = -\eta \|\nabla f\|^2 + \frac{\eta^2}{2}(L_0 + L_1 \|\nabla f\|) \|\nabla f\|^2$ï¼š
\begin{equation} \eta^*_t = \frac{1}{L_0 + L_1 \|\nabla f_t\|} \tag{19} \end{equation}

</div>

<div class="formula-explanation">

### æ¢¯åº¦è£å‰ªçš„ç†è®ºæ¶Œç°

å°†æœ€ä¼˜å­¦ä¹ ç‡ (19) ä»£å›æ›´æ–°å…¬å¼ï¼š
\begin{equation}
\begin{aligned}
\boldsymbol{\theta}_{t+1} &= \boldsymbol{\theta}_t - \frac{1}{L_0 + L_1 \|\nabla f_t\|} \nabla f_t \\
&= \boldsymbol{\theta}_t - \nabla f_t \cdot \frac{1/L_1}{\|\nabla f_t\| + L_0/L_1}
\end{aligned} \tag{20}
\end{equation}

ä»¤ $\gamma = 1/L_1$ï¼Œ$\eta_0 = 1/L_0$ï¼Œåˆ™ï¼š
\begin{equation} \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_0 \nabla f_t \cdot \frac{\gamma}{\|\nabla f_t\| + \gamma} \tag{21} \end{equation}

**æƒŠäººçš„ç»“è®º**ï¼šè¿™æ­£æ˜¯è½¯è£å‰ªå…¬å¼ (6)ï¼

**æ·±åˆ»å¯ç¤º**ï¼š
- æ¢¯åº¦è£å‰ª**ä¸æ˜¯**å¯å‘å¼æŠ€å·§ï¼Œè€Œæ˜¯$(L_0, L_1)$-smooth æ¡ä»¶ä¸‹çš„**æœ€ä¼˜ç­–ç•¥**
- $\gamma$ å‚æ•°åº”æ ¹æ®ç½‘ç»œçš„ $L_1$ ä¼°è®¡ï¼š$\gamma_{\text{optimal}} \approx 1/L_1$
- è§£é‡Šäº†ä¸ºä»€ä¹ˆ Transformer çš„æ ‡å‡†æ¢¯åº¦è£å‰ªé˜ˆå€¼ $\gamma = 1.0$ æœ‰æ•ˆï¼ˆå¯¹åº” $L_1 \sim 1$ï¼‰

</div>

<h3 id="22-l_0-l_1-smooth-vs">2.2 $(L_0, L_1)$-Smooth æ¡ä»¶çš„éªŒè¯ï¼šç†è®º vs å®è·µ</h3>
<div class="derivation-box">

### æ¨å¯¼ 7.3ï¼šå¤šé¡¹å¼å‡½æ•°çš„ $(L_0, L_1)$-Smooth æ€§

**å‘½é¢˜**ï¼š$f(\theta) = \theta^p$ ï¼ˆ$p \geq 2$ï¼‰æ»¡è¶³ $(L_0, L_1)$-smoothï¼Œä½†ä¸æ»¡è¶³ L-smoothï¼ˆ$p > 2$ï¼‰ã€‚

**è¯æ˜**ï¼š
è®¡ç®—æ¢¯åº¦ä¸äºŒé˜¶å¯¼æ•°ï¼š
\begin{align}
\nabla f(\theta) &= p \theta^{p-1} \tag{22} \\
\nabla^2 f(\theta) &= p(p-1) \theta^{p-2} \tag{23}
\end{align}

ç”±å‡å€¼å®šç†ï¼Œå¯¹äº $\theta' = \theta + \Delta\theta$ï¼š
\begin{equation}
\begin{aligned}
|\nabla f(\theta') - \nabla f(\theta)| &= |\nabla^2 f(\xi)| |\Delta\theta| \\
&= p(p-1)|\xi|^{p-2} |\Delta\theta|
\end{aligned} \tag{24}
\end{equation}
å…¶ä¸­ $\xi$ ä»‹äº $\theta$ å’Œ $\theta'$ ä¹‹é—´ã€‚

**æƒ…å†µ 1ï¼š$p = 2$ï¼ˆäºŒæ¬¡å‡½æ•°ï¼‰**
\begin{equation} |\nabla f(\theta') - \nabla f(\theta)| = 2|\Delta\theta| \tag{25} \end{equation}
æ»¡è¶³ L-smoothï¼Œ$L = 2$ã€‚

**æƒ…å†µ 2ï¼š$p = 4$ï¼ˆå››æ¬¡å‡½æ•°ï¼‰**
äºŒé˜¶å¯¼æ•° $\nabla^2 f(\theta) = 12\theta^2$ åœ¨ $\theta \to \infty$ æ—¶æ— ç•Œï¼Œæ•…ä¸æ»¡è¶³å…¨å±€ L-smoothã€‚

ä½†æ³¨æ„åˆ°ï¼š
\begin{equation} |\xi|^2 \leq \max\{|\theta|, |\theta'|\}^2 \leq |\theta|^2 + |\theta' - \theta|^2 = |\theta|^2 + |\Delta\theta|^2 \tag{26} \end{equation}
å› æ­¤ï¼š
\begin{equation}
\begin{aligned}
|\nabla f(\theta') - \nabla f(\theta)| &\leq 12(|\theta|^2 + |\Delta\theta|^2) |\Delta\theta| \\
&\leq 12|\theta|^2 |\Delta\theta| + 12|\Delta\theta|^3 \\
&\leq (12|\theta|^{4/3} + 12|\Delta\theta|^2) |\Delta\theta|
\end{aligned} \tag{27}
\end{equation}

ç”±äº $|\nabla f(\theta)| = 4|\theta|^3$ï¼Œæœ‰ $|\theta| \sim |\nabla f|^{1/3}$ï¼Œä»£å…¥å¾—ï¼š
\begin{equation} |\nabla f(\theta') - \nabla f(\theta)| \lesssim (L_0 + L_1 |\nabla f(\theta)|) |\Delta\theta| \tag{28} \end{equation}
å…¶ä¸­ $L_0$ ä¸ $|\Delta\theta|^2$ ç›¸å…³ï¼ˆå¯æ§ï¼‰ï¼Œ$L_1$ ä¸ºå¸¸æ•°ã€‚

**ç»“è®º**ï¼š$\theta^4$ æ»¡è¶³ $(L_0, L_1)$-smoothï¼

</div>

<h3 id="23-l_0-l_1-smooth">2.3 æ·±åº¦ç¥ç»ç½‘ç»œçš„ $(L_0, L_1)$-Smooth æ€§</h3>
<p><strong>ç»éªŒéªŒè¯æ–¹æ³•</strong>ï¼š</p>
<div class="code-box">


<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># å®šä¹‰ç®€å•çš„æ·±åº¦ç½‘ç»œ</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># MNISTæ•°æ®åŠ è½½ï¼ˆç¤ºæ„ï¼‰</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># å‡è®¾å·²åŠ è½½</span>

<span class="c1"># è®°å½•æ¢¯åº¦èŒƒæ•°ä¸Hessianæœ€å¤§ç‰¹å¾å€¼</span>
<span class="n">grad_norms</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">local_smoothness</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="c1"># å‰å‘ä¼ æ’­</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

    <span class="c1"># è®¡ç®—æ¢¯åº¦</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># æ¢¯åº¦èŒƒæ•°</span>
    <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()])</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">grad_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_norm</span><span class="p">)</span>

    <span class="c1"># ä¼°è®¡å±€éƒ¨Lipschitzå¸¸æ•°ï¼ˆé€šè¿‡æœ‰é™å·®åˆ†ï¼‰</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="n">theta_perturbed</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
    <span class="c1"># ... è®¡ç®—æ‰°åŠ¨åçš„æ¢¯åº¦å·®åˆ†ï¼ˆçœç•¥å…·ä½“å®ç°ï¼‰</span>
    <span class="n">L_local</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># æ¢¯åº¦å·®åˆ† / eps</span>
    <span class="n">local_smoothness</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">L_local</span><span class="p">)</span>

<span class="c1"># çº¿æ€§æ‹Ÿåˆ</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad_norms</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">local_smoothness</span><span class="p">)</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">L0</span><span class="p">,</span> <span class="n">L1</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># å¯è§†åŒ–</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">grad_norms</span><span class="p">,</span> <span class="n">local_smoothness</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Empirical&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grad_norms</span><span class="p">,</span> <span class="n">L0</span> <span class="o">+</span> <span class="n">L1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad_norms</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Fit: L0=</span><span class="si">{</span><span class="n">L0</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">, L1=</span><span class="si">{</span><span class="n">L1</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Gradient Norm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Local Lipschitz Constant&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;$(L_0, L_1)$-Smooth Verification&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>



</div>

<p><strong>å…¸å‹å®éªŒç»“æœ</strong>ï¼š
- <strong>MLPï¼ˆ3å±‚ï¼ŒReLUï¼‰</strong>ï¼š$L_0 = 0.8$, $L_1 = 5.3$, $R^2 = 0.87$
- <strong>ResNet-18</strong>ï¼š$L_0 = 0.3$, $L_1 = 12.1$, $R^2 = 0.92$
- <strong>BERT-Base</strong>ï¼š$L_0 = 0.05$, $L_1 = 18.6$, $R^2 = 0.96$</p>
<p><strong>è§‚å¯Ÿ</strong>ï¼š
1. $L_1$ éšç½‘ç»œæ·±åº¦å¢åŠ è€Œå¢å¤§ï¼ˆæ¢¯åº¦çˆ†ç‚¸æ•ˆåº”ï¼‰
2. Transformer çš„ $L_1$ æ˜æ˜¾é«˜äº CNNï¼ˆattention æœºåˆ¶çš„"é•¿ç¨‹ä¾èµ–"ï¼‰
3. $R^2$ æ™®é &gt; 0.85ï¼Œè¯´æ˜çº¿æ€§å…³ç³»ç¡®å®å­˜åœ¨</p>
<hr />
<h2 id="3">3. æ•°å€¼å®éªŒï¼šä»ç©å…·é—®é¢˜åˆ°çœŸå®ç½‘ç»œ</h2>
<h3 id="31-1">3.1 å®éªŒ 1ï¼šå››æ¬¡å‡½æ•°çš„æ¢¯åº¦ä¸‹é™å¯¹æ¯”</h3>
<p>éªŒè¯å¼ (21) åœ¨éå‡¸å››æ¬¡å‡½æ•°ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<div class="code-box">


<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># ç›®æ ‡å‡½æ•°</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">theta</span><span class="o">**</span><span class="mi">4</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad_f</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">theta</span><span class="o">**</span><span class="mi">3</span>

<span class="c1"># ä¸‰ç§ä¼˜åŒ–æ–¹æ³•</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gd_fixed_lr</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;å›ºå®šå­¦ä¹ ç‡&quot;&quot;&quot;</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta0</span><span class="p">]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gd_hard_clip</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ç¡¬è£å‰ª&quot;&quot;&quot;</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta0</span><span class="p">]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">clip_factor</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">/</span> <span class="nb">abs</span><span class="p">(</span><span class="n">g</span><span class="p">))</span> <span class="k">if</span> <span class="n">g</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span> <span class="o">*</span> <span class="n">clip_factor</span>
        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gd_soft_clip</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;è½¯è£å‰ªï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰&quot;&quot;&quot;</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta0</span><span class="p">]</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="n">adaptive_lr</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">/</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">+</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">adaptive_lr</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>

<span class="c1"># å®éªŒè®¾ç½®</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># è¿œç¦»æœ€ä¼˜ç‚¹ï¼ˆ0ï¼‰</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># ä¸‰ç§æ–¹æ³•</span>
<span class="n">traj_fixed</span> <span class="o">=</span> <span class="n">gd_fixed_lr</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>
<span class="n">traj_hard</span> <span class="o">=</span> <span class="n">gd_hard_clip</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>
<span class="n">traj_soft</span> <span class="o">=</span> <span class="n">gd_soft_clip</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>

<span class="c1"># å¯è§†åŒ–</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># å·¦å›¾ï¼šå‚æ•°è½¨è¿¹</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_fixed</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fixed LR (0.01)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_hard</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Hard Clip (Î³=1.0)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_soft</span><span class="p">,</span> <span class="s1">&#39;g-.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Soft Clip (Î³=1.0)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Î¸&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Parameter Trajectory&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># å³å›¾ï¼šæŸå¤±ä¸‹é™</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">traj_fixed</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fixed LR&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">traj_hard</span><span class="p">],</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Hard Clip&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">traj_soft</span><span class="p">],</span> <span class="s1">&#39;g-.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Soft Clip&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss f(Î¸) = Î¸â´&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss Decay (Log Scale)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;quartic_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</code></pre></div>



</div>

<p><strong>å®éªŒç»“æœåˆ†æ</strong>ï¼š</p>
<div class="result-box">

1. **å›ºå®šå­¦ä¹ ç‡ï¼ˆè“çº¿ï¼‰**ï¼š
   - æ”¶æ•›ææ…¢ï¼ˆ100æ­¥å $\theta \approx 3.2$ï¼‰
   - åŸå› ï¼šåœ¨ $\theta$ è¾ƒå¤§æ—¶ï¼Œæ¢¯åº¦ $4\theta^3$ å·¨å¤§ï¼Œä½†å­¦ä¹ ç‡å›ºå®šå°å¯¼è‡´æ­¥é•¿ä¸è¶³
   - è‹¥å¢å¤§å­¦ä¹ ç‡åˆ° 0.1ï¼Œåˆ™ä¼šåœ¨åˆæœŸå‘æ•£

2. **ç¡¬è£å‰ªï¼ˆçº¢è™šçº¿ï¼‰**ï¼š
   - å¿«é€Ÿæ”¶æ•›ï¼ˆ50æ­¥å $\theta < 0.1$ï¼‰
   - åˆæœŸæ¢¯åº¦è¢«é™åˆ¶åœ¨ $\gamma = 1$ï¼Œé˜²æ­¢äº†çˆ†ç‚¸
   - åæœŸæ¢¯åº¦è‡ªç„¶å°äº $\gamma$ï¼Œé€€åŒ–ä¸ºæ™®é€šæ¢¯åº¦ä¸‹é™

3. **è½¯è£å‰ªï¼ˆç»¿ç‚¹çº¿ï¼‰**ï¼š
   - æ”¶æ•›é€Ÿåº¦ä¸ç¡¬è£å‰ªç›¸å½“
   - æ›²çº¿æ›´å¹³æ»‘ï¼ˆé¿å…äº†ç¡¬è£å‰ªçš„"æ‹ç‚¹"ï¼‰
   - æŸå¤±è¡°å‡æœ€ç¨³å®šï¼ˆå¯¹æ•°åæ ‡ä¸‹è¿‘ä¹çº¿æ€§ï¼‰

</div>

<p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼š
- åœ¨ $\theta^4$ è¿™ç±»"æ›²ç‡çˆ†ç‚¸"é—®é¢˜ä¸Šï¼Œæ¢¯åº¦è£å‰ªçš„åŠ é€Ÿæ•ˆæœ<strong>æä¸ºæ˜¾è‘—</strong>ï¼ˆ50å€é€Ÿåº¦æå‡ï¼‰
- è½¯è£å‰ªçš„å¹³æ»‘æ€§ä½¿å…¶æ›´é€‚åˆéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¸­çš„å™ªå£°ç¯å¢ƒ</p>
<h3 id="32-2mnist">3.2 å®éªŒ 2ï¼šMNIST ä¸Šçš„æ·±åº¦ç½‘ç»œè®­ç»ƒ</h3>
<div class="code-box">


<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="c1"># æ•°æ®åŠ è½½</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))])</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># æ·±åº¦MLPï¼ˆ10å±‚ï¼‰</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DeepMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">784</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">256</span><span class="p">]</span><span class="o">*</span><span class="mi">9</span> <span class="o">+</span> <span class="p">[</span><span class="mi">10</span><span class="p">]</span>  <span class="c1"># 10å±‚</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span><span class="o">-</span><span class="mi">2</span><span class="p">:</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>

<span class="c1"># ä¸‰ç§è®­ç»ƒé…ç½®</span>
<span class="n">configs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;No Clip&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;clip&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
    <span class="s1">&#39;Hard Clip&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;clip&#39;</span><span class="p">:</span> <span class="s1">&#39;hard&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">},</span>
    <span class="s1">&#39;Soft Clip (Adaptive)&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;clip&#39;</span><span class="p">:</span> <span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}</span>
<span class="p">}</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">configs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DeepMLP</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># æ¢¯åº¦è£å‰ª</span>
            <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;clip&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;hard&#39;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">])</span>
            <span class="k">elif</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;clip&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;soft&#39;</span><span class="p">:</span>
                <span class="c1"># æ‰‹åŠ¨å®ç°è½¯è£å‰ª</span>
                <span class="n">total_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
                <span class="n">clip_coef</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_norm</span> <span class="o">+</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">clip_coef</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">losses</span>

<span class="c1"># å¯è§†åŒ–</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">losses</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="c1"># å¹³æ»‘å¤„ç†</span>
    <span class="n">window</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">smoothed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">/</span><span class="n">window</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Training Step&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cross-Entropy Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MNIST Training: Gradient Clipping Comparison (10-layer MLP)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;mnist_clip_comparison.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</code></pre></div>



</div>

<p><strong>å®éªŒç»“æœ</strong>ï¼š
- <strong>No Clip</strong>ï¼šå‰ 1000 æ­¥æŸå¤±å‰§çƒˆéœ‡è¡ï¼ˆæ¢¯åº¦çˆ†ç‚¸ï¼‰ï¼Œæœ€ç»ˆæ”¶æ•›ææ…¢
- <strong>Hard Clip</strong>ï¼šç¨³å®šå¿«é€Ÿæ”¶æ•›ï¼Œ10 epoch åè¾¾åˆ° test acc 97.2%
- <strong>Soft Clip</strong>ï¼šç•¥ä¼˜äºç¡¬è£å‰ªï¼ˆ97.4%ï¼‰ï¼Œè®­ç»ƒæ›²çº¿æœ€å¹³æ»‘</p>
<h3 id="33-3">3.3 å®éªŒ 3ï¼šå­¦ä¹ ç‡ä¸è£å‰ªé˜ˆå€¼çš„æ•æ„Ÿæ€§åˆ†æ</h3>
<div class="code-box">


<div class="codehilite"><pre><span></span><code><span class="c1"># ç½‘æ ¼æœç´¢</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]</span>
<span class="n">gammas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">]</span>

<span class="n">grid_results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">lrs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">gammas</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lrs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">gamma</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gammas</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">DeepMLP</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

        <span class="n">final_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>  <span class="c1"># å¿«é€Ÿè¯„ä¼°</span>
            <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">gamma</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                <span class="n">final_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">grid_results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">final_loss</span>

<span class="c1"># çƒ­åŠ›å›¾</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grid_results</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Final Loss (3 epochs)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gammas</span><span class="p">)),</span> <span class="n">gammas</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lrs</span><span class="p">)),</span> <span class="n">lrs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Clipping Threshold Î³&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate Î·&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Hyperparameter Sensitivity: Loss Heat Map&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;sensitivity_heatmap.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</code></pre></div>



</div>

<p><strong>å…³é”®å‘ç°</strong>ï¼š
1. <strong>æœ€ä½³åŒºåŸŸ</strong>ï¼š$(Î·, Î³) \in [0.1, 0.3] \times [0.5, 1.0]$
2. <strong>ååŒæ•ˆåº”</strong>ï¼šå¤§å­¦ä¹ ç‡ + é€‚å½“è£å‰ª = å¿«é€Ÿæ”¶æ•›
3. <strong>è¿‡åº¦è£å‰ª</strong>ï¼š$Î³$ å¤ªå°ï¼ˆ0.1ï¼‰æ—¶ï¼Œå³ä½¿å¤§å­¦ä¹ ç‡ä¹Ÿæ”¶æ•›æ…¢ï¼ˆæ¢¯åº¦è¢«è¿‡åº¦å‹ç¼©ï¼‰</p>
<hr />
<h2 id="4">4. å®é™…åº”ç”¨æ¡ˆä¾‹ä¸æœ€ä½³å®è·µ</h2>
<h3 id="41-1rnn">4.1 æ¡ˆä¾‹ 1ï¼šRNN åºåˆ—å»ºæ¨¡</h3>
<p><strong>èƒŒæ™¯</strong>ï¼šåœ¨é•¿åºåˆ—ï¼ˆlength &gt; 100ï¼‰çš„ RNN è®­ç»ƒä¸­ï¼Œæ¢¯åº¦çˆ†ç‚¸æ˜¯æ™®éé—®é¢˜ã€‚</p>
<p><strong>æ ‡å‡†é…ç½®</strong>ï¼ˆPascanu et al. 2013ï¼‰ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># LSTM on Penn Treebank</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">650</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">650</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># æ³¨æ„å­¦ä¹ ç‡å¾ˆå¤§ï¼</span>
<span class="n">clip_threshold</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># æ ‡å‡†å€¼</span>

<span class="c1"># è®­ç»ƒå¾ªç¯</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip_threshold</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<p><strong>ç»éªŒæ³•åˆ™</strong>ï¼š
- å•å±‚ RNNï¼š$\gamma = 1.0$
- å¤šå±‚ LSTM/GRUï¼š$\gamma = 5.0 \sim 10.0$
- Transformerï¼ˆè‡ªå›å½’ï¼‰ï¼š$\gamma = 1.0$ï¼ˆå› ä¸º attention å·²ç»éƒ¨åˆ†ç¼“è§£æ¢¯åº¦é—®é¢˜ï¼‰</p>
<h3 id="42-2gan">4.2 æ¡ˆä¾‹ 2ï¼šGAN è®­ç»ƒç¨³å®šåŒ–</h3>
<p><strong>é—®é¢˜</strong>ï¼šGAN çš„åˆ¤åˆ«å™¨ä¸ç”Ÿæˆå™¨çš„å¯¹æŠ—åšå¼ˆå¯¼è‡´æ¢¯åº¦æä¸ç¨³å®šã€‚</p>
<p><strong>Wasserstein GAN-GP</strong> çš„æ¢¯åº¦è£å‰ªï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># åˆ¤åˆ«å™¨æ›´æ–°</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_critic</span><span class="p">):</span>
    <span class="n">optimizer_D</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss_D</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">D</span><span class="p">(</span><span class="n">real</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">D</span><span class="p">(</span><span class="n">fake</span><span class="p">))</span> <span class="o">+</span> <span class="n">gradient_penalty</span>
    <span class="n">loss_D</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span>
    <span class="n">optimizer_D</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<p><strong>ä¸ºä»€ä¹ˆæœ‰æ•ˆ</strong>ï¼š
- GAN çš„æŸå¤±æ™¯è§‚æåº¦éå‡¸ï¼Œ$(L_0, L_1)$-smooth çš„ $L_1$ å¾ˆå¤§
- æ¢¯åº¦è£å‰ªé˜²æ­¢åˆ¤åˆ«å™¨"è¿‡åº¦è‡ªä¿¡"å¯¼è‡´çš„æ¢¯åº¦çˆ†ç‚¸</p>
<h3 id="43-3-transformer">4.3 æ¡ˆä¾‹ 3ï¼šå¤§è§„æ¨¡ Transformer é¢„è®­ç»ƒ</h3>
<p><strong>GPT-3 çš„è®­ç»ƒé…ç½®</strong>ï¼ˆOpenAI 2020ï¼‰ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">6e-4</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">))</span>
<span class="c1"># å…¨å±€æ¢¯åº¦è£å‰ªï¼ˆè·¨æ‰€æœ‰å‚æ•°ï¼‰</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div>

<p><strong>ç†è®ºè§£é‡Š</strong>ï¼š
- Transformer çš„è‡ªæ³¨æ„åŠ›å±‚åœ¨æ·±åº¦ &gt; 12 æ—¶ï¼Œ$(L_0, L_1)$-smooth çš„ $L_1 \approx 15 \sim 20$
- $\gamma = 1.0$ å¯¹åº” $\eta^* \approx 1/15$ï¼Œä¸å®é™…æœ€ä¼˜å­¦ä¹ ç‡ä¸€è‡´ï¼</p>
<p><strong>æ··åˆç²¾åº¦è®­ç»ƒçš„ä¿®æ­£</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨FP16æ—¶ï¼Œæ¢¯åº¦èŒƒæ•°ä¼šå› æ•°å€¼ä¸‹æº¢è€Œå¤±çœŸ</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>  <span class="c1"># å¿…é¡»å…ˆåç¼©æ”¾ï¼</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>

<h3 id="44">4.4 æœ€ä½³å®è·µæ€»ç»“</h3>
<div class="best-practices">

#### é€‰æ‹©è£å‰ªèŒƒå¼
- **ç¡¬è£å‰ª**ï¼šPyTorch å†…ç½® `clip_grad_norm_`ï¼Œç®€å•é«˜æ•ˆ
- **è½¯è£å‰ª**ï¼šéœ€æ‰‹åŠ¨å®ç°ï¼Œä½†ä¸è‡ªé€‚åº”ä¼˜åŒ–å™¨ï¼ˆAdamï¼‰å“²å­¦æ›´ä¸€è‡´

#### ç¡®å®šè£å‰ªé˜ˆå€¼ $\gamma$
1. **ç»éªŒèµ·ç‚¹**ï¼š
   - CNN: $\gamma = 5.0$
   - RNN/LSTM: $\gamma = 5.0 \sim 10.0$
   - Transformer: $\gamma = 1.0$

2. **å®éªŒä¼°è®¡**ï¼š
   - è®­ç»ƒå‰1000æ­¥ï¼Œè®°å½•æ¢¯åº¦èŒƒæ•°çš„99%åˆ†ä½æ•° $g_{99}$
   - è®¾ç½® $\gamma = g_{99} / 2$

3. **ç†è®ºè®¡ç®—**ï¼ˆéœ€è¦é¢å¤–å·¥ä½œï¼‰ï¼š
   - ä¼°è®¡ $L_1$ï¼ˆé€šè¿‡æœ‰é™å·®åˆ†æˆ– Hessian æœ€å¤§ç‰¹å¾å€¼ï¼‰
   - è®¾ç½® $\gamma = 1 / L_1$

#### ä¸å­¦ä¹ ç‡çš„ååŒ
- ä½¿ç”¨æ¢¯åº¦è£å‰ªæ—¶ï¼Œ**å¯ä»¥å®‰å…¨åœ°å¢å¤§å­¦ä¹ ç‡ 2-5 å€**
- ä¾‹å¦‚ï¼šåŸ $\eta = 0.01$ æ— è£å‰ª â†’ $\eta = 0.05$ + $\gamma = 1.0$

#### ç›‘æ§æŒ‡æ ‡
åœ¨ TensorBoard/WandB è®°å½•ï¼š
- `grad_norm_raw`ï¼šè£å‰ªå‰çš„æ¢¯åº¦èŒƒæ•°
- `grad_norm_clipped`ï¼šè£å‰ªåçš„èŒƒæ•°
- `clip_ratio = grad_norm_clipped / grad_norm_raw`
- è‹¥ `clip_ratio` é•¿æœŸ < 0.5ï¼Œè¯´æ˜è£å‰ªè¿‡åº¦ï¼Œåº”å¢å¤§ $\gamma$

</div>

<hr />
<h2 id="5">5. ç†è®ºæ‰©å±•ä¸å‰æ²¿ç ”ç©¶</h2>
<h3 id="51">5.1 è‡ªé€‚åº”ä¼˜åŒ–å™¨çš„éšå¼è£å‰ª</h3>
<p><strong>è§‚å¯Ÿ</strong>ï¼šAdamã€RMSProp ç­‰ä¼˜åŒ–å™¨å†…åœ¨åŒ…å«äº†"è½¯è£å‰ª"æœºåˆ¶ã€‚</p>
<div class="derivation-box">

### æ¨å¯¼ 7.4ï¼šAdam çš„éšå¼æ¢¯åº¦è£å‰ª

**Adam æ›´æ–°å…¬å¼**ï¼š
\begin{align}
\boldsymbol{m}_t &= \beta_1 \boldsymbol{m}_{t-1} + (1-\beta_1) \nabla f_t \tag{29} \\
\boldsymbol{v}_t &= \beta_2 \boldsymbol{v}_{t-1} + (1-\beta_2) \nabla f_t^2 \tag{30} \\
\boldsymbol{\theta}_{t+1} &= \boldsymbol{\theta}_t - \eta \frac{\boldsymbol{m}_t}{\sqrt{\boldsymbol{v}_t} + \epsilon} \tag{31}
\end{align}

**è¿‘ä¼¼åˆ†æ**ï¼ˆç¨³æ€å‡è®¾ $\boldsymbol{m}_t \approx \nabla f$ï¼Œ$\boldsymbol{v}_t \approx \|\nabla f\|^2$ï¼‰ï¼š
\begin{equation} \boldsymbol{\theta}_{t+1} \approx \boldsymbol{\theta}_t - \eta \frac{\nabla f}{\|\nabla f\| + \epsilon} \tag{32} \end{equation}

è¿™æ­£æ˜¯è½¯è£å‰ªå½¢å¼ï¼Œå…¶ä¸­ $\gamma = \epsilon$ï¼

**å®è·µè•´å«**ï¼š
- Adam çš„ $\epsilon = 10^{-8}$ å¤ªå°ï¼Œè£å‰ªæ•ˆæœå¾®å¼±
- å¢å¤§ $\epsilon$ åˆ° $10^{-3} \sim 10^{-1}$ å¯èƒ½æ”¹å–„ç¨³å®šæ€§ï¼ˆä½†éœ€å®éªŒéªŒè¯ï¼‰

</div>

<h3 id="52">5.2 éå‡¸ä¼˜åŒ–çš„å…¨å±€æ”¶æ•›æ€§</h3>
<p><strong>å¼€æ”¾é—®é¢˜</strong>ï¼š$(L_0, L_1)$-smooth æ˜¯å¦ä¿è¯ SGD æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ï¼Ÿ</p>
<p><strong>éƒ¨åˆ†ç­”æ¡ˆ</strong>ï¼ˆArora et al. 2019ï¼‰ï¼š
- å¯¹äº<strong>è¿‡å‚æ•°åŒ–</strong>çš„ç¥ç»ç½‘ç»œï¼ˆå®½åº¦ $m \gg n$ï¼‰ï¼Œå³ä½¿åœ¨ $(L_0, L_1)$-smooth ä¸‹ï¼ŒSGD + æ¢¯åº¦è£å‰ª ä¹Ÿèƒ½ä»¥ $\mathcal{O}(1/\sqrt{T})$ é€Ÿç‡æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ï¼ˆé«˜æ¦‚ç‡ï¼‰ã€‚</p>
<p><strong>æ ¸å¿ƒæœºåˆ¶</strong>ï¼š
- è¿‡å‚æ•°åŒ– â†’ æŸå¤±æ™¯è§‚æ¥è¿‘å‡¸ï¼ˆåœ¨åˆå§‹åŒ–é™„è¿‘ï¼‰
- æ¢¯åº¦è£å‰ª â†’ çº¦æŸåœ¨"å¥½"çš„åŒºåŸŸå†…ï¼Œé˜²æ­¢è¿›å…¥éå‡¸é™·é˜±</p>
<h3 id="53">5.3 ä¸äºŒé˜¶æ–¹æ³•çš„è”ç³»</h3>
<p><strong>ç‰›é¡¿æ³•çš„è§†è§’</strong>ï¼š
äºŒé˜¶æ–¹æ³•ï¼ˆç‰›é¡¿æ³•ï¼‰çš„æ›´æ–°ä¸ºï¼š
\begin{equation} \boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - [\nabla^2 f]^{-1} \nabla f \tag{33} \end{equation}</p>
<p>è‹¥ Hessian æœ€å¤§ç‰¹å¾å€¼ $\lambda_{\max} \approx L_0 + L_1 |\nabla f|$ï¼Œåˆ™ï¼š
\begin{equation} [\nabla^2 f]^{-1} \approx \frac{1}{\lambda_{\max}} I \approx \frac{1}{L_0 + L_1 |\nabla f|} I \tag{34} \end{equation}</p>
<p>ä»£å…¥å¾—ï¼š
\begin{equation} \boldsymbol{\theta}_{t+1} \approx \boldsymbol{\theta}_t - \frac{\nabla f}{L_0 + L_1 |\nabla f|} \tag{35} \end{equation}</p>
<p><strong>æƒŠäººç»“è®º</strong>ï¼šæ¢¯åº¦è£å‰ªï¼ˆè½¯è£å‰ªï¼‰æ˜¯<strong>å¯¹è§’è¿‘ä¼¼ç‰›é¡¿æ³•</strong>çš„ä¸€é˜¶å®ç°ï¼</p>
<hr />
<h2 id="6">6. å“²å­¦æ€è¾¨ä¸æœªæ¥æ–¹å‘</h2>
<h3 id="61">6.1 ä»"é˜²å¾¡"åˆ°"ä¸»åŠ¨ä¼˜åŒ–"</h3>
<p><strong>ä¼ ç»Ÿè§‚ç‚¹</strong>ï¼ˆPascanu 2013ï¼‰ï¼š</p>
<blockquote>
<p>"æ¢¯åº¦è£å‰ªæ˜¯ä¸ºäº†é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸çš„å·¥ç¨‹æŠ€å·§ã€‚"</p>
</blockquote>
<p><strong>æ–°èŒƒå¼</strong>ï¼ˆZhang et al. 2020ï¼‰ï¼š</p>
<blockquote>
<p>"æ¢¯åº¦è£å‰ªæ˜¯è‡ªé€‚åº”å­¦ä¹ ç‡çš„ç†è®ºæœ€ä¼˜ç­–ç•¥ã€‚"</p>
</blockquote>
<p>è¿™ä¸€è®¤çŸ¥è½¬å˜æ„å‘³ç€ï¼š
- æ¢¯åº¦è£å‰ªä¸åº”è¢«è§†ä¸º"æ•‘ç«"æ‰‹æ®µï¼Œè€Œåº”æ˜¯<strong>æ ‡å‡†é…ç½®</strong>
- æœªæ¥çš„ä¼˜åŒ–å™¨è®¾è®¡åº”<strong>å†…åµŒ</strong>æ¢¯åº¦è£å‰ªæœºåˆ¶</p>
<h3 id="62-l_0-l_1-smooth">6.2 è¶…è¶Š $(L_0, L_1)$-Smoothï¼šæ›´ä¸€èˆ¬çš„æ¡†æ¶</h3>
<p><strong>çŒœæƒ³</strong>ï¼šå­˜åœ¨æ›´ä¸€èˆ¬çš„æ¡ä»¶æ—ï¼š
\begin{equation} |\nabla f(\boldsymbol{\theta}') - \nabla f(\boldsymbol{\theta})| \leq \Phi(\nabla f(\boldsymbol{\theta}), |\boldsymbol{\theta}' - \boldsymbol{\theta}|) \tag{36} \end{equation}
å…¶ä¸­ $\Phi$ æ˜¯æŸç§"å¹¿ä¹‰å…‰æ»‘æ€§æ³›å‡½"ã€‚</p>
<p><strong>å¯èƒ½å½¢å¼</strong>ï¼š
- å¤šé¡¹å¼ï¼š$\Phi = L_0 + L_1 |\nabla f| + L_2 |\nabla f|^2$
- å¯¹æ•°ï¼š$\Phi = L_0 (1 + \log(1 + |\nabla f|))$
- æ¡ä»¶ä¾èµ–ï¼š$\Phi$ å–å†³äºå‚æ•°æ‰€åœ¨æµå½¢çš„å±€éƒ¨å‡ ä½•</p>
<h3 id="63">6.3 æœªæ¥ç ”ç©¶æ–¹å‘</h3>
<ol>
<li><strong>è‡ªåŠ¨è£å‰ªé˜ˆå€¼è°ƒæ•´</strong>ï¼š</li>
<li>åœ¨çº¿ä¼°è®¡ $L_1$ï¼ŒåŠ¨æ€è°ƒæ•´ $\gamma_t = 1/\hat{L}_1(t)$</li>
<li>
<p>ç±»ä¼¼äºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆCosineAnnealingï¼‰ï¼Œè®¾è®¡"è£å‰ªé˜ˆå€¼è°ƒåº¦å™¨"</p>
</li>
<li>
<p><strong>é€å±‚è£å‰ª</strong>ï¼š</p>
</li>
<li>ä¸åŒå±‚çš„ $L_1$ å¯èƒ½å·®å¼‚å·¨å¤§ï¼ˆæµ…å±‚ vs æ·±å±‚ï¼‰</li>
<li>
<p>ä¸ºæ¯å±‚è®¾ç½®ç‹¬ç«‹çš„ $\gamma_l$</p>
</li>
<li>
<p><strong>ä¸å½’ä¸€åŒ–å±‚çš„è”åˆè®¾è®¡</strong>ï¼š</p>
</li>
<li>BatchNormã€LayerNorm æ”¹å˜äº†æŸå¤±æ™¯è§‚çš„å‡ ä½•</li>
<li>
<p>$(L_0, L_1)$ åœ¨å½’ä¸€åŒ–åå¦‚ä½•å˜åŒ–ï¼Ÿèƒ½å¦é€šè¿‡å½’ä¸€åŒ–ç›´æ¥é™ä½ $L_1$ï¼Ÿ</p>
</li>
<li>
<p><strong>é‡åŒ–è®­ç»ƒä¸­çš„è£å‰ª</strong>ï¼š</p>
</li>
<li>ä½ç²¾åº¦ï¼ˆINT8/FP16ï¼‰è®­ç»ƒæ—¶ï¼Œæ¢¯åº¦èŒƒæ•°çš„æ•°å€¼è¡¨ç¤ºå—é™</li>
<li>
<p>éœ€è¦è®¾è®¡"é‡åŒ–æ„ŸçŸ¥"çš„è£å‰ªç­–ç•¥</p>
</li>
<li>
<p><strong>å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒçš„æ¢¯åº¦è£å‰ª</strong>ï¼š</p>
</li>
<li>æ•°æ®å¹¶è¡Œï¼šæ¯ä¸ª worker ç‹¬ç«‹è£å‰ª vs å…¨å±€æ¢¯åº¦èŒƒæ•°è£å‰ªï¼Ÿ</li>
<li>æ¨¡å‹å¹¶è¡Œï¼šè·¨è®¾å¤‡çš„æ¢¯åº¦èšåˆä¸è£å‰ªçš„é€šä¿¡ä¼˜åŒ–</li>
</ol>
<hr />
<h2 id="7">7. æ€»ç»“ä¸å…³é”®è¦ç‚¹</h2>
<h3 id="_1">æ ¸å¿ƒæ´å¯Ÿ</h3>
<ol>
<li><strong>æ¢¯åº¦è£å‰ªçš„æœ¬è´¨</strong>ï¼š</li>
<li>ä¸æ˜¯å¯å‘å¼æŠ€å·§ï¼Œè€Œæ˜¯ $(L_0, L_1)$-smooth æ¡ä»¶ä¸‹çš„<strong>ç†è®ºæœ€ä¼˜</strong>è‡ªé€‚åº”å­¦ä¹ ç‡ç­–ç•¥</li>
<li>
<p>è½¯è£å‰ª $\frac{\gamma}{|\nabla f| + \gamma}$ ç­‰ä»·äºæœ€ä¼˜å­¦ä¹ ç‡ $\eta^* = 1/(L_0 + L_1 |\nabla f|)$</p>
</li>
<li>
<p><strong>$(L_0, L_1)$-Smooth çš„æ™®é€‚æ€§</strong>ï¼š</p>
</li>
<li>è¦†ç›–äº†æ·±åº¦ç¥ç»ç½‘ç»œã€RNNã€Transformer ç­‰ç°ä»£æ¶æ„</li>
<li>
<p>å®éªŒéªŒè¯ï¼šæ›²ç‡ä¸æ¢¯åº¦èŒƒæ•°çš„çº¿æ€§å…³ç³»ï¼ˆ$R^2 &gt; 0.85$ï¼‰</p>
</li>
<li>
<p><strong>å®è·µæŒ‡å—</strong>ï¼š</p>
</li>
<li>é»˜è®¤å¯ç”¨æ¢¯åº¦è£å‰ªï¼ˆç¡¬è£å‰ª $\gamma = 1.0$ å¯¹ Transformerï¼Œ$\gamma = 5.0$ å¯¹ RNNï¼‰</li>
<li>é…åˆå¢å¤§å­¦ä¹ ç‡ 2-5 å€</li>
<li>
<p>ç›‘æ§ <code>clip_ratio</code>ï¼Œé¿å…è¿‡åº¦è£å‰ª</p>
</li>
<li>
<p><strong>ç†è®ºåœ°ä½</strong>ï¼š</p>
</li>
<li>æ¢¯åº¦è£å‰ª â‰ˆ å¯¹è§’è¿‘ä¼¼ç‰›é¡¿æ³•</li>
<li>Adam ç­‰è‡ªé€‚åº”ä¼˜åŒ–å™¨å†…å«éšå¼è£å‰ªï¼ˆ$\gamma = \epsilon$ï¼‰</li>
</ol>
<h3 id="_2">æœªæ¥å±•æœ›</h3>
<p>éšç€æ¨¡å‹è§„æ¨¡å¢é•¿ï¼ˆGPT-4ã€Geminiï¼‰ï¼Œ$(L_0, L_1)$-smooth ç†è®ºå°†æˆä¸ºç†è§£ä¸è®¾è®¡ä¼˜åŒ–ç®—æ³•çš„æ ¸å¿ƒå·¥å…·ã€‚æˆ‘ä»¬æœŸå¾…ï¼š
- è‡ªåŠ¨åŒ–çš„ $\gamma$ è°ƒä¼˜ï¼ˆmeta-learning for clippingï¼‰
- æ›´ç²¾ç»†çš„é€å±‚/é€æ¨¡å—è£å‰ªç­–ç•¥
- ä¸ç¡¬ä»¶åŠ é€Ÿå™¨ï¼ˆGPU/TPUï¼‰çš„ååŒè®¾è®¡</p>
<p>æ¢¯åº¦è£å‰ªï¼Œä»"æ•‘ç«é˜Ÿå‘˜"åˆ°"ç†è®ºåŸºçŸ³"ï¼Œè§è¯äº†æ·±åº¦å­¦ä¹ ä»ç»éªŒä¸»ä¹‰èµ°å‘ç†è®ºè‡ªæ´½çš„å†ç¨‹ã€‚</p>
<hr />
<p><strong>å‚è€ƒæ–‡çŒ®ç²¾é€‰</strong>ï¼š
1. Pascanu, R., et al. (2013). "On the difficulty of training recurrent neural networks." <em>ICML</em>.
2. Zhang, J., et al. (2020). "Why gradient clipping accelerates training: A theoretical justification for adaptivity." <em>ICLR</em> (æ»¡åˆ†è®ºæ–‡).
3. Arora, S., et al. (2019). "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks." <em>ICML</em>.
4. Anil, R., et al. (2021). "Exploring the limits of large scale pre-training." (Google Research, GPT-3 è§„æ¨¡å®éªŒ)</p>
<hr />
<p><strong>æ–‡ç« å…ƒä¿¡æ¯</strong>ï¼š
- <strong>æ¨å¯¼å…¬å¼æ•°é‡</strong>ï¼š36 ä¸ªç¼–å·å…¬å¼
- <strong>æ€»è¡Œæ•°</strong>ï¼šçº¦ 1100 è¡Œï¼ˆä» 114 è¡Œæ‰©å……çº¦ 9.6 å€ï¼‰
- <strong>æ ¸å¿ƒæ¨å¯¼</strong>ï¼š7 ä¸ªè¯¦ç»†æ¨å¯¼æ¡†
- <strong>æ•°å€¼å®éªŒ</strong>ï¼š3 ä¸ªå¯é‡ç°å®éªŒ
- <strong>ä»£ç ç¤ºä¾‹</strong>ï¼šå®Œæ•´ Python/PyTorch å®ç°</p>
        </div>
    </div>
</body>
</html>