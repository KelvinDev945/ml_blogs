<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ä¸ºä»€ä¹ˆPre Normçš„æ•ˆæœä¸å¦‚Post Normï¼Ÿ</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>ä¸ºä»€ä¹ˆPre Normçš„æ•ˆæœä¸å¦‚Post Normï¼Ÿ</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2025-11-27 | ğŸ“„ å¤§å°: 27.9 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/9009">https://spaces.ac.cn/archives/9009</a></p>
<p><strong>å‘å¸ƒæ—¥æœŸ</strong>: </p>
<hr />
<p>Pre Normä¸Post Normä¹‹é—´çš„å¯¹æ¯”æ˜¯ä¸€ä¸ªâ€œè€ç”Ÿå¸¸è°ˆâ€çš„è¯é¢˜äº†ï¼Œæœ¬åšå®¢å°±å¤šæ¬¡è®¨è®ºè¿‡è¿™ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚æ–‡ç« <a href="/archives/8620">ã€Šæµ…è°ˆTransformerçš„åˆå§‹åŒ–ã€å‚æ•°åŒ–ä¸æ ‡å‡†åŒ–ã€‹</a>ã€<a href="/archives/8747">ã€Šæ¨¡å‹ä¼˜åŒ–æ¼«è°ˆï¼šBERTçš„åˆå§‹æ ‡å‡†å·®ä¸ºä»€ä¹ˆæ˜¯0.02ï¼Ÿã€‹</a>ç­‰ã€‚ç›®å‰æ¯”è¾ƒæ˜ç¡®çš„ç»“è®ºæ˜¯ï¼šåŒä¸€è®¾ç½®ä¹‹ä¸‹ï¼ŒPre Normç»“æ„å¾€å¾€æ›´å®¹æ˜“è®­ç»ƒï¼Œä½†æœ€ç»ˆæ•ˆæœé€šå¸¸ä¸å¦‚Post Normã€‚Pre Normæ›´å®¹æ˜“è®­ç»ƒå¥½ç†è§£ï¼Œå› ä¸ºå®ƒçš„æ’ç­‰è·¯å¾„æ›´çªå‡ºï¼Œä½†ä¸ºä»€ä¹ˆå®ƒæ•ˆæœåè€Œæ²¡é‚£ä¹ˆå¥½å‘¢ï¼Ÿ</p>
<p>ç¬”è€…ä¹‹å‰ä¹Ÿä¸€ç›´æ²¡æœ‰å¥½çš„ç­”æ¡ˆï¼Œç›´åˆ°å‰äº›æ—¶é—´åœ¨çŸ¥ä¹ä¸Šçœ‹åˆ° <a href="https://www.zhihu.com/question/519668254/answer/2371885202">@å”ç¿”æ˜Š</a> çš„ä¸€ä¸ªå›å¤åæ‰â€œæç„¶å¤§æ‚Ÿâ€ï¼ŒåŸæ¥è¿™ä¸ªé—®é¢˜ç«Ÿç„¶æœ‰ä¸€ä¸ªéå¸¸ç›´è§‚çš„ç†è§£ï¼æœ¬æ–‡è®©æˆ‘ä»¬ä¸€èµ·æ¥å­¦ä¹ ä¸€ä¸‹ã€‚</p>
<h2 id="_1">åŸºæœ¬ç»“è®º</h2>
<p>Pre Normå’ŒPost Normçš„å¼å­åˆ†åˆ«å¦‚ä¸‹ï¼š<br />
\begin{align}<br />
\text{Pre Norm: } \quad \boldsymbol{x}<em t_1="t+1">{t+1} = \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t))\\\<br />
\text{Post Norm: }\quad \boldsymbol{x}</em>_t))} = \text{Norm}(\boldsymbol{x}_t + F_t(\boldsymbol{x<br />
\end{align}<br />
åœ¨Transformerä¸­ï¼Œè¿™é‡Œçš„$\text{Norm}$ä¸»è¦æŒ‡Layer Normalizationï¼Œä½†åœ¨ä¸€èˆ¬çš„æ¨¡å‹ä¸­ï¼Œå®ƒä¹Ÿå¯ä»¥æ˜¯Batch Normalizationã€Instance Normalizationç­‰ï¼Œç›¸å…³ç»“è®ºæœ¬è´¨ä¸Šæ˜¯é€šç”¨çš„ã€‚</p>
<p>åœ¨ç¬”è€…æ‰¾åˆ°çš„èµ„æ–™ä¸­ï¼Œæ˜¾ç¤ºPost Normä¼˜äºPre Normçš„å·¥ä½œæœ‰ä¸¤ç¯‡ï¼Œä¸€ç¯‡æ˜¯<a href="https://papers.cool/arxiv/2004.08249">ã€ŠUnderstanding the Difficulty of Training Transformersã€‹</a>ï¼Œä¸€ç¯‡æ˜¯<a href="https://papers.cool/arxiv/2012.11747">ã€ŠRealFormer: Transformer Likes Residual Attentionã€‹</a>ã€‚å¦å¤–ï¼Œç¬”è€…è‡ªå·±ä¹Ÿåšè¿‡å¯¹æ¯”å®éªŒï¼Œæ˜¾ç¤ºPost Normçš„ç»“æ„è¿ç§»æ€§èƒ½æ›´åŠ å¥½ï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨Pretrainingä¸­ï¼ŒPre Normå’ŒPost Norméƒ½èƒ½åšåˆ°å¤§è‡´ç›¸åŒçš„ç»“æœï¼Œä½†æ˜¯Post Normçš„Finetuneæ•ˆæœæ˜æ˜¾æ›´å¥½ã€‚</p>
<p>å¯èƒ½è¯»è€…ä¼šåé—®<a href="https://papers.cool/arxiv/2002.04745">ã€ŠOn Layer Normalization in the Transformer Architectureã€‹</a>ä¸æ˜¯æ˜¾ç¤ºPre Normè¦å¥½äºPost Normå—ï¼Ÿè¿™æ˜¯ä¸æ˜¯çŸ›ç›¾äº†ï¼Ÿå…¶å®è¿™ç¯‡æ–‡ç« æ¯”è¾ƒçš„æ˜¯åœ¨å®Œå…¨ç›¸åŒçš„è®­ç»ƒè®¾ç½®ä¸‹Pre Normçš„æ•ˆæœè¦ä¼˜äºPost Normï¼Œè¿™åªèƒ½æ˜¾ç¤ºå‡ºPre Normæ›´å®¹æ˜“è®­ç»ƒï¼Œå› ä¸ºPost Normè¦è¾¾åˆ°è‡ªå·±çš„æœ€ä¼˜æ•ˆæœï¼Œä¸èƒ½ç”¨è·ŸPre Normä¸€æ ·çš„è®­ç»ƒé…ç½®ï¼ˆæ¯”å¦‚Pre Normå¯ä»¥ä¸åŠ Warmupä½†Post Normé€šå¸¸è¦åŠ ï¼‰ï¼Œæ‰€ä»¥ç»“è®ºå¹¶ä¸çŸ›ç›¾ã€‚</p>
<h2 id="_2">ç›´è§‚ç†è§£</h2>
<p>ä¸ºä»€ä¹ˆPre Normçš„æ•ˆæœä¸å¦‚Post Normï¼ŸçŸ¥ä¹ä¸Š <a href="https://www.zhihu.com/question/519668254/answer/2371885202">@å”ç¿”æ˜Š</a> ç»™å‡ºçš„ç­”æ¡ˆæ˜¯ï¼šPre Normçš„æ·±åº¦æœ‰â€œæ°´åˆ†â€ï¼ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€ä¸ª$L$å±‚çš„Pre Normæ¨¡å‹ï¼Œå…¶å®é™…ç­‰æ•ˆå±‚æ•°ä¸å¦‚$L$å±‚çš„Post Normæ¨¡å‹ï¼Œè€Œå±‚æ•°å°‘äº†å¯¼è‡´æ•ˆæœå˜å·®äº†ã€‚</p>
<p>å…·ä½“æ€ä¹ˆç†è§£å‘¢ï¼Ÿå¾ˆç®€å•ï¼Œå¯¹äºPre Normæ¨¡å‹æˆ‘ä»¬è¿­ä»£å¾—åˆ°ï¼š<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{x}<em t-1="t-1">{t+1} =&amp;\,\boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t)) \\\<br />
=&amp;\, \boldsymbol{x}</em>} + F_{t-1}(\text{Norm}(\boldsymbol{x<em t-1="t-1">{t-1})) + F_t(\text{Norm}(\boldsymbol{x}_t)) \\\<br />
=&amp;\, \cdots \\\<br />
=&amp;\, \boldsymbol{x}_0 + F_0 (\text{Norm}(\boldsymbol{x}_0)) + \cdots + F</em>}(\text{Norm}(\boldsymbol{x<em t_1="t+1">{t-1})) + F_t(\text{Norm}(\boldsymbol{x}_t))<br />
\end{aligned}\end{equation}<br />
å…¶ä¸­æ¯ä¸€é¡¹éƒ½æ˜¯åŒä¸€é‡çº§çš„ï¼Œé‚£ä¹ˆæœ‰$\boldsymbol{x}</em>(t+1)$ï¼Œä¹Ÿå°±æ˜¯è¯´ç¬¬$t+1$å±‚è·Ÿç¬¬$t$å±‚çš„å·®åˆ«å°±ç›¸å½“äº$t+1$ä¸$t$çš„å·®åˆ«ï¼Œå½“$t$è¾ƒå¤§æ—¶ï¼Œä¸¤è€…çš„ç›¸å¯¹å·®åˆ«æ˜¯å¾ˆå°çš„ï¼Œå› æ­¤}=\mathcal{O<br />
\begin{equation}\begin{aligned}<br />
&amp;\,F_t(\text{Norm}(\boldsymbol{x}<em t_1="t+1">t)) + F</em>}(\text{Norm}(\boldsymbol{x<em t_1="t+1">{t+1})) \\\<br />
\approx&amp;\,F_t(\text{Norm}(\boldsymbol{x}_t)) + F</em>}(\text{Norm}(\boldsymbol{x<em t_1="t+1">t)) \\\<br />
=&amp;\, \begin{pmatrix} 1 &amp; 1\end{pmatrix}\begin{pmatrix} F_t \\\ F</em>}\end{pmatrix}(\text{Norm}(\boldsymbol{x<em t_1="t+1">t))<br />
\end{aligned}\end{equation}<br />
è¿™ä¸ªæ„æ€æ˜¯è¯´ï¼Œå½“$t$æ¯”è¾ƒå¤§æ—¶ï¼Œ$\boldsymbol{x}_t,\boldsymbol{x}</em>}$ç›¸å·®è¾ƒå°ï¼Œæ‰€ä»¥$F_{t+1}(\text{Norm}(\boldsymbol{x<em t_1="t+1">{t+1}))$ä¸$F</em>_t))$å¾ˆæ¥è¿‘ï¼Œå› æ­¤åŸæœ¬ä¸€ä¸ª$t$å±‚çš„æ¨¡å‹ä¸$t+1$å±‚å’Œï¼Œè¿‘ä¼¼ç­‰æ•ˆäºä¸€ä¸ªæ›´å®½çš„$t$å±‚æ¨¡å‹ï¼Œæ‰€ä»¥åœ¨Pre Normä¸­å¤šå±‚å åŠ çš„ç»“æœæ›´å¤šæ˜¯å¢åŠ å®½åº¦è€Œä¸æ˜¯æ·±åº¦ï¼Œå±‚æ•°è¶Šå¤šï¼Œè¿™ä¸ªå±‚å°±è¶Šâ€œè™šâ€ã€‚}(\text{Norm}(\boldsymbol{x</p>
<p>è¯´ç™½äº†ï¼ŒPre Normç»“æ„æ— å½¢åœ°å¢åŠ äº†æ¨¡å‹çš„å®½åº¦è€Œé™ä½äº†æ¨¡å‹çš„æ·±åº¦ï¼Œè€Œæˆ‘ä»¬çŸ¥é“æ·±åº¦é€šå¸¸æ¯”å®½åº¦æ›´é‡è¦ï¼Œæ‰€ä»¥æ˜¯æ— å½¢ä¹‹ä¸­çš„é™ä½æ·±åº¦å¯¼è‡´æœ€ç»ˆæ•ˆæœå˜å·®äº†ã€‚è€ŒPost Normåˆšåˆšç›¸åï¼Œåœ¨<a href="/archives/8620">ã€Šæµ…è°ˆTransformerçš„åˆå§‹åŒ–ã€å‚æ•°åŒ–ä¸æ ‡å‡†åŒ–ã€‹</a>ä¸­æˆ‘ä»¬å°±åˆ†æè¿‡ï¼Œå®ƒæ¯Normä¸€æ¬¡å°±å‰Šå¼±ä¸€æ¬¡æ’ç­‰åˆ†æ”¯çš„æƒé‡ï¼Œæ‰€ä»¥Post Normåè€Œæ˜¯æ›´çªå‡ºæ®‹å·®åˆ†æ”¯çš„ï¼Œå› æ­¤Post Normä¸­çš„å±‚æ•°æ›´åŠ â€œè¶³ç§¤â€ï¼Œä¸€æ—¦è®­ç»ƒå¥½ä¹‹åæ•ˆæœæ›´ä¼˜ã€‚</p>
<h2 id="_3">ç›¸å…³å·¥ä½œ</h2>
<p>å‰æ®µæ—¶é—´å·ç§°èƒ½è®­ç»ƒ1000å±‚Transformerçš„<a href="/archives/8978">DeepNet</a>æƒ³å¿…ä¸å°‘è¯»è€…éƒ½å¬è¯´è¿‡ï¼Œåœ¨å…¶è®ºæ–‡<a href="https://papers.cool/arxiv/2203.00555">ã€ŠDeepNet: Scaling Transformers to 1,000 Layersã€‹</a>ä¸­å¯¹Pre Normçš„æè¿°æ˜¯ï¼š</p>
<blockquote>
<p>However, the gradients of Pre-LN at bottom layers tend to be larger than at top layers, leading to a degradation in performance compared with Post-LN.</p>
</blockquote>
<p>ä¸å°‘è¯»è€…å½“æ—¶å¯èƒ½å¹¶ä¸ç†è§£è¿™æ®µè¯çš„é€»è¾‘å…³ç³»ï¼Œä½†çœ‹äº†å‰ä¸€èŠ‚å†…å®¹çš„è§£é‡Šåï¼Œæƒ³å¿…ä¼šæœ‰æ–°çš„ç†è§£ã€‚</p>
<p>ç®€å•æ¥è¯´ï¼Œæ‰€è°“â€œthe gradients of Pre-LN at bottom layers tend to be larger than at top layersâ€ï¼Œå°±æ˜¯æŒ‡Pre Normç»“æ„ä¼šè¿‡åº¦å€¾å‘äºæ’ç­‰åˆ†æ”¯ï¼ˆbottom layersï¼‰ï¼Œä»è€Œä½¿å¾—Pre Normå€¾å‘äºé€€åŒ–ï¼ˆdegradationï¼‰ä¸ºä¸€ä¸ªâ€œæµ…è€Œå®½â€çš„æ¨¡å‹ï¼Œæœ€ç»ˆä¸å¦‚åŒä¸€æ·±åº¦çš„Post Normã€‚è¿™è·Ÿå‰é¢çš„ç›´è§‚ç†è§£æœ¬è´¨ä¸Šæ˜¯ä¸€è‡´çš„ã€‚</p>
<h2 id="_4">æ–‡ç« å°ç»“</h2>
<p>æœ¬æ–‡ä¸»è¦åˆ†äº«äº†â€œä¸ºä»€ä¹ˆPre Normçš„æ•ˆæœä¸å¦‚Post Normâ€çš„ä¸€ä¸ªç›´è§‚ç†è§£ã€‚</p>
<p><em><strong>è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š</strong><a href="https://spaces.ac.cn/archives/9009">https://spaces.ac.cn/archives/9009</a></em></p>
<p><em><strong>æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="ã€Šç§‘å­¦ç©ºé—´FAQã€‹">ã€Šç§‘å­¦ç©ºé—´FAQã€‹</a></p>
<p><strong>å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚</strong></p>
<p><strong>å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼</strong></p>
<p>æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>å¾®ä¿¡æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>æ”¯ä»˜å®æ‰“èµ</p>
<p>å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>ç‚¹å‡»è¿™é‡Œ</strong></a>æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚</p>
<p><strong>å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š</strong></p>
<p>è‹å‰‘æ—. (Mar. 29, 2022). ã€Šä¸ºä»€ä¹ˆPre Normçš„æ•ˆæœä¸å¦‚Post Normï¼Ÿ ã€‹[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9009">https://spaces.ac.cn/archives/9009</a></p>
<p>@online{kexuefm-9009,<br />
title={ä¸ºä»€ä¹ˆPre Normçš„æ•ˆæœä¸å¦‚Post Normï¼Ÿ},<br />
author={è‹å‰‘æ—},<br />
year={2022},<br />
month={Mar},<br />
url={\url{https://spaces.ac.cn/archives/9009}},<br />
} </p>
<hr />
<h2 id="_5">å…¬å¼æ¨å¯¼ä¸æ³¨é‡Š</h2>
<h3 id="1">ç¬¬1éƒ¨åˆ†ï¼šæ ¸å¿ƒç†è®ºã€å…¬ç†ä¸å†å²åŸºç¡€</h3>
<h4 id="11">1.1 ç†è®ºèµ·æºä¸å†å²å‘å±•</h4>
<p><strong>å½’ä¸€åŒ–ä¸æ®‹å·®è¿æ¥çš„ç†è®ºæ ¹æº</strong>å¯è¿½æº¯åˆ°å¤šä¸ªç ”ç©¶æ–¹å‘çš„èåˆï¼š</p>
<div class="theorem-box">

**å¤šå­¦ç§‘äº¤å‰**ï¼š
- **æ‰¹å½’ä¸€åŒ–ç†è®º** (2015, Ioffe & Szegedy)ï¼šé¦–æ¬¡åœ¨æ·±åº¦ç½‘ç»œä¸­ç³»ç»Ÿå¼•å…¥å½’ä¸€åŒ–
- **æ®‹å·®å­¦ä¹ ** (2015, He et al.)ï¼šResNeté€šè¿‡skip connectionè§£å†³æ¢¯åº¦æ¶ˆå¤±
- **Layer Normalization** (2016, Ba et al.)ï¼šé’ˆå¯¹RNNå’Œå°batchåœºæ™¯çš„å½’ä¸€åŒ–æ–¹æ¡ˆ
- **Transformeræ¶æ„** (2017, Vaswani et al.)ï¼šé¦–æ¬¡åœ¨Attentionæœºåˆ¶ä¸­ä½¿ç”¨Layer Norm
- **å½’ä¸€åŒ–ä½ç½®ç ”ç©¶** (2018-2020)ï¼šå¼€å§‹ç³»ç»Ÿç ”ç©¶Pre/Post Normçš„å·®å¼‚

</div>

<p><strong>å…³é”®é‡Œç¨‹ç¢‘</strong>ï¼š</p>
<ol>
<li><strong>2015 - Batch Normalization</strong>ï¼šè¯æ˜å½’ä¸€åŒ–èƒ½åŠ é€Ÿè®­ç»ƒã€æå‡æ³›åŒ–</li>
<li><strong>2015 - ResNet</strong>ï¼šæ®‹å·®è¿æ¥ä½¿å¾—è®­ç»ƒè¶…æ·±ç½‘ç»œæˆä¸ºå¯èƒ½ï¼ˆ152å±‚ï¼‰</li>
<li><strong>2016 - Layer Normalization</strong>ï¼šç‹¬ç«‹äºbatchç»´åº¦çš„å½’ä¸€åŒ–ï¼Œé€‚ç”¨äºåºåˆ—æ¨¡å‹</li>
<li><strong>2017 - åŸå§‹Transformer</strong>ï¼šé‡‡ç”¨Post Normç»“æ„</li>
<li><strong>2020 - Pre Normçš„æµè¡Œ</strong>ï¼šGPTã€BERTç­‰æ¨¡å‹å¼€å§‹é‡‡ç”¨Pre Normä»¥æå‡è®­ç»ƒç¨³å®šæ€§</li>
<li><strong>2020 - æ·±å…¥å¯¹æ¯”ç ”ç©¶</strong>ï¼šå¤šç¯‡è®ºæ–‡ç³»ç»Ÿå¯¹æ¯”Pre/Post Normçš„æ€§èƒ½å·®å¼‚</li>
<li><strong>2022 - DeepNet</strong>ï¼šé€šè¿‡æ®‹å·®ç¼©æ”¾å®ç°1000å±‚Transformer</li>
</ol>
<h4 id="12">1.2 æ•°å­¦å…¬ç†ä¸åŸºç¡€å‡è®¾</h4>
<div class="theorem-box">

### å…¬ç†1ï¼šæ®‹å·®è¿æ¥çš„æ’ç­‰æ˜ å°„å‡è®¾

æ·±åº¦ç½‘ç»œä¸­çš„æ®‹å·®è¿æ¥åº”æä¾›ä¸€æ¡**æ— éšœç¢çš„æ¢¯åº¦é€šè·¯**ï¼š

$$\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + \mathcal{R}_t(\boldsymbol{x}_t)$$

å…¶ä¸­$\mathcal{R}_t$æ˜¯æ®‹å·®å‡½æ•°ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼š
- å‰å‘ï¼š$\mathcal{R}_t = \mathbf{0}$æ—¶ç½‘ç»œé€€åŒ–ä¸ºæ’ç­‰æ˜ å°„ï¼ˆä¸æŸå®³æ€§èƒ½ï¼‰
- åå‘ï¼š$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_t}$åŒ…å«ç›´æ¥æ¥è‡ª$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_{t+1}}$çš„æ’ç­‰åˆ†é‡

</div>

<div class="theorem-box">

### å…¬ç†2ï¼šå½’ä¸€åŒ–çš„åå˜æ€§åŸåˆ™

Layer Normalizationåº”ä¿æŒè¡¨ç¤ºçš„**æ–¹å‘ä¸å˜æ€§**ï¼š

$$\text{LN}(\alpha \boldsymbol{x}) = \text{LN}(\boldsymbol{x}), \quad \forall \alpha > 0$$

**æ„ä¹‰**ï¼šå½’ä¸€åŒ–å»é™¤å¹…å€¼ä¿¡æ¯ï¼Œåªä¿ç•™æ–¹å‘ä¿¡æ¯ï¼Œä½¿å¾—ç½‘ç»œå­¦ä¹ å¯¹å°ºåº¦é²æ£’çš„ç‰¹å¾ã€‚

</div>

<div class="theorem-box">

### å…¬ç†3ï¼šæ·±åº¦ç½‘ç»œçš„å±‚çº§åŒ–è¡¨ç¤ºå‡è®¾

æ·±åº¦ç½‘ç»œåº”å­¦ä¹ **å±‚çº§åŒ–çš„ç‰¹å¾æŠ½è±¡**ï¼š

$$\text{feature}_t = h_t(\text{feature}_{t-1})$$

å…¶ä¸­$h_t$å°†æµ…å±‚ç‰¹å¾æ˜ å°„ä¸ºæ›´æŠ½è±¡çš„é«˜å±‚ç‰¹å¾ã€‚

**Pre Normçš„é—®é¢˜**ï¼šæ’ç­‰è·¯å¾„è¿‡å¼º â†’ é˜»ç¢å±‚çº§åŒ–å­¦ä¹ 
**Post Normçš„ä¼˜åŠ¿**ï¼šå‰Šå¼±æ’ç­‰ â†’ å¼ºåˆ¶æ¯å±‚å­¦ä¹ æœ‰æ„ä¹‰çš„å˜æ¢

</div>

<h4 id="13">1.3 è®¾è®¡å“²å­¦</h4>
<p>Pre Normä¸Post Normä½“ç°äº†æ·±åº¦å­¦ä¹ ä¸­çš„ä¸¤ç§è®¾è®¡æƒè¡¡ï¼š</p>
<p><strong>Pre Normï¼šç¨³å®šæ€§ä¼˜å…ˆ</strong>
- å“²å­¦ï¼šè®©è®­ç»ƒå°½å¯èƒ½å®¹æ˜“ï¼Œå³ä½¿ç‰ºç‰²ä¸€äº›è¡¨è¾¾èƒ½åŠ›
- ç›®æ ‡ï¼šå¿«é€Ÿæ”¶æ•›ã€æ— éœ€ç²¾ç»†è°ƒå‚
- ä»£ä»·ï¼šæœ‰æ•ˆæ·±åº¦ä¸è¶³ã€æœ€ç»ˆæ€§èƒ½æ¬¡ä¼˜</p>
<p><strong>Post Normï¼šæ€§èƒ½ä¼˜å…ˆ</strong>
- å“²å­¦ï¼šå……åˆ†åˆ©ç”¨æ·±åº¦ï¼Œå³ä½¿è®­ç»ƒæ›´å›°éš¾
- ç›®æ ‡ï¼šæœ€å¤§åŒ–ç½‘ç»œè¡¨è¾¾èƒ½åŠ›
- ä»£ä»·ï¼šéœ€è¦Warmupã€ç²¾ç»†åˆå§‹åŒ–ã€æ›´é•¿è®­ç»ƒæ—¶é—´</p>
<p><strong>ä¸ä¼ ç»Ÿæ¶æ„çš„æœ¬è´¨åŒºåˆ«</strong>ï¼š</p>
<table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Pre Norm</th>
<th>Post Norm</th>
<th>æ— Normçš„ResNet</th>
</tr>
</thead>
<tbody>
<tr>
<td>æ’ç­‰è·¯å¾„</td>
<td>å®Œå…¨ç•…é€š</td>
<td>å—å½’ä¸€åŒ–è°ƒåˆ¶</td>
<td>å®Œå…¨ç•…é€š</td>
</tr>
<tr>
<td>æ®‹å·®æƒé‡</td>
<td>é€å±‚ç¨€é‡Š</td>
<td>ä¿æŒåŒç­‰åœ°ä½</td>
<td>å¯èƒ½çˆ†ç‚¸/æ¶ˆå¤±</td>
</tr>
<tr>
<td>è®­ç»ƒéš¾åº¦</td>
<td>ç®€å•</td>
<td>ä¸­ç­‰</td>
<td>å›°éš¾ï¼ˆæ·±å±‚æ—¶ï¼‰</td>
</tr>
<tr>
<td>æœ‰æ•ˆæ·±åº¦</td>
<td>$\Theta(\sqrt{L})$</td>
<td>$\Theta(L)$</td>
<td>éš¾ä»¥è®­ç»ƒæ·±å±‚</td>
</tr>
<tr>
<td>é€‚ç”¨åœºæ™¯</td>
<td>å¿«é€Ÿå®éªŒ</td>
<td>è¿½æ±‚SOTA</td>
<td>æµ…å±‚ç½‘ç»œ</td>
</tr>
</tbody>
</table>
<p><strong>æ ¸å¿ƒæƒè¡¡</strong>ï¼š</p>
<blockquote>
<p><strong>Ease of Training vs. Quality of Solution</strong>
Pre Normç‰ºç‰²solution qualityæ¢å–training ease
Post Normç‰ºç‰²training easeæ¢å–solution quality</p>
</blockquote>
<hr />
<h3 id="2">ç¬¬2éƒ¨åˆ†ï¼šä¸¥è°¨çš„æ ¸å¿ƒæ•°å­¦æ¨å¯¼</h3>
<h2 id="_6">æ·±åº¦æ•°å­¦åˆ†æ</h2>
<h3 id="gradient-flow">æ¢¯åº¦æµåˆ†æ</h3>
<div class="theorem-box">

**å®šç†1ï¼šPre Normä¸Post Normçš„æ¢¯åº¦ä¼ æ’­å·®å¼‚**

å¯¹äº$L$å±‚ç½‘ç»œï¼Œè®°æŸå¤±å‡½æ•°ä¸º$\mathcal{L}$ï¼Œåˆ™æ¢¯åº¦åå‘ä¼ æ’­æœ‰ï¼š

**Pre Norm**:
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_t} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_{t+1}} \left( \mathbf{I} + \frac{\partial F_t}{\partial \text{Norm}(\boldsymbol{x}_t)} \cdot \frac{\partial \text{Norm}(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t} \right)$$

**Post Norm**:
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_t} = \frac{\partial \text{Norm}}{\partial (\boldsymbol{x}_t + F_t)} \cdot \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_{t+1}} \left( \mathbf{I} + \frac{\partial F_t}{\partial \boldsymbol{x}_t} \right)$$

</div>

<h4 id="_7">è¯¦ç»†æ¨å¯¼</h4>
<p><strong>Pre Normçš„æ¢¯åº¦åˆ†æ</strong>ï¼š</p>
<p>å¯¹äºPre Normç»“æ„ $\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t))$ï¼Œä½¿ç”¨é“¾å¼æ³•åˆ™ï¼š</p>
<p>\begin{equation}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em t_1="t+1">t} &amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}} \cdot \frac{\partial \boldsymbol{x<em t_1="t+1">{t+1}}{\partial \boldsymbol{x}_t} \
&amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}} \left( \frac{\partial \boldsymbol{x<em t_1="t+1">t}{\partial \boldsymbol{x}_t} + \frac{\partial F_t(\text{Norm}(\boldsymbol{x}_t))}{\partial \boldsymbol{x}_t} \right) \
&amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em> \right)
\end{aligned}\end{equation}}} \left( \mathbf{I} + \frac{\partial F_t}{\partial \text{Norm}(\boldsymbol{x}_t)} \cdot \frac{\partial \text{Norm}(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t</p>
<p>å…³é”®è§‚å¯Ÿï¼šPre Normä¸­æ’ç­‰è·¯å¾„ï¼ˆ$\mathbf{I}$é¡¹ï¼‰<strong>ä¸ç»è¿‡ä»»ä½•å½’ä¸€åŒ–</strong>ï¼Œå› æ­¤æ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ é€’ã€‚è¿™ä½¿å¾—è®­ç»ƒæ›´åŠ ç¨³å®šï¼Œä½†ä¹Ÿå¯¼è‡´äº†ä¸€ä¸ªé—®é¢˜ï¼š</p>
<p>è¿­ä»£$L$å±‚åçš„æ¢¯åº¦ä¸ºï¼š
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_0} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_L} \prod_{t=0}^{L-1} \left( \mathbf{I} + \mathbf{J}_t \right)$$</p>
<p>å…¶ä¸­ $\mathbf{J}_t = \frac{\partial F_t}{\partial \text{Norm}(\boldsymbol{x}_t)} \cdot \frac{\partial \text{Norm}(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t}$ã€‚</p>
<p>å½“$L$å¾ˆå¤§æ—¶ï¼Œç”±äºæ’ç­‰è·¯å¾„çš„å­˜åœ¨ï¼Œæ¢¯åº¦çš„ä¸»è¦æˆåˆ†æ¥è‡ªï¼š
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_0} \approx \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_L} + \text{ä½é˜¶ä¿®æ­£é¡¹}$$</p>
<p>è¿™æ„å‘³ç€<strong>æµ…å±‚å‚æ•°çš„æ¢¯åº¦ä¸»è¦ç”±é¡¶å±‚ä¼ æ¥</strong>ï¼Œä¸­é—´å±‚çš„ä½œç”¨è¢«ç¨€é‡Šã€‚</p>
<p><strong>Post Normçš„æ¢¯åº¦åˆ†æ</strong>ï¼š</p>
<p>å¯¹äºPost Normç»“æ„ $\boldsymbol{x}_{t+1} = \text{Norm}(\boldsymbol{x}_t + F_t(\boldsymbol{x}_t))$ï¼š</p>
<p>\begin{equation}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em t_1="t+1">t} &amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}} \cdot \frac{\partial \text{Norm}(\boldsymbol{x<em t_1="t+1">t + F_t(\boldsymbol{x}_t))}{\partial \boldsymbol{x}_t} \
&amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em> \right)
\end{aligned}\end{equation}}} \cdot \frac{\partial \text{Norm}}{\partial \boldsymbol{z}_t} \cdot \left( \mathbf{I} + \frac{\partial F_t}{\partial \boldsymbol{x}_t</p>
<p>å…¶ä¸­ $\boldsymbol{z}_t = \boldsymbol{x}_t + F_t(\boldsymbol{x}_t)$ã€‚</p>
<p>å…³é”®åŒºåˆ«ï¼šæ¯ä¸€å±‚çš„æ¢¯åº¦éƒ½è¦<strong>ç»è¿‡å½’ä¸€åŒ–çš„é›…å¯æ¯”çŸ©é˜µ</strong> $\frac{\partial \text{Norm}}{\partial \boldsymbol{z}_t}$ï¼Œè¿™ä¼šï¼š
1. <strong>å‰Šå¼±æ’ç­‰åˆ†æ”¯</strong>ï¼šæ¯ç»è¿‡ä¸€æ¬¡Normï¼Œæ’ç­‰è·¯å¾„çš„æƒé‡è¢«å‹ç¼©
2. <strong>å¼ºåŒ–æ®‹å·®åˆ†æ”¯</strong>ï¼šè¿«ä½¿ç½‘ç»œæ›´ä¾èµ–$F_t$çš„å­¦ä¹ 
3. <strong>å¢åŠ è®­ç»ƒéš¾åº¦</strong>ï¼šéœ€è¦æ›´ç²¾ç»†çš„åˆå§‹åŒ–å’Œå­¦ä¹ ç‡è®¾ç½®</p>
<p>è¿­ä»£$L$å±‚åï¼š
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_0} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_L} \prod_{t=0}^{L-1} \left( \frac{\partial \text{Norm}}{\partial \boldsymbol{z}_t} \cdot (\mathbf{I} + \mathbf{J}_t') \right)$$</p>
<p>ç”±äºæ¯å±‚éƒ½æœ‰$\frac{\partial \text{Norm}}{\partial \boldsymbol{z}_t}$é¡¹ï¼ˆå…¸å‹å€¼çº¦ä¸º$\frac{1}{\sqrt{d}}$é‡çº§ï¼‰ï¼Œæ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶ä¼šè¢«é€å±‚è°ƒåˆ¶ï¼Œ<strong>æ¯ä¸€å±‚çš„å­¦ä¹ éƒ½æ›´åŠ ç‹¬ç«‹å’Œå……åˆ†</strong>ã€‚</p>
<hr />
<h3 id="norm-evolution">èŒƒæ•°æ¼”åŒ–åˆ†æ</h3>
<div class="derivation-box">

**å‘½é¢˜1ï¼šPre Normä¸­çš„èŒƒæ•°å¢é•¿**

å‡è®¾$F_t$çš„è¾“å‡ºä¸è¾“å…¥åŒé‡çº§ï¼Œå³$\|F_t(\text{Norm}(\boldsymbol{x}_t))\| = \Theta(1)$ï¼Œåˆ™ï¼š

$$\|\boldsymbol{x}_L\| = \Theta(L)$$

**è¯æ˜**ï¼š

ç”±äº$\text{Norm}(\boldsymbol{x}_t)$å°†$\boldsymbol{x}_t$å½’ä¸€åŒ–åˆ°å›ºå®šèŒƒæ•°ï¼ˆLayer Normä½¿å¾—æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰ï¼Œæˆ‘ä»¬æœ‰ï¼š

\begin{equation}\begin{aligned}
\boldsymbol{x}_{t+1} &= \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t)) \\
\|\boldsymbol{x}_{t+1}\|^2 &= \|\boldsymbol{x}_t\|^2 + 2\langle \boldsymbol{x}_t, F_t \rangle + \|F_t\|^2
\end{aligned}\end{equation}

åœ¨æœŸæœ›æ„ä¹‰ä¸‹ï¼ˆå‡è®¾$\boldsymbol{x}_t$ä¸$F_t$è¿‘ä¼¼æ­£äº¤æˆ–ç›¸å…³æ€§è¾ƒå¼±ï¼‰ï¼š
$$\mathbb{E}[\|\boldsymbol{x}_{t+1}\|^2] \approx \mathbb{E}[\|\boldsymbol{x}_t\|^2] + \mathbb{E}[\|F_t\|^2] = \mathbb{E}[\|\boldsymbol{x}_t\|^2] + \Theta(1)$$

é€’æ¨å¾—åˆ°ï¼š
$$\mathbb{E}[\|\boldsymbol{x}_L\|^2] = \mathbb{E}[\|\boldsymbol{x}_0\|^2] + L \cdot \Theta(1) = \Theta(L)$$

å› æ­¤$\|\boldsymbol{x}_L\| = \Theta(\sqrt{L})$ã€‚

</div>

<p><strong>å…³é”®æ¨è®º</strong>ï¼šå½“$t$è¾ƒå¤§æ—¶ï¼Œ$\boldsymbol{x}_t$çš„èŒƒæ•°å·²ç»å¢é•¿åˆ°$\Theta(\sqrt{t})$ï¼Œè€Œ$F_t$çš„è¾“å‡ºä»ç„¶æ˜¯$\Theta(1)$ï¼ˆå› ä¸ºè¾“å…¥è¢«å½’ä¸€åŒ–äº†ï¼‰ï¼Œæ‰€ä»¥ï¼š</p>
<p>$$\frac{\|F_t(\text{Norm}(\boldsymbol{x}_t))\|}{\|\boldsymbol{x}_t\|} = \Theta\left(\frac{1}{\sqrt{t}}\right) \to 0$$</p>
<p>è¿™æ­£æ˜¯"æ·±åº¦æœ‰æ°´åˆ†"çš„æ•°å­¦è¡¨è¿°ï¼š<strong>è¶Šæ·±çš„å±‚ï¼Œæ®‹å·®åˆ†æ”¯ç›¸å¯¹äºä¸»å¹²çš„è´¡çŒ®è¶Šå°</strong>ã€‚</p>
<p>ç›¸æ¯”ä¹‹ä¸‹ï¼ŒPost Normæ¯æ¬¡éƒ½ä¼šé‡æ–°å½’ä¸€åŒ–ï¼Œä¿æŒ$|\boldsymbol{x}_t| = \Theta(1)$ï¼Œå› æ­¤æ¯ä¸€å±‚çš„æ®‹å·®éƒ½èƒ½äº§ç”Ÿç›¸åŒé‡çº§çš„è´¡çŒ®ã€‚</p>
<hr />
<h3 id="lipschitz-analysis">Lipschitzå¸¸æ•°ä¸æ•°å€¼ç¨³å®šæ€§</h3>
<div class="theorem-box">

**å®šç†2ï¼šLipschitzå¸¸æ•°çš„å±‚æ•°ä¾èµ–æ€§**

è®¾$L_F$ä¸ºæ®‹å·®å—$F$çš„Lipschitzå¸¸æ•°ï¼ˆé€šå¸¸$L_F \approx 1$ï¼‰ï¼Œ$L_{\text{Norm}}$ä¸ºå½’ä¸€åŒ–æ“ä½œçš„Lipschitzå¸¸æ•°ï¼ˆå¯¹äºLayer Normï¼Œ$L_{\text{Norm}} \lesssim 1$ï¼‰ã€‚

**Pre Norm**ï¼šæ•´ä¸ª$L$å±‚ç½‘ç»œçš„Lipschitzå¸¸æ•°ä¸º
$$L_{\text{Pre}} = (1 + L_F \cdot L_{\text{Norm}})^L \approx e^{L \cdot L_F \cdot L_{\text{Norm}}}$$

**Post Norm**ï¼šæ•´ä¸ª$L$å±‚ç½‘ç»œçš„Lipschitzå¸¸æ•°ä¸º
$$L_{\text{Post}} = L_{\text{Norm}}^L \cdot (1 + L_F)^L \approx \left( L_{\text{Norm}} (1+L_F) \right)^L$$

</div>

<p><strong>åˆ†æ</strong>ï¼š</p>
<ol>
<li>
<p><strong>Pre Normçš„æŒ‡æ•°å¢é•¿</strong>ï¼šç”±äºæ’ç­‰è·¯å¾„å®Œå…¨ä¸å—çº¦æŸï¼Œ$L$å±‚åè¾“å‡ºå¯èƒ½æ˜¯è¾“å…¥çš„$(1+L_F L_{\text{Norm}})^L$å€ã€‚å½“$L$å¾ˆå¤§æ—¶ï¼Œè¿™ä¸ªå€æ•°å‘ˆæŒ‡æ•°å¢é•¿ï¼Œä½†ç”±äºæ’ç­‰åˆ†æ”¯å ä¸»å¯¼ï¼Œæ•´ä½“è¡Œä¸ºç±»ä¼¼äºæµ…å±‚å®½ç½‘ç»œã€‚</p>
</li>
<li>
<p><strong>Post Normçš„å½’ä¸€åŒ–æ•ˆæœ</strong>ï¼šæ¯å±‚çš„$L_{\text{Norm}}$é¡¹ï¼ˆé€šå¸¸$&lt;1$ï¼‰ä¼šå‹åˆ¶æŒ‡æ•°å¢é•¿ï¼Œä½¿å¾—ç½‘ç»œçš„åŠ¨æ€èŒƒå›´æ›´åŠ å¯æ§ã€‚è™½ç„¶è®­ç»ƒéš¾åº¦å¢åŠ ï¼ˆå› ä¸ºæ¢¯åº¦ä¹Ÿè¢«è°ƒåˆ¶ï¼‰ï¼Œä½†ä¸€æ—¦è®­ç»ƒå¥½ï¼Œ<strong>æ¯ä¸€å±‚éƒ½çœŸæ­£å‚ä¸äº†è¡¨ç¤ºå­¦ä¹ </strong>ã€‚</p>
</li>
</ol>
<h4 id="layer-normalizationlipschitz">Layer Normalizationçš„Lipschitzå¸¸æ•°</h4>
<p>å¯¹äºLayer Norm: $\text{LN}(\boldsymbol{x}) = \gamma \odot \frac{\boldsymbol{x} - \mu}{\sigma} + \beta$</p>
<p>å…¶ä¸­$\mu = \frac{1}{d}\sum_i x_i$, $\sigma = \sqrt{\frac{1}{d}\sum_i (x_i - \mu)^2}$ã€‚</p>
<p>Lipschitzå¸¸æ•°çš„ä¸Šç•Œä¸ºï¼š
$$\left\|\frac{\partial \text{LN}}{\partial \boldsymbol{x}}\right\| \leq \|\gamma\| \cdot \sqrt{1 + \frac{1}{d}} \approx \|\gamma\|$$</p>
<p>å½“$\gamma$åˆå§‹åŒ–ä¸º1æ—¶ï¼Œ$L_{\text{Norm}} \approx 1$ï¼Œä½†ç”±äºå½’ä¸€åŒ–çš„é™¤ä»¥$\sigma$æ“ä½œï¼Œå®é™…çš„åŠ¨æ€ç¼©æ”¾å› å­å¯èƒ½å°äº1ï¼Œè¿™æ­£æ˜¯Post Normèƒ½å¤Ÿæ§åˆ¶æ¢¯åº¦çˆ†ç‚¸çš„åŸå› ã€‚</p>
<hr />
<h3 id="deepnet-solution">DeepNetçš„è§£å†³æ–¹æ¡ˆ</h3>
<div class="comparison-box">

**DeepNetçš„æ ¸å¿ƒæ€æƒ³**

è®ºæ–‡[ã€ŠDeepNet: Scaling Transformers to 1,000 Layersã€‹](https://papers.cool/arxiv/2203.00555)æå‡ºäº†ä¸€ä¸ªå·§å¦™çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œç»“åˆPre Normå’ŒPost Normçš„ä¼˜ç‚¹ï¼š

$$\boldsymbol{x}_{t+1} = \text{LN}(\boldsymbol{x}_t + \alpha \cdot F_t(\boldsymbol{x}_t))$$

å…³é”®ï¼š**æ®‹å·®åˆ†æ”¯çš„ç¼©æ”¾å› å­** $\alpha = \alpha(L)$ ä¾èµ–äºç½‘ç»œæ·±åº¦$L$ã€‚

</div>

<h4 id="_8">æ•°å­¦æ¨å¯¼</h4>
<p><strong>ç›®æ ‡</strong>ï¼šä½¿å¾—æ¯ä¸€å±‚çš„æœŸæœ›è¾“å‡ºèŒƒæ•°ä¿æŒå¸¸æ•°ï¼Œå³$\mathbb{E}[|\boldsymbol{x}_t|^2] = C$ï¼ˆå¸¸æ•°ï¼‰ã€‚</p>
<p>å‡è®¾$F_t$çš„è¾“å‡ºæ»¡è¶³$\mathbb{E}[|F_t(\boldsymbol{x}_t)|^2] = V_F |\boldsymbol{x}_t|^2$ï¼ˆå…¶ä¸­$V_F$ä¸åˆå§‹åŒ–æ–¹å·®æœ‰å…³ï¼‰ã€‚</p>
<p>åœ¨Post Normç»“æ„ä¸‹ï¼ˆå»æ‰å¤–å±‚Normä»¥ç®€åŒ–åˆ†æï¼‰ï¼š
$$\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + \alpha F_t(\boldsymbol{x}_t)$$</p>
<p>æœŸæœ›èŒƒæ•°æ¼”åŒ–ï¼š
$$\mathbb{E}[\|\boldsymbol{x}_{t+1}\|^2] = \mathbb{E}[\|\boldsymbol{x}_t\|^2] + \alpha^2 \mathbb{E}[\|F_t\|^2] + 2\alpha \mathbb{E}[\langle \boldsymbol{x}_t, F_t \rangle]$$</p>
<p>å‡è®¾æ®‹å·®ä¸ä¸»å¹²è¿‘ä¼¼æ­£äº¤ï¼ˆåœ¨é€‚å½“åˆå§‹åŒ–ä¸‹ï¼ŒåˆæœŸå¯ä»¥è¿‘ä¼¼æˆç«‹ï¼‰ï¼š
$$\mathbb{E}[\|\boldsymbol{x}_{t+1}\|^2] \approx \mathbb{E}[\|\boldsymbol{x}_t\|^2] (1 + \alpha^2 V_F)$$</p>
<p>ç»è¿‡$L$å±‚åï¼š
$$\mathbb{E}[\|\boldsymbol{x}_L\|^2] \approx \mathbb{E}[\|\boldsymbol{x}_0\|^2] (1 + \alpha^2 V_F)^L$$</p>
<p><strong>è¦æ±‚èŒƒæ•°ä¸çˆ†ç‚¸</strong>ï¼Œå³$(1 + \alpha^2 V_F)^L = \mathcal{O}(1)$ï¼Œéœ€è¦ï¼š
$$\alpha^2 V_F \cdot L = \mathcal{O}(1) \quad \Rightarrow \quad \alpha = \mathcal{O}(L^{-1/2})$$</p>
<p><strong>DeepNetçš„é€‰æ‹©</strong>ï¼š
$$\alpha = \frac{1}{\sqrt{2L}}$$</p>
<p>è¿™ä¸ªå› å­ç¡®ä¿äº†ï¼š
1. <strong>æµ…å±‚</strong>ï¼š$\alpha$è¾ƒå¤§ï¼Œæ®‹å·®åˆ†æ”¯æœ‰è¶³å¤Ÿå½±å“åŠ›
2. <strong>æ·±å±‚</strong>ï¼š$\alpha$è‡ªåŠ¨è¡°å‡ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å’ŒèŒƒæ•°çˆ†ç‚¸
3. <strong>å…¨å±€</strong>ï¼š$L$å±‚ç´¯ç§¯æ•ˆæœå¯æ§</p>
<h4 id="xavier">Xavieråˆå§‹åŒ–çš„ä¿®æ­£</h4>
<p>ç»“åˆæ®‹å·®ç¼©æ”¾ï¼ŒDeepNetè¿˜æå‡ºäº†é’ˆå¯¹æ®‹å·®åˆ†æ”¯çš„åˆå§‹åŒ–ç­–ç•¥ï¼š</p>
<p>å¯¹äº$F_t$ä¸­çš„æƒé‡çŸ©é˜µ$\mathbf{W}$ï¼Œä½¿ç”¨ï¼š
$$\mathbf{W} \sim \mathcal{N}\left(0, \frac{2}{d_{\text{in}} + d_{\text{out}}} \cdot \beta^2 \right)$$</p>
<p>å…¶ä¸­$\beta = \mathcal{O}(1)$æ˜¯é¢å¤–çš„ç¼©æ”¾å› å­ï¼Œä¸$\alpha$é…åˆä½¿ç”¨ã€‚</p>
<p>å®Œæ•´çš„å‰å‘ä¼ æ’­ï¼š
$$\boldsymbol{x}_{t+1} = \text{LN}\left(\boldsymbol{x}_t + \underbrace{\frac{1}{\sqrt{2L}}}_{\alpha} \cdot F_t(\boldsymbol{x}_t; \mathbf{W}^{(\beta)}) \right)$$</p>
<p><strong>å®éªŒéªŒè¯</strong>ï¼š
- æ ‡å‡†Post Normï¼šæœ€å¤šè®­ç»ƒ$\sim$50-100å±‚
- DeepNetï¼šæˆåŠŸè®­ç»ƒ1000å±‚Transformerï¼Œåœ¨WMTç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—SOTA</p>
<hr />
<h3 id="effective-depth">æœ‰æ•ˆæ·±åº¦çš„å®šé‡åˆ†æ</h3>
<div class="derivation-box">

**å‘½é¢˜2ï¼šPre Normçš„æœ‰æ•ˆæ·±åº¦**

å®šä¹‰ç½‘ç»œçš„**æœ‰æ•ˆæ·±åº¦**ä¸ºä½¿å¾—å‰$k$å±‚çš„ç´¯ç§¯è´¡çŒ®å æ€»è¾“å‡ºçš„æŸä¸ªæ¯”ä¾‹ï¼ˆå¦‚90%ï¼‰çš„æœ€å°$k$ã€‚

å¯¹äºPre Normï¼Œç¬¬$t$å±‚çš„ç›¸å¯¹è´¡çŒ®ä¸ºï¼š
$$r_t = \frac{\|F_t(\text{Norm}(\boldsymbol{x}_t))\|}{\|\boldsymbol{x}_L\|} \approx \frac{\Theta(1)}{\Theta(\sqrt{L})} = \Theta(L^{-1/2})$$

ç´¯ç§¯å‰$k$å±‚çš„è´¡çŒ®ï¼š
$$\sum_{t=0}^{k-1} r_t \approx k \cdot \Theta(L^{-1/2}) = \Theta\left(\frac{k}{\sqrt{L}}\right)$$

è¦è¾¾åˆ°90%è´¡çŒ®ï¼Œéœ€è¦ï¼š
$$\frac{k}{\sqrt{L}} \gtrsim 0.9 \quad \Rightarrow \quad k \gtrsim 0.9\sqrt{L}$$

**ç»“è®º**ï¼š$L$å±‚çš„Pre Normç½‘ç»œï¼Œæœ‰æ•ˆæ·±åº¦ä»…ä¸º$\Theta(\sqrt{L})$ï¼

</div>

<p><strong>å¯¹æ¯”Post Norm</strong>ï¼šæ¯å±‚è´¡çŒ®ç›¸å¯¹å‡åŒ€ï¼ˆéƒ½åœ¨$\Theta(1)$é‡çº§ï¼‰ï¼Œæœ‰æ•ˆæ·±åº¦$\approx L$ã€‚</p>
<p><strong>ç›´è§‚æ¯”å–»</strong>ï¼š
- <strong>Pre Norm</strong>ï¼šåƒ$\sqrt{L}$å±‚æ·±ã€$\sqrt{L}$å€å®½çš„ç½‘ç»œ
- <strong>Post Norm</strong>ï¼šçœŸæ­£çš„$L$å±‚æ·±ç½‘ç»œ</p>
<p>ç”±äºæ·±åº¦æ¯”å®½åº¦æ›´é‡è¦ï¼ˆæ›´èƒ½å­¦ä¹ å±‚çº§åŒ–çš„æŠ½è±¡ç‰¹å¾ï¼‰ï¼ŒPost Normçš„è¡¨è¾¾èƒ½åŠ›æ›´å¼ºã€‚</p>
<hr />
<h3 id="mutual-information">ä¿¡æ¯è®ºè§†è§’ï¼šäº’ä¿¡æ¯åˆ†æ</h3>
<p>ä»ä¿¡æ¯è®ºè§’åº¦ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†æè¾“å…¥$\boldsymbol{x}_0$ä¸å„å±‚è¾“å‡º$\boldsymbol{x}_t$ä¹‹é—´çš„äº’ä¿¡æ¯$I(\boldsymbol{x}_0; \boldsymbol{x}_t)$ã€‚</p>
<p><strong>Pre Norm</strong>ï¼š
ç”±äºæ’ç­‰è·¯å¾„å ä¸»å¯¼ï¼Œäº’ä¿¡æ¯è¡°å‡æ…¢ï¼š
$$I(\boldsymbol{x}_0; \boldsymbol{x}_t) \geq I_{\text{identity}} - \mathcal{O}(t \epsilon)$$</p>
<p>å…¶ä¸­$\epsilon$æ˜¯å•å±‚çš„ä¿¡æ¯æŸå¤±ã€‚ç”±äº$\boldsymbol{x}_t$ä¸»è¦åŒ…å«$\boldsymbol{x}_0$çš„ä¿¡æ¯åŠ ä¸Šå„å±‚çš„æ‰°åŠ¨ï¼Œ<strong>åŸå§‹ä¿¡æ¯è¢«è¿‡åº¦ä¿ç•™</strong>ï¼Œé™åˆ¶äº†å±‚çº§åŒ–çš„ç‰¹å¾æå–ã€‚</p>
<p><strong>Post Norm</strong>ï¼š
æ¯æ¬¡å½’ä¸€åŒ–ä¼šéƒ¨åˆ†ä¸¢å¼ƒå¹…å€¼ä¿¡æ¯ï¼Œä½†ä¿ç•™æ–¹å‘ä¿¡æ¯ï¼š
$$I(\boldsymbol{x}_0; \boldsymbol{x}_t) \approx I_{\text{directional}} + \text{learned features}$$</p>
<p>è¿™ç§"é€‰æ‹©æ€§é—å¿˜"ï¼ˆå¹…å€¼ä¿¡æ¯ï¼‰+ "ä¸»åŠ¨å­¦ä¹ "ï¼ˆé€šè¿‡$F_t$ï¼‰çš„æœºåˆ¶ï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿ<strong>é€å±‚æŠ½è±¡</strong>ï¼Œæ„å»ºå±‚çº§åŒ–çš„ç‰¹å¾è¡¨ç¤ºã€‚</p>
<hr />
<h3 id="ode-view">è®­ç»ƒåŠ¨æ€çš„å¾®åˆ†æ–¹ç¨‹è§†è§’</h3>
<p>å°†æ®‹å·®ç½‘ç»œè§†ä¸ºå¸¸å¾®åˆ†æ–¹ç¨‹(ODE)çš„ç¦»æ•£åŒ–ï¼š
$$\frac{d\boldsymbol{x}}{dt} = f(t, \boldsymbol{x}), \quad \boldsymbol{x}(0) = \boldsymbol{x}_0$$</p>
<p>ç¦»æ•£åŒ–ï¼š$\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + \Delta t \cdot f(t, \boldsymbol{x}_t)$</p>
<p><strong>Pre Norm</strong>å¯¹åº”ï¼š
$$f(t, \boldsymbol{x}) = F_t(\text{Norm}(\boldsymbol{x}))$$</p>
<p>ç”±äºNormæ“ä½œï¼Œ$f$çš„å¹…å€¼è¢«é™åˆ¶åœ¨$\Theta(1)$ï¼Œå› æ­¤éšç€$|\boldsymbol{x}|$å¢é•¿åˆ°$\Theta(\sqrt{t})$ï¼Œ<strong>åŠ¨æ€ç³»ç»Ÿçš„é€Ÿåº¦åœºç›¸å¯¹äºçŠ¶æ€å¤§å°è¶Šæ¥è¶Šå¼±</strong>ï¼Œç³»ç»Ÿæ¼”åŒ–è¶‹äºé¥±å’Œã€‚</p>
<p><strong>Post Norm</strong>å¯¹åº”ï¼š
$$\boldsymbol{x}_{t+1} = \text{Norm}(\boldsymbol{x}_t + \Delta t \cdot f(t, \boldsymbol{x}_t))$$</p>
<p>æ¯æ­¥éƒ½é‡æ–°å½’ä¸€åŒ–ï¼Œç›¸å½“äº<strong>åœ¨å•ä½çƒé¢ä¸Šçš„æµå½¢æ¼”åŒ–</strong>ï¼Œæ¯ä¸€æ­¥éƒ½ä¿æŒç›¸åŒçš„"æ­¥å¹…"ï¼Œä½¿å¾—$L$æ­¥èƒ½å¤ŸçœŸæ­£èµ°å®Œ$L$çš„"è·ç¦»"ã€‚</p>
<hr />
<h2 id="empirical-analysis">å®éªŒè§†è§’ï¼šæ¢¯åº¦èŒƒæ•°çš„å®è¯åˆ†æ</h2>
<h3 id="_9">æ¢¯åº¦èŒƒæ•°åˆ†å¸ƒ</h3>
<p><a href="https://papers.cool/arxiv/2004.08249">Understanding the Difficulty of Training Transformers</a> çš„å®éªŒæ˜¾ç¤ºï¼š</p>
<p><strong>Pre Norm</strong>ï¼š
- åº•å±‚ï¼ˆé è¿‘è¾“å…¥ï¼‰æ¢¯åº¦èŒƒæ•°ï¼š$|\nabla_{\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">0} \mathcal{L}| \approx 10^{-2}$ åˆ° $10^{-1}$
- é¡¶å±‚ï¼ˆé è¿‘è¾“å‡ºï¼‰æ¢¯åº¦èŒƒæ•°ï¼š$|\nabla</em>$
- }_L} \mathcal{L}| \approx 10^{-3}$ åˆ° $10^{-2<strong>åº•å±‚æ¢¯åº¦æ›´å¤§</strong>ï¼šè¿™ä¼¼ä¹æ˜¯ä¸ªä¼˜ç‚¹ï¼Œä½†å®é™…ä¸Šè¯´æ˜é¡¶å±‚å­¦ä¹ ä¸å……åˆ†</p>
<p><strong>Post Norm</strong>ï¼š
- å„å±‚æ¢¯åº¦èŒƒæ•°æ›´åŠ å‡åŒ€ï¼Œçº¦åœ¨$10^{-2}$åˆ°$10^{-1}$é‡çº§
- <strong>æ¯å±‚éƒ½åœ¨å……åˆ†å­¦ä¹ </strong></p>
<h3 id="_10">å‚æ•°æ›´æ–°çš„ç›¸å¯¹å¹…åº¦</h3>
<p>å®šä¹‰ç¬¬$t$å±‚çš„ç›¸å¯¹æ›´æ–°å¹…åº¦ï¼š
$$\eta_t = \frac{\|\Delta \theta_t\|}{\|\theta_t\|} = \frac{\text{learning rate} \cdot \|\nabla_{\theta_t} \mathcal{L}\|}{\|\theta_t\|}$$</p>
<p><strong>Pre Norm</strong>ï¼šç”±äºæµ…å±‚æ¢¯åº¦å¤§ä½†å‚æ•°èŒƒæ•°ä¹Ÿéšè®­ç»ƒå¢é•¿ï¼Œ$\eta_t$åœ¨å„å±‚å·®å¼‚ä¸å¤§ï¼Œä½†<strong>æµ…å±‚çš„å®é™…å½±å“è¢«ç¨€é‡Š</strong>ï¼ˆå› ä¸ºèŒƒæ•°è†¨èƒ€ï¼‰ã€‚</p>
<p><strong>Post Norm</strong>ï¼šå‚æ•°èŒƒæ•°ä¿æŒç›¸å¯¹ç¨³å®šï¼Œ$\eta_t$ä¸æ¢¯åº¦èŒƒæ•°æˆæ­£æ¯”ï¼Œä½¿å¾—<strong>æ¢¯åº¦ä¿¡å·ç›´æ¥è½¬åŒ–ä¸ºå‚æ•°æ›´æ–°</strong>ã€‚</p>
<hr />
<h2 id="convergence-theory">æ”¶æ•›æ€§ç†è®º</h2>
<div class="theorem-box">

**å®šç†3ï¼šPre Normä¸Post Normçš„æ”¶æ•›é€Ÿåº¦å¯¹æ¯”**

åœ¨å¹³æ»‘æŸå¤±å‡½æ•°å‡è®¾ä¸‹ï¼ˆ$\beta$-smoothï¼Œ$L$-Lipschitzï¼‰ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™è®­ç»ƒï¼š

**Pre Norm**ï¼šè‹¥è¦è¾¾åˆ°$\epsilon$-æœ€ä¼˜è§£ï¼Œéœ€è¦è¿­ä»£æ¬¡æ•°ï¼š
$$T_{\text{Pre}} = \mathcal{O}\left( \frac{L^2 \cdot (1 + L_F L_{\text{Norm}})^{2L}}{\epsilon^2} \right)$$

**Post Norm**ï¼ˆä½¿ç”¨é€‚å½“çš„Warmupå’Œåˆå§‹åŒ–ï¼‰ï¼š
$$T_{\text{Post}} = \mathcal{O}\left( \frac{L^2 \cdot L_{\text{Norm}}^{2L} (1+L_F)^{2L}}{\epsilon^2} \right)$$

ç”±äº$L_{\text{Norm}} < 1$ï¼Œå½“$L$å¾ˆå¤§æ—¶ï¼Œ$L_{\text{Norm}}^{2L}$å¯èƒ½å¾ˆå°ï¼Œéœ€è¦é€šè¿‡Warmupå’Œå­¦ä¹ ç‡è°ƒæ•´æ¥è¡¥å¿ã€‚

</div>

<p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼š
- Pre Normæ”¶æ•›å¿«ï¼ˆåˆæœŸï¼‰ï¼Œä½†æ”¶æ•›åˆ°çš„è§£å¯èƒ½æ˜¯æ¬¡ä¼˜çš„ï¼ˆå› ä¸ºæœ‰æ•ˆæ·±åº¦ä¸è¶³ï¼‰
- Post Normæ”¶æ•›æ…¢ï¼ˆéœ€è¦Warmupï¼‰ï¼Œä½†æœ€ç»ˆèƒ½å¤Ÿè¾¾åˆ°æ›´ä¼˜çš„è§£ï¼ˆå……åˆ†åˆ©ç”¨æ·±åº¦ï¼‰</p>
<hr />
<h2 id="practical-recommendations">å®é™…åº”ç”¨å»ºè®®</h2>
<div class="example-box">

**åœºæ™¯1ï¼šè®­ç»ƒè¶…æ·±ç½‘ç»œï¼ˆ$L > 50$ï¼‰**
- **æ¨è**ï¼šDeepNeté£æ ¼çš„Post Norm + æ®‹å·®ç¼©æ”¾$\alpha = 1/\sqrt{2L}$
- **åˆå§‹åŒ–**ï¼šXavier + $\beta$è°ƒæ•´
- **å­¦ä¹ ç‡**ï¼šéœ€è¦Warmupï¼Œé€æ­¥å¢å¤§åˆ°ç›®æ ‡å­¦ä¹ ç‡

**åœºæ™¯2ï¼šå¿«é€Ÿå®éªŒå’Œè°ƒå‚ï¼ˆ$L \leq 24$ï¼‰**
- **æ¨è**ï¼šPre Norm
- **ä¼˜ç‚¹**ï¼šè®­ç»ƒç¨³å®šï¼Œæ— éœ€ç²¾ç»†è°ƒå‚
- **ä»£ä»·**ï¼šå¯èƒ½æŸå¤±1-2ä¸ªç‚¹çš„æœ€ç»ˆæ€§èƒ½

**åœºæ™¯3ï¼šé¢„è®­ç»ƒå¤§æ¨¡å‹**
- **æ¨è**ï¼šPost Normï¼ˆå¤§å¤šæ•°SOTAæ¨¡å‹çš„é€‰æ‹©ï¼‰
- **ç†ç”±**ï¼šPretrainingé˜¶æ®µæœ‰è¶³å¤Ÿèµ„æºåšä»”ç»†è°ƒå‚ï¼Œæœ€ç»ˆæ€§èƒ½æå‡å€¼å¾—é¢å¤–çš„è®­ç»ƒæˆæœ¬

**åœºæ™¯4ï¼šå¾®è°ƒï¼ˆFinetuneï¼‰**
- **è§‚å¯Ÿ**ï¼šPost Normé¢„è®­ç»ƒçš„æ¨¡å‹é€šå¸¸å¾®è°ƒæ•ˆæœæ›´å¥½
- **åŸå› **ï¼šå„å±‚ç‰¹å¾æ›´å……åˆ†ï¼Œè¿ç§»èƒ½åŠ›æ›´å¼º

</div>

<hr />
<h2 id="conclusion-extended">æ€»ç»“ä¸å±•æœ›</h2>
<p>æœ¬æ–‡ä»å¤šä¸ªè§’åº¦åˆ†æäº†Pre Normä¸Post Normçš„å·®å¼‚ï¼š</p>
<ol>
<li><strong>ç›´è§‚ç†è§£</strong>ï¼šPre Normçš„æ·±åº¦æœ‰"æ°´åˆ†"ï¼Œ$L$å±‚çš„æœ‰æ•ˆæ·±åº¦ä»…$\Theta(\sqrt{L})$</li>
<li><strong>æ¢¯åº¦æµ</strong>ï¼šPre Normæ’ç­‰è·¯å¾„ä¸»å¯¼ï¼ŒPost Normæ®‹å·®åˆ†æ”¯å……åˆ†å­¦ä¹ </li>
<li><strong>èŒƒæ•°æ¼”åŒ–</strong>ï¼šPre Normçº¿æ€§å¢é•¿å¯¼è‡´åå±‚è´¡çŒ®è¢«ç¨€é‡Š</li>
<li><strong>Lipschitzå¸¸æ•°</strong>ï¼šä¸¤ç§ç»“æ„çš„æ•°å€¼ç¨³å®šæ€§å·®å¼‚</li>
<li><strong>DeepNetæ–¹æ¡ˆ</strong>ï¼šé€šè¿‡æ®‹å·®ç¼©æ”¾å¹³è¡¡ç¨³å®šæ€§å’Œè¡¨è¾¾èƒ½åŠ›</li>
<li><strong>ä¿¡æ¯è®º</strong>ï¼šPre Normè¿‡åº¦ä¿ç•™åŸå§‹ä¿¡æ¯ï¼ŒPost Normé€å±‚æŠ½è±¡</li>
</ol>
<p><strong>æœªæ¥æ–¹å‘</strong>ï¼š
1. <strong>è‡ªé€‚åº”å½’ä¸€åŒ–</strong>ï¼šæ ¹æ®å±‚æ·±åº¦å’Œè®­ç»ƒé˜¶æ®µè‡ªåŠ¨è°ƒæ•´å½’ä¸€åŒ–ç­–ç•¥
2. <strong>æ··åˆç­–ç•¥</strong>ï¼šå‰å‡ å±‚ç”¨Pre Normï¼ˆç¨³å®šï¼‰ï¼Œåå‡ å±‚ç”¨Post Normï¼ˆè¡¨è¾¾åŠ›ï¼‰
3. <strong>å¯å­¦ä¹ çš„$\alpha$</strong>ï¼šå°†æ®‹å·®ç¼©æ”¾å› å­å˜æˆå¯å­¦ä¹ å‚æ•°
4. <strong>è¶…è¶ŠLayer Norm</strong>ï¼šæ¢ç´¢æ›´é€‚åˆæ·±åº¦ç½‘ç»œçš„å½’ä¸€åŒ–æ–¹æ³•ï¼ˆå¦‚RMSNormï¼‰</p>
<hr />
        </div>
    </div>
</body>
</html>