<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>çº¿æ€§æ³¨æ„åŠ›ç®€å²ï¼šä»æ¨¡ä»¿ã€åˆ›æ–°åˆ°åå“º</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>çº¿æ€§æ³¨æ„åŠ›ç®€å²ï¼šä»æ¨¡ä»¿ã€åˆ›æ–°åˆ°åå“º</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2025-12-31 | ğŸ“„ å¤§å°: 48.4 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/11033">https://spaces.ac.cn/archives/11033</a></p>
<p><strong>å‘å¸ƒæ—¥æœŸ</strong>: </p>
<hr />
<p>åœ¨ä¸­æ–‡åœˆï¼Œæœ¬ç«™åº”è¯¥ç®—æ˜¯æ¯”è¾ƒæ—©å…³æ³¨çº¿æ€§Attentionçš„äº†ï¼Œåœ¨2020å¹´å†™é¦–ç¯‡ç›¸å…³åšå®¢<a href="/archives/7546">ã€Šçº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿã€‹</a>æ—¶ï¼Œå¤§å®¶ä¸»è¦è®¨è®ºçš„è¿˜æ˜¯BERTç›¸å…³çš„Softmax Attentionã€‚äº‹åæ¥çœ‹ï¼Œåœ¨BERTæ—¶ä»£è€ƒè™‘çº¿æ€§Attentionå¹¶ä¸æ˜¯å¤ªæ˜æ™ºï¼Œå› ä¸ºå½“æ—¶è®­ç»ƒé•¿åº¦æ¯”è¾ƒçŸ­ï¼Œä¸”æ¨¡å‹ä¸»è¦è¿˜æ˜¯Encoderï¼Œç”¨çº¿æ€§Attentionæ¥åšåŸºæœ¬æ²¡æœ‰ä¼˜åŠ¿ã€‚å¯¹æ­¤ï¼Œç¬”è€…ä¹Ÿæ›¾æ’°æ–‡<a href="/archives/8610">ã€Šçº¿æ€§Transformeråº”è¯¥ä¸æ˜¯ä½ è¦ç­‰çš„é‚£ä¸ªæ¨¡å‹ã€‹</a>è¡¨è¾¾è¿™ä¸€è§‚ç‚¹ã€‚</p>
<p>ç›´åˆ°ChatGPTçš„å‡ºä¸–ï¼Œå€’é€¼å¤§å®¶éƒ½å»åšDecoder-onlyçš„ç”Ÿæˆå¼æ¨¡å‹ï¼Œè¿™è·Ÿçº¿æ€§Attentionçš„RNNå½¢å¼é«˜åº¦å¥‘åˆã€‚åŒæ—¶ï¼Œè¿½æ±‚æ›´é•¿çš„è®­ç»ƒé•¿åº¦ä¹Ÿä½¿å¾—Softmax Attentionçš„äºŒæ¬¡å¤æ‚åº¦ç“¶é¢ˆæ„ˆå‘æ˜æ˜¾ã€‚åœ¨è¿™æ ·çš„æ–°èƒŒæ™¯ä¸‹ï¼Œçº¿æ€§Attentionè¶Šæ¥è¶Šä½“ç°å‡ºç«äº‰åŠ›ï¼Œç”šè‡³å‡ºç°äº†â€œåå“ºâ€Softmax Attentionçš„è¿¹è±¡ã€‚</p>
<h2 id="_1">å¹³æ–¹å¤æ‚åº¦</h2>
<p>é¦–å…ˆå¼•å…¥ä¸€äº›è®°å·ï¼š<br />
\begin{equation}\begin{gathered}<br />
\boldsymbol{q}<em _:t_="[:t]">i,\boldsymbol{k}_i,\boldsymbol{v}_i,\boldsymbol{o}_i \in \mathbb{R}^{d\times 1} \\\[6pt]<br />
\boldsymbol{Q}=[\boldsymbol{q}_1,\boldsymbol{q}_2,\cdots,\boldsymbol{q}_n]^{\top}\in\mathbb{R}^{n\times d} \\\[6pt]<br />
\boldsymbol{K}=[\boldsymbol{k}_1,\boldsymbol{k}_2,\cdots,\boldsymbol{k}_n]^{\top}\in\mathbb{R}^{n\times d} \\\[6pt]<br />
\boldsymbol{V}=[\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_n]^{\top}\in\mathbb{R}^{n\times d} \\\[6pt]<br />
\boldsymbol{O}=[\boldsymbol{o}_1,\boldsymbol{o}_2,\cdots,\boldsymbol{o}_n]^{\top}\in\mathbb{R}^{n\times d} \\\[6pt]<br />
\end{gathered}\end{equation}<br />
ä¸€ä¸ªAttentionæ¨¡å‹ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}\to \boldsymbol{O}$çš„æ˜ å°„ã€‚æœ¬æ–‡ä¸»è¦å…³å¿ƒCausalåœºæ™¯ï¼Œè¿™æ„å‘³ç€$\boldsymbol{o}_t$è‡³å¤šè·Ÿ$\boldsymbol{Q}</em>},\boldsymbol{K<em _:t_="[:t]">{[:t]},\boldsymbol{V}</em>$çš„$d$å¯ä»¥ä¸ä¸€è‡´ï¼Œæ¯”å¦‚}$ç›¸å…³ã€‚åŸåˆ™ä¸Šï¼Œ$\boldsymbol{Q},\boldsymbol{K}$çš„$d$ä¸$\boldsymbol{V},\boldsymbol{O<a href="/archives/8934">GAU</a>å’Œ<a href="/archives/10091">MLA</a>ä¾¿æ˜¯å¦‚æ­¤ï¼Œä½†å°†å®ƒä»¬ç®€åŒ–æˆåŒä¸€ä¸ªå¹¶ä¸æ”¹å˜é—®é¢˜æœ¬è´¨ã€‚</p>
<p>æ ‡å‡†çš„Softmax Attentionï¼Œé€šå¸¸æ˜¯æŒ‡<a href="/archives/4765">ã€ŠAttention is All You Needã€‹</a>æ‰€æçš„Attentionæœºåˆ¶ï¼š<br />
\begin{equation}\boldsymbol{O} = \mathop{\text{softmax}}(\boldsymbol{Q}\boldsymbol{K}^{\top} + \log \boldsymbol{M})\boldsymbol{V}\end{equation}<br />
è¿™é‡Œçœç•¥äº†ç¼©æ”¾å› å­$1/\sqrt{d}$ï¼Œå› ä¸ºå®ƒæ€»å¯ä»¥å¸æ”¶åˆ°$\boldsymbol{Q},\boldsymbol{K}$é‡Œè¾¹ï¼Œ$\mathop{\text{softmax}}$æ˜¯å¯¹ç¬¬äºŒä¸ªç»´åº¦è¿›è¡ŒæŒ‡æ•°å½’ä¸€åŒ–ï¼Œè€Œ$\boldsymbol{M}\in\mathbb{R}^{n\times n}$æ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’é˜µï¼Œç§°ä¸ºæ©ç çŸ©é˜µï¼Œå®šä¹‰ä¸º<br />
\begin{equation}M_{i,j} = \left\{\begin{aligned} &amp;1, &amp;i \geq j \\\ &amp;0, &amp;i &lt; j\end{aligned}\right.\end{equation}<br />
$\log\boldsymbol{M}$æ˜¯æŒ‡å¯¹$\boldsymbol{M}$çš„åˆ†é‡é€ä¸€å–$\log$ï¼Œå…¶ä¸­$\log 0 = -\infty$ã€‚Softmax Attentionç”¨åˆ†é‡å½¢å¼å†™å‡ºæ¥åˆ™æ˜¯<br />
\begin{equation}\boldsymbol{o}<em j="1">t = \frac{\sum</em>}^t \exp(\boldsymbol{q<em j="1">t^{\top}\boldsymbol{k}_j) \boldsymbol{v}_j}{\sum</em>}^t \exp(\boldsymbol{q}_t^{\top}\boldsymbol{k}_j) }\end{equation<br />
å…¶ä¸­åˆ†æ¯çš„ä½œç”¨ä¸»è¦æ˜¯ä¿æŒæ•°å€¼ç¨³å®šæ€§ï¼Œå¦å¤–å°±æ˜¯å¦‚æœæˆ‘ä»¬ç»™$\boldsymbol{O}$åŠ ä¸ŠRMSNormï¼Œé‚£ä¹ˆåˆ†æ¯ä¹Ÿä¼šè‡ªåŠ¨æ¶ˆå»ï¼Œæ‰€ä»¥Softmax Attentionçš„æ ¸å¿ƒæ˜¯åˆ†å­éƒ¨åˆ†ï¼Œå³<br />
\begin{equation}\boldsymbol{O} = \exp(\boldsymbol{Q}\boldsymbol{K}^{\top} + \log \boldsymbol{M})\boldsymbol{V} = (\exp(\boldsymbol{Q}\boldsymbol{K}^{\top})\odot \boldsymbol{M})\boldsymbol{V}\end{equation}<br />
å…¶ä¸­$\odot$æ˜¯Hadamardç§¯ï¼Œ$\exp$æ˜¯é€åˆ†é‡å–æŒ‡æ•°ã€‚ä¸éš¾çœ‹å‡ºï¼Œåˆ†æ¯å…¶å®å°±æ˜¯å°†$\boldsymbol{V}$æ¢æˆä¸€ä¸ª$n\times 1$çš„å…¨1çŸ©é˜µï¼Œå¦‚æœæœ‰éœ€è¦ï¼Œæˆ‘ä»¬å†è¡¥ä¸Šå³å¯ã€‚Softmax Attentionçš„æ ‡å‡†å®ç°éœ€è¦æŠŠ$n\times n$çš„çŸ©é˜µ$\exp(\boldsymbol{Q}\boldsymbol{K}^{\top})$ç®—å‡ºæ¥ï¼Œæ‰€ä»¥ç©ºé—´å’Œæ—¶é—´å¤æ‚åº¦éƒ½æ­£æ¯”äº$n^2$ã€‚<a href="https://papers.cool/arxiv/2205.14135">Flash Attention</a>çš„å‡ºç°é™ä½äº†ç©ºé—´éœ€æ±‚ï¼Œä½†å¹³æ–¹çš„æ—¶é—´å¤æ‚åº¦ä¾ç„¶æ— æ³•é¿å…ã€‚</p>
<h2 id="_2">æœ€åˆçš„æ¨¡æ ·</h2>
<p>çº¿æ€§Attentionæœ€æ—©çš„æ€è·¯ä¸»è¦æ˜¯æ¨¡ä»¿å’Œè¿‘ä¼¼Softmax Attentionï¼Œå…¶ä¸­æœ€ç®€å•çš„æ–¹æ¡ˆæ˜¯ç›´æ¥å»æ‰$\exp$ï¼Œå¾—åˆ°<br />
\begin{equation}\boldsymbol{O} = (\boldsymbol{Q}\boldsymbol{K}^{\top}\odot \boldsymbol{M})\boldsymbol{V}\label{eq:linear-attn}\end{equation}<br />
ç®€å•èµ·è§ï¼Œæˆ‘ä»¬çº¦å®šçŸ©é˜µä¹˜æ³•çš„ä¼˜å…ˆçº§é«˜äºHadamardç§¯ï¼Œè¿™æ ·å¯ä»¥çœæ‰ä¸€ç»„æ‹¬å·ã€‚ä¸ºä»€ä¹ˆè¿™ä¸ªå½¢å¼æ˜¯â€œçº¿æ€§â€Attentionçš„å‘¢ï¼Ÿä¸ºäº†å¿«é€Ÿç†è§£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¸å¦¨å…ˆè€ƒè™‘å»æ‰$\odot \boldsymbol{M}$çš„éCausalç‰ˆï¼Œæ­¤æ—¶æˆç«‹$\boldsymbol{O} = (\boldsymbol{Q}\boldsymbol{K}^{\top})\boldsymbol{V} = \boldsymbol{Q}(\boldsymbol{K}^{\top}\boldsymbol{V})$ï¼Œæ³¨æ„è®¡ç®—$\boldsymbol{K}^{\top}\boldsymbol{V}$çš„å¤æ‚åº¦æ˜¯$\mathcal{O}(nd^2)$ï¼Œç»“æœæ˜¯$d\times d$çŸ©é˜µï¼Œç„¶åè·Ÿ$\boldsymbol{Q}$ç›¸ä¹˜å¤æ‚åº¦ä¹Ÿæ˜¯$\mathcal{O}(nd^2)$ï¼Œæ‰€ä»¥å®ƒå¤æ‚åº¦æ˜¯çº¿æ€§ä¾èµ–äº$n$ã€‚</p>
<p>è‡³äºCausalç‰ˆ$\eqref{eq:linear-attn}$ï¼Œæˆ‘ä»¬å¯ä»¥ä»åˆ†é‡å½¢å¼ç†è§£ï¼Œå†™å‡ºï¼š<br />
\begin{equation}\boldsymbol{o}<em j="1">t = \sum</em>}^t \boldsymbol{v<em j="1">j (\boldsymbol{k}_j^{\top} \boldsymbol{q}_t) = \sum</em>}^t (\boldsymbol{v<em j="1">j \boldsymbol{k}_j^{\top}) \boldsymbol{q}_t = \left(\sum</em>}^t \boldsymbol{v<em t-1="t-1">j \boldsymbol{k}_j^{\top}\right) \boldsymbol{q}_t\end{equation}<br />
å¦‚æœæˆ‘ä»¬è®°æ‹¬å·éƒ¨åˆ†ä¸º$\boldsymbol{S}_t$ï¼Œé‚£ä¹ˆæœ‰<br />
\begin{equation}\boldsymbol{o}_t = \boldsymbol{S}_t \boldsymbol{q}_t, \qquad \boldsymbol{S}_t = \boldsymbol{S}</em>} + \boldsymbol{v}_t \boldsymbol{k}_t^{\top}\label{eq:linear-attn-rnn}\end{equation<br />
ç”±æ­¤å¯è§ï¼ŒCausalå½¢å¼çš„Attentionå¯ä»¥å†™æˆä¸€ä¸ªä»¥$\boldsymbol{S}_t$ä¸ºStateçš„çº¿æ€§RNNï¼Œå› æ­¤æ¯ä¸€æ­¥çš„å¤æ‚åº¦æ˜¯å¸¸æ•°ï¼Œæ€»çš„å¤æ‚åº¦æ­£æ¯”äºåºåˆ—é•¿åº¦$n$ã€‚æ³¨æ„è¿™é‡Œå‡ºç°äº†â€œçº¿æ€§RNNâ€ï¼Œå®ƒæ˜¯æ›´å¹¿ä¹‰çš„æ¦‚å¿µï¼Œçº¿æ€§Attentionå±äºçº¿æ€§RNNçš„ä¸€ç§ï¼Œçº¿æ€§RNNä¹Ÿå•ç‹¬å‘å±•è¿‡ä¸€æ®µæ—¶é—´ï¼Œæ¯”å¦‚ä¹‹å‰ä»‹ç»è¿‡çš„<a href="/archives/9554">LRU</a>ã€<a href="/tag/ssm/">SSM</a>ç­‰ï¼Œä½†æœ€è¿‘æ¯”è¾ƒæœ‰ç«äº‰åŠ›çš„çº¿æ€§æ¶æ„éƒ½å…·æœ‰çº¿æ€§Attentionçš„å½¢å¼ã€‚</p>
<p>æ—©å¹´çš„çº¿æ€§Attentionè¿˜æœ‰ä¸€äº›éå¸¸æ˜æ˜¾çš„æ¨¡ä»¿Softmax Attentionçš„ç‰¹ç‚¹ï¼Œæ¯”å¦‚ä¼šç»™å¼$\eqref{eq:linear-attn}$åŠ å…¥åˆ†æ¯æ¥å½’ä¸€åŒ–ï¼Œè€Œä¸ºäº†å½’ä¸€åŒ–ï¼Œé‚£ä¹ˆ$\boldsymbol{k}_j^{\top} \boldsymbol{q}_t$å°±å¿…é¡»éè´Ÿï¼Œäºæ˜¯åˆç»™$\boldsymbol{Q},\boldsymbol{K}$åŠ ä¸Šäº†éè´Ÿçš„æ¿€æ´»å‡½æ•°ï¼Œä»¥<a href="/archives/7921">Performer</a>ã€<a href="https://papers.cool/arxiv/2103.02143">RFA</a>ä¸ºä»£è¡¨çš„ä¸€ç³»åˆ—å·¥ä½œï¼Œæ›´æ˜¯ä»¥è¿‘ä¼¼$\exp(\boldsymbol{Q}\boldsymbol{K}^{\top})$ä¸ºå‡ºå‘ç‚¹æ¥æ„å»ºæ¨¡å‹ã€‚</p>
<p>ç„¶è€Œï¼Œåæ¥çš„ç ”ç©¶å¦‚<a href="https://papers.cool/arxiv/2210.10340">ã€ŠThe Devil in Linear Transformerã€‹</a>å‘ç°ï¼Œåœ¨åºåˆ—é•¿åº¦ç»´åº¦å½’ä¸€åŒ–å¹¶ä¸èƒ½å®Œå…¨é¿å…æ•°å€¼ä¸ç¨³å®šæ€§ï¼Œå€’ä¸å¦‚ç›´æ¥äº‹åå½’ä¸€åŒ–ï¼Œå¦‚<br />
\begin{equation}\boldsymbol{O} = \mathop{\text{RMSNorm}}((\boldsymbol{Q}\boldsymbol{K}^{\top}\odot \boldsymbol{M})\boldsymbol{V})\end{equation}<br />
è€Œæ—¢ç„¶ä¸ç”¨å½’ä¸€åŒ–ï¼Œé‚£ä¹ˆç»™$\boldsymbol{Q},\boldsymbol{K}$åŠ éè´Ÿçš„æ¿€æ´»å‡½æ•°æ¥ä¿è¯$\boldsymbol{k}_j^{\top} \boldsymbol{q}_t$éè´Ÿå°±éå¿…é¡»äº†ã€‚é‚£ç»™$\boldsymbol{Q},\boldsymbol{K}$åŠ ï¼ˆä¸ä¸€å®šéè´Ÿçš„ï¼‰æ¿€æ´»å‡½æ•°è¿˜æœ‰æ„ä¹‰å—ï¼Ÿç¬”è€…çš„è§‚ç‚¹æ˜¯ï¼ŒåŠ æ¿€æ´»å‡½æ•°æ˜¯å¤§å®¶çš„è‡ªç”±ï¼Œä¸æ’é™¤åŠ æŸä¸ªæ¿€æ´»å‡½æ•°èƒ½å¤Ÿè°ƒå‡ºæ›´å¥½çš„æ•ˆæœï¼Œä½†åŠ æ¿€æ´»å‡½æ•°å¹¶ä¸æ”¹å˜çº¿æ€§Attentionçš„å½¢å¼ï¼Œæ‰€ä»¥ä¸å½±å“æˆ‘ä»¬çš„æè¿°ï¼Œå¦å¤–å°±æ˜¯ç°æœ‰çš„ç»“æœè¡¨æ˜ï¼Œå…¶å®ä¸åŠ å·²ç»è¶³å¤Ÿå¥½ã€‚</p>
<h2 id="_3">èŠ±å¼é—å¿˜é—¨</h2>
<p>ä»å¼$\eqref{eq:linear-attn-rnn}$æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œç›®å‰çš„çº¿æ€§Attentionæœ¬è´¨ä¸Šå°±æ˜¯ä¸ª$\mathop{\text{cumsum}}$ï¼Œå³å°†æ‰€æœ‰å†å²ä¿¡æ¯éƒ½ç­‰æƒåœ°å åŠ ï¼Œä¸éš¾æƒ³è±¡å½“å åŠ çš„tokenè¶³å¤Ÿå¤šæ—¶ï¼Œæ¯ä¸ªtokençš„ä¿¡æ¯å æ¯”éƒ½ä¼šå˜å¾—æå°ï¼Œäºæ˜¯å•é å›ºå®šå¤§å°çš„$\boldsymbol{S}_t$çŸ©é˜µç”šè‡³æ— æ³•å‡†ç¡®é‡å»ºä»»æ„ä¸€ä¸ªtokenï¼Œç›´è§‚ç±»æ¯”å°±æ˜¯æ¯ä¸ªtokençš„è®°å¿†éƒ½å˜å¾—æ¨¡ç³Šä¸æ¸…ã€‚</p>
<p>ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œ<a href="https://papers.cool/arxiv/2307.08621">RetNet</a>ç»™çº¿æ€§Attentionå¼•å…¥äº†é—å¿˜æ•ˆåº”ï¼š<br />
\begin{equation}\boldsymbol{o}<em t-1="t-1">t = \boldsymbol{S}_t \boldsymbol{q}_t, \qquad \boldsymbol{S}_t = \gamma\boldsymbol{S}</em>} + \boldsymbol{v}_t \boldsymbol{k}_t^{\top}\label{eq:linear-attn-retnet}\end{equation<br />
å…¶ä¸­è¡°å‡å› å­$\gamma\in(0,1)$ï¼Œåœ¨RetNetä¸­è¢«è®¾ä¸ºå¸¸æ•°ï¼Œä¹Ÿæœ‰è®¾ä¸ºå¯è®­ç»ƒå‚æ•°çš„ï¼Œä»¥åŠå°†$\gamma$æ”¹ä¸ºå¯¹è§’çŸ©é˜µçš„ï¼Œç­‰ç­‰ï¼Œ<a href="https://papers.cool/arxiv/2501.08313">MiniMax-01</a>æ‰€ç”¨çš„çº¿æ€§Attentionä¹Ÿæ˜¯è¿™ç§ã€‚æ³¨æ„ï¼Œè¡°å‡å› å­åœ¨RetNetå‰ä¹Ÿæœ‰ï¼Œä¸è¿‡å®ƒä»¬å¤šä»¥çº¿æ€§RNNçš„å½¢å¼å‡ºç°ï¼Œå¦‚ä¸Šä¸€èŠ‚æåˆ°çš„<a href="/archives/9554">LRU</a>ã€<a href="/tag/ssm/">SSM</a>ç­‰ï¼ŒRetNetåº”è¯¥æ˜¯é¦–æ¬¡å°†å®ƒè·Ÿçº¿æ€§Attentionç»“åˆèµ·æ¥ã€‚åŠ å…¥è¡°å‡å› å­åï¼Œæ¨¡å‹ä¼šå€¾å‘äºé—å¿˜æ‰æ›´ä¸ºä¹…è¿œçš„å†å²ä¿¡æ¯ï¼Œä»è€Œè‡³å°‘ä¿è¯æœ€è¿‘tokençš„åˆ†è¾¨ç‡ï¼Œè¯´ç™½äº†å°±æ˜¯è·Ÿè¯­è¨€æ¨¡å‹ç‰¹æ€§ç›¸ç¬¦çš„â€œå°±è¿‘åŸåˆ™ï¼ˆRecency Biasï¼‰â€çš„ä½“ç°ï¼Œä»è€Œå¾€å¾€èƒ½å·¥ä½œå¾—æ›´å¥½ã€‚</p>
<p>æ­¤å¤–ï¼Œä¸€ä¸ªå€¼å¾—å…³æ³¨çš„ç»†èŠ‚æ˜¯RetNetè¿˜ç»™$\boldsymbol{Q},\boldsymbol{K}$åŠ ä¸Šäº†<a href="/archives/9403">RoPE</a>ï¼Œè¿™ç›¸å½“äºå°†è¡°å‡å› å­æ¨å¹¿åˆ°å¤æ•°$\gamma e^{\text{i}\theta}$ï¼Œä»<a href="/archives/9554">LRU</a>çš„è§’åº¦çœ‹åˆ™æ˜¯è€ƒè™‘äº†å¤æ•°çš„ç‰¹å¾å€¼ã€‚å°½ç®¡ç»™RNNåŠ ä½ç½®ç¼–ç çš„æ“ä½œçœ‹ä¸Šå»ä¼¼ä¹æœ‰ç‚¹è¿å’Œï¼Œä½†æœ‰äº›å®éªŒæ¯”å¦‚æœ€è¿‘çš„<a href="https://papers.cool/arxiv/2506.09507">TransXSSM</a>è¡¨æ˜ï¼Œç»™çº¿æ€§AttentionåŠ RoPEä¹Ÿæœ‰ä¸€å®šçš„æ­£é¢ä½œç”¨ã€‚å½“ç„¶ï¼Œè¿™å¯èƒ½å–å†³äºå…·ä½“çš„æ¨¡å‹å˜ä½“å’Œå®éªŒè®¾ç½®ã€‚</p>
<p>å¼$\eqref{eq:linear-attn-retnet}$çš„ä¸€ä¸ªç®€å•æ¨å¹¿æ˜¯å°†$\gamma$æ›´æ¢ä¸ºä½ç½®$t$çš„å‡½æ•°$\gamma_t$ï¼Œè¿™åœ¨<a href="/tag/ssm/">SSM</a>ä¸­å·²ç»æœ‰æ‰€ä½“ç°ã€‚åæ¥ï¼Œ<a href="https://papers.cool/arxiv/2210.04243">DFW</a>ã€<a href="https://papers.cool/arxiv/2312.00752">Mamba</a>ã€<a href="https://papers.cool/arxiv/2405.21060">Mamba2</a>ç­‰å·¥ä½œï¼Œå°†å®ƒæ¨å¹¿æˆè·Ÿè¾“å…¥ç›¸å…³ï¼Œå½¢æˆäº†â€œdata-dependent decayâ€ç›¸å…³çš„ä¸€ç³»åˆ—å·¥ä½œï¼Œè¿™è·Ÿä»¥å¾€GRUã€LSTMç­‰éçº¿æ€§RNNçš„â€œé—å¿˜é—¨ï¼ˆforget gateï¼‰â€å…¶å®å·²ç»éå¸¸ç›¸ä¼¼äº†ï¼Œåªä¸è¿‡ä¸ºäº†ä¿æŒæ¨¡å‹çš„çº¿æ€§æ€§ï¼Œå»æ‰äº†é—å¿˜é—¨å¯¹Stateï¼ˆå¦‚$\boldsymbol{S}_t$ï¼‰çš„ä¾èµ–ã€‚</p>
<p>ä¸ºä»€ä¹ˆæˆ‘ä»¬åçˆ±çº¿æ€§RNNå‘¢ï¼Ÿå› ä¸ºçº¿æ€§RNNåŸºæœ¬éƒ½èƒ½æ‰¾åˆ°æŸç§æ–¹å¼æ¥å¹¶è¡Œè®­ç»ƒï¼Œè¿™ä½¿å¾—å®ƒç›¸æ¯”Softmax Attentionæ›´å…·ç«äº‰åŠ›â€”â€”åœ¨è®­ç»ƒæ•ˆç‡å’Œæ¨ç†æ•ˆç‡ä¸Šéƒ½ä¸é€Šè‰²ã€‚å…¶ä¸­ï¼Œå¹¶è¡ŒåŒ–çš„â€œé€šè§£â€æ˜¯è½¬åŒ–ä¸º<a href="https://en.wikipedia.org/wiki/Prefix_sum">Prefix Sum</a>é—®é¢˜ç„¶åAssociative Scanï¼Œå¤§ä½“æ€è·¯æˆ‘ä»¬åœ¨<a href="/archives/9554">ã€ŠGoogleæ–°ä½œè¯•å›¾â€œå¤æ´»â€RNNï¼šRNNèƒ½å¦å†æ¬¡è¾‰ç…Œï¼Ÿã€‹</a>çš„â€œå¹¶è¡ŒåŒ–â€ä¸€èŠ‚ä¹Ÿç®€å•ä»‹ç»è¿‡ã€‚</p>
<p>ç„¶è€Œï¼Œâ€œé€šè§£â€å¹¶ä¸æ˜¯GPUé«˜æ•ˆçš„ï¼ŒGPUæœ€é«˜æ•ˆçš„æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œæ‰€ä»¥æ‰¾åˆ°å¤§é‡ä½¿ç”¨çŸ©é˜µä¹˜æ³•çš„å¹¶è¡Œç®—æ³•æ˜¯æœ€ç†æƒ³çš„ï¼Œç”šè‡³éƒ½ä¸ç”¨å¹¶è¡Œï¼Œåªè¦æ‰¾åˆ°å……åˆ†ä½¿ç”¨çŸ©é˜µä¹˜æ³•çš„Chunk by Chunké€’å½’æ ¼å¼ï¼Œéƒ½èƒ½æ˜æ˜¾æé«˜è®­ç»ƒæ•ˆç‡ã€‚è¿™åè¿‡æ¥å¯¹æ¨¡å‹æå‡ºäº†è¦æ±‚ï¼Œå¦‚åªæœ‰å¤–ç§¯å½¢å¼çš„é—å¿˜é—¨æ‰èƒ½å®ç°è¿™ä¸ªç›®çš„ï¼Œå…¸å‹åä¾‹å°±æ˜¯Mambaï¼Œå®ƒæ˜¯éå¤–ç§¯çš„é—å¿˜é—¨ï¼Œæ— æ³•å……åˆ†å‘æŒ¥GPUçš„æ€§èƒ½ï¼Œæ‰€ä»¥æ‰æœ‰äº†åç»­Mamba2å’Œ<a href="https://papers.cool/arxiv/2312.06635">GLA</a>ç­‰å˜åŒ–ã€‚</p>
<h2 id="_4">æµ‹è¯•æ—¶è®­ç»ƒ</h2>
<p>è‡³æ­¤ï¼Œçº¿æ€§Attentionä»æœ€åˆçš„ç®€å•æ¨¡ä»¿Softmax Attentionï¼Œåˆ°å¼•å…¥é™æ€è¡°å‡å› å­ä¹ƒè‡³â€œdata-dependent decayâ€ï¼Œå·²ç»å½¢æˆäº†è‡ªèº«çš„ç‰¹è‰²å¹¶åœ¨ä¸å°‘ä»»åŠ¡ä¸Šå‘æŒ¥ä»·å€¼ã€‚ç„¶è€Œï¼Œè¿™äº›è¿›å±•å¤šæ•°æ˜¯é äººå·¥å‡­ç»éªŒè®¾è®¡å‡ºæ¥çš„ï¼Œæˆ‘ä»¬ä¸ç¦è¦é—®ï¼š<strong>æœ‰æ²¡æœ‰æ›´ä¸Šå±‚çš„åŸåˆ™æ¥æŒ‡å¯¼çº¿æ€§Attentionç”šè‡³æ˜¯ä¸€èˆ¬çš„åºåˆ—æ¨¡å‹ï¼ˆToken-Mixerï¼‰çš„è®¾è®¡ï¼Ÿ</strong></p>
<p>å¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œ<a href="https://papers.cool/arxiv/2407.04620">TTTï¼ˆTest Time Trainingï¼‰</a>ç»™å‡ºäº†è‡ªå·±çš„ç­”æ¡ˆï¼Œå®ƒå°†åºåˆ—æ¨¡å‹çš„æ„å»ºè§†ä¸ºä¸€ä¸ªâ€œåœ¨çº¿å­¦ä¹ ï¼ˆOnline Learningï¼‰â€é—®é¢˜ï¼Œå¹¶æå‡ºç”¨ä¼˜åŒ–å™¨æ¥æ„å»ºï¼ˆä¸ä¸€å®šæ˜¯çº¿æ€§çš„ï¼‰RNNçš„åšæ³•ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå°†$\boldsymbol{K},\boldsymbol{V}$è§†ä½œè¯­æ–™å¯¹$(\boldsymbol{k}_1, \boldsymbol{v}_1),(\boldsymbol{k}_2, \boldsymbol{v}_2),\cdots,(\boldsymbol{k}_t, \boldsymbol{v}_t)$ï¼Œæ ¹æ®è¿™äº›è¯­æ–™è®­ç»ƒå¾—åˆ°ä¸€ä¸ªæ¨¡å‹$\boldsymbol{v} = \boldsymbol{f}(\boldsymbol{S}_t;\boldsymbol{k})$ï¼Œæœ€åè¾“å‡º$\boldsymbol{o}_t = \boldsymbol{f}(\boldsymbol{S}_t;\boldsymbol{q}_t)$ï¼Œå…¶ä¸­$\boldsymbol{S}_t$æ˜¯æ¨¡å‹å‚æ•°ï¼Œè‡³äºæ¨¡å‹ç»“æ„å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä»»æ„çš„ã€‚</p>
<p>è¿™è·ŸRNNæœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿå¾ˆç®€å•ï¼Œä¼˜åŒ–å™¨å¦‚SGDã€Adamç­‰ï¼Œå®ƒä»¬æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªå…³äºæ¨¡å‹å‚æ•°çš„RNNï¼å…¶å®è¿™ä¸ªè§‚ç‚¹å¹¶ä¸æ–°é²œï¼Œæ—©åœ¨2017å¹´Meta Learningç››è¡Œé‚£ä¼šå°±å·²ç»æœ‰ç ”ç©¶äººå‘˜æå‡ºå¹¶åˆ©ç”¨äº†è¿™ç‚¹ï¼Œåªä¸è¿‡å½“æ—¶çš„æƒ³æ³•æ˜¯å°è¯•ç”¨RNNï¼ˆLSTMï¼‰å»æ¨¡æ‹Ÿä¸€ä¸ªæ›´å¥½çš„ä¼˜åŒ–å™¨ï¼Œè¯¦æƒ…å¯ä»¥å‚è€ƒ<a href="https://openreview.net/forum?id=rJY0-Kcll">ã€ŠOptimization as a Model for Few-Shot Learningã€‹</a>ã€‚</p>
<p>æ­£æ‰€è°“â€œé£æ°´è½®æµè½¬â€ï¼Œæ—¶éš”å¤šå¹´TTTåè¿‡æ¥æå‡ºé€šè¿‡ä¼˜åŒ–å™¨æ¥æ„å»ºRNNã€‚å®ƒçš„æµç¨‹æ˜¯è¿™æ ·çš„ï¼šé¦–å…ˆï¼Œå½“å‰æ¨¡å‹å‚æ•°ä¸º$\boldsymbol{S}<em t-1="t-1">{t-1}$ï¼Œä¼˜åŒ–å™¨ï¼ˆSGDï¼‰æ¥æ”¶åˆ°æ–°æ•°æ®$(\boldsymbol{k}_t, \boldsymbol{v}_t)$ï¼Œæ ¹æ®è¯¥æ•°æ®å°†æ¨¡å‹å‚æ•°æ›´æ–°ä¸º$\boldsymbol{S}_t$ï¼Œæœ€åè¿”å›$\boldsymbol{q}_t$çš„é¢„æµ‹ç»“æœ$\boldsymbol{f}(\boldsymbol{S}</em>};\boldsymbol{q<em t-1="t-1">t)$ï¼Œä¾æ­¤ç±»æ¨ã€‚æ‰€ä»¥ï¼ŒTTTæ‰€å®ç°çš„RNNå¯ä»¥ç»Ÿä¸€åœ°å†™æˆ<br />
\begin{equation}\boldsymbol{o}_t = \boldsymbol{f}(\boldsymbol{S}_t; \boldsymbol{q}_t), \qquad \boldsymbol{S}_t = \boldsymbol{S}</em>} - \eta_t\nabla_{\boldsymbol{S<em t-1="t-1">{t-1}}\mathcal{L}(\boldsymbol{f}(\boldsymbol{S}</em>};\boldsymbol{k<em t-1="t-1">t), \boldsymbol{v}_t)\label{eq:ttt-rnn}\end{equation}<br />
å…¶ä¸­$\mathcal{L}(\boldsymbol{f}(\boldsymbol{S}</em>};\boldsymbol{k<em t-1="t-1">t), \boldsymbol{v}_t)$æ˜¯å½“å‰æ•°æ®$(\boldsymbol{k}_t, \boldsymbol{v}_t)$åœ¨å½“å‰å‚æ•°$\boldsymbol{S}</em>$éƒ½æ˜¯å®ƒçš„ä¸€ä¸ªç‰¹ä¾‹ï¼š}$ä¸‹çš„æŸå¤±å‡½æ•°ï¼Œ$\eta_t$åˆ™æ˜¯å­¦ä¹ ç‡å‚æ•°ï¼Œå‚è€ƒä¸Šä¸€èŠ‚çš„â€œdata-dependent decayâ€ï¼Œå®ƒä¹Ÿå¯ä»¥åšæˆdata-dependentçš„ã€‚è¿™ä¸ªå½¢å¼å¯ä»¥è¦†ç›–éå¸¸å¤šçš„RNNæ¨¡å‹ï¼Œæ¯”å¦‚å¼$\eqref{eq:linear-attn-rnn}$å’Œ$\eqref{eq:linear-attn-retnet<br />
$$\begin{array}{c|cc|ccc}  
\hline  
& \text{RNN} & \boldsymbol{o}_t & \boldsymbol{f}(\boldsymbol{S};\boldsymbol{k}) & \mathcal{L}(\boldsymbol{f}(\boldsymbol{S};\boldsymbol{k}),\boldsymbol{v}) & \eta_t \\\  
\hline  
\eqref{eq:linear-attn-rnn} & \boldsymbol{S}_t = \boldsymbol{S}_{t-1} + \boldsymbol{v}_t \boldsymbol{k}_t^{\top} & \boldsymbol{o}_t = \boldsymbol{S}_t \boldsymbol{q}_t & \boldsymbol{S}\boldsymbol{k} & -\boldsymbol{v}^{\top}(\boldsymbol{S}\boldsymbol{k}) & 1 \\\  
\eqref{eq:linear-attn-retnet} & \boldsymbol{S}_t = \gamma\boldsymbol{S}_{t-1} + \boldsymbol{v}_t \boldsymbol{k}_t^{\top} & \boldsymbol{o}_t = \boldsymbol{S}_t \boldsymbol{q}_t & \boldsymbol{S}\boldsymbol{k} & -\boldsymbol{v}^{\top}(\boldsymbol{S}\boldsymbol{k}) + \frac{1-\gamma}{2}\Vert\boldsymbol{S}\Vert_F^2 & 1 \\\  
\hline  
\end{array}$$<br />
TTTåŸæ–‡åˆ™è‡´åŠ›äºæ¢ç´¢mini-batchä¸‹çš„éçº¿æ€§RNNï¼Œåæ¥çš„<a href="https://papers.cool/arxiv/2501.00663">Titans</a>åˆ™ç»™TTTçš„SGDåŠ ä¸Šäº†åŠ¨é‡ï¼Œå†åé¢<a href="https://papers.cool/arxiv/2505.23884">ã€ŠTest-Time Training Done Rightã€‹</a>åˆ™æ¢ç´¢äº†large-batchçš„TTTç”¨æ³•ï¼Œè¿˜æ¢ç´¢äº†â€œTTT + Muonâ€çš„ç»„åˆã€‚æ³¨æ„ï¼ŒTTTåªæ˜¯åˆ©ç”¨ä¼˜åŒ–å™¨æ¥æ„å»ºRNNï¼ŒRNNä»¥å¤–çš„å‚æ•°å¦‚$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}$çš„å¯è®­ç»ƒå‚æ•°ï¼Œè¿˜æ˜¯å°†æ•´ä¸ªæ¨¡å‹æ„å»ºèµ·æ¥åç”¨æ•´ä½“çš„ä¼˜åŒ–å™¨è®­ç»ƒçš„ã€‚</p>
<p>ä¸€ä¸ªæ›´å€¼å¾—æ€è€ƒçš„é—®é¢˜æ˜¯ï¼šä¸ºä»€ä¹ˆTTTå¯ä»¥æˆä¸ºæ„å»ºRNNçš„â€œæŒ‡å¯¼åŸåˆ™â€å‘¢ï¼ŸRNNçš„æ ¸å¿ƒç›®æ ‡ï¼Œæ˜¯å°†å†å²æ•°æ®æœ‰æ•ˆåœ°å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šå¤§å°çš„Stateä¸­ï¼Œè€Œæ¨¡å‹å‚æ•°æ­£å¥½æ˜¯å›ºå®šå¤§å°çš„ï¼Œè®­ç»ƒæ¨¡å‹æŸç§ç¨‹åº¦ä¸Šå°±ç›¸å½“äºæŠŠè®­ç»ƒæ•°æ®å‹ç¼©åˆ°æ¨¡å‹æƒé‡ä¸­ï¼ŒTTTæ­£æ˜¯åˆ©ç”¨äº†å®ƒè·ŸRNNç›®æ ‡çš„é«˜åº¦å¥‘åˆæ€§ã€‚è¯´ç›´ç™½ä¸€ç‚¹ï¼Œå¦‚æœå°†RNNè§†ä¸ºä¸€ä¸ªå‹ç¼©ä»»åŠ¡ï¼ŒTTTå°†æ¨¡å‹$\boldsymbol{f}$è§†ä¸ºâ€œè§£å‹å™¨â€ï¼Œå®ƒçš„æƒé‡åˆ™æ˜¯â€œå‹ç¼©åŒ…â€ï¼Œè€Œå‹ç¼©ç®—æ³•åˆ™æ˜¯SGDï¼Œå‹ç¼©ç‡åˆ™æ˜¯æŸå¤±$\mathcal{L}$ã€‚</p>
<p>è¿™æ ·ä¸€æ¥ï¼Œæˆ‘ä»¬å°±ä¸ç”¨èŠ±å¿ƒæ€æ„å»ºé€’å½’æ ¼å¼äº†ï¼Œè½¬è€Œæ„å»ºæ¨¡å‹$\boldsymbol{f}$å’ŒæŸå¤±$\mathcal{L}$ï¼Œä¸€ä¸ªRNNå¼ºä¸å¼ºã€é ä¸é è°±ï¼Œæˆ‘ä»¬ä¹Ÿåªéœ€çœ‹å¯¹åº”çš„$\boldsymbol{f}$å’Œ$\mathcal{L}$å°±å¯ä»¥å¿ƒä¸­æœ‰æ•°ã€‚</p>
<p>é™¤æ­¤ä¹‹å¤–ï¼ŒTTTç”¨Online Learningæ„å»ºRNNï¼Œæ„å‘³ç€æ‰€å¾—RNNå¿…ç„¶éå¸¸å¥‘åˆICLï¼ˆIn Context Learningï¼‰ä»»åŠ¡ï¼Œè¿™ä¹Ÿæ˜¯TTTä½œä¸ºâ€œæŒ‡å¯¼åŸåˆ™â€çš„ä¼˜åŠ¿ã€‚æ­¤å‰<a href="https://papers.cool/arxiv/2212.10559">ã€ŠWhy Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizersã€‹</a>ç”šè‡³åè¿‡æ¥ï¼Œå°†Softmax Attentionå»æ‰Softmaxæˆçº¿æ€§Attentionæ¥è§£é‡Šå®ƒçš„ICLèƒ½åŠ›ï¼Œç”¨ç°åœ¨çš„è§†è§’çœ‹å®ƒå°±æ˜¯æ„é€ äº†å¯¹åº”çš„TTTå‡ºæ¥ã€‚</p>
<h2 id="_5">é™¤æ—§è€Œè¿æ–°</h2>
<p>ä¾‹å¦‚ï¼Œæœ€æ—©çš„çº¿æ€§Attentionå¯¹åº”çš„æŸå¤±å‡½æ•°æ˜¯$-\boldsymbol{v}^{\top}(\boldsymbol{S}\boldsymbol{k})$ï¼Œè¿™ä¸€çœ‹å°±æ˜¯ä¸ªä¸å¤§é è°±çš„ç›®æ ‡ï¼Œå› ä¸ºå®ƒæ˜¯æ— ä¸‹ç•Œçš„ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´$\boldsymbol{S}$è¶‹äºæ— ç©·ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRetNetå¾€æŸå¤±å‡½æ•°åŠ å…¥äº†L2æ­£åˆ™é¡¹ï¼Œé¿å…äº†è¿™ç§é£é™©ï¼Œä»ä¼˜åŒ–è§’åº¦çœ‹ä¹Ÿç¼“è§£äº†è¿‡æ‹Ÿåˆçš„é£é™©ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„RNNã€‚</p>
<p>ç„¶è€Œï¼Œç”¨å†…ç§¯ä½œä¸ºæŸå¤±å‡½æ•°è™½ç„¶ç®€æ´ä¸”æœ‰ä¸€å®šé“ç†ï¼Œä½†å®ƒä¸æ˜¯ç›´æ¥é¼“åŠ±$\boldsymbol{S}\boldsymbol{k}=\boldsymbol{v}$ï¼Œæ‰€ä»¥å¹¶éä¸€ä¸ªç†æƒ³çš„å›å½’æŸå¤±ã€‚æ›´å¥½çš„ç›®æ ‡å‡½æ•°åº”è¯¥æ˜¯å¹³æ–¹æŸå¤±ï¼Œå³$\frac{1}{2}\Vert\boldsymbol{S}\boldsymbol{k} - \boldsymbol{v}\Vert^2$ï¼Œå°†å®ƒä»£å…¥åˆ°TTTçš„å…¬å¼$\eqref{eq:ttt-rnn}$å¾—åˆ°<br />
\begin{equation}\boldsymbol{o}<em t-1="t-1">t = \boldsymbol{S}_t\boldsymbol{q}_t, \qquad \boldsymbol{S}_t = \boldsymbol{S}</em>} - \eta_t \underbrace{(\boldsymbol{S<em _nabla__boldsymbol_S="\nabla_{\boldsymbol{S">{t-1} \boldsymbol{k}_t - \boldsymbol{v}_t)\boldsymbol{k}_t^{\top}}</em><em t-1="t-1">{t-1}}\frac{1}{2}\Vert\boldsymbol{S}</em>}\boldsymbol{k<em t-1="t-1">t - \boldsymbol{v}_t\Vert^2}\end{equation}<br />
è¿™ä¾¿æ˜¯DeltaNetï¼Œè¿™ä¸ªåå­—å‡ºè‡ª<a href="https://papers.cool/arxiv/2406.06484">ã€ŠParallelizing Linear Transformers with the Delta Rule over Sequence Lengthã€‹</a>ï¼Œæ›´æ—©åˆ™æ˜¯ç”±<a href="https://papers.cool/arxiv/2102.11174">ã€ŠLinear Transformers Are Secretly Fast Weight Programmersã€‹</a>æå‡ºã€‚ç•™æ„åˆ°$\eta_t (\boldsymbol{S}</em>} \boldsymbol{k<em t-1="t-1">t - \boldsymbol{v}_t)\boldsymbol{k}_t^{\top} = (\boldsymbol{S}</em>} (\sqrt{\eta_t}\boldsymbol{k<em t-1="t-1">t) - (\sqrt{\eta_t}\boldsymbol{v}_t))(\sqrt{\eta_t}\boldsymbol{k}_t)^{\top}$ï¼Œè¿™æ„å‘³ç€$\eta_t$æ€»å¯ä»¥å¸æ”¶åˆ°$\boldsymbol{k}_t,\boldsymbol{v}_t$çš„å®šä¹‰ä¸­å»ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥ä¸‹æ¥çš„åˆ†æéƒ½åªè€ƒè™‘$\eta_t=1$çš„æƒ…å†µï¼š<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{S}_t =&amp;\, \boldsymbol{S}</em>} -(\boldsymbol{S<em t-1="t-1">{t-1} \boldsymbol{k}_t - \boldsymbol{v}_t)\boldsymbol{k}_t^{\top} \\\<br />
=&amp;\, \boldsymbol{S}</em>} -(\boldsymbol{S<em t-1="t-1">{t-1} \boldsymbol{k}_t)\boldsymbol{k}_t^{\top} + \boldsymbol{v}_t\boldsymbol{k}_t^{\top} \\\<br />
=&amp;\, \boldsymbol{S}</em>} (\boldsymbol{I} - \boldsymbol{k<em t-1="t-1">t\boldsymbol{k}_t^{\top}) + \boldsymbol{v}_t\boldsymbol{k}_t^{\top}<br />
\end{aligned}\label{eq:linear-attn-deltanet}\end{equation}<br />
å¦‚æœæœ‰éœ€è¦ï¼Œæˆ‘ä»¬å†æŠŠ$\boldsymbol{k}_t,\boldsymbol{v}_t$æ¢æˆ$\sqrt{\eta_t}\boldsymbol{k}_t,\sqrt{\eta_t}\boldsymbol{v}_t$ï¼Œå°±å¯ä»¥å°†$\eta_t$æ¢å¤å‡ºæ¥ã€‚å¯¹æ¯”çº¿æ€§Attentionæœ€æ—©çš„å½¢å¼$\eqref{eq:linear-attn-rnn}$ï¼ŒDeltaNetçš„åŒºåˆ«æ˜¯åœ¨åŠ $\boldsymbol{v}_t\boldsymbol{k}_t^{\top}$å‰å¤šå‡äº†ä¸ª$(\boldsymbol{S}</em>} \boldsymbol{k<em t-1="t-1">t)\boldsymbol{k}_t^{\top}$ï¼Œå…¶ä¸­$\boldsymbol{S}</em>} \boldsymbol{k<em t-1="t-1">t$å¯ä»¥ç†è§£ä¸ºæ–°è¾“å…¥$\boldsymbol{k}_t$åœ¨æ—§æ¨¡å‹$\boldsymbol{S}</em>$ä¸‹çš„é¢„æµ‹ç»“æœã€‚</p>
<p>ç›´è§‚æ¥æƒ³ï¼Œâ€œå…ˆå‡ååŠ â€å°±æ˜¯å…ˆç§»é™¤æ¨¡å‹å¯¹$\boldsymbol{k}_t$çš„æ—§è®¤çŸ¥ï¼Œç„¶åæ ¹æ®$(\boldsymbol{k}_t,\boldsymbol{v}_t)$è¡¥å……æ–°è®¤çŸ¥ï¼Œè¾¾åˆ°â€œé™¤æ—§è¿æ–°â€çš„æ•ˆæœã€‚è¿™ä¸ªè§„åˆ™ç§°ä¸ºâ€œ<a href="https://en.wikipedia.org/wiki/Delta_rule">Delta Rule</a>â€ï¼Œæ­£æ˜¯DeltaNetä¸€è¯ä¸­â€œDeltaâ€çš„æ¥æºã€‚Delta Ruleå¹¶ä¸æ–°é²œï¼Œå®ƒåˆç§°ä¸º<a href="https://en.wikipedia.org/wiki/Least_mean_squares_filter">Least Mean Square</a>ã€Widrow-Hoff Algorithmç­‰ï¼Œå·²ç»æ˜¯ä¸Šä¸ªä¸–çºª60å¹´ä»£çš„äº§ç‰©äº†ã€‚äº‹å®ä¸Šï¼Œè¿™ä¸ªé¢†åŸŸå®Œå…¨æ–°çš„ä¸œè¥¿å¾ˆå°‘ï¼Œå¾ˆå¤šæ”¹åŠ¨éƒ½å¯ä»¥è¿½æº¯åˆ°æŸä¸ªâ€œä¸Šå¤æ—¶æœŸâ€çš„å·¥ä½œï¼Œç›®å‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨æŒ–æ˜å…¶ä¸­èƒ½Scalableçš„éƒ¨åˆ†ã€‚</p>
<p>å¦å¤–éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼ŒæŒ‰ç…§æ—¶é—´çš„é¡ºåºï¼Œæ˜¯DeltaNetåœ¨å‰ï¼ŒTTTåœ¨åï¼Œä»Online Learningè§’åº¦ç†è§£RNNï¼Œå…¶å®åœ¨TTTä¹‹å‰å·²ç»é›¶æ˜Ÿåœ°ä½“ç°åœ¨ä¸€äº›å·¥ä½œä¸­ï¼Œä½†TTTç³»ç»Ÿåœ°æå‡ºäº†è¿™ä¸ªâ€œæŒ‡å¯¼åŸåˆ™â€ï¼Œå¹¶ä¸”å°†å®ƒç”¨äºæ„å»ºæ–°RNNæ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬æŠŠTTTæ”¾åœ¨å‰é¢ï¼Œä½¿å¾—æ•´ä¸ªä»‹ç»æ›´åŠ æµç•…è‡ªç„¶ä¸€äº›ã€‚</p>
<p>æœ‰äº›è¯»è€…å¯èƒ½ç–‘é—®ï¼šDeltaNetè¿˜ç®—çº¿æ€§RNNå—ï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ã€‚æˆ‘ä»¬æ‰€è¯´çš„çº¿æ€§RNNï¼Œæ˜¯æŒ‡é€’å½’å…¬å¼å¯¹Stateå˜é‡çš„ä¾èµ–å…³ç³»æ˜¯çº¿æ€§çš„ï¼Œä½†å¯¹è¾“å…¥æˆ–$\boldsymbol{q},\boldsymbol{k},\boldsymbol{v}$çš„ä¾èµ–å¯ä»¥æ˜¯éçº¿æ€§çš„ï¼ˆå½“ç„¶ä¸åŒä¾èµ–å½¢å¼çš„å¹¶è¡Œæ•ˆç‡ä¼šæœ‰æ‰€ä¸åŒï¼‰ï¼Œä»å¼$\eqref{eq:linear-attn-deltanet}$å¯ä»¥çœ‹å‡ºï¼Œç­‰å·å³ç«¯å§‹ç»ˆåªæ˜¯å‡ºç°äº†$\boldsymbol{S}_{t-1}$çš„ä¸€æ¬¡æ–¹ï¼Œæ‰€ä»¥å®ƒæ»¡è¶³çº¿æ€§çš„å®šä¹‰ã€‚</p>
<h2 id="_6">æ±‚é€†ä¸æ¨å¹¿</h2>
<p>å‰é¢æˆ‘ä»¬è¯´äº†ï¼Œçº¿æ€§RNNæœ€ç†æƒ³çš„ï¼ˆå³GPUé«˜æ•ˆçš„ï¼‰å¹¶è¡Œç®—æ³•æ˜¯å……åˆ†ä½¿ç”¨çŸ©é˜µä¹˜æ³•çš„å½¢å¼ã€‚ä¸ºäº†å®Œæˆè¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å…ˆå°†DeltaNetå†™æˆ<br />
\begin{equation}\boldsymbol{S}<em t-1="t-1">t = \boldsymbol{S}</em>} + (\boldsymbol{v<em t-1="t-1">t - \boldsymbol{S}</em>} \boldsymbol{k<em t-1="t-1">t)\boldsymbol{k}_t^{\top}\end{equation}<br />
è®°$\boldsymbol{u}_t = \boldsymbol{v}_t - \boldsymbol{S}</em>} \boldsymbol{k<em t-1="t-1">t$ï¼Œé‚£ä¹ˆ$\boldsymbol{S}_t = \boldsymbol{S}</em>} + \boldsymbol{u<em t-1="t-1">t\boldsymbol{k}_t^{\top}$ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒåªæ˜¯åœ¨æœ€æ—©çš„çº¿æ€§AttentionåŸºç¡€ä¸ŠæŠŠ$\boldsymbol{V}$æ¢æˆäº†$\boldsymbol{U}=[\boldsymbol{u}_1,\boldsymbol{u}_2,\cdots,\boldsymbol{u}_n]^{\top}$ï¼Œå°†å®ƒè¿­ä»£$t-1$æ¬¡ï¼Œæˆ‘ä»¬æœ‰<br />
\begin{equation}\boldsymbol{S}</em>} = \sum_{j=1}^{t-1} \boldsymbol{u<em j="1">j\boldsymbol{k}_j^{\top}\qquad\Rightarrow\qquad \boldsymbol{u}_t = \boldsymbol{v}_t - \left(\sum</em>}^{t-1} \boldsymbol{u<em j="1">j\boldsymbol{k}_j^{\top}\right)\boldsymbol{k}_t = \boldsymbol{v}_t - \sum</em>}^{t-1} \boldsymbol{u<em _text_è®°ä¸º="\text{è®°ä¸º">j(\boldsymbol{k}_j^{\top}\boldsymbol{k}_t)\end{equation}<br />
æœ€åçš„ç­‰å¼å†™æˆçŸ©é˜µå½¢å¼æ˜¯$\boldsymbol{U} = \boldsymbol{V} - (\boldsymbol{K}\boldsymbol{K}^{\top}\odot \boldsymbol{M}^-)\boldsymbol{U}$ï¼Œå…¶ä¸­$\boldsymbol{M}^-=\boldsymbol{M} - \boldsymbol{I}$ï¼Œè¿™æ˜¯ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹ç»„ï¼Œå®ƒçš„è§£å¯ä»¥ç›´æ¥è¡¨ç¤ºä¸º<br />
\begin{equation}\boldsymbol{U} = (\boldsymbol{I} + \underbrace{\boldsymbol{K}\boldsymbol{K}^{\top}\odot \boldsymbol{M}^-}</em>}\boldsymbol{B}})^{-1}\boldsymbol{V}\end{equation<br />
è¿™é‡Œå‡ºç°äº†$(\boldsymbol{I}+\boldsymbol{B})^{-1}$ï¼Œä¸€ä¸ª$n\times n$çŸ©é˜µçš„é€†ï¼Œæ ‡å‡†å¤æ‚åº¦æ˜¯$\mathcal{O}(n^3)$ï¼Œæ¯”Softmax Attentionè¿˜é«˜ï¼ä¸è¿‡å¥½åœ¨æˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼çš„é€†è€Œæ˜¯åªè¦$\boldsymbol{U}$ï¼Œè¿™å¯ä»¥è½¬åŒ–ä¸ºè§£æ–¹ç¨‹ç»„$(\boldsymbol{I}+\boldsymbol{B})\boldsymbol{U}=\boldsymbol{V}$ï¼Œå¤æ‚åº¦é™åˆ°$\mathcal{O}(n^2)$ã€‚è¿›ä¸€æ­¥åœ°ï¼Œåˆ©ç”¨$\boldsymbol{I}+\boldsymbol{B}$æ˜¯ä¸‹ä¸‰è§’é˜µä»¥åŠ$\boldsymbol{B}$çš„ä½ç§©ç»“æ„ï¼Œå¯ä»¥å°†å¤æ‚åº¦é™åˆ°çº¿æ€§ï¼Œå†™æˆåˆ†å—çŸ©é˜µä¹˜æ³•åå°±å¯ä»¥å……åˆ†åˆ©ç”¨GPUã€‚è¿™äº›ç»†èŠ‚åªèƒ½è¯·å¤§å®¶é˜…è¯»åŸè®ºæ–‡äº†ï¼Œæœ¬æ–‡å…ˆæŠŠä¸»è¦æ•°å­¦åŸç†ä»‹ç»æ¸…æ¥šã€‚</p>
<p>DeltaNetä¹‹åï¼Œ<a href="https://papers.cool/arxiv/2412.06464">Gated DeltaNetï¼ˆGDNï¼‰</a>è¿›ä¸€æ­¥åœ°å°†é—å¿˜é—¨å¼•å…¥åˆ°DeltaNetä¹‹ä¸­ï¼Œè¿™å€’æ˜¯å¯ä»¥é¢„æ–™çš„å˜åŒ–ã€‚Gated DeltaNetçš„åŸå§‹å¼•å…¥æ–¹å¼æ˜¯<br />
\begin{equation}\boldsymbol{S}<em t-1="t-1">t = \alpha_t \boldsymbol{S}</em>} (\boldsymbol{I} - \beta_t\boldsymbol{k<em t-1="t-1">t\boldsymbol{k}_t^{\top}) + \beta_t\boldsymbol{v}_t\boldsymbol{k}_t^{\top}\label{eq:gdn-orgi}\end{equation}<br />
ä½†ä¸ªäººè®¤ä¸ºï¼Œè¿™ä¸ªææ³•å…¶å®æ˜¾å¼æ‰“ç ´äº†Delta Ruleï¼Œæ›´å¥½çš„ææ³•åº”è¯¥æ˜¯åƒ<a href="https://papers.cool/arxiv/2506.02475">Comba</a>ä¸€æ ·ï¼Œåªä¹˜åˆ°ç¬¬ä¸€ä¸ª$\boldsymbol{S}</em>$ä¸Šï¼š<br />
\begin{equation}\boldsymbol{S}<em t-1="t-1">t = \gamma_t\boldsymbol{S}</em>} + \eta_t(\boldsymbol{v<em t-1="t-1">t - \boldsymbol{S}</em>}\boldsymbol{k<em t-1="t-1">t)\boldsymbol{k}_t^{\top}\label{eq:gdn-comba}\end{equation}<br />
å®ƒç›¸å½“äºå°†æŸå¤±å‡½æ•°å–$\frac{1}{2}\Vert\boldsymbol{S}\boldsymbol{k} - \boldsymbol{v}\Vert^2 + \frac{1-\gamma}{\eta}\Vert\boldsymbol{S}\Vert_F^2$ã€‚å½“ç„¶ï¼Œä»æ•°å­¦ä¸Šæ¥è¯´ï¼Œè¿™ä¸¤ä¸ªææ³•éƒ½æ˜¯ç­‰ä»·çš„ï¼š<br />
\begin{equation}\alpha_t\boldsymbol{S}</em>} (\boldsymbol{I} - \beta_t\boldsymbol{k<em t-1="t-1">t\boldsymbol{k}_t^{\top}) + \beta_t\boldsymbol{v}_t\boldsymbol{k}_t^{\top} = \alpha_t \boldsymbol{S}</em>} + \alpha_t \beta_t (\boldsymbol{v<em t-1="t-1">t/\alpha_t - \boldsymbol{S}</em>}\boldsymbol{k}_t)\boldsymbol{k}_t^{\top}\end{equation<br />
å³$\gamma_t = \alpha_t, \eta_t = \alpha_t \beta_t$ç„¶åæŠŠ$1/\alpha_t$å¸æ”¶åˆ°$\boldsymbol{v}_t$å°±å¯ä»¥è½¬åŒ–ä¸ºåè€…äº†ã€‚æ‰€ä»¥è¯´ï¼Œè¿™ä¸¤ä¸ªå½¢å¼åœ¨æ•°å­¦ä¸Šå¹¶æ²¡æœ‰åŒºåˆ«ï¼Œç”±äºå¤šæ•°$\alpha_t$ä¼šæ¥è¿‘äº1ï¼Œæ‰€ä»¥èƒ½åŠ›ä¸Šä¼°è®¡ä¹Ÿæ²¡å•¥åŒºåˆ«ï¼ˆCombaè¯´$\eqref{eq:gdn-comba}$ä¼šå¥½ä¸€ç‚¹ï¼‰ï¼Œåªä¸è¿‡åè€…æ›´ç›´è§‚åœ°ä¿ç•™äº†Delta Ruleçš„æ ·å­ã€‚</p>
<p>ä»ç†è®ºä¸Šæ¥è¯´ï¼ŒGated DeltaNetä¹Ÿå¯ä»¥å†™æˆDeltaNetçš„å½¢å¼ï¼Œå› ä¸ºåªéœ€è¦å®šä¹‰$\bar{\alpha}<em j="1">t = \prod</em>}^t \alpha_t$ï¼Œé‚£ä¹ˆå¼$\eqref{eq:gdn-orgi}$ä¸¤è¾¹åŒæ—¶é™¤ä»¥$\bar{\alpha<em t-1="t-1">t$ï¼Œå°±å¾—åˆ°<br />
\begin{equation}\bar{\alpha}_t^{-1}\boldsymbol{S}_t = \bar{\alpha}</em>}^{-1}\boldsymbol{S}_{t-1} (\boldsymbol{I} - \beta_t\boldsymbol{k}_t\boldsymbol{k}_t^{\top}) + \beta_t(\bar{\alpha}_t^{-1}\boldsymbol{v}_t)\boldsymbol{k}_t^{\top}\end{equation<br />
ç„¶åç»“åˆ$\boldsymbol{o}_t = \boldsymbol{S}_t \boldsymbol{q}_t = (\bar{\alpha}_t^{-1}\boldsymbol{S}_t) (\bar{\alpha}_t\boldsymbol{q}_t)$ï¼Œå¯ä»¥å‘ç°åªéœ€è¦åˆ†åˆ«å°†$\bar{\alpha}_t\boldsymbol{q}_t,\bar{\alpha}_t^{-1}\boldsymbol{v}_t$è®¾ç½®ä¸ºæ–°çš„$\boldsymbol{q}_t,\boldsymbol{v}_t$ï¼Œé‚£ä¹ˆå°±èƒ½ç®€åŒ–æˆDeltaNetçš„å½¢å¼ã€‚ä¸è¿‡ï¼Œè¿™ä¸ªç»“æœåªæœ‰åœ¨æŸäº›æƒ…å†µä¸‹å…·æœ‰ç†è®ºæ¨å¯¼çš„ä»·å€¼ï¼ˆæ¯”å¦‚æ¨å¯¼ä¸‹ä¸€èŠ‚çš„AttentionçŸ©é˜µï¼‰ï¼Œå› ä¸ºå®é™…è®¡ç®—ä¸­ï¼Œä¸ç®¡æ€ä¹ˆå‚æ•°åŒ–ï¼Œå¯¹äºè¶³å¤Ÿå¤§çš„$t$ï¼Œ$\bar{\alpha}_t$å’Œ$\bar{\alpha}_t^{-1}$ä¹‹ä¸€å¿…æœ‰æº¢å‡ºçš„é£é™©ã€‚</p>
<p>DeltaNetä¹‹åè¿˜æœ‰å¦ä¸€ä¸ªæ¨å¹¿<a href="https://papers.cool/arxiv/2502.10297">DeltaProduct</a>ï¼Œå®ƒæ˜¯å°†$\boldsymbol{k},\boldsymbol{v}$æ‰©å±•è‹¥å¹²å€åå†åšDeltaNetæˆ–è€…Gated DeltaNetï¼Œè¯•å›¾å¢å¼ºæ¨¡å‹çš„çŠ¶æ€è¿½è¸ªèƒ½åŠ›ã€‚ä¸è¿‡ï¼Œå°±ç¬”è€…çš„å®¡ç¾è€Œè¨€ï¼Œä¸å…¶åƒDeltaProducté‚£æ ·æ‰©å±•å¸¸æ•°å€ï¼Œè¿˜ä¸å¦‚åƒ<a href="/archives/10017">ã€Šæ—¶ç©ºä¹‹ç« ï¼šå°†Attentionè§†ä¸ºå¹³æ–¹å¤æ‚åº¦çš„RNNã€‹</a>ä¸€æ ·å°è¯•å¹³æ–¹å¤æ‚åº¦çš„RNNï¼Œçœ‹æœ‰æ²¡æœ‰æœºä¼šè¶…è¶ŠSoftmax Attentionã€‚</p>
<h2 id="_7">åå“ºè¿›è¡Œæ—¶</h2>
<p>è¯´åˆ°è¶…è¶ŠSoftmax Attentionï¼Œå¼€å¤´æåˆ°ï¼Œå¦‚ä»Šçš„çº¿æ€§Attentionä¸ä»…èƒ½ä¸Softmax Attentionä¸€è¾ƒé«˜ä½ï¼Œç”šè‡³å¼€å§‹â€œåå“ºâ€å®ƒã€‚è¿™çœ‹ä¼¼ä¸å¯æ€è®®ï¼Œä½†ç»†æ€ä¹‹ä¸‹å¹¶ä¸éš¾ç†è§£ã€‚æŸç§æ„ä¹‰ä¸Šï¼Œè¿™äº›å¹´Softmax Attentionä¸€ç›´åœ¨é€€æ­¥ï¼Œä»MHAã€GQAåˆ°MQAéƒ½æ˜¯ä¸ºäº†å‹ç¼©KV Cacheè€Œåšå‡æ³•ã€‚è€Œçº¿æ€§Attentionæ²¡æœ‰KV Cacheé—®é¢˜ï¼Œæ‰€ä»¥ä¸€ç›´å¾€æ›´å¥½çš„æ–¹å‘å‰è¿›ã€‚</p>
<p>ä¸ºäº†æ›´å¥½çœ‹å‡ºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¸å¦¨å°†å‰é¢æåˆ°çš„Attentionæœºåˆ¶éƒ½ä»¥çŸ©é˜µå½¢å¼å†™å‡ºæ¥ï¼š<br />
\begin{array}{c|c}<br />
\hline<br />
&amp; \text{å…¬å¼} \\\[4pt]<br />
\hline<br />
\text{Softmax Attention} &amp; (\exp(\boldsymbol{Q}\boldsymbol{K}^{\top})\odot \boldsymbol{M})\boldsymbol{V} \\\[4pt]<br />
\text{æœ€æ—©çš„çº¿æ€§Attention} &amp; (\boldsymbol{Q}\boldsymbol{K}^{\top}\odot \boldsymbol{M})\boldsymbol{V} \\\[4pt]<br />
\text{åŠ å…¥é—å¿˜é—¨å} &amp; (\boldsymbol{Q}\boldsymbol{K}^{\top}\odot \boldsymbol{\Gamma})\boldsymbol{V} \\\[4pt]<br />
\text{DeltaNet} &amp; (\boldsymbol{Q}\boldsymbol{K}^{\top}\odot \boldsymbol{M})(\boldsymbol{I} + \boldsymbol{K}\boldsymbol{K}^{\top}\odot \boldsymbol{M}^-)^{-1}\boldsymbol{V} \\\[4pt]<br />
\text{Gated DeltaNet} &amp; \begin{gathered}((\boldsymbol{Q}\boldsymbol{K}^{\top}\odot \boldsymbol{M})(\boldsymbol{I} + \boldsymbol{K}\boldsymbol{K}^{\top}\odot \boldsymbol{M}^-)^{-1}\odot\boldsymbol{\Gamma})\boldsymbol{V} \\\ =(\boldsymbol{Q}\boldsymbol{K}^{\top}\odot \boldsymbol{\Gamma})(\boldsymbol{I} + \boldsymbol{K}\boldsymbol{K}^{\top}\odot \boldsymbol{\Gamma}^-)^{-1}\boldsymbol{V}\end{gathered} \\\[4pt]<br />
\hline<br />
\end{array}<br />
å…¶ä¸­<br />
\begin{equation}\Gamma_{i,j} = \left\{\begin{aligned} &amp;\prod_{\tau=j+1}^i \gamma_{\tau}, &amp;i &gt; j \\\[6pt] &amp;\qquad 1, &amp;i = j \\\[6pt] &amp;\qquad 0, &amp;i &lt; j\end{aligned}\right.\end{equation}<br />
ä»¥åŠ$\boldsymbol{\Gamma}^- = \boldsymbol{\Gamma} - \boldsymbol{I}$ã€‚è¿™æ ·çœ‹æ¥ï¼ŒSoftmax Attentionçš„å½¢å¼è¿˜ä»…åœç•™åœ¨æœ€æ—©çš„çº¿æ€§Attentioné‚£ä¼šï¼ˆå½“ç„¶è¿™ä¹Ÿè¯æ˜äº†å®ƒçš„å¼ºå¤§ï¼‰ã€‚é‚£â€œåå“ºâ€æ€ä¹ˆå®ç°å‘¢ï¼Ÿé¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æŠŠSoftmax Attentionè½¬åŒ–ä¸ºçº¿æ€§Attentionï¼Œè¿™ä¸ªå¹¶ä¸éš¾ï¼Œæ—©åœ¨<a href="/archives/8601">ã€ŠTransformerå‡çº§ä¹‹è·¯ï¼š5ã€ä½œä¸ºæ— é™ç»´çš„çº¿æ€§Attentionã€‹</a>æˆ‘ä»¬å°±æ€»ç»“äº†ä¸‰ç§å°†Softmax Attentionè½¬åŒ–ä¸º <em>æ— é™ç»´</em> çº¿æ€§Attentionçš„æ–¹æ¡ˆã€‚</p>
<p>æ€»ä¹‹ï¼Œå°±æ˜¯å­˜åœ¨ä¸€ä¸ªæ˜ å°„$\phi$ï¼Œå°†$\boldsymbol{Q},\boldsymbol{K}$ä»$n\times d$æ˜ å°„åˆ°$n\times \infty$ï¼Œæ»¡è¶³$\exp(\boldsymbol{Q}\boldsymbol{K}^{\top}) = \phi(\boldsymbol{Q})\phi(\boldsymbol{K})^{\top}$ï¼Œè¿™ç§°ä¸ºâ€œæ ¸æŠ€å·§â€ã€‚é‚£æ¥ä¸‹æ¥çš„äº‹æƒ…å°±ç®€å•äº†ï¼Œæˆ‘ä»¬åªéœ€å°†ä¸Šè¿°è¡¨æ ¼ä¸­çš„çº¿æ€§Attentionçš„$\boldsymbol{Q},\boldsymbol{K}$æ¢æˆ$\phi(\boldsymbol{Q}),\phi(\boldsymbol{K})$ï¼Œæœ€åå†è®¾æ³•æ¢å¤$\exp$å¹¶å½’ä¸€åŒ–ï¼Œå°±å¾—åˆ°æ–°çš„Softmax Attentionå˜ä½“äº†ã€‚ä¾‹å¦‚ï¼Œä»£å…¥åˆ°é—å¿˜é—¨çš„å…¬å¼ï¼Œæˆ‘ä»¬æœ‰<br />
\begin{equation}(\phi(\boldsymbol{Q})\phi(\boldsymbol{K})^{\top}\odot \boldsymbol{\Gamma})\boldsymbol{V} = \exp(\boldsymbol{Q}\boldsymbol{K}^{\top} + \log\boldsymbol{\Gamma})\boldsymbol{V}\end{equation}<br />
å¦‚æœ$\gamma_t$å–å¸¸æ•°ï¼Œé‚£ä¹ˆå…¶å®å°±æ˜¯<a href="https://papers.cool/arxiv/2108.12409">ã€ŠTrain Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolationã€‹</a>æ‰€æçš„ALIBIï¼Œè€Œå¦‚æœ$\gamma_t$æ˜¯ä¾èµ–äºè¾“å…¥çš„ï¼Œé‚£ä¹ˆå°±æ˜¯<a href="https://papers.cool/arxiv/2503.02130">ã€ŠForgetting Transformer: Softmax Attention with a Forget Gateã€‹</a>æ‰€æçš„FoXã€‚</p>
<p>ä¸€ä¸ªæ›´æœ‰æ„æ€çš„ç»“æœæ˜¯<a href="https://papers.cool/arxiv/2505.19488">ã€ŠUnderstanding Transformer from the Perspective of Associative Memoryã€‹</a>æ‰€æçš„DeltaFormerï¼Œé¡¾åæ€ä¹‰å®ƒæ˜¯Softmax Attentionçš„DeltaNetç‰ˆæœ¬ã€‚å°†DeltaNetçš„$\boldsymbol{Q},\boldsymbol{K}$æ¢æˆ$\phi(\boldsymbol{Q}),\phi(\boldsymbol{K})$ï¼Œæˆ‘ä»¬æœ‰<br />
\begin{equation}\begin{aligned}<br />
&amp;\,(\phi(\boldsymbol{Q})\phi(\boldsymbol{K})^{\top}\odot \boldsymbol{M})(\boldsymbol{I} + \phi(\boldsymbol{K})\phi(\boldsymbol{K})^{\top}\odot \boldsymbol{M}^-)^{-1}\boldsymbol{V} \\\[8pt]<br />
=&amp;\,\underbrace{\exp(\boldsymbol{Q} \boldsymbol{K}^{\top} + \log\boldsymbol{M})}<em _text_è®°ä¸º="\text{è®°ä¸º">{\text{è®°ä¸º}\boldsymbol{A}}(\boldsymbol{I} + \underbrace{\exp(\boldsymbol{K} \boldsymbol{K}^{\top} + \log\boldsymbol{M}^-)}</em>}\boldsymbol{B}})^{-1}\boldsymbol{V<br />
\end{aligned}\end{equation}<br />
å¦‚æœè¦å½’ä¸€åŒ–ï¼Œæˆ‘ä»¬å°†$\exp$æ¢æˆ$\text{softmax}$å³å¯ã€‚ç›¸æ¯”Softmax Attentionï¼ŒDeltaFormerå°†åŸæœ¬çš„$\boldsymbol{A}\boldsymbol{V}$æ”¹æˆäº†$\boldsymbol{A}(\boldsymbol{I}+\boldsymbol{B})^{-1}\boldsymbol{V}$ï¼Œæ³¨æ„åˆ°<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{A}(\boldsymbol{I}+\boldsymbol{B})^{-1}\boldsymbol{V} =&amp;\, \boldsymbol{A}(\boldsymbol{I}-\boldsymbol{B}+\boldsymbol{B}^2- \boldsymbol{B}^3 + \cdots)\boldsymbol{V} \\\<br />
=&amp;\, \boldsymbol{A}(\boldsymbol{V}-\boldsymbol{B}\boldsymbol{V}+\boldsymbol{B}^2\boldsymbol{V}- \boldsymbol{B}^3\boldsymbol{V} + \cdots)<br />
\end{aligned}\end{equation}<br />
æ‰€ä»¥DeltaFormerç›¸å½“äºå…ˆç”¨$\boldsymbol{K},\boldsymbol{K},\boldsymbol{V}$ç®—å¤šæ¬¡Attentionï¼Œå°†ç»“æœå åŠ èµ·æ¥åä½œä¸ºæ–°çš„$\boldsymbol{V}$ï¼Œå†è·Ÿ$\boldsymbol{Q},\boldsymbol{K}$ç®—ä¸€æ¬¡Attentionï¼Œè¿™ä¸ªç‰¹æ€§è®©å®ƒå¯¹Multi-Hopçš„ä»»åŠ¡æœ‰å¥‡æ•ˆï¼ˆæ¯”å¦‚Codeï¼‰ã€‚æ­¤å¤–ï¼ŒDeltaFormerçš„è¿™ä¸ªç‰¹ç‚¹è¿˜æ„å‘³ç€å®ƒè·ŸMQAç‰¹åˆ«æ­é…ï¼Œå› ä¸º$(\boldsymbol{I}+\boldsymbol{B})^{-1}\boldsymbol{V}$è¿™éƒ¨åˆ†åªæœ‰$\boldsymbol{K},\boldsymbol{V}$å‚ä¸ï¼Œè€Œå¯¹äºMQAæ¥è¯´$\boldsymbol{K},\boldsymbol{V}$åªæœ‰Single-Headï¼Œè®¡ç®—é‡ç›¸æ¯”MHAä¼šæ˜æ˜¾é™ä½ã€‚</p>
<p>ä¸è¿‡ï¼Œåœ¨ç¬”è€…çœ‹æ¥ï¼Œè¿™ç§å›ºå®šç³»æ•°çš„å åŠ å¯èƒ½æ˜¯â€œæ²¡æœ‰å…è´¹åˆé¤â€ï¼Œæ¯”å¦‚ç¬”è€…çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeltaFormerçš„è¯­è¨€æ¨¡å‹æŸå¤±å¹¶æ— å¤ªå¤§å˜åŒ–ï¼Œè¿™æ„å‘³ç€å¦‚æœæŸäº›ä»»åŠ¡çš„æŸå¤±æ˜æ˜¾é™ä½ï¼Œå¿…ç„¶æœ‰å¦ä¸€äº›ä»»åŠ¡çš„æŸå¤±ä¸Šå‡äº†ã€‚</p>
<h2 id="_8">ç¡¬æ ¸ç¼–ç æœ¯</h2>
<p>è¿˜æœ‰ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„åå“ºå·¥ä½œæ˜¯PaTH Attentionï¼Œå‡ºè‡ª<a href="https://papers.cool/arxiv/2505.16381">ã€ŠPaTH Attention: Position Encoding via Accumulating Householder Transformationsã€‹</a>ï¼Œå®ƒä»ä½ç½®ç¼–ç çš„è§’åº¦å°†DeltaNetåå“ºåˆ°Softmax Attentionã€‚</p>
<p>æˆ‘ä»¬åœ¨<a href="/archives/9403">ã€ŠTransformerå‡çº§ä¹‹è·¯ï¼š6ã€æ—‹è½¬ä½ç½®ç¼–ç çš„å®Œå¤‡æ€§åˆ†æã€‹</a>æŒ‡å‡ºï¼Œå¯¹äºä»»ä½•æ­£äº¤çŸ©é˜µ$\boldsymbol{\Omega}$ï¼Œ$\boldsymbol{R}_m = \boldsymbol{\Omega}^m$éƒ½æ˜¯å¹¿ä¹‰çš„RoPEã€‚é™¤äº†æ—‹è½¬çŸ©é˜µï¼Œè¿˜æœ‰å“ªäº›å®¹æ˜“æ„å»ºçš„æ­£äº¤çŸ©é˜µå‘¢ï¼ŸPaTHç”¨çš„æ˜¯<a href="https://en.wikipedia.org/wiki/Householder_transformation">HouseholderçŸ©é˜µ</a>ï¼šè®¾$\boldsymbol{w}$æ˜¯ä»»æ„æ¨¡é•¿ä¸º$\sqrt{2}$çš„åˆ—å‘é‡ï¼Œé‚£ä¹ˆ$\boldsymbol{I}-\boldsymbol{w}\boldsymbol{w}^{\top}$æ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼Œè¿™æˆ‘ä»¬åœ¨<a href="/archives/8453">ã€Šä»ä¸€ä¸ªå•ä½å‘é‡å˜æ¢åˆ°å¦ä¸€ä¸ªå•ä½å‘é‡çš„æ­£äº¤çŸ©é˜µã€‹</a>ä¹Ÿæ¨å¯¼è¿‡ï¼Œå‡ ä½•æ„ä¹‰æ˜¯é•œé¢åå°„ã€‚</p>
<p>å®¹æ˜“çœ‹å‡ºï¼Œè¿™è·ŸDeltaNetä¸­$\boldsymbol{S}<em i-1="i-1">{t-1}$æ‰€ä¹˜çš„$\boldsymbol{I}-\boldsymbol{k}_t\boldsymbol{k}_t^{\top}$æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥PaTHå¹²è„†æŠŠè¿™éƒ¨åˆ†ç…§æ¬è¿‡æ¥ï¼Œå³æ”¾å¼ƒ$\boldsymbol{\Omega}^m$è¿™ä¸ªå½¢å¼ï¼Œä¹Ÿæ”¾å¼ƒ$\boldsymbol{w}$æ¨¡é•¿ä¸º$\sqrt{2}$çš„çº¦æŸï¼Œç›´æ¥ç”¨ä¸€ç³»åˆ—$\boldsymbol{I}-\boldsymbol{w}\boldsymbol{w}^{\top}$è¿ä¹˜æ¥è¡¨è¾¾ä½ç½®ä¿¡æ¯ï¼š<br />
\begin{equation}\boldsymbol{q}_i^{\top}\boldsymbol{k}_j \qquad\to\qquad \boldsymbol{q}_i^{\top}\underbrace{(\boldsymbol{I}-\boldsymbol{w}_i\boldsymbol{w}_i^{\top})(\boldsymbol{I}-\boldsymbol{w}</em>}\boldsymbol{w<em j_1="j+1">{i-1}^{\top})\cdots(\boldsymbol{I}-\boldsymbol{w}</em>}\boldsymbol{w<em _text_è®°ä¸º="\text{è®°ä¸º">{j+1}^{\top})}</em>}\boldsymbol{R<em i_j="i,j">{i,j}}\boldsymbol{k}_j \end{equation}<br />
å°†$\boldsymbol{R}</em>}$å†™æˆé€’å½’å½¢å¼æ˜¯$\boldsymbol{R<em i-1_j="i-1,j">{i,j} = (\boldsymbol{I}-\boldsymbol{w}_i\boldsymbol{w}_i^{\top})\boldsymbol{R}</em>},\boldsymbol{R<em i_j="i,j">{j,j} = \boldsymbol{I}$ã€‚å¯¹æ¯”DeltaNetçš„å¼$\eqref{eq:linear-attn-deltanet}$ï¼Œä¸Šå¼ç›¸å½“äº$\boldsymbol{v}_t$æ’ç­‰äºé›¶ï¼Œä½†åˆå€¼$\boldsymbol{S}_0$ä¸å†æ˜¯é›¶ã€‚ä½¿ç”¨â€œæ±‚é€†æ¥ç›¸åŠ©â€ä¸€èŠ‚åŒæ ·çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°<br />
\begin{equation}\boldsymbol{R}</em>} = \boldsymbol{I} - \boldsymbol{W<em _j:i_="[j:i]">{[j:i]}^{\top}(\boldsymbol{I} + \boldsymbol{W}</em>}\boldsymbol{W<em _j:i_="[j:i]">{[j:i]}^{\top}\odot\boldsymbol{M}^-)^{-1}\boldsymbol{W}</em>}\end{equation<br />
å…¶ä¸­$\boldsymbol{W}=[\boldsymbol{w}<em _j:i_="[j:i]">1,\boldsymbol{w}_2,\cdots,\boldsymbol{w}_n]^{\top}$ï¼Œåˆ‡ç‰‡æŒ‰Numpyæ¥ç†è§£ï¼Œå¦‚$\boldsymbol{W}</em>}=[\boldsymbol{w<em j_2="j+2">{j+1},\boldsymbol{w}</em>},\cdots,\boldsymbol{w<em _j:i_="[j:i]">i]^{\top}$ï¼Œåˆ‡ç‰‡ä¼˜å…ˆçº§é«˜äºè½¬ç½®ã€‚æ³¨æ„æ±‚é€†çš„æ˜¯ä¸‹ä¸‰è§’é˜µï¼Œä¸‰è§’é˜µæœ‰ä¸€ä¸ªé‡è¦ç‰¹æ€§ï¼Œé€†çŸ©é˜µçš„å¯¹è§’çº¿å…ƒç´ ç­‰äºåŸçŸ©é˜µå¯¹è§’çº¿å…ƒç´ çš„å€’æ•°ï¼Œå¦‚æœæ˜¯åˆ†å—ä¸‰è§’é˜µåˆ™å¯¹è§’å—ä¹Ÿæ»¡è¶³è¿™ä¸ªç‰¹æ€§ï¼Œäºæ˜¯æˆ‘ä»¬å¯ä»¥å†™å‡º<br />
\begin{equation}(\boldsymbol{I} + \boldsymbol{W}</em>}\boldsymbol{W<em _text_è®°ä¸º="\text{è®°ä¸º">{[j:i]}^{\top}\odot\boldsymbol{M}^-)^{-1} = (\underbrace{(\boldsymbol{I} + \boldsymbol{W}\boldsymbol{W}^{\top}\odot\boldsymbol{M}^-)^{-1}}</em>)}\boldsymbol{J}<em i_j="i,j">{[j:i,j:i]}\end{equation}<br />
æ¥ä¸‹æ¥çš„å˜æ¢ï¼Œå†™æˆåˆ†é‡å½¢å¼å¯èƒ½å¥½ç†è§£ä¸€äº›<br />
\begin{equation}\begin{aligned}<br />
A</em>} =&amp;\, \boldsymbol{q<em i_j="i,j">i^{\top} \boldsymbol{R}</em>} \boldsymbol{k<em _j:i_="[j:i]">j \\\[6pt]<br />
=&amp;\, \boldsymbol{q}_i^{\top}\boldsymbol{k}_j - \boldsymbol{q}_i^{\top}\boldsymbol{W}</em>}^{\top}\boldsymbol{J<em _j:i_="[j:i]">{[j:i,j:i]}\boldsymbol{W}</em>}\boldsymbol{k<em p="1">j \\\<br />
=&amp;\, \boldsymbol{q}_i^{\top}\boldsymbol{k}_j - \sum</em> \\\}^d \sum_{l=j+1}^i \sum_{r=j+1}^i \sum_{s=1}^d Q_{i,p} W_{l,p} J_{l,r} W_{r,s} K_{j,s<br />
=&amp;\, \boldsymbol{q}<em p="1">i^{\top}\boldsymbol{k}_j - \sum</em> \\\}^d \sum_{l=1}^i \sum_{r=j+1}^n \sum_{s=1}^d Q_{i,p} W_{l,p} J_{l,r} W_{r,s} K_{j,s<br />
=&amp;\, \boldsymbol{q}<em p="1">i^{\top}\boldsymbol{k}_j - \sum</em> \\\}^d \sum_{l=1}^n \sum_{r=1}^n \sum_{s=1}^d Q_{i,p} W_{l,p} \chi_{l \leq i} J_{l,r} \chi_{r \geq j+1}W_{r,s} K_{j,s<br />
=&amp;\, \boldsymbol{q}<em l="1">i^{\top}\boldsymbol{k}_j - \sum</em>}^n \sum_{r=1}^n \underbrace{\left(\chi_{l \leq i}\sum_{p=1}^d Q_{i,p} W_{l,p}\right)<em l_r="l,r">{\boldsymbol{Q}\boldsymbol{W}^{\top}\odot\boldsymbol{M}} J</em>} \underbrace{\left(\chi_{r \geq j+1} \sum_{s=1}^d W_{r,s} K_{j,s}\right)<em l_r="l,r">{\boldsymbol{W}\boldsymbol{K}^{\top}\odot\boldsymbol{M}^-} \\\<br />
\end{aligned}\end{equation}<br />
è¿™é‡Œæœ‰å‡ ä¸ªå…³é”®ç‚¹ï¼šæ¯”è¾ƒå·§å¦™çš„æ˜¯ç¬¬4ä¸ªç­‰å·ï¼Œå®ƒåˆ©ç”¨äº†$\boldsymbol{J}$æ˜¯ä¸‹ä¸‰è§’çŸ©é˜µè¿™ä¸€ç‚¹ï¼Œæ‰€ä»¥$l &lt; r$æ—¶$J</em>$çš„ä¸‹ä¸‰è§’éƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰ã€‚}$è‡ªåŠ¨ä¸ºé›¶ï¼›ç¬¬5ä¸ªç­‰å·ï¼Œ$\chi$ä¸ºç¤ºæ€§å‡½æ•°ï¼Œæ»¡è¶³ä¸‹æ ‡çš„æ¡ä»¶æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0ï¼›ç¬¬6ä¸ªç­‰å·ï¼Œå½“æˆ‘ä»¬åˆ†åˆ«å¤„ç†$p,s$ä¸¤éƒ¨åˆ†æ±‚å’Œæ—¶ï¼Œç»“æœæ˜¯$\boldsymbol{Q}\boldsymbol{W}^{\top}$å’Œ$\boldsymbol{W}\boldsymbol{K}^{\top}$ï¼Œè€Œä¹˜$\chi_{l \leq i}$åˆšå¥½è¡¨ç¤ºä¿ç•™$\boldsymbol{Q}\boldsymbol{W}^{\top}$çš„ä¸‹ä¸‰è§’éƒ¨åˆ†ï¼ˆè¿åŒå¯¹è§’çº¿ï¼‰ï¼Œè€Œä¹˜$\chi_{r \geq j+1}$åˆ™è¡¨ç¤ºä¿ç•™$\boldsymbol{W}\boldsymbol{K}^{\top</p>
<p>è‡³æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ•´ä¸ªï¼ˆSoftmaxä¹‹å‰çš„ï¼‰æ³¨æ„åŠ›çŸ©é˜µå†™å‡ºæ¥ï¼š<br />
\begin{equation}\boldsymbol{A} = \boldsymbol{Q}\boldsymbol{K}^{\top}\odot\boldsymbol{M} - (\boldsymbol{Q}\boldsymbol{W}^{\top}\odot\boldsymbol{M})(\boldsymbol{I} + \boldsymbol{W}\boldsymbol{W}^{\top}\odot\boldsymbol{M}^-)^{-1}(\boldsymbol{W}\boldsymbol{K}^{\top}\odot\boldsymbol{M}^-) \label{eq:path-attn}\end{equation}<br />
æœ‰æ²¡æœ‰è¢«éœ‡æƒŠåˆ°ï¼Ÿè¿™è¿˜æ²¡å®Œã€‚ç›´æ¥æ±‚é€†å¤æ‚åº¦æ˜¯$\mathcal{O}(n^3)$ï¼Œè¿™è‚¯å®šæ— æ³•æ¥å—ï¼Œè¿˜è¦æƒ³åŠæ³•åˆ©ç”¨$\boldsymbol{W}\boldsymbol{W}^{\top}$çš„ä½ç§©ç‰¹ç‚¹å°†å¤æ‚åº¦é™ä½åˆ°$\mathcal{O}(n^2)$ï¼Œç„¶åè¿˜è¦æ¨åå‘ä¼ æ’­ï¼Œæœ€åå†™æˆç±»ä¼¼Flash Attentionçš„é«˜æ•ˆå®ç°ï¼Œè¿™äº›ç»†èŠ‚å¤§å®¶åªèƒ½çœ‹åŸè®ºæ–‡æŒ–æ˜äº†ï¼Œæ€»ä¹‹å…¨ç¨‹éƒ½éå¸¸ç¡¬æ ¸ã€‚</p>
<p>ä»ä½ç½®ç¼–ç çš„è§’åº¦çœ‹ï¼ŒPaTHæ˜¯<a href="https://papers.cool/arxiv/2405.18719">CoPEï¼ˆContextual Position Encodingï¼‰</a>çš„ä¸€ç§ï¼Œå®ƒçš„ä½ç½®å¹¶ä¸æ˜¯ç¼–å·$1,2,3,\cdots$ï¼Œè€Œæ˜¯æ ¹æ®ä¸Šä¸‹æ–‡å†…å®¹è‡ªåŠ¨ç”Ÿæˆçš„ä½ç½®ä¿¡å·ã€‚ç±»ä¼¼åœ°ï¼ŒFoXä¹Ÿå¯ä»¥çœ‹æˆæ˜¯Contextualç‰ˆçš„ALIBIã€‚ä¸Šä¸‹æ–‡ç›¸å…³çš„ä½ç½®ä¿¡æ¯æ˜¯å½“å‰çº¿æ€§Attentionçš„ä¸»è¦ç‰¹å¾ï¼Œä¹Ÿå¯èƒ½æ˜¯åå“ºSoftmax Attentionçš„ä¸»è¦æ–¹å‘ã€‚</p>
<h2 id="_9">åŒ–ç®€ä¹æ— ç©·</h2>
<p>æˆ‘ä»¬ä¸å¦¨å†æ·±å…¥ç‚¹æ¢è®¨ä¸€ä¸‹PaTHï¼Œè¿™ä¸ä»…æœ‰åŠ©äºæˆ‘ä»¬äº†è§£PaTHï¼Œä¹Ÿèƒ½å¸®åŠ©æˆ‘ä»¬æ›´ç†Ÿæ‚‰DeltaNetï¼Œä¸¤è€…æœ¬èº«å°±æ˜¯é«˜åº¦ç›¸å…³çš„ã€‚è¿™ä¸€èŠ‚æˆ‘ä»¬ä»PaTHçš„ä¸¤ä¸ªç‰¹ä¾‹å…¥æ‰‹ï¼Œå®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£PaTHä¸DeltaNetçš„å…³è”ã€‚</p>
<p>ç¬¬ä¸€ä¸ªç‰¹ä¾‹æ˜¯$\boldsymbol{W}=\boldsymbol{K}$ï¼Œä»£å…¥åˆ°$\eqref{eq:path-attn}$å¾—åˆ°<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{A} =&amp;\, (\boldsymbol{Q}\boldsymbol{K}^{\top}\odot\boldsymbol{M})(\boldsymbol{I} - (\boldsymbol{I} + \boldsymbol{K}\boldsymbol{K}^{\top}\odot\boldsymbol{M}^-)^{-1}(\boldsymbol{K}\boldsymbol{K}^{\top}\odot\boldsymbol{M}^-)) \\\[6pt]<br />
=&amp;\, (\boldsymbol{Q}\boldsymbol{K}^{\top}\odot\boldsymbol{M})(\boldsymbol{I} + \boldsymbol{K}\boldsymbol{K}^{\top}\odot\boldsymbol{M}^-)^{-1} \qquad (\text{æ³¨}:\boldsymbol{I} - (\boldsymbol{I} + \boldsymbol{A})^{-1} \boldsymbol{A} = (\boldsymbol{I}+\boldsymbol{A})^{-1})<br />
\end{aligned}\end{equation}<br />
æœ‰æ²¡æœ‰è§‰å¾—æœ‰ç‚¹ç†Ÿæ‚‰ï¼Ÿè¿™åˆšå¥½å°±æ˜¯DeltaNetçš„AttentionçŸ©é˜µï¼ä»è¿™ä¸ªç‰¹ä¾‹çœ‹æ¥ï¼ŒPaTHå’ŒDeltaFormerçš„åŒºåˆ«å°±åœ¨äºï¼ŒDeltaFormeråŸºäºæ ¸æŠ€å·§ï¼Œç»™DeltaNetçš„$\boldsymbol{Q}\boldsymbol{K}^{\top}$å’Œ$\boldsymbol{K}\boldsymbol{K}^{\top}$åˆ†åˆ«åŠ ä¸Š$\exp$ï¼Œè€ŒPaTHç›´æ¥ç»™DeltaNetçš„AttentionçŸ©é˜µåŠ ä¸Š$\exp$ã€‚</p>
<p>ç¬¬äºŒä¸ªç‰¹ä¾‹æ˜¯é‡æ–°å¼•å…¥$\Vert\boldsymbol{w}\Vert=\sqrt{2}$è¿™ä¸ªçº¦æŸï¼Œæ­¤æ—¶$\boldsymbol{I}-\boldsymbol{w}\boldsymbol{w}^{\top}$æ˜¯æ­£äº¤çŸ©é˜µï¼Œæˆ‘ä»¬å¼•å…¥<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{R}<em i-1="i-1">i \triangleq&amp;\, (\boldsymbol{I}-\boldsymbol{w}_i\boldsymbol{w}_i^{\top})(\boldsymbol{I}-\boldsymbol{w}</em>}\boldsymbol{w<em _:i_="[:i]">{i-1}^{\top})\cdots(\boldsymbol{I}-\boldsymbol{w}_1\boldsymbol{w}_1^{\top}) \\\<br />
=&amp;\, \boldsymbol{I} - \boldsymbol{W}</em>}^{\top}(\boldsymbol{I} + \boldsymbol{W<em _:i_="[:i]">{[:i]}\boldsymbol{W}</em>}^{\top}\odot\boldsymbol{M}^-)^{-1}\boldsymbol{W<em i_0="i,0">{[:i]} \\\<br />
=&amp;\,\boldsymbol{R}</em><br />
\end{aligned}\end{equation}<br />
é‚£ä¹ˆ$\boldsymbol{R}<em i="i">{i,j} = \boldsymbol{R}_i \boldsymbol{R}_j^{\top}$ã€‚è¿™ä¸ªç­‰å¼æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åƒRoPEä¸€æ ·ï¼Œç”¨ç»å¯¹ä½ç½®çš„æ–¹å¼å®ç°ç›¸å¯¹ä½ç½®çš„PaTHï¼Œå³åªéœ€è¦ç»™æ¯ä¸ª$\boldsymbol{q}_i^{\top},\boldsymbol{k}_i^{\top}$éƒ½ä¹˜ä¸Š$\boldsymbol{R}_i$ï¼Œç„¶åå¥—ç”¨Softmax Attentionçš„å®ç°å°±è¡Œã€‚é‚£ä¹ˆä¹˜$\boldsymbol{R}_i$æ˜¯ä»€ä¹ˆè¿ç®—å‘¢ï¼Ÿé‡å¤ä¸Šä¸€èŠ‚çš„å±•å¼€è¿‡ç¨‹ï¼Œæˆ‘ä»¬æœ‰<br />
\begin{equation}\begin{aligned}<br />
(\boldsymbol{q}_i^{\top} \boldsymbol{R}</em>)<em _:i_="[:i]">s =&amp;\, (\boldsymbol{q}_i^{\top} - \boldsymbol{q}_i^{\top}\boldsymbol{W}</em>}^{\top}\boldsymbol{J<em _:i_="[:i]">{[:i,:i]}\boldsymbol{W}</em>)<em i_s="i,s">s \\\<br />
=&amp;\, Q</em> \\\} - \sum_{p=1}^d \sum_{l=1}^i \sum_{r=1}^i Q_{i,p} W_{l,p} J_{l,r} W_{r,s<br />
=&amp;\, Q_{i,s} - \sum_{p=1}^d \sum_{l=1}^i \sum_{r=1}^n Q_{i,p} W_{l,p} J_{l,r} W_{r,s} \\\<br />
=&amp;\, Q_{i,s} - \sum_{p=1}^d \sum_{l=1}^n \sum_{r=1}^n \chi_{l\leq i} Q_{i,p} W_{l,p} J_{l,r} W_{r,s} \\\<br />
=&amp;\, Q_{i,s} - \sum_{l=1}^n \underbrace{\chi_{l\leq i} \sum_{p=1}^d Q_{i,p} W_{l,p}}<em r="1">{\boldsymbol{Q}\boldsymbol{W}^{\top}\odot\boldsymbol{M}}\, \underbrace{\sum</em>}^n J_{l,r} W_{r,s}<em _tilde_boldsymbol_Q="\tilde{\boldsymbol{Q">{\boldsymbol{J}\boldsymbol{W}}<br />
\end{aligned}\end{equation}<br />
å†™æˆçŸ©é˜µå½¢å¼å°±æ˜¯<br />
\begin{equation}\boldsymbol{\boldsymbol{Q}} - (\boldsymbol{Q}\boldsymbol{W}^{\top}\odot\boldsymbol{M})(\boldsymbol{I} + \boldsymbol{W}\boldsymbol{W}^{\top}\odot\boldsymbol{M}^-)^{-1}\boldsymbol{W}\end{equation}<br />
æ˜¯ä¸æ˜¯åˆè§‰å¾—æœ‰ç‚¹ç†Ÿæ‚‰ï¼Ÿå…¶å®ç¬¬äºŒéƒ¨åˆ†å°±æ˜¯$\text{DeltaNet}(\boldsymbol{Q},\boldsymbol{W},\boldsymbol{W})$ï¼æ‰€ä»¥è¿™ç§æƒ…å†µä¸‹PaTHå®ç°çš„æ•ˆæœç­‰ä»·äºæ˜¯<br />
\begin{equation}\mathop{\text{SoftmaxAttention}}(\underbrace{\boldsymbol{Q}-\mathop{\text{DeltaNet}}(\boldsymbol{Q},\boldsymbol{W},\boldsymbol{W})}</em>}}},\underbrace{\boldsymbol{K}-\mathop{\text{DeltaNet}}(\boldsymbol{K},\boldsymbol{W},\boldsymbol{W})}_{\tilde{\boldsymbol{K}}},\boldsymbol{V})\end{equation<br />
ä¹Ÿå°±æ˜¯ç”¨DeltaNetç»™$\boldsymbol{Q},\boldsymbol{K}$åŠ ä½ç½®ç¼–ç ã€‚è¿™æ ·çœ‹PaTHï¼ˆåœ¨$\Vert\boldsymbol{w}\Vert=\sqrt{2}$è¿™ä¸ªçº¦æŸä¸‹ï¼‰å°±ç›¸å½“äºSoftmax Attentionä¸DeltaNetçš„æŸç§å±‚å†…æ··åˆã€‚å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥è€ƒè™‘æ”¾å¼ƒå‰é¢çš„æ¨å¯¼ï¼Œå³ä¾¿$\Vert\boldsymbol{w}\Vert\neq\sqrt{2}$æ—¶ä¹ŸæŒ‰ç…§ä¸Šå¼æ¥å®ç°ï¼Œè¿™å°±ç±»ä¼¼äºé€šè¿‡<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5240330">Canon Layers</a>çš„æ–¹æ¡ˆï¼Œç”¨å·ç§¯ç»™$\boldsymbol{Q},\boldsymbol{K}$åŠ ä½ç½®ä¿¡æ¯äº†ï¼Œåªä¸è¿‡è¿™é‡Œçš„å·ç§¯ä¸å†æ˜¯çŸ­å·ç§¯ï¼Œè€Œæ˜¯DeltaNetè¿™ç§é•¿å·ç§¯ã€‚</p>
<h2 id="_10">å‰‘èµ°åé”‹æ³•</h2>
<p>æœ€åï¼Œæˆ‘ä»¬å†çœ‹æœ€è¿‘çš„ä¸€ä¸ªåŒæ ·å€¼å¾—å…³æ³¨çš„çº¿æ€§Attentionæ¨¡å‹â€”â€”MesaNetï¼ˆè¿˜æœ‰ä¸€ä¸ªå¤§åŒå°å¼‚çš„åŒæœŸå·¥ä½œ<a href="https://papers.cool/arxiv/2505.23735">Atlas</a>ï¼‰ã€‚TTTçš„Online Learningè§†è§’å‘Šè¯‰æˆ‘ä»¬ï¼ŒDeltaNetå…¶å®å°±æ˜¯åœ¨ç”¨SGDä¼˜åŒ–ç›®æ ‡å‡½æ•°$\frac{1}{2}\Vert\boldsymbol{S}\boldsymbol{k} - \boldsymbol{v}\Vert^2$ï¼Œè€Œæˆ‘ä»¬ä»”ç»†è§‚å¯Ÿå°±ä¼šå‘ç°ï¼Œ$\boldsymbol{S}\boldsymbol{k}$åªæ˜¯$\boldsymbol{k}$çš„çº¿æ€§å‡½æ•°ï¼Œæ‰€ä»¥è¿™å®é™…ä¸Šåªæ˜¯ä¸€ä¸ªçº¿æ€§å›å½’é—®é¢˜ï¼Œçº¿æ€§å›å½’æ˜¯æœ‰è§£æè§£çš„ï¼<br />
\begin{equation}\boldsymbol{S}<em j="1">t = \boldsymbol{G}_t \boldsymbol{H}_t^{-1},\quad \boldsymbol{G}_t = \sum</em>}^t \boldsymbol{v<em j="1">j \boldsymbol{k}_j^{\top},\quad \boldsymbol{H}_t = \sum</em>}^t \boldsymbol{k<em t-1="t-1">j \boldsymbol{k}_j^{\top}\end{equation}<br />
MesaNetå°±æ˜¯åˆ©ç”¨è¿™ä¸ªè§£æè§£æ¥æ„å»ºåºåˆ—æ¨¡å‹çš„ï¼Œå…¶æƒ³æ³•èµ·æºäº<a href="https://papers.cool/arxiv/2309.05858">ã€ŠUncovering mesa-optimization algorithms in Transformersã€‹</a>ï¼Œé«˜æ•ˆè®­ç»ƒåˆ™æ˜¯ç”±<a href="https://papers.cool/arxiv/2506.05233">ã€ŠMesaNet: Sequence Modeling by Locally Optimal Test-Time Trainingã€‹</a>å®ç°ã€‚MesaNetåœ¨ä¸Šè¿°å…¬å¼åŸºç¡€ä¸Šç»™$\boldsymbol{G}_t,\boldsymbol{H}_t$åŠ å…¥é—å¿˜é—¨ï¼Œç„¶åæ±‚æ—¶åŠ ä¸Šå¯¹è§’é˜µ$\boldsymbol{\Lambda}_t$é¿å…ä¸å¯é€†ï¼Œæ€»çš„æ¨¡å‹æ˜¯<br />
\begin{equation}\boldsymbol{o}_t = \boldsymbol{G}_t (\boldsymbol{H}_t + \boldsymbol{\Lambda}_t)^{-1} \boldsymbol{q}_t,\quad \boldsymbol{G}_t = \gamma_t \boldsymbol{G}</em>} + \boldsymbol{v<em t-1="t-1">t \boldsymbol{k}_t^{\top},\quad\boldsymbol{H}_t = \gamma_t \boldsymbol{H}</em>} + \boldsymbol{k}_t \boldsymbol{k}_t^{\top}\end{equation<br />
å¾ˆæ˜æ˜¾ï¼Œ$\boldsymbol{G}_t,\boldsymbol{H}_t$å…³äºåºåˆ—é•¿åº¦çš„å¤æ‚åº¦æ˜¯çº¿æ€§çš„ï¼Œæ‰€ä»¥$\boldsymbol{o}_t$çš„è®¡ç®—å¤æ‚åº¦ä¹Ÿæ˜¯çº¿æ€§çš„ï¼Œå› æ­¤MesaNetä»ç„¶å±äºçº¿æ€§Attentionçš„èŒƒç•´ï¼Œå¹¶ä¸”ç”±äºè§£æè§£çš„ç¼˜æ•…ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥ä¿è¯å¤§å¤šæ•°æƒ…å†µä¸‹å®ƒä¼˜äºDeltaNetç”šè‡³Gated DeltaNetã€‚ä»ä¿¡å·å¤„ç†çš„è§’åº¦çœ‹ï¼ŒMesaNetä¸DeltaNetæ˜¯<a href="https://en.wikipedia.org/wiki/Recursive_least_squares_filter">Recursive Least Square</a>å’Œ<a href="https://en.wikipedia.org/wiki/Least_mean_squares_filter">Least Mean Square</a>çš„åŒºåˆ«ã€‚</p>
<p>çœ‹ä¸Šå»éƒ½æ˜¯ä¼˜ç‚¹ï¼Œä¸ºå•¥ç¬”è€…ä¼šå°†å®ƒå½’å…¥â€œå‰‘èµ°åé”‹â€å‘¢ï¼Ÿåœ¨ç¬”è€…çœ‹æ¥ï¼ŒMesaNetâ€œæˆä¹Ÿè§£æè§£ï¼Œè´¥ä¹Ÿè§£æè§£â€ï¼Œè§£æè§£ä½¿å¾—å®ƒé€šå¸¸ä¼˜äºDeltaNetï¼Œä½†ä¹Ÿç»™äººä¸€ç§â€œåˆ°æ­¤ä¸ºæ­¢â€çš„æ„Ÿè§‰ï¼Œå› ä¸ºåªè¦ç¨å˜ä¸€ä¸‹å°±å‡ ä¹æ²¡æœ‰æœºä¼šæ±‚å¾—è§£æè§£äº†ã€‚çºµè§‚æ•´ä¸ªæ•°å­¦å²ï¼Œæ‰€æœ‰ä¾èµ–äºè§£æè§£çš„åˆ†æ”¯åœ¨ä»Šå¤©å‡ ä¹å·²ç»éƒ½æ²¡è½äº†ï¼Œå› ä¸ºè§£æè§£å®åœ¨å¤ªç¨€ç½•ã€å¤ªæ²¡æœ‰ä»£è¡¨æ€§äº†ã€‚</p>
<p>ä»å®ç°ä¸Šæ¥çœ‹ï¼ŒMesaNetéœ€è¦æ±‚é€†çš„çŸ©é˜µ$\boldsymbol{H}_t + \boldsymbol{\Lambda}_t$å¹¶ä¸æ˜¯ä¸‰è§’é˜µï¼Œå°½ç®¡$(\boldsymbol{H}_t + \boldsymbol{\Lambda}_t)^{-1} \boldsymbol{q}_t$ä»ç„¶å¯ä»¥è½¬åŒ–ä¸ºè§£æ–¹ç¨‹è€Œä¸éœ€è¦æ˜¾å¼é€†ï¼Œä½†éä¸‰è§’é˜µä»ä½¿å¾—å®ƒæ±‚è§£å¤æ‚åº¦ä¼šå¢åŠ ä¸å°‘ã€‚å¦‚ä½•å°½å¯èƒ½ä½æˆæœ¬åœ°å¹¶è¡Œè®¡ç®—å…¨ä½“$(\boldsymbol{H}_t + \boldsymbol{\Lambda}_t)^{-1} \boldsymbol{q}_t$å°†ä¼šæ˜¯MesaNeté•¿æœŸçš„éš¾ç‚¹ï¼Œç›®å‰è®ºæ–‡ç”¨åˆ°çš„æ˜¯â€œ<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">å…±è½­æ¢¯åº¦æ³•</a>â€æ±‚è¿‘ä¼¼è§£ï¼Œèƒ½ç”¨ä½†å¹¶ä¸å®Œç¾ã€‚</p>
<p>å†å°±æ˜¯ä»ç†è®ºèƒ½åŠ›ä¸Šçœ‹ï¼ŒMesaNetä¹Ÿå¹¶éä¸¥æ ¼ä¼˜äºDeltaNetã€‚è¿™æ˜¯å› ä¸ºMesaNetçš„$\boldsymbol{G}_t,\boldsymbol{H}_t$æ›´æ–°è§„åˆ™è¿˜æ˜¯ç®€å•çš„æ»‘åŠ¨å¹³å‡ï¼Œå®ƒçš„æ±‚é€†ä¹Ÿä¸æ¶‰åŠåˆ°Tokenä¹‹é—´çš„äº¤äº’ï¼Œæ‰€ä»¥å®ƒçš„èƒ½åŠ›æé™å¤§æ¦‚ä¸å¦‚æ‹¥æœ‰Delta Ruleçš„DeltaNetã€‚ç›´è§‚ç†è§£å°±æ˜¯ï¼ŒMesaNetä¼šå°½åŠ›è®°ä½å…¨ä½“$\boldsymbol{k},\boldsymbol{v}$ï¼Œâ€œå…¨éƒ½è¦â€å¯èƒ½ä¼šå¯¼è‡´æ¯”è¾ƒæ¨¡ç³Šçš„è®°å¿†ï¼Œè€ŒDeltaNetçš„åŸåˆ™æ˜¯â€œé™¤æ—§è¿æ–°â€ï¼Œå› ä¸ºâ€œé™¤æ—§â€çš„ç¼˜æ•…ï¼Œå®ƒå¯ä»¥å®ç°é•¿æœŸã€ç²¾å‡†åœ°è®°å¿†æŸäº›å†…å®¹ã€‚</p>
<p>æˆ‘ä»¬è¿˜å¯ä»¥ä»ä¸€ä¸ªç‰¹æ®Šä¾‹å­æ¥ç†è§£è¿™ä¸ªéæœ€ä¼˜æ€§ï¼Œé‚£å°±æ˜¯ç›®å‰ä¸ºæ­¢é™¤MesaNetçš„æ‰€æœ‰Attentionï¼Œéƒ½å…è®¸Kã€Vå…±äº«çš„é€‰æ‹©ï¼Œâ€œå…è®¸â€çš„æ„æ€æ˜¯ä¸ä¸€å®šæœ€ä¼˜ï¼Œä½†è‡³å°‘èƒ½è®­å‡ºéå¹³å‡¡çš„ç»“æœï¼Œç„¶è€ŒMesaNetå¹¶ä¸è¡Œï¼Œå› ä¸ºKã€Vç›¸åŒçš„è¯ï¼ŒMesaNetçš„$\boldsymbol{S}_t$å°±æ’ä¸ºå•ä½é˜µäº†ã€‚</p>
<p>æ€»çš„æ¥è¯´ï¼ŒMesaNetæ˜¯ä¸€ä¸ªè®©äººèµå¿ƒæ‚¦ç›®çš„æ¨¡å‹ï¼Œä½†è§£æè§£ä¹Ÿå¢åŠ äº†å®ƒçš„å¤æ‚æ€§å’Œé™åˆ¶äº†å®ƒçš„çµæ´»æ€§ï¼Œç•™ä¸‹äº†ä¸å°‘äºŸå¾…æ¢ç´¢çš„ç©ºé—´ã€‚å¦‚æœè¯»è€…æƒ³è¦äº†è§£æ›´å¤šåŸºäºçº¿æ€§å›å½’æ¥æ„å»ºåºåˆ—æ¨¡å‹çš„å†…å®¹ï¼Œè¿˜å¯ä»¥é˜…è¯»<a href="https://papers.cool/arxiv/2501.12352">TTR</a>ï¼Œå®ƒå¯¹å„ç§çº¿æ€§å›å½’ç›®æ ‡ä¸‹çš„åºåˆ—æ¨¡å‹åšäº†è¯¦ç»†è®¨è®ºã€‚</p>
<h2 id="_11">æ–¹å…´æœªè‰¾è·¯</h2>
<p>æœ¬æ–‡ç®€è¦æ¢³ç†äº†çº¿æ€§Attentionçš„å‘å±•è„‰ç»œï¼Œå¹¶ä»‹ç»äº†éƒ¨åˆ†æ¨¡å‹çš„æ•°å­¦åŸç†ã€‚çº¿æ€§Attentionä»æ¨¡ä»¿Softmax Attentionèµ·æ­¥ï¼Œé€æ¸å‘å±•å‡ºè‡ªèº«ç‰¹è‰²ï¼Œå¦‚ä»Šå·²æˆä¸ºæå…·ç«äº‰åŠ›çš„åºåˆ—å»ºæ¨¡æ–¹æ¡ˆï¼Œç”šè‡³åè¿‡æ¥ä¸ºSoftmax Attentionçš„å‘å±•æä¾›äº†æ–°æ€è·¯ï¼Œè¿™ä¸€è¿‡ç¨‹æœ¬èº«å……æ»¡äº†è¶£å‘³æ€§å’Œå¯å‘æ€§ã€‚</p>
<p><em><strong>è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š</strong><a href="https://spaces.ac.cn/archives/11033">https://spaces.ac.cn/archives/11033</a></em></p>
<p><em><strong>æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="ã€Šç§‘å­¦ç©ºé—´FAQã€‹">ã€Šç§‘å­¦ç©ºé—´FAQã€‹</a></p>
<p><strong>å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚</strong></p>
<p><strong>å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼</strong></p>
<p>æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>å¾®ä¿¡æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>æ”¯ä»˜å®æ‰“èµ</p>
<p>å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>ç‚¹å‡»è¿™é‡Œ</strong></a>æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚</p>
<p><strong>å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š</strong></p>
<p>è‹å‰‘æ—. (Jun. 20, 2025). ã€Šçº¿æ€§æ³¨æ„åŠ›ç®€å²ï¼šä»æ¨¡ä»¿ã€åˆ›æ–°åˆ°åå“º ã€‹[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11033">https://spaces.ac.cn/archives/11033</a></p>
<p>@online{kexuefm-11033,<br />
title={çº¿æ€§æ³¨æ„åŠ›ç®€å²ï¼šä»æ¨¡ä»¿ã€åˆ›æ–°åˆ°åå“º},<br />
author={è‹å‰‘æ—},<br />
year={2025},<br />
month={Jun},<br />
url={\url{https://spaces.ac.cn/archives/11033}},<br />
} </p>
<hr />
<h2 id="_12">å…¬å¼æ¨å¯¼ä¸æ³¨é‡Š</h2>
<p>TODO: æ·»åŠ è¯¦ç»†çš„æ•°å­¦å…¬å¼æ¨å¯¼å’Œæ³¨é‡Š</p>
        </div>
    </div>
</body>
</html>