<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">← 返回首页</a>
        <header>
            <h1>当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？</h1>
            <div class="meta">📅 最后更新: 2025-12-31 | 📄 大小: 13.1 KB</div>
        </header>
        <div class="content">
            <p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/7877">https://spaces.ac.cn/archives/7877</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p><a href="/usr/uploads/2020/11/3741703949.png" title="点击查看原图"><img alt="中国象棋" src="/usr/uploads/2020/11/3741703949.png" /></a></p>
<p>中国象棋</p>
<p>不知道读者有没有看过量子位年初的文章<a href="https://mp.weixin.qq.com/s/PQysIztZpkDse4hMS0tw3Q">《最强写作AI竟然学会象棋和作曲，语言模型跨界操作引热议，在线求战》</a>，里边提到有网友用GPT2模型训练了一个下国际象棋的模型。笔者一直在想，这么有趣的事情怎么可以没有中文版呢？对于国际象棋来说，其中文版自然就是中国象棋了，于是我一直有想着把它的结果在中国象棋上面复现一下。拖了大半年，在最近几天终于把这个事情完成了，在此跟大家分享一下。</p>
<blockquote>
<p><strong>象棋谱式</strong><br />
 将军不离九宫内，士止相随不出官。<br />
象飞四方营四角，马行一步一尖冲。<br />
炮须隔子打一子，车行直路任西东。<br />
唯卒只能行一步，过河横进退无踪。</p>
</blockquote>
<h2 id="_1">背棋谱</h2>
<p>其实，简单看看量子位的文章，就能理解GPT2下象棋的原理了，无非就是“背棋谱”。简单来说，就是象棋的棋谱可以表示为一个连续的文本字符串，而GPT2正是擅长于背诵文本，因此可以用GPT2把人类的棋谱都背诵下来，而对于下棋来说，就可以看成是根据已经走的部分棋谱背诵下一步棋谱了，因此整个任务理论上确实是可以用GPT2完成。</p>
<p>为了完成这个任务，我们就需要了解计算机是如何记谱的。关于记谱的标准，比较通用的是ICCS记谱法和FEN局面表示法，其细节可以参考文章<a href="https://www.xqbase.com/protocol/cchess_move.htm">《中国象棋电脑应用规范(二)：着法表示》</a>和<a href="https://www.xqbase.com/protocol/cchess_fen.htm">《中国象棋电脑应用规范(二)：FEN文件格式》</a>。</p>
<h3 id="iccs">ICCS记谱</h3>
<p>简单来说，ICCS记谱就是将棋盘用如下图的横纵坐标表示，每一步走法只需要记录起始坐标，比如“h2e2”就是指将原来位于坐标(h, 2)的子移动到(e, 2)，如果是当前局面是新开局，那么这就对应着走法“炮二平五”。这样一来每一步就只需要4个字符来记录了，$n$步的棋谱就变成了$4n$长度的字符串了。当然，如果要输入到模型的话，不一定非得要按照这样的方式来，比如我也可以把“h2”只用一个id表示、“e2”用另一个id表示，也就是每个格点都用一个坐标而不是两个坐标来描述，这样每一步的只需要两个id来记录，以此来缩小棋谱的序列长度，这没有什么定法，有兴趣大家自己改进着完就好。</p>
<p><a href="/usr/uploads/2020/11/2832785545.png" title="点击查看原图"><img alt="中国象棋棋盘ICCS坐标示意图" src="/usr/uploads/2020/11/2832785545.png" /></a></p>
<p>中国象棋棋盘ICCS坐标示意图</p>
<h3 id="fen">FEN局面</h3>
<p>至于FEN局面表示法，则是用来表示当前局面有哪些子，轮到谁走。本文所建模的棋谱实际上都是全局棋谱，所以实际上本文的模型不需要用到它（局面都是默认的新开局面），不过为了方便有兴趣的读者做出改进，这里也简单介绍下。所谓FEN表示法，主要就是想办法表示出每一行有哪些子，比如开局表示为“rnbakabnr/9/1c5c1/p1p1p1p1p/9/9/P1P1P1P1P/1C5C1/9/RNBAKABNR w - - 0 1”。其中 <em>红色部分</em> 表示局面，小写表示黑方，大写表示红方，不同行之间用/隔开，字母含义如下表。这样，“rnbakabnr”就表示第一行为黑子的“車馬象士將士象馬車”，“9”表示第二行9个点都是空白的，“1c5c1”表示第三行是“1个空白+1个黑砲+5个空白+1个黑砲+1个空白”，等等； <em>绿色部分</em> 表示轮到哪一方走子，“w”表示红方，“b”表示黑方；剩下部分一般不大重要，有兴趣的读者自己去看链接就行。</p>
<p>\begin{array}{c}<br />
\text{中国象棋fen表示法含义表} \\\<br />
{\begin{array}{c|c|c|c|c|c|c}<br />
\hline<br />
\color{red}{R}/\color{black}{r} &amp; \color{red}{N}/\color{black}{n} &amp; \color{red}{B}/\color{black}{b} &amp; \color{red}{A}/\color{black}{a} &amp; \color{red}{K}/\color{black}{k} &amp; \color{red}{C}/\color{black}{c} &amp; \color{red}{P}/\color{black}{p} &amp; \text{阿拉伯数字}m\\\<br />
\hline<br />
\color{red}{\text{俥}}/\color{black}{\text{車}} &amp; \color{red}{\text{傌}}/\color{black}{\text{馬}} &amp; \color{red}{\text{相}}/\color{black}{\text{象}} &amp; \color{red}{\text{仕}}/\color{black}{\text{士}} &amp; \color{red}{\text{帥}}/\color{black}{\text{將}} &amp; \color{red}{\text{炮}}/\color{black}{\text{砲}} &amp; \color{red}{\text{兵}}/\color{black}{\text{卒}} &amp; \text{表示连续}m\text{个空位} \\\<br />
\hline<br />
\end{array}}<br />
\end{array}</p>
<h2 id="_2">建模型</h2>
<p>看了上述对记谱表示的介绍，我们知道，不管是每步的走法还是局面的表示，都被我们转化为了一串文本了，而对于象棋的推演都是以局面与走法作为输入输出的，所以理论上来说 <em>象棋的建模完全就是一个“文本处理”问题</em> ！这便是GPT2下象棋的理论依据了。本质上来讲，GPT也就是BERT加上语言模型的Attention Mask，所以这样的做法我们说是BERT下象棋或者GPT下象棋都行～</p>
<h3 id="_3">代码分享</h3>
<p>模型原理就没什么好写的了，之前就有文章<a href="/archives/6933">《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》</a>介绍，相关的例子有<a href="/archives/7124">《基于Conditional Layer Normalization的条件文本生成》</a>、<a href="/archives/7809">《BERT可以上几年级了？Seq2Seq“硬刚”小学数学应用题》</a>等，读者可以自行翻看。本文的处理其实很简单，就是只保留全局棋谱，将棋谱的ICCS记法当成一个长句子，然后训练一个语言模型。</p>
<p><strong>项目链接：<a href="https://github.com/bojone/gpt_cchess">https://github.com/bojone/gpt_cchess</a></strong></p>
<p>训练过程使用渐进式训练，即逐步增加序列长度，而不是一次性使用同一的长度，有些文章将这种做法称之为“课程学习（Curriculum Learning）”。这种做法能提高模型的训练速度和收敛速度（一开始序列更短，训练速度更快，也更容易收敛），直观来看就是让模型先学习“开局”，然后再学习“开局+中局”，最后学习“开局+中局+残局”，逐步增加难度。训练前加载了BERT的权重，可能读者会疑问BERT的权重还能跟棋谱有关系？其实没什么关系，但是不管怎么说，用BERT的权重比完全随机初始化的权重要好点，收敛会快一点点。</p>
<h3 id="_4">测试一下</h3>
<p>模型脚本还包含了一个可以跟模型交互式下棋的实现，读者可以自行体验一下，这个交互式下棋使用了python的<a href="https://github.com/walker8088/cchess">cchess模块</a>来辅助实现，在此表示感谢。GPT本身是一个生成模型，但是在决定下一步棋走什么的时候，笔者并不是用生成式方法（因为无约束生成有可能输出不可行的走法），而是用打分式的方法，即直接生成当前局面的所有可行走法，然后输入到模型打分，取分数最高的那个走法，这样就保证模型输出的每一步都是可行的，保证了可以跟AI一直对局下去，直到分出输赢。</p>
<p><a href="/usr/uploads/2020/11/99703461.png" title="点击查看原图"><img alt="交互式下棋效果" src="/usr/uploads/2020/11/99703461.png" /></a></p>
<p>交互式下棋效果</p>
<p>也许读者可能会有疑问，枚举所有可行走法计算量会不会很大？其实，对于每个局面来说，可行走法并不多，可以通过简单论证它不会超过111种（是不是有点意外？中国象棋每一步的候选走法不超过111种，而不是一个非常大的数字），所以这一步的batch_size不会超过111，因此是可以接受的。</p>
<blockquote>
<p>推导过程很简单：1个车或炮最多有17种走法，2车2炮最多有68种走法；兵如果都过河了，那么每个兵最多有3种走法，5个兵最多有15种走法；1个马最多有8种走法，2个马就是16种；2个相最多有6种走法（1个在中间，1个在边，4+2）；1个士在花心最多有4种走法（2个士反而相互堵塞）；最后的帅最多有2种走法。因此结果是68+15+16+6+4+2=111。具体局面设计可以参考数学研发论坛的<a href="https://bbs.emath.ac.cn/thread-17051-1-1.html">《一个中国象棋局面设计难题》</a>。</p>
</blockquote>
<p>大家最关心的可能就是这样弄出来的模型棋力究竟怎样？笔者简单跟它测了一下，大概的结论是：基本上可以开一个比较好的局，开局的时候具有不错的应变能力，不过一旦到了中局之后，应变能力会大大下降。对于吃子不是很敏感，也就是说当你乱吃它的子的时候，它可能不会应对。可以看出这些其实都是纯背棋谱的缺点。当然，前面说了每一步的输出都是可行，因此你可以跟它一直玩下去，直到把棋下完。</p>
<h2 id="_5">谈改进</h2>
<p>应该有读者会问能不能自己跟自己对弈来提高棋力？理论上当然是可以的，但很遗憾这里没有实现，一是没那个心思实现，二是没那个算力实现。此外，增加模型大小应该也能进一步提升棋力，要注意笔者上述结果只用了Base版本（1亿参数）的模型，本文开头提到的网友用GPT2下国际象棋可是用了15亿参数的GPT2，是我们的15倍。还有一个改进的地方，那就是上面的建模中我们直接学习了整个对局棋谱，按道理为了更好的棋力我们可以只学习赢家的走法，不能学习输家的走法。</p>
<p>当然，这些做法就算有提升，估计也是有限的，归根结底，这跟我们所理解的下棋原理不一样。我们下棋是根据局面形势往前推的，但上述的语言模型做法则没有局面这个概念，或者说它的局面需要用已经走的所有步骤来确定，这对于中后局来说历史步数太多，确实有点强模型所难了。改进方法其实也很简单，改为“以局面为输入、以走法为输出”就好了，前面我们说了，局面也可以用FEN表示法表示为一个文本，因此这也是只是个Seq2Seq任务而已。</p>
<p>除此之外，还有一些别的做法，比如我们可以把赢家的每一个局面都当作正样本，输家的每一个局面都当作负样本，那么就可以训练一个二分类模型，来判断局面优劣，有了这个判断函数，我们也可以直接枚举每一个可行走法，根据判断函数的结果来选择最优下法。而局面可以表示为文本，这就意味这我们将下棋变成了一个文本分类任务了。</p>
<p>总之，得益于Transformer模型对文本的强大的建模能力，这使得我们对下棋的建模思路也变得简单多样起来了～</p>
<h2 id="_6">总小结</h2>
<p>本文尝试了通过bert4keras用GPT来下中国象棋的做法，主要思路是通过“语言模型背棋谱”的方式来让模型具有预测下一步的能力，并谈及了一些改进思路。尽管本文的做法并非对下棋这个任务进行建模的标准做法，但通过这样的方式，能让我们进一步体会到语言模型的强大之处。</p>
<p>欢迎大家报告自己所训练的下棋模型的棋力智商，哈哈～</p>
<p><a href="/usr/uploads/2020/11/160577010.png" title="点击查看原图"><img alt="送你一对象" src="/usr/uploads/2020/11/160577010.png" /></a></p>
<p>送你一对象</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/7877">https://spaces.ac.cn/archives/7877</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Nov. 11, 2020). 《当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/7877">https://spaces.ac.cn/archives/7877</a></p>
<p>@online{kexuefm-7877,<br />
title={当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？},<br />
author={苏剑林},<br />
year={2020},<br />
month={Nov},<br />
url={\url{https://spaces.ac.cn/archives/7877}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>
    </div>
</body>
</html>