<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>如何应对Seq2Seq中的“根本停不下来”问题？</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">← 返回首页</a>
        <header>
            <h1>如何应对Seq2Seq中的“根本停不下来”问题？</h1>
            <div class="meta">📅 最后更新: 2025-12-31 | 📄 大小: 12.6 KB</div>
        </header>
        <div class="content">
            <p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/7500">https://spaces.ac.cn/archives/7500</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在Seq2Seq的解码过程中，我们是逐个token地递归生成的，直到出现<eos>标记为止，这就是所谓的“自回归”生成模型。然而，研究过Seq2Seq的读者应该都能发现，这种自回归的解码偶尔会出现“根本停不下来”的现象，主要是某个片段反复出现，比如“今天天气不错不错不错不错不错...”、“你觉得我说得对不对不对不对不对不对...”等等，但就是死活不出现<eos>标记。ICML 2020的文章<a href="https://papers.cool/arxiv/2002.02492">《Consistency of a Recurrent Language Model With Respect to Incomplete Decoding》</a>比较系统地讨论了这个现象，并提出了一些对策，本文来简单介绍一下论文的主要内容。</p>
<h2 id="_1">解码算法</h2>
<p>对于自回归模型来说，我们建立的是如下的条件语言模型<br />
\begin{equation}p(y_t|y_{\lt t}, x)\label{eq:p}\end{equation}<br />
那么解码算法就是在已知上述模型时，给定$x$来输出对应的$y=(y_1,y_2,\dots,y_T)$来。解码算法大致可以分为两类：确定性解码算法和随机性解码算法，原论文分别针对这两类解码讨论来讨论了“根本停不下来”问题，所以我们需要来了解一下这两类解码算法。</p>
<h3 id="_2">确定解码</h3>
<p>确定性解码算法就是当输入文本固定之后，解码出来的输出文本也是固定的，这类算法包含<strong>贪心搜索</strong> （Greedy Search）和<strong>束搜索</strong> （Beam Search），事实上贪心搜索是束搜索的一个特例，所以只需要讨论束搜索。</p>
<p>束搜索我们需要固定一个“束”的大小（Beam Size）$k$，然后从左往右逐个token地解码，每步只保留总得分最高的$k$个序列。比如$k=2$，token空间为$V=\{a,b,c,d\}$，那么解码流程示例如下：</p>
<blockquote>
<p>第一步，算$p(y_1|y_0,x)$（$y_0$是固定的起始标记<bos>），然后保留最大的两个，比如$\{a,b\}$，并记录它们的得分（概率对数）；</p>
<p>第二步，算$p(y_2|y_0,a,x)$和$p(y_2|y_0,b,x)$，这时候候选序列一共有$k\times |V|=8$个，保留总得分（也就是当前token分数加上$a$,$b$本身的分数）最大的两个，比如$\{(a,c),(b,d)\}$，并记录各自的总得分；</p>
<p>第三步，算$p(y_3|y_0,a,c,x)$和$p(y_3|y_0,b,d,x)$，这时候候选序列一共有$k\times |V|=8$个，保留总得分（也就是当前token分数加上$(a,c)$,$(b,d)$本身的分数）最大的两个，比如$\{(a,c,d),(a,c,c)\}$，并记录各自的总得分；</p>
<p>...</p>
<p>依此类推，每个序列直到出现<eos>就停止，最后从这$k$个已经完成终止的序列中选最优的那个输出。一般有两种选择，一是输出总得分最大的，二是输出平均得分最大的（处以各自token数），有时候还会根据实际需求加入长度惩罚等。</p>
</blockquote>
<h3 id="_3">随机解码</h3>
<p>随机性解码算法，顾名思义，就是哪怕输入文本固定了，解码出来的输出文本也不是固定的，比如从训练语言模型进行随机采样就是这这种算法（参考<a href="/archives/7292">《现在可以用Keras玩中文GPT2了》</a>）。对于Seq2Seq来说，我们很多时候希望得到确定性的结果，所以多数场景下我们都是用Beam Search。但是Beam Searc的生成结果可能会出现过于单一的现象（即类似“好的”、“不知道”、“谢谢”这类比较“安全”的回复），或者有时候我们希望增加输出的多样性（比如我们之前开源的做相似句生成的<a href="/archives/7427">SimBERT</a>模型），这时候就需要随机性解码算法，它包含三种情况：原生随机解码、top-k随机解码、Nucleus随机解码。</p>
<p><strong>原生随机解码</strong> 很简单，就是每步按概率随机采样一个token，比如第一步算$p(y_1|y_0,x)$，然后按概率随机采样一个token，比如$c$；然后第二步算$p(y_2|y_0,c,x)$，接着按概率随机采样一个token，比如$a$；那么第三步就算$p(y_3|y_0,c,a,x)$，再按概率随机采样；...；依此类推，直到采样到<eos>停止。</p>
<p><strong>top-k随机解码</strong> 出自文章<a href="https://papers.cool/arxiv/1805.04833">《Hierarchical Neural Story Generation》</a>，其实就是在原生随机解码基础上加了个截断：每一步只保留概率最高的$k$个token，然后重新归一化后再采样，这样做是希望在“得分高”和“多样性”方面做一个折中。显然，当$k=1$时，其实就等价于贪心搜索。</p>
<p><strong>Nucleus随机解码</strong> 则来自文章<a href="https://papers.cool/arxiv/1904.09751">《The Curious Case of Neural Text Degeneration》</a>，跟top-k随机解码类似，也是对采样空间做了个截断，截断方式是：固定$p\in(0, 1)$，然后只保留概率最高的、概率和刚好超过$p$的若干个token，所以它也叫top-p采样。</p>
<p>除了top-k和top-p这两种截断方式外，还有一些自适应的截断方式，比如论文<a href="https://papers.cool/arxiv/1905.05702">《Sparse Sequence-to-Sequence Models》</a>将最后预测分布的softmax函数换成了稀疏版本的softmax，它能自动让大部分不可能的token概率变为0，而不需要认为地选择$k$或$p$。</p>
<h2 id="_4">适可而止</h2>
<p>从Seq2Seq的模型设计和上面介绍的解码算法来看，并没有任何的理论保证解码过程一定能停下来，也就是说并没有东西保证一定会出现<eos>标记，这只能靠模型自己学出来，而当模型学得不够好时，就会出现“根本停不下来”的现象了。而原论文则针对不同的解码算法做了相应的分析，提出了对应的策略，让解码过程能够“适可而止”。</p>
<h3 id="_5">有界的隐向量</h3>
<p>建模概率$\eqref{eq:p}$的经典方式就是<br />
\begin{equation}p(y_t|y_{\lt t}, x)=softmax(Wh_t+b),\quad h_t=f(y_{\lt t}, x)\end{equation}<br />
也就是说，先算一个隐向量$h_t$，然后接一个全连接，然后softmax激活。在这种形式下，原论文说</p>
<blockquote>
<p>如果对于任意的$t$，$\Vert h_t\Vert$是有上界的，那么原生随机解码就能够“适可而止”。</p>
</blockquote>
<p>看上去很强很实用的一个结论是不是？让$\Vert h_t\Vert$是有上界是一个很简单的事情，比如加个Layer Norm就可以了，那是不是说加个Layer Norm就可以解决所有的问题呢？并不是。上述结论理论上是对的，推理过程是：因为$\Vert h_t\Vert$是有上界的，所以对于任意的$t$、任意的token，$p(y_t|y_{\lt t}, x)$是有正的下界的（因为$Wh_t$不会无穷大，所以$e^{Wh_t}$也不会无穷大，归一化后也不会无限接近于0），那也就意味着存在一个正数$\epsilon &gt; 0$，总有$p(\text{<eos>}|y_{\lt t}, x)\geq \epsilon$，因为概率是一个正数，因此只要你采样足够多步，总有机会采样到<eos>的，所以不会永远停不下来。</p>
<p>这推理过程是不是有点让人啼笑皆非？没错，是能停，但是要采样足够多步，感觉就像是“只要你买足够多张彩票就一定能中头奖”一样，并没什么确切的实际价值。采样足够多步之后，该循环的、该重复的token可能都已经重复多次了，就算能停下来，得到的输出可能也没有意义了，或许还不如直接按长度截断。</p>
<h3 id="_6">主动添加<eos></h3>
<p>注意上述结论还只是对原生随机解码成立，对于top-k随机解码和Nucleus随机解码不一定成立，因为经过截断后<eos>就不一定出现在采样空间中了，当然，我们可以手动把<eos>添加进采样空间，所以就有了如下的结论：</p>
<blockquote>
<p>如果对于任意的$t$，$\Vert h_t\Vert$是有上界的，并且我们把<eos>也加入到采样空间中，那么top-k随机解码和Nucleus随机解码就能够“适可而止”。</p>
</blockquote>
<p>只不过，这有点像是废话...</p>
<h3 id="_7">自截断设计</h3>
<p>注意，上面的两个结论都只能用于随机解码，对于确定性解码来说，因为没有了随机性，所以我们没法保证一定能碰到<eos>。为此，原论文提出了一个自截断的设计：想办法让$p(\text{<eos>}|y_{\lt t}, x)$有正的下界，而且这个下界随着$t$的增大而增大，最终逐渐趋于1。</p>
<p>这种自截断的设计也不复杂，就是定义$p(\text{<eos>}|y_{\lt t}, x) = 1 - \alpha(h_t)$，其中<br />
\begin{equation}\begin{aligned}<br />
\alpha(h_0)=&amp;\,\sigma\left(w_{\text{<eos>}}^{\top} h_0 + b_{\text{<eos>}}\right)<br />
\\\<br />
\alpha(h_t)=&amp;\,\sigma\left(w_{\text{<eos>}}^{\top} h_t + b_{\text{<eos>}}\right)\left[1 - p(\text{<eos>}|y_{\lt {t-1}}, x)\right]<br />
\end{aligned}\end{equation}<br />
这里的$\sigma(\cdot)$负责将$\mathbb{R}$映射到$[0, 1-\epsilon]$，比如可以用$\sigma(\cdot)=(1-\epsilon)\text{sigmoid}(\cdot)$。设计好$p(\text{<eos>}|y_{\lt t}, x)$后，剩下的token概率还是按照原来的softmax方式计算，然后乘以$\alpha(h_t)$即可。</p>
<p>现在我们有<br />
\begin{equation}\begin{aligned}<br />
p(\text{<eos>}|y_{\lt t}, x)=&amp;\,1 - \sigma\left(w_{\text{<eos>}}^{\top} h_t + b_{\text{<eos>}}\right)\left[1 - p(\text{<eos>}|y_{\lt {t-1}}, x)\right]\\\<br />
=&amp;\,1 - \prod_{i=0}^t\sigma\left(w_{\text{<eos>}}^{\top} h_i + b_{\text{<eos>}}\right)\\\<br />
\geq&amp;\, 1 - (1 - \epsilon)^{t+1}<br />
\end{aligned}\end{equation}<br />
显然只要$t &gt; -\ln 2/\ln (1-\epsilon)$，$p(\text{<eos>}|y_{\lt t}, x) &gt; 0.5$，也就是说，对于贪心搜索来说必然在$-\ln 2/\ln (1-\epsilon)$步内停止，而对随着$p(\text{<eos>}|y_{\lt t}, x)$越来越接近1，显然Beam Search也能在有限步停止。</p>
<h2 id="_8">个人评价</h2>
<p>原论文的主要内容大体上就是这样了，总的来说，它确实给我们提供了对解码算法的一些新认识，以及提供了一些缓解“根本停不下来”问题的有效策略。但是，作为一篇ICML论文来说，我觉得原论文的视角并不高，总体显得有些显浅。</p>
<p>原论文的大部分篇幅，是在用数学化的语言来重新表述已有的内容，比如什么是解码算法、什么是top-k随机解码、什么是Beam Search、什么是“根本停不下来”等等，原论文都给下了个数学定义，这不能说没有意义，但对论文本身要探讨的问题并没有什么价值，而除去这部分东西，原论文就没多少内容了。其次，原论文的结论太弱，关于随机解码的应对策略前面已经点评过了，结论是对的，但基本没实用价值；而对于确定性解码的自截断设计，其实很生硬，有种粗暴截断的感觉，完全没有优雅感。</p>
<p>最关键的问题是，对于“根本停不下来”这个问题，论文通篇都是在回答“是什么”、“怎么办”这两个问题，没有探讨“为什么”，没有提供任何关于理解“根本停不下来”本质的有用信息，从而并没有得到更贴近本质的应对策略，这是笔者觉得相当难以接受的。</p>
<h2 id="_9">文章小结</h2>
<p>本文介绍了Seq2Seq的解码算法，讨论了解码过程中可能出现的“根本停不下来”的现象，并介绍了ICML 2020的一篇论文中提供的应对策略。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/7500">https://spaces.ac.cn/archives/7500</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jun. 16, 2020). 《如何应对Seq2Seq中的“根本停不下来”问题？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/7500">https://spaces.ac.cn/archives/7500</a></p>
<p>@online{kexuefm-7500,<br />
title={如何应对Seq2Seq中的“根本停不下来”问题？},<br />
author={苏剑林},<br />
year={2020},<br />
month={Jun},<br />
url={\url{https://spaces.ac.cn/archives/7500}},<br />
} </p>
<hr />
<h2 id="_10">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>
    </div>
</body>
</html>