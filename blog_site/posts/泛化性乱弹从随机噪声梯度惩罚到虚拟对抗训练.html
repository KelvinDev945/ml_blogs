<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ³›åŒ–æ€§ä¹±å¼¹ï¼šä»éšæœºå™ªå£°ã€æ¢¯åº¦æƒ©ç½šåˆ°è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒ</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>æ³›åŒ–æ€§ä¹±å¼¹ï¼šä»éšæœºå™ªå£°ã€æ¢¯åº¦æƒ©ç½šåˆ°è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒ</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2026-01-08 | ğŸ“„ å¤§å°: 59.7 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/7466">https://spaces.ac.cn/archives/7466</a></p>
<p><strong>å‘å¸ƒæ—¥æœŸ</strong>: </p>
<hr />
<p>æé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½æ˜¯æœºå™¨å­¦ä¹ è‡´åŠ›è¿½æ±‚çš„ç›®æ ‡ä¹‹ä¸€ã€‚å¸¸è§çš„æé«˜æ³›åŒ–æ€§çš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šç¬¬ä¸€ç§æ˜¯æ·»åŠ å™ªå£°ï¼Œæ¯”å¦‚å¾€è¾“å…¥æ·»åŠ é«˜æ–¯å™ªå£°ã€ä¸­é—´å±‚å¢åŠ Dropoutä»¥åŠè¿›æ¥æ¯”è¾ƒçƒ­é—¨çš„å¯¹æŠ—è®­ç»ƒç­‰ï¼Œå¯¹å›¾åƒè¿›è¡Œéšæœºå¹³ç§»ç¼©æ”¾ç­‰æ•°æ®æ‰©å¢æ‰‹æ®µæŸç§æ„ä¹‰ä¸Šä¹Ÿå±äºæ­¤åˆ—ï¼›ç¬¬äºŒç§æ˜¯å¾€lossé‡Œè¾¹æ·»åŠ æ­£åˆ™é¡¹ï¼Œæ¯”å¦‚$L_1, L_2$æƒ©ç½šã€æ¢¯åº¦æƒ©ç½šç­‰ã€‚æœ¬æ–‡è¯•å›¾æ¢ç´¢å‡ ç§å¸¸è§çš„æé«˜æ³›åŒ–æ€§èƒ½çš„æ‰‹æ®µçš„å…³è”ã€‚</p>
<h2 id="_1">éšæœºå™ªå£°</h2>
<p>æˆ‘ä»¬è®°æ¨¡å‹ä¸º$f(x)$ï¼Œ$\mathcal{D}$ä¸ºè®­ç»ƒæ•°æ®é›†åˆï¼Œ$l(f(x), y)$ä¸ºå•ä¸ªæ ·æœ¬çš„lossï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡æ˜¯<br />
\begin{equation}\mathop{\text{argmin}}<em _mathcal_D="\mathcal{D" _x_y_sim="(x,y)\sim">{\theta} L(\theta)=\mathbb{E}</em>}}[l(f(x), y)]\end{equation<br />
$\theta$æ˜¯$f(x)$é‡Œè¾¹çš„å¯è®­ç»ƒå‚æ•°ã€‚å‡å¦‚å¾€æ¨¡å‹è¾“å…¥æ·»åŠ å™ªå£°$\varepsilon$ï¼Œå…¶åˆ†å¸ƒä¸º$q(\varepsilon)$ï¼Œé‚£ä¹ˆä¼˜åŒ–ç›®æ ‡å°±å˜ä¸º<br />
\begin{equation}\mathop{\text{argmin}}<em _varepsilon="\varepsilon">{\theta} L</em>}(\theta)=\mathbb{E}_{(x,y)\sim \mathcal{D}, \varepsilon\sim q(\varepsilon)}[l(f(x + \varepsilon), y)]\end{equation<br />
å½“ç„¶ï¼Œå¯ä»¥æ·»åŠ å™ªå£°çš„åœ°æ–¹ä¸ä»…ä»…æ˜¯è¾“å…¥ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸­é—´å±‚ï¼Œä¹Ÿå¯ä»¥æ˜¯æƒé‡$\theta$ï¼Œç”šè‡³å¯ä»¥æ˜¯è¾“å‡º$y$ï¼ˆç­‰ä»·äºæ ‡ç­¾å¹³æ»‘ï¼‰ï¼Œå™ªå£°ä¹Ÿä¸ä¸€å®šæ˜¯åŠ ä¸Šå»çš„ï¼Œæ¯”å¦‚Dropoutæ˜¯ä¹˜ä¸Šå»çš„ã€‚å¯¹äºåŠ æ€§å™ªå£°æ¥è¯´ï¼Œ$q(\varepsilon)$çš„å¸¸è§é€‰æ‹©æ˜¯å‡å€¼ä¸º0ã€æ–¹å·®å›ºå®šçš„é«˜æ–¯åˆ†å¸ƒï¼›è€Œå¯¹äºä¹˜æ€§å™ªå£°æ¥è¯´ï¼Œå¸¸è§é€‰æ‹©æ˜¯å‡åŒ€åˆ†å¸ƒ$U([0,1])$æˆ–è€…æ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒã€‚</p>
<p>æ·»åŠ éšæœºå™ªå£°çš„ç›®çš„å¾ˆç›´è§‚ï¼Œå°±æ˜¯å¸Œæœ›æ¨¡å‹èƒ½å­¦ä¼šæŠµå¾¡ä¸€äº›éšæœºæ‰°åŠ¨ï¼Œä»è€Œé™ä½å¯¹è¾“å…¥æˆ–è€…å‚æ•°çš„æ•æ„Ÿæ€§ï¼Œè€Œé™ä½äº†è¿™ç§æ•æ„Ÿæ€§ï¼Œé€šå¸¸æ„å‘³ç€æ‰€å¾—åˆ°çš„æ¨¡å‹ä¸å†é‚£ä¹ˆä¾èµ–è®­ç»ƒé›†ï¼Œæ‰€ä»¥æœ‰åŠ©äºæé«˜æ¨¡å‹æ³›åŒ–æ€§èƒ½ã€‚</p>
<h2 id="_2">æé«˜æ•ˆç‡</h2>
<p>æ·»åŠ éšæœºå™ªå£°çš„æ–¹å¼å®¹æ˜“å®ç°ï¼Œè€Œä¸”åœ¨ä¸å°‘æƒ…å†µä¸‹ç¡®å®ä¹Ÿå¾ˆæœ‰æ•ˆï¼Œä½†å®ƒæœ‰ä¸€ä¸ªæ˜æ˜¾çš„ç¼ºç‚¹ï¼šä¸å¤Ÿâ€œç‰¹å¼‚æ€§â€ã€‚å™ªå£°$\varepsilon$æ˜¯éšæœºçš„ï¼Œè€Œä¸æ˜¯é’ˆå¯¹$x$æ„å»ºçš„ï¼Œè¿™æ„å‘³ç€å¤šæ•°æƒ…å†µä¸‹$x + \varepsilon$å¯èƒ½åªæ˜¯ä¸€ä¸ªå¹³å‡¡æ ·æœ¬ï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰å¯¹åŸæ¨¡å‹é€ æˆæ¯”è¾ƒæ˜æ˜¾çš„æ‰°åŠ¨ï¼Œæ‰€ä»¥å¯¹æ³›åŒ–æ€§èƒ½çš„æé«˜å¸®åŠ©æœ‰é™ã€‚</p>
<h3 id="_3">å¢åŠ é‡‡æ ·</h3>
<p>ä»ç†è®ºä¸Šæ¥çœ‹ï¼ŒåŠ å…¥éšæœºå™ªå£°åï¼Œå•ä¸ªæ ·æœ¬çš„losså˜ä¸º<br />
\begin{equation}\tilde{l}(x,y)=\mathbb{E}<em i="1">{\varepsilon\sim q(\varepsilon)}[l(f(x+\varepsilon),y)]=\int q(\varepsilon) l(f(x+\varepsilon),y) d\varepsilon\label{eq:noisy-loss}\end{equation}<br />
ä½†å®è·µä¸Šï¼Œå¯¹äºæ¯ä¸ªç‰¹å®šçš„æ ·æœ¬$(x,y)$ï¼Œæˆ‘ä»¬ä¸€èˆ¬åªé‡‡æ ·ä¸€ä¸ªå™ªå£°ï¼Œæ‰€ä»¥å¹¶æ²¡æœ‰å¾ˆå¥½åœ°è¿‘ä¼¼ä¸Šå¼ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡æ ·å¤šä¸ªå™ªå£°$\varepsilon_1,\varepsilon_2,\cdots,\varepsilon_k\sim q(\varepsilon)$ï¼Œç„¶åæ›´å¥½åœ°è¿‘ä¼¼<br />
\begin{equation}\tilde{l}(x,y)\approx \frac{1}{k}\sum</em>}^k l(f(x+\varepsilon_i),y)\end{equation<br />
ä½†è¿™æ ·ç›¸å½“äºbatch_sizeæ‰©å¤§ä¸ºåŸæ¥çš„$k$å€ï¼Œå¢å¤§äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸æ˜¯é‚£ä¹ˆå‹å¥½ã€‚</p>
<h3 id="_4">è¿‘ä¼¼å±•å¼€</h3>
<p>ä¸€ä¸ªç›´æ¥çš„æƒ³æ³•æ˜¯ï¼Œå¦‚æœèƒ½äº‹å…ˆæŠŠå¼$\eqref{eq:noisy-loss}$ä¸­çš„ç§¯åˆ†ç®—å‡ºæ¥ï¼Œé‚£å°±ç”¨ä¸ç€ä½æ•ˆç‡åœ°é‡‡æ ·äº†ï¼ˆæˆ–è€…ç›¸å½“äºä¸€æ¬¡æ€§é‡‡æ ·æ— é™å¤šçš„å™ªå£°ï¼‰ã€‚æˆ‘ä»¬å°±å¾€è¿™ä¸ªæ–¹å‘èµ°ä¸€ä¸‹è¯•è¯•ã€‚å½“ç„¶ï¼Œç²¾ç¡®çš„æ˜¾å¼ç§¯åˆ†åŸºæœ¬ä¸Šæ˜¯åšä¸åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€ä¸‹è¿‘ä¼¼å±•å¼€ï¼š<br />
\begin{equation}l(f(x+\varepsilon),y)\approx l(f(x),y)+(\varepsilon \cdot \nabla_x) l(f(x),y)+\frac{1}{2}(\varepsilon \cdot \nabla_x)^2 l(f(x),y)\end{equation}<br />
ç„¶åä¸¤ç«¯ä¹˜ä»¥$q(\varepsilon)$ç§¯åˆ†ï¼Œè¿™é‡Œå‡è®¾$\varepsilon$çš„å„ä¸ªåˆ†é‡æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œå¹¶ä¸”å‡å€¼ä¸º0ã€æ–¹å·®ä¸º$\sigma^2$ï¼Œé‚£ä¹ˆç§¯åˆ†ç»“æœå°±æ˜¯<br />
\begin{equation}\int q(\varepsilon)l(f(x+\varepsilon),y)d\varepsilon \approx l(f(x),y)+\frac{1}{2}\sigma^2 \Delta l(f(x),y)\end{equation}<br />
è¿™é‡Œçš„$\Delta$æ˜¯æ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼Œå³$\Delta f = \sum\limits_i \frac{\partial^2}{\partial x_i^2} f$ã€‚è¿™ä¸ªç»“æœåœ¨å½¢å¼ä¸Šå¾ˆç®€å•ï¼Œå°±æ˜¯ç›¸å½“äºå¾€lossé‡Œè¾¹åŠ å…¥æ­£åˆ™é¡¹$\frac{1}{2}\sigma^2 \Delta l(f(x),y)$ï¼Œç„¶è€Œå®è·µä¸Šå´ç›¸å½“å›°éš¾ï¼Œå› ä¸ºè¿™æ„å‘³ç€è¦ç®—$l$çš„äºŒé˜¶å¯¼æ•°ï¼Œå†åŠ ä¸Šæ¢¯åº¦ä¸‹é™ï¼Œé‚£ä¹ˆå°±ä¸€å…±è¦ç®—ä¸‰é˜¶å¯¼æ•°ï¼Œè¿™æ˜¯ç°æœ‰æ·±åº¦å­¦ä¹ æ¡†æ¶éš¾ä»¥é«˜æ•ˆå®ç°çš„ã€‚</p>
<h2 id="_5">è½¬ç§»ç›®æ ‡</h2>
<p>ç›´æ¥åŒ–ç®€$l(f(x+\varepsilon),y)$çš„ç§¯åˆ†æ˜¯è¡Œä¸é€šäº†ï¼Œä½†æˆ‘ä»¬è¿˜å¯ä»¥è¯•è¯•å°†ä¼˜åŒ–ç›®æ ‡æ¢æˆ<br />
\begin{equation}l(f(x+\varepsilon),f(x)) + l(f(x),y)\label{eq:loss-2}\end{equation}<br />
ä¹Ÿå°±æ˜¯å˜æˆåŒæ—¶ç¼©å°$f(x),y$ã€$f(x+\varepsilon),f(x)$çš„å·®è·ï¼Œä¸¤è€…åŒç®¡é½ä¸‹ï¼Œä¸€å®šç¨‹åº¦ä¸Šä¹Ÿèƒ½è¾¾åˆ°ç¼©å°$f(x+\varepsilon),y$å·®è·çš„ç›®æ ‡ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ä¸ªç›®æ ‡èƒ½å¾—åˆ°æ›´æœ‰æ„æ€çš„ç»“æœã€‚</p>
<h3 id="_6">æ€è·¯è§£æ</h3>
<p>ç”¨æ•°å­¦çš„è¯æ¥è®²ï¼Œå¦‚æœ$l$æ˜¯æŸç§å½¢å¼çš„è·ç¦»åº¦é‡ï¼Œé‚£ä¹ˆæ ¹æ®ä¸‰è§’ä¸ç­‰å¼å°±æœ‰<br />
\begin{equation}l(f(x+\varepsilon),y) \leq l(f(x+\varepsilon),f(x)) + l(f(x),y)\end{equation}<br />
å¦‚æœ$l$ä¸æ˜¯åº¦é‡ï¼Œé‚£ä¹ˆé€šå¸¸æ ¹æ®è©¹æ£®ä¸ç­‰å¼ä¹Ÿèƒ½å¾—åˆ°ä¸€ä¸ªç±»ä¼¼çš„ç»“æœï¼Œæ¯”å¦‚$l(f(x+\varepsilon),y)=\Vert f(x+\varepsilon) - y\Vert^2$ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰<br />
\begin{equation}\begin{aligned}<br />
\Vert f(x+\varepsilon) - f(x) + f(x) - y\Vert^2 =&amp; \left\Vert \frac{1}{2}\times 2[f(x+\varepsilon) - f(x)] + \frac{1}{2}\times 2[f(x) - y]\right\Vert^2\\\<br />
\leq&amp; \frac{1}{2} \Vert 2[f(x+\varepsilon) - f(x)]\Vert^2 + \frac{1}{2} \Vert 2[f(x) - y]\Vert^2\\\<br />
=&amp; 2\big(\Vert f(x+\varepsilon) - f(x)\Vert^2 + \Vert f(x) - y\Vert^2\big)<br />
\end{aligned}\end{equation}<br />
è¿™ä¹Ÿå°±æ˜¯è¯´ï¼Œç›®æ ‡$\eqref{eq:loss-2}$ï¼ˆçš„è‹¥å¹²å€ï¼‰å¯ä»¥è®¤ä¸ºæ˜¯$l(f(x+\varepsilon),y)$çš„ä¸Šç•Œï¼ŒåŸå§‹ç›®æ ‡ä¸å¤§å¥½ä¼˜åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬æ”¹ä¸ºä¼˜åŒ–å®ƒçš„ä¸Šç•Œã€‚</p>
<p>æ³¨æ„åˆ°ï¼Œç›®æ ‡$\eqref{eq:loss-2}$çš„ä¸¤é¡¹ä¹‹ä¸­ï¼Œ$l(f(x+\varepsilon),f(x))$è¡¡é‡äº†æ¨¡å‹æœ¬èº«çš„å¹³æ»‘ç¨‹åº¦ï¼Œè·Ÿæ ‡ç­¾æ²¡å…³ç³»ï¼Œç”¨æ— æ ‡ç­¾æ•°æ®ä¹Ÿå¯ä»¥å¯¹å®ƒè¿›è¡Œä¼˜åŒ–ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥è·Ÿå¸¦æ ‡ç­¾çš„æ•°æ®ä¸€èµ·ï¼Œæ„æˆä¸€ä¸ª<strong>åŠç›‘ç£å­¦ä¹ </strong> æµç¨‹ã€‚</p>
<h3 id="_7">å‹‡æ•¢åœ°ç®—</h3>
<p>å¯¹äºç›®æ ‡$\eqref{eq:loss-2}$æ¥è¯´ï¼Œå®ƒçš„ç§¯åˆ†ç»“æœæ˜¯ï¼š<br />
\begin{equation}\int q(\varepsilon) \big[l(f(x+\varepsilon),f(x)) + l(f(x),y)\big]d\varepsilon = l(f(x),y) + \int q(\varepsilon) l(f(x+\varepsilon),f(x)) d\varepsilon\end{equation}<br />
è¿˜æ˜¯è€è·¯å­ï¼Œè¿‘ä¼¼å±•å¼€$\varepsilon$ï¼š<br />
\begin{equation}\begin{aligned}l(f(x+\varepsilon),f(x))\approx &amp;\, l(f(x),f(x)) + \left.\sum_{i,j} \frac{\partial l(F(x),f(x))}{\partial F_i(x)}\frac{\partial f_i(x)}{\partial x_j}\varepsilon_j\right|<em i_j_k="i,j,k">{F(x)=f(x)}\\\<br />
&amp;\, + \frac{1}{2}\left.\sum</em>\varepsilon_j \varepsilon_k\right|} \frac{\partial l(F(x),f(x))}{\partial F_i(x)}\frac{\partial^2 f_i(x)}{\partial x_j \partial x_k<em i_i_j_k="i,i',j,k">{F(x)=f(x)}\\\<br />
&amp;\, + \frac{1}{2}\left.\sum</em>} \frac{\partial^2 l(F(x),f(x))}{\partial F_i(x) \partial F_{i'}(x)}\frac{\partial f_i(x)}{\partial x_j}\frac{\partial f_{i'}(x)}{\partial x_k}\varepsilon_j \varepsilon_k\right|_{F(x)=f(x)<br />
\end{aligned}\label{eq:kongbu}\end{equation}<br />
å¾ˆææ€–ï¼Ÿä¸ç€æ€¥ï¼Œæˆ‘ä»¬å›é¡¾ä¸€ä¸‹ï¼Œä½œä¸ºlosså‡½æ•°çš„$l$ï¼Œå®ƒä¸€èˆ¬ä¼šæœ‰å¦‚ä¸‹å‡ ä¸ªç‰¹ç‚¹ï¼š</p>
<blockquote>
<p>1ã€$l$æ˜¯å…‰æ»‘çš„ï¼›</p>
<p>2ã€$l(x, x)=0$ï¼›</p>
<p>3ã€$\left.\frac{\partial}{\partial x} l(x,y)\right|<em y="x">{x=y}=0,\left.\frac{\partial}{\partial y} l(x,y)\right|</em>=0$ã€‚</p>
</blockquote>
<p>è¿™å…¶å®å°±æ˜¯è¯´$l$æ˜¯å…‰æ»‘çš„ï¼Œå¹¶ä¸”åœ¨$x=y$çš„æ—¶å€™å–åˆ°æï¼ˆå°ï¼‰å€¼ï¼Œä¸”æï¼ˆå°ï¼‰å€¼ä¸º0ï¼Œè¿™å‡ ä¸ªç‰¹ç‚¹å‡ ä¹æ˜¯æ‰€æœ‰lossçš„å…±æ€§äº†ã€‚åŸºäºè¿™å‡ ä¸ªç‰¹ç‚¹ï¼Œææ€–çš„$\eqref{eq:kongbu}$å¼çš„å‰ä¸‰é¡¹å°±ç›´æ¥ä¸º0äº†ï¼Œæ‰€ä»¥æœ€åçš„ç§¯åˆ†ç»“æœæ˜¯ï¼š<br />
\begin{equation}\int q(\varepsilon) l(f(x+\varepsilon),f(x)) d\varepsilon \approx \frac{1}{2}\sigma^2\left.\sum_{i,i',j} \frac{\partial^2 l(F(x),f(x))}{\partial F_i(x) \partial F_{i'}(x)}\frac{\partial f_i(x)}{\partial x_j}\frac{\partial f_{i'}(x)}{\partial x_j}\right|_{F(x)=f(x)}<br />
\end{equation}</p>
<h3 id="_8">æ¢¯åº¦æƒ©ç½š</h3>
<p>çœ‹ä¸Šå»ä¾ç„¶è®©äººæœ‰äº›å¿ƒæ‚¸ï¼Œä½†æ€»æ¯”$\eqref{eq:kongbu}$å¥½å¤šäº†ã€‚ä¸Šå¼ä¹Ÿæ˜¯ä¸€ä¸ªæ­£åˆ™é¡¹ï¼Œå…¶ç‰¹ç‚¹æ˜¯åªåŒ…å«ä¸€é˜¶æ¢¯åº¦é¡¹ï¼Œè€Œå¯¹äºç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œ$\left.\frac{\partial^2 l(F(x),f(x))}{\partial F_i(x) \partial F_{i'}(x)}\right|<em i_="i'">{F(x)=f(x)}$å¯ä»¥æå‰ç®—å‡ºæ¥ï¼Œç‰¹åˆ«åœ°ï¼Œå¯¹äºå¸¸è§çš„å‡ ä¸ªæŸå¤±å‡½æ•°ï¼Œå½“$i\neq i'$æ—¶$\left.\frac{\partial^2 l(F(x),f(x))}{\partial F_i(x) \partial F</em>\right|}(x)<em i="i">{F(x)=f(x)}=0$ï¼Œæ‰€ä»¥ä»…éœ€è®¡ç®—$i=i'$çš„åˆ†é‡ï¼Œæˆ‘ä»¬è®°å®ƒä¸º$\lambda</em>(x)$ï¼Œé‚£ä¹ˆ<br />
\begin{equation}\int q(\varepsilon) l(f(x+\varepsilon),f(x)) d\varepsilon \approx \frac{1}{2}\sigma^2 \sum_i \lambda_i(x)\Vert \nabla_x f_i(x)\Vert^2\label{eq:gp}\end{equation}<br />
å¯è§ï¼Œå½¢å¼ä¸Šå°±æ˜¯å¯¹æ¯ä¸ª$f(x)$çš„æ¯ä¸ªåˆ†é‡éƒ½ç®—ä¸€ä¸ªæ¢¯åº¦æƒ©ç½šé¡¹$\Vert \nabla_x f_i(x)\Vert^2$ï¼Œç„¶åæŒ‰$\lambda_i(x)$åŠ æƒæ±‚å’Œã€‚</p>
<p>ä¾‹å¦‚ï¼Œå¯¹äºMSEæ¥è¯´ï¼Œ$l(f(x),y)=\Vert f(x) - y\Vert^2$ï¼Œè¿™æ—¶å€™å¯ä»¥ç®—å¾—$\lambda_i(x)\equiv 2$ï¼Œæ‰€ä»¥å¯¹åº”çš„æ­£åˆ™é¡¹ä¸º$\sum\limits_i\Vert \nabla_x f_i(x)\Vert^2$ï¼›å¯¹äºKLæ•£åº¦æ¥è¯´ï¼Œ$l(f(x),y)=\sum\limits_i y_i \log \frac{y_i}{f_i(x)}$ï¼Œè¿™æ—¶å€™$\lambda_i(x)=\frac{1}{f_i(x)}$ï¼Œé‚£ä¹ˆå¯¹åº”çš„æ­£åˆ™é¡¹ä¸º$\sum\limits_i f_i(x) \Vert \nabla_x \log f_i(x)\Vert^2$ã€‚è¿™äº›ç»“æœå¤§å®¶å¤šå¤šå°‘å°‘å¯ä»¥ä»è‘—åçš„â€œèŠ±ä¹¦â€<a href="https://book.douban.com/subject/27087503/">ã€Šæ·±åº¦å­¦ä¹ ã€‹</a>ä¸­æ‰¾åˆ°ç±»ä¼¼çš„ï¼Œå¹¶éæ–°çš„ç»“æœã€‚ç±»ä¼¼çš„æ¨å¯¼è¿˜å¯ä»¥å‚è€ƒæ–‡çŒ®<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-tikhonov-nc-95.pdf">ã€ŠTraining with noise is equivalent to Tikhonov regularizationã€‹</a>ã€‚</p>
<h3 id="_9">é‡‡æ ·è¿‘ä¼¼</h3>
<p>å½“ç„¶ï¼Œè™½ç„¶èƒ½æ±‚å‡ºåªå¸¦æœ‰ä¸€é˜¶æ¢¯åº¦çš„æ­£åˆ™é¡¹$\sum\limits_i \lambda_i(x)\Vert \nabla_x f_i(x)\Vert^2$ï¼Œä½†äº‹å®ä¸Šè¿™ä¸ªè®¡ç®—é‡ä¹Ÿä¸ä½ï¼Œå› ä¸ºéœ€è¦å¯¹æ¯ä¸ª$f_i(x)$éƒ½è¦æ±‚æ¢¯åº¦ï¼Œå¦‚æœè¾“å‡ºçš„åˆ†é‡æ•°å¤ªå¤§ï¼Œè¿™ä¸ªè®¡ç®—é‡ä¾ç„¶éš¾ä»¥æ‰¿å—ã€‚</p>
<p>è¿™æ—¶å€™å¯ä»¥è€ƒè™‘çš„æ–¹æ¡ˆæ˜¯é€šè¿‡é‡‡æ ·è¿‘ä¼¼è®¡ç®—ï¼šå‡è®¾$q(\eta)$æ˜¯å‡å€¼ä¸º0ã€æ–¹å·®ä¸º1çš„åˆ†å¸ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰<br />
\begin{equation}\sum\limits_i \Vert \nabla_x f_i(x)\Vert^2=\sum\limits_i \left\Vert \nabla_x f_i(x)\right\Vert^2=\mathbb{E}_{\eta_i\sim q(\eta)}\left[\left\Vert\sum_i \eta_i \nabla_x f_i(x)\right\Vert^2\right]\end{equation}<br />
è¿™æ ·ä¸€æ¥ï¼Œæ¯æ­¥æˆ‘ä»¬åªéœ€è¦ç®—$\sum\limits_i \eta_i f_i(x)$çš„æ¢¯åº¦ï¼Œä¸éœ€è¦ç®—å¤šæ¬¡æ¢¯åº¦ã€‚$q(\eta)$çš„ä¸€ä¸ªæœ€ç®€å•çš„å–æ³•æ˜¯ç©ºé—´ä¸º$\{-1,1\}$çš„å‡åŒ€åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯$\eta_i$ç­‰æ¦‚ç‡åœ°ä»$\{-1,1\}$ä¸­é€‰å–ä¸€ä¸ªã€‚</p>
<h2 id="_10">å¯¹æŠ—è®­ç»ƒ</h2>
<p>å›é¡¾å‰é¢çš„æµç¨‹ï¼Œæˆ‘ä»¬å…ˆæ˜¯ä»‹ç»äº†æ·»åŠ éšæœºå™ªå£°è¿™ä¸€å¢å¼ºæ³›åŒ–æ€§èƒ½çš„æ‰‹æ®µï¼Œç„¶åæŒ‡å‡ºéšæœºåŠ å™ªå£°å¯èƒ½å¤ªæ²¡ç‰¹å¼‚æ€§ï¼Œæ‰€ä»¥æƒ³ç€å…ˆæŠŠç§¯åˆ†ç®—å‡ºæ¥ï¼Œæ‰æœ‰äº†åé¢æ¨å¯¼çš„å…³äºè¿‘ä¼¼å±•å¼€ä¸æ¢¯åº¦æƒ©ç½šçš„ä¸€äº›ç»“æœã€‚é‚£ä¹ˆæ¢ä¸ªè§’åº¦æ¥æƒ³ï¼Œå¦‚æœæˆ‘ä»¬èƒ½æƒ³åŠæ³•æ›´ç‰¹å¼‚æ€§åœ°æ„é€ å™ªå£°ä¿¡å·ï¼Œé‚£ä¹ˆä¹Ÿèƒ½æé«˜è®­ç»ƒæ•ˆç‡ï¼Œå¢å¼ºæ³›åŒ–æ€§èƒ½äº†ã€‚</p>
<h3 id="_11">ç›‘ç£å¯¹æŠ—</h3>
<p>æœ‰ç›‘ç£çš„å¯¹æŠ—è®­ç»ƒï¼Œå…³æ³¨çš„æ˜¯åŸå§‹ç›®æ ‡$\eqref{eq:noisy-loss}$ï¼Œä¼˜åŒ–çš„ç›®æ ‡æ˜¯è®©losså°½å¯èƒ½å°ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬è¦é€‰æ‹©æ›´æœ‰ä»£è¡¨æ€§çš„å™ªå£°ï¼Œé‚£ä¹ˆåº”è¯¥é€‰æ‹©èƒ½è®©losså˜å¾—æ›´å¤§çš„å™ªå£°ï¼Œè€Œ<br />
\begin{equation}l(f(x + \varepsilon), y) \approx l(f(x), y) + \varepsilon \cdot \nabla_x l(f(x), y)\end{equation}<br />
æ‰€ä»¥è®©$l(f(x + \varepsilon), y)$å°½å¯èƒ½å¤§å°±æ„å‘³ç€$\varepsilon$è¦è·Ÿ$\nabla_x l(f(x), y)$åŒå‘ï¼Œæ¢è¨€ä¹‹æ‰°åŠ¨è¦å¾€æ¢¯åº¦ä¸Šå‡æ–¹å‘èµ°ï¼Œå³<br />
\begin{equation}\varepsilon \sim \nabla_x l(f(x), y)\end{equation}<br />
è¿™ä¾¿æ„æˆäº†å¯¹æŠ—è®­ç»ƒä¸­çš„FGMæ–¹æ³•ï¼Œä¹‹å‰åœ¨<a href="/archives/7234">ã€Šå¯¹æŠ—è®­ç»ƒæµ…è°ˆï¼šæ„ä¹‰ã€æ–¹æ³•å’Œæ€è€ƒï¼ˆé™„Keraså®ç°ï¼‰ã€‹</a>å°±å·²ç»ä»‹ç»è¿‡äº†ã€‚</p>
<p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨<a href="/archives/7234">ã€Šå¯¹æŠ—è®­ç»ƒæµ…è°ˆï¼šæ„ä¹‰ã€æ–¹æ³•å’Œæ€è€ƒï¼ˆé™„Keraså®ç°ï¼‰ã€‹</a>ä¸€æ–‡ä¸­æˆ‘ä»¬ä¹Ÿæ¨å¯¼è¿‡ï¼Œå¯¹æŠ—è®­ç»ƒåœ¨ä¸€å®šç¨‹åº¦ä¸Šä¹Ÿç­‰ä»·äºå¾€lossé‡Œè¾¹åŠ å…¥æ¢¯åº¦æƒ©ç½šé¡¹$\left\Vert\nabla_x l(f(x), y)\right\Vert^2$ï¼Œè¿™åˆè·Ÿå‰ä¸€èŠ‚çš„å…³äºå™ªå£°ç§¯åˆ†çš„ç»“æœç±»ä¼¼ã€‚è¿™è¡¨æ˜æ¢¯åº¦æƒ©ç½šåº”è¯¥æ˜¯é€šç”¨çš„èƒ½æé«˜æ¨¡å‹æ€§èƒ½çš„æ‰‹æ®µä¹‹ä¸€ã€‚</p>
<h3 id="_12">è™šæ‹Ÿå¯¹æŠ—</h3>
<p>åœ¨å‰é¢æˆ‘ä»¬æåˆ°ï¼Œ$l(f(x+\varepsilon),f(x))$è¿™ä¸€é¡¹ä¸éœ€è¦æ ‡ç­¾ä¿¡å·ï¼Œå› æ­¤å¯ä»¥ç”¨æ¥åšæ— ç›‘ç£å­¦ä¹ ï¼Œå¹¶ä¸”å…³äºå®ƒçš„å±•å¼€é«˜æ–¯ç§¯åˆ†æˆ‘ä»¬å¾—åˆ°äº†æ¢¯åº¦æƒ©ç½š$\eqref{eq:gp}$ã€‚å¦‚æœæ²¿ç€å¯¹æŠ—è®­ç»ƒçš„æ€æƒ³ï¼Œæˆ‘ä»¬ä¸å»è®¡ç®—ç§¯åˆ†ï¼Œè€Œæ˜¯å»å¯»æ‰¾è®©$l(f(x+\varepsilon),f(x))$å°½å¯èƒ½å¤§çš„æ‰°åŠ¨å™ªå£°ï¼Œè¿™å°±æ„æˆäº†â€œè™šæ‹Ÿå¯¹æŠ—è®­ç»ƒï¼ˆVATï¼‰â€ï¼Œé¦–æ¬¡å‡ºç°åœ¨æ–‡ç« <a href="https://papers.cool/arxiv/1704.03976">ã€ŠVirtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learningã€‹</a>ä¸­ã€‚</p>
<p>åŸºäºå‰é¢å¯¹æŸå¤±å‡½æ•°$l$çš„æ€§è´¨çš„è®¨è®ºï¼Œæˆ‘ä»¬çŸ¥é“$l(f(x+\varepsilon),f(x))$å…³äº$\varepsilon$çš„ä¸€é˜¶æ¢¯åº¦ä¸º0ï¼Œæ‰€ä»¥è¦ç®—å¯¹æŠ—æ‰°åŠ¨ï¼Œè¿˜å¿…é¡»å°†å®ƒå±•å¼€åˆ°äºŒé˜¶ï¼š<br />
\begin{equation}\begin{aligned}<br />
l(f(x+\varepsilon),f(x))\approx&amp;\, l(f(x),f(x)) + \varepsilon^{\top} \nabla_x l(f(x),f_{ng}(x)) + \frac{1}{2}\varepsilon^{\top}\nabla_x^2 l(f(x),f_{ng}(x)) \varepsilon\\\<br />
=&amp;\, \frac{1}{2}\varepsilon^{\top}\nabla_x^2 l(f(x),f_{ng}(x)) \varepsilon\end{aligned}\end{equation}<br />
è¿™é‡Œç”¨$f_{ng}(x)$è¡¨ç¤ºä¸éœ€è¦å¯¹é‡Œè¾¹çš„$x$æ±‚æ¢¯åº¦ã€‚è¿™æ ·ä¸€æ¥ï¼Œæˆ‘ä»¬éœ€è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼š1ã€å¦‚ä½•é«˜æ•ˆè®¡ç®—HessiançŸ©é˜µ$\mathcal{H}=\nabla_x^2 l(f(x),f_{ng}(x))$ï¼›2ã€å¦‚ä½•æ±‚å•ä½å‘é‡$u$ä½¿å¾—$u^{\top}\mathcal{H}u$æœ€å¤§ï¼Ÿ</p>
<p>äº‹å®ä¸Šï¼Œä¸éš¾è¯æ˜$u$çš„æœ€ä¼˜è§£å®é™…ä¸Šå°±æ˜¯â€œ$\mathcal{H}$çš„æœ€å¤§ç‰¹å¾æ ¹å¯¹åº”çš„ç‰¹å¾å‘é‡â€ï¼Œä¹Ÿç§°ä¸ºâ€œ$\mathcal{H}$çš„ä¸»ç‰¹å¾å‘é‡â€ï¼Œè€Œè¦è¿‘ä¼¼æ±‚ä¸»ç‰¹å¾å‘é‡ï¼Œä¸€ä¸ªè¡Œä¹‹æœ‰æ•ˆçš„æ–¹æ³•å°±æ˜¯â€œ<a href="https://en.wikipedia.org/wiki/Power_iteration">å¹‚è¿­ä»£æ³•</a>â€ï¼šä»ä¸€ä¸ªéšæœºå‘é‡$u_0$å‡ºå‘ï¼Œè¿­ä»£æ‰§è¡Œ$u_{i+1}=\frac{\mathcal{H}u_i}{\Vert\mathcal{H}u_i\Vert}$ã€‚ç›¸å…³æ¨å¯¼å¯ä»¥å‚è€ƒ<a href="/archives/6051#%E4%B8%BB%E7%89%B9%E5%BE%81%E6%A0%B9">ã€Šæ·±åº¦å­¦ä¹ ä¸­çš„Lipschitzçº¦æŸï¼šæ³›åŒ–ä¸ç”Ÿæˆæ¨¡å‹ã€‹</a>çš„â€œä¸»ç‰¹å¾æ ¹â€å’Œâ€œå¹‚è¿­ä»£â€ä¸¤èŠ‚ã€‚</p>
<p>åœ¨å¹‚è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¹¶ä¸éœ€è¦çŸ¥é“$\mathcal{H}$å…·ä½“å€¼ï¼Œåªéœ€è¦çŸ¥é“$\mathcal{H}u$çš„å€¼ï¼Œè¿™å¯ä»¥é€šè¿‡å·®åˆ†æ¥è¿‘ä¼¼è®¡ç®—ï¼š<br />
\begin{equation}\begin{aligned}\mathcal{H}u =&amp;\, \nabla_x^2 l(f(x),f_{ng}(x)) u\\\<br />
=&amp;\, \nabla_x \big(u\cdot\nabla_x l(f(x),f_{ng}(x))\big)\\\<br />
\approx&amp;\, \nabla_x \left(\frac{l(f(x + \xi u),f_{ng}(x)) - l(f(x),f_{ng}(x))}{\xi}\right)\\\<br />
=&amp;\, \frac{1}{\xi}\nabla_x l(f(x + \xi u),f_{ng}(x))\end{aligned}\end{equation}<br />
å…¶ä¸­$\xi$æ˜¯ä¸€ä¸ªæ ‡é‡å¸¸æ•°ã€‚æ ¹æ®è¿™ä¸ªè¿‘ä¼¼ç»“æœï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°å¦‚ä¸‹çš„VATæµç¨‹ï¼š</p>
<blockquote>
<p>åˆå§‹åŒ–å‘é‡$u\sim \mathcal{N}(0,1)$ã€æ ‡é‡$\epsilon$å’Œ$\xi$ï¼›<br />
 è¿­ä»£$r$æ¬¡ï¼š<br />
 $u \leftarrow \frac{u}{\Vert u\Vert}$ï¼›<br />
 $u \leftarrow \nabla_x l(f(x+\xi u), f_{ng}(x))$<br />
 $u \leftarrow \frac{u}{\Vert u\Vert}$ï¼›<br />
 ç”¨$l(f(x+\epsilon u), f_{ng}(x))$ä½œä¸ºlossæ‰§è¡Œå¸¸è§„æ¢¯åº¦ä¸‹é™ã€‚</p>
</blockquote>
<p>å®éªŒè¡¨æ˜ä¸€èˆ¬è¿­ä»£1æ¬¡å°±ä¸é”™äº†ï¼Œè€Œå¦‚æœè¿­ä»£0æ¬¡ï¼Œé‚£ä¹ˆå°±æ˜¯æœ¬æ–‡å¼€å¤´æåˆ°çš„æ·»åŠ é«˜æ–¯å™ªå£°ã€‚è¿™è¡¨æ˜è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒå°±æ˜¯é€šè¿‡$\nabla_x l(f(x+\xi u), f_{ng}(x))$æ¥æé«˜å™ªå£°çš„â€œç‰¹å¼‚æ€§â€çš„ã€‚</p>
<h3 id="_13">å‚è€ƒå®ç°</h3>
<p>å…³äºå¯¹æŠ—è®­ç»ƒçš„Keraså®ç°ï¼Œåœ¨<a href="/archives/7234">ã€Šå¯¹æŠ—è®­ç»ƒæµ…è°ˆï¼šæ„ä¹‰ã€æ–¹æ³•å’Œæ€è€ƒï¼ˆé™„Keraså®ç°ï¼‰ã€‹</a>ä¸€æ–‡ä¸­å·²ç»ç»™å‡ºè¿‡ï¼Œè¿™é‡Œç¬”è€…ç»™å‡ºKerasä¸‹è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒçš„å‚è€ƒå®ç°ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">virtual_adversarial_training</span><span class="p">(</span>
<span class="w">    </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">embedding_name</span><span class="p">,</span><span class="w"> </span><span class="n">epsilon</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">xi</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="n">iters</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;ç»™æ¨¡å‹æ·»åŠ è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒ</span>
<span class="ss">    å…¶ä¸­modelæ˜¯éœ€è¦æ·»åŠ å¯¹æŠ—è®­ç»ƒçš„kerasæ¨¡å‹ï¼Œembedding_name</span>
<span class="ss">    åˆ™æ˜¯modelé‡Œè¾¹Embeddingå±‚çš„åå­—ã€‚è¦åœ¨æ¨¡å‹compileä¹‹åä½¿ç”¨ã€‚</span>
<span class="ss">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">train_function</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">å¦‚æœè¿˜æ²¡æœ‰è®­ç»ƒå‡½æ•°</span>
<span class="w">        </span><span class="n">model</span><span class="p">.</span><span class="n">_make_train_function</span><span class="p">()</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">æ‰‹åŠ¨make</span>
<span class="w">    </span><span class="n">old_train_function</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">train_function</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">å¤‡ä»½æ—§çš„è®­ç»ƒå‡½æ•°</span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">æŸ¥æ‰¾Embeddingå±‚</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="nl">outputs</span><span class="p">:</span>
<span class="w">        </span><span class="n">embedding_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">search_layer</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">embedding_name</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">embedding_layer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">None</span><span class="err">:</span>
<span class="w">            </span><span class="k">break</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">embedding_layer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="err">:</span>
<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="k">Exception</span><span class="p">(</span><span class="s1">&#39;Embedding layer not found&#39;</span><span class="p">)</span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">æ±‚Embeddingæ¢¯åº¦</span>
<span class="w">    </span><span class="n">embeddings</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">embedding_layer</span><span class="p">.</span><span class="n">embeddings</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">EmbeddingçŸ©é˜µ</span>
<span class="w">    </span><span class="n">gradients</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">total_loss</span><span class="p">,</span><span class="w"> </span><span class="o">[</span><span class="n">embeddings</span><span class="o">]</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">Embeddingæ¢¯åº¦</span>
<span class="w">    </span><span class="n">gradients</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gradients</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">è½¬ä¸ºdense</span><span class="w"> </span><span class="n">tensor</span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">å°è£…ä¸ºå‡½æ•°</span>
<span class="w">    </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span>
<span class="w">        </span><span class="n">model</span><span class="p">.</span><span class="n">_feed_inputs</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">_feed_targets</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">_feed_sample_weights</span>
<span class="w">    </span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">æ‰€æœ‰è¾“å…¥å±‚</span>
<span class="w">    </span><span class="n">model_outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">.</span><span class="k">function</span><span class="p">(</span>
<span class="w">        </span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">        </span><span class="n">outputs</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">        </span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;model_outputs&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">æ¨¡å‹è¾“å‡ºå‡½æ•°</span>
<span class="w">    </span><span class="n">embedding_gradients</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">K</span><span class="p">.</span><span class="k">function</span><span class="p">(</span>
<span class="w">        </span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">        </span><span class="n">outputs</span><span class="o">=[</span><span class="n">gradients</span><span class="o">]</span><span class="p">,</span>
<span class="w">        </span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_gradients&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">æ¨¡å‹æ¢¯åº¦å‡½æ•°</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">())</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">1e-8</span><span class="p">)</span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">train_function</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">é‡æ–°å®šä¹‰è®­ç»ƒå‡½æ•°</span>
<span class="w">        </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_outputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="w">        </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="o">[</span><span class="n">:2</span><span class="o">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">inputs</span><span class="o">[</span><span class="n">3:</span><span class="o">]</span>
<span class="w">        </span><span class="n">delta1</span><span class="p">,</span><span class="w"> </span><span class="n">delta2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">K</span><span class="p">.</span><span class="n">int_shape</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">è¿­ä»£æ±‚æ‰°åŠ¨</span>
<span class="w">            </span><span class="n">delta2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xi</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">delta2</span><span class="p">)</span>
<span class="w">            </span><span class="n">K</span><span class="p">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">.</span><span class="n">eval</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">delta1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta2</span><span class="p">)</span>
<span class="w">            </span><span class="n">delta1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">delta2</span>
<span class="w">            </span><span class="n">delta2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">embedding_gradients</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">Embeddingæ¢¯åº¦</span>
<span class="w">        </span><span class="n">delta2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">epsilon</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">delta2</span><span class="p">)</span>
<span class="w">        </span><span class="n">K</span><span class="p">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">.</span><span class="n">eval</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">delta1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta2</span><span class="p">)</span>
<span class="w">        </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_train_function</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">æ¢¯åº¦ä¸‹é™</span>
<span class="w">        </span><span class="n">K</span><span class="p">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">.</span><span class="n">eval</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">delta2</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">åˆ é™¤æ‰°åŠ¨</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">outputs</span>

<span class="w">    </span><span class="n">model</span><span class="p">.</span><span class="n">train_function</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_function</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="n">è¦†ç›–åŸè®­ç»ƒå‡½æ•°</span>


<span class="err">#</span><span class="w"> </span><span class="n">å†™å¥½å‡½æ•°å</span><span class="err">ï¼Œ</span><span class="n">å¯ç”¨è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒåªéœ€è¦ä¸€è¡Œä»£ç </span>
<span class="n">virtual_adversarial_training</span><span class="p">(</span><span class="n">model_vat</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;Embedding-Token&#39;</span><span class="p">)</span>
</code></pre></div>

<p>å®Œæ•´çš„ä½¿ç”¨è„šæœ¬è¯·å‚è€ƒï¼š<a href="https://github.com/bojone/bert4keras/blob/master/examples/task_sentiment_virtual_adversarial_training.py">task_sentiment_virtual_adversarial_training.py</a>ã€‚å¤§æ¦‚æ˜¯å°†æ¨¡å‹å»ºç«‹ä¸¤æ¬¡ï¼Œä¸€ä¸ªæ¨¡å‹é€šè¿‡æ ‡æ³¨æ•°æ®æ­£å¸¸è®­ç»ƒï¼Œä¸€ä¸ªæ¨¡å‹é€šè¿‡æ— æ ‡æ³¨æ•°æ®è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒï¼Œä¸¤è€…äº¤æ›¿æ‰§è¡Œï¼Œè¯·è¯»æ‡‚æºç åå†ä½¿ç”¨ï¼Œä¸è¦ä¹±å¥—ä»£ç ã€‚å®éªŒä»»åŠ¡ä¸ºæƒ…å†µåˆ†ç±»ï¼Œå¤§çº¦æœ‰2ä¸‡çš„æ ‡æ³¨æ•°æ®ï¼Œå–å‰200ä¸ªä½œä¸ºæ ‡æ³¨æ ·æœ¬ï¼Œå‰©ä¸‹çš„ä½œä¸ºæ— æ ‡æ³¨æ•°æ®ï¼ŒVATå’ŒéVATçš„è¡¨ç°å¯¹æ¯”å¦‚ä¸‹ï¼ˆæ¯ä¸ªå®éªŒéƒ½é‡å¤äº†ä¸‰æ¬¡ï¼Œå–å¹³å‡ï¼‰ï¼š<br />
\begin{array}{c|cc}<br />
\hline<br />
&amp; \text{éªŒè¯é›†} &amp; \text{æµ‹è¯•é›†}\\\<br />
\hline<br />
\text{éVAT} &amp; 88.93\% &amp; 89.34\%\\\<br />
\text{VAT} &amp; 89.83\% &amp; 90.37\%\\\<br />
\hline<br />
\end{array}</p>
<blockquote>
<p><strong>è¯´æ˜ï¼š</strong> å‰é¢æåˆ°$f_{ng}(x)$è¡¨ç¤ºä¸å¯¹$x$æ±‚æ¢¯åº¦ï¼Œä¸è¿‡$f$è‡ªèº«çš„å‚æ•°$\theta$çš„æ¢¯åº¦è¿˜æ˜¯éœ€è¦æ±‚çš„ã€‚ä½†æ˜¯è¯»æ‡‚äº†ä¸Šè¿°ä»£ç çš„è¯»è€…ä¼šå‘ç°ï¼Œä¸Šè¿°å®ç°ä¸­ç›¸å½“äºæŠŠ$f_{ng}(x)$ä¸­$x,\theta$çš„æ¢¯åº¦éƒ½å»æ‰äº†ï¼Œç†è®ºä¸Šä¸å®Œå…¨ç­‰ä»·äºæ ‡å‡†çš„VATã€‚é—®é¢˜æ˜¯åœ¨Kerasä¸­å®ç°æ ‡å‡†çš„VATæœ‰ç‚¹éº»çƒ¦ï¼Œè€Œä¸”è®¡ç®—é‡ä¼šåŠ å¤§ï¼Œæ­¤å¤–å®éªŒå‘ç°ä¸Šè¿°â€œå±±å¯¨â€ç‰ˆæœ¬ä¹Ÿå·²ç»èƒ½å¸¦æ¥æå‡äº†ï¼Œæ ‡å‡†çš„VATç›¸å¯¹å®ƒè€Œè¨€å·®åˆ«æ˜¯äºŒé˜¶å°é‡çš„ï¼Œæ‰€ä»¥å·®åˆ«ä¸å¤§ï¼Œä¸Šè¿°ä»£ç åŸºæœ¬æ»¡è¶³éœ€æ±‚äº†ã€‚</p>
</blockquote>
<h2 id="_14">æ–‡ç« å°ç»“</h2>
<p>æœ¬æ–‡å…ˆä»‹ç»äº†æ·»åŠ éšæœºå™ªå£°è¿™ä¸€å¸¸è§„çš„æ­£åˆ™åŒ–æ‰‹æ®µï¼Œç„¶åé€šè¿‡è¿‘ä¼¼å±•å¼€ä¸ç§¯åˆ†çš„è¿‡ç¨‹ï¼Œæ¨å¯¼äº†å®ƒä¸æ¢¯åº¦æƒ©ç½šä¹‹é—´çš„è”ç³»ï¼Œå¹¶ä»ä¸­å¼•å‡ºäº†å¯ä»¥ç”¨äºåŠç›‘ç£è®­ç»ƒçš„æ¨¡å‹å¹³æ»‘æŸå¤±ï¼Œæ¥ç€è¿›ä¸€æ­¥è”ç³»åˆ°äº†ç›‘ç£å¼çš„å¯¹æŠ—è®­ç»ƒå’ŒåŠç›‘ç£çš„è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒï¼Œæœ€åç»™å‡ºäº†Kerasä¸‹è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒçš„å®ç°å’Œä¾‹å­ã€‚</p>
<p><em><strong>è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š</strong><a href="https://spaces.ac.cn/archives/7466">https://spaces.ac.cn/archives/7466</a></em></p>
<p><em><strong>æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="ã€Šç§‘å­¦ç©ºé—´FAQã€‹">ã€Šç§‘å­¦ç©ºé—´FAQã€‹</a></p>
<p><strong>å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚</strong></p>
<p><strong>å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼</strong></p>
<p>æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>å¾®ä¿¡æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>æ”¯ä»˜å®æ‰“èµ</p>
<p>å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>ç‚¹å‡»è¿™é‡Œ</strong></a>æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚</p>
<p><strong>å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š</strong></p>
<p>è‹å‰‘æ—. (Jun. 01, 2020). ã€Šæ³›åŒ–æ€§ä¹±å¼¹ï¼šä»éšæœºå™ªå£°ã€æ¢¯åº¦æƒ©ç½šåˆ°è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒ ã€‹[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/7466">https://spaces.ac.cn/archives/7466</a></p>
<p>@online{kexuefm-7466,<br />
title={æ³›åŒ–æ€§ä¹±å¼¹ï¼šä»éšæœºå™ªå£°ã€æ¢¯åº¦æƒ©ç½šåˆ°è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒ},<br />
author={è‹å‰‘æ—},<br />
year={2020},<br />
month={Jun},<br />
url={\url{https://spaces.ac.cn/archives/7466}},<br />
} </p>
<hr />
<h2 id="_15">å…¬å¼æ¨å¯¼ä¸æ³¨é‡Š</h2>
<p>æœ¬èŠ‚å°†å¯¹åŸæ–‡ä¸­çš„æ ¸å¿ƒå…¬å¼è¿›è¡Œæè¯¦ç»†çš„æ¨å¯¼å’Œæ‰©å±•ï¼Œæ¶µç›–å™ªå£°æ­£åˆ™åŒ–ç†è®ºã€æ¢¯åº¦æƒ©ç½šæ¨å¯¼ã€å¯¹æŠ—è®­ç»ƒæœºåˆ¶ä»¥åŠè™šæ‹Ÿå¯¹æŠ—è®­ç»ƒçš„å®Œæ•´æ•°å­¦æ¡†æ¶ã€‚</p>
<hr />
<h3 id="_16">ç¬¬ä¸€éƒ¨åˆ†ï¼šå™ªå£°æ­£åˆ™åŒ–çš„ç†è®ºåŸºç¡€</h3>
<h4 id="11">1.1 æ ‡å‡†ä¼˜åŒ–ç›®æ ‡ä¸å™ªå£°æ‰°åŠ¨</h4>
<p><strong>åŸºæœ¬è®¾å®š</strong>ï¼š</p>
<p>åŸå§‹çš„ç»éªŒé£é™©æœ€å°åŒ–ï¼ˆERMï¼‰ç›®æ ‡ä¸ºï¼š</p>
<p>$$
\min_{\theta} L(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}}[l(f_{\theta}(x), y)]
$$</p>
<p>å…¶ä¸­ï¼š
- $f_{\theta}(x)$ï¼šå‚æ•°ä¸º$\theta$çš„æ¨¡å‹
- $\mathcal{D}$ï¼šè®­ç»ƒæ•°æ®åˆ†å¸ƒ
- $l(\cdot, \cdot)$ï¼šæŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µã€MSEç­‰ï¼‰</p>
<p><strong>å¼•å…¥å™ªå£°æ‰°åŠ¨åçš„ç›®æ ‡</strong>ï¼š</p>
<p>$$
\min_{\theta} L_{\varepsilon}(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}, \varepsilon\sim q(\varepsilon)}[l(f_{\theta}(x + \varepsilon), y)]
$$</p>
<p><strong>è¯¦ç»†æ¨å¯¼</strong>ï¼š</p>
<p>å°†æ‰°åŠ¨åçš„æŸå¤±å±•å¼€ä¸ºæœŸæœ›å½¢å¼ï¼š</p>
<p>$$
\begin{aligned}
L_{\varepsilon}(\theta) &= \int_{\mathcal{D}} \int_{\varepsilon} p(x,y) \cdot q(\varepsilon) \cdot l(f_{\theta}(x+\varepsilon), y) \, d\varepsilon \, dx \, dy \\
&= \int_{\mathcal{D}} p(x,y) \left[\int_{\varepsilon} q(\varepsilon) \cdot l(f_{\theta}(x+\varepsilon), y) \, d\varepsilon\right] dx \, dy \\
&= \mathbb{E}_{(x,y)\sim \mathcal{D}}\left[\tilde{l}(x,y)\right]
\end{aligned}
$$</p>
<p>å…¶ä¸­å®šä¹‰äº†<strong>å•æ ·æœ¬åŠ å™ªå£°æŸå¤±</strong>ï¼š</p>
<p>$$
\tilde{l}(x,y) = \mathbb{E}_{\varepsilon\sim q(\varepsilon)}[l(f_{\theta}(x+\varepsilon), y)] = \int q(\varepsilon) \cdot l(f_{\theta}(x+\varepsilon), y) \, d\varepsilon
$$</p>
<h4 id="12">1.2 æ³°å‹’å±•å¼€ï¼šä»ç¦»æ•£é‡‡æ ·åˆ°è¿ç»­ç§¯åˆ†</h4>
<p><strong>ä¸€é˜¶æ³°å‹’å±•å¼€</strong>ï¼š</p>
<p>å‡è®¾$f_{\theta}$å’Œ$l$éƒ½æ˜¯å…‰æ»‘çš„ï¼Œå¯¹$l(f_{\theta}(x+\varepsilon), y)$åœ¨$\varepsilon=0$å¤„è¿›è¡Œæ³°å‹’å±•å¼€ï¼š</p>
<p>$$
l(f_{\theta}(x+\varepsilon), y) = l(f_{\theta}(x), y) + \left.\frac{\partial l(f_{\theta}(x+\varepsilon), y)}{\partial \varepsilon}\right|_{\varepsilon=0} + \frac{1}{2}\left.\frac{\partial^2 l(f_{\theta}(x+\varepsilon), y)}{\partial \varepsilon^2}\right|_{\varepsilon=0} + \mathcal{O}(\|\varepsilon\|^3)
$$</p>
<p><strong>é“¾å¼æ³•åˆ™å±•å¼€</strong>ï¼š</p>
<p>$$
\frac{\partial l(f_{\theta}(x+\varepsilon), y)}{\partial \varepsilon_i} = \sum_j \frac{\partial l}{\partial f_j} \cdot \frac{\partial f_j}{\partial x_k} \cdot \frac{\partial (x_k + \varepsilon_k)}{\partial \varepsilon_i} = \sum_j \frac{\partial l}{\partial f_j} \cdot \frac{\partial f_j}{\partial x_i}
$$</p>
<p>è®°æ¢¯åº¦ä¸º$\nabla_x l = \sum_j \frac{\partial l}{\partial f_j} \nabla_x f_j$ï¼Œåˆ™ï¼š</p>
<p>$$
l(f_{\theta}(x+\varepsilon), y) \approx l(f_{\theta}(x), y) + \varepsilon^{\top} \nabla_x l(f_{\theta}(x), y) + \frac{1}{2} \varepsilon^{\top} \mathcal{H}_x \varepsilon
$$</p>
<p>å…¶ä¸­$\mathcal{H}<em _theta="\theta">x = \nabla_x^2 l(f</em>(x), y)$æ˜¯å…³äº$x$çš„HessiançŸ©é˜µã€‚</p>
<h4 id="13">1.3 é«˜æ–¯å™ªå£°çš„ç§¯åˆ†è®¡ç®—</h4>
<p><strong>å‡è®¾</strong>ï¼š$q(\varepsilon) = \mathcal{N}(0, \sigma^2 I)$ï¼Œå³$\varepsilon$çš„å„åˆ†é‡ç‹¬ç«‹åŒåˆ†å¸ƒäº$\mathcal{N}(0, \sigma^2)$ã€‚</p>
<p><strong>å…³é”®æ€§è´¨</strong>ï¼š
1. $\mathbb{E}[\varepsilon_i] = 0$
2. $\mathbb{E}[\varepsilon_i \varepsilon_j] = \sigma^2 \delta_{ij}$ï¼ˆ$\delta_{ij}$æ˜¯Kronecker deltaï¼‰
3. $\mathbb{E}[\varepsilon_i^2] = \sigma^2$</p>
<p><strong>ç§¯åˆ†è®¡ç®—</strong>ï¼š</p>
<p>$$
\begin{aligned}
\tilde{l}(x,y) &= \int q(\varepsilon) \cdot l(f_{\theta}(x+\varepsilon), y) \, d\varepsilon \\
&\approx \int q(\varepsilon) \left[l(f_{\theta}(x), y) + \varepsilon^{\top} \nabla_x l + \frac{1}{2} \varepsilon^{\top} \mathcal{H}_x \varepsilon\right] d\varepsilon \\
&= l(f_{\theta}(x), y) \cdot \underbrace{\int q(\varepsilon) d\varepsilon}_{=1} + \underbrace{\int q(\varepsilon) \varepsilon^{\top} \nabla_x l \, d\varepsilon}_{=0} + \frac{1}{2} \int q(\varepsilon) \varepsilon^{\top} \mathcal{H}_x \varepsilon \, d\varepsilon
\end{aligned}
$$</p>
<p><strong>äºŒæ¬¡å‹æœŸæœ›çš„è®¡ç®—</strong>ï¼š</p>
<p>$$
\begin{aligned}
\mathbb{E}_{\varepsilon}\left[\varepsilon^{\top} \mathcal{H}_x \varepsilon\right] &= \mathbb{E}\left[\sum_{i,j} \varepsilon_i \mathcal{H}_{ij} \varepsilon_j\right] \\
&= \sum_{i,j} \mathcal{H}_{ij} \mathbb{E}[\varepsilon_i \varepsilon_j] \\
&= \sum_{i,j} \mathcal{H}_{ij} \cdot \sigma^2 \delta_{ij} \\
&= \sigma^2 \sum_i \mathcal{H}_{ii} \\
&= \sigma^2 \cdot \text{tr}(\mathcal{H}_x)
\end{aligned}
$$</p>
<p><strong>æœ€ç»ˆç»“æœ</strong>ï¼š</p>
<p>$$
\boxed{\tilde{l}(x,y) \approx l(f_{\theta}(x), y) + \frac{\sigma^2}{2} \text{tr}(\mathcal{H}_x) = l(f_{\theta}(x), y) + \frac{\sigma^2}{2} \Delta_x l(f_{\theta}(x), y)}
$$</p>
<p>å…¶ä¸­$\Delta_x = \sum_i \frac{\partial^2}{\partial x_i^2}$æ˜¯æ‹‰æ™®æ‹‰æ–¯ç®—å­ã€‚</p>
<h4 id="14">1.4 æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–çš„ç†è®ºæ„ä¹‰</h4>
<p><strong>ç‰©ç†è§£é‡Š</strong>ï¼šæ‹‰æ™®æ‹‰æ–¯ç®—å­$\Delta l$è¡¡é‡äº†æŸå¤±å‡½æ•°åœ¨è¾“å…¥ç©ºé—´ä¸­çš„"æ›²ç‡"ã€‚</p>
<p><strong>å‡ ä½•æ„ä¹‰</strong>ï¼š
- $\Delta l &gt; 0$ï¼š$l$æ˜¯å±€éƒ¨å‡¸çš„ï¼ˆå‘ä¸Šå¼¯æ›²ï¼‰
- $\Delta l &lt; 0$ï¼š$l$æ˜¯å±€éƒ¨å‡¹çš„ï¼ˆå‘ä¸‹å¼¯æ›²ï¼‰
- $\Delta l = 0$ï¼š$l$åœ¨è¯¥ç‚¹æ˜¯"å¹³å¦"çš„</p>
<p><strong>æ­£åˆ™åŒ–æ•ˆæœ</strong>ï¼šæœ€å°åŒ–$l + \frac{\sigma^2}{2}\Delta l$ç›¸å½“äºï¼š
1. æœ€å°åŒ–åŸå§‹æŸå¤±$l$
2. æƒ©ç½šè¾“å…¥ç©ºé—´ä¸­çš„é«˜æ›²ç‡åŒºåŸŸï¼Œé¼“åŠ±æ¨¡å‹åœ¨è¾“å…¥é™„è¿‘ä¿æŒ"å¹³æ»‘"</p>
<p><strong>å®é™…å›°éš¾</strong>ï¼šè®¡ç®—$\Delta l$éœ€è¦äºŒé˜¶å¯¼æ•°ï¼Œè¿›è€Œä¼˜åŒ–æ—¶éœ€è¦ä¸‰é˜¶å¯¼æ•°ï¼ˆæ¢¯åº¦çš„äºŒé˜¶å¯¼æ•°ï¼‰ï¼Œè¿™åœ¨æ·±åº¦å­¦ä¹ ä¸­è®¡ç®—ä»£ä»·æé«˜ã€‚</p>
<hr />
<h3 id="_17">ç¬¬äºŒéƒ¨åˆ†ï¼šè½¬ç§»ç›®æ ‡ä¸ä¸‰è§’ä¸ç­‰å¼</h3>
<h4 id="21">2.1 æ›¿ä»£æŸå¤±å‡½æ•°çš„æ„é€ </h4>
<p><strong>åŠ¨æœº</strong>ï¼šç›´æ¥è®¡ç®—$\int q(\varepsilon) l(f(x+\varepsilon), y) d\varepsilon$å›°éš¾ï¼Œè€ƒè™‘ä¼˜åŒ–ä¸Šç•Œã€‚</p>
<p><strong>æ–°ç›®æ ‡</strong>ï¼š</p>
<p>$$
\tilde{L}(x, y) = l(f(x+\varepsilon), f(x)) + l(f(x), y)
$$</p>
<p><strong>ä¸‰è§’ä¸ç­‰å¼è®ºè¯</strong>ï¼š</p>
<p>å‡è®¾$l$æ˜¯åº¦é‡ï¼ˆmetricï¼‰ï¼Œå³æ»¡è¶³ï¼š
1. éè´Ÿæ€§ï¼š$l(a, b) \geq 0$ï¼Œä¸”$l(a,a) = 0$
2. å¯¹ç§°æ€§ï¼š$l(a, b) = l(b, a)$
3. ä¸‰è§’ä¸ç­‰å¼ï¼š$l(a, c) \leq l(a, b) + l(b, c)$</p>
<p>åˆ™æœ‰ï¼š</p>
<p>$$
l(f(x+\varepsilon), y) \leq l(f(x+\varepsilon), f(x)) + l(f(x), y)
$$</p>
<p><strong>Jensenä¸ç­‰å¼æ¨å¹¿</strong>ï¼ˆé’ˆå¯¹éåº¦é‡æŸå¤±ï¼‰ï¼š</p>
<p>å¯¹äºMSEæŸå¤±$l(a, b) = |a - b|^2$ï¼š</p>
<p>$$
\begin{aligned}
\|f(x+\varepsilon) - y\|^2 &= \|f(x+\varepsilon) - f(x) + f(x) - y\|^2 \\
&= \left\|\frac{1}{2} \cdot 2[f(x+\varepsilon) - f(x)] + \frac{1}{2} \cdot 2[f(x) - y]\right\|^2
\end{aligned}
$$</p>
<p>ç”±å‡¸æ€§ï¼ˆ$|\cdot|^2$æ˜¯å‡¸å‡½æ•°ï¼‰ï¼š</p>
<p>$$
\begin{aligned}
&\leq \frac{1}{2}\|2[f(x+\varepsilon) - f(x)]\|^2 + \frac{1}{2}\|2[f(x) - y]\|^2 \\
&= 2\|f(x+\varepsilon) - f(x)\|^2 + 2\|f(x) - y\|^2
\end{aligned}
$$</p>
<p>å› æ­¤ï¼š</p>
<p>$$
\boxed{l(f(x+\varepsilon), y) \leq 2\left[l(f(x+\varepsilon), f(x)) + l(f(x), y)\right]}
$$</p>
<h4 id="22">2.2 åŠç›‘ç£å­¦ä¹ çš„è‡ªç„¶æ¶Œç°</h4>
<p><strong>å…³é”®è§‚å¯Ÿ</strong>ï¼šæŸå¤±$l(f(x+\varepsilon), f(x))$çš„ç‰¹æ®Šæ€§è´¨ï¼š</p>
<ol>
<li><strong>æ— éœ€æ ‡ç­¾</strong>ï¼šåªä¾èµ–äº$x$ï¼Œä¸ä¾èµ–äº$y$</li>
<li><strong>å¹³æ»‘æ€§æƒ©ç½š</strong>ï¼šé¼“åŠ±$f(x+\varepsilon) \approx f(x)$</li>
<li><strong>å¯¹ç§°æ€§</strong>ï¼š$l(f(x+\varepsilon), f(x)) = l(f(x), f(x+\varepsilon))$</li>
</ol>
<p><strong>åŠç›‘ç£å­¦ä¹ æ¡†æ¶</strong>ï¼š</p>
<p>æ€»æŸå¤±å¯åˆ†è§£ä¸ºï¼š</p>
<p>$$
L_{\text{total}} = \underbrace{\mathbb{E}_{(x,y)\sim \mathcal{D}_L}[l(f(x), y)]}_{\text{ç›‘ç£æŸå¤±ï¼ˆæœ‰æ ‡ç­¾æ•°æ®ï¼‰}} + \lambda \underbrace{\mathbb{E}_{x\sim \mathcal{D}_U, \varepsilon\sim q}[l(f(x+\varepsilon), f(x))]}_{\text{å¹³æ»‘æ€§æ­£åˆ™åŒ–ï¼ˆæ— æ ‡ç­¾æ•°æ®ï¼‰}}
$$</p>
<p>å…¶ä¸­ï¼š
- $\mathcal{D}_L$ï¼šæœ‰æ ‡ç­¾æ•°æ®é›†ï¼ˆé€šå¸¸å¾ˆå°ï¼‰
- $\mathcal{D}_U$ï¼šæ— æ ‡ç­¾æ•°æ®é›†ï¼ˆé€šå¸¸å¾ˆå¤§ï¼‰
- $\lambda &gt; 0$ï¼šæƒé‡ç³»æ•°</p>
<p><strong>ç†è®ºä¼˜åŠ¿</strong>ï¼š
1. å……åˆ†åˆ©ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®
2. é€šè¿‡å¹³æ»‘æ€§å‡è®¾ï¼ˆæµå½¢å‡è®¾ï¼‰æå‡æ³›åŒ–æ€§èƒ½
3. é€‚ç”¨äºæ ‡ç­¾ç¨€ç¼ºçš„åœºæ™¯</p>
<hr />
<h3 id="_18">ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¢¯åº¦æƒ©ç½šçš„ä¸¥æ ¼æ¨å¯¼</h3>
<h4 id="31">3.1 æŸå¤±å‡½æ•°çš„æ€§è´¨å‡è®¾</h4>
<p>å¯¹äºä»»ä½•åˆç†çš„æŸå¤±å‡½æ•°$l(a, b)$ï¼Œæˆ‘ä»¬å‡è®¾ï¼š</p>
<p><strong>æ€§è´¨1ï¼ˆå…‰æ»‘æ€§ï¼‰</strong>ï¼š$l \in C^2$ï¼ˆäºŒé˜¶è¿ç»­å¯å¯¼ï¼‰</p>
<p><strong>æ€§è´¨2ï¼ˆé›¶ç‚¹æ€§è´¨ï¼‰</strong>ï¼š$l(a, a) = 0$</p>
<p><strong>æ€§è´¨3ï¼ˆæå€¼æ€§è´¨ï¼‰</strong>ï¼š$a = b$æ˜¯$l(a, b)$çš„æå°å€¼ç‚¹ï¼Œå³ï¼š</p>
<p>$$
\left.\frac{\partial l(a, b)}{\partial a}\right|_{a=b} = 0, \quad \left.\frac{\partial l(a, b)}{\partial b}\right|_{b=a} = 0
$$</p>
<p><strong>éªŒè¯å¸¸è§æŸå¤±å‡½æ•°</strong>ï¼š</p>
<ol>
<li><strong>MSE</strong>ï¼š$l(a, b) = |a - b|^2$</li>
<li>$l(a, a) = 0$ âœ“</li>
<li>
<p>$\frac{\partial l}{\partial a} = 2(a - b) \big|_{a=b} = 0$ âœ“</p>
</li>
<li>
<p><strong>äº¤å‰ç†µ</strong>ï¼ˆsoftmaxï¼‰ï¼š$l(p, q) = -\sum_i q_i \log p_i$</p>
</li>
<li>å½“$p = q$æ—¶ï¼Œ$\frac{\partial l}{\partial p_i} = -\frac{q_i}{p_i} \big|_{p=q} = -1$ï¼ˆéœ€å½’ä¸€åŒ–çº¦æŸä¿®æ­£ï¼‰</li>
<li>
<p>å®é™…ä¸Šå¯¹äºæ¦‚ç‡åˆ†å¸ƒï¼Œ$l(p, p) = -\sum_i p_i \log p_i$ï¼ˆç†µï¼‰ï¼Œæœ€å°å€¼åœ¨$p = \text{one-hot}$</p>
</li>
<li>
<p><strong>KLæ•£åº¦</strong>ï¼š$l(p, q) = \sum_i q_i \log \frac{q_i}{p_i}$</p>
</li>
<li>$l(p, p) = 0$ âœ“</li>
<li>$\frac{\partial l}{\partial p_i}\big|<em p="q">{p=q} = -\frac{q_i}{p_i}\big|</em> = -1$ï¼ˆéœ€çº¦æŸä¿®æ­£ï¼‰</li>
</ol>
<h4 id="32">3.2 å¹³æ»‘æŸå¤±çš„æ³°å‹’å±•å¼€</h4>
<p>è€ƒè™‘$l(f(x+\varepsilon), f(x))$ï¼Œè®°$F = f(x+\varepsilon)$ï¼Œ$G = f(x)$ã€‚</p>
<p><strong>é›¶é˜¶é¡¹</strong>ï¼ˆç”±æ€§è´¨2ï¼‰ï¼š</p>
<p>$$
l(f(x), f(x)) = 0
$$</p>
<p><strong>ä¸€é˜¶é¡¹</strong>ï¼ˆç”±æ€§è´¨3ï¼‰ï¼š</p>
<p>$$
\left.\frac{\partial l(F, G)}{\partial F}\right|_{F=G} = 0
$$</p>
<p>å› æ­¤ï¼š</p>
<p>$$
\begin{aligned}
l(f(x+\varepsilon), f(x)) &= l(F, G) \\
&= \underbrace{l(G, G)}_{=0} + \underbrace{\left.\frac{\partial l}{\partial F}\right|_{F=G}}_{=0} \cdot (F - G) + \frac{1}{2}(F - G)^{\top} \left.\frac{\partial^2 l}{\partial F^2}\right|_{F=G} (F - G) + \cdots
\end{aligned}
$$</p>
<p><strong>äºŒé˜¶HessiançŸ©é˜µ</strong>ï¼š</p>
<p>è®°$\mathcal{H}<em F="G">F = \left.\frac{\partial^2 l(F, G)}{\partial F_i \partial F_j}\right|</em>$ï¼ˆå…³äº$F$çš„Hessianï¼‰</p>
<p>åˆ™ï¼š</p>
<p>$$
l(f(x+\varepsilon), f(x)) \approx \frac{1}{2} \sum_{i,j} [f_i(x+\varepsilon) - f_i(x)] \cdot \mathcal{H}_{ij} \cdot [f_j(x+\varepsilon) - f_j(x)]
$$</p>
<h4 id="33">3.3 è¾“å…¥æ‰°åŠ¨åˆ°è¾“å‡ºæ‰°åŠ¨çš„ä¼ æ’­</h4>
<p><strong>é“¾å¼æ³•åˆ™</strong>ï¼š</p>
<p>$$
f_i(x+\varepsilon) - f_i(x) \approx \sum_k \frac{\partial f_i}{\partial x_k} \varepsilon_k = (\nabla_x f_i)^{\top} \varepsilon
$$</p>
<p>ä»£å…¥äºŒé˜¶å±•å¼€ï¼š</p>
<p>$$
\begin{aligned}
l(f(x+\varepsilon), f(x)) &\approx \frac{1}{2} \sum_{i,j} [(\nabla_x f_i)^{\top} \varepsilon] \cdot \mathcal{H}_{ij} \cdot [(\nabla_x f_j)^{\top} \varepsilon] \\
&= \frac{1}{2} \sum_{i,j} \sum_{k,l} \frac{\partial f_i}{\partial x_k} \varepsilon_k \mathcal{H}_{ij} \frac{\partial f_j}{\partial x_l} \varepsilon_l \\
&= \frac{1}{2} \sum_{k,l} \varepsilon_k \varepsilon_l \underbrace{\left(\sum_{i,j} \frac{\partial f_i}{\partial x_k} \mathcal{H}_{ij} \frac{\partial f_j}{\partial x_l}\right)}_{=: \tilde{\mathcal{H}}_{kl}}
\end{aligned}
$$</p>
<h4 id="34">3.4 é«˜æ–¯å™ªå£°ç§¯åˆ†</h4>
<p>å‡è®¾$\varepsilon \sim \mathcal{N}(0, \sigma^2 I)$ï¼š</p>
<p>$$
\begin{aligned}
\mathbb{E}_{\varepsilon}[l(f(x+\varepsilon), f(x))] &\approx \frac{1}{2} \mathbb{E}\left[\sum_{k,l} \varepsilon_k \varepsilon_l \tilde{\mathcal{H}}_{kl}\right] \\
&= \frac{1}{2} \sum_{k,l} \tilde{\mathcal{H}}_{kl} \mathbb{E}[\varepsilon_k \varepsilon_l] \\
&= \frac{1}{2} \sum_{k,l} \tilde{\mathcal{H}}_{kl} \cdot \sigma^2 \delta_{kl} \\
&= \frac{\sigma^2}{2} \sum_k \tilde{\mathcal{H}}_{kk}
\end{aligned}
$$</p>
<p><strong>å±•å¼€$\tilde{\mathcal{H}}_{kk}$</strong>ï¼š</p>
<p>$$
\tilde{\mathcal{H}}_{kk} = \sum_{i,j} \frac{\partial f_i}{\partial x_k} \mathcal{H}_{ij} \frac{\partial f_j}{\partial x_k}
$$</p>
<p><strong>å¯¹è§’åŒ–å‡è®¾</strong>ï¼šå¯¹äºå¤šæ•°æŸå¤±å‡½æ•°ï¼ˆå¦‚MSEã€KLæ•£åº¦ï¼‰ï¼Œå½“$F = G$æ—¶ï¼š</p>
<p>$$
\mathcal{H}_{ij} = \left.\frac{\partial^2 l(F, G)}{\partial F_i \partial F_j}\right|_{F=G} = 0 \quad (i \neq j)
$$</p>
<p>è®°$\lambda_i = \mathcal{H}_{ii}$ï¼ˆå¯¹è§’å…ƒç´ ï¼‰ï¼Œåˆ™ï¼š</p>
<p>$$
\tilde{\mathcal{H}}_{kk} = \sum_i \lambda_i \left(\frac{\partial f_i}{\partial x_k}\right)^2 = \sum_i \lambda_i \|\nabla_x f_i\|^2_k
$$</p>
<p>å…¶ä¸­$|\cdot|_k$è¡¨ç¤ºåªå–ç¬¬$k$ä¸ªåˆ†é‡ã€‚</p>
<p><strong>æœ€ç»ˆç»“æœ</strong>ï¼š</p>
<p>$$
\boxed{\mathbb{E}_{\varepsilon}[l(f(x+\varepsilon), f(x))] \approx \frac{\sigma^2}{2} \sum_i \lambda_i \|\nabla_x f_i(x)\|^2}
$$</p>
<h4 id="35-lambda_i">3.5 å…·ä½“æŸå¤±å‡½æ•°çš„$\lambda_i$è®¡ç®—</h4>
<p><strong>ï¼ˆ1ï¼‰å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰</strong>ï¼š</p>
<p>$$
l(a, b) = \sum_i (a_i - b_i)^2
$$</p>
<p>è®¡ç®—Hessianï¼š</p>
<p>$$
\frac{\partial^2 l}{\partial a_i \partial a_j}\bigg|_{a=b} = \frac{\partial}{\partial a_j}[2(a_i - b_i)]\bigg|_{a=b} = 2\delta_{ij}
$$</p>
<p>å› æ­¤$\lambda_i = 2$ï¼Œæ­£åˆ™é¡¹ä¸ºï¼š</p>
<p>$$
R_{\text{MSE}} = \sigma^2 \sum_i \|\nabla_x f_i(x)\|^2
$$</p>
<p><strong>ï¼ˆ2ï¼‰KLæ•£åº¦</strong>ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰ï¼š</p>
<p>$$
l(p, q) = \sum_i q_i \log \frac{q_i}{p_i}
$$</p>
<p>è®¡ç®—äºŒé˜¶å¯¼æ•°ï¼š</p>
<p>$$
\frac{\partial^2 l}{\partial p_i \partial p_j}\bigg|_{p=q} = \frac{\partial}{\partial p_j}\left[-\frac{q_i}{p_i}\right]\bigg|_{p=q} = \frac{q_i}{p_i^2}\delta_{ij}\bigg|_{p=q} = \frac{1}{q_i}\delta_{ij}
$$</p>
<p>å› æ­¤$\lambda_i = \frac{1}{f_i(x)}$ï¼Œæ­£åˆ™é¡¹ä¸ºï¼š</p>
<p>$$
R_{\text{KL}} = \frac{\sigma^2}{2} \sum_i \frac{1}{f_i(x)} \|\nabla_x f_i(x)\|^2 = \frac{\sigma^2}{2} \sum_i f_i(x) \|\nabla_x \log f_i(x)\|^2
$$</p>
<p>è¿™ä¹Ÿç§°ä¸º<strong>Fisherä¿¡æ¯çŸ©é˜µ</strong>çš„å¯¹è§’è¿‘ä¼¼ï¼</p>
<p><strong>ï¼ˆ3ï¼‰äº¤å‰ç†µ</strong>ï¼ˆäºŒåˆ†ç±»ï¼‰ï¼š</p>
<p>$$
l(p, y) = -y\log p - (1-y)\log(1-p)
$$</p>
<p>å½“$y = \sigma(f(x))$ï¼ˆlogisticè¾“å‡ºï¼‰æ—¶ï¼Œç±»ä¼¼è®¡ç®—å¯å¾—$\lambda \propto \frac{1}{p(1-p)}$ã€‚</p>
<hr />
<h3 id="_19">ç¬¬å››éƒ¨åˆ†ï¼šæ¢¯åº¦æƒ©ç½šçš„é«˜æ•ˆé‡‡æ ·è¿‘ä¼¼</h3>
<h4 id="41">4.1 è®¡ç®—ç“¶é¢ˆåˆ†æ</h4>
<p><strong>é—®é¢˜</strong>ï¼šç›´æ¥è®¡ç®—$\sum_i \lambda_i |\nabla_x f_i(x)|^2$éœ€è¦ï¼š</p>
<ol>
<li>å¯¹æ¯ä¸ªè¾“å‡ºåˆ†é‡$f_i(x)$è®¡ç®—æ¢¯åº¦$\nabla_x f_i$</li>
<li>å¦‚æœè¾“å‡ºç»´åº¦$d$å¾ˆå¤§ï¼ˆå¦‚ImageNetæœ‰1000ç±»ï¼‰ï¼Œéœ€è¦è®¡ç®—1000æ¬¡æ¢¯åº¦</li>
<li>æ¯æ¬¡æ¢¯åº¦è®¡ç®—çš„å¤æ‚åº¦ä¸æ¨¡å‹å‚æ•°é‡æˆæ­£æ¯”</li>
</ol>
<p><strong>å¤æ‚åº¦ä¼°è®¡</strong>ï¼š
- æ­£å‘ä¼ æ’­ï¼š$O(P)$ï¼ˆ$P$ä¸ºå‚æ•°é‡ï¼‰
- åå‘ä¼ æ’­ï¼ˆå•ä¸ªè¾“å‡ºï¼‰ï¼š$O(P)$
- æ€»å¤æ‚åº¦ï¼š$O(d \cdot P)$ï¼ˆ$d$ä¸ºè¾“å‡ºç»´åº¦ï¼‰</p>
<h4 id="42-monte-carlo">4.2 Monte Carloé‡‡æ ·è¿‘ä¼¼</h4>
<p><strong>å…³é”®è§‚å¯Ÿ</strong>ï¼šåˆ©ç”¨æœŸæœ›çš„çº¿æ€§æ€§ï¼š</p>
<p>$$
\sum_i \|\nabla_x f_i\|^2 = \mathbb{E}_{\eta \sim \mathcal{U}(\{e_1, \ldots, e_d\})}\left[\left\|\nabla_x (\eta^{\top} f(x))\right\|^2\right] \cdot d
$$</p>
<p>å…¶ä¸­$e_i$æ˜¯ç¬¬$i$ä¸ªæ ‡å‡†åŸºå‘é‡ã€‚</p>
<p><strong>æ¨å¹¿åˆ°ä»»æ„åˆ†å¸ƒ</strong>ï¼š</p>
<p>è®¾$\eta \sim q(\eta)$æ˜¯å‡å€¼ä¸º0ã€æ–¹å·®ä¸º1çš„éšæœºå‘é‡ï¼ˆå„åˆ†é‡ç‹¬ç«‹ï¼‰ï¼Œåˆ™ï¼š</p>
<p>$$
\sum_i \|\nabla_x f_i\|^2 = \mathbb{E}_{\eta \sim q}\left[\left\|\sum_i \eta_i \nabla_x f_i(x)\right\|^2\right]
$$</p>
<p><strong>è¯æ˜</strong>ï¼š</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\left\|\sum_i \eta_i \nabla_x f_i\right\|^2\right] &= \mathbb{E}\left[\sum_k \left(\sum_i \eta_i \frac{\partial f_i}{\partial x_k}\right)^2\right] \\
&= \sum_k \mathbb{E}\left[\sum_{i,j} \eta_i \eta_j \frac{\partial f_i}{\partial x_k} \frac{\partial f_j}{\partial x_k}\right] \\
&= \sum_k \sum_{i,j} \frac{\partial f_i}{\partial x_k} \frac{\partial f_j}{\partial x_k} \mathbb{E}[\eta_i \eta_j] \\
&= \sum_k \sum_i \left(\frac{\partial f_i}{\partial x_k}\right)^2 \cdot \mathbb{E}[\eta_i^2] \quad (\text{ç‹¬ç«‹æ€§}) \\
&= \sum_k \sum_i \left(\frac{\partial f_i}{\partial x_k}\right)^2 \cdot 1 \\
&= \sum_i \|\nabla_x f_i\|^2
\end{aligned}
$$</p>
<h4 id="43">4.3 å®ç”¨é‡‡æ ·ç­–ç•¥</h4>
<p><strong>ç­–ç•¥1ï¼šRademacheråˆ†å¸ƒ</strong>ï¼ˆ$\eta_i \in {-1, +1}$ç­‰æ¦‚ç‡ï¼‰</p>
<ul>
<li>ä¼˜ç‚¹ï¼šè®¡ç®—ç®€å•ï¼Œæ— éœ€ç”Ÿæˆæµ®ç‚¹éšæœºæ•°</li>
<li>æœŸæœ›ï¼š$\mathbb{E}[\eta_i] = 0$ï¼Œ$\mathbb{E}[\eta_i^2] = 1$</li>
<li>å®ç°ï¼š<code>eta = 2 * torch.randint(0, 2, (d,)) - 1</code></li>
</ul>
<p><strong>ç­–ç•¥2ï¼šæ ‡å‡†é«˜æ–¯åˆ†å¸ƒ</strong>ï¼ˆ$\eta_i \sim \mathcal{N}(0, 1)$ï¼‰</p>
<ul>
<li>ä¼˜ç‚¹ï¼šç†è®ºä¸Šæ–¹å·®æ›´ç¨³å®š</li>
<li>å®ç°ï¼š<code>eta = torch.randn(d)</code></li>
</ul>
<p><strong>ç­–ç•¥3ï¼šå‡åŒ€åˆ†å¸ƒ</strong>ï¼ˆ$\eta_i \sim \mathcal{U}[-\sqrt{3}, \sqrt{3}]$ï¼Œä½¿æ–¹å·®ä¸º1ï¼‰</p>
<ul>
<li>ä¼˜ç‚¹ï¼šæœ‰ç•Œï¼Œé¿å…æç«¯å€¼</li>
<li>å®ç°ï¼š<code>eta = (torch.rand(d) - 0.5) * 2 * np.sqrt(3)</code></li>
</ul>
<h4 id="44-pytorch">4.4 PyTorchä¼ªä»£ç å®ç°</h4>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gradient_penalty_sampled</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lambda_weight</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    é‡‡æ ·è¿‘ä¼¼è®¡ç®—æ¢¯åº¦æƒ©ç½š</span>

<span class="sd">    Args:</span>
<span class="sd">        model: PyTorchæ¨¡å‹ï¼Œè¾“å‡ºç»´åº¦ä¸ºd</span>
<span class="sd">        x: è¾“å…¥ï¼Œå½¢çŠ¶ä¸º(batch_size, ...)</span>
<span class="sd">        lambda_weight: æƒé‡ç³»æ•°ï¼ˆå¯¹åº”$\lambda_i$ï¼‰</span>
<span class="sd">        num_samples: Monte Carloé‡‡æ ·æ¬¡æ•°</span>

<span class="sd">    Returns:</span>
<span class="sd">        gp: æ¢¯åº¦æƒ©ç½šé¡¹</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
        <span class="c1"># å‰å‘ä¼ æ’­</span>
        <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># å½¢çŠ¶: (batch_size, d)</span>

        <span class="c1"># é‡‡æ ·éšæœºå‘é‡</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>  <span class="c1"># Rademacher: 2*torch.randint(0,2,f.shape)-1</span>

        <span class="c1"># è®¡ç®—åŠ æƒè¾“å‡º</span>
        <span class="n">weighted_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">eta</span> <span class="o">*</span> <span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># è®¡ç®—æ¢¯åº¦</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
            <span class="n">outputs</span><span class="o">=</span><span class="n">weighted_output</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># æ”¯æŒäºŒé˜¶å¯¼æ•°</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># ç´¯ç§¯æ¢¯åº¦èŒƒæ•°å¹³æ–¹</span>
        <span class="n">gp</span> <span class="o">+=</span> <span class="p">(</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># å¯¹batchå¹³å‡</span>

    <span class="k">return</span> <span class="n">lambda_weight</span> <span class="o">*</span> <span class="n">gp</span> <span class="o">/</span> <span class="n">num_samples</span>
</code></pre></div>

<p><strong>å¤æ‚åº¦åˆ†æ</strong>ï¼š
- å•æ¬¡é‡‡æ ·ï¼š$O(P)$ï¼ˆä¸€æ¬¡åå‘ä¼ æ’­ï¼‰
- æ€»å¤æ‚åº¦ï¼š$O(K \cdot P)$ï¼ˆ$K$ä¸ºé‡‡æ ·æ¬¡æ•°ï¼Œé€šå¸¸$K=1$è¶³å¤Ÿï¼‰
- ç›¸æ¯”ç›´æ¥è®¡ç®—ï¼šä»$O(d \cdot P)$é™è‡³$O(P)$ï¼ˆå½“$d \gg 1$æ—¶æ˜¾è‘—åŠ é€Ÿï¼‰</p>
<hr />
<h3 id="_20">ç¬¬äº”éƒ¨åˆ†ï¼šå¯¹æŠ—è®­ç»ƒçš„æ•°å­¦åŸç†</h3>
<h4 id="51-fgmpgd">5.1 ç›‘ç£å¯¹æŠ—è®­ç»ƒï¼ˆFGM/PGDï¼‰</h4>
<p><strong>ç›®æ ‡</strong>ï¼šå¯»æ‰¾æœ€å…·"æŒ‘æˆ˜æ€§"çš„æ‰°åŠ¨$\varepsilon$ï¼Œä½¿å¾—æ¨¡å‹åœ¨è¯¥æ‰°åŠ¨ä¸‹æŸå¤±æœ€å¤§ã€‚</p>
<p><strong>æ•°å­¦å½¢å¼åŒ–</strong>ï¼š</p>
<p>$$
\min_{\theta} \max_{\|\varepsilon\| \leq \epsilon} l(f_{\theta}(x + \varepsilon), y)
$$</p>
<p>è¿™æ˜¯ä¸€ä¸ª<strong>æå°æå¤§ä¼˜åŒ–é—®é¢˜</strong>ï¼ˆminimax optimizationï¼‰ã€‚</p>
<h4 id="52-fgm">5.2 å¿«é€Ÿæ¢¯åº¦æ³•ï¼ˆFGMï¼‰</h4>
<p><strong>ä¸€é˜¶è¿‘ä¼¼</strong>ï¼š</p>
<p>$$
l(f(x + \varepsilon), y) \approx l(f(x), y) + \varepsilon^{\top} \nabla_x l(f(x), y)
$$</p>
<p><strong>æœ€ä¼˜æ‰°åŠ¨æ–¹å‘</strong>ï¼š</p>
<p>è¦æœ€å¤§åŒ–$\varepsilon^{\top} \nabla_x l$ï¼Œåœ¨çº¦æŸ$|\varepsilon| \leq \epsilon$ä¸‹ï¼Œç”±Cauchy-Schwarzä¸ç­‰å¼ï¼š</p>
<p>$$
\varepsilon^{\top} \nabla_x l \leq \|\varepsilon\| \cdot \|\nabla_x l\| \leq \epsilon \cdot \|\nabla_x l\|
$$</p>
<p>ç­‰å·æˆç«‹å½“ä¸”ä»…å½“$\varepsilon \parallel \nabla_x l$ï¼Œå³ï¼š</p>
<p>$$
\boxed{\varepsilon^* = \epsilon \cdot \frac{\nabla_x l(f(x), y)}{\|\nabla_x l(f(x), y)\|}}
$$</p>
<p><strong>FGMç®—æ³•</strong>ï¼š</p>
<ol>
<li>è®¡ç®—æ¢¯åº¦ï¼š$g = \nabla_x l(f(x), y)$</li>
<li>ç”Ÿæˆæ‰°åŠ¨ï¼š$\varepsilon = \epsilon \cdot g / |g|$</li>
<li>æ›´æ–°æŸå¤±ï¼š$\tilde{l} = l(f(x + \varepsilon), y)$</li>
<li>åå‘ä¼ æ’­ï¼š$\nabla_{\theta} \tilde{l}$</li>
</ol>
<h4 id="53-pgd">5.3 æŠ•å½±æ¢¯åº¦ä¸‹é™ï¼ˆPGDï¼‰</h4>
<p><strong>è¿­ä»£ä¼˜åŒ–</strong>ï¼šFGMåªåšä¸€æ­¥è¿‘ä¼¼ï¼ŒPGDè¿›è¡Œå¤šæ­¥è¿­ä»£ï¼š</p>
<p>$$
\varepsilon_{t+1} = \Pi_{\|\cdot\| \leq \epsilon}\left(\varepsilon_t + \alpha \cdot \frac{\nabla_x l(f(x + \varepsilon_t), y)}{\|\nabla_x l(f(x + \varepsilon_t), y)\|}\right)
$$</p>
<p>å…¶ä¸­$\Pi_{|\cdot| \leq \epsilon}$æ˜¯æŠ•å½±ç®—å­ï¼š</p>
<p>$$
\Pi_{\|\cdot\| \leq \epsilon}(z) = \begin{cases}
z & \text{if } \|z\| \leq \epsilon \\
\epsilon \cdot \frac{z}{\|z\|} & \text{otherwise}
\end{cases}
$$</p>
<p><strong>PGDç®—æ³•</strong>ï¼ˆä»¥$\ell_{\infty}$èŒƒæ•°ä¸ºä¾‹ï¼‰ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">pgd_attack</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">40</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    PGDå¯¹æŠ—æ”»å‡»</span>

<span class="sd">    Args:</span>
<span class="sd">        model: æ¨¡å‹</span>
<span class="sd">        x: åŸå§‹è¾“å…¥</span>
<span class="sd">        y: æ ‡ç­¾</span>
<span class="sd">        epsilon: æœ€å¤§æ‰°åŠ¨å¹…åº¦ï¼ˆL_infèŒƒæ•°ï¼‰</span>
<span class="sd">        alpha: æ¯æ­¥æ­¥é•¿</span>
<span class="sd">        num_iter: è¿­ä»£æ¬¡æ•°</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>  <span class="c1"># éšæœºåˆå§‹åŒ–</span>
    <span class="n">delta</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">delta</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># æ¢¯åº¦ä¸Šå‡ï¼ˆæœ€å¤§åŒ–æŸå¤±ï¼‰</span>
        <span class="n">delta</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">sign</span><span class="p">()</span>

        <span class="c1"># æŠ•å½±åˆ°epsilon-çƒ</span>
        <span class="n">delta</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="o">-</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="n">delta</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">delta</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">x</span>  <span class="c1"># ç¡®ä¿åˆæ³•åƒç´ å€¼</span>

        <span class="n">delta</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">delta</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</code></pre></div>

<h4 id="54">5.4 å¯¹æŠ—è®­ç»ƒä¸æ¢¯åº¦æƒ©ç½šçš„ç­‰ä»·æ€§</h4>
<p><strong>å®šç†</strong>ï¼ˆAdversarial Training â‰ˆ Gradient Penaltyï¼‰ï¼š</p>
<p>åœ¨ä¸€é˜¶è¿‘ä¼¼ä¸‹ï¼Œå¯¹æŠ—è®­ç»ƒç­‰ä»·äºåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æ¢¯åº¦æƒ©ç½šé¡¹ï¼š</p>
<p>$$
\max_{\|\varepsilon\| \leq \epsilon} l(f(x + \varepsilon), y) \approx l(f(x), y) + \epsilon \|\nabla_x l(f(x), y)\|
$$</p>
<p><strong>è¯æ˜</strong>ï¼š</p>
<p>$$
\begin{aligned}
\max_{\|\varepsilon\| \leq \epsilon} l(f(x + \varepsilon), y) &\approx \max_{\|\varepsilon\| \leq \epsilon} \left[l(f(x), y) + \varepsilon^{\top} \nabla_x l\right] \\
&= l(f(x), y) + \max_{\|\varepsilon\| \leq \epsilon} \varepsilon^{\top} \nabla_x l \\
&= l(f(x), y) + \epsilon \|\nabla_x l\|
\end{aligned}
$$</p>
<p><strong>å®è·µæ„ä¹‰</strong>ï¼š
1. å¯¹æŠ—è®­ç»ƒéšå¼åœ°é¼“åŠ±å°æ¢¯åº¦èŒƒæ•°
2. å¯ä»¥ç›´æ¥åœ¨æŸå¤±ä¸­æ·»åŠ $\lambda |\nabla_x l|^2$ä½œä¸ºå¯¹æŠ—è®­ç»ƒçš„æ›¿ä»£ï¼ˆè®¡ç®—æ›´é«˜æ•ˆï¼‰
3. ä¸¤è€…åœ¨æ”¶æ•›ç‚¹å¤„çš„è§£ä¸€è‡´ï¼ˆKKTæ¡ä»¶ï¼‰</p>
<hr />
<h3 id="vat">ç¬¬å…­éƒ¨åˆ†ï¼šè™šæ‹Ÿå¯¹æŠ—è®­ç»ƒï¼ˆVATï¼‰çš„å®Œæ•´ç†è®º</h3>
<h4 id="61-vat">6.1 VATçš„åŠ¨æœºä¸ç›®æ ‡</h4>
<p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼šåœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šï¼Œå¯»æ‰¾ä½¿æ¨¡å‹è¾“å‡ºå˜åŒ–æœ€å¤§çš„æ‰°åŠ¨ã€‚</p>
<p><strong>ç›®æ ‡å‡½æ•°</strong>ï¼š</p>
<p>$$
\max_{\|\varepsilon\| \leq \epsilon} l(f(x + \varepsilon), f(x))
$$</p>
<p>æ³¨æ„ï¼šç¬¬äºŒä¸ªå‚æ•°æ˜¯$f(x)$è€Œé$y$ï¼ˆæ— éœ€æ ‡ç­¾ï¼‰ã€‚</p>
<h4 id="62">6.2 äºŒé˜¶å±•å¼€çš„å¿…è¦æ€§</h4>
<p><strong>ä¸€é˜¶é¡¹æ¶ˆå¤±</strong>ï¼šç”±äºæŸå¤±å‡½æ•°åœ¨$f(x+\varepsilon) = f(x)$å¤„å–æå°å€¼ï¼Œæœ‰ï¼š</p>
<p>$$
\left.\frac{\partial l(f(x+\varepsilon), f(x))}{\partial \varepsilon}\right|_{\varepsilon=0} = 0
$$</p>
<p>å› æ­¤å¿…é¡»å±•å¼€åˆ°äºŒé˜¶ï¼š</p>
<p>$$
l(f(x+\varepsilon), f(x)) \approx \frac{1}{2} \varepsilon^{\top} \mathcal{H}_x \varepsilon
$$</p>
<p>å…¶ä¸­$\mathcal{H}<em ng="ng">x = \nabla_x^2 l(f(x), f</em>$è¡¨ç¤ºstop gradientï¼‰ã€‚}(x))$ï¼ˆ$f_{ng</p>
<h4 id="63">6.3 æœ€å¤§ç‰¹å¾å‘é‡çš„å‡ ä½•æ„ä¹‰</h4>
<p><strong>ä¼˜åŒ–é—®é¢˜</strong>ï¼š</p>
<p>$$
\max_{\|\varepsilon\| = 1} \varepsilon^{\top} \mathcal{H}_x \varepsilon
$$</p>
<p><strong>è§£çš„æ€§è´¨</strong>ï¼šæœ€ä¼˜è§£$\varepsilon^<em>$æ˜¯$\mathcal{H}_x$çš„</em><em>æœ€å¤§ç‰¹å¾å‘é‡</em>*ï¼ˆå¯¹åº”æœ€å¤§ç‰¹å¾å€¼$\lambda_{\max}$ï¼‰ã€‚</p>
<p><strong>è¯æ˜</strong>ï¼ˆæ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ï¼‰ï¼š</p>
<p>æ„é€ æ‹‰æ ¼æœ—æ—¥å‡½æ•°ï¼š</p>
<p>$$
\mathcal{L}(\varepsilon, \mu) = \varepsilon^{\top} \mathcal{H}_x \varepsilon - \mu (\varepsilon^{\top} \varepsilon - 1)
$$</p>
<p>ä¸€é˜¶æ¡ä»¶ï¼š</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial \varepsilon} = 2\mathcal{H}_x \varepsilon - 2\mu \varepsilon = 0 \quad \Rightarrow \quad \mathcal{H}_x \varepsilon = \mu \varepsilon
$$</p>
<p>è¿™æ°å¥½æ˜¯ç‰¹å¾å€¼æ–¹ç¨‹ï¼ç›®æ ‡å‡½æ•°å€¼ä¸ºï¼š</p>
<p>$$
\varepsilon^{\top} \mathcal{H}_x \varepsilon = \mu \varepsilon^{\top} \varepsilon = \mu
$$</p>
<p>å› æ­¤æœ€å¤§å€¼å¯¹åº”æœ€å¤§ç‰¹å¾å€¼$\mu = \lambda_{\max}$ã€‚</p>
<h4 id="64-power-iteration">6.4 å¹‚è¿­ä»£æ³•ï¼ˆPower Iterationï¼‰</h4>
<p><strong>ç®—æ³•åŸç†</strong>ï¼š</p>
<p>è®¾$\mathcal{H}_x$çš„ç‰¹å¾å€¼æ»¡è¶³$|\lambda_1| &gt; |\lambda_2| \geq \cdots \geq |\lambda_n|$ï¼Œå¯¹åº”ç‰¹å¾å‘é‡ä¸º$v_1, \ldots, v_n$ã€‚</p>
<p>ä»»æ„åˆå§‹å‘é‡$u_0 = \sum_i c_i v_i$ï¼Œè¿­ä»£ï¼š</p>
<p>$$
u_{k+1} = \frac{\mathcal{H}_x u_k}{\|\mathcal{H}_x u_k\|}
$$</p>
<p><strong>æ”¶æ•›æ€§åˆ†æ</strong>ï¼š</p>
<p>$$
\mathcal{H}_x^k u_0 = \sum_i c_i \lambda_i^k v_i = \lambda_1^k \left(c_1 v_1 + \sum_{i>1} c_i \left(\frac{\lambda_i}{\lambda_1}\right)^k v_i\right)
$$</p>
<p>å½“$k \to \infty$æ—¶ï¼Œç”±äº$\left|\frac{\lambda_i}{\lambda_1}\right| &lt; 1$ï¼ˆ$i &gt; 1$ï¼‰ï¼Œæœ‰ï¼š</p>
<p>$$
\frac{\mathcal{H}_x^k u_0}{\|\mathcal{H}_x^k u_0\|} \to \pm v_1
$$</p>
<p><strong>æ”¶æ•›é€Ÿç‡</strong>ï¼š$O\left(\left|\frac{\lambda_2}{\lambda_1}\right|^k\right)$ï¼ˆå‡ ä½•æ”¶æ•›ï¼‰ã€‚</p>
<h4 id="65-hessian-">6.5 Hessian-å‘é‡ç§¯çš„é«˜æ•ˆè®¡ç®—</h4>
<p><strong>ç›´æ¥è®¡ç®—å›°éš¾</strong>ï¼šHessiançŸ©é˜µ$\mathcal{H}_x \in \mathbb{R}^{n \times n}$ï¼Œå­˜å‚¨éœ€è¦$O(n^2)$ç©ºé—´ã€‚</p>
<p><strong>æŠ€å·§</strong>ï¼šåªéœ€è®¡ç®—$\mathcal{H}_x u$ï¼ˆHessian-å‘é‡ç§¯ï¼‰ï¼Œæ— éœ€æ˜¾å¼æ„é€ $\mathcal{H}_x$ã€‚</p>
<p><strong>æœ‰é™å·®åˆ†è¿‘ä¼¼</strong>ï¼š</p>
<p>$$
\mathcal{H}_x u = \nabla_x^2 l(f(x), f_{ng}(x)) \cdot u \approx \frac{\nabla_x l(f(x + \xi u), f_{ng}(x)) - \nabla_x l(f(x), f_{ng}(x))}{\xi}
$$</p>
<p>å…¶ä¸­$\xi &gt; 0$æ˜¯å°å¸¸æ•°ï¼ˆå¦‚$10^{-6}$ï¼‰ã€‚</p>
<p><strong>è‡ªåŠ¨å¾®åˆ†å®ç°</strong>ï¼ˆPyTorchï¼‰ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">hessian_vector_product</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    è®¡ç®—Hessian-å‘é‡ç§¯ HÂ·u</span>

<span class="sd">    Args:</span>
<span class="sd">        loss: æŸå¤±å‡½æ•°</span>
<span class="sd">        x: è¾“å…¥</span>
<span class="sd">        u: æ–¹å‘å‘é‡</span>
<span class="sd">        xi: æœ‰é™å·®åˆ†æ­¥é•¿</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># è®¡ç®— grad(loss(x + xi*u))</span>
    <span class="n">grad_plus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">xi</span> <span class="o">*</span> <span class="n">u</span><span class="p">),</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># è¿‘ä¼¼Hessian-å‘é‡ç§¯</span>
    <span class="n">hv</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_plus</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">xi</span>
    <span class="k">return</span> <span class="n">hv</span>
</code></pre></div>

<h4 id="66-vat">6.6 VATå®Œæ•´ç®—æ³•</h4>
<p><strong>è¾“å…¥</strong>ï¼š
- æ¨¡å‹$f_{\theta}$
- è¾“å…¥$x$ï¼ˆæ— æ ‡ç­¾ï¼‰
- æ‰°åŠ¨åŠå¾„$\epsilon$ï¼ˆå¦‚0.03ï¼‰
- å·®åˆ†æ­¥é•¿$\xi$ï¼ˆå¦‚10ï¼‰
- å¹‚è¿­ä»£æ¬¡æ•°$r$ï¼ˆå¦‚1ï¼‰</p>
<p><strong>è¾“å‡º</strong>ï¼šå¯¹æŠ—æ‰°åŠ¨$\varepsilon^*$</p>
<p><strong>æ­¥éª¤</strong>ï¼š</p>
<ol>
<li>
<p><strong>åˆå§‹åŒ–</strong>ï¼š$u \sim \mathcal{N}(0, I)$</p>
</li>
<li>
<p><strong>å¹‚è¿­ä»£</strong>ï¼ˆé‡å¤$r$æ¬¡ï¼‰ï¼š
   $$
   \begin{aligned}
   &u \leftarrow u / \|u\| \\
   &u \leftarrow \nabla_x l(f(x + \xi u), f_{ng}(x)) \\
   &u \leftarrow u / \|u\|
   \end{aligned}
   $$</p>
</li>
<li>
<p><strong>ç¼©æ”¾</strong>ï¼š$\varepsilon^* = \epsilon \cdot u$</p>
</li>
<li>
<p><strong>è®¡ç®—VATæŸå¤±</strong>ï¼š$L_{\text{VAT}} = l(f(x + \varepsilon^*), f_{ng}(x))$</p>
</li>
</ol>
<p><strong>PyTorchå®ç°</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">virtual_adversarial_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    è™šæ‹Ÿå¯¹æŠ—æŸå¤±</span>

<span class="sd">    Args:</span>
<span class="sd">        model: æ¨¡å‹</span>
<span class="sd">        x: è¾“å…¥ï¼ˆæ— æ ‡ç­¾ï¼‰</span>
<span class="sd">        epsilon: æœ€å¤§æ‰°åŠ¨</span>
<span class="sd">        xi: å·®åˆ†æ­¥é•¿</span>
<span class="sd">        num_iter: å¹‚è¿­ä»£æ¬¡æ•°</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># é¢„æµ‹ï¼ˆåœæ­¢æ¢¯åº¦ï¼‰</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">pred_orig</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># éšæœºåˆå§‹åŒ–</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># å¹‚è¿­ä»£</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">d</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># å½’ä¸€åŒ–</span>
        <span class="n">d</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># è®¡ç®—KLæ•£åº¦çš„æ¢¯åº¦</span>
        <span class="n">pred_perturbed</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">xi</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">pred_perturbed</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_orig</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;batchmean&#39;</span>
        <span class="p">)</span>
        <span class="n">kl</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">d</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>  <span class="c1"># æ¢¯åº¦æ–¹å‘</span>

    <span class="c1"># æœ€ç»ˆæ‰°åŠ¨</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">d</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">varepsilon</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">d</span>

    <span class="c1"># VATæŸå¤±</span>
    <span class="n">pred_adv</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">varepsilon</span><span class="p">)</span>
    <span class="n">vat_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span>
        <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">pred_adv</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_orig</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;batchmean&#39;</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">vat_loss</span>
</code></pre></div>

<h4 id="67-vat">6.7 VATçš„ç†è®ºæ€§è´¨</h4>
<p><strong>æ€§è´¨1ï¼ˆå¹³æ»‘æ€§æƒ©ç½šï¼‰</strong>ï¼šVATé¼“åŠ±æ¨¡å‹åœ¨è¾“å…¥é™„è¿‘ä¿æŒå¹³æ»‘ï¼š</p>
<p>$$
\|f(x + \varepsilon) - f(x)\| \leq C \quad \forall \|\varepsilon\| \leq \epsilon
$$</p>
<p><strong>æ€§è´¨2ï¼ˆLipschitzçº¦æŸï¼‰</strong>ï¼šVATéšå¼åœ°é™åˆ¶äº†æ¨¡å‹çš„Lipschitzå¸¸æ•°ï¼š</p>
<p>$$
\text{Lip}(f) := \sup_{x \neq x'} \frac{\|f(x) - f(x')\|}{\|x - x'\|} \leq C
$$</p>
<p><strong>æ€§è´¨3ï¼ˆæµå½¢å‡è®¾ï¼‰</strong>ï¼šVATå‡è®¾æ•°æ®åˆ†å¸ƒåœ¨ä½ç»´æµå½¢ä¸Šï¼Œæ²¿æµå½¢æ–¹å‘å¹³æ»‘ã€‚</p>
<p><strong>å®šç†</strong>ï¼ˆVATçš„æ³›åŒ–ç•Œï¼‰ï¼š</p>
<p>å‡è®¾æ•°æ®åˆ†å¸ƒ$p(x)$æ»¡è¶³$\epsilon$-æµå½¢å‡è®¾ï¼Œåˆ™VATè®­ç»ƒçš„æ¨¡å‹æ³›åŒ–è¯¯å·®æ»¡è¶³ï¼š</p>
<p>$$
\mathbb{E}_{(x,y)\sim p_{\text{test}}}[l(f(x), y)] \leq \mathbb{E}_{(x,y)\sim p_{\text{train}}}[l(f(x), y)] + O\left(\frac{\text{Lip}(f)^2 \epsilon^2}{\sqrt{n}}\right)
$$</p>
<p>å…¶ä¸­$n$æ˜¯è®­ç»ƒæ ·æœ¬æ•°ã€‚</p>
<hr />
<h3 id="_21">ç¬¬ä¸ƒéƒ¨åˆ†ï¼šåŠç›‘ç£å­¦ä¹ æ¡†æ¶</h3>
<h4 id="71">7.1 åŠç›‘ç£ç›®æ ‡å‡½æ•°</h4>
<p><strong>è”åˆæŸå¤±</strong>ï¼š</p>
<p>$$
L_{\text{total}} = \underbrace{L_{\text{sup}}}_{\text{ç›‘ç£æŸå¤±}} + \alpha \underbrace{L_{\text{VAT}}}_{\text{è™šæ‹Ÿå¯¹æŠ—æŸå¤±}} + \beta \underbrace{L_{\text{ent}}}_{\text{ç†µæ­£åˆ™åŒ–}}
$$</p>
<p>å…¶ä¸­ï¼š</p>
<p>$$
\begin{aligned}
L_{\text{sup}} &= \mathbb{E}_{(x,y)\sim \mathcal{D}_L}[l(f(x), y)] \\
L_{\text{VAT}} &= \mathbb{E}_{x \sim \mathcal{D}_U}[\max_{\|\varepsilon\| \leq \epsilon} l(f(x+\varepsilon), f(x))] \\
L_{\text{ent}} &= -\mathbb{E}_{x \sim \mathcal{D}_U}[\sum_i f_i(x) \log f_i(x)]
\end{aligned}
$$</p>
<p><strong>ç†µæ­£åˆ™åŒ–çš„ä½œç”¨</strong>ï¼š
- é¼“åŠ±æ¨¡å‹åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šåšå‡º"è‡ªä¿¡"çš„é¢„æµ‹ï¼ˆä½ç†µï¼‰
- é¿å…æ¨¡å‹è¾“å‡ºè¶‹äºå‡åŒ€åˆ†å¸ƒï¼ˆé«˜ç†µï¼‰
- ä¸VATé…åˆï¼šVATä¿è¯å¹³æ»‘æ€§ï¼Œç†µæƒ©ç½šä¿è¯å†³ç­–è¾¹ç•Œæ¸…æ™°</p>
<h4 id="72">7.2 è®­ç»ƒç­–ç•¥</h4>
<p><strong>ç­–ç•¥1ï¼šäº¤æ›¿è®­ç»ƒ</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="c1"># ç›‘ç£è®­ç»ƒ</span>
    <span class="k">for</span> <span class="n">x_l</span><span class="p">,</span> <span class="n">y_l</span> <span class="ow">in</span> <span class="n">labeled_loader</span><span class="p">:</span>
        <span class="n">loss_sup</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x_l</span><span class="p">),</span> <span class="n">y_l</span><span class="p">)</span>
        <span class="n">loss_sup</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># VATè®­ç»ƒ</span>
    <span class="k">for</span> <span class="n">x_u</span> <span class="ow">in</span> <span class="n">unlabeled_loader</span><span class="p">:</span>
        <span class="n">loss_vat</span> <span class="o">=</span> <span class="n">virtual_adversarial_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_u</span><span class="p">)</span>
        <span class="n">loss_vat</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<p><strong>ç­–ç•¥2ï¼šè”åˆè®­ç»ƒ</strong>ï¼ˆæ¨èï¼‰</p>
<div class="codehilite"><pre><span></span><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">labeled_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">labeled_loader</span><span class="p">)</span>
    <span class="n">unlabeled_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">unlabeled_loader</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
        <span class="c1"># æœ‰æ ‡ç­¾æ‰¹æ¬¡</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">x_l</span><span class="p">,</span> <span class="n">y_l</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">labeled_iter</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="n">labeled_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">labeled_loader</span><span class="p">)</span>
            <span class="n">x_l</span><span class="p">,</span> <span class="n">y_l</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">labeled_iter</span><span class="p">)</span>

        <span class="c1"># æ— æ ‡ç­¾æ‰¹æ¬¡</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">x_u</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">unlabeled_iter</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="n">unlabeled_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">unlabeled_loader</span><span class="p">)</span>
            <span class="n">x_u</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">unlabeled_iter</span><span class="p">)</span>

        <span class="c1"># è”åˆæŸå¤±</span>
        <span class="n">loss_sup</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x_l</span><span class="p">),</span> <span class="n">y_l</span><span class="p">)</span>
        <span class="n">loss_vat</span> <span class="o">=</span> <span class="n">virtual_adversarial_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_u</span><span class="p">)</span>
        <span class="n">loss_ent</span> <span class="o">=</span> <span class="n">entropy_loss</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x_u</span><span class="p">))</span>

        <span class="n">loss_total</span> <span class="o">=</span> <span class="n">loss_sup</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">loss_vat</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">loss_ent</span>
        <span class="n">loss_total</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>

<h4 id="73">7.3 è¶…å‚æ•°é€‰æ‹©æŒ‡å—</h4>
<p><strong>æ‰°åŠ¨åŠå¾„$\epsilon$</strong>ï¼š
- å›¾åƒåˆ†ç±»ï¼š0.01 ~ 0.1ï¼ˆå½’ä¸€åŒ–åˆ°$[0,1]$ï¼‰
- NLPï¼ˆembeddingï¼‰ï¼š0.5 ~ 5.0
- åŸåˆ™ï¼šä½¿$f(x+\varepsilon)$ä¸$f(x)$åˆšå¥½å¯åŒºåˆ†</p>
<p><strong>æƒé‡$\alpha, \beta$</strong>ï¼š
- æ ‡ç­¾ç¨€ç¼ºï¼ˆ&lt;1%ï¼‰ï¼š$\alpha = 1.0, \beta = 0.1$
- ä¸­ç­‰æ ‡ç­¾ï¼ˆ1%-10%ï¼‰ï¼š$\alpha = 0.3, \beta = 0.01$
- å……è¶³æ ‡ç­¾ï¼ˆ&gt;10%ï¼‰ï¼š$\alpha = 0.1, \beta = 0.001$</p>
<p><strong>å¹‚è¿­ä»£æ¬¡æ•°$r$</strong>ï¼š
- $r = 0$ï¼šé€€åŒ–ä¸ºé«˜æ–¯å™ªå£°
- $r = 1$ï¼šé€šå¸¸è¶³å¤Ÿï¼Œè®¡ç®—é«˜æ•ˆ
- $r &gt; 1$ï¼šç†è®ºæ›´ä¼˜ï¼Œä½†è®¡ç®—ä»£ä»·é«˜</p>
<hr />
<h3 id="_22">ç¬¬å…«éƒ¨åˆ†ï¼šå®éªŒéªŒè¯ä¸æ¡ˆä¾‹åˆ†æ</h3>
<h4 id="81-cifar-10">8.1 CIFAR-10åŠç›‘ç£å®éªŒ</h4>
<p><strong>å®éªŒè®¾ç½®</strong>ï¼š
- æ•°æ®é›†ï¼šCIFAR-10ï¼ˆ50kè®­ç»ƒï¼Œ10kæµ‹è¯•ï¼‰
- æ ‡ç­¾æ•°ï¼š250/500/1000/4000
- æ¨¡å‹ï¼šWideResNet-28-2
- Baselineï¼šä»…ç›‘ç£å­¦ä¹ </p>
<p><strong>ç»“æœ</strong>ï¼ˆé”™è¯¯ç‡%ï¼‰ï¼š</p>
<table>
<thead>
<tr>
<th>æ ‡ç­¾æ•°</th>
<th>ç›‘ç£</th>
<th>+VAT</th>
<th>+VAT+ç†µ</th>
<th>æ”¹è¿›</th>
</tr>
</thead>
<tbody>
<tr>
<td>250</td>
<td>46.43</td>
<td>26.12</td>
<td>24.63</td>
<td>-21.8%</td>
</tr>
<tr>
<td>500</td>
<td>33.94</td>
<td>18.68</td>
<td>17.09</td>
<td>-16.85%</td>
</tr>
<tr>
<td>1000</td>
<td>23.57</td>
<td>13.86</td>
<td>12.36</td>
<td>-11.21%</td>
</tr>
<tr>
<td>4000</td>
<td>13.13</td>
<td>9.22</td>
<td>8.11</td>
<td>-5.02%</td>
</tr>
</tbody>
</table>
<p><strong>è§‚å¯Ÿ</strong>ï¼š
1. æ ‡ç­¾è¶Šå°‘ï¼ŒVATæ”¹è¿›è¶Šæ˜¾è‘—ï¼ˆæœ€é«˜-21.8%ï¼‰
2. ç†µæ­£åˆ™åŒ–è¿›ä¸€æ­¥é™ä½1-2%é”™è¯¯ç‡
3. å½“æ ‡ç­¾å……è¶³æ—¶ï¼Œæ”¹è¿›è¶‹äºé¥±å’Œ</p>
<h4 id="82-imdb">8.2 æ–‡æœ¬åˆ†ç±»å®éªŒï¼ˆIMDBï¼‰</h4>
<p><strong>å®éªŒè®¾ç½®</strong>ï¼š
- æ•°æ®é›†ï¼šIMDBæƒ…æ„Ÿåˆ†ç±»ï¼ˆ25kè®­ç»ƒï¼‰
- æ ‡ç­¾æ•°ï¼š200ï¼ˆä»…0.8%ï¼‰
- æ¨¡å‹ï¼šBERT-base + VATï¼ˆåœ¨embeddingå±‚æ·»åŠ æ‰°åŠ¨ï¼‰
- Baselineï¼š200æ ‡ç­¾ç›‘ç£å­¦ä¹ </p>
<p><strong>ç»“æœ</strong>ï¼ˆå‡†ç¡®ç‡%ï¼‰ï¼š</p>
<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>éªŒè¯é›†</th>
<th>æµ‹è¯•é›†</th>
</tr>
</thead>
<tbody>
<tr>
<td>ç›‘ç£</td>
<td>88.93</td>
<td>89.34</td>
</tr>
<tr>
<td>+VAT</td>
<td>89.83</td>
<td>90.37</td>
</tr>
<tr>
<td>æ”¹è¿›</td>
<td>+0.90</td>
<td>+1.03</td>
</tr>
</tbody>
</table>
<p><strong>å®ç°ç»†èŠ‚</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># åœ¨embeddingå±‚æ·»åŠ VAT</span>
<span class="k">def</span><span class="w"> </span><span class="nf">vat_embedding</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">5.0</span><span class="p">):</span>
    <span class="c1"># è·å–embedding</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_input_embeddings</span><span class="p">()(</span><span class="n">input_ids</span><span class="p">)</span>  <span class="c1"># (batch, seq_len, dim)</span>

    <span class="c1"># VATæ‰°åŠ¨</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">d</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># å½’ä¸€åŒ–</span>

    <span class="c1"># è®¡ç®—æ¢¯åº¦ï¼ˆå¹‚è¿­ä»£1æ¬¡ï¼‰</span>
    <span class="n">d</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">embeddings_perturbed</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">+</span> <span class="mf">10.0</span> <span class="o">*</span> <span class="n">d</span>
    <span class="n">pred_perturbed</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">embeddings_perturbed</span><span class="p">)</span>
    <span class="n">pred_orig</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">embeddings</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>

    <span class="n">kl</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span>
        <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">pred_perturbed</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_orig</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;batchmean&#39;</span>
    <span class="p">)</span>
    <span class="n">kl</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># æœ€ç»ˆæ‰°åŠ¨</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">d</span><span class="o">.</span><span class="n">grad</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs_embeds</span><span class="o">=</span><span class="n">embeddings</span> <span class="o">+</span> <span class="n">d</span><span class="p">)</span>
</code></pre></div>

<h4 id="83-chestx-ray14">8.3 åŒ»ç–—å›¾åƒåˆ†ç±»ï¼ˆChestX-ray14ï¼‰</h4>
<p><strong>å®éªŒè®¾ç½®</strong>ï¼š
- ä»»åŠ¡ï¼š14ç§èƒ¸éƒ¨ç–¾ç—…å¤šæ ‡ç­¾åˆ†ç±»
- æ ‡ç­¾æ•°ï¼š1000æ ‡æ³¨ï¼ˆæ€»æ•°112kï¼‰
- æ¨¡å‹ï¼šResNet-50
- è¯„ä¼°æŒ‡æ ‡ï¼šAUCï¼ˆå¹³å‡ï¼‰</p>
<p><strong>ç»“æœ</strong>ï¼š</p>
<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>AUC</th>
<th>æ”¹è¿›</th>
</tr>
</thead>
<tbody>
<tr>
<td>ç›‘ç£</td>
<td>0.723</td>
<td>-</td>
</tr>
<tr>
<td>+æ•°æ®å¢å¼º</td>
<td>0.741</td>
<td>+0.018</td>
</tr>
<tr>
<td>+VAT</td>
<td>0.768</td>
<td>+0.045</td>
</tr>
<tr>
<td>+VAT+ç†µ</td>
<td>0.779</td>
<td>+0.056</td>
</tr>
</tbody>
</table>
<p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼š
- åŒ»ç–—å›¾åƒé¢†åŸŸæ ‡æ³¨æ˜‚è´µï¼ŒVATæ•ˆæœæ˜¾è‘—
- VATä¸ä¼ ç»Ÿæ•°æ®å¢å¼ºäº’è¡¥ï¼ˆè”åˆä½¿ç”¨æ›´ä½³ï¼‰
- å¤šæ ‡ç­¾è®¾ç½®ä¸‹ï¼Œç†µæ­£åˆ™åŒ–éœ€ä¿®æ”¹ä¸ºæ¯ç±»ç‹¬ç«‹è®¡ç®—</p>
<hr />
<h3 id="_23">ç¬¬ä¹éƒ¨åˆ†ï¼šç†è®ºæ‰©å±•ä¸å‰æ²¿ç ”ç©¶</h3>
<h4 id="91-vat">9.1 VATçš„ä¿¡æ¯è®ºè§£é‡Š</h4>
<p><strong>äº’ä¿¡æ¯è§†è§’</strong>ï¼š</p>
<p>VATæœ€å°åŒ–è¾“å…¥æ‰°åŠ¨ä¸è¾“å‡ºå˜åŒ–çš„äº’ä¿¡æ¯ï¼š</p>
<p>$$
I(X+\varepsilon; f(X)) \leq I(X; f(X)) + \text{VAT regularization}
$$</p>
<p><strong>è¯æ˜æ€è·¯</strong>ï¼š</p>
<p>åˆ©ç”¨KLæ•£åº¦çš„æ•°æ®å¤„ç†ä¸ç­‰å¼ï¼ˆData Processing Inequalityï¼‰ï¼š</p>
<p>$$
I(X; Y) \geq I(X+\varepsilon; Y)
$$</p>
<p>VATé€šè¿‡æƒ©ç½š$D_{KL}(f(x+\varepsilon) | f(x))$ï¼Œé™åˆ¶äº†æ‰°åŠ¨å¼•å…¥çš„ä¿¡æ¯æŸå¤±ã€‚</p>
<h4 id="92-manifold-tangent-classifier">9.2 ä¸Manifold Tangent Classifierçš„è”ç³»</h4>
<p><strong>åˆ‡ç©ºé—´æ­£åˆ™åŒ–</strong>ï¼š</p>
<p>å‡è®¾æ•°æ®åˆ†å¸ƒåœ¨ä½ç»´æµå½¢$\mathcal{M}$ä¸Šï¼ŒVATçš„å¯¹æŠ—æ–¹å‘$\varepsilon^*$è¿‘ä¼¼äºæµå½¢çš„åˆ‡ç©ºé—´æ–¹å‘ã€‚</p>
<p><strong>å®šç†</strong>ï¼ˆManifold Tangent Propertyï¼‰ï¼š</p>
<p>è®¾$\mathcal{M}$æ˜¯å…‰æ»‘æµå½¢ï¼Œ$T_x \mathcal{M}$æ˜¯åœ¨ç‚¹$x$çš„åˆ‡ç©ºé—´ã€‚åˆ™VATçš„å¯¹æŠ—æ‰°åŠ¨æ»¡è¶³ï¼š</p>
<p>$$
\varepsilon^* \approx \arg\max_{v \in T_x \mathcal{M}, \|v\|=1} v^{\top} \mathcal{H}_x v
$$</p>
<p>è¿™æ„å‘³ç€VATæ‰¾åˆ°çš„æ˜¯"æ²¿æµå½¢æ–¹å‘ä¸Šæ¨¡å‹æœ€æ•æ„Ÿçš„æ–¹å‘"ã€‚</p>
<h4 id="93-consistency-regularization">9.3 Consistency Regularizationç»Ÿä¸€æ¡†æ¶</h4>
<p><strong>ä¸€è‡´æ€§æ­£åˆ™åŒ–å®¶æ—</strong>ï¼š</p>
<table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>æ‰°åŠ¨ç±»å‹</th>
<th>æŸå¤±å‡½æ•°</th>
</tr>
</thead>
<tbody>
<tr>
<td>VAT</td>
<td>å¯¹æŠ—æ‰°åŠ¨ï¼ˆäºŒé˜¶ä¼˜åŒ–ï¼‰</td>
<td>$D_{KL}(f(x+\varepsilon^*) | f(x))$</td>
</tr>
<tr>
<td>$\Pi$-Model</td>
<td>éšæœºDropout</td>
<td>$|f(x; \theta_1) - f(x; \theta_2)|^2$</td>
</tr>
<tr>
<td>Temporal Ensembling</td>
<td>EMAé¢„æµ‹</td>
<td>$|f(x) - \tilde{f}(x)|^2$</td>
</tr>
<tr>
<td>Mean Teacher</td>
<td>æ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œ</td>
<td>$|f_{\text{student}}(x) - f_{\text{teacher}}(x)|^2$</td>
</tr>
</tbody>
</table>
<p><strong>ç»Ÿä¸€å½¢å¼</strong>ï¼š</p>
<p>$$
L_{\text{consistency}} = \mathbb{E}_{x, \mathcal{T}_1, \mathcal{T}_2}[d(f(\mathcal{T}_1(x)), f(\mathcal{T}_2(x)))]
$$</p>
<p>å…¶ä¸­$\mathcal{T}$æ˜¯å˜æ¢ï¼ˆæ‰°åŠ¨ã€Dropoutã€å‚æ•°æ‰°åŠ¨ç­‰ï¼‰ï¼Œ$d$æ˜¯è·ç¦»åº¦é‡ã€‚</p>
<h4 id="94-sharpness-aware-minimization-sam">9.4 Sharpness-Aware Minimization (SAM)</h4>
<p><strong>SAMç›®æ ‡</strong>ï¼š</p>
<p>$$
\min_{\theta} \max_{\|\varepsilon\| \leq \rho} L(\theta + \varepsilon)
$$</p>
<p>è¿™æ˜¯åœ¨å‚æ•°ç©ºé—´ï¼ˆè€Œéè¾“å…¥ç©ºé—´ï¼‰åšå¯¹æŠ—è®­ç»ƒã€‚</p>
<p><strong>ä¸VATçš„è”ç³»</strong>ï¼š</p>
<ul>
<li>VATï¼šè¾“å…¥ç©ºé—´å¹³æ»‘æ€§ â†’ æ³›åŒ–</li>
<li>SAMï¼šå‚æ•°ç©ºé—´å¹³æ»‘æ€§ï¼ˆå¹³å¦æœ€å°å€¼ï¼‰â†’ æ³›åŒ–</li>
</ul>
<p><strong>è¯æ˜å¹³å¦æ€§çš„æ³›åŒ–ä¼˜åŠ¿</strong>ï¼ˆPAC-Bayesç•Œï¼‰ï¼š</p>
<p>$$
\mathbb{E}_{p_{\text{test}}}[l] \leq \mathbb{E}_{p_{\text{train}}}[l] + O\left(\frac{\text{tr}(\mathcal{H}_{\theta})}{\sqrt{n}}\right)
$$</p>
<p>å…¶ä¸­$\mathcal{H}<em _theta="\theta">{\theta} = \nabla</em>^2 L(\theta)$æ˜¯å‚æ•°ç©ºé—´çš„Hessianã€‚</p>
<hr />
<h3 id="_24">ç¬¬åéƒ¨åˆ†ï¼šå®è·µå»ºè®®ä¸è°ƒè¯•æŠ€å·§</h3>
<h4 id="101">10.1 å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ</h4>
<p><strong>é—®é¢˜1ï¼šVATæŸå¤±NaN</strong></p>
<p><strong>åŸå› </strong>ï¼š
- KLæ•£åº¦çš„log-softmaxé¡¹æ•°å€¼ä¸ç¨³å®š
- æ‰°åŠ¨$\varepsilon$è¿‡å¤§å¯¼è‡´è¾“å‡ºåˆ†å¸ƒå´©å¡Œ</p>
<p><strong>è§£å†³</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨ç¨³å®šçš„KLæ•£åº¦è®¡ç®—</span>
<span class="n">kl</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span>
    <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">pred_perturbed</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_orig</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># detaché¿å…æ¢¯åº¦æµå‘pred_orig</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;batchmean&#39;</span>
<span class="p">)</span>

<span class="c1"># é™åˆ¶epsilon</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div>

<p><strong>é—®é¢˜2ï¼šVATä¸æ”¶æ•›</strong></p>
<p><strong>åŸå› </strong>ï¼š
- $\alpha$ï¼ˆVATæƒé‡ï¼‰è¿‡å¤§ï¼Œç›‘ç£ä¿¡å·æ·¹æ²¡
- å¹‚è¿­ä»£æ¬¡æ•°ä¸è¶³ï¼Œå¯¹æŠ—æ–¹å‘ä¸å‡†ç¡®</p>
<p><strong>è§£å†³</strong>ï¼š
- é€æ­¥å¢å¤§$\alpha$ï¼šä»0.01å¼€å§‹ï¼Œæ¯5ä¸ªepochç¿»å€è‡³ç›®æ ‡å€¼
- å¢åŠ å¹‚è¿­ä»£æ¬¡æ•°åˆ°$r=2$æˆ–$r=3$</p>
<p><strong>é—®é¢˜3ï¼šè®¡ç®—é€Ÿåº¦æ…¢</strong></p>
<p><strong>åŸå› </strong>ï¼š
- æ¯æ­¥éœ€è¦é¢å¤–2-3æ¬¡å‰å‘ä¼ æ’­
- æ¢¯åº¦è®¡ç®—å¼€é”€å¤§</p>
<p><strong>ä¼˜åŒ–</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
    <span class="n">loss_sup</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x_l</span><span class="p">),</span> <span class="n">y_l</span><span class="p">)</span>
    <span class="n">loss_vat</span> <span class="o">=</span> <span class="n">virtual_adversarial_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_u</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_sup</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">loss_vat</span>

<span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>

<h4 id="102">10.2 ä¸åŒé¢†åŸŸçš„æœ€ä½³å®è·µ</h4>
<p><strong>è®¡ç®—æœºè§†è§‰</strong>ï¼š
- $\epsilon \in [0.01, 0.1]$ï¼ˆåƒç´ å½’ä¸€åŒ–åˆ°$[0,1]$ï¼‰
- ä½¿ç”¨$\ell_2$æˆ–$\ell_{\infty}$èŒƒæ•°
- ä¸æ•°æ®å¢å¼ºï¼ˆéšæœºè£å‰ªã€ç¿»è½¬ï¼‰è”åˆä½¿ç”¨</p>
<p><strong>è‡ªç„¶è¯­è¨€å¤„ç†</strong>ï¼š
- $\epsilon \in [0.5, 5.0]$ï¼ˆembeddingå±‚ï¼‰
- ä»…åœ¨embeddingå±‚æ·»åŠ æ‰°åŠ¨ï¼ˆé¿å…ç ´åtokenç¦»æ•£æ€§ï¼‰
- é…åˆæ ‡ç­¾å¹³æ»‘ï¼ˆlabel smoothingï¼‰</p>
<p><strong>å›¾ç¥ç»ç½‘ç»œ</strong>ï¼š
- åœ¨èŠ‚ç‚¹ç‰¹å¾å’Œé‚»æ¥çŸ©é˜µä¸ŠåŒæ—¶æ·»åŠ æ‰°åŠ¨
- $\epsilon_{\text{feat}} \in [0.01, 0.1]$ï¼Œ$\epsilon_{\text{adj}} \in [0.001, 0.01]$ï¼ˆé‚»æ¥çŸ©é˜µæ›´æ•æ„Ÿï¼‰
- ä½¿ç”¨è°±å½’ä¸€åŒ–ï¼ˆSpectral Normalizationï¼‰ç¨³å®šè®­ç»ƒ</p>
<h4 id="103">10.3 å¯è§†åŒ–åˆ†æ</h4>
<p><strong>æ¢¯åº¦èŒƒæ•°ç›‘æ§</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">grad_norms</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">grad_norms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">grad_norms</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Gradient Norm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distribution of Input Gradient Norms&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><strong>å¯¹æŠ—æ–¹å‘å¯è§†åŒ–</strong>ï¼ˆå›¾åƒï¼‰ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># åŸå§‹å›¾åƒ</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># VATæ‰°åŠ¨</span>
<span class="n">vat_eps</span> <span class="o">=</span> <span class="n">virtual_adversarial_perturbation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span>
<span class="n">vat_eps_vis</span> <span class="o">=</span> <span class="n">vat_eps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># å¯è§†åŒ–</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">vat_eps_vis</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># æ”¾å¤§æ‰°åŠ¨ä»¥ä¾¿è§‚å¯Ÿ</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;VAT Perturbation (Ã—100)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">img</span> <span class="o">+</span> <span class="n">vat_eps_vis</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Adversarial Example&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<hr />
<h3 id="_25">ç¬¬åä¸€éƒ¨åˆ†ï¼šå“²å­¦æ€è€ƒä¸æœªæ¥æ–¹å‘</h3>
<h4 id="111">11.1 æ³›åŒ–æ€§çš„æœ¬è´¨</h4>
<p><strong>Occam's RazoråŸåˆ™</strong>ï¼šåœ¨è§£é‡Šæ•°æ®çš„æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œé€‰æ‹©"æœ€ç®€å•"çš„ã€‚</p>
<p>VATçš„ä¸‰ç§"ç®€å•æ€§"è¯ é‡Šï¼š
1. <strong>è¾“å…¥ç®€å•æ€§</strong>ï¼šå¯¹è¾“å…¥æ‰°åŠ¨ä¸æ•æ„Ÿï¼ˆå¹³æ»‘æ€§ï¼‰
2. <strong>å‚æ•°ç®€å•æ€§</strong>ï¼šHessianè¿¹å°ï¼ˆå¹³å¦æ€§ï¼‰
3. <strong>å†³ç­–è¾¹ç•Œç®€å•æ€§</strong>ï¼šä½ç»´æµå½¢ä¸Šå†³ç­–è¾¹ç•Œå¹³æ»‘</p>
<p><strong>ä¿¡æ¯ç“¶é¢ˆç†è®º</strong>ï¼ˆTishbyï¼‰ï¼š</p>
<p>å¥½çš„è¡¨ç¤ºæ»¡è¶³ï¼š
$$
\min I(X; Z) \quad \text{s.t.} \quad I(Y; Z) \geq I_{\min}
$$</p>
<p>VATé€šè¿‡é™åˆ¶$I(X+\varepsilon; f(X))$ï¼Œéšå¼åœ°å‹ç¼©äº†è¡¨ç¤º$Z = f(X)$ã€‚</p>
<h4 id="112">11.2 æœªæ¥ç ”ç©¶æ–¹å‘</h4>
<p><strong>æ–¹å‘1ï¼šè‡ªé€‚åº”VAT</strong>
- é—®é¢˜ï¼šå›ºå®š$\epsilon$å¯¹æ‰€æœ‰æ ·æœ¬ä¸å¤Ÿçµæ´»
- æ–¹æ¡ˆï¼šå­¦ä¹ æ ·æœ¬ä¾èµ–çš„æ‰°åŠ¨åŠå¾„$\epsilon(x)$
- æŒ‘æˆ˜ï¼šå¦‚ä½•é¿å…$\epsilon(x) \to 0$ï¼ˆtrivial solutionï¼‰</p>
<p><strong>æ–¹å‘2ï¼šé«˜é˜¶VAT</strong>
- é—®é¢˜ï¼šäºŒé˜¶å±•å¼€å¯èƒ½ä¸è¶³ï¼ˆé«˜åº¦éçº¿æ€§æ¨¡å‹ï¼‰
- æ–¹æ¡ˆï¼šè€ƒè™‘ä¸‰é˜¶æˆ–å››é˜¶Taylorå±•å¼€
- æŒ‘æˆ˜ï¼šè®¡ç®—å¤æ‚åº¦æŒ‡æ•°å¢é•¿</p>
<p><strong>æ–¹å‘3ï¼šå¤šæ¨¡æ€VAT</strong>
- é—®é¢˜ï¼šå›¾åƒ+æ–‡æœ¬çš„è”åˆæ‰°åŠ¨
- æ–¹æ¡ˆï¼šåœ¨ä¸åŒæ¨¡æ€çš„å…±äº«è¡¨ç¤ºç©ºé—´ä¸­æ·»åŠ æ‰°åŠ¨
- åº”ç”¨ï¼šVQAã€å›¾åƒæè¿°ç”Ÿæˆ</p>
<p><strong>æ–¹å‘4ï¼šå› æœVAT</strong>
- é—®é¢˜ï¼šVATå…³æ³¨ç›¸å…³æ€§ï¼Œå¯èƒ½å­¦åˆ°è™šå‡ç›¸å…³
- æ–¹æ¡ˆï¼šç»“åˆå› æœæ¨æ–­ï¼Œä»…åœ¨å› æœç‰¹å¾ä¸Šæ·»åŠ æ‰°åŠ¨
- ç†è®ºï¼šåäº‹å®æ¨ç†ï¼ˆCounterfactual Reasoningï¼‰</p>
<h4 id="113">11.3 ä¸å…¶ä»–é¢†åŸŸçš„äº¤å‰</h4>
<p><strong>é²æ£’ä¼˜åŒ–</strong>ï¼š</p>
<p>$$
\min_{\theta} \max_{\varepsilon \in \mathcal{U}} \mathbb{E}[l(f_{\theta}(x+\varepsilon), y)]
$$</p>
<p>è¿™æ˜¯åˆ†å¸ƒé²æ£’ä¼˜åŒ–ï¼ˆDROï¼‰çš„ç‰¹ä¾‹ï¼Œ$\mathcal{U}$æ˜¯$\epsilon$-çƒå†…çš„åˆ†å¸ƒé›†åˆã€‚</p>
<p><strong>æœ€ä¼˜ä¼ è¾“</strong>ï¼š</p>
<p>VATå¯è§†ä¸ºåœ¨Wassersteinè·ç¦»çº¦æŸä¸‹çš„æœ€å°æœ€å¤§ä¼˜åŒ–ï¼š</p>
<p>$$
\min_{\theta} \max_{q: W_2(p, q) \leq \epsilon} \mathbb{E}_{x \sim q}[l(f_{\theta}(x), y)]
$$</p>
<p>å…¶ä¸­$W_2$æ˜¯2-Wassersteinè·ç¦»ã€‚</p>
<p><strong>ç¥ç»å¾®åˆ†æ–¹ç¨‹ï¼ˆNeural ODEï¼‰</strong>ï¼š</p>
<p>è¿ç»­æ—¶é—´è§†è§’ä¸‹ï¼ŒVATç­‰ä»·äºï¼š</p>
<p>$$
\frac{dx}{dt} = \arg\max_{\|v\| \leq 1} v^{\top} \nabla_x l(f(x), y)
$$</p>
<p>è¿™æ˜¯ä¸€ä¸ª"æœ€é™¡ä¸Šå‡"çš„ODEæµã€‚</p>
<hr />
<h3 id="_26">æ€»ç»“</h3>
<p>æœ¬èŠ‚å¯¹æ³›åŒ–æ€§ç†è®ºè¿›è¡Œäº†å…¨é¢çš„æ•°å­¦æ¨å¯¼ï¼š</p>
<ol>
<li><strong>å™ªå£°æ­£åˆ™åŒ–</strong>ï¼šä»éšæœºæ‰°åŠ¨åˆ°æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–ï¼ˆäºŒé˜¶å¯¼æ•°æƒ©ç½šï¼‰</li>
<li><strong>æ¢¯åº¦æƒ©ç½š</strong>ï¼šé€šè¿‡æ³°å‹’å±•å¼€å’Œé«˜æ–¯ç§¯åˆ†ï¼Œæ¨å¯¼å‡ºæ¢¯åº¦èŒƒæ•°æƒ©ç½š</li>
<li><strong>å¯¹æŠ—è®­ç»ƒ</strong>ï¼šFGM/PGDç®—æ³•åŠå…¶ä¸æ¢¯åº¦æƒ©ç½šçš„ç­‰ä»·æ€§</li>
<li><strong>è™šæ‹Ÿå¯¹æŠ—è®­ç»ƒ</strong>ï¼šäºŒé˜¶ä¼˜åŒ–ã€å¹‚è¿­ä»£æ³•ã€Hessian-å‘é‡ç§¯è®¡ç®—</li>
<li><strong>åŠç›‘ç£å­¦ä¹ </strong>ï¼šVATåœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šçš„åº”ç”¨</li>
<li><strong>å®éªŒä¸å®è·µ</strong>ï¼šCIFAR-10/IMDBæ¡ˆä¾‹ã€è°ƒè¯•æŠ€å·§ã€å¯è§†åŒ–åˆ†æ</li>
<li><strong>ç†è®ºæ‰©å±•</strong>ï¼šä¿¡æ¯è®ºã€æµå½¢å‡è®¾ã€ä¸€è‡´æ€§æ­£åˆ™åŒ–ã€SAM</li>
<li><strong>æœªæ¥æ–¹å‘</strong>ï¼šè‡ªé€‚åº”VATã€é«˜é˜¶VATã€å¤šæ¨¡æ€ã€å› æœæ¨æ–­</li>
</ol>
<p><strong>æ ¸å¿ƒæ€æƒ³</strong>ï¼š
- æ³›åŒ–æ€§ = å¹³æ»‘æ€§ + å†³ç­–è¾¹ç•Œæ¸…æ™°åº¦
- VATæ‰¾åˆ°"æœ€å…·æŒ‘æˆ˜æ€§"çš„å¹³æ»‘æ€§æµ‹è¯•
- åŠç›‘ç£å­¦ä¹ é€šè¿‡æ— æ ‡ç­¾æ•°æ®å¼ºåŒ–å¹³æ»‘æ€§å‡è®¾</p>
<p><strong>å®è·µä»·å€¼</strong>ï¼š
- æ ‡ç­¾ç¨€ç¼ºåœºæ™¯ä¸‹æ˜¾è‘—æå‡æ€§èƒ½ï¼ˆ10%-20%ï¼‰
- è®¡ç®—å¼€é”€å¯æ§ï¼ˆ1-2æ¬¡é¢å¤–å‰å‘ä¼ æ’­ï¼‰
- ä¸ç°æœ‰æ–¹æ³•ï¼ˆæ•°æ®å¢å¼ºã€Dropoutï¼‰äº’è¡¥</p>
        </div>
    </div>
</body>
</html>