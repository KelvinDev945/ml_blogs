<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformerå‡çº§ä¹‹è·¯ï¼š3ã€ä»Performeråˆ°çº¿æ€§Attention</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>Transformerå‡çº§ä¹‹è·¯ï¼š3ã€ä»Performeråˆ°çº¿æ€§Attention</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2025-12-31 | ğŸ“„ å¤§å°: 14.6 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/8338">https://spaces.ac.cn/archives/8338</a></p>
<p><strong>å‘å¸ƒæ—¥æœŸ</strong>: </p>
<hr />
<p>çœ‹è¿‡ç¬”è€…ä¹‹å‰çš„æ–‡ç« <a href="/archives/7546">ã€Šçº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿã€‹</a>å’Œ<a href="/archives/7921">ã€ŠPerformerï¼šç”¨éšæœºæŠ•å½±å°†Attentionçš„å¤æ‚åº¦çº¿æ€§åŒ–ã€‹</a>çš„è¯»è€…ï¼Œå¯èƒ½ä¼šè§‰å¾—æœ¬æ–‡çš„æ ‡é¢˜æœ‰ç‚¹ä¸è‡ªç„¶ï¼Œå› ä¸ºæ˜¯å…ˆæœ‰çº¿æ€§Attentionç„¶åæ‰æœ‰Performerçš„ï¼Œå®ƒä»¬çš„å…³ç³»ä¸ºâ€œPerformeræ˜¯çº¿æ€§Attentionçš„ä¸€ç§å®ç°ï¼Œåœ¨ä¿è¯çº¿æ€§å¤æ‚åº¦çš„åŒæ—¶ä¿æŒäº†å¯¹æ ‡å‡†Attentionçš„è¿‘ä¼¼â€ï¼Œæ‰€ä»¥æ­£å¸¸æ¥è¯´æ˜¯â€œä»çº¿æ€§Attentionåˆ°Performerâ€æ‰å¯¹ã€‚</p>
<p>ç„¶è€Œï¼Œæœ¬æ–‡å¹¶ä¸æ˜¯æ‰“ç®—æ¢³ç†çº¿æ€§Attentionçš„å‘å±•å²ï¼Œè€Œæ˜¯æ‰“ç®—åè¿‡æ¥æ€è€ƒPerformerç»™çº¿æ€§Attentionæ‰€å¸¦æ¥çš„å¯ç¤ºï¼Œæ‰€ä»¥æ˜¯â€œä»Performeråˆ°çº¿æ€§Attentionâ€ã€‚</p>
<h2 id="_1">æ¿€æ´»å‡½æ•°</h2>
<p>çº¿æ€§Attentionçš„å¸¸è§å½¢å¼æ˜¯<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})<em j="1">i = \frac{\sum\limits</em>}^n \text{sim}(\boldsymbol{q<em j="1">i, \boldsymbol{k}_j)\boldsymbol{v}_j}{\sum\limits</em>}^n \text{sim}(\boldsymbol{q<em j="1">i, \boldsymbol{k}_j)} = \frac{\sum\limits</em>}^n \phi(\boldsymbol{q<em j="1">i)^{\top} \varphi(\boldsymbol{k}_j)\boldsymbol{v}_j}{\sum\limits</em>}^n \phi(\boldsymbol{q}_i)^{\top} \varphi(\boldsymbol{k}_j)}\end{equation<br />
å…¶ä¸­$\phi(\cdot)$ã€$\varphi(\cdot)$æ˜¯å€¼åŸŸéè´Ÿçš„æ¿€æ´»å‡½æ•°ã€‚é‚£ä¹ˆå¦‚ä½•é€‰å–è¿™ä¸ªæ¿€æ´»å‡½æ•°å‘¢ï¼ŸPerformerå‘Šè¯‰æˆ‘ä»¬ï¼Œåº”è¯¥é€‰æ‹©æŒ‡æ•°å‡½æ•°<br />
\begin{equation}\phi(x)=\varphi(x)=e^x\end{equation}</p>
<p>é¦–å…ˆï¼Œæˆ‘ä»¬æ¥çœ‹å®ƒè·Ÿå·²æœ‰çš„ç»“æœæœ‰ä»€ä¹ˆä¸ä¸€æ ·ã€‚åœ¨<a href="https://papers.cool/arxiv/2006.16236">ã€ŠTransformers are RNNs: Fast Autoregressive Transformers with Linear Attentionã€‹</a>ç»™å‡ºçš„é€‰æ‹©æ˜¯ï¼š<br />
\begin{equation}\phi(x)=\varphi(x)=1 + \text{elu}(x) = \left\{\begin{aligned}1 + x,\, x \geq 0\\\ e^x,\, x &lt; 0\end{aligned}\right.\end{equation}<br />
æˆ‘ä»¬çŸ¥é“$1+x$æ­£æ˜¯$e^x$åœ¨$x=0$å¤„çš„ä¸€é˜¶æ³°å‹’å±•å¼€ï¼Œå› æ­¤$1+\text{elu}(x)$è¿™ä¸ªé€‰æ‹©å…¶å®å·²ç»ç›¸å½“æ¥è¿‘$e^x$äº†ã€‚</p>
<p>æ­¤å¤–ï¼Œ$\phi(x)=\varphi(x)=e^x$è¿™ä¸ªæ–¹æ¡ˆè¿˜è·Ÿ<a href="https://papers.cool/arxiv/1812.01243">ã€ŠEfficient Attention: Attention with Linear Complexities<br />
ã€‹</a>ä¸€æ–‡ä¸­å¼•å…¥çš„åŒé‡softmaxæ¥æ„å»ºçº¿æ€§Attentionçš„è®¾è®¡å¾ˆç›¸ä¼¼ï¼Œåœ¨é‚£ç§è®¾è®¡ä¸­æœ‰$\phi(\boldsymbol{q})=softmax(\boldsymbol{q}),\varphi(\boldsymbol{k})=e^{\boldsymbol{k}}$ï¼Œç›¸æ¯”ç›´æ¥$\phi(x)=\varphi(x)=e^x$åªä¸è¿‡å½’ä¸€åŒ–çš„ä½ç½®æœ‰æ‰€ä¸åŒã€‚</p>
<h2 id="_2">ç®€å•æ¨å¯¼</h2>
<p>ä¸ºä»€ä¹ˆè¯´Performerå‘Šè¯‰æˆ‘ä»¬æ¿€æ´»å‡½æ•°çš„æœ€ä½³é€‰æ‹©æ˜¯$e^x$å‘¢ï¼Ÿæˆ‘ä»¬æ¥çœ‹Performeræ‰¾åˆ°çš„å°†æ ‡å‡†Attentionçº¿æ€§åŒ–çš„æ˜ å°„ï¼š<br />
\begin{equation}\begin{aligned}<br />
e^{\boldsymbol{q}\cdot \boldsymbol{k}}&amp;=\mathbb{E}<em _tilde_boldsymbol_q="\tilde{\boldsymbol{q">{\boldsymbol{\omega}\sim \mathcal{N}(\boldsymbol{\omega};0,\boldsymbol{1}_d)}\left[e^{\boldsymbol{\omega}\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2} \times e^{\boldsymbol{\omega}\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2}\right]\\\[6pt]<br />
&amp;\approx\underbrace{\frac{1}{\sqrt{m}}\begin{pmatrix}e^{\boldsymbol{\omega}_1\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2} \\\<br />
e^{\boldsymbol{\omega}_2\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2}\\\<br />
\vdots\\\<br />
e^{\boldsymbol{\omega}_m\cdot \boldsymbol{q}-\Vert \boldsymbol{q}\Vert^2 / 2} \end{pmatrix}}</em>}}<br />
\cdot \underbrace{\frac{1}{\sqrt{m}}\begin{pmatrix}e^{\boldsymbol{\omega}<em _tilde_boldsymbol_k="\tilde{\boldsymbol{k">1\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2} \\\<br />
e^{\boldsymbol{\omega}_2\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2}\\\<br />
\vdots\\\<br />
e^{\boldsymbol{\omega}_m\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2} \end{pmatrix}}</em>}}<br />
\end{aligned}\end{equation}<br />
ç®€å•æ¥è¯´ï¼ŒPerformeræ‰¾åˆ°äº†ä¸€ä¸ªæ˜ å°„ï¼Œä½¿å¾—$d$ç»´å‘é‡$\boldsymbol{q},\boldsymbol{k}$è¢«æ˜ å°„ä¸ºäº†$m$ç»´å‘é‡$\tilde{\boldsymbol{q}},\tilde{\boldsymbol{k}}$ï¼Œå¹¶ä¸”æ»¡è¶³è¿‘ä¼¼å…³ç³»$e^{\boldsymbol{q}\cdot \boldsymbol{k}}\approx \tilde{\boldsymbol{q}}\cdot\tilde{\boldsymbol{k}}$ï¼Œæ­¤æ—¶<br />
\begin{equation}a_{i,j} = \frac{e^{\boldsymbol{q}_i\cdot \boldsymbol{k}_j}}{\sum\limits_j e^{\boldsymbol{q}_i\cdot \boldsymbol{k}_j}}\approx \frac{\tilde{\boldsymbol{q}}_i\cdot\tilde{\boldsymbol{k}}_j}{\sum\limits_j \tilde{\boldsymbol{q}}_i\cdot\tilde{\boldsymbol{k}}_j} = \frac{(\lambda(\tilde{\boldsymbol{q}}_i)\tilde{\boldsymbol{q}}_i)\cdot\tilde{\boldsymbol{k}}_j}{\sum\limits_j (\lambda(\tilde{\boldsymbol{q}}_i)\tilde{\boldsymbol{q}}_i)\cdot\tilde{\boldsymbol{k}}_j}\end{equation}<br />
æœ€åä¸€ä¸ªç­‰å¼è¡¨æ˜ï¼Œå¾€$\tilde{\boldsymbol{q}}$é‡Œè¾¹ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•°ï¼ˆå“ªæ€•è¿™ä¸ªå¸¸æ•°è·Ÿ$\tilde{\boldsymbol{q}}$æœ‰å…³ï¼‰ï¼ŒPerformerçš„ç»“æœå®Œå…¨ä¸æ”¹å˜ï¼Œè¿™æ„å‘³ç€å°†æ˜ å°„æ”¹ä¸º<br />
\begin{equation}<br />
\tilde{\boldsymbol{q}} = \begin{pmatrix}e^{\boldsymbol{\omega}_1\cdot \boldsymbol{q}} \\\<br />
e^{\boldsymbol{\omega}_2\cdot \boldsymbol{q}}\\\<br />
\vdots\\\<br />
e^{\boldsymbol{\omega}_m\cdot \boldsymbol{q}} \end{pmatrix},\qquad<br />
\tilde{\boldsymbol{k}}=\begin{pmatrix}e^{\boldsymbol{\omega}_1\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2} \\\<br />
e^{\boldsymbol{\omega}_2\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2}\\\<br />
\vdots\\\<br />
e^{\boldsymbol{\omega}_m\cdot \boldsymbol{k}-\Vert \boldsymbol{k}\Vert^2 / 2} \end{pmatrix}<br />
\end{equation}<br />
Performerçš„ç»“æœä¸ä¼šæœ‰ä»»ä½•å˜åŒ–ã€‚å½“ç„¶ï¼Œè¿™é‡Œ$\Vert \boldsymbol{k}\Vert^2$è¿™ä¸€é¡¹è¿˜ä¸èƒ½å»æ‰ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬å‡è®¾$\Vert \boldsymbol{k}\Vert^2$ä¸ä¼šæ³¢åŠ¨å¤ªå¤§ï¼Œå®ƒå¹¶ä¸æ˜¯Attentionçš„ä¸»è¦å› ç´ ï¼Œé‚£ä¹ˆè¿™ä¸€é¡¹ä¹Ÿç›¸å½“äºä¸€ä¸ªå¸¸æ•°ï¼Œäºæ˜¯æœ€ç»ˆçš„æ˜ å°„ï¼ˆè¿‘ä¼¼åœ°ï¼‰ç­‰ä»·ä¸º<br />
\begin{equation}<br />
\tilde{\boldsymbol{q}} = \begin{pmatrix}e^{\boldsymbol{\omega}_1\cdot \boldsymbol{q}} \\\<br />
e^{\boldsymbol{\omega}_2\cdot \boldsymbol{q}}\\\<br />
\vdots\\\<br />
e^{\boldsymbol{\omega}_m\cdot \boldsymbol{q}} \end{pmatrix},\qquad<br />
\tilde{\boldsymbol{k}}=\begin{pmatrix}e^{\boldsymbol{\omega}_1\cdot \boldsymbol{k}} \\\<br />
e^{\boldsymbol{\omega}_2\cdot \boldsymbol{k}}\\\<br />
\vdots\\\<br />
e^{\boldsymbol{\omega}_m\cdot \boldsymbol{k}} \end{pmatrix}<br />
\end{equation}<br />
è¿™ä¸ªçœ‹ä¸Šå»å·²ç»ç®€åŒ–å¾ˆå¤šçš„æ˜ å°„è¯¥æ€ä¹ˆç†è§£å‘¢ï¼Ÿå…¶å®$m$ä¸ªéšæœºå‘é‡$\boldsymbol{\omega}_1,\boldsymbol{\omega}_2,\cdots,\boldsymbol{\omega}_m$æ‹¼æˆäº†ä¸€ä¸ª$d\times m$çš„çŸ©é˜µï¼Œå®ƒå°†$d$ç»´çš„$\boldsymbol{q},\boldsymbol{k}$æ˜ å°„ä¸ºäº†$m$ç»´çš„å‘é‡ï¼Œç„¶ååŠ ä¸Šæ¿€æ´»å‡½æ•°$e^x$å¾—åˆ°äº†$\tilde{\boldsymbol{q}},\tilde{\boldsymbol{k}}$ã€‚æˆ‘ä»¬çŸ¥é“Attentionçš„$\boldsymbol{q},\boldsymbol{k}$éƒ½æœ‰ä¸€ä¸ªå…¨è¿æ¥å±‚å˜æ¢ï¼Œå¦‚æœæˆ‘ä»¬å°†è¿™ä¸ª$d\times m$çš„æ˜ å°„çŸ©é˜µæ•´åˆåˆ°å…¨è¿æ¥å±‚ä¸­ï¼Œé‚£ä¹ˆå‰©ä¸‹çš„å°±æ˜¯ä¸€ä¸ªæ¿€æ´»å‡½æ•°$e^x$äº†ï¼</p>
<p>æ‰€ä»¥è¿™å°±æ˜¯æœ€ä¼˜æ¿€æ´»å‡½æ•°$e^x$çš„æ¥æºäº†ï¼Œåªè¦æˆ‘ä»¬å°†$\boldsymbol{q},\boldsymbol{k}$çš„è¾“å‡ºç»´åº¦ä»$d$ç»´æ”¹ä¸º$m$ç»´ï¼Œç„¶åé…åˆ$e^x$çš„æ¿€æ´»å‡½æ•°ï¼Œé‚£ä¹ˆç†è®ºä¸Šå®ƒå°±æœ‰Performerçš„æ‹Ÿåˆèƒ½åŠ›ï¼Œç”šè‡³æ›´å¼ºï¼Œå› ä¸ºPerformerçš„$d\times m$çŸ©é˜µæ˜¯ä¸€ä¸ªå›ºå®šçš„éšæœºçŸ©é˜µï¼Œè€Œè¿™é‡Œæˆ‘ä»¬ç›¸å½“äºæŠŠè¯¥çŸ©é˜µä¹Ÿè®¾ä¸ºå¯è®­ç»ƒäº†ï¼Œè¿˜å»æ‰äº†ä½ç§©çº¦æŸï¼Œç©ºé—´æ˜¯æ¯”Performeræ›´å¤§çš„ã€‚</p>
<h2 id="_3">ä½ç§©é—®é¢˜</h2>
<p>ä¸ç®¡æ˜¯æœ¬æ–‡çš„ä¸»è§’Performerï¼Œè¿˜æ˜¯ä¹‹å‰åœ¨<a href="/archives/8180">ã€ŠNystrÃ¶mformerï¼šåŸºäºçŸ©é˜µåˆ†è§£çš„çº¿æ€§åŒ–Attentionæ–¹æ¡ˆã€‹</a>ä»‹ç»çš„NystrÃ¶mformerï¼Œå®ƒä»¬çš„æ€è·¯éƒ½æ˜¯â€œå¯»æ‰¾ä¸€ä¸ªèƒ½é€¼è¿‘æ ‡å‡†Attentionçš„çº¿æ€§Attentionâ€ã€‚é‚£ä¹ˆä¸€ä¸ªå¾ˆè‡ªç„¶çš„é—®é¢˜å°±æ˜¯ï¼šæ ‡å‡†Attentionæœ‰ä»€ä¹ˆå¥½çš„ï¼Ÿå“ªé‡Œå€¼å¾—å¤§å®¶å‘å®ƒå¯¹é½ï¼Ÿ</p>
<p>ä»ä¿¡æ¯æŸå¤±çš„è§’åº¦æ¥çœ‹ï¼Œæ ‡å‡†AttentionçŸ©é˜µçš„â€œç§©â€å¯èƒ½æ›´å¤§ï¼Œå³æ›´æ¥è¿‘å¯é€†çŸ©é˜µï¼Œè¿™æ„å‘³ç€å®ƒèƒ½ä¿ç•™æ›´å¤šæœ‰æ•ˆä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒAttentionçŸ©é˜µæ˜¯ä¸€ä¸ª$n\times n$çš„çŸ©é˜µï¼Œå®ƒç”±$\boldsymbol{Q},\boldsymbol{K}\in\mathbb{R}^{n\times d}$é€šè¿‡$softmax(\boldsymbol{Q}\boldsymbol{K}^{\top})$è€Œæ¥ï¼Œè¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„$d$æ˜¯Attentionçš„key_sizeï¼Œæ¯”å¦‚å¯¹äºBERT baseæ¥è¯´å®ƒåªæ˜¯64ï¼Œè€Œ$n$å¾€å¾€æ¯”è¾ƒå¤§ï¼Œè¿™è¯´æ˜$\boldsymbol{Q}\boldsymbol{K}^{\top}$çš„ç§©ä¸è¶…è¿‡$d$ï¼Œè€Œä¸”$d\ll n$ï¼Œå³ç¦»æ»¡ç§©è¿˜è¿œå¾—å¾ˆã€‚ä¸è¿‡ï¼Œ$softmax$çš„å…³é”®è¿ç®—æ˜¯$e^{\boldsymbol{Q}\boldsymbol{K}^{\top}}$ï¼Œä¸€ä¸ªçŸ©é˜µå¦‚æœæ¯ä¸ªå…ƒç´ å–æŒ‡æ•°çš„è¯ï¼Œé‚£ä¹ˆæ–°çŸ©é˜µçš„ç§©æ˜¯å¯èƒ½å¢åŠ çš„ï¼æ‰€ä»¥æ ‡å‡†AttentionçŸ©é˜µæœ‰å‡ç§©çš„å¯èƒ½æ€§ï¼Œæ„å‘³ç€å®ƒè•´å«äº†æ›´æœ‰æ•ˆå¤„ç†ä¿¡æ¯çš„èƒ½åŠ›ã€‚</p>
<p>ç›¸æ¯”ä¹‹ä¸‹ï¼Œçº¿æ€§AttentionçŸ©é˜µæ˜¯$\tilde{\boldsymbol{Q}}\tilde{\boldsymbol{K}}^{\top}$çš„å½¢å¼ï¼Œæ‰€ä»¥çº¿æ€§AttentionçŸ©é˜µçš„ç§©ä¸€å®šä¸è¶…è¿‡$m$ï¼Œè€Œä¸ºäº†å¼¥è¡¥ç§©çš„æŸå¤±ï¼Œæ‰€ä»¥ä¸€èˆ¬è¦è®¾ç½®$m &gt; d$ï¼Œåœ¨Performerçš„å®éªŒä¸­é€‰æ‹©çš„æ˜¯$m = 4d$ï¼Œä¹Ÿå°±æ˜¯key_sizeæ‰©å¤§ä¸º4å€ï¼Œç§©çš„é‡è¦æ€§å¯è§ä¸€æ–‘ã€‚å½“ç„¶ï¼Œæ‰©å¤§äº†key_sizeï¼Œä¸€ä¸ªç›´æ¥çš„åæœæ˜¯å¤„ç†çŸ­åºåˆ—çš„æ—¶å€™ï¼Œçº¿æ€§Attentionè¿˜æ¯”æ ‡å‡†Attentionè¦æ…¢ï¼Œè¿™æ˜¯çº¿æ€§Attentionçš„å›ºæœ‰ç“¶é¢ˆã€‚</p>
<p>å…³äºAttentionçŸ©é˜µçš„ç§©çš„ç†è®ºåˆ†æï¼Œä¹Ÿæœ‰ä¸€äº›è®ºæ–‡å¯ä»¥å‚è€ƒï¼Œæ¯”å¦‚<a href="https://papers.cool/arxiv/2002.07028">ã€ŠLow-Rank Bottleneck in Multi-head Attention Modelsã€‹</a>å°±æŒ‡å‡ºå“ªæ€•åœ¨æ ‡å‡†Attentionä¸­ï¼Œä½ç§©æ€§ä¹Ÿæ˜¯ä¸€ä¸ªä¸¥é‡çš„ç“¶é¢ˆï¼Œå¢å¤§key_sizeå¯ä»¥æå‡æ€§èƒ½ï¼›ä¸Šä¸ªæœˆçš„<a href="https://papers.cool/arxiv/2103.03404">ã€ŠAttention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depthã€‹</a>åˆ™æŒ‡å‡ºï¼Œå¦‚æœæ²¡æœ‰æ®‹å·®å’ŒFFNï¼Œé‚£ä¹ˆæ ‡å‡†Attentionæœ‰æå¤§çš„é£é™©é€€åŒ–ä¸ºç§©ç­‰äº1çš„ç®€å•å˜æ¢ã€‚è¿æ ‡å‡†Attentionè¿™ä¸ªæœ‰â€œå‡ç§©æ½œåŠ›â€çš„æ¨¡å‹éƒ½æœ‰ä½ç§©é—®é¢˜ï¼Œæ›´ä¸ç”¨è¯´çº¿æ€§Attentionè¿™ç§æœ¬èº«ç§©å°±æœ‰ä¸Šé™çš„æ¨¡å‹äº†ã€‚</p>
<p>æ‰€ä»¥ï¼Œä¸€å¥è¯å°±æ˜¯ï¼šç”¨çº¿æ€§Attentionéœ€è¦ç”¨æ›´å¤§çš„key_sizeæ¥ç»´æŒçŸ©é˜µçš„ç§©ã€‚</p>
<h2 id="_4">é›†ä¸­æ³¨æ„</h2>
<p>æˆ‘ä»¬è¿˜å¯ä»¥ä»ç¨€ç–æ€§è§’åº¦æ¥ç†è§£æ ‡å‡†Attentionçš„å¥½å¤„ã€‚ç›´è§‚æ¥æƒ³ï¼Œæ—¢ç„¶æ˜¯â€œæ³¨æ„åŠ›æœºåˆ¶â€ï¼Œé‚£ä¹ˆè‚¯å®šéœ€è¦â€œé›†ä¸­æ³¨æ„åŠ›â€ï¼Œå¦‚æœå¤ªåˆ†æ•£ï¼Œé‚£ä¹ˆå¯èƒ½å°±ç›¸å½“äºå¹³å‡æ± åŒ–äº†ï¼Œè€Œâ€œé›†ä¸­æ³¨æ„åŠ›â€ï¼Œæ„å‘³ç€æ¯ä¸ªtokenåº”è¯¥åªèƒ½æ˜¾è‘—åœ°å…³è”åˆ°è‹¥å¹²ä¸ªtokenï¼Œç”¨æ•°å­¦çš„è¯è¯´ï¼Œé‚£å°±æ˜¯æ„å‘³ç€AttentionçŸ©é˜µæ˜¯ç¨€ç–çš„ï¼Œæˆ–è€…è¯´è‡³å°‘è¦å…·å¤‡å˜å¾—ç¨€ç–çš„å¯èƒ½æ€§ã€‚</p>
<p>å¯¹äºæ ‡å‡†Attentionæ¥è¯´ï¼Œå®ƒé€šè¿‡softmaxæ¥å½’ä¸€åŒ–<br />
\begin{equation}a_{i,j} = \frac{e^{\boldsymbol{q}_i\cdot \boldsymbol{k}_j}}{\sum\limits_j e^{\boldsymbol{q}_i\cdot \boldsymbol{k}_j}}\end{equation}<br />
å…¶ä¸­æŒ‡æ•°å‡½æ•°$e^x$èµ·åˆ°äº†ä¸€ä¸ªæ”¾å¤§çš„ä½œç”¨ï¼Œåªè¦å„ä¸ª$\boldsymbol{q}_i\cdot \boldsymbol{k}_j$æœ¬èº«èƒ½æ‹‰å¼€ä¸€å®šå·®è·ï¼Œé‚£ä¹ˆ$e^{\boldsymbol{q}_i\cdot \boldsymbol{k}_j}$ä¼šè¿›ä¸€æ­¥æ”¾å¤§è¿™ç§å·®è·ï¼Œç»“æœå°±æ˜¯å½’ä¸€åŒ–ä¹‹åé™¤äº†æœ€å¤§å€¼çš„é‚£å‡ ä¸ªä½ç½®ä¹‹å¤–ï¼Œå‰©ä¸‹çš„æ¦‚ç‡éƒ½å¾ˆæ¥è¿‘äº0äº†ï¼Œè¿™è¯´æ˜æ ‡å‡†Attentionæ˜¯æœ‰æ½œåŠ›â€œé›†ä¸­æ³¨æ„åŠ›â€çš„ã€‚è€Œå¯¹äºçº¿æ€§Attentionæ¥è¯´ï¼Œå®ƒæ˜¯ç›´æ¥å†…ç§¯çš„ç»“æœï¼Œæ²¡æœ‰å¾—åˆ°$e^x$çš„è¿›ä¸€æ­¥æ”¾å¤§ï¼Œæ‰€ä»¥å®ƒçš„æ³¨æ„åŠ›æ˜¯æ¯”è¾ƒç¨ å¯†çš„ï¼Œåœ¨åºåˆ—é•¿åº¦è¾ƒå¤§çš„æ—¶å€™ï¼Œå®ƒå¾€å¾€å°±å¾ˆæ¥è¿‘å¹³å‡æ± åŒ–äº†ã€‚è¦ç¼“è§£è¿™ä¸€ç‚¹ï¼Œè¿˜æ˜¯éœ€è¦å¢å¤§key_sizeï¼Œæ¥æ”¾å¤§å·®è·ï¼Œç›´è§‚æ¥è¯´ï¼Œå°±æ˜¯$n$å‘é‡æ”¾åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´å¤ªâ€œæŒ¤â€äº†ï¼Œæ¢åˆ°æ›´é«˜ç»´çš„ç©ºé—´å°±â€œæ¾â€ä¸€äº›äº†ã€‚</p>
<p>æ€ä¹ˆæ ·éªŒè¯ç¨€ç–çš„é‡è¦æ€§å‘¢ï¼Ÿç¬”è€…æ›¾ç»å°è¯•è¿‡ï¼Œå°†çº¿æ€§Attentionçš„AttentionçŸ©é˜µå…ˆç®—å‡ºæ¥ï¼Œç„¶åå¼ºè¡Œæˆªæ–­AttentionçŸ©é˜µï¼ˆä¹Ÿå°±æ˜¯æ¯ä¸ªtokenåªè·Ÿå‰åå‡ ä¸ªtokenåšattentionï¼Œå˜æˆå±€éƒ¨å½¢å¼çš„Attentionï¼‰è®©å®ƒå˜å¾—ç¨€ç–ï¼Œç»“æœå‘ç°è¿™ç§æˆªæ–­åçš„çº¿æ€§Attentionæ•ˆæœæ˜æ˜¾å¥½äºå…¨çŸ©é˜µçš„çº¿æ€§Attentionã€‚è¿™å°±è‚¯å®šäº†ç¨€ç–çš„é‡è¦æ€§äº†ï¼Œå½“ç„¶ï¼Œè¿™æ ·æŠŠAttentionçŸ©é˜µå…ˆç®—å‡ºæ¥ç„¶åå‰è¡Œæˆªæ–­çš„æ–¹å¼ï¼Œä½¿å¾—çº¿æ€§Attentionçš„å¤æ‚åº¦ä¸å†æ˜¯çº¿æ€§çš„äº†ï¼Œå› æ­¤ä¸å…·å¤‡å®ç”¨ä»·å€¼ï¼Œä»…ç”¨äºç†è®ºéªŒè¯ã€‚</p>
<p>è¿˜æœ‰ä¸€ä¸ªå®éªŒç°è±¡å¯ä»¥è¾…åŠ©è¯æ˜ç¨€ç–çš„é‡è¦æ€§ï¼Œé‚£å°±æ˜¯çº¿æ€§Attentionåšè¯­è¨€æ¨¡å‹æˆ–è€…è§£ç å™¨çš„æ—¶å€™ï¼Œæ•ˆæœæ˜¯è·Ÿæ ‡å‡†Attentionå·®ä¸äº†å¤šå°‘çš„ï¼Œè¿™æ—¶å€™çº¿æ€§Attentionå˜æˆäº†å•å‘çš„RNNï¼ˆå‚è€ƒ<a href="https://papers.cool/arxiv/2006.16236">ã€ŠTransformers are RNNs: Fast Autoregressive Transformers with Linear Attentionã€‹</a>ï¼‰ï¼Œç­‰ä»·äºAttentionçŸ©é˜µå˜æˆäº†ä¸‹ä¸‰è§’é˜µï¼Œä¹Ÿæ˜¯æ›´ç¨€ç–äº†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚æœç”¨ä¸ç¨€ç–çš„åŒå‘çš„çº¿æ€§Attentionç›´æ¥åšMLMæ¨¡å‹ï¼Œåˆ™æ‰ç‚¹ä¼šç›¸å½“æ˜æ˜¾ã€‚</p>
<p>æ›´é‡è¦çš„æ˜¯ï¼Œç¨€ç–æ€§å’Œå‰ä¸€èŠ‚æåˆ°çš„ç§©æ˜¯æœ‰å¯†åˆ‡å…³è”çš„ï¼Œç”šè‡³å¯ä»¥è¯´å®ƒä»¬æ˜¯â€œä¸€ä½“ä¸¤é¢â€ï¼šé€‚å½“çš„ç¨€ç–åŒ–æ–¹æ³•èƒ½æé«˜çŸ©é˜µçš„ç§©ï¼æ¯”å¦‚åšè¯­è¨€æ¨¡å‹çš„ä¸‹ä¸‰è§’AttentionçŸ©é˜µï¼Œåªè¦å¯¹è§’çº¿å…ƒç´ éé›¶ï¼ˆå¾€å¾€éƒ½èƒ½è¾¾åˆ°ï¼‰ï¼Œé‚£ä¹ˆè¿™æ—¶å€™çš„çŸ©é˜µç›´æ¥å°±æ˜¯æ»¡ç§©å¯é€†é˜µäº†ï¼è¿˜æœ‰ç¬”è€…å®éªŒçš„å±€éƒ¨Attentionæˆªæ–­ï¼Œä¹Ÿèƒ½å¢åŠ çŸ©é˜µçš„ç§©ï¼Œæ¯”å¦‚æç«¯æƒ…å†µä¸‹ï¼Œæ¯ä¸ªtokenåªè·Ÿè‡ªèº«åšattentionï¼Œé‚£ä¹ˆAttentionçŸ©é˜µå°±æ˜¯æ»¡ç§©çš„å•ä½é˜µäº†ï¼</p>
<h2 id="_5">æ–‡ç« å°ç»“</h2>
<p>æœ¬æ–‡ä»Performerå‡ºå‘æ€è€ƒäº†çº¿æ€§Attentionçš„ä¸€äº›é—®é¢˜ï¼ŒåŒ…æ‹¬å…³äºçº¿æ€§Attentionçš„æ¿€æ´»å‡½æ•°é€‰æ‹©ï¼Œä»¥åŠçº¿æ€§Attentionçš„ç“¶é¢ˆæ‰€åœ¨ï¼ˆä½ç§©æ€§ã€ç¨€ç–æ€§ï¼‰ï¼Œæ€»çš„ç»“è®ºæ˜¯ï¼Œçº¿æ€§Attentionçš„æœ€ä½³æ¿€æ´»å‡½æ•°åº”å½“æ˜¯æŒ‡æ•°å‡½æ•°ï¼Œè€Œæœ‰æ•ˆçš„Attentionæœºåˆ¶åº”å½“å…·å¤‡æ›´é«˜çš„ç§©å’Œæ›´å¤§çš„ç¨€ç–æ€§ã€‚</p>
<p><em><strong>è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š</strong><a href="https://spaces.ac.cn/archives/8338">https://spaces.ac.cn/archives/8338</a></em></p>
<p><em><strong>æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="ã€Šç§‘å­¦ç©ºé—´FAQã€‹">ã€Šç§‘å­¦ç©ºé—´FAQã€‹</a></p>
<p><strong>å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚</strong></p>
<p><strong>å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼</strong></p>
<p>æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>å¾®ä¿¡æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>æ”¯ä»˜å®æ‰“èµ</p>
<p>å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>ç‚¹å‡»è¿™é‡Œ</strong></a>æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚</p>
<p><strong>å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š</strong></p>
<p>è‹å‰‘æ—. (Apr. 22, 2021). ã€ŠTransformerå‡çº§ä¹‹è·¯ï¼š3ã€ä»Performeråˆ°çº¿æ€§Attention ã€‹[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8338">https://spaces.ac.cn/archives/8338</a></p>
<p>@online{kexuefm-8338,<br />
title={Transformerå‡çº§ä¹‹è·¯ï¼š3ã€ä»Performeråˆ°çº¿æ€§Attention},<br />
author={è‹å‰‘æ—},<br />
year={2021},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/8338}},<br />
} </p>
<hr />
<h2 id="_6">å…¬å¼æ¨å¯¼ä¸æ³¨é‡Š</h2>
<p>TODO: æ·»åŠ è¯¦ç»†çš„æ•°å­¦å…¬å¼æ¨å¯¼å’Œæ³¨é‡Š</p>
        </div>
    </div>
</body>
</html>