<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>当生成模型肆虐：互联网将有“疯牛病”之忧？</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">← 返回首页</a>
        <header>
            <h1>当生成模型肆虐：互联网将有“疯牛病”之忧？</h1>
            <div class="meta">📅 最后更新: 2025-11-10 | 📄 大小: 11.2 KB</div>
        </header>
        <div class="content">
            <p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9687">https://spaces.ac.cn/archives/9687</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，不管是文本还是视觉领域，各种生成模型正在以无法阻挡的势头“肆虐”互联网。虽然大家都明白，实现真正的通用人工智能（AGI）还有很长的路要走，但这并不妨碍人们越来越频繁地利用生成模型来创作和分享内容。君不见，很多网络文章已经配上了Stable Diffusion模型生成的插图；君不见，很多新闻风格已经越来越显现出ChatGPT的影子。看似无害的这种趋势，正悄然引发了一个问题：我们是否应该对互联网上充斥的生成模型数据保持警惕？</p>
<p>近期发表的论文<a href="https://papers.cool/arxiv/2307.01850">《Self-Consuming Generative Models Go MAD》</a>揭示了一种令人担忧的可能性，那就是生成模型正在互联网上的无节制扩张，可能会导致一场数字版的“疯牛病”疫情。本文一起学习这篇论文，探讨其可能带来的影响。</p>
<h2 id="_1">“食自己”</h2>
<p>一方面，人们使用生成模型的频率越来越高，将会导致互联网上由生成模型创作的内容越来越多；另一方面，生成模型也在更新迭代，其所用的数据也是从互联网爬取的，可以想像，后面的训练集中由生成模型创作的部分占比将会越来越高。换句话说，后面的每一代模型迭代时可能都没有足够多的新鲜数据，纯粹是用自己生产的数据来训，用广东话说就是“食自己”，这将导致模型的质量或者多样性越来越差，原论文称之为“模型自噬紊乱（Model Autophagy Disorder，MAD）”。</p>
<p>无独有偶，生物学上也曾出现了类似的例子。牛是草食动物，然而，一些畜牧业者为了增强其营养供应，将其他牛的残骸（包括大脑）粉碎并混入饲料中。这在当时看起来是一个机智的做法，但未曾想到最后导致了“<a href="https://zh.wikipedia.org/zh-cn/%E7%89%9B%E8%85%A6%E6%B5%B7%E7%B6%BF%E7%8B%80%E7%97%85%E8%AE%8A">疯牛症</a>”的出现和大规模传播。这一事例说明，长期的“食自己”可能会导致有害因素累积在生物体内，一旦达到一定程度，甚至可能触发灾难性的疾病。</p>
<p>因此，我们同样需要反思生成模型的“肆虐”是否会在互联网上引发另一场“疯牛症”——这不仅可能导致信息的同质化，使得各种内容开始变得千篇一律，缺乏原创性和多样性，还有可能引发一系列无法预见的问题。</p>
<h2 id="_2">降多样性</h2>
<p>可能有读者会产生疑问：生成模型不就是对真实数据分布的模拟吗？即便连续地使用生成模型的数据进行迭代训练，应该只是在重复呈现真实的数据分布，怎么会导致多样性的丧失呢？</p>
<p>这其中的原因是多方面的。首先，训练生成模型的数据往往并非直接取自真实分布，而是经过人为的加工处理，比如去噪、规范化和对齐。经过加工后，训练集就已经丧失了部分多样性。例如，我们之所以能观察到很多新闻报道或知乎回答都有一股ChatGPT的味道，并非是因为内容本身，而是因为它们的格式与ChatGPT的相似性，这就说明ChatGPT的训练数据和输出结果的风格都比较明显且局限。再比如，为了降低图像生成模型的训练难度，我们通常需要对图像进行对齐处理，如在训练人脸生成模型时，常常需要将所有人脸的眼睛对齐到同一位置，这些操作也导致了多样性的丧失。</p>
<p>此外，还有一个很关键的因素是，由于生成模型本身或者训练技巧等限制，每个生成模型都无法做到完美，此时我们通常会主动地引入一些牺牲多样性来提高生成质量的技巧。比如，对于GAN、<a href="/archives/5807">Flow</a>等生成模型，我们会选择降低采样噪声的方差，以获得质量更高的生成结果，这就是所谓的截断技巧或退火技巧。另外，如<a href="/archives/9257">《生成扩散模型漫谈（九）：条件控制生成结果》</a>所述，在扩散模型中我们通常引入条件信息以控制输出结果，不管是Classifier-Guidance还是Classifier-Free方案，额外条件的引入也会限制生成结果的多样性。总而言之，在生成模型不尽完美时，我们在平衡质量与多样性的过程中，就主动地放弃了部分多样性。</p>
<h2 id="_3">正态分布</h2>
<p>为了更深刻地认识到这种现象，我们接下来将探讨一些具体的例子。作为开始，我们首先考虑的是正态分布，因为它足够简单，所以求解和分析都更加清晰。但后面我们可以观察到，结果已经足够有代表性了。</p>
<p>假设真实分布是多元正态分布$\mathcal{N}(\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0)$，我们用来建模的分布也是正态分布$\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$，那么训练模型的过程，就是从训练集里边估计均值向量$\boldsymbol{\mu}$和协方差矩阵$\boldsymbol{\Sigma}$。接下来我们假设每一代生成模型训练时，都只用到上一代生成模型创作的数据，这是比较极端的假设，但不可否认当生成模型进一步普及时，这个假设越来越接近成立。</p>
<p>在这些假设下，我们从$t-1$代生成模型$\mathcal{N}(\boldsymbol{\mu}<em t-1="t-1">{t-1},\boldsymbol{\Sigma}</em>})$中采样$n$个样本$\boldsymbol{x<em t-1="t-1">{t-1}^{(1)},\boldsymbol{x}</em>}^{(2)},\cdots,\boldsymbol{x<em i="1">{t-1}^{(n)}$，来训练第$t$代的生成模型：<br />
\begin{equation}\boldsymbol{\mu}_t = \frac{1}{n}\sum</em>}^n \boldsymbol{x<em i="1">{t-1}^{(i)},\quad \boldsymbol{\Sigma}_t=\frac{1}{n-1} \sum</em>}^n \big(\boldsymbol{x<em t-1="t-1">{t-1}^{(i)} - \boldsymbol{\mu}_t\big)\big(\boldsymbol{x}</em>}^{(i)} - \boldsymbol{\mu<em i="1">t\big)^{\top}\end{equation}<br />
注意，如果加上截断技巧，那么第$t$代的生成模型就是$\mathcal{N}(\boldsymbol{\mu}_t,\lambda\boldsymbol{\Sigma}_t)$，其中$\lambda\in(0,1)$。于是可以想象，每一代的方差（多样性）都将以$\lambda$的比率衰减下去，最后变成零（完全丧失多样性）。如果不使用截断技巧（即$\lambda=1$）是不是就没事了？并不是。根据定义$\boldsymbol{\mu}_t = \frac{1}{n}\sum\limits</em>}^n \boldsymbol{x<em t-1="t-1">{t-1}^{(i)}$，由于$\boldsymbol{x}</em>}^{(i)}$都是随机采样得到的，所以$\boldsymbol{\mu<em t-1="t-1">t$也是一个随机变量，根据正态分布的叠加性，它实际上服从<br />
\begin{equation}\boldsymbol{\mu}_t \sim \mathcal{N}\left(\boldsymbol{\mu}</em>},\frac{1}{n}\boldsymbol{\Sigma}_{t-1}\right)\quad\Rightarrow\quad\boldsymbol{\mu}_t \sim \mathcal{N}\left(\boldsymbol{\mu}_0,\frac{t}{n}\boldsymbol{\Sigma}_0\right)\end{equation<br />
可以预见，当$t$足够大时，$\boldsymbol{\mu}_t$本身就会明显偏离$\boldsymbol{\mu}_0$，这对应的是质量的崩溃，而不单单是多样性的降低。</p>
<p>总的来说，截断技巧的引入，会大大加速多样性的丧失速度，而即便没有截断技巧，在长期有限样本的迭代训练中，生成分布也有可能明显偏离原始的真实分布。注意，正态分布这个例子所做的假设已经比一般的生成模型要弱得多，至少它的拟合能力是保证足够的，但这依然不可避免多样性衰减或者质量崩溃，而对于真实世界的数据和能力有限的生成模型来说，理论上只会更加糟糕。</p>
<h2 id="_4">生成模型</h2>
<p>对于实际的生成模型，理论分析难以进行，所以只能通过实验来探索结果了。原论文做了非常丰富的实验，结果基本上跟正态分布的结论一致，即如果加入截断技巧的话，多样性将会迅速丧失，即使没有截断技巧，经过反复迭代后的模型依然会不可避免地出现一些偏离。</p>
<p>这是带有截断技巧的一个例子：  </p>
<p><a href="/usr/uploads/2023/07/3150920671.jpg" title="点击查看原图"><img alt="带截断技巧，第1代生成结果" src="/usr/uploads/2023/07/3150920671.jpg" /></a></p>
<p>带截断技巧，第1代生成结果</p>
<p><a href="/usr/uploads/2023/07/1165517418.jpg" title="点击查看原图"><img alt="带截断技巧，第5代生成结果" src="/usr/uploads/2023/07/1165517418.jpg" /></a></p>
<p>带截断技巧，第5代生成结果</p>
<p>这是不带截断技巧的一个例子：  </p>
<p><a href="/usr/uploads/2023/07/4113965011.jpg" title="点击查看原图"><img alt="不带截断技巧，第1代生成结果" src="/usr/uploads/2023/07/4113965011.jpg" /></a></p>
<p>不带截断技巧，第1代生成结果</p>
<p><a href="/usr/uploads/2023/07/47157620.jpg" title="点击查看原图"><img alt="不带截断技巧，第7代生成结果" src="/usr/uploads/2023/07/47157620.jpg" /></a></p>
<p>不带截断技巧，第7代生成结果</p>
<p>当然，“每一轮的迭代只用上一轮的模型生成的数据”这个假设比较极端，原论文还分析了每一轮都包含一定数量的真实数据的情况，这个情况有包含两个子情况：1、真实数据的采样结果一开始就恒定不变；2、每次迭代都能采样到新鲜的真实数据。第1种方式比较容易实现，但原论文显示它只能减缓退化的速度，无法从根本上解决这个问题；第2种方式虽然可以解决退化问题，但在实际背景下，我们却很难有效筛选出真实数据和模型生成的数据。</p>
<h2 id="_5">文章小结</h2>
<p>本文探讨了当各种生成模型大规模“肆虐”互联网时可能出现的后果，在生成模型反复用自己生成的数据进行更新迭代时，可能导致信息严重同质化、丧失多样性的问题，类似于曾经因“牛吃牛”而出现的“疯牛病”。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9687">https://spaces.ac.cn/archives/9687</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jul. 14, 2023). 《当生成模型肆虐：互联网将有“疯牛病”之忧？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9687">https://spaces.ac.cn/archives/9687</a></p>
<p>@online{kexuefm-9687,<br />
title={当生成模型肆虐：互联网将有“疯牛病”之忧？},<br />
author={苏剑林},<br />
year={2023},<br />
month={Jul},<br />
url={\url{https://spaces.ac.cn/archives/9687}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>
    </div>
</body>
</html>