<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">← 返回首页</a>
        <header>
            <h1>曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果</h1>
            <div class="meta">📅 最后更新: 2025-12-31 | 📄 大小: 7.6 KB</div>
        </header>
        <div class="content">
            <p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8671">https://spaces.ac.cn/archives/8671</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在五花八门的预训练任务设计中，NSP通常认为是比较糟糕的一种，因为它难度较低，加入到预训练中并没有使下游任务微调时有明显受益，甚至RoBERTa的论文显示它会带来负面效果。所以，后续的预训练工作一般有两种选择：一是像RoBERTa一样干脆去掉NSP任务，二是像ALBERT一样想办法提高NSP的难度。也就是说，一直以来NSP都是比较“让人嫌弃”的。</p>
<p>不过，反转来了，NSP可能要“翻身”了。最近的一篇论文<a href="https://papers.cool/arxiv/2109.03564">《NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction》</a>（下面简称NSP-BERT）显示NSP居然也可以做到非常不错的Zero Shot效果！这又是一个基于模版（Prompt）的Few/Zero Shot的经典案例，只不过这一次的主角是NSP。</p>
<h2 id="_1">背景回顾</h2>
<p>曾经我们认为预训练纯粹就是预训练，它只是为下游任务的训练提供更好的初始化，像BERT的预训练任务有MLM（Masked Language Model和NSP（Next Sentence Prediction），在相当长的一段时间内，大家都不关心这两个预训练任务本身，而只是专注于如何通过微调来使得下游任务获得更好的性能。哪怕是T5将模型参数训练到了110亿，走的依然是“预训练+微调”这一路线。</p>
<p>首先有力地打破我们这个思维定式的，当属去年发布的<a href="https://papers.cool/arxiv/2005.14165">GPT3</a>，它显示在足够大的预训练模型下，我们可以设计特定的模版，使得不进行微调就可以起到很好的Few/Zero Shot效果。有GPT的地方，BERT从来不会缺席，既然GPT可以，那么BERT应该也行，这就导致了后来的<a href="https://papers.cool/arxiv/2009.07118">PET</a>工作，它同样构建特别的模版，利用预训练的MLM模型来做Few/Zero Shot，还不了解的读者可以参考<a href="/archives/7764">《必须要GPT3吗？不，BERT的MLM模型也能小样本学习》</a>。</p>
<p>自此，“预训练+模版”的工作逐渐增多，现在甚至有“爆发”之势，这系列工作现在大致都统称“Prompt-based Language Models”，随便搜搜就可以找到很多。如今，大家已经形成了一个共识：构建适当的Prompt，使得下游任务的形式跟预训练任务更贴近，通常能获得更好的效果。所以如何构建Prompt，便是这系列工作的重点之一，比如<a href="https://papers.cool/arxiv/2103.10385">P-tuning</a>就是其中的经典工作（参考<a href="/archives/8295">《P-tuning：自动构建模版，释放语言模型潜能》</a>）。</p>
<h2 id="nsp">NSP入场</h2>
<p>仔细观察一下Prompt-based的相关工作就会发现，当前主要的内容都是研究如何更好地利用预训练好的GPT、MLM或者Encoder-Decoder模型，鲜有关注其余预训练任务的。而NSP-BERT这个工作，则充分挖掘了NSP任务的潜力，并且启发我们哪怕局限在Prompt-based，其研究思路还有很大的发散空间。</p>
<p>所谓NSP任务，并不是真的去预测下一句，而是给定两个句子，判断这两个句子是否相邻。相应地，NSP-BERT的思路其实很简单，以分类问题为例，就是 <em>把输入视为第一句</em> ，然后 <em>将每个候选类别添加特定的Prompt作为第二句</em> ，逐一 <em>判断第一句与哪个第二句更加连贯</em> 。可以发现NSP-BERT思路跟PET很相似，其实Prompt-based的工作都很容易理解，难的是如何首先想到这样做。</p>
<p>下图演示了NSP-BERT做常见的一些NLU任务的参考Prompt方案，可以看到NSP-BERT能做到任务还是不少的：  </p>
<p><a href="/usr/uploads/2021/09/3815303747.png" title="点击查看原图"><img alt="NSP-BERT做常见NLU任务的一些Prompt" src="/usr/uploads/2021/09/3815303747.png" /></a></p>
<p>NSP-BERT做常见NLU任务的一些Prompt</p>
<p>其实看完这张图，就已经了解了NSP-BERT的大部分思想了，论文的其他部分，只不过是对这张图的细节进行展开描述而已。想要深入了解的同学，自行仔细阅读原论文即可。</p>
<p>如果说NSP-BERT这个模式，倒不是第一次出现，早前就有人提出用NLI模型来做Zero Shot的（参考<a href="https://jaketae.github.io/study/zero-shot-classification/">《NLI Models as Zero-Shot Classifiers》</a>），它的格式跟NSP是基本一致的，但需要标签语料有监督地微调，而纯无监督的NSP的利用，这还是第一次尝试。</p>
<h2 id="_2">实验效果</h2>
<p>有意思的是，对于我们来说，NSP-BERT是非常“接地气”的良心工作。比如，它是中国人写的，它的实验任务都是中文的（FewCLUE和DuEL2.0），并且开源了代码。下面是作者开源地址：</p>
<blockquote>
<p><strong>Github：<a href="https://github.com/sunyilgdx/NSP-BERT">https://github.com/sunyilgdx/NSP-BERT</a></strong></p>
</blockquote>
<p>最重要的是，NSP-BERT的效果真的不错：  </p>
<p><a href="/usr/uploads/2021/09/1037746494.png" title="点击查看原图"><img alt="NSP-BERT的Zero Shot效果" src="/usr/uploads/2021/09/1037746494.png" /></a></p>
<p>NSP-BERT的Zero Shot效果</p>
<p><a href="/usr/uploads/2021/09/3495277876.png" title="点击查看原图"><img alt="在实体链接任务上的效果" src="/usr/uploads/2021/09/3495277876.png" /></a></p>
<p>在实体链接任务上的效果</p>
<p><a href="/usr/uploads/2021/09/1096881522.png" title="点击查看原图"><img alt="模型规模对效果的影响" src="/usr/uploads/2021/09/1096881522.png" /></a></p>
<p>模型规模对效果的影响</p>
<p>总的来说，看完这些实验结果后，笔者只向对NSP说一句“失敬失敬”，这么一位模型界的大佬在面前，但却一直没有意识到，这必须得为NSP-BERT的作者的观察力点赞了。</p>
<h2 id="_3">文章小结</h2>
<p>本文分享了用BERT的预训练任务NSP来做Zero Shot的一篇论文，论文结果显示用NSP来做Zero Shot也能做到非常优秀的效果，也许假以时日，NSP要“崛起”了。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8671">https://spaces.ac.cn/archives/8671</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 10, 2021). 《曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8671">https://spaces.ac.cn/archives/8671</a></p>
<p>@online{kexuefm-8671,<br />
title={曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果},<br />
author={苏剑林},<br />
year={2021},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/8671}},<br />
} </p>
<hr />
<h2 id="_4">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>
    </div>
</body>
</html>