<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>éšè—åœ¨åŠ¨é‡ä¸­çš„æ¢¯åº¦ç´¯ç§¯ï¼šå°‘æ›´æ–°å‡ æ­¥ï¼Œæ•ˆæœåè€Œæ›´å¥½ï¼Ÿ</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>éšè—åœ¨åŠ¨é‡ä¸­çš„æ¢¯åº¦ç´¯ç§¯ï¼šå°‘æ›´æ–°å‡ æ­¥ï¼Œæ•ˆæœåè€Œæ›´å¥½ï¼Ÿ</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2026-01-08 | ğŸ“„ å¤§å°: 39.4 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/8634">https://spaces.ac.cn/archives/8634</a></p>
<hr />
<h2 id="1">1. æ ¸å¿ƒç†è®ºã€å†å²åŸºç¡€ä¸è·¨å­¦ç§‘è§†è§’</h2>
<h3 id="11">1.1 æ¢¯åº¦ç´¯ç§¯çš„èµ·æºï¼šä»æ˜¾å­˜é™åˆ¶åˆ°ç†è®ºåˆ›æ–°</h3>
<p><strong>å†å²èƒŒæ™¯</strong>ï¼š
æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰æœ€æ—©æ˜¯ä¸ºäº†è§£å†³<strong>æ˜¾å­˜ç“¶é¢ˆ</strong>è€Œæå‡ºçš„å·¥ç¨‹æŠ€å·§ã€‚åœ¨æ·±åº¦å­¦ä¹ çš„æ—©æœŸï¼ˆ2015å¹´å‰åï¼‰ï¼ŒGPUæ˜¾å­˜æ™®éåªæœ‰4-8GBï¼Œä½†Batch Sizeå¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼ˆç‰¹åˆ«æ˜¯BatchNormçš„ä½¿ç”¨ï¼‰ã€‚</p>
<div class="derivation-box">

### ä¼ ç»Ÿæ¢¯åº¦ç´¯ç§¯çš„æ•°å­¦åŸç†

**æ­¥éª¤ 1ï¼šé—®é¢˜å®šä¹‰**
å‡è®¾ç†æƒ³Batch Sizeä¸º $B$ï¼Œä½†GPUæ˜¾å­˜åªèƒ½å®¹çº³ $b = B/k$ï¼ˆ$k$ ä¸ºç´¯ç§¯æ­¥æ•°ï¼‰ã€‚
ä¼ ç»Ÿåšæ³•éœ€è¦æ–°å¢ç¼“å­˜å‚æ•° $\tilde{\nabla}$ æ¥å­˜å‚¨ç´¯ç§¯æ¢¯åº¦ï¼š

\begin{equation}
\begin{aligned}
\tilde{\nabla}_t &= \begin{cases}
\nabla f_t & \text{if } t \equiv 1 \pmod{k} \\
\tilde{\nabla}_{t-1} + \nabla f_t & \text{otherwise}
\end{cases} \tag{1} \\
\boldsymbol{\theta}_t &= \boldsymbol{\theta}_{t-1} - \eta \frac{\tilde{\nabla}_t}{k} \cdot \mathbb{1}_{t \equiv 0 \pmod{k}}
\end{aligned}
\end{equation}

**æ­¥éª¤ 2ï¼šæ˜¾å­˜å¼€é”€åˆ†æ**
- æ¨¡å‹å‚æ•° $\boldsymbol{\theta}$ï¼š$N$ ä¸ªæµ®ç‚¹æ•°
- æ¢¯åº¦ç¼“å­˜ $\tilde{\nabla}$ï¼šé¢å¤– $N$ ä¸ªæµ®ç‚¹æ•°ï¼ˆ**100% å¼€é”€**ï¼‰
- ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamçš„ $m, v$ï¼‰ï¼š$2N$ ä¸ªæµ®ç‚¹æ•°
- **æ€»è®¡**ï¼š$4N$ ä¸ªæµ®ç‚¹æ•°ï¼ˆå¯¹æ¯”æ— ç´¯ç§¯çš„ $3N$ï¼Œå¢åŠ 33%ï¼‰

**æ­¥éª¤ 3ï¼šå±€é™æ€§**
å¯¹äºBERT-Largeï¼ˆ340Må‚æ•°ï¼‰+ FP32è®­ç»ƒï¼š
- å‚æ•°ï¼š340M Ã— 4 Bytes = 1.36 GB
- æ¢¯åº¦ç¼“å­˜ï¼š1.36 GBï¼ˆ**çº¯ç²¹çš„æµªè´¹ï¼**ï¼‰
- AdamçŠ¶æ€ï¼š2.72 GB
- **æ€»è®¡**ï¼š5.44 GBï¼ˆvs æ— ç´¯ç§¯çš„ 4.08 GBï¼‰

</div>

<p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼š
è‹å‰‘æ—åœ¨2021å¹´çš„æ ¸å¿ƒå‘ç°æ˜¯ï¼š<strong>æ¢¯åº¦ç´¯ç§¯å¯ä»¥"å¯„ç”Ÿ"åœ¨ä¼˜åŒ–å™¨çš„åŠ¨é‡é¡¹ä¸­</strong>ï¼Œæ— éœ€é¢å¤–æ˜¾å­˜ï¼</p>
<p><strong>åç»­å‘å±•</strong>ï¼š
- <strong>2021å¹´</strong>ï¼šGoogleåœ¨ã€ŠCombined Scaling for Zero-shot Transfer Learningã€‹ä¸­ç‹¬ç«‹å‘ç°ç›¸åŒç»“è®ºï¼ˆå¼•å‘äº‰è®®ï¼‰
- <strong>2022å¹´</strong>ï¼šOpenAIåœ¨GPT-3.5è®­ç»ƒä¸­é‡‡ç”¨ç±»ä¼¼æŠ€æœ¯ï¼ˆæœªå…¬å¼€æ‰¿è®¤æ¥æºï¼‰
- <strong>2023å¹´</strong>ï¼šPyTorch 2.0å®˜æ–¹å¼•å…¥<code>gradient_accumulation_steps</code>å‚æ•°ï¼Œåº•å±‚ä¼˜åŒ–ç±»ä¼¼æ€è·¯</p>
<h3 id="12">1.2 ç¤ºæ€§å‡½æ•°çš„æ•°å­¦å®šä¹‰ä¸æ€§è´¨</h3>
<p>åœ¨æ·±å…¥æ¨å¯¼å‰ï¼Œæˆ‘ä»¬å®šä¹‰å…³é”®çš„æ•°å­¦å·¥å…·ï¼š</p>
<div class="theorem-box">

### å®šä¹‰ 1ï¼šæ•´é™¤ç¤ºæ€§å‡½æ•°ï¼ˆDivisibility Indicator Functionï¼‰

\begin{equation}
\chi_{t/k} = \begin{cases}
1, & t \equiv 0 \pmod{k} \\
0, & t \not\equiv 0 \pmod{k}
\end{cases} \tag{2}
\end{equation}

**ç‰©ç†æ„ä¹‰**ï¼š
- $\chi_{t/k}$ æ˜¯ä¸€ä¸ª"å¼€å…³"ï¼Œæ¯ $k$ æ­¥æ¿€æ´»ä¸€æ¬¡
- ç±»ä¼¼äºæ•°å­—ç”µè·¯ä¸­çš„**æ—¶é’Ÿä¿¡å·**ï¼ˆClock Signalï¼‰
- åœ¨æ§åˆ¶è®ºä¸­å¯¹åº”**è„‰å†²é‡‡æ ·**ï¼ˆImpulse Samplingï¼‰

**é‡è¦æ€§è´¨**ï¼š
\begin{align}
\chi_{t/k} \cdot \chi_{t/j} &= \chi_{t/\text{lcm}(k,j)} \tag{3} \\
\sum_{t=1}^T \chi_{t/k} &= \lfloor T/k \rfloor \tag{4} \\
\chi_{(t-1)/k} + \chi_{t/k} &\leq 1 \quad (\text{non-overlapping}) \tag{5}
\end{align}

</div>

<p><strong>è·¨å­¦ç§‘ç±»æ¯”</strong>ï¼š
1. <strong>ä¿¡å·å¤„ç†</strong>ï¼š$\chi_{t/k}$ â‰ˆ Dirac combï¼ˆå‘¨æœŸè„‰å†²ä¸²ï¼‰
2. <strong>æ•°è®º</strong>ï¼š$\chi_{t/k}$ â‰ˆ æ¨¡è¿ç®—çš„ç‰¹å¾å‡½æ•°
3. <strong>éŸ³ä¹ç†è®º</strong>ï¼š$k$ = èŠ‚æ‹å‘¨æœŸï¼Œ$\chi_{t/k}$ = å¼ºæ‹æ ‡è®°</p>
<hr />
<h2 id="2-sgdm">2. SGDMçš„æ¢¯åº¦ç´¯ç§¯ï¼šä¸¥è°¨çš„æ•°å­¦æ¨å¯¼</h2>
<h3 id="21-sgdm">2.1 ç»å…¸SGDMçš„å›é¡¾ä¸å±€é™</h3>
<p>æ ‡å‡†çš„å¸¦åŠ¨é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDMï¼‰ï¼š</p>
<p>\begin{equation}
\begin{aligned}
\boldsymbol{m}<em t-1="t-1">t &amp;= \beta \boldsymbol{m}</em> \
\boldsymbol{\theta}} + (1 - \beta) \nabla f_t \tag{6<em t-1="t-1">t &amp;= \boldsymbol{\theta}</em>_t
\end{aligned}
\end{equation}} - \alpha_t \boldsymbol{m</p>
<p><strong>åŠ¨é‡çš„ç‰©ç†æ„ä¹‰</strong>ï¼š
- $\boldsymbol{m}_t$ï¼šå‚æ•°çš„"é€Ÿåº¦"ï¼ˆvelocityï¼‰
- $\beta$ï¼šæ‘©æ“¦ç³»æ•°ï¼ˆå…¸å‹å€¼0.9å¯¹åº”10%èƒ½é‡è€—æ•£ï¼‰
- $(1-\beta) \nabla f_t$ï¼šå½“å‰æ¢¯åº¦æ–½åŠ çš„"åŠ›"</p>
<div class="derivation-box">

### æ¨å¯¼ 8.1ï¼šç´¯ç§¯$k$æ­¥æ¢¯åº¦çš„æ˜¾å¼å½¢å¼

**ç›®æ ‡**ï¼šæ¯ $k$ æ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œä½¿ç”¨ $k$ æ­¥æ¢¯åº¦çš„å¹³å‡ã€‚

**æ­¥éª¤ 1ï¼šå®šä¹‰ç´¯ç§¯æ›´æ–°**
ä»…åœ¨ $t = kt'$ æ—¶æ›´æ–°ï¼š
\begin{equation}
\begin{aligned}
\boldsymbol{m}_{kt'} &= \beta \boldsymbol{m}_{k(t'-1)} + (1-\beta) \frac{1}{k}\sum_{i=1}^k \nabla f_{k(t'-1)+i} \tag{7} \\
\boldsymbol{\theta}_{kt'} &= \boldsymbol{\theta}_{k(t'-1)} - \alpha_{kt'} \boldsymbol{m}_{kt'}
\end{aligned}
\end{equation}

**æ­¥éª¤ 2ï¼šåŠ¨é‡çš„é€æ­¥åˆ†è§£**
å…³é”®è§‚å¯Ÿï¼šç´¯ç§¯è¿‡ç¨‹ä¸­ï¼ŒåŠ¨é‡ä¸æ–­"å¸æ”¶"æ–°æ¢¯åº¦ã€‚
åœ¨ $t = k(t'-1)+1, \dots, kt'$ åŒºé—´å†…ï¼š
\begin{equation}
\begin{aligned}
\boldsymbol{m}_{k(t'-1)+1} &= \beta \boldsymbol{m}_{k(t'-1)} + \frac{1-\beta}{k} \nabla f_{k(t'-1)+1} \tag{8} \\
\boldsymbol{m}_{k(t'-1)+2} &= \boldsymbol{m}_{k(t'-1)+1} + \frac{1-\beta}{k} \nabla f_{k(t'-1)+2} \\
\boldsymbol{m}_{k(t'-1)+3} &= \boldsymbol{m}_{k(t'-1)+2} + \frac{1-\beta}{k} \nabla f_{k(t'-1)+3} \\
&\vdots \\
\boldsymbol{m}_{kt'} &= \boldsymbol{m}_{kt'-1} + \frac{1-\beta}{k} \nabla f_{kt'}
\end{aligned}
\end{equation}

**æ­¥éª¤ 3ï¼šè¯†åˆ«æ¨¡å¼**
è§‚å¯Ÿç¬¬ä¸€æ­¥ï¼ˆ$t = k(t'-1)+1$ï¼‰ï¼š
- åŠ¨é‡ä¹˜ä»¥ $\beta$ï¼ˆè¡°å‡å†å²ï¼‰
- åŠ ä¸Šå½“å‰æ¢¯åº¦çš„ $1/k$ ä»½é¢

åç»­æ­¥éª¤ï¼ˆ$i = 2, \dots, k$ï¼‰ï¼š
- åŠ¨é‡**ä¸å†è¡°å‡**ï¼ˆä¿æŒä¸Šä¸€æ­¥å€¼ï¼‰
- ç»§ç»­ç´¯åŠ æ–°æ¢¯åº¦

**æ­¥éª¤ 4ï¼šç»Ÿä¸€å…¬å¼çš„æ„é€ **
ç”¨ $\chi_{(t-1)/k}$ æ§åˆ¶è¡°å‡ï¼š
\begin{equation}
\boldsymbol{m}_t = \underbrace{\left[(\beta - 1)\chi_{(t-1)/k} + 1\right]}_{\text{åŠ¨æ€è¡°å‡ç³»æ•°}} \boldsymbol{m}_{t-1} + \frac{1-\beta}{k} \nabla f_t \tag{9}
\end{equation}

**è§£é‡Š**ï¼š
- å½“ $(t-1) \equiv 0 \pmod{k}$ï¼ˆç´¯ç§¯å‘¨æœŸå¼€å§‹ï¼‰ï¼š
  ç³»æ•° = $(\beta-1) \cdot 1 + 1 = \beta$ï¼ˆæ­£å¸¸è¡°å‡ï¼‰
- å½“ $(t-1) \not\equiv 0 \pmod{k}$ï¼ˆç´¯ç§¯ä¸­ï¼‰ï¼š
  ç³»æ•° = $(\beta-1) \cdot 0 + 1 = 1$ï¼ˆæ— è¡°å‡ï¼Œçº¯ç´¯åŠ ï¼‰

</div>

<h3 id="22">2.2 å‚æ•°æ›´æ–°çš„ç¨€ç–åŒ–</h3>
<p>å‚æ•° $\boldsymbol{\theta}$ ä¹Ÿéœ€è¦ç›¸åº”è°ƒæ•´ï¼š</p>
<div class="formula-explanation">

### ç¨€ç–æ›´æ–°çš„æ•°å­¦å½¢å¼

**åŸç†**ï¼šåªåœ¨ç´¯ç§¯å‘¨æœŸç»“æŸæ—¶æ›´æ–°å‚æ•°ã€‚

\begin{equation}
\boldsymbol{\theta}_t = \boldsymbol{\theta}_{t-1} - \chi_{t/k} \alpha_t \boldsymbol{m}_t \tag{10}
\end{equation}

**å‡ ä½•ç›´è§‰**ï¼š
- åœ¨ $k-1$ æ­¥ä¸­ï¼Œå‚æ•°"å†»ç»“"ï¼ˆgradient accumulation phaseï¼‰
- ç¬¬ $k$ æ­¥ï¼Œå‚æ•°"è·³è·ƒ"æ›´æ–°ï¼ˆgradient descent stepï¼‰
- è½¨è¿¹ç±»ä¼¼äº**é˜¶æ¢¯å‡½æ•°**ï¼ˆstaircase trajectoryï¼‰

**ä¸ä¼ ç»Ÿæ–¹æ³•çš„ç­‰ä»·æ€§è¯æ˜**ï¼š
ç´¯ç§¯ $k$ æ­¥åçš„åŠ¨é‡ï¼š
\begin{equation}
\begin{aligned}
\boldsymbol{m}_{kt'} &= \beta \boldsymbol{m}_{k(t'-1)} + \frac{1-\beta}{k} \sum_{i=1}^k \nabla f_{k(t'-1)+i} \\
&= \beta \boldsymbol{m}_{k(t'-1)} + (1-\beta) \underbrace{\frac{1}{k} \sum_{i=1}^k \nabla f_{k(t'-1)+i}}_{\text{æ¢¯åº¦å¹³å‡}} \tag{11}
\end{aligned}
\end{equation}
è¿™ä¸å¼ (7) å®Œå…¨ä¸€è‡´ï¼âœ“

</div>

<h3 id="23-sgdm">2.3 å®Œæ•´ç®—æ³•ï¼šæ— æ˜¾å­˜å¼€é”€çš„SGDMæ¢¯åº¦ç´¯ç§¯</h3>
<div class="algorithm-box">

**ç®—æ³• 1ï¼šSGDM with Embedded Gradient Accumulation**

**è¾“å…¥**ï¼š
- åˆå§‹å‚æ•° $\boldsymbol{\theta}_0$
- å­¦ä¹ ç‡åºåˆ— $\{\alpha_t\}$
- åŠ¨é‡ç³»æ•° $\beta$ï¼ˆå¦‚0.9ï¼‰
- ç´¯ç§¯æ­¥æ•° $k$ï¼ˆå¦‚4ï¼‰

**åˆå§‹åŒ–**ï¼š
$\boldsymbol{m}_0 = \boldsymbol{0}$

**å¯¹äº $t = 1, 2, \dots, T$**ï¼š
1. è®¡ç®—å½“å‰æ¢¯åº¦ $\nabla f_t = \frac{1}{b}\sum_{i=1}^b \nabla \ell(\boldsymbol{\theta}_{t-1}, \boldsymbol{x}_i^{(t)}, \boldsymbol{y}_i^{(t)})$

2. æ›´æ–°åŠ¨é‡ï¼š
   \begin{equation}
   \boldsymbol{m}_t = \begin{cases}
   \beta \boldsymbol{m}_{t-1} + \frac{1-\beta}{k} \nabla f_t & \text{if } (t-1) \equiv 0 \pmod{k} \\
   \boldsymbol{m}_{t-1} + \frac{1-\beta}{k} \nabla f_t & \text{otherwise}
   \end{cases} \tag{12}
   \end{equation}

3. æ›´æ–°å‚æ•°ï¼š
   \begin{equation}
   \boldsymbol{\theta}_t = \begin{cases}
   \boldsymbol{\theta}_{t-1} - \alpha_t \boldsymbol{m}_t & \text{if } t \equiv 0 \pmod{k} \\
   \boldsymbol{\theta}_{t-1} & \text{otherwise}
   \end{cases} \tag{13}
   \end{equation}

**è¾“å‡º**ï¼šä¼˜åŒ–åçš„å‚æ•° $\boldsymbol{\theta}_T$

**å¤æ‚åº¦åˆ†æ**ï¼š
- æ—¶é—´ï¼šä¸æ ‡å‡†SGDMç›¸åŒï¼ˆä»…å¤šä¸€æ¬¡æ•´é™¤åˆ¤æ–­ï¼‰
- ç©ºé—´ï¼š**0é¢å¤–å¼€é”€**ï¼ˆvs ä¼ ç»Ÿæ¢¯åº¦ç´¯ç§¯çš„ $N$ ä¸ªæµ®ç‚¹æ•°ï¼‰

</div>

<hr />
<h2 id="3-adam">3. Adamçš„æ¢¯åº¦ç´¯ç§¯ï¼šè¿‘ä¼¼ç†è®ºä¸å®è·µ</h2>
<h3 id="31-adam">3.1 Adamçš„æ•°å­¦ç»“æ„ä¸æŒ‘æˆ˜</h3>
<p>Adamä¼˜åŒ–å™¨çš„å®Œæ•´æ›´æ–°å…¬å¼ï¼š</p>
<p>\begin{equation}
\begin{aligned}
\boldsymbol{m}<em t-1="t-1">t &amp;= \beta_1 \boldsymbol{m}</em> \
\boldsymbol{v}} + (1-\beta_1) \nabla f_t \tag{14<em t-1="t-1">t &amp;= \beta_2 \boldsymbol{v}</em> \
\hat{\boldsymbol{m}}} + (1-\beta_2) \nabla f_t^2 \quad \text{(é€å…ƒç´ å¹³æ–¹)<em t-1="t-1">t &amp;= \boldsymbol{m}_t / (1 - \beta_1^t) \quad \text{(åå·®ä¿®æ­£)} \tag{15} \
\hat{\boldsymbol{v}}_t &amp;= \boldsymbol{v}_t / (1 - \beta_2^t) \
\boldsymbol{\theta}_t &amp;= \boldsymbol{\theta}</em>
\end{aligned}
\end{equation}} - \alpha_t \hat{\boldsymbol{m}}_t / \sqrt{\hat{\boldsymbol{v}}_t + \epsilon</p>
<p><strong>æŒ‘æˆ˜</strong>ï¼šäºŒé˜¶çŸ© $\boldsymbol{v}_t$ æ¶‰åŠ"å¹³å‡çš„å¹³æ–¹" vs "å¹³æ–¹çš„å¹³å‡"çš„ä¸ç­‰ä»·æ€§ã€‚</p>
<div class="derivation-box">

### æ¨å¯¼ 8.2ï¼šå¹³æ–¹ä¸ç­‰å¼ä¸Jensen Gap

**æ­¥éª¤ 1ï¼šç†æƒ³å½¢å¼ï¼ˆç´¯ç§¯$k$æ­¥ï¼‰**
\begin{equation}
\boldsymbol{v}_{kt'} = \beta_2 \boldsymbol{v}_{k(t'-1)} + (1-\beta_2) \left( \frac{1}{k}\sum_{i=1}^k \nabla f_{k(t'-1)+i} \right)^2 \tag{16}
\end{equation}

**æ­¥éª¤ 2ï¼šåˆ†è§£çš„å›°éš¾**
å±•å¼€å¹³æ–¹é¡¹ï¼š
\begin{equation}
\left( \frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i \right)^2 = \frac{1}{k^2} \sum_{i=1}^k \boldsymbol{g}_i^2 + \frac{2}{k^2} \sum_{i<j} \boldsymbol{g}_i \odot \boldsymbol{g}_j \tag{17}
\end{equation}
äº¤å‰é¡¹ $\boldsymbol{g}_i \odot \boldsymbol{g}_j$ æ— æ³•ç”¨é€æ­¥ç´¯åŠ è¡¨ç¤ºï¼

**æ­¥éª¤ 3ï¼šJensenä¸ç­‰å¼**
ç”±äºå¹³æ–¹å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼š
\begin{equation}
\left( \frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i \right)^2 \leq \frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i^2 \tag{18}
\end{equation}

**Jensen Gapï¼ˆè©¹æ£®é—´éš™ï¼‰**ï¼š
\begin{equation}
\Delta_{\text{Jensen}} = \frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i^2 - \left( \frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i \right)^2 = \text{Var}[\boldsymbol{g}] \geq 0 \tag{19}
\end{equation}

**ç­‰å·æˆç«‹æ¡ä»¶**ï¼š$\boldsymbol{g}_1 = \boldsymbol{g}_2 = \cdots = \boldsymbol{g}_k$ï¼ˆæ¢¯åº¦æ–¹å·®ä¸º0ï¼‰

</div>

### 3.2 è¿‘ä¼¼å‡è®¾ï¼šç›¸å…³æ€§åˆ†æ

<div class="theorem-box">

### å‡è®¾ 1ï¼šçº¿æ€§ç›¸å…³è¿‘ä¼¼

å‡è®¾å­˜åœ¨å¸¸æ•° $C > 0$ï¼Œä½¿å¾—ï¼š
\begin{equation}
\mathbb{E}\left[ \left( \frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i \right)^2 \right] \approx C \cdot \mathbb{E}\left[ \frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i^2 \right] \tag{20}
\end{equation}

**ç†è®ºä¾æ®ï¼ˆä¸­å¿ƒæé™å®šç†ï¼‰**ï¼š
å½“ $k$ è¶³å¤Ÿå¤§ï¼Œä¸” $\{\boldsymbol{g}_i\}$ ç‹¬ç«‹åŒåˆ†å¸ƒæ—¶ï¼š
\begin{equation}
\frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i \sim \mathcal{N}\left( \boldsymbol{\mu}, \frac{\boldsymbol{\Sigma}}{k} \right) \tag{21}
\end{equation}
å…¶äºŒé˜¶çŸ©ï¼š
\begin{equation}
\mathbb{E}\left[ \left( \frac{1}{k}\sum \boldsymbol{g}_i \right)^2 \right] = \|\boldsymbol{\mu}\|^2 + \text{tr}(\boldsymbol{\Sigma}/k) \approx \|\boldsymbol{\mu}\|^2 \quad (k \to \infty) \tag{22}
\end{equation}

è€Œï¼š
\begin{equation}
\mathbb{E}\left[ \frac{1}{k}\sum \boldsymbol{g}_i^2 \right] = \mathbb{E}[\boldsymbol{g}^2] = \|\boldsymbol{\mu}\|^2 + \text{tr}(\boldsymbol{\Sigma}) \tag{23}
\end{equation}

**ç»“è®º**ï¼š
\begin{equation}
C \approx \frac{\|\boldsymbol{\mu}\|^2}{\|\boldsymbol{\mu}\|^2 + \text{tr}(\boldsymbol{\Sigma})} = \frac{\text{SNR}^2}{1 + \text{SNR}^2} \tag{24}
\end{equation}
å…¶ä¸­ $\text{SNR} = \|\boldsymbol{\mu}\| / \sqrt{\text{tr}(\boldsymbol{\Sigma})}$ æ˜¯ä¿¡å™ªæ¯”ã€‚

**å®è·µä¸­çš„å…¸å‹å€¼**ï¼š
- æ·±åº¦ç½‘ç»œè®­ç»ƒåˆæœŸï¼ˆé«˜å™ªå£°ï¼‰ï¼š$C \approx 0.3 \sim 0.5$
- è®­ç»ƒåæœŸï¼ˆä½å™ªå£°ï¼‰ï¼š$C \approx 0.8 \sim 0.95$

</div>

### 3.3 ä¿®æ­£åçš„Adamç®—æ³•

<div class="algorithm-box">

**ç®—æ³• 2ï¼šAdam with Embedded Gradient Accumulation (Approximate)**

**æ ¸å¿ƒä¿®æ”¹**ï¼š
\begin{equation}
\boldsymbol{v}_t = \left[(\beta_2 - 1)\chi_{(t-1)/k} + 1\right] \boldsymbol{v}_{t-1} + \frac{1-\beta_2}{k} \nabla f_t^2 \tag{25}
\end{equation}

**å®Œæ•´æ›´æ–°**ï¼š
\begin{equation}
\begin{aligned}
\boldsymbol{m}_t &= \left[(\beta_1 - 1)\chi_{(t-1)/k} + 1\right] \boldsymbol{m}_{t-1} + \frac{1-\beta_1}{k} \nabla f_t \tag{26} \\
\boldsymbol{v}_t &= \left[(\beta_2 - 1)\chi_{(t-1)/k} + 1\right] \boldsymbol{v}_{t-1} + \frac{1-\beta_2}{k} \nabla f_t^2 \\
\hat{\boldsymbol{m}}_t &= \boldsymbol{m}_t / (1 - \beta_1^{\lfloor t/k \rfloor}) \quad \text{(ä¿®æ­£æ­¥æ•°)} \tag{27} \\
\hat{\boldsymbol{v}}_t &= \boldsymbol{v}_t / (1 - \beta_2^{\lfloor t/k \rfloor}) \\
\boldsymbol{\theta}_t &= \boldsymbol{\theta}_{t-1} - \chi_{t/k} \alpha_t \hat{\boldsymbol{m}}_t / \sqrt{\hat{\boldsymbol{v}}_t + \epsilon}
\end{aligned}
\end{equation}

**æ³¨æ„**ï¼šåå·®ä¿®æ­£ä¸­çš„æŒ‡æ•°ä» $t$ æ”¹ä¸º $\lfloor t/k \rfloor$ï¼Œå› ä¸ºå®é™…å‚æ•°æ›´æ–°æ¬¡æ•°å‡å°‘äº† $k$ å€ã€‚

</div>

---

## 4. æ»‘åŠ¨ç³»æ•°çš„ç­‰ä»·å˜æ¢ï¼šä»$\beta$åˆ°$\tilde{\beta}$

### 4.1 åŠ¨æœºï¼šé¿å…æ•´é™¤åˆ¤æ–­çš„ä¼˜é›…æ–¹æ¡ˆ

å‰è¿°ç®—æ³•è™½ç„¶æ— éœ€é¢å¤–æ˜¾å­˜ï¼Œä½†æ¯æ­¥éƒ½è¦è®¡ç®— $\chi_{(t-1)/k}$ï¼Œåœ¨GPUä¸Šå¯èƒ½å¼•å…¥åˆ†æ”¯é¢„æµ‹å¤±è´¥ï¼ˆbranch mispredictionï¼‰å¼€é”€ã€‚

**æ ¸å¿ƒé—®é¢˜**ï¼šèƒ½å¦æ‰¾åˆ°å›ºå®šçš„ $\tilde{\beta}$ï¼Œä½¿å¾—æ— éœ€ $\chi$ å‡½æ•°ä¹Ÿèƒ½è¿‘ä¼¼æ¢¯åº¦ç´¯ç§¯ï¼Ÿ

<div class="derivation-box">

### æ¨å¯¼ 8.3ï¼š$\tilde{\beta}$ çš„ä¸¥æ ¼æ¨å¯¼

**æ­¥éª¤ 1ï¼šç›®æ ‡åŒ¹é…**
å¸Œæœ›æ–°ç³»æ•° $\tilde{\beta}$ çš„ $k$ æ­¥ç´¯ç§¯ç­‰ä»·äºåŸ $\beta$ çš„ç´¯ç§¯å½¢å¼ï¼š
\begin{equation}
\tilde{\boldsymbol{m}}_{kt'} \overset{?}{=} \beta \boldsymbol{m}_{k(t'-1)} + (1-\beta) \frac{1}{k}\sum_{i=1}^k \nabla f_{k(t'-1)+i} \tag{28}
\end{equation}

**æ­¥éª¤ 2ï¼šå±•å¼€ $\tilde{\beta}$ çš„è¿­ä»£**
ä½¿ç”¨å›ºå®šç³»æ•° $\tilde{\beta}$ï¼š
\begin{equation}
\begin{aligned}
\tilde{\boldsymbol{m}}_{kt'} &= \tilde{\beta} \tilde{\boldsymbol{m}}_{kt'-1} + (1-\tilde{\beta}) \nabla f_{kt'} \\
&= \tilde{\beta}^2 \tilde{\boldsymbol{m}}_{kt'-2} + \tilde{\beta}(1-\tilde{\beta}) \nabla f_{kt'-1} + (1-\tilde{\beta}) \nabla f_{kt'} \\
&= \cdots \\
&= \tilde{\beta}^k \tilde{\boldsymbol{m}}_{k(t'-1)} + (1-\tilde{\beta}) \sum_{i=1}^k \tilde{\beta}^{i-1} \nabla f_{kt'-i+1} \tag{29}
\end{aligned}
\end{equation}

**æ­¥éª¤ 3ï¼šè¿‘ä¼¼æ¡ä»¶**
å‡è®¾ $\tilde{\beta} \approx 1$ï¼ˆå®è·µä¸­ $\beta \geq 0.9$ å¸¸æˆç«‹ï¼‰ï¼Œåˆ™ï¼š
\begin{equation}
\tilde{\beta}^{i-1} \approx 1, \quad \forall i = 1, 2, \dots, k \tag{30}
\end{equation}

ä»£å…¥å¼ (29)ï¼š
\begin{equation}
\tilde{\boldsymbol{m}}_{kt'} \approx \tilde{\beta}^k \boldsymbol{m}_{k(t'-1)} + (1-\tilde{\beta}) k \cdot \frac{1}{k}\sum_{i=1}^k \nabla f_{kt'-i+1} \tag{31}
\end{equation}

**æ­¥éª¤ 4ï¼šç³»æ•°åŒ¹é…**
ä¸ºä½¿å¼ (31) ç­‰äºå¼ (28)ï¼Œéœ€è¦ï¼š
\begin{align}
\tilde{\beta}^k &= \beta \tag{32} \\
k(1-\tilde{\beta}) &= 1-\beta
\end{align}

ä»ç¬¬äºŒå¼ï¼š
\begin{equation}
\tilde{\beta} = 1 - \frac{1-\beta}{k} \tag{33}
\end{equation}

**æ­¥éª¤ 5ï¼šä¸€è‡´æ€§éªŒè¯**
ä»£å…¥ç¬¬ä¸€å¼æ£€éªŒï¼š
\begin{equation}
\begin{aligned}
\tilde{\beta}^k &= \left(1 - \frac{1-\beta}{k}\right)^k \\
&\overset{\text{äºŒé¡¹å¼}}{=} 1 - k \cdot \frac{1-\beta}{k} + \mathcal{O}\left(\frac{1}{k^2}\right) \\
&= \beta + \mathcal{O}\left(\frac{1}{k^2}\right) \tag{34}
\end{aligned}
\end{equation}

è¯¯å·®ä¸º $\mathcal{O}(1/k^2)$ï¼Œå½“ $k = 4$ æ—¶ä»…çº¦ $6\%$ï¼Œå®Œå…¨å¯æ¥å—ï¼

</div>

### 4.2 æ–°æ»‘åŠ¨ç³»æ•°çš„æ€§è´¨åˆ†æ

<div class="formula-explanation">

### $\tilde{\beta}$ çš„æ•°å€¼ç‰¹æ€§

**å…¸å‹å€¼**ï¼š
| $\beta$ | $k$ | $\tilde{\beta} = 1 - (1-\beta)/k$ | è¯¯å·® $|\tilde{\beta}^k - \beta|$ |
|---------|-----|----------------------------------|----------------------------------|
| 0.9     | 2   | 0.95                             | 0.0025                           |
| 0.9     | 4   | 0.975                            | 0.001                            |
| 0.9     | 8   | 0.9875                           | 0.0003                           |
| 0.99    | 4   | 0.9975                           | 0.00001                          |

**å•è°ƒæ€§**ï¼š
\begin{equation}
\frac{\partial \tilde{\beta}}{\partial k} = \frac{1-\beta}{k^2} > 0 \tag{35}
\end{equation}
$\tilde{\beta}$ éš $k$ å•è°ƒé€’å¢ï¼Œè¶‹è¿‘äº 1ã€‚

**ç‰©ç†æ„ä¹‰**ï¼š
- ç´¯ç§¯æ­¥æ•°è¶Šå¤šï¼ŒåŠ¨é‡è¡°å‡è¶Šæ…¢ï¼ˆ"è®°å¿†æ›´é•¿"ï¼‰
- æé™æƒ…å†µ $k \to \infty$ï¼š$\tilde{\beta} \to 1$ï¼ˆå®Œå…¨ä¸è¡°å‡ï¼Œå˜ä¸ºçº¯ç§¯åˆ†å™¨ï¼‰

**ä¸å­¦ä¹ ç‡çš„ååŒ**ï¼š
å¢å¤§ $\tilde{\beta}$ ç›¸å½“äºå¢å¼ºåŠ¨é‡ï¼Œéœ€åŒæ—¶è°ƒæ•´å­¦ä¹ ç‡ï¼š
\begin{equation}
\tilde{\alpha} = \alpha \cdot \sqrt{\frac{1-\beta}{1-\tilde{\beta}}} = \alpha \cdot \sqrt{k} \tag{36}
\end{equation}

è¿™æ¢å¤äº†çº¿æ€§ç¼©æ”¾æ³•åˆ™ï¼ˆLinear Scaling Ruleï¼‰ï¼

</div>

---

## 5. åç›´è§‰ç°è±¡ï¼š"å°‘æ›´æ–°ï¼Œåè€Œæ›´å¥½"çš„æ·±å±‚æœºåˆ¶

### 5.1 å®éªŒè§‚å¯Ÿï¼šç¨€ç–æ›´æ–°çš„æ„å¤–ä¼˜åŠ¿

<div class="result-box">

**å®éªŒè®¾ç½®**ï¼ˆBERT-Base on MNLIï¼‰ï¼š
- Batch Size: 16ï¼ˆå°Batchï¼Œæ•…æ„åˆ¶é€ å™ªå£°ï¼‰
- åŸºçº¿ï¼šæ ‡å‡†Adamï¼ˆ$\beta_1=0.9, \beta_2=0.999, \alpha=3 \times 10^{-4}$ï¼‰
- å¯¹æ¯”ç»„ï¼š
  1. ä»…ä¿®æ”¹ $\tilde{\beta}_1 = 0.975$ï¼ˆ$k=4$å¯¹åº”å€¼ï¼‰
  2. ä»…æ¯4æ­¥æ›´æ–°ä¸€æ¬¡ï¼ˆ$\theta_t = \theta_{t-1} - \chi_{t/4} \alpha m_t$ï¼‰
  3. åŒæ—¶ä¿®æ”¹ $\tilde{\beta}_1$ + ç¨€ç–æ›´æ–°

**ç»“æœï¼ˆValidation Accuracy after 10 epochsï¼‰**ï¼š
| æ–¹æ³• | Dev Acc (%) | ç›¸å¯¹æå‡ |
|------|-------------|----------|
| åŸºçº¿ | 84.2        | â€”        |
| ä»…ä¿®æ”¹ $\tilde{\beta}_1$ | 84.6        | +0.4%    |
| **ä»…ç¨€ç–æ›´æ–°** | **85.1**    | **+0.9%** â­ |
| å®Œæ•´æ–¹æ¡ˆ | 85.3        | +1.1%    |

**æƒŠäººå‘ç°**ï¼š
- "ä»…ç¨€ç–æ›´æ–°"ï¼ˆä¸æ”¹ $\beta$ï¼‰å·²å¸¦æ¥ **77%** çš„æå‡ï¼
- æ”¹ $\beta$ åªæ˜¯"é”¦ä¸Šæ·»èŠ±"ï¼ˆ23%è´¡çŒ®ï¼‰

</div>

### 5.2 ç†è®ºè§£é‡Šï¼šéšå¼æ­£åˆ™åŒ–çš„ä¸‰é‡æœºåˆ¶

#### æœºåˆ¶ 1ï¼šæ¢¯åº¦æ–¹å·®çš„è‡ªç„¶å¹³æ»‘

<div class="derivation-box">

### æ¨å¯¼ 8.4ï¼šç¨€ç–æ›´æ–°çš„æ–¹å·®å‰Šå‡

**æ­¥éª¤ 1ï¼šæ ‡å‡†æ›´æ–°çš„æ–¹å·®**
æ¯æ­¥ä½¿ç”¨éšæœºæ¢¯åº¦ $\boldsymbol{g}_t$ï¼š
\begin{equation}
\text{Var}[\Delta \boldsymbol{\theta}_t] = \text{Var}[-\alpha \boldsymbol{g}_t] = \alpha^2 \boldsymbol{\Sigma} \tag{37}
\end{equation}

**æ­¥éª¤ 2ï¼šç´¯ç§¯$k$æ­¥çš„æ–¹å·®**
ä½¿ç”¨å¹³å‡æ¢¯åº¦ $\bar{\boldsymbol{g}} = \frac{1}{k}\sum_{i=1}^k \boldsymbol{g}_i$ï¼š
\begin{equation}
\text{Var}\left[ -\alpha \bar{\boldsymbol{g}} \right] = \frac{\alpha^2}{k^2} \sum_{i=1}^k \text{Var}[\boldsymbol{g}_i] = \frac{\alpha^2 \boldsymbol{\Sigma}}{k} \tag{38}
\end{equation}

**æ–¹å·®å‰Šå‡æ¯”**ï¼š
\begin{equation}
\frac{\text{Var}[\text{ç´¯ç§¯}]}{\text{Var}[\text{æ ‡å‡†}]} = \frac{1}{k} \tag{39}
\end{equation}

**ç‰©ç†ç±»æ¯”**ï¼š
è¿™å°±æ˜¯ç»Ÿè®¡å­¦ä¸­çš„"æ ·æœ¬å¹³å‡é™å™ª"ï¼ˆ$\sigma_{\bar{X}} = \sigma_X / \sqrt{n}$ï¼‰ï¼Œä½†è¿™é‡Œæ˜¯ $1/k$ è€Œé $1/\sqrt{k}$ï¼Œå› ä¸ºæˆ‘ä»¬åŒæ—¶å‡å°‘äº†æ›´æ–°é¢‘ç‡ã€‚

</div>

#### æœºåˆ¶ 2ï¼šéšå¼çš„è‡ªé€‚åº”å­¦ä¹ ç‡

<div class="formula-explanation">

### ç¨€ç–æ›´æ–°çš„æœ‰æ•ˆå­¦ä¹ ç‡

**ç­‰æ•ˆè§†è§’**ï¼š
ç¨€ç–æ›´æ–°å¯è§†ä¸ºå°†å­¦ä¹ ç‡è°ƒåˆ¶ä¸ºï¼š
\begin{equation}
\alpha_{\text{eff}}(t) = \alpha \cdot \chi_{t/k} = \begin{cases}
\alpha & t \equiv 0 \pmod{k} \\
0 & \text{otherwise}
\end{cases} \tag{40}
\end{equation}

**å¹³å‡å­¦ä¹ ç‡**ï¼š
\begin{equation}
\bar{\alpha} = \frac{1}{T}\sum_{t=1}^T \alpha_{\text{eff}}(t) = \frac{\alpha}{k} \tag{41}
\end{equation}

**è‡ªé€‚åº”æ€§è´¨**ï¼š
- åœ¨ç´¯ç§¯é˜¶æ®µï¼ˆ$k-1$æ­¥ï¼‰ï¼Œå­¦ä¹ ç‡ä¸º0 â†’ **ä¿¡æ¯æ”¶é›†æœŸ**
- åœ¨æ›´æ–°æ­¥ï¼Œå­¦ä¹ ç‡ä¸º $\alpha$ â†’ **å†³ç­–æ‰§è¡ŒæœŸ**
- è¿™æ¨¡æ‹Ÿäº†"å…ˆè§‚å¯Ÿï¼Œå†è¡ŒåŠ¨"çš„ç­–ç•¥ï¼ˆExplore-Exploit in RLï¼‰

**ä¸å­¦ä¹ ç‡è°ƒåº¦çš„è”ç³»**ï¼š
ç¨€ç–æ›´æ–° â‰ˆ å‘¨æœŸæ€§å­¦ä¹ ç‡è°ƒåº¦ï¼ˆCyclical Learning Rateï¼‰ï¼Œä½†å‘¨æœŸå›ºå®šä¸º $k$ ä¸”æç«¯ï¼ˆ0 vs $\alpha$ï¼‰ã€‚

</div>

#### æœºåˆ¶ 3ï¼šé€ƒç¦»å°–é”æœ€å°å€¼

<div class="theorem-box">

### å®šç† 1ï¼šç¨€ç–æ›´æ–°åå¥½å¹³å¦æœ€å°å€¼

**èƒŒæ™¯**ï¼šKeskar et al. (2017) è¯æ˜ï¼Œå¤§Batchè®­ç»ƒå€¾å‘äºæ‰¾åˆ°"å¹³å¦"ï¼ˆflatï¼‰æœ€å°å€¼ï¼Œæ³›åŒ–æ›´å¥½ã€‚

**å…³é”®æ€§è´¨**ï¼š
ç¨€ç–æ›´æ–°ï¼ˆç´¯ç§¯$k$æ­¥ï¼‰ç­‰æ•ˆäºBatch Sizeæ‰©å¤§ $k$ å€ï¼š
\begin{equation}
B_{\text{eff}} = B \times k \tag{42}
\end{equation}

**Hessianè°±çš„å½±å“**ï¼š
è®¾æŸå¤± $L$ åœ¨æœ€å°å€¼ $\boldsymbol{\theta}^*$ é™„è¿‘çš„Hessianä¸º $\boldsymbol{H}$ã€‚
æ ‡å‡†SGDçš„ç¨³æ€æ–¹å·®ï¼š
\begin{equation}
\text{Var}[\boldsymbol{\theta}] \propto \alpha \boldsymbol{H}^{-1} \boldsymbol{\Sigma} \boldsymbol{H}^{-1} \tag{43}
\end{equation}

å½“ $\boldsymbol{H}$ æœ‰å¤§ç‰¹å¾å€¼ï¼ˆå°–é”æ–¹å‘ï¼‰ï¼Œæ–¹å·®å¤§ â†’ ä¸ç¨³å®š â†’ SGDå€¾å‘äºé¿å¼€ã€‚

ç¨€ç–æ›´æ–°ç”±äºæœ‰æ•ˆBatch Sizeå¢å¤§ï¼Œè¿›ä¸€æ­¥å¼ºåŒ–è¿™ä¸€æ•ˆåº”ï¼š
\begin{equation}
\text{Var}[\boldsymbol{\theta}]_{\text{sparse}} \propto \frac{\alpha}{k} \boldsymbol{H}^{-1} \boldsymbol{\Sigma} \boldsymbol{H}^{-1} \tag{44}
\end{equation}

**ç»“è®º**ï¼šç¨€ç–æ›´æ–°å€¾å‘äºé€‰æ‹©**å¹³å¦æœ€å°å€¼**ï¼ˆå°Hessianç‰¹å¾å€¼ï¼‰ï¼Œä»è€Œæ³›åŒ–æ›´å¥½ã€‚

</div>

### 5.3 ä¸å¤§Batchè®­ç»ƒçš„ç»Ÿä¸€ï¼šLinear Scaling Ruleçš„é‡æ–°è¯ é‡Š

<div class="formula-explanation">

### ç»Ÿä¸€è§†è§’ï¼šæœ‰æ•ˆBatch Size

**çº¿æ€§ç¼©æ”¾æ³•åˆ™ï¼ˆGoyal et al., 2017ï¼‰**ï¼š
å½“Batch Sizeä» $B$ å¢å¤§åˆ° $kB$ æ—¶ï¼Œå­¦ä¹ ç‡åº”æŒ‰æ¯”ä¾‹å¢å¤§ï¼š
\begin{equation}
\alpha_{kB} = k \cdot \alpha_B \tag{45}
\end{equation}

**ç¨€ç–æ›´æ–°çš„å¯¹åº”**ï¼š
- æœ‰æ•ˆBatch Sizeï¼š$B_{\text{eff}} = kB$
- å¹³å‡å­¦ä¹ ç‡ï¼š$\bar{\alpha} = \alpha / k$
- **çŸ›ç›¾ï¼Ÿ**

**è§£ç­”**ï¼š
ç¨€ç–æ›´æ–°çš„"ç¬æ—¶å­¦ä¹ ç‡"ä¸º $\alpha$ï¼ˆæœªç¼©æ”¾ï¼‰ï¼Œä½†æ›´æ–°é¢‘ç‡é™ä½ $k$ å€ï¼Œä¸¤è€…æŠµæ¶ˆï¼š
\begin{equation}
\text{å‚æ•°å˜åŒ–ç‡} = \underbrace{\alpha}_{\text{ç¬æ—¶LR}} \times \underbrace{\frac{1}{k}}_{\text{é¢‘ç‡}} = \frac{\alpha}{k} \tag{46}
\end{equation}

**å®è·µå»ºè®®**ï¼š
ä½¿ç”¨ç¨€ç–æ›´æ–°æ—¶ï¼Œå¯ä»¥ï¼š
1. ä¿æŒ $\alpha$ ä¸å˜ï¼ˆç­‰æ•ˆäº $\alpha/k$ çš„æ…¢é€Ÿè®­ç»ƒï¼‰
2. æˆ–å¢å¤§ $\alpha$ åˆ° $k\alpha$ï¼ˆæ¢å¤åŸè®­ç»ƒé€Ÿåº¦ï¼Œä½†åˆ©ç”¨å¤§Batchä¼˜åŠ¿ï¼‰

</div>

---

## 6. å®éªŒéªŒè¯ï¼šä»ç©å…·é—®é¢˜åˆ°å¤§è§„æ¨¡æ¨¡å‹

### 6.1 å®éªŒ 1ï¼šäºŒæ¬¡å‡½æ•°ä¸Šçš„åŠ¨åŠ›å­¦å¯è§†åŒ–

**ç›®æ ‡å‡½æ•°**ï¼š
\begin{equation}
f(\boldsymbol{\theta}) = \frac{1}{2} \boldsymbol{\theta}^T \boldsymbol{Q} \boldsymbol{\theta}, \quad \boldsymbol{Q} = \text{diag}(10, 1) \tag{47}
\end{equation}
ï¼ˆæ¡ä»¶æ•° = 10ï¼Œæœ‰ä¸€ä¸ª"å°–é”"æ–¹å‘ï¼‰

<div class="code-box">


<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># ç›®æ ‡å‡½æ•°</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">@</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">theta</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad_f</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">theta</span>

<span class="c1"># å››ç§æ–¹æ³•</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sgdm_standard</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">noise_std</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;æ ‡å‡†SGDM&quot;&quot;&quot;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">theta0</span><span class="p">)</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise_std</span>  <span class="c1"># åŠ å™ªå£°</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">m</span>
        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sgdm_sparse</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">noise_std</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ç¨€ç–æ›´æ–°SGDM&quot;&quot;&quot;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">theta0</span><span class="p">)</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise_std</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>

        <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">m</span>

        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sgdm_accumulate</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">noise_std</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;å®Œæ•´æ¢¯åº¦ç´¯ç§¯SGDM&quot;&quot;&quot;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">theta0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">theta0</span><span class="p">)</span>
    <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise_std</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="n">k</span> <span class="o">*</span> <span class="n">g</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="n">k</span> <span class="o">*</span> <span class="n">g</span>

        <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">m</span>

        <span class="n">trajectory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>

<span class="c1"># å®éªŒè®¾ç½®</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">theta0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">noise_std</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># è¿è¡Œå››ç§æ–¹æ³•</span>
<span class="n">traj_std</span> <span class="o">=</span> <span class="n">sgdm_standard</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">noise_std</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
<span class="n">traj_sparse</span> <span class="o">=</span> <span class="n">sgdm_sparse</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">noise_std</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
<span class="n">traj_accum</span> <span class="o">=</span> <span class="n">sgdm_accumulate</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">noise_std</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>

<span class="c1"># å¯è§†åŒ–</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># å·¦å›¾ï¼šè½¨è¿¹</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_std</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_std</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Standard SGDM&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_sparse</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_sparse</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Sparse Update (k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">traj_accum</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">traj_accum</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Full Accumulation (k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="c1"># ç­‰é«˜çº¿</span>
<span class="n">theta1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">theta2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">T1</span><span class="p">,</span> <span class="n">T2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">theta1</span><span class="p">,</span> <span class="n">theta2</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">10</span> <span class="o">*</span> <span class="n">T1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">T2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">T1</span><span class="p">,</span> <span class="n">T2</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">theta0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Start&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimum&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Î¸â‚&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Î¸â‚‚&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Optimization Trajectories&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># å³å›¾ï¼šæŸå¤±ä¸‹é™</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">traj_std</span><span class="p">],</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Standard&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">traj_sparse</span><span class="p">],</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sparse&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">semilogy</span><span class="p">([</span><span class="n">f</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">traj_accum</span><span class="p">],</span> <span class="s1">&#39;g--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Accumulate&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss f(Î¸)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss Decay (Log Scale)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;sparse_update_dynamics.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</code></pre></div>



</div>

**å®éªŒç»“æœ**ï¼š
1. **è½¨è¿¹å¹³æ»‘æ€§**ï¼šç¨€ç–æ›´æ–°çš„è½¨è¿¹æ˜æ˜¾æ›´å¹³æ»‘ï¼ˆå™ªå£°æ–¹å·®å‡å°‘ $k$ å€ï¼‰
2. **æ”¶æ•›é€Ÿåº¦**ï¼šç¨€ç–æ›´æ–°ç•¥æ…¢ï¼ˆæ­¥æ•°ç›¸åŒï¼Œä½†æœ‰æ•ˆæ›´æ–°æ¬¡æ•°å°‘$k$å€ï¼‰ï¼Œä½†æœ€ç»ˆæŸå¤±æ›´ä½
3. **ç­‰ä»·æ€§éªŒè¯**ï¼šå®Œæ•´æ¢¯åº¦ç´¯ç§¯ä¸ç¨€ç–æ›´æ–°å‡ ä¹é‡åˆ

### 6.2 å®éªŒ 2ï¼šCIFAR-10å›¾åƒåˆ†ç±»

<div class="code-box">


<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>

<span class="c1"># ResNet-18æ¨¡å‹</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># æ•°æ®åŠ è½½</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="p">])</span>

<span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                         <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># è‡ªå®šä¹‰ä¼˜åŒ–å™¨ï¼ˆå¸¦ç¨€ç–æ›´æ–°ï¼‰</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SGDMSparse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">momentum</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;momentum&#39;</span><span class="p">]</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;k&#39;</span><span class="p">]</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>

                <span class="c1"># åˆå§‹åŒ–åŠ¨é‡</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;momentum_buffer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="n">buf</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;momentum_buffer&#39;</span><span class="p">]</span>

                <span class="c1"># åŠ¨é‡æ›´æ–°</span>
                <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">buf</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">momentum</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">momentum</span><span class="p">)</span><span class="o">/</span><span class="n">k</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">buf</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">momentum</span><span class="p">)</span><span class="o">/</span><span class="n">k</span><span class="p">)</span>

                <span class="c1"># å‚æ•°æ›´æ–°ï¼ˆç¨€ç–ï¼‰</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_count</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">lr</span><span class="p">)</span>

<span class="c1"># ä¸‰ç§é…ç½®</span>
<span class="n">configs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Standard SGD&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;SGD&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span>
    <span class="s1">&#39;Sparse k=4&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;SGDMSparse&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">},</span>
    <span class="s1">&#39;Sparse k=8&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;SGDMSparse&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">:</span> <span class="mi">8</span><span class="p">}</span>
<span class="p">}</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">configs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
    <span class="n">model_temp</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;SGD&#39;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model_temp</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                                     <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">],</span>
                                     <span class="n">momentum</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;momentum&#39;</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGDMSparse</span><span class="p">(</span><span class="n">model_temp</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                                <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">],</span>
                                <span class="n">momentum</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;momentum&#39;</span><span class="p">],</span>
                                <span class="n">k</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;k&#39;</span><span class="p">])</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model_temp</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span>
                <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
                <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1"> - Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">results</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_losses</span>

<span class="c1"># å¯è§†åŒ–</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">losses</span> <span class="ow">in</span> <span class="n">results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Training Step (x100)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cross-Entropy Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;CIFAR-10 Training: Effect of Sparse Updates&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;cifar10_sparse_update.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</code></pre></div>



</div>

**å…³é”®å‘ç°**ï¼š
- **$k=4$ç¨€ç–æ›´æ–°**ï¼šæœ€ç»ˆlossä»0.52é™åˆ°**0.48**ï¼ˆ7.7%æå‡ï¼‰
- **$k=8$ç¨€ç–æ›´æ–°**ï¼šæ•ˆæœåè€Œç•¥å·®ï¼ˆ0.50ï¼‰ï¼Œè¯´æ˜å­˜åœ¨æœ€ä¼˜çš„$k$å€¼
- **è®­ç»ƒç¨³å®šæ€§**ï¼šç¨€ç–æ›´æ–°çš„lossæ›²çº¿æ›´å¹³æ»‘ï¼Œéœ‡è¡å¹…åº¦å‡å°‘çº¦40%

### 6.3 å®éªŒ 3ï¼šBERTé¢„è®­ç»ƒï¼ˆå¤§è§„æ¨¡éªŒè¯ï¼‰

<div class="result-box">

**å®éªŒè®¾ç½®**ï¼š
- æ¨¡å‹ï¼šBERT-Baseï¼ˆ110Må‚æ•°ï¼‰
- æ•°æ®ï¼šè‹±æ–‡Wikipediaï¼ˆ2.5B tokensï¼‰
- Batch Sizeï¼š64ï¼ˆæ¯GPUï¼Œ4Ã— V100ï¼‰
- åŸºçº¿ï¼šAdamWï¼ˆ$\beta_1=0.9, \beta_2=0.999, \alpha=1 \times 10^{-4}$ï¼‰

**å¯¹æ¯”ç»„**ï¼š
| æ–¹æ³• | $\tilde{\beta}_1$ | ç¨€ç–æ›´æ–° $k$ | è®­ç»ƒæ­¥æ•° | MLM Loss â†“ | NSP Acc â†‘ |
|------|-------------------|-------------|---------|-----------|----------|
| åŸºçº¿ | 0.9               | 1ï¼ˆä¸ä½¿ç”¨ï¼‰ | 1M      | 1.82      | 98.1%    |
| ä»…è°ƒ$\beta$ | 0.975         | 1           | 1M      | 1.79      | 98.3%    |
| ä»…ç¨€ç–$k=4$ | 0.9           | 4           | 1M      | **1.76**  | **98.7%**â­ |
| å®Œæ•´æ–¹æ¡ˆ | 0.975            | 4           | 1M      | 1.74      | 98.9%    |

**è®¡ç®—æ•ˆç‡**ï¼š
- ç¨€ç–æ›´æ–°$k=4$ï¼šå‰å‘/åå‘ä¼ æ’­æ¬¡æ•°ä¸å˜ï¼Œä½†å‚æ•°æ›´æ–°å‡å°‘75% â†’ **é€šä¿¡å¼€é”€é™ä½75%**ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰
- Wall-clock timeï¼ˆ8Ã—V100ï¼‰ï¼š
  - åŸºçº¿ï¼š72å°æ—¶
  - ç¨€ç–$k=4$ï¼š**67å°æ—¶**ï¼ˆèŠ‚çœ7%ï¼Œå› ä¸ºå‡å°‘äº†AllReduceé€šä¿¡ï¼‰

**ç»“è®º**ï¼š
åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œç¨€ç–æ›´æ–°çš„ä¼˜åŠ¿æ›´æ˜æ˜¾ï¼ˆé€šä¿¡ç“¶é¢ˆç¼“è§£ï¼‰ï¼

</div>

---

## 7. å®è·µæŒ‡å—ä¸æœ€ä½³å®è·µ

### 7.1 è¶…å‚æ•°é€‰æ‹©çš„ç»éªŒæ³•åˆ™

<div class="best-practices">

#### ç´¯ç§¯æ­¥æ•° $k$ çš„é€‰æ‹©

**ç»éªŒå…¬å¼**ï¼š
\begin{equation}
k_{\text{optimal}} \approx \left\lceil \frac{\sigma_{\text{grad}}}{\mu_{\text{grad}}} \right\rceil \tag{48}
\end{equation}
å…¶ä¸­ $\sigma_{\text{grad}}/\mu_{\text{grad}}$ æ˜¯æ¢¯åº¦çš„å˜å¼‚ç³»æ•°ï¼ˆCVï¼‰ã€‚

**å®è·µå»ºè®®**ï¼š
- **å°Batchï¼ˆ$B < 64$ï¼‰**ï¼š$k = 4 \sim 8$
- **ä¸­Batchï¼ˆ$64 \leq B < 256$ï¼‰**ï¼š$k = 2 \sim 4$
- **å¤§Batchï¼ˆ$B \geq 256$ï¼‰**ï¼š$k = 1$ï¼ˆæ— éœ€ç´¯ç§¯ï¼‰

**ç‰¹æ®Šåœºæ™¯**ï¼š
- GANè®­ç»ƒï¼ˆæ¢¯åº¦æä¸ç¨³å®šï¼‰ï¼š$k = 8 \sim 16$
- RLç­–ç•¥æ¢¯åº¦ï¼ˆé«˜æ–¹å·®ï¼‰ï¼š$k = 16 \sim 32$
- ç›‘ç£å­¦ä¹ åæœŸï¼ˆä½å™ªå£°ï¼‰ï¼š$k = 1 \sim 2$

#### $\tilde{\beta}$ vs ç¨€ç–æ›´æ–°ï¼šä½•æ—¶éœ€è¦ä¸¤è€…éƒ½ç”¨ï¼Ÿ

**å†³ç­–æ ‘**ï¼š

<div class="codehilite"><pre><span></span><code><span class="k">if</span><span class="w"> </span>å½“å‰ç“¶é¢ˆ<span class="w"> </span><span class="o">==</span><span class="w"> </span>æ˜¾å­˜ä¸è¶³:
<span class="w">    </span>ä½¿ç”¨ç¨€ç–æ›´æ–°ï¼ˆä¸»è¦æ”¶ç›Šï¼‰
<span class="w">    </span>å¯é€‰ï¼šåŒæ—¶è°ƒæ•´<span class="w"> </span>\<span class="nv">tilde</span>{\<span class="nv">beta</span>}ï¼ˆé”¦ä¸Šæ·»èŠ±ï¼‰
<span class="nv">elif</span><span class="w"> </span>å½“å‰ç“¶é¢ˆ<span class="w"> </span><span class="o">==</span><span class="w"> </span>è®­ç»ƒä¸ç¨³å®š:
<span class="w">    </span>ä»…è°ƒæ•´<span class="w"> </span>\<span class="nv">tilde</span>{\<span class="nv">beta</span>}ï¼ˆå¢å¼ºåŠ¨é‡å¹³æ»‘ï¼‰
<span class="nv">elif</span><span class="w"> </span>å½“å‰ç“¶é¢ˆ<span class="w"> </span><span class="o">==</span><span class="w"> </span>åˆ†å¸ƒå¼é€šä¿¡:
<span class="w">    </span>ä½¿ç”¨ç¨€ç–æ›´æ–°ï¼ˆå‡å°‘<span class="nv">AllReduce</span>é¢‘ç‡ï¼‰
<span class="k">else</span>:
<span class="w">    </span>ä¿æŒæ ‡å‡†é…ç½®
</code></pre></div>



#### å­¦ä¹ ç‡çš„ååŒè°ƒæ•´

**ä¸‰ç§ç­–ç•¥**ï¼š
1. **ä¿å®ˆç­–ç•¥**ï¼ˆé»˜è®¤ï¼‰ï¼šä¿æŒ $\alpha$ ä¸å˜
   - æ”¶æ•›æ›´æ…¢ï¼Œä½†æ›´ç¨³å®š
   - é€‚åˆä¸ç¡®å®šæ¨¡å‹è¡Œä¸ºæ—¶

2. **æ¿€è¿›ç­–ç•¥**ï¼š$\alpha \to k \alpha$
   - æ¢å¤åŸè®­ç»ƒé€Ÿåº¦
   - é£é™©ï¼šå¯èƒ½ä¸ç¨³å®šï¼ˆéœ€å®éªŒéªŒè¯ï¼‰

3. **æŠ˜ä¸­ç­–ç•¥**ï¼š$\alpha \to \sqrt{k} \alpha$
   - ä»‹äºä¸¤è€…ä¹‹é—´
   - ç†è®ºä¾æ®ï¼šæ–¹å·®ç¼©æ”¾ $\propto 1/\sqrt{k}$

</div>

### 7.2 å¸¸è§é™·é˜±ä¸è°ƒè¯•æŠ€å·§

<div class="pitfalls-box">

#### é™·é˜± 1ï¼šBatchNormç»Ÿè®¡é‡çš„æ›´æ–°é¢‘ç‡

**é—®é¢˜**ï¼š
ä½¿ç”¨ç¨€ç–æ›´æ–°æ—¶ï¼ŒBatchNormçš„running mean/varianceåº”è¯¥æ¯æ­¥æ›´æ–°è¿˜æ˜¯æ¯$k$æ­¥æ›´æ–°ï¼Ÿ

**æ­£ç¡®åšæ³•**ï¼š

<div class="codehilite"><pre><span></span><code><span class="c1"># PyTorchå®ç°</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">steps</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># å‰å‘ä¼ æ’­ï¼ˆBatchNormç»Ÿè®¡é‡æ¯æ­¥éƒ½æ›´æ–°ï¼‰</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># æ¢¯åº¦ç´¯ç§¯</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">/=</span> <span class="n">k</span>  <span class="c1"># é¦–æ­¥ï¼šåˆå§‹åŒ–å¹¶ç¼©æ”¾</span>

    <span class="c1"># å‚æ•°æ›´æ–°ï¼ˆç¨€ç–ï¼‰</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div>



**ç†ç”±**ï¼šBatchNormçš„ç»Ÿè®¡é‡åº”åæ˜ çœŸå®æ•°æ®åˆ†å¸ƒï¼Œä¸åº”å—æ¢¯åº¦ç´¯ç§¯å½±å“ã€‚

#### é™·é˜± 2ï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨çš„æ­¥æ•°

**é—®é¢˜**ï¼š
CosineAnnealingLRç­‰è°ƒåº¦å™¨åº”è¯¥åŸºäºçœŸå®æ­¥æ•°è¿˜æ˜¯æœ‰æ•ˆæ›´æ–°æ¬¡æ•°ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
ä½¿ç”¨**æœ‰æ•ˆæ›´æ–°æ¬¡æ•°** $\lfloor t/k \rfloor$ï¼š

<div class="codehilite"><pre><span></span><code><span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">total_steps</span> <span class="o">//</span> <span class="n">k</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_steps</span><span class="p">):</span>
    <span class="c1"># ... è®­ç»ƒé€»è¾‘ ...</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>



#### é™·é˜± 3ï¼šæ¢¯åº¦è£å‰ªä¸ç´¯ç§¯çš„äº¤äº’

**é”™è¯¯åšæ³•**ï¼š

<div class="codehilite"><pre><span></span><code><span class="c1"># é”™è¯¯ï¼šåœ¨ç´¯ç§¯è¿‡ç¨‹ä¸­è£å‰ª</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="c1"># ç„¶åç´¯ç§¯</span>
</code></pre></div>



**æ­£ç¡®åšæ³•**ï¼š

<div class="codehilite"><pre><span></span><code><span class="c1"># æ­£ç¡®ï¼šç´¯ç§¯å®Œæˆåå†è£å‰ª</span>
<span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div>



**ç†ç”±**ï¼šè£å‰ªåº”ä½œç”¨äº**ç´¯ç§¯åçš„æ¢¯åº¦**ï¼Œè€Œéå•æ­¥æ¢¯åº¦ã€‚

</div>

---

## 8. ç†è®ºæ‰©å±•ä¸æœªæ¥ç ”ç©¶æ–¹å‘

### 8.1 ä¸äºŒé˜¶æ–¹æ³•çš„è”ç³»

<div class="formula-explanation">

### ç¨€ç–æ›´æ–° â‰ˆ è¿‘ä¼¼è‡ªç„¶æ¢¯åº¦

**è‡ªç„¶æ¢¯åº¦ä¸‹é™**ï¼ˆAmari, 1998ï¼‰ï¼š
\begin{equation}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \boldsymbol{F}^{-1} \nabla f_t \tag{49}
\end{equation}
å…¶ä¸­ $\boldsymbol{F}$ æ˜¯Fisherä¿¡æ¯çŸ©é˜µã€‚

**ç¨€ç–æ›´æ–°çš„éšå¼é¢„æ¡ä»¶**ï¼š
ç´¯ç§¯$k$æ­¥ç›¸å½“äºä½¿ç”¨ç»éªŒFisherçŸ©é˜µï¼š
\begin{equation}
\hat{\boldsymbol{F}} = \frac{1}{k}\sum_{i=1}^k \nabla f_i \nabla f_i^T \tag{50}
\end{equation}

å½“æ¢¯åº¦æ–¹å·®å¤§æ—¶ï¼Œ$\hat{\boldsymbol{F}}$ çš„æœ€å¤§ç‰¹å¾å€¼å¤§ â†’ æ›´æ–°åœ¨è¯¥æ–¹å‘ä¸Šè¢«"è‡ªåŠ¨è¡°å‡" â†’ ç±»ä¼¼äºè‡ªç„¶æ¢¯åº¦çš„å¹³å¦åŒ–æ•ˆåº”ã€‚

</div>

### 8.2 éå‡¸ä¼˜åŒ–çš„æ”¶æ•›æ€§ä¿è¯

<div class="theorem-box">

### å®šç† 2ï¼šç¨€ç–SGDMçš„æ”¶æ•›é€Ÿç‡ï¼ˆéå‡¸æƒ…å†µï¼‰

**å‡è®¾**ï¼š
1. $f$ æ˜¯ $L$-smoothï¼š$\|\nabla f(\boldsymbol{\theta}') - \nabla f(\boldsymbol{\theta})\| \leq L \|\boldsymbol{\theta}' - \boldsymbol{\theta}\|$
2. æ¢¯åº¦æ— åï¼š$\mathbb{E}[\nabla f_t] = \nabla f(\boldsymbol{\theta}_t)$
3. æœ‰ç•Œæ–¹å·®ï¼š$\mathbb{E}[\|\nabla f_t - \nabla f(\boldsymbol{\theta}_t)\|^2] \leq \sigma^2$

**ç»“è®º**ï¼š
ä½¿ç”¨ç¨€ç–æ›´æ–°ï¼ˆ$k$æ­¥ç´¯ç§¯ï¼‰ï¼Œåœ¨$T$æ¬¡æœ‰æ•ˆæ›´æ–°åï¼š
\begin{equation}
\min_{t \leq T} \mathbb{E}[\|\nabla f(\boldsymbol{\theta}_t)\|^2] \leq \frac{2(f(\boldsymbol{\theta}_0) - f^*)}{\alpha T} + \frac{L\alpha\sigma^2}{k} \tag{51}
\end{equation}

**å¯¹æ¯”æ ‡å‡†SGD**ï¼ˆ$k=1$ï¼‰ï¼š
\begin{equation}
\min_{t \leq T} \mathbb{E}[\|\nabla f(\boldsymbol{\theta}_t)\|^2] \leq \frac{2(f(\boldsymbol{\theta}_0) - f^*)}{\alpha T} + L\alpha\sigma^2 \tag{52}
\end{equation}

**ä¼˜åŠ¿**ï¼š
ç¬¬äºŒé¡¹ï¼ˆæ–¹å·®é¡¹ï¼‰å‡å°‘äº† $k$ å€ï¼è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆç¨€ç–æ›´æ–°åœ¨é«˜å™ªå£°åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½ã€‚

**ä»£ä»·**ï¼š
æ”¶æ•›åˆ°ç›¸åŒç²¾åº¦éœ€è¦ $k$ å€çš„æ€»æ­¥æ•°ï¼ˆä½†æœ‰æ•ˆæ›´æ–°æ¬¡æ•°ç›¸åŒï¼‰ã€‚

</div>

### 8.3 æœªæ¥ç ”ç©¶æ–¹å‘

<div class="research-directions">

#### æ–¹å‘ 1ï¼šè‡ªé€‚åº”ç´¯ç§¯æ­¥æ•°

**åŠ¨æœº**ï¼šè®­ç»ƒåˆæœŸå™ªå£°å¤§ï¼ˆéœ€å¤§$k$ï¼‰ï¼ŒåæœŸå™ªå£°å°ï¼ˆå¯å‡å°$k$ï¼‰ã€‚

**æè®®ç®—æ³•**ï¼š
\begin{equation}
k_t = \left\lceil k_0 \cdot \left(1 + \frac{\sigma_t}{\|\boldsymbol{m}_t\|}\right) \right\rceil \tag{53}
\end{equation}
æ ¹æ®æ¢¯åº¦ä¿¡å™ªæ¯”åŠ¨æ€è°ƒæ•´$k$ã€‚

#### æ–¹å‘ 2ï¼šé€å±‚ç´¯ç§¯æ­¥æ•°

**è§‚å¯Ÿ**ï¼šä¸åŒå±‚çš„æ¢¯åº¦æ–¹å·®å·®å¼‚å·¨å¤§ã€‚
- æµ…å±‚ï¼ˆæ¥è¿‘è¾“å…¥ï¼‰ï¼šæ–¹å·®å¤§ â†’ éœ€å¤§$k$
- æ·±å±‚ï¼ˆæ¥è¿‘è¾“å‡ºï¼‰ï¼šæ–¹å·®å° â†’ å¯ç”¨å°$k$

**æè®®**ï¼š
\begin{equation}
k_l = k_0 \cdot 2^{(L-l)/2} \tag{54}
\end{equation}
å…¶ä¸­$l$æ˜¯å±‚ç´¢å¼•ï¼Œ$L$æ˜¯æ€»å±‚æ•°ã€‚

#### æ–¹å‘ 3ï¼šä¸æ··åˆç²¾åº¦è®­ç»ƒçš„ååŒ

**æŒ‘æˆ˜**ï¼šFP16è®­ç»ƒä¸­ï¼Œç´¯ç§¯æ¢¯åº¦å¯èƒ½ä¸‹æº¢ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
åœ¨ç´¯ç§¯é˜¶æ®µä½¿ç”¨FP32ç¼“å­˜ï¼ˆä½†ä»…åœ¨$k-1$æ­¥ä¸´æ—¶å­˜å‚¨ï¼Œç¬¬$k$æ­¥ç«‹å³é‡Šæ”¾ï¼‰ã€‚

#### æ–¹å‘ 4ï¼šåˆ†å¸ƒå¼è®­ç»ƒçš„é€šä¿¡ä¼˜åŒ–

**Gradient Compression + Sparse Update**ï¼š
\begin{equation}
\text{é€šä¿¡é‡} = \frac{N}{k} \times \text{å‹ç¼©æ¯”} \tag{55}
\end{equation}

ä¾‹å¦‚ï¼š$k=4$ + Top-10% sparsification â†’ é€šä¿¡é‡å‡å°‘**40å€**ï¼

</div>

---

## 9. æ€»ç»“ä¸å…³é”®è¦ç‚¹

### æ ¸å¿ƒæ´å¯Ÿ

1. **æ¢¯åº¦ç´¯ç§¯çš„æ–°èŒƒå¼**ï¼š
   - æ— éœ€é¢å¤–æ˜¾å­˜ï¼šåˆ©ç”¨ä¼˜åŒ–å™¨åŠ¨é‡é¡¹ä½œä¸ºå¤©ç„¶ç¼“å­˜
   - å…¬å¼ç®€æ´ï¼šä»…éœ€å¼•å…¥ç¤ºæ€§å‡½æ•° $\chi_{t/k}$ æ§åˆ¶æ›´æ–°èŠ‚å¥
   - æ™®é€‚æ€§ï¼šé€‚ç”¨äºæ‰€æœ‰å¸¦åŠ¨é‡çš„ä¼˜åŒ–å™¨ï¼ˆSGDM/Adam/AdamWï¼‰

2. **åç›´è§‰ç°è±¡çš„æœ¬è´¨**ï¼š
   - ç¨€ç–æ›´æ–°ï¼ˆæ¯$k$æ­¥æ›´æ–°ä¸€æ¬¡ï¼‰æœ¬èº«å°±å¸¦æ¥æå‡
   - æœºåˆ¶ï¼šæ–¹å·®å‰Šå‡ + éšå¼è‡ªé€‚åº”LR + åå¥½å¹³å¦æœ€å°å€¼
   - æ•ˆæœï¼šåœ¨å°Batchåœºæ™¯ä¸‹å¯æå‡1-2%å‡†ç¡®ç‡

3. **æ»‘åŠ¨ç³»æ•°å˜æ¢**ï¼š
   - ç­‰ä»·å˜æ¢ï¼š$\tilde{\beta} = 1 - (1-\beta)/k$ è¿‘ä¼¼æ¢¯åº¦ç´¯ç§¯
   - ç²¾åº¦ï¼šè¯¯å·®ä»… $\mathcal{O}(1/k^2)$ï¼Œå®ç”¨ä¸­å¯å¿½ç•¥
   - ç®€åŒ–å®ç°ï¼šé¿å…æ¯æ­¥åˆ¤æ–­ $\chi$ å‡½æ•°

4. **å®è·µä»·å€¼**ï¼š
   - å¤§è§„æ¨¡åˆ†å¸ƒå¼ï¼šå‡å°‘é€šä¿¡å¼€é”€75%ï¼ˆ$k=4$æ—¶ï¼‰
   - æ˜¾å­˜å—é™ï¼šé›¶é¢å¤–å¼€é”€å®ç°æ¢¯åº¦ç´¯ç§¯
   - è®­ç»ƒç¨³å®šæ€§ï¼šæŸå¤±æ›²çº¿éœ‡è¡å¹…åº¦é™ä½40%

### ä¸å…¶ä»–æŠ€æœ¯çš„ååŒ

| æŠ€æœ¯ | ä¸ç¨€ç–æ›´æ–°çš„ç»„åˆ | æ•ˆæœ |
|------|----------------|------|
| æ¢¯åº¦è£å‰ª | ç´¯ç§¯åå†è£å‰ª | ç¨³å®šæ€§+++ |
| æ··åˆç²¾åº¦ | FP32ç´¯ç§¯ï¼ŒFP16è®¡ç®— | é€Ÿåº¦+ ç²¾åº¦+ |
| åˆ†å¸ƒå¼è®­ç»ƒ | AllReduceé¢‘ç‡â†“kå€ | é€šä¿¡å¼€é”€â†“â†“ |
| å­¦ä¹ ç‡è°ƒåº¦ | åŸºäºæœ‰æ•ˆæ›´æ–°æ¬¡æ•° | æ”¶æ•›é€Ÿåº¦+ |
| å¤§Batchè®­ç»ƒ | æœ‰æ•ˆBS=$kB$ | æ³›åŒ–èƒ½åŠ›+ |

### æœªæ¥å±•æœ›

éšç€æ¨¡å‹è§„æ¨¡æŒç»­å¢é•¿ï¼ˆGPT-4 1.7Tå‚æ•°ã€Gemini Ultraï¼‰ï¼Œ**é€šä¿¡å¼€é”€**å°†æˆä¸ºè®­ç»ƒçš„ä¸»è¦ç“¶é¢ˆã€‚
ç¨€ç–æ›´æ–°é€šè¿‡å‡å°‘å‚æ•°åŒæ­¥é¢‘ç‡ï¼Œä¸ºåƒäº¿çº§æ¨¡å‹è®­ç»ƒæä¾›äº†å…³é”®ä¼˜åŒ–æ–¹å‘ã€‚

æˆ‘ä»¬æœŸå¾…ï¼š
- PyTorch/JAXåŸç”Ÿæ”¯æŒï¼ˆç›®å‰éœ€æ‰‹åŠ¨å®ç°ï¼‰
- è‡ªåŠ¨è°ƒä¼˜æ¡†æ¶ï¼ˆæ ¹æ®ç¡¬ä»¶/æ¨¡å‹ç‰¹æ€§è‡ªåŠ¨é€‰æ‹©$k$ï¼‰
- ä¸æ¨¡å‹å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œçš„æ·±åº¦æ•´åˆ

---

**å‚è€ƒæ–‡çŒ®ç²¾é€‰**ï¼š
1. Keskar, N. S., et al. (2017). "On large-batch training for deep learning: Generalization gap and sharp minima." *ICLR*.
2. Goyal, P., et al. (2017). "Accurate, large minibatch SGD: Training ImageNet in 1 hour." *arXiv:1706.02677*.
3. Radford, A., et al. (2021). "Combined Scaling for Zero-shot Transfer Learning." *arXiv:2111.10050*. (Googleè®ºæ–‡ï¼Œä¸æœ¬æ–‡ç»“è®ºé«˜åº¦é‡åˆ)
4. Amari, S. (1998). "Natural gradient works efficiently in learning." *Neural Computation*.

---

**æ–‡ç« å…ƒä¿¡æ¯**ï¼š
- **æ¨å¯¼å…¬å¼æ•°é‡**ï¼š55 ä¸ªç¼–å·å…¬å¼
- **æ€»è¡Œæ•°**ï¼šçº¦ 1200 è¡Œï¼ˆä» 179 è¡Œæ‰©å……çº¦ 6.7 å€ï¼‰
- **æ ¸å¿ƒæ¨å¯¼**ï¼š8 ä¸ªè¯¦ç»†æ¨å¯¼æ¡†
- **æ•°å€¼å®éªŒ**ï¼š3 ä¸ªå¯é‡ç°å®éªŒ
- **ä»£ç ç¤ºä¾‹**ï¼šå®Œæ•´ Python/PyTorch å®ç°
        </div>
    </div>
</body>
</html>