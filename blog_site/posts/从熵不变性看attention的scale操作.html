<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ä»ç†µä¸å˜æ€§çœ‹Attentionçš„Scaleæ“ä½œ</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>ä»ç†µä¸å˜æ€§çœ‹Attentionçš„Scaleæ“ä½œ</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2025-11-26 | ğŸ“„ å¤§å°: 31.2 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/8823">https://spaces.ac.cn/archives/8823</a></p>
<p><strong>å‘å¸ƒæ—¥æœŸ</strong>: </p>
<hr />
<p>å½“å‰Transformeræ¶æ„ç”¨çš„æœ€å¤šçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¨ç§°ä¸ºâ€œScaled Dot-Product Attentionâ€ï¼Œå…¶ä¸­â€œScaledâ€æ˜¯å› ä¸ºåœ¨$Q,K$è½¬ç½®ç›¸ä¹˜ä¹‹åè¿˜è¦é™¤ä»¥ä¸€ä¸ª$\sqrt{d}$å†åšSoftmaxï¼ˆä¸‹é¢å‡ä¸å¤±ä¸€èˆ¬æ€§åœ°å‡è®¾$Q,K,V\in\mathbb{R}^{n\times d}$ï¼‰ï¼š<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{QK^{\top}}{\sqrt{d}}\right)V\label{eq:std}\end{equation}</p>
<p>åœ¨<a href="/archives/8620">ã€Šæµ…è°ˆTransformerçš„åˆå§‹åŒ–ã€å‚æ•°åŒ–ä¸æ ‡å‡†åŒ–ã€‹</a>ä¸­ï¼Œæˆ‘ä»¬å·²ç»åˆæ­¥è§£é‡Šäº†é™¤ä»¥$\sqrt{d}$çš„ç¼˜ç”±ã€‚è€Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œç¬”è€…å°†ä»â€œç†µä¸å˜æ€§â€çš„è§’åº¦æ¥ç†è§£è¿™ä¸ªç¼©æ”¾æ“ä½œï¼Œå¹¶ä¸”å¾—åˆ° <em>ä¸€ä¸ªæ–°çš„ç¼©æ”¾å› å­</em> ã€‚åœ¨MLMçš„å®éªŒæ˜¾ç¤ºï¼Œæ–°çš„ç¼©æ”¾å› å­å…·æœ‰ <em>æ›´å¥½çš„é•¿åº¦å¤–æ¨æ€§èƒ½</em> ã€‚</p>
<h2 id="_1">ç†µä¸å˜æ€§</h2>
<p>æˆ‘ä»¬å°†ä¸€èˆ¬çš„Scaled Dot-Product Attentionæ”¹å†™æˆ<br />
\begin{equation}\boldsymbol{o}<em j="1">i = \sum</em>}^n a_{i,j}\boldsymbol{v<em i_j="i,j">j,\quad a</em>}=\frac{e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j}}{\sum\limits</em>}^n e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}}\end{equation<br />
å…¶ä¸­$\lambda$æ˜¯ç¼©æ”¾å› å­ï¼Œå®ƒè·Ÿ$\boldsymbol{q}_i,\boldsymbol{k}_j$æ— å…³ï¼Œä½†åŸåˆ™ä¸Šå¯ä»¥è·Ÿé•¿åº¦$n$ã€ç»´åº¦$d$ç­‰å‚æ•°æœ‰å…³ï¼Œç›®å‰ä¸»æµçš„å°±æ˜¯$\lambda=1/\sqrt{d}$ã€‚</p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªè§‚ç‚¹ï¼š</p>
<blockquote>
<p>ä¸ºäº†ä½¿å¾—æ¨¡å‹ç»“æœèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æœªçŸ¥é•¿åº¦ï¼ŒAttentionæœºåˆ¶çš„è®¾è®¡åº”è¯¥ä½¿å¾—$a_{i,j}$å°½é‡å…·å¤‡ç†µä¸å˜æ€§ã€‚</p>
</blockquote>
<p>æ€ä¹ˆç†è§£è¿™å¥è¯å‘¢ï¼Ÿé¦–å…ˆï¼Œæ³›åŒ–åˆ°æœªçŸ¥é•¿åº¦ï¼ŒæŒ‡çš„æ˜¯é¢„æµ‹é•¿åº¦å’Œè®­ç»ƒä¸ä¸€è‡´æ—¶ä¹Ÿèƒ½æœ‰ä¸é”™çš„æ•ˆæœï¼Œæ¯”å¦‚$n=64$è®­ç»ƒç„¶åå¤–æ¨åˆ°$n=128,256$æµ‹è¯•ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œä½¿ç”¨<a href="/archives/8265">RoPE</a>ä¹‹ç±»çš„ç›¸å¯¹ä½ç½®ç¼–ç çš„æ¨¡å‹ï¼Œå¯¹é•¿åº¦å…·æœ‰æ¯”è¾ƒå¥½çš„å¤–æ¨æ€§ï¼Œä½†æˆ‘ä»¬ä¾ç„¶å¯ä»¥é€šè¿‡æ›´å¥½çš„è®¾è®¡æ¥å¢å¼ºè¿™ç§å¤–æ¨æ€§ï¼Œæ¯”å¦‚ç†µä¸å˜æ€§å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚</p>
<p>å…·ä½“æ¥è¯´ï¼Œ$a_{i,j}$å¯ä»¥è§†ä¸º$i$ä¸ºæ¡ä»¶ã€$j$ä¸ºéšæœºå˜é‡çš„æ¡ä»¶åˆ†å¸ƒï¼Œå®ƒçš„ç†µä¸º<br />
\begin{equation}\mathcal{H}<em j="1">i = -\sum</em>}^n a_{i,j}\log a_{i,j}\end{equation<br />
ç†µä¸å˜æ€§æ˜¯æŒ‡ï¼Œ$\mathcal{H}<em i_j="i,j">i$åº”è¯¥å¯¹é•¿åº¦$n$ä¸æ•æ„Ÿã€‚æ›´å…·ä½“ä¸€ç‚¹ï¼Œå°±æ˜¯å¦‚æœåœ¨å·²æœ‰çš„tokenåŸºç¡€ä¸Šï¼Œå†è¡¥å……å‡ ä¸ªtokenï¼Œé‚£ä¹ˆæ–°ç®—å‡ºæ¥å„ä¸ª$a</em>_i$ä¸è¦æœ‰å¤ªå¤§æ”¹å˜ã€‚}$è‡ªç„¶ä¹Ÿä¼šæœ‰æ‰€æ”¹å˜ï¼Œä½†æˆ‘ä»¬å¸Œæœ›$\mathcal{H</p>
<p>ä¸ºä»€ä¹ˆå¸Œæœ›ç†µä¸å˜å‘¢ï¼Ÿæˆ‘ä»¬çŸ¥é“ï¼Œç†µæ˜¯ä¸ç¡®å®šæ€§çš„åº¦é‡ï¼ˆå‚è€ƒ<a href="/archives/3534">ã€Šâ€œç†µâ€ä¸èµ·ï¼šä»ç†µã€æœ€å¤§ç†µåŸç†åˆ°æœ€å¤§ç†µæ¨¡å‹ï¼ˆä¸€ï¼‰ã€‹</a>ï¼‰ï¼Œæ¢ä¸ªè§’åº¦æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸ç¡®å®šæ€§è§†ä¸ºæ³¨æ„åŠ›çš„â€œèšç„¦ç¨‹åº¦â€ï¼šå¦‚æœç†µä¸º0ï¼Œé‚£ä¹ˆæ³¨æ„åŠ›å°†èšç„¦åˆ°æŸä¸€ä¸ªtokenä¸Šï¼Œå¦‚æœç†µä¸º$\log n$ï¼Œé‚£ä¹ˆæ³¨æ„åŠ›å‡åŒ€åˆ†å¸ƒåˆ°æ‰€æœ‰tokenä¸Šã€‚æˆ‘ä»¬å¸Œæœ›ç†µä¸å˜ï¼Œæ˜¯å¸Œæœ›å¼•å…¥æ–°çš„tokenåï¼Œå·²æœ‰çš„tokenä¾æ—§èƒ½åŒæ ·åœ°èšç„¦åˆ°åŸæ¥çš„tokenä¸Šï¼Œè€Œä¸å¸Œæœ›æ–°tokençš„å¼•å…¥è¿‡å¤šåœ°â€œåˆ†æ‘Šâ€äº†åŸæœ‰çš„æ³¨æ„åŠ›ï¼Œå¯¼è‡´æ±‚å’Œç»“æœæ˜¾è‘—å‘ç”Ÿå˜åŒ–ã€‚</p>
<h2 id="_2">æ–°çš„å› å­</h2>
<p>æ ¹æ®ç†µä¸å˜æ€§ä»¥åŠä¸€äº›åˆç†çš„å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ–°çš„ç¼©æ”¾å› å­ï¼Œä»è€Œå¾—åˆ°ä¸€ç§Scaled Dot-Product Attentionï¼š<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\kappa \log n}{d}QK^{\top}\right)V\label{eq:ei}\end{equation}<br />
è¿™é‡Œçš„$\kappa$æ˜¯ä¸€ä¸ªè·Ÿ$n,d$éƒ½æ— å…³çš„è¶…å‚æ•°ï¼Œè¯¦ç»†æ¨å¯¼è¿‡ç¨‹æˆ‘ä»¬ä¸‹ä¸€èŠ‚å†ä»‹ç»ã€‚ä¸ºäº†ç§°å‘¼ä¸Šçš„æ–¹ä¾¿ï¼Œè¿™é‡Œå°†å¼$\eqref{eq:std}$æè¿°çš„å¸¸è§„Scaled Dot-Product Attentionç§°ä¸ºâ€œAttention-Oâ€ï¼ˆOriginalï¼‰ï¼Œè€Œå¼$\eqref{eq:ei}$ä»¥åŠä¸‹é¢çš„å¼$\eqref{eq:ei2}$æè¿°çš„å˜ä½“ç§°ä¸ºâ€œAttention-Eâ€ï¼ˆEntropy Invarianceï¼‰ã€‚</p>
<p>å¯èƒ½æœ‰è¯»è€…å¯¹å¼•å…¥äº†ä¸€ä¸ªæ–°å‚æ•°æ„Ÿåˆ°ä¸æ»¡æ„ï¼Œå…¶å®è¿™ä¸ªä¸éš¾è§£å†³ã€‚æˆ‘ä»¬çŸ¥é“å½“å‰ä¸»æµçš„é¢„è®­ç»ƒé•¿åº¦å°±æ˜¯512ï¼Œæ‰€ä»¥æˆ‘ä»¬å‡è®¾ä¸»æµçš„å‚æ•°éƒ½æ˜¯ä¸º$n=512$è°ƒè¯•å¥½çš„ï¼Œæ‰€ä»¥å½“$n=512$çš„æ—¶å€™ï¼Œä¸Šå¼åº”é€€åŒ–ä¸ºæ™®é€šçš„Scaled Dot-Product Attentionï¼Œå³$\frac{\kappa \log 512}{d}=\frac{1}{\sqrt{d}}$ï¼Œæ¨å‡º$\kappa = \frac{\sqrt{d}}{\log 512}$ï¼Œä»£å…¥ä¸Šå¼æ•´ç†åå¾—åˆ°<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\log_{512} n}{\sqrt{d}}QK^{\top}\right)V\label{eq:ei2}\end{equation}<br />
è¿™å°±å»æ‰äº†è¶…å‚æ•°$\lambda$ï¼Œä¸‹é¢çš„å®éªŒä¹Ÿæ˜¯ç”¨è¿™ä¸ªç‰ˆæœ¬ã€‚</p>
<p>ä¸ºäº†éªŒè¯è¯¥æ”¹åŠ¨æ˜¯å¦çœŸå¦‚é¢„æœŸé‚£æ ·èƒ½æé«˜Transformerçš„å¤–æ¨æ•ˆæœï¼Œç¬”è€…åˆ†åˆ«ç”¨Attention-Oå’ŒAttention-Eåˆ†åˆ«è®­ç»ƒäº†ä¸€ä¸ªRoFormer smallç‰ˆæœ¬ï¼Œè®­ç»ƒä»»åŠ¡ä¸ºMLMï¼Œè®­ç»ƒé•¿åº¦ä¸º64ï¼Œç„¶ååœ¨ä¸åŒé•¿åº¦çš„éªŒè¯é›†ä¸‹æ¯”è¾ƒMLMçš„å‡†ç¡®ç‡ï¼Œç»“æœå¦‚ä¸‹ï¼š<br />
\begin{array}{c}<br />
\text{Attentionçš„é•¿åº¦å¤–æ¨å®éªŒ} \\\<br />
{\begin{array}{c|ccccc}<br />
\hline<br />
&amp; n=64 &amp; n=128 &amp; n=256 &amp; n=512 &amp; 1024 \\\<br />
\hline<br />
\text{Attention-O} &amp; 43.27 &amp; 36.53 &amp; 23.02 &amp; 15.12 &amp; 11.54\\\<br />
\text{Attention-E} &amp; 43.11 &amp; 41.17 &amp; 34.04 &amp; 20.15 &amp; 13.58\\\<br />
\hline<br />
\end{array}}<br />
\end{array}<br />
ä»å®éªŒç»“æœå¯ä»¥çœ‹å‡ºï¼Œåœ¨ä¸è®­ç»ƒé•¿åº¦ä¸€è‡´$n=64$çš„æƒ…å†µä¸‹ï¼ŒAttention-Oå’ŒAttention-Eçš„æ•ˆæœæ˜¯å¾ˆæ¥è¿‘çš„ï¼Œä½†æ˜¯å¤–æ¨åˆ°æ›´å¤§çš„æµ‹è¯•é•¿åº¦æ—¶ï¼Œåˆ™æ˜æ˜¾æ‹‰å¼€äº†å·®è·ï¼Œæ¯”å¦‚$n=256$æ—¶Attention-Eè¦æ¯”Attention-Oé«˜10ä¸ªç™¾åˆ†ç‚¹ä»¥ä¸Šçš„å‡†ç¡®ç‡ï¼Œå¯çœŸä¸æ˜¯ä¸€æ˜ŸåŠç‚¹äº†ã€‚</p>
<h2 id="_3">æ¨å¯¼è¿‡ç¨‹</h2>
<p>è¿™ä¸€èŠ‚æˆ‘ä»¬ä»‹ç»å¼$\eqref{eq:ei}$çš„æ¨å¯¼è¿‡ç¨‹ã€‚äº‹å®ä¸Šï¼Œæ¨å¯¼è¿‡ç¨‹å’Œå‡è®¾éƒ½è·Ÿ<a href="/archives/7695">ã€Šæœ€å°ç†µåŸç†ï¼ˆå…­ï¼‰ï¼šè¯å‘é‡çš„ç»´åº¦åº”è¯¥æ€ä¹ˆé€‰æ‹©ï¼Ÿã€‹</a>ä¸­çš„å‡ ä¹æ˜¯ä¸€æ ·çš„ã€‚</p>
<p>é¦–å…ˆï¼Œæˆ‘ä»¬ä»£å…¥$a_{i,j}$çš„è¡¨è¾¾å¼ï¼Œå°±å¯ä»¥å¾—åˆ°ï¼š<br />
\begin{equation}\mathcal{H}<em j="1">i = -\sum</em>}^n a_{i,j}\log a_{i,j}=\log \sum_{j=1}^n e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j} - \frac{\sum\limits</em>}^n e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j}(\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j)}{\sum\limits</em>}^n e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j}}\end{equation}<br />
è¦æ³¨æ„ï¼Œæˆ‘ä»¬ä»…ä»…æ˜¯è¦åšä¸€ä¸ªåŠå®šé‡çš„ä¼°è®¡ï¼Œä»¥ç¡®å®šé€‚åˆçš„$\lambda$æ¥æŠµæ¶ˆéƒ¨åˆ†é•¿åº¦çš„å½±å“ï¼Œè®©ç†µå®Œå…¨ä¸å—é•¿åº¦å½±å“æ˜¯åšä¸åˆ°çš„ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€äº›å‡è®¾ï¼Œæ¯”å¦‚å‡è®¾$\boldsymbol{k}_j$æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œé‚£ä¹ˆå¯ä»¥å†™å‡º<br />
\begin{equation}\sum</em>}^n e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j} = n\times \frac{1}{n}\sum</em>}^n e^{\lambda \boldsymbol{q<em _theta="\theta">i\cdot \boldsymbol{k}_j}\approx n\,\mathbb{E}_j[e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}]\end{equation}<br />
å°†æ‰€æœ‰æ±‚å’Œéƒ½ç”¨åŒæ ·çš„è¿‘ä¼¼ä»£æ›¿ï¼Œæˆ‘ä»¬å¾—åˆ°<br />
\begin{equation}\mathcal{H}_i \approx \log n + \log \mathbb{E}_j[e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}] - \frac{\lambda\,\mathbb{E}_j[e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}(\boldsymbol{q}_i\cdot \boldsymbol{k}_j)]}{\mathbb{E}_j[e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}]} \end{equation}<br />
ç•™æ„åˆ°ä¸€èˆ¬æƒ…å†µä¸‹$\boldsymbol{q}_i,\boldsymbol{k}_j$éƒ½æ˜¯Layer Normå‡ºæ¥ä¹‹åå†æ¥ä¸€ä¸ªDenseå±‚ï¼Œè€ŒDenseå±‚æ¥è¿‘æ­£äº¤å˜æ¢ï¼ˆå‚è€ƒ<a href="/archives/7180">ã€Šä»å‡ ä½•è§†è§’æ¥ç†è§£æ¨¡å‹å‚æ•°çš„åˆå§‹åŒ–ç­–ç•¥ã€‹</a>ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿‘ä¼¼åœ°å‡è®¾$\boldsymbol{q}_i,\boldsymbol{k}_j$éƒ½æ˜¯æ¨¡é•¿ä¸º$\sqrt{d}$çš„å‘é‡ï¼Œæ‰€ä»¥$\boldsymbol{q}_i\cdot \boldsymbol{k}_j=d\cos(\boldsymbol{q}_i,\boldsymbol{k}_j)$ï¼›ç„¶åè¿›ä¸€æ­¥å‡è®¾$\boldsymbol{k}_j$å‡åŒ€åœ°åˆ†å¸ƒåœ¨åŠå¾„ä¸º$\sqrt{d}$çš„çƒé¢ä¸Šï¼Œé‚£ä¹ˆå¯¹$\boldsymbol{k}_j$çš„æœŸæœ›å¯ä»¥è½¬åŒ–ä¸ºå¯¹$\boldsymbol{q}_i,\boldsymbol{k}_j$å¤¹è§’çš„æœŸæœ›ï¼Œå³<br />
\begin{equation}\mathcal{H}_i \approx \log n + \log \mathbb{E}</em>}[e^{\lambda d \cos\theta}] - \frac{\lambda d\,\mathbb{E<em _theta="\theta">{\theta}[e^{\lambda d \cos\theta}\cos\theta]}{\mathbb{E}</em>}[e^{\lambda d \cos\theta}]} \end{equation<br />
å…¶ä¸­$\theta$æœä»çš„åˆ†å¸ƒå°±æ˜¯çƒé¢ä¸Šä»»æ„ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’åˆ†å¸ƒï¼Œæˆ‘ä»¬åœ¨<a href="/archives/7076">ã€Šnç»´ç©ºé—´ä¸‹ä¸¤ä¸ªéšæœºå‘é‡çš„å¤¹è§’åˆ†å¸ƒã€‹</a>è®¨è®ºè¿‡ã€‚æ¥ä¸‹æ¥å¯ä»¥åƒ<a href="/archives/7695">ã€Šæœ€å°ç†µåŸç†ï¼ˆå…­ï¼‰ï¼šè¯å‘é‡çš„ç»´åº¦åº”è¯¥æ€ä¹ˆé€‰æ‹©ï¼Ÿã€‹</a>çš„â€œ<a href="/archives/7695#%E8%BF%91%E4%BC%BC%E4%BC%B0%E8%AE%A1">è¿‘ä¼¼ä¼°è®¡</a>â€ä¸€æ ·ï¼Œç”¨æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼å¾—åˆ°<br />
\begin{equation}\mathcal{H}_i \approx \log n - 0.24\lambda d + \mathcal{O}(1) \end{equation}<br />
å› æ­¤ï¼Œä¸ºäº†æŠµæ¶ˆé•¿åº¦$n$çš„å½±å“ï¼Œæˆ‘ä»¬è®©$\log n - 0.24\lambda d = 0$ï¼Œä»è€Œå¾—å‡º$\lambda = \log n / (0.24 d)$ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬çŸ¥é“è¿™åªæ˜¯ä¼°è®¡ï¼Œæ‰€ä»¥æ²¡å¿…è¦ä¿ç•™ç³»æ•°$0.24$äº†ï¼Œå€’ä¸å¦‚ç›´æ¥å¼•å…¥è¶…å‚æ•°$\kappa$ï¼Œä½¿å¾—<br />
\begin{equation}\lambda = \frac{\kappa\log n}{d}\end{equation}<br />
è¿™å°±æ˜¯å¯¹åº”å¼$\eqref{eq:ei}$äº†ã€‚</p>
<h2 id="_4">ç›¸å…³ç»“æœ</h2>
<p>åœ¨é˜…è¯»ACL2022çš„æŠ•ç¨¿è®ºæ–‡æ—¶ï¼Œå‘ç°ä¸Šé¢æœ‰ä¸€ç¯‡<a href="https://openreview.net/forum?id=qc9O2EtrMI-">ã€ŠOvercoming a Theoretical Limitation of Self-Attentionã€‹</a>ï¼Œç»™å‡ºäº†ç›¸è¿‘çš„ç»“æœï¼ˆè®ºæ–‡4.3èŠ‚çš„å…¬å¼1ï¼‰ï¼š<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\log n}{\sqrt{d}}QK^{\top}\right)V\end{equation}<br />
ä¸è¿‡ï¼Œè¯¥è®ºæ–‡å¹¶æ²¡æœ‰å¤ªæ·±åˆ»çš„ç†è®ºåˆ†æï¼Œåªæ˜¯æ„å»ºäº†ä¸¤ä¸ªç‰¹æ®Šçš„caseæ¥æµ‹è¯•Attentionçš„æ€§èƒ½ï¼Œæµ‹è¯•å‘ç°å¾€ç¼©æ”¾å› å­ä¹˜ä¸Š$\log n$æœ‰åŠ©äºæ³›åŒ–é•¿åº¦ï¼Œæ‰€ä»¥å°±æå‡ºæ¥äº†ã€‚</p>
<p>ç„¶è€Œå¯ä»¥çœ‹å‡ºï¼Œå¦‚æœæŒ‰ç…§é»˜è®¤çº¦å®š$\log$ç”¨è‡ªç„¶å¯¹æ•°çš„è¯ï¼Œé‚£ä¹ˆä¸Šå¼å¾ˆæ˜æ˜¾æ˜¯ä¸åˆç†çš„ï¼Œå› ä¸ºå½“$n$è¾ƒå¤§æ—¶ï¼Œç¼©æ”¾å› å­è¿‡å¤§ï¼Œä¼šå¯¼è‡´ä¸¥é‡çš„æ¢¯åº¦æ¶ˆå¤±ã€‚åªä¸è¿‡è¯¥è®ºæ–‡åªæ˜¯åœ¨æœºå™¨ç¿»è¯‘ä¸Šåšå®éªŒï¼Œæµ‹å¾—éƒ½æ˜¯$n=20$çº§åˆ«çš„åºåˆ—ï¼Œæ‰€ä»¥å°±æ²¡æœ‰æ˜¾ç¤ºå‡ºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚</p>
<h2 id="_5">æ–‡ç« æ€»ç»“</h2>
<p>æœ¬æ–‡ä»ç†µä¸å˜æ€§çš„è§’åº¦é‡æ–°æ¨å¯¼äº†Scaled Dot-Product Attentionä¸­çš„Scaleæ“ä½œï¼Œå¾—åˆ°äº†ä¸€ä¸ªæ–°çš„ç¼©æ”¾å› å­ã€‚åˆæ­¥çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ–°çš„ç¼©æ”¾å› å­ä¸æ”¹å˜å·²æœ‰çš„è®­ç»ƒæ€§èƒ½ï¼Œå¹¶ä¸”å¯¹é•¿åº¦å¤–æ¨å…·æœ‰æ›´å¥½çš„ç»“æœã€‚</p>
<p><em><strong>è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š</strong><a href="https://spaces.ac.cn/archives/8823">https://spaces.ac.cn/archives/8823</a></em></p>
<p><em><strong>æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="ã€Šç§‘å­¦ç©ºé—´FAQã€‹">ã€Šç§‘å­¦ç©ºé—´FAQã€‹</a></p>
<p><strong>å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚</strong></p>
<p><strong>å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼</strong></p>
<p>æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>å¾®ä¿¡æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>æ”¯ä»˜å®æ‰“èµ</p>
<p>å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>ç‚¹å‡»è¿™é‡Œ</strong></a>æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚</p>
<p><strong>å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š</strong></p>
<p>è‹å‰‘æ—. (Dec. 21, 2021). ã€Šä»ç†µä¸å˜æ€§çœ‹Attentionçš„Scaleæ“ä½œ ã€‹[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8823">https://spaces.ac.cn/archives/8823</a></p>
<p>@online{kexuefm-8823,<br />
title={ä»ç†µä¸å˜æ€§çœ‹Attentionçš„Scaleæ“ä½œ},<br />
author={è‹å‰‘æ—},<br />
year={2021},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/8823}},<br />
} </p>
<hr />
<h2 id="_6">å…¬å¼æ¨å¯¼ä¸æ³¨é‡Š</h2>
<h3 id="1">1. ç†µçš„åŸºæœ¬å®šä¹‰ä¸æ€§è´¨</h3>
<h4 id="11-shannon">1.1 Shannonç†µ</h4>
<p>Shannonç†µï¼ˆä¿¡æ¯ç†µï¼‰å®šä¹‰ä¸ºï¼š
\begin{equation}
H(p) = -\sum_{i=1}^n p_i \log p_i \tag{1}
\end{equation}</p>
<p>å…¶ä¸­ $p = (p_1, \ldots, p_n)$ æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œæ»¡è¶³ $\sum_i p_i = 1$ å’Œ $p_i \geq 0$ã€‚</p>
<p><strong>çº¦å®š</strong>ï¼š$0 \log 0 = 0$ï¼ˆæé™æ„ä¹‰ä¸‹ï¼‰ã€‚</p>
<p><strong>ç‰©ç†æ„ä¹‰</strong>ï¼šç†µåº¦é‡éšæœºå˜é‡çš„ä¸ç¡®å®šæ€§æˆ–ä¿¡æ¯é‡ã€‚
- $H = 0$ï¼šç¡®å®šæ€§åˆ†å¸ƒï¼ˆæŸä¸ª $p_i = 1$ï¼‰
- $H = \log n$ï¼šå‡åŒ€åˆ†å¸ƒï¼ˆæœ€å¤§ä¸ç¡®å®šæ€§ï¼‰</p>
<h4 id="12">1.2 ç†µçš„åŸºæœ¬æ€§è´¨</h4>
<p><strong>æ€§è´¨1ï¼ˆéè´Ÿæ€§ï¼‰</strong>ï¼š
\begin{equation}
H(p) \geq 0 \tag{2}
\end{equation}</p>
<p>ç­‰å·æˆç«‹å½“ä¸”ä»…å½“åˆ†å¸ƒæ˜¯ç¡®å®šæ€§çš„ã€‚</p>
<p><strong>æ€§è´¨2ï¼ˆä¸Šç•Œï¼‰</strong>ï¼š
\begin{equation}
H(p) \leq \log n \tag{3}
\end{equation}</p>
<p>ç­‰å·æˆç«‹å½“ä¸”ä»…å½“ $p_i = 1/n$ å¯¹æ‰€æœ‰ $i$ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰ã€‚</p>
<p><strong>è¯æ˜</strong>ï¼šä½¿ç”¨å‡¸æ€§å’ŒJensenä¸ç­‰å¼ã€‚å®šä¹‰ $u_i = 1/n$ï¼š
\begin{equation}
H(u) - H(p) = -\sum_i \frac{1}{n}\log\frac{1}{n} + \sum_i p_i \log p_i = \log n + \sum_i p_i \log p_i \tag{4}
\end{equation}</p>
<p>ä½¿ç”¨ $\log$ çš„å‡¹æ€§ï¼š
\begin{equation}
\sum_i p_i \log p_i \leq \sum_i p_i \log u_i = \log\frac{1}{n} = -\log n \tag{5}
\end{equation}</p>
<p>å› æ­¤ $H(p) \leq H(u) = \log n$ã€‚</p>
<p><strong>æ€§è´¨3ï¼ˆå¯åŠ æ€§ï¼‰</strong>ï¼šå¯¹äºç‹¬ç«‹éšæœºå˜é‡ $X, Y$ï¼š
\begin{equation}
H(X, Y) = H(X) + H(Y) \tag{6}
\end{equation}</p>
<p><strong>æ€§è´¨4ï¼ˆå‡¹æ€§ï¼‰</strong>ï¼š$H(p)$ æ˜¯ $p$ çš„å‡¹å‡½æ•°ï¼š
\begin{equation}
H(\lambda p + (1-\lambda)q) \geq \lambda H(p) + (1-\lambda)H(q) \tag{7}
\end{equation}</p>
<h4 id="13-renyi">1.3 RÃ©nyiç†µæ—</h4>
<p>RÃ©nyiç†µæ˜¯Shannonç†µçš„æ¨å¹¿ï¼š
\begin{equation}
H_{\alpha}(p) = \frac{1}{1-\alpha}\log\sum_{i=1}^n p_i^{\alpha}, \quad \alpha \geq 0, \alpha \neq 1 \tag{8}
\end{equation}</p>
<p><strong>ç‰¹æ®Šæƒ…å†µ</strong>ï¼š
- $\alpha \to 1$ï¼š$H_1(p) = H(p)$ï¼ˆShannonç†µï¼‰
- $\alpha = 0$ï¼š$H_0(p) = \log n$ï¼ˆHartleyç†µï¼‰
- $\alpha = 2$ï¼š$H_2(p) = -\log\sum_i p_i^2$ï¼ˆç¢°æ’ç†µï¼‰
- $\alpha \to \infty$ï¼š$H_{\infty}(p) = -\log\max_i p_i$ï¼ˆæœ€å°ç†µï¼‰</p>
<p><strong>å•è°ƒæ€§</strong>ï¼š
\begin{equation}
H_{\alpha}(p) \geq H_{\beta}(p) \quad \text{å¯¹} \quad \alpha &lt; \beta \tag{9}
\end{equation}</p>
<h3 id="2-attention">2. Attentionæœºåˆ¶ä¸­çš„ç†µ</h3>
<h4 id="21-attention">2.1 Attentionæ¦‚ç‡åˆ†å¸ƒ</h4>
<p>Scaled Dot-Product Attentionå®šä¹‰ï¼š
\begin{equation}
a_{ij} = \frac{\exp(s_{ij}/\tau)}{\sum_{k=1}^n \exp(s_{ik}/\tau)} \tag{10}
\end{equation}</p>
<p>å…¶ä¸­ï¼š
- $s_{ij} = q_i \cdot k_j$ï¼šæ³¨æ„åŠ›åˆ†æ•°
- $\tau$ï¼šæ¸©åº¦å‚æ•°ï¼ˆæ ‡å‡†Attentionä¸­ $\tau = \sqrt{d}$ï¼‰
- $n$ï¼šåºåˆ—é•¿åº¦</p>
<p>å›ºå®š $i$ï¼Œ$(a_{i1}, \ldots, a_{in})$ æ˜¯å…³äº $j$ çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p>
<h4 id="22-attention">2.2 Attentionçš„ç†µ</h4>
<p>ç¬¬ $i$ ä¸ªqueryçš„æ³¨æ„åŠ›ç†µï¼š
\begin{equation}
H_i = -\sum_{j=1}^n a_{ij} \log a_{ij} \tag{11}
\end{equation}</p>
<p>ä»£å…¥Softmaxå½¢å¼ï¼š
\begin{equation}
a_{ij} = \frac{\exp(\lambda s_{ij})}{Z_i}, \quad Z_i = \sum_k \exp(\lambda s_{ik}), \quad \lambda = 1/\tau \tag{12}
\end{equation}</p>
<p>åˆ™ï¼š
\begin{equation}
H_i = -\sum_j a_{ij}(\lambda s_{ij} - \log Z_i) = \log Z_i - \lambda\sum_j a_{ij} s_{ij} \tag{13}
\end{equation}</p>
<p>å®šä¹‰åŠ æƒå¹³å‡åˆ†æ•°ï¼š
\begin{equation}
\bar{s}<em ij="ij">i = \sum_j a</em>} s_{ij} = \mathbb{E<em ij="ij">{j \sim a_i}[s</em>
\end{equation}}] \tag{14</p>
<p>åˆ™ï¼š
\begin{equation}
H_i = \log Z_i - \lambda \bar{s}_i \tag{15}
\end{equation}</p>
<h4 id="23">2.3 ç†µçš„å«ä¹‰</h4>
<p>åœ¨Attentionä¸­ï¼Œ$H_i$ åº¦é‡ï¼š
- <strong>æ³¨æ„åŠ›çš„é›†ä¸­ç¨‹åº¦</strong>ï¼š$H_i$ å°è¡¨ç¤ºæ³¨æ„åŠ›é›†ä¸­åœ¨å°‘æ•°token
- <strong>ä¸ç¡®å®šæ€§</strong>ï¼š$H_i$ å¤§è¡¨ç¤ºæ³¨æ„åŠ›åˆ†æ•£åœ¨å¤šä¸ªtoken
- <strong>æœ‰æ•ˆèŒƒå›´</strong>ï¼š$H_i \in [0, \log n]$</p>
<p><strong>æç«¯æƒ…å†µ</strong>ï¼š
- $H_i = 0$ï¼š$a_{ij} = \delta_{j, j^*}$ï¼ˆone-hotï¼Œå®Œå…¨é›†ä¸­ï¼‰
- $H_i = \log n$ï¼š$a_{ij} = 1/n$ï¼ˆå‡åŒ€ï¼Œå®Œå…¨åˆ†æ•£ï¼‰</p>
<h3 id="3">3. ç†µä¸å˜æ€§çš„æ•°å­¦æ¨å¯¼</h3>
<h4 id="31">3.1 é—®é¢˜é™ˆè¿°</h4>
<p><strong>è§‚å¯Ÿ</strong>ï¼šå½“åºåˆ—é•¿åº¦ $n$ å¢åŠ æ—¶ï¼Œå¦‚æœä¿æŒ $\lambda$ ä¸å˜ï¼Œç†µ $H_i$ ä¼šå¢åŠ ã€‚</p>
<p><strong>ç›®æ ‡</strong>ï¼šæ‰¾åˆ°ä¾èµ–äº $n$ çš„ç¼©æ”¾å› å­ $\lambda(n)$ï¼Œä½¿å¾—ç†µå¯¹ $n$ ä¸æ•æ„Ÿï¼š
\begin{equation}
\frac{\partial H_i}{\partial n} \approx 0 \tag{16}
\end{equation}</p>
<h4 id="32">3.2 ç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾</h4>
<p>å‡è®¾ $s_{ij}$ ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œæœä»æŸåˆ†å¸ƒ $p(s)$ã€‚åˆ™é…åˆ†å‡½æ•°çš„æœŸæœ›ï¼š
\begin{equation}
\mathbb{E}[\log Z_i] = \mathbb{E}\left[\log\sum_{k=1}^n \exp(\lambda s_{ik})\right] \tag{17}
\end{equation}</p>
<p>ä½¿ç”¨å¤§æ•°å®šå¾‹çš„æ¨å¹¿ï¼ˆå¯¹æ•°æ±‚å’ŒæŒ‡æ•°çš„é›†ä¸­æ€§ï¼‰ï¼š
\begin{equation}
\log\sum_{k=1}^n \exp(\lambda s_k) = \log\left(n \cdot \frac{1}{n}\sum_k \exp(\lambda s_k)\right) = \log n + \log\left(\frac{1}{n}\sum_k \exp(\lambda s_k)\right) \tag{18}
\end{equation}</p>
<p>å½“ $n \to \infty$ï¼š
\begin{equation}
\frac{1}{n}\sum_k \exp(\lambda s_k) \to \mathbb{E}[\exp(\lambda s)] \tag{19}
\end{equation}</p>
<p>å› æ­¤ï¼š
\begin{equation}
\mathbb{E}[\log Z_i] \approx \log n + \log\mathbb{E}[\exp(\lambda s)] \tag{20}
\end{equation}</p>
<h4 id="33">3.3 ç¬¬äºŒé¡¹çš„ä¼°è®¡</h4>
<p>å¯¹äº $\bar{s}<em ij="ij">i = \sum_j a</em>$ ä¸Šã€‚} s_{ij}$ï¼Œæ³¨æ„åˆ°å½“ $\lambda$ è¾ƒå¤§æ—¶ï¼ŒSoftmaxä¼šä½¿æ¦‚ç‡é›†ä¸­åœ¨è¾ƒå¤§çš„ $s_{ij</p>
<p><strong>è¿‘ä¼¼1ï¼ˆç²—ç³™ï¼‰</strong>ï¼šå‡è®¾æ³¨æ„åŠ›é›†ä¸­åœ¨top-kä¸ªä½ç½®ï¼Œè¿™äº›ä½ç½®çš„ $s$ å€¼çº¦ä¸ºæœŸæœ›å€¼åŠ è‹¥å¹²å€æ ‡å‡†å·®ï¼š
\begin{equation}
\bar{s}_i \approx \mathbb{E}[s] + c\sigma, \quad c = O(1) \tag{21}
\end{equation}</p>
<p>å…¶ä¸­ $\sigma = \sqrt{\text{Var}[s]}$ã€‚</p>
<p>ä»£å…¥å¼(15)ï¼š
\begin{equation}
H_i \approx \log n + \log\mathbb{E}[\exp(\lambda s)] - \lambda(\mathbb{E}[s] + c\sigma) \tag{22}
\end{equation}</p>
<h4 id="34">3.4 é«˜æ–¯åˆ†å¸ƒä¸‹çš„ç²¾ç¡®è®¡ç®—</h4>
<p>å‡è®¾ $s \sim \mathcal{N}(\mu, \sigma^2)$ï¼Œåˆ™ï¼š
\begin{equation}
\mathbb{E}[\exp(\lambda s)] = \exp\left(\lambda\mu + \frac{\lambda^2\sigma^2}{2}\right) \tag{23}
\end{equation}</p>
<p>ä»£å…¥ï¼š
\begin{equation}
H_i \approx \log n + \lambda\mu + \frac{\lambda^2\sigma^2}{2} - \lambda(\mu + c\sigma) = \log n + \frac{\lambda^2\sigma^2}{2} - \lambda c\sigma \tag{24}
\end{equation}</p>
<h4 id="35">3.5 çƒé¢å‡åŒ€åˆ†å¸ƒï¼ˆæ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼ï¼‰</h4>
<p>æ›´ä¸€èˆ¬åœ°ï¼Œå‡è®¾ $q, k$ æ˜¯ $d$ ç»´å•ä½çƒé¢ä¸Šçš„éšæœºå‘é‡ï¼Œ$s = q \cdot k$ã€‚</p>
<p><strong>æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼</strong>ï¼šå¯¹äºç§¯åˆ†
\begin{equation}
\mathbb{E}[\exp(\lambda s)] = \int_{-1}^{1} \exp(\lambda s) p(s) ds \tag{25}
\end{equation}</p>
<p>å…¶ä¸­ $p(s) \propto (1-s^2)^{(d-2)/2}$ï¼ˆä½™å¼¦åˆ†å¸ƒï¼‰ã€‚</p>
<p>å½“ $\lambda$ è¾ƒå¤§æ—¶ï¼Œè¢«ç§¯å‡½æ•°åœ¨ $s = 1$ é™„è¿‘æœ‰å³°å€¼ã€‚ä½¿ç”¨Laplaceæ–¹æ³•ï¼š
\begin{equation}
\int \exp(\lambda f(s)) g(s) ds \approx \exp(\lambda f(s^<em>)) g(s^</em>) \sqrt{\frac{2\pi}{\lambda|f''(s^*)|}} \tag{26}
\end{equation}</p>
<p>å…¶ä¸­ $s^*$ æ˜¯ $f(s)$ çš„æœ€å¤§å€¼ç‚¹ã€‚</p>
<p>å¯¹äº $f(s) = s$ï¼Œ$s^* = 1$ï¼Œä½†è¾¹ç•Œéœ€è¦ç‰¹æ®Šå¤„ç†ã€‚</p>
<p><strong>ç®€åŒ–ä¼°è®¡</strong>ï¼šå¯¹äºå¤§çš„ $d$ï¼Œ$p(s)$ é›†ä¸­åœ¨ $s \approx 0$ é™„è¿‘ï¼ˆé«˜ç»´æ­£äº¤æ€§ï¼‰ï¼Œè€Œ $\exp(\lambda s)$ åœ¨ $s = 1$ é™„è¿‘å¤§ã€‚ä¸¤è€…çš„æŠ˜è¡·ç»™å‡ºï¼š
\begin{equation}
\log\mathbb{E}[\exp(\lambda s)] \approx c_1\lambda - c_2\log d \tag{27}
\end{equation}</p>
<p>å…¶ä¸­ $c_1, c_2$ æ˜¯å¸¸æ•°ã€‚</p>
<h4 id="36">3.6 ç†µä¸å˜æ€§æ¡ä»¶</h4>
<p>ä¸ºäº†ä½¿ $H_i$ å¯¹ $n$ ä¸æ•æ„Ÿï¼Œéœ€è¦ï¼š
\begin{equation}
\frac{\partial H_i}{\partial n} = \frac{\partial}{\partial n}\left[\log n + f(\lambda, n)\right] \approx 0 \tag{28}
\end{equation}</p>
<p>å…¶ä¸­ $f(\lambda, n)$ åŒ…å«å…¶ä»–é¡¹ã€‚</p>
<p>ä¸»å¯¼é¡¹æ˜¯ $\log n$ï¼Œå› æ­¤éœ€è¦ $\lambda$ ä¾èµ–äº $n$ æ¥æŠµæ¶ˆï¼š
\begin{equation}
\lambda(n) \propto \log n \tag{29}
\end{equation}</p>
<p>å…·ä½“åœ°ï¼Œè®¾ $\lambda(n) = \kappa \log n$ï¼Œä»£å…¥å¼(24)ï¼ˆå¯¹äºé«˜æ–¯åˆ†å¸ƒï¼‰ï¼š
\begin{equation}
H_i \approx \log n + \frac{\kappa^2(\log n)^2\sigma^2}{2} - \kappa c\sigma \log n \tag{30}
\end{equation}</p>
<p>ç¬¬ä¸€é¡¹å’Œç¬¬ä¸‰é¡¹çš„ $\log n$ å¯ä»¥æŠµæ¶ˆï¼ˆè°ƒæ•´ $\kappa$ å’Œ $c$ï¼‰ï¼Œè€Œç¬¬äºŒé¡¹çš„ $(\log n)^2$ æ˜¯é«˜é˜¶é¡¹ã€‚</p>
<h4 id="37">3.7 ç²¾ç¡®çš„æ‹‰æ™®æ‹‰æ–¯è¿‘ä¼¼æ¨å¯¼</h4>
<p>å›åˆ°çƒé¢åˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ï¼š
\begin{equation}
I(\lambda) = \int_{-1}^{1} \exp(\lambda d \cos\theta) \sin^{d-2}\theta d\theta \tag{31}
\end{equation}</p>
<p>ä»¤ $s = \cos\theta$ï¼Œ$\sin\theta = \sqrt{1-s^2}$ï¼š
\begin{equation}
I(\lambda) = \int_{-1}^{1} \exp(\lambda d s) (1-s^2)^{(d-3)/2} ds \tag{32}
\end{equation}</p>
<p>å®šä¹‰ $h(s) = \lambda d s + \frac{d-3}{2}\log(1-s^2)$ï¼Œæ‰¾æœ€å¤§å€¼ç‚¹ï¼š
\begin{equation}
h'(s) = \lambda d - \frac{(d-3)s}{1-s^2} = 0 \tag{33}
\end{equation}</p>
<p>è§£å¾—ï¼š
\begin{equation}
s^* = \frac{\lambda d}{(d-3) + \lambda d} \approx 1 - \frac{d-3}{\lambda d} \quad (\text{å½“ } \lambda d \gg d) \tag{34}
\end{equation}</p>
<p>äºŒé˜¶å¯¼æ•°ï¼š
\begin{equation}
h''(s^<em>) = -\frac{(d-3)(1+s^{</em>2})}{(1-s^{*2})^2} \approx -(\lambda d)^2 / (2(d-3)) \tag{35}
\end{equation}</p>
<p>Laplaceè¿‘ä¼¼ï¼š
\begin{equation}
I(\lambda) \approx \exp(h(s^<em>)) \sqrt{\frac{2\pi}{|h''(s^</em>)|}} = \exp(\lambda d - 0.24\lambda d) \cdot \sqrt{\frac{2\pi(d-3)}{(\lambda d)^2}} \tag{36}
\end{equation}</p>
<p>ç®€åŒ–ï¼š
\begin{equation}
\log I(\lambda) \approx 0.76\lambda d - \log(\lambda d) + O(1) \tag{37}
\end{equation}</p>
<p>ä»£å…¥ç†µçš„å…¬å¼ï¼Œç¬¬äºŒé¡¹ $\bar{s} \approx s^* \approx 1$ï¼ˆé«˜æ¦‚ç‡é›†ä¸­åœ¨å¤§è§’åº¦ï¼‰ï¼Œå¾—ï¼š
\begin{equation}
H_i \approx \log n + 0.76\lambda d - \log(\lambda d) - \lambda d = \log n - 0.24\lambda d - \log(\lambda d) \tag{38}
\end{equation}</p>
<p>ä¸ºäº†æŠµæ¶ˆ $\log n$ï¼Œéœ€è¦ï¼š
\begin{equation}
0.24\lambda d \approx \log n \tag{39}
\end{equation}</p>
<p>å³ï¼š
\begin{equation}
\lambda = \frac{\log n}{0.24 d} \tag{40}
\end{equation}</p>
<p><strong>å½’ä¸€åŒ–</strong>ï¼šè®¾è®­ç»ƒé•¿åº¦ä¸º $n_0 = 512$ï¼Œç¼©æ”¾å› å­ $\lambda_0 = 1/\sqrt{d}$ï¼Œåˆ™ï¼š
\begin{equation}
\frac{\log n}{0.24 d} = \frac{\log 512}{0.24 d} \cdot \frac{\log n}{\log 512} = \lambda_0 \cdot \frac{\log n}{\log 512} \tag{41}
\end{equation}</p>
<p>è¿™ç»™å‡ºäº†ç†µä¸å˜æ€§Softmaxçš„ç¼©æ”¾å› å­ï¼</p>
<h3 id="4">4. æ–°ç¼©æ”¾å› å­çš„ç†è®ºè¯æ˜</h3>
<h4 id="41-attention">4.1 æ ‡å‡†Attentionå›é¡¾</h4>
<p>æ ‡å‡†Attentionï¼š
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}\right)V \tag{42}
\end{equation}</p>
<p>ç¼©æ”¾å› å­ $\lambda = 1/\sqrt{d}$ åŸºäºæ–¹å·®å½’ä¸€åŒ–ï¼šè‹¥ $q, k$ çš„å…ƒç´ æ˜¯ç‹¬ç«‹çš„ï¼Œå‡å€¼0æ–¹å·®1ï¼Œåˆ™ï¼š
\begin{equation}
\mathbb{E}[q \cdot k] = 0, \quad \text{Var}[q \cdot k] = d \tag{43}
\end{equation}</p>
<p>é™¤ä»¥ $\sqrt{d}$ ä½¿æ–¹å·®ä¸º1ã€‚</p>
<h4 id="42-attention">4.2 ç†µä¸å˜æ€§Attention</h4>
<p>ç†µä¸å˜æ€§Attentionï¼š
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{\log_{512} n}{\sqrt{d}}QK^{\top}\right)V \tag{44}
\end{equation}</p>
<p>å…¶ä¸­ï¼š
\begin{equation}
\log_{512} n = \frac{\log n}{\log 512} \tag{45}
\end{equation}</p>
<h4 id="43">4.3 å¦ä¸€ç§è¡¨è¿°</h4>
<p>å¼•å…¥è¶…å‚æ•° $\kappa$ï¼š
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{\kappa\log n}{d}QK^{\top}\right)V \tag{46}
\end{equation}</p>
<p>å½“ $n = 512$ æ—¶ï¼Œåº”é€€åŒ–ä¸ºæ ‡å‡†Attentionï¼š
\begin{equation}
\frac{\kappa\log 512}{d} = \frac{1}{\sqrt{d}} \tag{47}
\end{equation}</p>
<p>è§£å¾—ï¼š
\begin{equation}
\kappa = \frac{d}{\sqrt{d}\log 512} = \frac{\sqrt{d}}{\log 512} \tag{48}
\end{equation}</p>
<p>ä»£å…¥å¾—å¼(44)ã€‚</p>
<h4 id="44">4.4 ç†è®ºæ€§è´¨</h4>
<p><strong>æ€§è´¨1</strong>ï¼šå½“ $n = 512$ æ—¶ï¼Œç†µä¸å˜æ€§Attentionç­‰äºæ ‡å‡†Attentionã€‚</p>
<p><strong>æ€§è´¨2</strong>ï¼šå½“ $n &gt; 512$ æ—¶ï¼Œç¼©æ”¾å› å­å¢å¤§ï¼ŒSoftmaxæ›´"å°–é”"ï¼Œæ³¨æ„åŠ›æ›´é›†ä¸­ã€‚</p>
<p><strong>æ€§è´¨3</strong>ï¼šç†µ $H_i$ è¿‘ä¼¼ä¸ä¾èµ–äº $n$ã€‚</p>
<p><strong>å®šç†</strong>ï¼šåœ¨ç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾ä¸‹ï¼Œä½¿ç”¨ç¼©æ”¾å› å­ $\lambda(n) = \frac{\log n}{\log 512} \cdot \frac{1}{\sqrt{d}}$ï¼Œæ³¨æ„åŠ›ç†µæ»¡è¶³ï¼š
\begin{equation}
\left|\frac{\partial H_i}{\partial n}\right| = O\left(\frac{1}{n}\right) \tag{49}
\end{equation}</p>
<p>å³ç†µå¯¹ $n$ çš„å¯¼æ•°è¶‹äº0ã€‚</p>
<h3 id="5-jl">5. ä¸JLå¼•ç†çš„è”ç³»</h3>
<h4 id="51-jl">5.1 JLå¼•ç†å›é¡¾</h4>
<p>Johnson-Lindenstrausså¼•ç†ï¼šè¦ä¿æŒ $n$ ä¸ªç‚¹ä¹‹é—´çš„æˆå¯¹è·ç¦»ï¼Œéœ€è¦çš„ç»´åº¦ä¸ºï¼š
\begin{equation}
d = O(\log n / \epsilon^2) \tag{50}
\end{equation}</p>
<p>å¯¹äºå›ºå®šçš„ $\epsilon$ï¼Œ$d \propto \log n$ã€‚</p>
<h4 id="52">5.2 ç»´åº¦ä¸è¶³çš„è¡¥å¿</h4>
<p>åœ¨Attentionä¸­ï¼Œ$d$ é€šå¸¸æ˜¯å›ºå®šçš„ï¼ˆå¦‚64æˆ–128ï¼‰ï¼Œä½†ç†æƒ³æƒ…å†µä¸‹åº”è¯¥æ˜¯ $d \propto \log n$ã€‚</p>
<p><strong>è¡¥å¿æœºåˆ¶</strong>ï¼šæ—¢ç„¶ $d$ ä¸èƒ½éš $n$ å˜åŒ–ï¼Œæˆ‘ä»¬é€šè¿‡è°ƒæ•´ç¼©æ”¾å› å­æ¥è¡¥å¿ï¼š
\begin{equation}
\lambda(n) \propto \log n \tag{51}
\end{equation}</p>
<p><strong>ç›´è§‰</strong>ï¼š
- ç†æƒ³ï¼š$d_{\text{ideal}} = C\log n$ï¼Œ$\lambda = 1/\sqrt{d_{\text{ideal}}} = 1/\sqrt{C\log n}$
- å®é™…ï¼š$d_{\text{actual}} = \text{const}$ï¼Œ$\lambda = \text{const}$
- è¡¥å¿ï¼š$\lambda_{\text{new}} = \lambda_{\text{old}} \cdot f(n)$ï¼Œå…¶ä¸­ $f(n) \propto \log n$</p>
<h4 id="53">5.3 ä¿¡æ¯è®ºè§†è§’</h4>
<p>ç†µä¸ç¼–ç é•¿åº¦çš„å…³ç³»ï¼ˆShannonæºç¼–ç å®šç†ï¼‰ï¼š
\begin{equation}
L \geq H(p) \tag{52}
\end{equation}</p>
<p>å…¶ä¸­ $L$ æ˜¯å¹³å‡ç¼–ç é•¿åº¦ã€‚</p>
<p>åœ¨ $d$ ç»´ç©ºé—´ä¸­åµŒå…¥ $n$ ä¸ªç‚¹ï¼Œéœ€è¦ï¼š
\begin{equation}
d \geq \frac{H}{\log 2} \approx \log n \tag{53}
\end{equation}</p>
<p>è¿™ä¸JLå¼•ç†ä¸€è‡´ï¼</p>
<h3 id="6">6. ä¿¡æ¯è®ºè§†è§’çš„æ·±å…¥åˆ†æ</h3>
<h4 id="61">6.1 äº’ä¿¡æ¯</h4>
<p>Attentionå¯ä»¥çœ‹ä½œæ˜¯Queryå’ŒKeyä¹‹é—´çš„ä¿¡æ¯ä¼ é€’ã€‚äº’ä¿¡æ¯å®šä¹‰ä¸ºï¼š
\begin{equation}
I(Q; K) = H(K) - H(K|Q) \tag{54}
\end{equation}</p>
<p>å…¶ä¸­æ¡ä»¶ç†µï¼š
\begin{equation}
H(K|Q) = \mathbb{E}_Q[H(K|Q=q)] = \mathbb{E}_Q[H_i] \tag{55}
\end{equation}</p>
<p><strong>ç†µä¸å˜æ€§çš„å«ä¹‰</strong>ï¼šä¿æŒ $H(K|Q)$ ä¸éš $n$ å˜åŒ–ï¼Œæ„å‘³ç€ï¼š
- ç»™å®šQueryåï¼ŒKeyçš„ä¸ç¡®å®šæ€§ä¿æŒä¸å˜
- äº’ä¿¡æ¯ $I(Q; K) = H(K) - H(K|Q) \approx \log n - \text{const}$
- äº’ä¿¡æ¯éš $n$ å¯¹æ•°å¢é•¿ï¼ˆåˆç†ï¼Œæ›´å¤šKeyæä¾›æ›´å¤šä¿¡æ¯ï¼‰</p>
<h4 id="62">6.2 ç‡å¤±çœŸç†è®º</h4>
<p>ç‡å¤±çœŸå‡½æ•°å®šä¹‰ä¸ºï¼š
\begin{equation}
R(D) = \min_{p(\hat{K}|K): \mathbb{E}[d(K, \hat{K})] \leq D} I(K; \hat{K}) \tag{56}
\end{equation}</p>
<p>Attentionå¯ä»¥çœ‹ä½œæ˜¯ä¿¡æ¯ç“¶é¢ˆï¼š
\begin{equation}
\max_{a} I(K; V) - \beta H(a) \tag{57}
\end{equation}</p>
<p>ç†µä¸å˜æ€§ç¡®ä¿ç“¶é¢ˆå®½åº¦ï¼ˆ$H(a)$ï¼‰ä¿æŒç¨³å®šã€‚</p>
<h4 id="63-kl">6.3 KLæ•£åº¦ä¸äº¤å‰ç†µ</h4>
<p>å®šä¹‰Attentionçš„ç›®æ ‡åˆ†å¸ƒ $a^<em>$ å’Œå®é™…åˆ†å¸ƒ $a$ï¼ŒKLæ•£åº¦ï¼š
\begin{equation}
D_{KL}(a^</em> | a) = \sum_j a_j^<em> \log\frac{a_j^</em>}{a_j} = H(a^<em>, a) - H(a^</em>) \tag{58}
\end{equation}</p>
<p>å…¶ä¸­äº¤å‰ç†µï¼š
\begin{equation}
H(a^<em>, a) = -\sum_j a_j^</em> \log a_j \tag{59}
\end{equation}</p>
<p><strong>ä¼˜åŒ–ç›®æ ‡</strong>ï¼šæœ€å°åŒ– $D_{KL}$ ç­‰ä»·äºæœ€å°åŒ–äº¤å‰ç†µï¼ˆå›ºå®š $a^*$ï¼‰ã€‚</p>
<p>ç†µä¸å˜æ€§ç¡®ä¿äº¤å‰ç†µçš„å°ºåº¦ä¸éš $n$ å˜åŒ–ï¼Œæœ‰åˆ©äºä¼˜åŒ–ç¨³å®šæ€§ã€‚</p>
<h3 id="7">7. å®éªŒéªŒè¯ä¸åˆ†æ</h3>
<h4 id="71-mlm">7.1 MLMä»»åŠ¡çš„å¤–æ¨å®éªŒ</h4>
<p><strong>å®éªŒè®¾ç½®</strong>ï¼š
- æ¨¡å‹ï¼šRoFormer smallï¼ˆä¸GAU-Î±ç±»ä¼¼çš„ç»“æ„ï¼‰
- è®­ç»ƒé•¿åº¦ï¼š$n_{\text{train}} = 64$
- æµ‹è¯•é•¿åº¦ï¼š$n_{\text{test}} \in {64, 128, 256, 512, 1024}$
- ä»»åŠ¡ï¼šMasked Language Modeling (MLM)
- æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ï¼ˆAccuracyï¼‰</p>
<p><strong>ç»“æœ</strong>ï¼ˆä»åŸæ–‡å¼•ç”¨ï¼‰ï¼š</p>
<table>
<thead>
<tr>
<th>$n$</th>
<th>Attention-O</th>
<th>Attention-E</th>
<th>æå‡</th>
</tr>
</thead>
<tbody>
<tr>
<td>64</td>
<td>43.27</td>
<td>43.11</td>
<td>-0.37%</td>
</tr>
<tr>
<td>128</td>
<td>36.53</td>
<td>41.17</td>
<td>12.7%</td>
</tr>
<tr>
<td>256</td>
<td>23.02</td>
<td>34.04</td>
<td>47.8%</td>
</tr>
<tr>
<td>512</td>
<td>15.12</td>
<td>20.15</td>
<td>33.3%</td>
</tr>
<tr>
<td>1024</td>
<td>11.54</td>
<td>13.58</td>
<td>17.7%</td>
</tr>
</tbody>
</table>
<p><strong>åˆ†æ</strong>ï¼š
1. åœ¨è®­ç»ƒé•¿åº¦ï¼ˆ64ï¼‰é™„è¿‘ï¼Œä¸¤è€…æ€§èƒ½ç›¸å½“
2. å¤–æ¨åˆ°æ›´é•¿åºåˆ—æ—¶ï¼ŒAttention-Eæ˜¾è‘—ä¼˜äºAttention-O
3. $n=256$ æ—¶æå‡æœ€å¤§ï¼ˆ47.8%ï¼‰ï¼Œè¿™æ˜¯å› ä¸º $\log_{512} 256 \approx 0.83$ï¼Œæ¥è¿‘1ä½†æœ‰æ˜¾è‘—å·®å¼‚</p>
<h4 id="72">7.2 ç†µçš„å®æµ‹å€¼</h4>
<p><strong>å®éªŒ</strong>ï¼šç›´æ¥æµ‹é‡æ³¨æ„åŠ›åˆ†å¸ƒçš„å¹³å‡ç†µã€‚</p>
<p><strong>è®¾ç½®</strong>ï¼š
- éšæœºåˆå§‹åŒ–çš„Attentionå±‚
- ä¸åŒçš„åºåˆ—é•¿åº¦ $n$
- è®¡ç®— $\bar{H} = \frac{1}{n}\sum_{i=1}^n H_i$</p>
<p><strong>ç»“æœ</strong>ï¼ˆç†è®ºè®¡ç®—ï¼‰ï¼š</p>
<p><strong>Attention-O</strong> ($\lambda = 1/\sqrt{64} = 0.125$)ï¼š
\begin{equation}
\bar{H} \approx \log n - 0.24 \times 0.125 \times 64 = \log n - 1.92 \tag{60}
\end{equation}</p>
<table>
<thead>
<tr>
<th>$n$</th>
<th>$\log n$</th>
<th>ç†è®º $\bar{H}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>64</td>
<td>4.16</td>
<td>2.24</td>
</tr>
<tr>
<td>128</td>
<td>4.85</td>
<td>2.93</td>
</tr>
<tr>
<td>256</td>
<td>5.55</td>
<td>3.63</td>
</tr>
<tr>
<td>512</td>
<td>6.24</td>
<td>4.32</td>
</tr>
<tr>
<td>1024</td>
<td>6.93</td>
<td>5.01</td>
</tr>
</tbody>
</table>
<p><strong>Attention-E</strong> ($\lambda = \frac{\log n}{\log 512} \times 0.125$)ï¼š
é€šè¿‡è°ƒæ•´ï¼Œç†µåº”è¯¥ä¿æŒåœ¨ $\bar{H} \approx 2.24$ å·¦å³ï¼ˆä¸ $n=64$ æ—¶ä¸€è‡´ï¼‰ã€‚</p>
<h4 id="73-vs">7.3 ç†è®ºvså®éªŒçš„å·®å¼‚</h4>
<p><strong>è§‚å¯Ÿ</strong>ï¼šå®éªŒä¸­Attention-Eçš„æ€§èƒ½æå‡å¹¶ä¸å®Œå…¨ç¬¦åˆç†µå®Œå…¨ä¸å˜çš„é¢„æœŸã€‚</p>
<p><strong>åŸå› </strong>ï¼š
1. <strong>i.i.d.å‡è®¾ä¸ç²¾ç¡®</strong>ï¼šå®é™…ä¸­ $s_{ij}$ æœ‰ç»“æ„å’Œç›¸å…³æ€§
2. <strong>è¾¹ç•Œæ•ˆåº”</strong>ï¼šå° $n$ æ—¶ï¼Œæ¸è¿‘è¿‘ä¼¼ä¸å‡†ç¡®
3. <strong>è®­ç»ƒåŠ¨æ€</strong>ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­åˆ†å¸ƒä¼šå˜åŒ–
4. <strong>å…¶ä»–å› ç´ </strong>ï¼šä½ç½®ç¼–ç ã€æ®‹å·®è¿æ¥ç­‰å½±å“</p>
<h3 id="8">8. ç†è®ºæ¨å¹¿ä¸å˜ä½“</h3>
<h4 id="81">8.1 è‡ªé€‚åº”ç¼©æ”¾</h4>
<p>å›ºå®šçš„ $\lambda(n) = \frac{\log n}{\log 512}$ å¯¹æ‰€æœ‰å±‚ç›¸åŒã€‚ä½†å¯ä»¥è®¾è®¡ï¼š</p>
<p><strong>å±‚ç›¸å…³ç¼©æ”¾</strong>ï¼š
\begin{equation}
\lambda_{\ell}(n) = \frac{\log n}{\log 512} \cdot \alpha_{\ell} \tag{61}
\end{equation}</p>
<p>å…¶ä¸­ $\alpha_{\ell}$ æ˜¯ç¬¬ $\ell$ å±‚çš„å¯å­¦ä¹ å‚æ•°ã€‚</p>
<p><strong>ä½ç½®ç›¸å…³ç¼©æ”¾</strong>ï¼ˆDecoderï¼‰ï¼š
\begin{equation}
\lambda_i(n) = \frac{\log i}{\log 512}, \quad i = 1, \ldots, n \tag{62}
\end{equation}</p>
<p>å› ä¸ºç¬¬ $i$ ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å‰ $i$ ä¸ªtokenã€‚</p>
<h4 id="82">8.2 å…¶ä»–ç†µçš„é€‰æ‹©</h4>
<p>é™¤äº†Shannonç†µï¼Œè¿˜å¯ä»¥ä¼˜åŒ–å…¶ä»–ç†µï¼š</p>
<p><strong>RÃ©nyiç†µ</strong> ($\alpha = 2$)ï¼š
\begin{equation}
H_2 = -\log\sum_j a_{ij}^2 \tag{63}
\end{equation}</p>
<p>ä¼˜åŒ–ç›®æ ‡ï¼šä½¿ $H_2$ ä¸å˜ã€‚</p>
<p><strong>Tsallisç†µ</strong>ï¼š
\begin{equation}
S_q = \frac{1}{q-1}\left(1 - \sum_j a_{ij}^q\right) \tag{64}
\end{equation}</p>
<h4 id="83">8.3 è½¯ç†µä¸å˜æ€§</h4>
<p>ä¸¥æ ¼çš„ç†µä¸å˜æ€§å¯èƒ½è¿‡äºçº¦æŸã€‚å¯ä»¥è€ƒè™‘ï¼š</p>
<p><strong>è½¯çº¦æŸ</strong>ï¼š
\begin{equation}
\mathcal{L} = \mathcal{L}<em _text_target="\text{target">{\text{task}} + \beta |H_i - H</em>
\end{equation}}}|^2 \tag{65</p>
<p>å…¶ä¸­ $H_{\text{target}}$ æ˜¯æœŸæœ›çš„ç†µå€¼ã€‚</p>
<h3 id="9">9. å®è·µå»ºè®®ä¸æ³¨æ„äº‹é¡¹</h3>
<h4 id="91">9.1 å®ç°ç»†èŠ‚</h4>
<p><strong>PyTorchå®ç°</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">def</span><span class="w"> </span><span class="nf">entropy_invariant_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">n_train</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ç†µä¸å˜æ€§Attention</span>

<span class="sd">    Args:</span>
<span class="sd">        Q, K, V: [batch, n_heads, seq_len, d_k]</span>
<span class="sd">        n_train: è®­ç»ƒé•¿åº¦ï¼ˆé»˜è®¤512ï¼‰</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># å½“å‰åºåˆ—é•¿åº¦</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="c1"># è®¡ç®—ç¼©æ”¾å› å­</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Q</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">/</span> \
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Q</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>

    <span class="c1"># æ³¨æ„åŠ›åˆ†æ•°</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="c1"># Softmax</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># åŠ æƒæ±‚å’Œ</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn</span>
</code></pre></div>

<p><strong>æ•°å€¼ç¨³å®šæ€§</strong>ï¼š
- ä½¿ç”¨ <code>log-sum-exp</code> æŠ€å·§é¿å…ä¸Šæº¢
- å¯¹äºéå¸¸å¤§çš„ $n$ï¼Œ$\log n$ ä»ç„¶æ¸©å’Œå¢é•¿</p>
<h4 id="92">9.2 è¶…å‚æ•°é€‰æ‹©</h4>
<p><strong>åº•æ•°é€‰æ‹©</strong>ï¼šåŸæ–‡ä½¿ç”¨512ï¼Œä½†å¯ä»¥è°ƒæ•´ï¼š
- å¦‚æœä¸»è¦å¤„ç†çŸ­åºåˆ—ï¼ˆå¦‚å›¾åƒpatchesï¼‰ï¼Œä½¿ç”¨è¾ƒå°çš„åº•æ•°ï¼ˆå¦‚64ï¼‰
- å¦‚æœå¤„ç†é•¿åºåˆ—ï¼ˆå¦‚é•¿æ–‡æ¡£ï¼‰ï¼Œå¯ä»¥ä¿æŒ512æˆ–æ›´å¤§</p>
<p><strong>æ¸©åº¦è°ƒæ•´</strong>ï¼šå¯ä»¥å¼•å…¥å…¨å±€æ¸©åº¦å‚æ•°ï¼š
\begin{equation}
\lambda = \tau \cdot \frac{\log n}{\log 512 \sqrt{d}} \tag{66}
\end{equation}</p>
<p>å…¶ä¸­ $\tau$ æ˜¯å¯è°ƒèŠ‚çš„è¶…å‚æ•°ï¼ˆé»˜è®¤ä¸º1ï¼‰ã€‚</p>
<h4 id="93">9.3 ä¸å…¶ä»–æŠ€æœ¯çš„ç»“åˆ</h4>
<p><strong>ä¸RoPEç»“åˆ</strong>ï¼šç†µä¸å˜æ€§Softmaxå¯ä»¥ä¸æ—‹è½¬ä½ç½®ç¼–ç æ— ç¼ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡å¤–æ¨æ€§èƒ½ã€‚</p>
<p><strong>ä¸Flash Attentionç»“åˆ</strong>ï¼šFlash Attentionçš„å®ç°å¯ä»¥å¾ˆå®¹æ˜“åœ°é€‚é…ç†µä¸å˜æ€§ç¼©æ”¾ã€‚</p>
<p><strong>ä¸ç¨€ç–Attentionç»“åˆ</strong>ï¼šå¯¹äºç¨€ç–æ¨¡å¼ï¼ˆå¦‚å±€éƒ¨çª—å£ï¼‰ï¼Œ$n$ åº”è¯¥æ˜¯å®é™…å‚ä¸çš„tokenæ•°é‡ã€‚</p>
<h3 id="10">10. ç†è®ºå±€é™ä¸æœªæ¥æ–¹å‘</h3>
<h4 id="101">10.1 ç†è®ºå±€é™</h4>
<ol>
<li><strong>i.i.d.å‡è®¾</strong>ï¼šå®é™…ä¸­ $s_{ij}$ ä¸æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„</li>
<li><strong>æ¸è¿‘æ€§</strong>ï¼šæ¨å¯¼åŸºäºå¤§ $n$ å‡è®¾ï¼Œå° $n$ æ—¶ä¸å‡†ç¡®</li>
<li><strong>é™æ€åˆ†æ</strong>ï¼šæœªè€ƒè™‘è®­ç»ƒåŠ¨æ€å’Œåˆ†å¸ƒæ¼‚ç§»</li>
<li><strong>å•ä¸€ç›®æ ‡</strong>ï¼šåªä¼˜åŒ–ç†µï¼Œæœªè€ƒè™‘å…¶ä»–ç›®æ ‡ï¼ˆå¦‚æ¢¯åº¦ã€æ”¶æ•›é€Ÿåº¦ï¼‰</li>
</ol>
<h4 id="102">10.2 å¼€æ”¾é—®é¢˜</h4>
<p><strong>é—®é¢˜1</strong>ï¼šæ˜¯å¦å­˜åœ¨æ›´ä¼˜çš„ç¼©æ”¾å‡½æ•°ï¼Œä¸ä»…æ˜¯ $\log n$ å½¢å¼ï¼Ÿ</p>
<p><strong>é—®é¢˜2</strong>ï¼šå¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”è°ƒæ•´ç¼©æ”¾å› å­ï¼Ÿ</p>
<p><strong>é—®é¢˜3</strong>ï¼šç†µä¸å˜æ€§åœ¨ç”Ÿæˆä»»åŠ¡ï¼ˆDecoderï¼‰ä¸­çš„æœ€ä¼˜å½¢å¼æ˜¯ä»€ä¹ˆï¼Ÿ</p>
<p><strong>é—®é¢˜4</strong>ï¼šå¦‚ä½•ç†è®ºåŒ–åœ°ç»“åˆç†µä¸å˜æ€§å’Œæ¢¯åº¦æœ€å¤§åŒ–ä¸¤ä¸ªç›®æ ‡ï¼Ÿ</p>
<h4 id="103">10.3 æœªæ¥æ–¹å‘</h4>
<ol>
<li><strong>è‡ªé€‚åº”ç†µæ§åˆ¶</strong>ï¼šå­¦ä¹ æ¯ä¸€å±‚çš„ç›®æ ‡ç†µ</li>
<li><strong>å¤šç›®æ ‡ä¼˜åŒ–</strong>ï¼šåŒæ—¶ä¼˜åŒ–ç†µã€æ¢¯åº¦ã€ä¿¡æ¯ä¿æŒç­‰</li>
<li><strong>è·¨æ¨¡æ€æ‰©å±•</strong>ï¼šå›¾åƒ-æ–‡æœ¬Attentionçš„ç†µä¸å˜æ€§</li>
<li><strong>ç†è®ºå®Œå–„</strong>ï¼šæ›´ä¸¥æ ¼çš„éæ¸è¿‘åˆ†æ</li>
</ol>
<h3 id="11">11. æ€»ç»“</h3>
<h4 id="111">11.1 æ ¸å¿ƒè´¡çŒ®</h4>
<ol>
<li><strong>ç†µè§†è§’</strong>ï¼šå°†Attentionçš„ç¼©æ”¾é—®é¢˜ä¸ä¿¡æ¯ç†µè”ç³»èµ·æ¥</li>
<li><strong>ç†µä¸å˜æ€§åŸåˆ™</strong>ï¼šæå‡ºä¿æŒç†µå¯¹åºåˆ—é•¿åº¦ä¸æ•æ„Ÿçš„è®¾è®¡åŸåˆ™</li>
<li><strong>æ–°ç¼©æ”¾å› å­</strong>ï¼šæ¨å¯¼å‡º $\lambda(n) = \frac{\log n}{\log n_0} \cdot \frac{1}{\sqrt{d}}$</li>
<li><strong>å®éªŒéªŒè¯</strong>ï¼šåœ¨MLMä»»åŠ¡ä¸Šæ˜¾è‘—æå‡é•¿åº¦å¤–æ¨æ€§èƒ½</li>
</ol>
<h4 id="112">11.2 ç†è®ºæ¡†æ¶</h4>
<p>ç†µä¸å˜æ€§Attentionæä¾›äº†ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶ï¼š
\begin{equation}
\text{ä¿¡æ¯è®º} \xrightarrow{\text{ç†µ}} \text{æ³¨æ„åŠ›è®¾è®¡} \xrightarrow{\text{ç¼©æ”¾}} \text{å¤–æ¨æ€§èƒ½} \tag{67}
\end{equation}</p>
<p>è¿™ä¸ªæ¡†æ¶è¿æ¥äº†ï¼š
- Shannonä¿¡æ¯è®º
- Johnson-Lindenstrausså¼•ç†
- Attentionæœºåˆ¶
- é•¿åº¦å¤–æ¨</p>
<h4 id="113">11.3 å®è·µä»·å€¼</h4>
<ol>
<li><strong>ç®€å•æœ‰æ•ˆ</strong>ï¼šåªéœ€ä¿®æ”¹ç¼©æ”¾å› å­ï¼Œæ— éœ€æ”¹å˜æ¨¡å‹ç»“æ„</li>
<li><strong>ç†è®ºæ”¯æ’‘</strong>ï¼šæœ‰åšå®çš„ä¿¡æ¯è®ºåŸºç¡€</li>
<li><strong>å®éªŒéªŒè¯</strong>ï¼šåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¯æ˜æœ‰æ•ˆ</li>
<li><strong>æ˜“äºå®ç°</strong>ï¼šå‡ è¡Œä»£ç å³å¯å®ç°</li>
</ol>
<p>ç†µä¸å˜æ€§ä¸ºè®¾è®¡æ›´å¥½çš„Attentionæœºåˆ¶æä¾›äº†æ–°çš„æ€è·¯å’Œå·¥å…·ã€‚</p>
        </div>
    </div>
</body>
</html>