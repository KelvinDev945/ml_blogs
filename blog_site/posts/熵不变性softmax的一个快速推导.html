<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç†µä¸å˜æ€§Softmaxçš„ä¸€ä¸ªå¿«é€Ÿæ¨å¯¼</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>ç†µä¸å˜æ€§Softmaxçš„ä¸€ä¸ªå¿«é€Ÿæ¨å¯¼</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2025-11-26 | ğŸ“„ å¤§å°: 28.1 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/9034">https://spaces.ac.cn/archives/9034</a></p>
<p><strong>å‘å¸ƒæ—¥æœŸ</strong>: </p>
<hr />
<p>åœ¨æ–‡ç« <a href="/archives/8823">ã€Šä»ç†µä¸å˜æ€§çœ‹Attentionçš„Scaleæ“ä½œã€‹</a>ä¸­ï¼Œæˆ‘ä»¬æ¨å¯¼äº†ä¸€ç‰ˆå…·æœ‰ç†µä¸å˜æ€§è´¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼š<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\kappa \log n}{d}QK^{\top}\right)V\label{eq:a}\end{equation}<br />
å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå®ƒä¸»è¦æ˜¯å¾€Softmaxé‡Œè¾¹å¼•å…¥äº†é•¿åº¦ç›¸å…³çš„ç¼©æ”¾å› å­$\log n$æ¥å®ç°çš„ã€‚åŸæ¥çš„æ¨å¯¼æ¯”è¾ƒç¹çï¼Œå¹¶ä¸”åšäº†è¾ƒå¤šçš„å‡è®¾ï¼Œä¸åˆ©äºç›´è§‚ç†è§£ï¼Œæœ¬æ–‡ä¸ºå…¶è¡¥å……ä¸€ä¸ªç›¸å¯¹ç®€æ˜å¿«é€Ÿçš„æ¨å¯¼ã€‚</p>
<h2 id="_1">æ¨å¯¼è¿‡ç¨‹</h2>
<p>æˆ‘ä»¬å¯ä»¥æŠ›å¼€æ³¨æ„åŠ›æœºåˆ¶çš„èƒŒæ™¯ï¼Œç›´æ¥è®¾æœ‰$s_1,s_2,\cdots,s_n\in\mathbb{R}$ï¼Œå®šä¹‰<br />
$$p_i = \frac{e^{\lambda s_i}}{\sum\limits_{i=1}^n e^{\lambda s_i}}$$<br />
æ˜¾ç„¶è¿™å°±æ˜¯$s_1,s_2,\cdots,s_n$åŒæ—¶ä¹˜ä¸Šç¼©æ”¾å› å­$\lambda$ååšSoftmaxçš„ç»“æœã€‚ç°åœ¨æˆ‘ä»¬ç®—å®ƒçš„ç†µ<br />
\begin{equation}\begin{aligned}H =&amp;\, -\sum_{i=1}^n p_i \log p_i = \log\sum_{i=1}^n e^{\lambda s_i} - \lambda\sum_{i=1}^n p_i s_i \\\<br />
=&amp;\, \log n + \log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} - \lambda\sum_{i=1}^n p_i s_i<br />
\end{aligned}\end{equation}<br />
ç¬¬ä¸€é¡¹çš„$\log$é‡Œè¾¹æ˜¯â€œå…ˆæŒ‡æ•°åå¹³å‡â€ï¼Œæˆ‘ä»¬ç”¨â€œå…ˆå¹³å‡åæŒ‡æ•°â€ï¼ˆå¹³å‡åœºï¼‰æ¥è¿‘ä¼¼å®ƒï¼š<br />
\begin{equation}<br />
\log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i}\approx \log\exp\left(\frac{1}{n}\sum_{i=1}^n \lambda s_i\right) = \lambda \bar{s}<br />
\end{equation}<br />
ç„¶åæˆ‘ä»¬çŸ¥é“Softmaxæ˜¯ä¼šä¾§é‡äº$\max$çš„é‚£ä¸ªï¼ˆå‚è€ƒ<a href="/archives/6620#softmax">ã€Šå‡½æ•°å…‰æ»‘åŒ–æ‚è°ˆï¼šä¸å¯å¯¼å‡½æ•°çš„å¯å¯¼é€¼è¿‘ã€‹</a>ï¼‰ï¼Œæ‰€ä»¥æœ‰è¿‘ä¼¼<br />
\begin{equation}\lambda\sum_{i=1}^n p_i s_i \approx \lambda s_{\max}\end{equation}<br />
æ‰€ä»¥<br />
\begin{equation}H\approx \log n - \lambda(s_{\max} - \bar{s})\end{equation}<br />
æ‰€è°“ç†µä¸å˜æ€§ï¼Œå°±æ˜¯å¸Œæœ›å°½å¯èƒ½åœ°æ¶ˆé™¤é•¿åº¦$n$çš„å½±å“ï¼Œæ‰€ä»¥æ ¹æ®ä¸Šå¼æˆ‘ä»¬éœ€è¦æœ‰$\lambda\propto \log n$ã€‚å¦‚æœæ”¾åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œé‚£ä¹ˆ$s$çš„å½¢å¼ä¸º$\langle \boldsymbol{q}, \boldsymbol{k}\rangle\propto d$ï¼ˆ$d$æ˜¯å‘é‡ç»´åº¦ï¼‰ï¼Œæ‰€ä»¥éœ€è¦æœ‰$\lambda\propto \frac{1}{d}$ï¼Œç»¼åˆèµ·æ¥å°±æ˜¯<br />
\begin{equation}\lambda\propto \frac{\log n}{d}\end{equation}<br />
è¿™å°±æ˜¯æ–‡ç« å¼€å¤´å¼$\eqref{eq:a}$çš„ç»“æœã€‚</p>
<h2 id="_2">æ–‡ç« å°ç»“</h2>
<p>ä¸ºä¹‹å‰æå‡ºçš„â€œç†µä¸å˜æ€§Softmaxâ€æ„æ€äº†ä¸€ä¸ªç®€å•æ˜å¿«çš„æ¨å¯¼ã€‚</p>
<p><em><strong>è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š</strong><a href="https://spaces.ac.cn/archives/9034">https://spaces.ac.cn/archives/9034</a></em></p>
<p><em><strong>æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="ã€Šç§‘å­¦ç©ºé—´FAQã€‹">ã€Šç§‘å­¦ç©ºé—´FAQã€‹</a></p>
<p><strong>å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚</strong></p>
<p><strong>å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼</strong></p>
<p>æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>å¾®ä¿¡æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>æ”¯ä»˜å®æ‰“èµ</p>
<p>å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>ç‚¹å‡»è¿™é‡Œ</strong></a>æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚</p>
<p><strong>å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š</strong></p>
<p>è‹å‰‘æ—. (Apr. 11, 2022). ã€Šç†µä¸å˜æ€§Softmaxçš„ä¸€ä¸ªå¿«é€Ÿæ¨å¯¼ ã€‹[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9034">https://spaces.ac.cn/archives/9034</a></p>
<p>@online{kexuefm-9034,<br />
title={ç†µä¸å˜æ€§Softmaxçš„ä¸€ä¸ªå¿«é€Ÿæ¨å¯¼},<br />
author={è‹å‰‘æ—},<br />
year={2022},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/9034}},<br />
} </p>
<hr />
<h2 id="_3">è¯¦ç»†æ•°å­¦æ¨å¯¼ä¸ç†è®ºåˆ†æ</h2>
<h3 id="1">1. é—®é¢˜çš„èƒŒæ™¯ä¸åŠ¨æœº</h3>
<p><strong>æ ‡å‡†Scaled Dot-Product Attention</strong>ï¼š</p>
<p>åœ¨Transformerä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶å®šä¹‰ä¸ºï¼š
\begin{equation}
\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d}}\right) \boldsymbol{V} \tag{1}
\end{equation}</p>
<p>å…¶ä¸­$\sqrt{d}$æ˜¯ç¼©æ”¾å› å­ï¼Œ$d$æ˜¯å‘é‡ç»´åº¦ã€‚</p>
<p><strong>é—®é¢˜</strong>ï¼šä¸ºä»€ä¹ˆæ˜¯$\sqrt{d}$è€Œä¸æ˜¯å…¶ä»–å€¼ï¼Ÿ</p>
<p>æ ‡å‡†è§£é‡Šï¼šä¿æŒæ–¹å·®ä¸å˜ï¼ˆè§Vaswani et al. 2017ï¼‰ã€‚</p>
<p><strong>æœ¬æ–‡çš„è§†è§’</strong>ï¼šä»<strong>ç†µä¸å˜æ€§</strong>çš„è§’åº¦ç†è§£ç¼©æ”¾å› å­ï¼Œå¹¶æ¨å¯¼å‡ºä¸åºåˆ—é•¿åº¦$n$ç›¸å…³çš„ç¼©æ”¾ã€‚</p>
<h3 id="2">2. ç†µä¸å˜æ€§çš„å®šä¹‰</h3>
<p><strong>ç›´è§‰</strong>ï¼šæˆ‘ä»¬å¸Œæœ›æ³¨æ„åŠ›åˆ†å¸ƒçš„ç†µä¸éšåºåˆ—é•¿åº¦$n$çš„å˜åŒ–è€Œå‰§çƒˆå˜åŒ–ã€‚</p>
<p><strong>å®šä¹‰</strong>ï¼šå¯¹äºæ³¨æ„åŠ›æƒé‡$\boldsymbol{\alpha} = [\alpha_1, \ldots, \alpha_n]$ï¼Œå…¶ç†µä¸ºï¼š
\begin{equation}
H(\boldsymbol{\alpha}) = -\sum_{i=1}^n \alpha_i \log \alpha_i \tag{2}
\end{equation}</p>
<p><strong>ç†µä¸å˜æ€§æ¡ä»¶</strong>ï¼šå¸Œæœ›å­˜åœ¨ç¼©æ”¾å› å­$\lambda(n, d)$ï¼Œä½¿å¾—ï¼š
\begin{equation}
H\left(\text{softmax}(\lambda \boldsymbol{s})\right) \approx C \tag{3}
\end{equation}</p>
<p>å…¶ä¸­$C$æ˜¯ä¸$n$æ— å…³çš„å¸¸æ•°ï¼Œ$\boldsymbol{s} = [s_1, \ldots, s_n]$æ˜¯æ³¨æ„åŠ›åˆ†æ•°ã€‚</p>
<h3 id="3">3. ç†µçš„å±•å¼€å¼</h3>
<p><strong>Softmaxå®šä¹‰</strong>ï¼š
\begin{equation}
\alpha_i = \frac{e^{\lambda s_i}}{\sum_{j=1}^n e^{\lambda s_j}} \tag{4}
\end{equation}</p>
<p><strong>ç†µçš„è®¡ç®—</strong>ï¼š
\begin{equation}
\begin{aligned}
H &amp;= -\sum_{i=1}^n \alpha_i \log \alpha_i \
&amp;= -\sum_{i=1}^n \alpha_i \left(\lambda s_i - \log\sum_{j=1}^n e^{\lambda s_j}\right) \
&amp;= -\lambda \sum_{i=1}^n \alpha_i s_i + \sum_{i=1}^n \alpha_i \log\sum_{j=1}^n e^{\lambda s_j} \
&amp;= \log\sum_{j=1}^n e^{\lambda s_j} - \lambda \sum_{i=1}^n \alpha_i s_i
\end{aligned} \tag{5}
\end{equation}</p>
<p>è®°<strong>é…åˆ†å‡½æ•°</strong>ï¼ˆpartition functionï¼‰ï¼š
\begin{equation}
Z = \sum_{j=1}^n e^{\lambda s_j} \tag{6}
\end{equation}</p>
<p>åˆ™ï¼š
\begin{equation}
H = \log Z - \lambda \mathbb{E}_\alpha[s] \tag{7}
\end{equation}</p>
<p>å…¶ä¸­$\mathbb{E}_\alpha[s] = \sum_i \alpha_i s_i$æ˜¯åŠ æƒå¹³å‡åˆ†æ•°ã€‚</p>
<h3 id="4">4. å¹³å‡åœºè¿‘ä¼¼</h3>
<p><strong>å…³é”®æ­¥éª¤</strong>ï¼šå¯¹é…åˆ†å‡½æ•°è¿›è¡Œè¿‘ä¼¼ã€‚</p>
<p><strong>Jensenä¸ç­‰å¼çš„åå‘åº”ç”¨</strong>ï¼š</p>
<p>å¯¹äºå‡¸å‡½æ•°$\exp$ï¼Œæœ‰ï¼š
\begin{equation}
\exp\left(\frac{1}{n}\sum_{i=1}^n \lambda s_i\right) \leq \frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} \tag{8}
\end{equation}</p>
<p>ä½†æˆ‘ä»¬éœ€è¦çš„æ˜¯å¯¹$\log Z$çš„è¿‘ä¼¼ï¼Œè€Œä¸æ˜¯$Z$æœ¬èº«ã€‚</p>
<p><strong>å¹³å‡åœºè¿‘ä¼¼</strong>ï¼ˆ"å…ˆå¹³å‡åæŒ‡æ•°"è¿‘ä¼¼"å…ˆæŒ‡æ•°åå¹³å‡"ï¼‰ï¼š</p>
<p>\begin{equation}
\log Z = \log\sum_{i=1}^n e^{\lambda s_i} = \log\left(n \cdot \frac{1}{n}\sum_{i=1}^n e^{\lambda s_i}\right) = \log n + \log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} \tag{9}
\end{equation}</p>
<p><strong>å…³é”®è¿‘ä¼¼</strong>ï¼š
\begin{equation}
\log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} \approx \log\exp\left(\frac{1}{n}\sum_{i=1}^n \lambda s_i\right) = \lambda \bar{s} \tag{10}
\end{equation}</p>
<p>å…¶ä¸­$\bar{s} = \frac{1}{n}\sum_i s_i$æ˜¯ç®—æœ¯å¹³å‡ã€‚</p>
<p><strong>ä»£å…¥å¼(9)</strong>ï¼š
\begin{equation}
\log Z \approx \log n + \lambda \bar{s} \tag{11}
\end{equation}</p>
<h3 id="5-softmaxmax">5. Softmaxä¾§é‡maxçš„æ€§è´¨</h3>
<p><strong>Softmaxä½œä¸ºmaxçš„å¹³æ»‘è¿‘ä¼¼</strong>ï¼š</p>
<p>å›é¡¾ä¸ç­‰å¼ï¼š
\begin{equation}
\max_i s_i \leq \log\sum_i e^{s_i} \leq \max_i s_i + \log n \tag{12}
\end{equation}</p>
<p><strong>åŠ æƒå¹³å‡çš„è¿‘ä¼¼</strong>ï¼š</p>
<p>Softmaxä¼šä¾§é‡äºè¾ƒå¤§çš„$s_i$ï¼Œå› æ­¤ï¼š
\begin{equation}
\mathbb{E}<em _max="\max">\alpha[s] = \sum_i \alpha_i s_i \approx s</em>
\end{equation}} \tag{13</p>
<p>å…¶ä¸­$s_{\max} = \max_i s_i$ã€‚</p>
<p><strong>ç†ç”±</strong>ï¼šå½“$\lambda$é€‚ä¸­æ—¶ï¼Œæœ€å¤§çš„$s_i$å¯¹åº”çš„$\alpha_i$å ä¸»å¯¼ã€‚</p>
<h3 id="6">6. ç†µçš„æœ€ç»ˆè¿‘ä¼¼</h3>
<p>ç»“åˆå¼(7)ã€(11)å’Œ(13)ï¼š
\begin{equation}
H \approx \log n + \lambda \bar{s} - \lambda s_{\max} = \log n - \lambda (s_{\max} - \bar{s}) \tag{14}
\end{equation}</p>
<p><strong>å®šä¹‰æ³¢åŠ¨å¹…åº¦</strong>ï¼š
\begin{equation}
\Delta s = s_{\max} - \bar{s} \tag{15}
\end{equation}</p>
<p>åˆ™ï¼š
\begin{equation}
H \approx \log n - \lambda \Delta s \tag{16}
\end{equation}</p>
<h3 id="7">7. ç†µä¸å˜æ€§æ¡ä»¶</h3>
<p><strong>ç›®æ ‡</strong>ï¼šä½¿$H$ä¸ä¾èµ–äº$n$ã€‚</p>
<p>ä»å¼(16)ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›$H \approx C$ï¼ˆå¸¸æ•°ï¼‰ï¼Œåˆ™éœ€è¦ï¼š
\begin{equation}
\log n - \lambda \Delta s \approx C \tag{17}
\end{equation}</p>
<p>ç§»é¡¹ï¼š
\begin{equation}
\lambda \Delta s \approx \log n - C \tag{18}
\end{equation}</p>
<p><strong>å‡è®¾$\Delta s$ä¸$n$æ— å…³</strong>ï¼ˆåˆç†å‡è®¾ï¼šåˆ†æ•°çš„æ³¢åŠ¨ä¸»è¦ç”±æ•°æ®å’Œæ¨¡å‹å†³å®šï¼‰ï¼Œåˆ™ï¼š
\begin{equation}
\lambda \propto \log n \tag{19}
\end{equation}</p>
<h3 id="8-d">8. ä¸ç»´åº¦$d$çš„å…³ç³»</h3>
<p><strong>åˆ†æ•°çš„å½¢å¼</strong>ï¼š</p>
<p>åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œ$s_i = \langle \boldsymbol{q}, \boldsymbol{k}_i \rangle$ï¼Œå…¶ä¸­$\boldsymbol{q}, \boldsymbol{k}_i \in \mathbb{R}^d$ã€‚</p>
<p><strong>æ–¹å·®åˆ†æ</strong>ï¼š</p>
<p>å‡è®¾$\boldsymbol{q}$å’Œ$\boldsymbol{k}_i$çš„å…ƒç´ ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œ$\mathbb{E}[q_j] = 0$ï¼Œ$\text{Var}(q_j) = \sigma^2$ã€‚</p>
<p>åˆ™ï¼š
\begin{equation}
s_i = \sum_{j=1}^d q_j k_{i,j} \tag{20}
\end{equation}</p>
<p>æœŸæœ›ï¼š$\mathbb{E}[s_i] = 0$</p>
<p>æ–¹å·®ï¼š
\begin{equation}
\text{Var}(s_i) = d \cdot \mathbb{E}[q_j^2] \mathbb{E}[k_{i,j}^2] = d\sigma^4 \tag{21}
\end{equation}</p>
<p><strong>æ ‡å‡†åŒ–</strong>ï¼š</p>
<p>ä¸ºäº†ä½¿æ–¹å·®ä¸éš$d$å˜åŒ–ï¼Œæˆ‘ä»¬é™¤ä»¥$\sqrt{d}$ï¼š
\begin{equation}
s_i' = \frac{s_i}{\sqrt{d}} \Rightarrow \text{Var}(s_i') = \sigma^4 \tag{22}
\end{equation}</p>
<p><strong>ç»“åˆä¸¤è€…</strong>ï¼š</p>
<p>æ€»çš„ç¼©æ”¾å› å­åº”è¯¥æ˜¯ï¼š
\begin{equation}
\lambda \propto \frac{\log n}{d} \tag{23}
\end{equation}</p>
<p>æ›´ç²¾ç¡®åœ°ï¼š
\begin{equation}
\lambda = \frac{\kappa \log n}{d} \tag{24}
\end{equation}</p>
<p>å…¶ä¸­$\kappa$æ˜¯ä¸€ä¸ªä¸$n$å’Œ$d$æ— å…³çš„å¸¸æ•°ï¼ˆé€šå¸¸å–$\kappa = 1$ï¼‰ã€‚</p>
<h3 id="9">9. æœ€ç»ˆçš„ç†µä¸å˜æ€§æ³¨æ„åŠ›</h3>
<p><strong>æå‡ºçš„å…¬å¼</strong>ï¼š
\begin{equation}
\boxed{\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\kappa \log n}{d} \boldsymbol{Q}\boldsymbol{K}^\top\right) \boldsymbol{V}} \tag{25}
\end{equation}</p>
<p><strong>ä¸æ ‡å‡†æ³¨æ„åŠ›çš„å¯¹æ¯”</strong>ï¼š</p>
<p>æ ‡å‡†æ³¨æ„åŠ›ä½¿ç”¨$\frac{1}{\sqrt{d}}$ï¼Œåªè€ƒè™‘äº†ç»´åº¦$d$ã€‚</p>
<p>ç†µä¸å˜æ€§æ³¨æ„åŠ›ä½¿ç”¨$\frac{\kappa \log n}{d}$ï¼ŒåŒæ—¶è€ƒè™‘äº†ç»´åº¦$d$å’Œåºåˆ—é•¿åº¦$n$ã€‚</p>
<h3 id="10-laplace">10. ä¸¥æ ¼çš„æ•°å­¦æ¨å¯¼ï¼ˆLaplaceè¿‘ä¼¼ï¼‰</h3>
<p>ä¸Šè¿°æ¨å¯¼ä½¿ç”¨äº†è¾ƒç²—ç³™çš„è¿‘ä¼¼ã€‚ç°åœ¨æˆ‘ä»¬ç”¨Laplaceè¿‘ä¼¼è¿›è¡Œæ›´ä¸¥æ ¼çš„åˆ†æã€‚</p>
<p><strong>Laplaceè¿‘ä¼¼çš„æ€æƒ³</strong>ï¼š</p>
<p>å¯¹äºç§¯åˆ†æˆ–æ±‚å’Œï¼š
\begin{equation}
I = \int e^{nf(x)} dx \approx e^{nf(x^<em>)} \sqrt{\frac{2\pi}{n|f''(x^</em>)|}} \tag{26}
\end{equation}</p>
<p>å…¶ä¸­$x^* = \arg\max_x f(x)$ã€‚</p>
<p><strong>åº”ç”¨åˆ°é…åˆ†å‡½æ•°</strong>ï¼š</p>
<p>\begin{equation}
Z = \sum_{i=1}^n e^{\lambda s_i} \tag{27}
\end{equation}</p>
<p>æ”¹å†™ä¸ºè¿ç»­å½¢å¼ï¼ˆå‡è®¾$s_i$æ˜¯ä»æŸä¸ªåˆ†å¸ƒé‡‡æ ·çš„ï¼‰ï¼š
\begin{equation}
Z \approx n \int e^{\lambda s} p(s) ds \tag{28}
\end{equation}</p>
<p>å…¶ä¸­$p(s)$æ˜¯$s$çš„åˆ†å¸ƒã€‚</p>
<p><strong>å¯¹æ•°é…åˆ†å‡½æ•°</strong>ï¼š</p>
<p>\begin{equation}
\log Z = \log n + \log\int e^{\lambda s} p(s) ds \tag{29}
\end{equation}</p>
<p><strong>Laplaceè¿‘ä¼¼</strong>ï¼ˆå½“$\lambda$é€‚ä¸­æ—¶ï¼‰ï¼š</p>
<p>\begin{equation}
\log\int e^{\lambda s} p(s) ds \approx \lambda s^* - \frac{1}{2}\log(2\pi) + \frac{1}{2}\log\lambda \tag{30}
\end{equation}</p>
<p>å…¶ä¸­$s^*$æ˜¯ä½¿$\lambda s + \log p(s)$æœ€å¤§çš„ç‚¹ã€‚</p>
<p>å¯¹äºå‡åŒ€åˆ†å¸ƒ$p(s)$ï¼Œ$s^* = s_{\max}$ã€‚</p>
<p><strong>ç®€åŒ–</strong>ï¼ˆå¿½ç•¥å¸¸æ•°é¡¹ï¼‰ï¼š
\begin{equation}
\log Z \approx \log n + \lambda s_{\max} \tag{31}
\end{equation}</p>
<p><strong>ç†µçš„è®¡ç®—</strong>ï¼ˆä½¿ç”¨$\mathbb{E}<em _max="\max">\alpha[s] \approx s</em>$ï¼‰ï¼š
\begin{equation}
H = \log Z - \lambda \mathbb{E}<em _max="\max">\alpha[s] \approx \log n + \lambda s</em>
\end{equation}} - \lambda s_{\max} = \log n \tag{32</p>
<p>è¿™ç»™å‡ºäº†$H \approx \log n$ï¼Œæ°å¥½æ˜¯å‡åŒ€åˆ†å¸ƒçš„ç†µï¼</p>
<p><strong>ä¿®æ­£</strong>ï¼šå®é™…ä¸Š$\mathbb{E}<em _max="\max">\alpha[s] \neq s</em>$ï¼Œæ‰€ä»¥éœ€è¦æ›´ç²¾ç»†çš„åˆ†æã€‚</p>
<h3 id="11">11. äºŒé˜¶ä¿®æ­£ä¸æ³¢åŠ¨</h3>
<p><strong>æ›´ç²¾ç¡®çš„$\mathbb{E}_\alpha[s]$</strong>ï¼š</p>
<p>è€ƒè™‘softmaxçš„äºŒé˜¶å±•å¼€ï¼š
\begin{equation}
\alpha_i = \frac{e^{\lambda s_i}}{\sum_j e^{\lambda s_j}} \approx \frac{e^{\lambda (s_i - s_{\max})}}{\sum_j e^{\lambda (s_j - s_{\max})}} \tag{33}
\end{equation}</p>
<p>è®°$\Delta_i = s_i - s_{\max} \leq 0$ï¼Œåˆ™ï¼š
\begin{equation}
\alpha_i \approx \frac{e^{\lambda \Delta_i}}{1 + \sum_{j \neq i^*} e^{\lambda \Delta_j}} \tag{34}
\end{equation}</p>
<p>å…¶ä¸­$i^* = \arg\max_i s_i$ã€‚</p>
<p><strong>å½“$\lambda$é€‚ä¸­æ—¶</strong>ï¼š</p>
<p>\begin{equation}
\alpha_{i^<em>} \approx \frac{1}{1 + \sum_{j \neq i^</em>} e^{\lambda \Delta_j}} \tag{35}
\end{equation}</p>
<p><strong>å¹³å‡åˆ†æ•°</strong>ï¼š
\begin{equation}
\mathbb{E}<em>\alpha[s] \approx \alpha</em>{i^<em>} s_{\max} + \sum_{j \neq i^</em>} \alpha_j s_j \tag{36}
\end{equation}</p>
<p><strong>ç®€åŒ–</strong>ï¼ˆå‡è®¾å…¶ä»–$\alpha_j$è¾ƒå°ï¼‰ï¼š
\begin{equation}
\mathbb{E}<em _max="\max">\alpha[s] \approx s</em>
\end{equation}} - \Delta s \tag{37</p>
<p>å…¶ä¸­$\Delta s = s_{\max} - \bar{s}$çš„å…·ä½“å€¼ä¾èµ–äºåˆ†å¸ƒã€‚</p>
<h3 id="12">12. ç¼©æ”¾å› å­çš„æ¸è¿‘åˆ†æ</h3>
<p><strong>å¤§$n$æé™</strong>ï¼š</p>
<p>å‡è®¾$s_i \sim \mathcal{N}(\mu, \sigma^2)$ç‹¬ç«‹åŒåˆ†å¸ƒã€‚</p>
<p><strong>æå€¼åˆ†å¸ƒ</strong>ï¼š</p>
<p>å½“$n \to \infty$æ—¶ï¼Œ$s_{\max} = \max_i s_i$çš„åˆ†å¸ƒè¶‹å‘äºGumbelåˆ†å¸ƒã€‚</p>
<p>å…·ä½“åœ°ï¼š
\begin{equation}
s_{\max} \approx \mu + \sigma\sqrt{2\log n} \tag{38}
\end{equation}</p>
<p><strong>å¹³å‡å€¼</strong>ï¼š
\begin{equation}
\bar{s} = \mathbb{E}[s] = \mu \tag{39}
\end{equation}</p>
<p><strong>æ³¢åŠ¨</strong>ï¼š
\begin{equation}
\Delta s = s_{\max} - \bar{s} \approx \sigma\sqrt{2\log n} \tag{40}
\end{equation}</p>
<p><strong>ä»£å…¥å¼(16)</strong>ï¼š
\begin{equation}
H \approx \log n - \lambda \sigma\sqrt{2\log n} \tag{41}
\end{equation}</p>
<p><strong>ç†µä¸å˜æ€§æ¡ä»¶</strong>ï¼š</p>
<p>å¸Œæœ›$H$ä¸ä¾èµ–äº$n$ï¼Œä½†ä»å¼(41)çœ‹ï¼Œè¿™ä¼¼ä¹ä¸å¯èƒ½ï¼</p>
<p><strong>åˆ†è¾¨ç‡</strong>ï¼šå®é™…ä¸Šï¼Œå¼(41)ä¸­çš„$\log n$é¡¹å’Œ$\lambda\sqrt{2\log n}$é¡¹éƒ½ä¸$n$æœ‰å…³ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬ç›¸äº’æŠµæ¶ˆã€‚</p>
<p>ä½†$\log n$å’Œ$\sqrt{\log n}$çš„å¢é•¿é€Ÿåº¦ä¸åŒï¼Œæ— æ³•å®Œå…¨æŠµæ¶ˆã€‚</p>
<p><strong>ä¿®æ­£ç†è§£</strong>ï¼š</p>
<p>"ç†µä¸å˜æ€§"ä¸æ˜¯æŒ‡$H$å®Œå…¨ä¸å˜ï¼Œè€Œæ˜¯æŒ‡$H$çš„ä¸»å¯¼é¡¹$\log n$è¢«åˆç†æ§åˆ¶ã€‚</p>
<p>å…·ä½“åœ°ï¼Œæˆ‘ä»¬å¸Œæœ›ï¼š
\begin{equation}
H \approx C \cdot \log n \tag{42}
\end{equation}</p>
<p>å…¶ä¸­$C$æ˜¯ä¸€ä¸ªæ¥è¿‘1çš„å¸¸æ•°ã€‚</p>
<h3 id="13-johnson-lindenstrauss">13. ä¸Johnson-Lindenstrausså¼•ç†çš„è”ç³»</h3>
<p><strong>Johnson-Lindenstrauss (JL) å¼•ç†</strong>ï¼š</p>
<p>å¯¹äº$n$ä¸ªç‚¹åœ¨é«˜ç»´ç©ºé—´$\mathbb{R}^d$ä¸­ï¼Œå¯ä»¥å°†å®ƒä»¬æŠ•å½±åˆ°$O(\log n / \epsilon^2)$ç»´ç©ºé—´ï¼Œä¿æŒè·ç¦»åœ¨$(1-\epsilon, 1+\epsilon)$èŒƒå›´å†…ã€‚</p>
<p><strong>ä¸æ³¨æ„åŠ›çš„ç±»æ¯”</strong>ï¼š</p>
<p>æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ç†è§£ä¸ºä¸€ç§<strong>è½¯æŠ•å½±</strong>ï¼šä»$n$ä¸ªå€¼å‘é‡${\boldsymbol{v}_i}$ä¸­æå–ä¿¡æ¯åˆ°å•ä¸ªè¾“å‡º$\boldsymbol{o}$ã€‚</p>
<p><strong>ç†µä¸ç»´åº¦</strong>ï¼š</p>
<p>JLå¼•ç†å‘Šè¯‰æˆ‘ä»¬ï¼Œä¿æŒ$n$ä¸ªç‚¹çš„ä¿¡æ¯éœ€è¦$O(\log n)$ç»´ã€‚</p>
<p>ç±»æ¯”åœ°ï¼Œæ³¨æ„åŠ›çš„ç†µï¼ˆåº¦é‡ä¿¡æ¯é‡ï¼‰ä¹Ÿåº”è¯¥æ˜¯$O(\log n)$ã€‚</p>
<p><strong>ç¼©æ”¾å› å­çš„è¿æ¥</strong>ï¼š</p>
<p>æˆ‘ä»¬çš„ç¼©æ”¾å› å­$\lambda \propto \log n / d$æ°å¥½ä½“ç°äº†è¿™ä¸ªå…³ç³»ï¼š
- å½“$d$å›ºå®šæ—¶ï¼Œ$\lambda \propto \log n$
- å½“$n$å›ºå®šæ—¶ï¼Œ$\lambda \propto 1/d$</p>
<h3 id="14">14. ä¿¡æ¯è®ºè§†è§’</h3>
<p><strong>ç†µçš„ä¿¡æ¯è®ºæ„ä¹‰</strong>ï¼š</p>
<p>ç†µ$H$åº¦é‡äº†æ³¨æ„åŠ›åˆ†å¸ƒçš„<strong>ä¸ç¡®å®šæ€§</strong>ï¼Œä¹Ÿæ˜¯ç¼–ç æ³¨æ„åŠ›åˆ†å¸ƒæ‰€éœ€çš„<strong>å¹³å‡æ¯”ç‰¹æ•°</strong>ã€‚</p>
<p><strong>ç†µä¸å˜æ€§çš„æ„ä¹‰</strong>ï¼š</p>
<p>å¦‚æœ$H \propto \log n$ï¼Œé‚£ä¹ˆï¼š
- ç¼–ç æ³¨æ„åŠ›åˆ†å¸ƒéœ€è¦$O(\log n)$æ¯”ç‰¹
- è¿™ä¸ç”¨$\lceil \log_2 n \rceil$æ¯”ç‰¹ç¼–ç $n$ä¸ªä½ç½®çš„ä»»æ„ä¸€ä¸ªä¸€è‡´</p>
<p><strong>é¦™å†œç†µç•Œ</strong>ï¼š</p>
<p>å¯¹äº$n$ä¸ªç­‰æ¦‚ç‡çš„é€‰æ‹©ï¼Œæœ€å°ç¼–ç é•¿åº¦æ˜¯$\log_2 n$æ¯”ç‰¹ã€‚</p>
<p>æ³¨æ„åŠ›åˆ†å¸ƒé€šå¸¸ä¸æ˜¯å‡åŒ€çš„ï¼ˆ$H &lt; \log n$ï¼‰ï¼Œæ‰€ä»¥å®é™…ç¼–ç å¯ä»¥æ›´çŸ­ã€‚</p>
<h3 id="15">15. å®éªŒéªŒè¯</h3>
<p><strong>è®¾ç½®</strong>ï¼š</p>
<p>ç”Ÿæˆéšæœºçš„æ³¨æ„åŠ›åˆ†æ•°$s_i \sim \mathcal{N}(0, 1)$ï¼Œå˜åŒ–$n$å’Œç¼©æ”¾å› å­$\lambda$ã€‚</p>
<p><strong>è§‚å¯Ÿ</strong>ï¼š</p>
<ol>
<li><strong>æ ‡å‡†ç¼©æ”¾</strong>ï¼ˆ$\lambda = 1/\sqrt{d}$ï¼‰ï¼š</li>
<li>$H$éš$n$å¢åŠ è€Œå¢åŠ </li>
<li>
<p>å¤§çº¦$H \approx \log n - C$</p>
</li>
<li>
<p><strong>ç†µä¸å˜æ€§ç¼©æ”¾</strong>ï¼ˆ$\lambda = \log n / d$ï¼‰ï¼š</p>
</li>
<li>$H$åœ¨ä¸åŒ$n$ä¸‹æ›´åŠ ç¨³å®š</li>
<li>æ¥è¿‘$H \approx \log n$ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰</li>
</ol>
<p><strong>é—®é¢˜</strong>ï¼šå¦‚æœ$H \approx \log n$ï¼Œé‚£ä¸å°±æ˜¯å‡åŒ€åˆ†å¸ƒå—ï¼Ÿè¿™æ ·æœ‰ç”¨å—ï¼Ÿ</p>
<p><strong>å›ç­”</strong>ï¼š
- $H \approx \log n$æ˜¯ç†è®ºä¸Šç•Œï¼ˆæœ€å¤§ç†µï¼‰
- å®é™…ä¸­ï¼Œç”±äºæ•°æ®çš„ç»“æ„ï¼Œ$H &lt; \log n$
- å…³é”®æ˜¯$H$çš„<strong>èŒƒå›´</strong>è¶³å¤Ÿå¤§ï¼Œå¯å­¦ä¹ çš„ä¿¡æ¯é‡å……è¶³</p>
<h3 id="16-layernormrmsnorm">16. ä¸LayerNormã€RMSNormçš„è”ç³»</h3>
<p><strong>LayerNorm</strong>ï¼š</p>
<p>åœ¨Transformerä¸­ï¼Œé€šå¸¸åœ¨attentionä¹‹å‰åº”ç”¨LayerNormï¼š
\begin{equation}
\boldsymbol{q} = \text{LayerNorm}(\boldsymbol{x}) \boldsymbol{W}_Q \tag{43}
\end{equation}</p>
<p>LayerNormå½’ä¸€åŒ–äº†å‡å€¼å’Œæ–¹å·®ï¼š
\begin{equation}
\text{LayerNorm}(\boldsymbol{x}) = \frac{\boldsymbol{x} - \mu}{\sigma} \cdot \gamma + \beta \tag{44}
\end{equation}</p>
<p><strong>æ•ˆæœ</strong>ï¼š</p>
<p>LayerNormä½¿å¾—$s_i = \langle \boldsymbol{q}, \boldsymbol{k}_i \rangle$çš„åˆ†å¸ƒæ›´åŠ ç¨³å®šï¼Œå‡å°‘äº†å¯¹åˆå§‹åŒ–çš„æ•æ„Ÿæ€§ã€‚</p>
<p><strong>ä¸ç†µä¸å˜æ€§çš„å…³ç³»</strong>ï¼š</p>
<p>LayerNorméƒ¨åˆ†å®ç°äº†ç†µä¸å˜æ€§çš„ç›®æ ‡ï¼Œä½†æ²¡æœ‰æ˜¾å¼åœ°è€ƒè™‘$n$çš„å½±å“ã€‚</p>
<p><strong>RMSNorm</strong>ï¼š</p>
<p>RMSNormåªå½’ä¸€åŒ–å‡æ–¹æ ¹ï¼Œä¸ä¸­å¿ƒåŒ–ï¼š
\begin{equation}
\text{RMSNorm}(\boldsymbol{x}) = \frac{\boldsymbol{x}}{\text{RMS}(\boldsymbol{x})} \cdot \gamma \tag{45}
\end{equation}</p>
<p>å…¶ä¸­$\text{RMS}(\boldsymbol{x}) = \sqrt{\frac{1}{d}\sum_i x_i^2}$ã€‚</p>
<p><strong>ä¼˜åŠ¿</strong>ï¼š</p>
<p>RMSNormæ›´ç®€å•ï¼Œè®¡ç®—æ›´å¿«ï¼ŒåŒæ—¶ä¿æŒäº†ç±»ä¼¼çš„æ•ˆæœã€‚</p>
<h3 id="17">17. åŠ¨æ€ç¼©æ”¾å› å­</h3>
<p><strong>é—®é¢˜</strong>ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œåºåˆ—é•¿åº¦$n$å¯èƒ½æ˜¯å˜åŒ–çš„ï¼ˆå¦‚ä¸åŒé•¿åº¦çš„å¥å­ï¼‰ã€‚</p>
<p><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼šä½¿ç”¨<strong>åŠ¨æ€ç¼©æ”¾å› å­</strong>ï¼Œæ ¹æ®å½“å‰çš„$n$è°ƒæ•´$\lambda$ã€‚</p>
<p><strong>å®ç°</strong>ï¼š
\begin{equation}
\lambda(n) = \frac{\kappa \log n}{d} \tag{46}
\end{equation}</p>
<p>åœ¨æ¯ä¸ªattentionå±‚ä¸­ï¼Œæ ¹æ®å½“å‰çš„$n$è®¡ç®—$\lambda$ã€‚</p>
<p><strong>æŒ‘æˆ˜</strong>ï¼š</p>
<ul>
<li>ä¸åŒé•¿åº¦çš„åºåˆ—ä½¿ç”¨ä¸åŒçš„$\lambda$ï¼Œå¯èƒ½å½±å“æ¨¡å‹çš„ä¸€è‡´æ€§</li>
<li>éœ€è¦é¢å¤–çš„è®¡ç®—å¼€é”€ï¼ˆè™½ç„¶å¾ˆå°ï¼‰</li>
</ul>
<p><strong>æŠ˜è¡·æ–¹æ¡ˆ</strong>ï¼š</p>
<p>ä½¿ç”¨$\lambda = \frac{\kappa \log n_{\max}}{d}$ï¼Œå…¶ä¸­$n_{\max}$æ˜¯è®­ç»ƒä¸­è§è¿‡çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚</p>
<h3 id="18">18. ä½ç½®ç¼–ç çš„å½±å“</h3>
<p><strong>ç»å¯¹ä½ç½®ç¼–ç </strong>ï¼š</p>
<p>æ·»åŠ ä½ç½®ç¼–ç $\boldsymbol{p}<em ij="ij">i$åï¼š
\begin{equation}
s</em>
\end{equation}} = \langle \boldsymbol{q}_i + \boldsymbol{p}_i, \boldsymbol{k}_j + \boldsymbol{p}_j \rangle \tag{47</p>
<p><strong>å¯¹ç†µçš„å½±å“</strong>ï¼š</p>
<p>ä½ç½®ç¼–ç å¼•å…¥äº†é¢å¤–çš„ç»“æ„ï¼ˆå¦‚å±€éƒ¨æ€§åå¥½ï¼‰ï¼Œä¼šå½±å“$s_{ij}$çš„åˆ†å¸ƒï¼Œä»è€Œå½±å“ç†µã€‚</p>
<p><strong>åˆ†æ</strong>ï¼š</p>
<p>å¦‚æœä½ç½®ç¼–ç æ˜¯Sinusoidalçš„ï¼š
\begin{equation}
p_{i,2k} = \sin(i / 10000^{2k/d}), \quad p_{i,2k+1} = \cos(i / 10000^{2k/d}) \tag{48}
\end{equation}</p>
<p>ç›¸é‚»ä½ç½®çš„å†…ç§¯è¾ƒå¤§ï¼Œè¿œè·ç¦»ä½ç½®çš„å†…ç§¯è¾ƒå°ã€‚</p>
<p><strong>ç»“æœ</strong>ï¼š</p>
<p>æ³¨æ„åŠ›å€¾å‘äºå…³æ³¨é™„è¿‘çš„tokenï¼Œ$H$å¯èƒ½ä¸‹é™ã€‚</p>
<p><strong>ä¿®æ­£</strong>ï¼š</p>
<p>åœ¨ä½¿ç”¨ä½ç½®ç¼–ç æ—¶ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´$\kappa$ä»¥è¡¥å¿ç†µçš„å˜åŒ–ã€‚</p>
<h3 id="19">19. ç¨€ç–æ³¨æ„åŠ›çš„ç†µ</h3>
<p><strong>ç¨€ç–æ³¨æ„åŠ›</strong>ï¼š</p>
<p>åªè®¡ç®—éƒ¨åˆ†$(i, j)$å¯¹çš„attention scoreï¼š
\begin{equation}
s_{ij} = \begin{cases}
\langle \boldsymbol{q}_i, \boldsymbol{k}_j \rangle, &amp; (i,j) \in \mathcal{S} \
-\infty, &amp; \text{otherwise}
\end{cases} \tag{49}
\end{equation}</p>
<p>å…¶ä¸­$\mathcal{S}$æ˜¯å…è®¸çš„ä½ç½®å¯¹é›†åˆï¼ˆå¦‚å±€éƒ¨çª—å£ã€è·¨æ­¥æ¨¡å¼ç­‰ï¼‰ã€‚</p>
<p><strong>æœ‰æ•ˆåºåˆ—é•¿åº¦</strong>ï¼š</p>
<p>å¯¹äºä½ç½®$i$ï¼Œæœ‰æ•ˆçš„$j$æ•°é‡ä¸º$|\mathcal{S}_i|$ï¼Œé€šå¸¸$|\mathcal{S}_i| \ll n$ã€‚</p>
<p><strong>ç†µçš„è°ƒæ•´</strong>ï¼š</p>
<p>æœ€å¤§ç†µå˜ä¸ºï¼š
\begin{equation}
H_{\max} = \log |\mathcal{S}_i| \tag{50}
\end{equation}</p>
<p><strong>ç¼©æ”¾å› å­</strong>ï¼š</p>
<p>ç›¸åº”åœ°ï¼Œåº”è¯¥ä½¿ç”¨ï¼š
\begin{equation}
\lambda = \frac{\kappa \log |\mathcal{S}_i|}{d} \tag{51}
\end{equation}</p>
<h3 id="20">20. äº¤å‰æ³¨æ„åŠ›ä¸è‡ªæ³¨æ„åŠ›</h3>
<p><strong>è‡ªæ³¨æ„åŠ›</strong>ï¼š</p>
<p>$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}$éƒ½æ¥è‡ªåŒä¸€åºåˆ—ï¼Œé•¿åº¦ä¸º$n$ã€‚</p>
<p><strong>äº¤å‰æ³¨æ„åŠ›</strong>ï¼ˆå¦‚encoder-decoder attentionï¼‰ï¼š</p>
<p>$\boldsymbol{Q}$æ¥è‡ªdecoderï¼ˆé•¿åº¦$n_q$ï¼‰ï¼Œ$\boldsymbol{K}, \boldsymbol{V}$æ¥è‡ªencoderï¼ˆé•¿åº¦$n_k$ï¼‰ã€‚</p>
<p><strong>ç†µçš„åˆ†æ</strong>ï¼š</p>
<p>å¯¹äºdecoderçš„æ¯ä¸ªä½ç½®$i$ï¼Œç†µä¸ºï¼š
\begin{equation}
H_i = -\sum_{j=1}^{n_k} \alpha_{ij} \log \alpha_{ij} \tag{52}
\end{equation}</p>
<p><strong>ç¼©æ”¾å› å­</strong>ï¼š</p>
<p>åº”è¯¥ä½¿ç”¨$n_k$ï¼ˆkey/valueçš„é•¿åº¦ï¼‰ï¼š
\begin{equation}
\lambda = \frac{\kappa \log n_k}{d} \tag{53}
\end{equation}</p>
<h3 id="21">21. å¤šå¤´æ³¨æ„åŠ›çš„ç†µ</h3>
<p><strong>å¤šå¤´æ³¨æ„åŠ›</strong>ï¼š</p>
<p>\begin{equation}
\text{MultiHead}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \boldsymbol{W}^O \tag{54}
\end{equation}</p>
<p>å…¶ä¸­ï¼š
\begin{equation}
\text{head}_i = \text{Attention}(\boldsymbol{Q}\boldsymbol{W}_i^Q, \boldsymbol{K}\boldsymbol{W}_i^K, \boldsymbol{V}\boldsymbol{W}_i^V) \tag{55}
\end{equation}</p>
<p><strong>æ¯ä¸ªå¤´çš„ç»´åº¦</strong>ï¼š</p>
<p>é€šå¸¸$d_{\text{head}} = d / h$ã€‚</p>
<p><strong>ç¼©æ”¾å› å­</strong>ï¼š</p>
<p>å¯¹äºæ¯ä¸ªå¤´ï¼š
\begin{equation}
\lambda_i = \frac{\kappa \log n}{d_{\text{head}}} = \frac{\kappa h \log n}{d} \tag{56}
\end{equation}</p>
<p><strong>è§‚å¯Ÿ</strong>ï¼š</p>
<p>å¤šå¤´æ³¨æ„åŠ›ç›¸å½“äºä½¿ç”¨äº†æ›´å¤§çš„ç¼©æ”¾å› å­ï¼ˆä¹˜ä»¥$h$ï¼‰ã€‚</p>
<h3 id="22-flash-attention">22. Flash Attentionä¸ç†µä¸å˜æ€§</h3>
<p><strong>Flash Attention</strong>ï¼š</p>
<p>Flash Attentionæ˜¯ä¸€ç§é«˜æ•ˆçš„attentionè®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡åˆ†å—ï¼ˆtilingï¼‰å’Œé‡è®¡ç®—ï¼ˆrecomputationï¼‰å‡å°‘å†…å­˜è®¿é—®ã€‚</p>
<p><strong>ä¸ç†µä¸å˜æ€§çš„å…³ç³»</strong>ï¼š</p>
<p>Flash Attentionæœ¬èº«ä¸æ”¹å˜attentionçš„æ•°å­¦å®šä¹‰ï¼Œæ‰€ä»¥ç†µä¸å˜æ€§ç¼©æ”¾å¯ä»¥ç›´æ¥åº”ç”¨ã€‚</p>
<p><strong>å®ç°</strong>ï¼š</p>
<p>åœ¨Flash Attentionçš„kernelä¸­ï¼Œå°†ç¼©æ”¾å› å­ä»$1/\sqrt{d}$æ”¹ä¸º$\log n / d$å³å¯ã€‚</p>
<h3 id="23">23. è®­ç»ƒç¨³å®šæ€§</h3>
<p><strong>é—®é¢˜</strong>ï¼šä½¿ç”¨$\lambda = \log n / d$æ—¶ï¼Œå½“$n$å¾ˆå¤§æ—¶ï¼Œ$\lambda$å¯èƒ½å¾ˆå¤§ï¼Œå¯¼è‡´softmaxè¿‡äºpeakedã€‚</p>
<p><strong>åˆ†æ</strong>ï¼š</p>
<p>å‡è®¾$n = 1024$ï¼Œ$d = 64$ï¼Œ$\kappa = 1$ï¼š
\begin{equation}
\lambda = \frac{\log 1024}{64} = \frac{10 \log 2}{64} \approx 0.108 \tag{57}
\end{equation}</p>
<p>æ ‡å‡†ç¼©æ”¾ï¼š
\begin{equation}
\lambda_{\text{std}} = \frac{1}{\sqrt{64}} = 0.125 \tag{58}
\end{equation}</p>
<p>ä¸¤è€…æ¥è¿‘ï¼</p>
<p><strong>ä¸€èˆ¬è§„å¾‹</strong>ï¼š</p>
<p>å¯¹äºå¸¸è§çš„$n$å’Œ$d$å€¼ï¼š
\begin{equation}
\frac{\log n}{d} \approx \frac{1}{\sqrt{d}} \tag{59}
\end{equation}</p>
<p>ä¾‹å¦‚ï¼Œå½“$n \approx e^{\sqrt{d}}$æ—¶ï¼Œä¸¤è€…ç›¸ç­‰ã€‚</p>
<p><strong>å®è·µä¸­çš„é€‰æ‹©</strong>ï¼š</p>
<p>å¯ä»¥ä½¿ç”¨æ··åˆç­–ç•¥ï¼š
\begin{equation}
\lambda = \frac{1}{\sqrt{d}} + \beta \frac{\log n}{d} \tag{60}
\end{equation}</p>
<p>å…¶ä¸­$\beta$æ˜¯ä¸€ä¸ªå°çš„æƒé‡ï¼ˆå¦‚$\beta = 0.1$ï¼‰ã€‚</p>
<h3 id="24">24. é•¿åºåˆ—å»ºæ¨¡</h3>
<p><strong>é—®é¢˜</strong>ï¼šå¯¹äºéå¸¸é•¿çš„åºåˆ—ï¼ˆå¦‚$n = 10^6$ï¼‰ï¼Œ$\log n \approx 14$ï¼Œç¼©æ”¾å› å­å¯èƒ½æ˜¾è‘—ä¸åŒã€‚</p>
<p><strong>è§£å†³æ–¹æ¡ˆ1</strong>ï¼šåˆ†æ®µå¤„ç†</p>
<p>å°†é•¿åºåˆ—åˆ†æˆå¤šä¸ªæ®µï¼Œæ¯æ®µé•¿åº¦ä¸º$n_{\text{seg}}$ï¼Œåˆ†åˆ«è®¡ç®—attentionã€‚</p>
<p><strong>è§£å†³æ–¹æ¡ˆ2</strong>ï¼šå±‚æ¬¡åŒ–attention</p>
<p>ä½¿ç”¨å±‚æ¬¡åŒ–çš„attentionç»“æ„ï¼Œå…ˆåœ¨å±€éƒ¨çª—å£å†…è®¡ç®—attentionï¼Œå†åœ¨å…¨å±€levelè®¡ç®—ã€‚</p>
<p><strong>è§£å†³æ–¹æ¡ˆ3</strong>ï¼šè‡ªé€‚åº”ç¼©æ”¾</p>
<p>æ ¹æ®ç†µçš„å®é™…å€¼åŠ¨æ€è°ƒæ•´$\lambda$ï¼š
\begin{equation}
\lambda \leftarrow \lambda \cdot \frac{H_{\text{target}}}{H_{\text{current}}} \tag{61}
\end{equation}</p>
<h3 id="25">25. ä¸æ¸©åº¦å‚æ•°çš„ç»Ÿä¸€</h3>
<p><strong>æ¸©åº¦å‚æ•°</strong>ï¼š</p>
<p>åœ¨å¾ˆå¤šåœºæ™¯ï¼ˆå¦‚çŸ¥è¯†è’¸é¦ï¼‰ä¸­ï¼Œä½¿ç”¨æ¸©åº¦å‚æ•°$T$ï¼š
\begin{equation}
\alpha_i = \frac{e^{s_i / T}}{\sum_j e^{s_j / T}} \tag{62}
\end{equation}</p>
<p><strong>ä¸ç¼©æ”¾å› å­çš„å…³ç³»</strong>ï¼š</p>
<p>$\lambda = 1/T$æ˜¯ç¼©æ”¾å› å­çš„å€’æ•°ã€‚</p>
<p><strong>ç»Ÿä¸€æ¡†æ¶</strong>ï¼š</p>
<p>æˆ‘ä»¬å¯ä»¥å°†ç†µä¸å˜æ€§ç¼©æ”¾çœ‹ä½œä¸€ç§<strong>è‡ªé€‚åº”æ¸©åº¦</strong>ï¼š
\begin{equation}
T(n, d) = \frac{d}{\kappa \log n} \tag{63}
\end{equation}</p>
<p><strong>è§£é‡Š</strong>ï¼š</p>
<ul>
<li>å½“$n$å¢å¤§æ—¶ï¼Œ$T$å‡å°ï¼ˆæ¸©åº¦é™ä½ï¼‰ï¼Œä½¿softmaxæ›´åŠ peaked</li>
<li>è¿™è¡¥å¿äº†åºåˆ—é•¿åº¦å¢åŠ å¸¦æ¥çš„ç†µè‡ªç„¶å¢é•¿</li>
</ul>
<h3 id="26">26. ç†è®ºå±€é™æ€§</h3>
<p><strong>å‡è®¾çš„å±€é™</strong>ï¼š</p>
<ol>
<li>
<p><strong>å¹³å‡åœºè¿‘ä¼¼</strong>ï¼šå¼(10)å‡è®¾$\log \mathbb{E}[e^{\lambda s}] \approx \mathbb{E}[\lambda s]$ï¼Œè¿™åœ¨$\lambda$æˆ–$s$çš„æ–¹å·®å¾ˆå¤§æ—¶ä¸å‡†ç¡®ã€‚</p>
</li>
<li>
<p><strong>ç‹¬ç«‹æ€§å‡è®¾</strong>ï¼šå‡è®¾$s_i$ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œä½†å®é™…ä¸Šå®ƒä»¬å¯èƒ½æœ‰ç›¸å…³æ€§ï¼ˆå¦‚ç›¸é‚»tokençš„ç›¸ä¼¼æ€§ï¼‰ã€‚</p>
</li>
<li>
<p><strong>$\mathbb{E}<em _max="\max">\alpha[s] \approx s</em>$</strong>ï¼šè¿™ä¸ªè¿‘ä¼¼åœ¨softmaxéå¸¸peakedæ—¶æ‰å‡†ç¡®ã€‚</p>
</li>
</ol>
<p><strong>é€‚ç”¨èŒƒå›´</strong>ï¼š</p>
<p>ç†µä¸å˜æ€§ç¼©æ”¾åœ¨ä»¥ä¸‹æƒ…å†µä¸‹æœ€æœ‰æ•ˆï¼š
- åºåˆ—é•¿åº¦$n$å˜åŒ–èŒƒå›´å¤§ï¼ˆå¦‚ä»10åˆ°10000ï¼‰
- ç»´åº¦$d$é€‚ä¸­ï¼ˆå¦‚64åˆ°512ï¼‰
- æ•°æ®åˆ†å¸ƒç›¸å¯¹å‡åŒ€ï¼ˆæ²¡æœ‰æç«¯çš„outliersï¼‰</p>
<h3 id="27">27. å®éªŒè®¾è®¡å»ºè®®</h3>
<p><strong>æ¶ˆèå®éªŒ</strong>ï¼š</p>
<p>å¯¹æ¯”ä»¥ä¸‹ç¼©æ”¾ç­–ç•¥ï¼š
1. æ ‡å‡†ç¼©æ”¾ï¼š$\lambda = 1/\sqrt{d}$
2. ç†µä¸å˜æ€§ç¼©æ”¾ï¼š$\lambda = \log n / d$
3. æ··åˆç¼©æ”¾ï¼š$\lambda = \alpha/\sqrt{d} + \beta \log n / d$</p>
<p><strong>è¯„ä¼°æŒ‡æ ‡</strong>ï¼š</p>
<ol>
<li><strong>ä»»åŠ¡æ€§èƒ½</strong>ï¼šå›°æƒ‘åº¦ï¼ˆperplexityï¼‰ã€å‡†ç¡®ç‡ç­‰</li>
<li><strong>ç†µçš„ç¨³å®šæ€§</strong>ï¼šåœ¨ä¸åŒ$n$ä¸‹ï¼Œç†µçš„æ–¹å·®</li>
<li><strong>æ”¶æ•›é€Ÿåº¦</strong>ï¼šè¾¾åˆ°ç›®æ ‡æ€§èƒ½æ‰€éœ€çš„è®­ç»ƒæ­¥æ•°</li>
</ol>
<p><strong>æ•°æ®é›†</strong>ï¼š</p>
<p>é€‰æ‹©åºåˆ—é•¿åº¦å˜åŒ–å¤§çš„æ•°æ®é›†ï¼Œå¦‚ï¼š
- æ–‡æœ¬ï¼šä¸åŒé•¿åº¦çš„å¥å­/æ®µè½
- å›¾åƒï¼šä¸åŒåˆ†è¾¨ç‡çš„patchåºåˆ—</p>
<h3 id="28">28. å¼€æ”¾é—®é¢˜</h3>
<p><strong>é—®é¢˜1</strong>ï¼šæœ€ä¼˜çš„$\kappa$å€¼</p>
<p>ç†è®ºä¸Š$\kappa = 1$ï¼Œä½†å®éªŒä¸­å¯èƒ½éœ€è¦è°ƒæ•´ã€‚å¦‚ä½•è‡ªåŠ¨ç¡®å®šæœ€ä¼˜$\kappa$ï¼Ÿ</p>
<p><strong>é—®é¢˜2</strong>ï¼šéå‡åŒ€åˆ†å¸ƒ</p>
<p>å¦‚æœ$s_i$ä¸æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œè€Œæ˜¯æœ‰æ˜æ˜¾çš„èšç±»ç»“æ„ï¼Œç†µä¸å˜æ€§æ¡ä»¶å¦‚ä½•ä¿®æ­£ï¼Ÿ</p>
<p><strong>é—®é¢˜3</strong>ï¼šä¸å…¶ä»–æ­£åˆ™åŒ–çš„ç»“åˆ</p>
<p>ç†µä¸å˜æ€§ç¼©æ”¾èƒ½å¦ä¸dropoutã€weight decayç­‰æ­£åˆ™åŒ–æŠ€æœ¯ç»“åˆï¼Ÿ</p>
<p><strong>é—®é¢˜4</strong>ï¼šå› æœæ³¨æ„åŠ›</p>
<p>åœ¨decoderçš„å› æœæ³¨æ„åŠ›ï¼ˆcausal attentionï¼‰ä¸­ï¼Œæœ‰æ•ˆ$n$éšä½ç½®å˜åŒ–ï¼Œå¦‚ä½•å¤„ç†ï¼Ÿ</p>
<h3 id="29">29. ä»£ç å®ç°</h3>
<p><strong>PyTorchå®ç°</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">EntropyInvariantAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">=</span> <span class="n">kappa</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># çº¿æ€§å˜æ¢</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># è®¡ç®—attention scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># (batch, heads, n, n)</span>

        <span class="c1"># ç†µä¸å˜æ€§ç¼©æ”¾</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">*</span> <span class="n">scale</span>

        <span class="c1"># åº”ç”¨mask</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

        <span class="c1"># Softmax</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># åŠ æƒæ±‚å’Œ</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># (batch, heads, n, d_k)</span>

        <span class="c1"># åˆå¹¶å¤šå¤´</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span>
</code></pre></div>

<p><strong>ä½¿ç”¨ç¤ºä¾‹</strong>ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># åˆå§‹åŒ–</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">EntropyInvariantAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># è¾“å…¥</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="c1"># å‰å‘ä¼ æ’­</span>
<span class="n">output</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (32, 100, 512)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention weights shape: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (32, 8, 100, 100)</span>
</code></pre></div>

<h3 id="30">30. æ€»ç»“ä¸å±•æœ›</h3>
<p><strong>æ ¸å¿ƒè´¡çŒ®</strong>ï¼š</p>
<ol>
<li>ä»ç†µä¸å˜æ€§è§’åº¦æ¨å¯¼äº†ç¼©æ”¾å› å­$\lambda \propto \log n / d$</li>
<li>è¿æ¥äº†åºåˆ—é•¿åº¦ã€ç»´åº¦å’Œæ³¨æ„åŠ›ç†µä¹‹é—´çš„å…³ç³»</li>
<li>æä¾›äº†æ¯”æ ‡å‡†$1/\sqrt{d}$æ›´ç²¾ç»†çš„ç¼©æ”¾ç­–ç•¥</li>
</ol>
<p><strong>ç†è®ºæ„ä¹‰</strong>ï¼š</p>
<ul>
<li>æ­ç¤ºäº†Softmaxæ³¨æ„åŠ›çš„ä¿¡æ¯è®ºæ€§è´¨</li>
<li>ä¸ºç†è§£Transformerçš„ç¼©æ”¾è§„å¾‹æä¾›äº†æ–°è§†è§’</li>
<li>è¿æ¥äº†ç¦»æ•£ä¼˜åŒ–ï¼ˆmaxï¼‰å’Œè¿ç»­ä¼˜åŒ–ï¼ˆsoftmaxï¼‰</li>
</ul>
<p><strong>å®è·µä»·å€¼</strong>ï¼š</p>
<ul>
<li>åœ¨åºåˆ—é•¿åº¦å˜åŒ–å¤§çš„ä»»åŠ¡ä¸­å¯èƒ½æé«˜æ€§èƒ½</li>
<li>ä¸ºè¶…é•¿åºåˆ—å»ºæ¨¡æä¾›äº†ç†è®ºæŒ‡å¯¼</li>
<li>æœ‰åŠ©äºç†è§£å’Œè°ƒè¯•attentionæœºåˆ¶</li>
</ul>
<p><strong>æœªæ¥æ–¹å‘</strong>ï¼š</p>
<ol>
<li>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­éªŒè¯ç†µä¸å˜æ€§ç¼©æ”¾</li>
<li>æ¢ç´¢è‡ªé€‚åº”$\kappa$çš„å­¦ä¹ æ–¹æ³•</li>
<li>æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„attentionï¼ˆå¦‚linear attentionï¼‰</li>
<li>ç ”ç©¶ç†µä¸æ³›åŒ–æ€§èƒ½çš„å…³ç³»</li>
</ol>
<hr />
<h2 id="_4">å‚è€ƒæ–‡çŒ®</h2>
<ol>
<li>Vaswani et al., "Attention Is All You Need", NeurIPS 2017</li>
<li>Johnson &amp; Lindenstrauss, "Extensions of Lipschitz mappings into a Hilbert space", 1984</li>
<li>Dao et al., "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", NeurIPS 2022</li>
<li>Zhang et al., "Root Mean Square Layer Normalization", NeurIPS 2019</li>
<li>Su et al., "RoFormer: Enhanced Transformer with Rotary Position Embedding", 2021</li>
</ol>
<h2 id="_5">æœ€ç»ˆæ€»ç»“</h2>
<p>æœ¬æ–‡ä»ç†µä¸å˜æ€§è§’åº¦é‡æ–°æ¨å¯¼äº†Softmaxæ³¨æ„åŠ›çš„ç¼©æ”¾å› å­ï¼Œå¾—åˆ°ï¼š</p>
<p>\begin{equation}
\lambda = \frac{\kappa \log n}{d}
\end{equation}</p>
<p><strong>å…³é”®æ´å¯Ÿ</strong>ï¼š
1. <strong>å¹³å‡åœºè¿‘ä¼¼</strong>ï¼š$\log \mathbb{E}[e^{\lambda s}] \approx \mathbb{E}[\lambda s]$è¿æ¥äº†æŒ‡æ•°å’Œå¯¹æ•°
2. <strong>Maxè¿‘ä¼¼</strong>ï¼šSoftmaxä¾§é‡æœ€å¤§å€¼ï¼Œ$\mathbb{E}<em _max="\max">\alpha[s] \approx s</em>$
3. <strong>ç†µçš„æ¸è¿‘</strong>ï¼š$H \approx \log n - \lambda(s_{\max} - \bar{s})$ç»™å‡ºäº†ç†µä¸$n$çš„å…³ç³»
4. <strong>æ–¹å·®æ§åˆ¶</strong>ï¼šé™¤ä»¥$d$ä¿æŒåˆ†æ•°æ–¹å·®ç¨³å®š</p>
<p><strong>å®ç”¨å»ºè®®</strong>ï¼š
- å¯¹äºå›ºå®šé•¿åº¦åºåˆ—ï¼Œä½¿ç”¨æ ‡å‡†$1/\sqrt{d}$ç¼©æ”¾å³å¯
- å¯¹äºå˜é•¿åºåˆ—ï¼ˆå¦‚$n \in [10, 1000]$ï¼‰ï¼Œè€ƒè™‘ç†µä¸å˜æ€§ç¼©æ”¾
- è¶…é•¿åºåˆ—å»ºæ¨¡ä¸­ï¼Œ$\log n$å› å­å¯èƒ½å¸¦æ¥æ˜¾è‘—æ”¹å–„</p>
<p>ç†µä¸å˜æ€§æä¾›äº†ç†è§£å’Œè®¾è®¡attentionæœºåˆ¶çš„æ–°è§†è§’ï¼Œå€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢ï¼</p>
        </div>
    </div>
</body>
</html>