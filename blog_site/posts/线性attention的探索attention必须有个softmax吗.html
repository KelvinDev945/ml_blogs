<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>çº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿ</title>
    <style>
* { margin: 0; padding: 0; box-sizing: border-box; }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
    line-height: 1.6; color: #333; max-width: 900px; margin: 0 auto; padding: 20px; background: #f5f5f5;
}
.container { background: white; padding: 40px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
header { border-bottom: 2px solid #e0e0e0; padding-bottom: 20px; margin-bottom: 30px; }
h1 { color: #2c3e50; margin-bottom: 10px; font-size: 2em; }
.meta { color: #666; font-size: 0.9em; margin-bottom: 20px; }
.content { margin-top: 30px; overflow-wrap: break-word; }
.content h2 { color: #34495e; margin-top: 30px; margin-bottom: 15px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
.content p { margin-bottom: 15px; }
.content pre { background: #f8f8f8; padding: 15px; border-radius: 5px; overflow-x: auto; border-left: 3px solid #3498db; margin: 15px 0; }
.content code { background: #f8f8f8; padding: 2px 5px; border-radius: 3px; font-family: monospace; font-size: 0.9em; }
.content blockquote { border-left: 4px solid #3498db; padding-left: 20px; margin: 20px 0; color: #555; font-style: italic; }
.content table { border-collapse: collapse; width: 100%; margin: 20px 0; }
.content th, .content td { border: 1px solid #ddd; padding: 10px; text-align: left; }
.back-link { display: inline-block; margin-bottom: 20px; color: #3498db; text-decoration: none; font-weight: 500; }
</style>
    
<script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\(', '\)']],
            displayMath: [['$$', '$$'], ['\[', '\]']],
            processEscapes: true
        }
    };
</script>
<script id="MathJax-script" async src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div class="container">
        <a href="../index.html" class="back-link">â† è¿”å›é¦–é¡µ</a>
        <header>
            <h1>çº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿ</h1>
            <div class="meta">ğŸ“… æœ€åæ›´æ–°: 2025-12-31 | ğŸ“„ å¤§å°: 17.6 KB</div>
        </header>
        <div class="content">
            <p><strong>åŸæ–‡é“¾æ¥</strong>: <a href="https://spaces.ac.cn/archives/7546">https://spaces.ac.cn/archives/7546</a></p>
<p><strong>å‘å¸ƒæ—¥æœŸ</strong>: </p>
<hr />
<p>ä¼—æ‰€å‘¨çŸ¥ï¼Œå°½ç®¡åŸºäºAttentionæœºåˆ¶çš„Transformerç±»æ¨¡å‹æœ‰ç€è‰¯å¥½çš„å¹¶è¡Œæ€§èƒ½ï¼Œä½†å®ƒçš„ç©ºé—´å’Œæ—¶é—´å¤æ‚åº¦éƒ½æ˜¯$\mathcal{O}(n^2)$çº§åˆ«çš„ï¼Œ$n$æ˜¯åºåˆ—é•¿åº¦ï¼Œæ‰€ä»¥å½“$n$æ¯”è¾ƒå¤§æ—¶Transformeræ¨¡å‹çš„è®¡ç®—é‡éš¾ä»¥æ‰¿å—ã€‚è¿‘æ¥ï¼Œä¹Ÿæœ‰ä¸å°‘å·¥ä½œè‡´åŠ›äºé™ä½Transformeræ¨¡å‹çš„è®¡ç®—é‡ï¼Œæ¯”å¦‚æ¨¡å‹å‰ªæã€é‡åŒ–ã€è’¸é¦ç­‰ç²¾ç®€æŠ€æœ¯ï¼Œåˆæˆ–è€…ä¿®æ”¹Attentionç»“æ„ï¼Œä½¿å¾—å…¶å¤æ‚åº¦èƒ½é™ä½åˆ°$\mathcal{O}(n\log n)$ç”šè‡³$\mathcal{O}(n)$ã€‚</p>
<p>å‰å‡ å¤©ç¬”è€…è¯»åˆ°äº†è®ºæ–‡<a href="https://papers.cool/arxiv/2006.16236">ã€ŠTransformers are RNNs: Fast Autoregressive Transformers with Linear Attentionã€‹</a>ï¼Œäº†è§£åˆ°äº†çº¿æ€§åŒ–Attentionï¼ˆLinear Attentionï¼‰è¿™ä¸ªæ¢ç´¢ç‚¹ï¼Œç»§è€Œé˜…è¯»äº†ä¸€äº›ç›¸å…³æ–‡çŒ®ï¼Œæœ‰ä¸€äº›ä¸é”™çš„æ”¶è·ï¼Œæœ€åå°†è‡ªå·±å¯¹çº¿æ€§åŒ–Attentionçš„ç†è§£æ±‡æ€»åœ¨æ­¤æ–‡ä¸­ã€‚</p>
<h2 id="attention">Attention</h2>
<p>å½“å‰æœ€æµè¡Œçš„Attentionæœºåˆ¶å½“å±<a href="https://papers.cool/arxiv/1706.03762">Scaled-Dot Attention</a>ï¼Œå½¢å¼ä¸º<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax\left(\boldsymbol{Q}\boldsymbol{K}^{\top}\right)\boldsymbol{V}\label{eq:std-att}\end{equation}<br />
è¿™é‡Œçš„$\boldsymbol{Q}\in\mathbb{R}^{n\times d_k}, \boldsymbol{K}\in\mathbb{R}^{m\times d_k}, \boldsymbol{V}\in\mathbb{R}^{m\times d_v}$ï¼Œç®€å•èµ·è§æˆ‘ä»¬å°±æ²¡æ˜¾å¼åœ°å†™å‡ºAttentionçš„ç¼©æ”¾å› å­äº†ã€‚æœ¬æ–‡æˆ‘ä»¬ä¸»è¦å…³å¿ƒSelf Attentionåœºæ™¯ï¼Œæ‰€ä»¥ä¸ºäº†ä»‹ç»ä¸Šçš„æ–¹ä¾¿ç»Ÿä¸€è®¾$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}\in\mathbb{R}^{n\times d}$ï¼Œä¸€èˆ¬åœºæ™¯ä¸‹éƒ½æœ‰$n &gt; d$ç”šè‡³$n\gg d$ï¼ˆBERT baseé‡Œè¾¹$d=64$ï¼‰ã€‚ç›¸å…³è§£è¯»å¯ä»¥å‚è€ƒç¬”è€…çš„<a href="/archives/4765">ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰</a>ï¼Œä»¥åŠå®ƒçš„ä¸€äº›æ”¹è¿›å·¥ä½œä¹Ÿå¯ä»¥å‚è€ƒ<a href="/archives/7325">ã€Šçªç ´ç“¶é¢ˆï¼Œæ‰“é€ æ›´å¼ºå¤§çš„Transformerã€‹</a>ã€<a href="/archives/7430">ã€ŠGoogleæ–°ä½œSynthesizerï¼šæˆ‘ä»¬è¿˜ä¸å¤Ÿäº†è§£è‡ªæ³¨æ„åŠ›ã€‹</a>ï¼Œè¿™é‡Œå°±ä¸å¤šæ·±å…¥ä»‹ç»äº†ã€‚</p>
<h3 id="softmax">æ‘˜æ‰Softmax</h3>
<p>è¯»è€…ä¹Ÿè®¸æƒ³ä¸åˆ°ï¼Œåˆ¶çº¦Attentionæ€§èƒ½çš„å…³é”®å› ç´ ï¼Œå…¶å®æ˜¯å®šä¹‰é‡Œè¾¹çš„Softmaxï¼äº‹å®ä¸Šï¼Œç®€å•åœ°æ¨å¯¼ä¸€ä¸‹å°±å¯ä»¥å¾—åˆ°è¿™ä¸ªç»“è®ºã€‚$\boldsymbol{Q}\boldsymbol{K}^{\top}$è¿™ä¸€æ­¥æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª$n\times n$çš„çŸ©é˜µï¼Œå°±æ˜¯è¿™ä¸€æ­¥å†³å®šäº†Attentionçš„å¤æ‚åº¦æ˜¯$\mathcal{O}(n^2)$ï¼›å¦‚æœæ²¡æœ‰Softmaxï¼Œé‚£ä¹ˆå°±æ˜¯ä¸‰ä¸ªçŸ©é˜µè¿ä¹˜$\boldsymbol{Q}\boldsymbol{K}^{\top}\boldsymbol{V}$ï¼Œè€ŒçŸ©é˜µä¹˜æ³•æ˜¯æ»¡è¶³ç»“åˆç‡çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å…ˆç®—$\boldsymbol{K}^{\top}\boldsymbol{V}$ï¼Œå¾—åˆ°ä¸€ä¸ª$d\times d$çš„çŸ©é˜µï¼Œç„¶åå†ç”¨$\boldsymbol{Q}$å·¦ä¹˜å®ƒï¼Œç”±äº$d \ll n$ï¼Œæ‰€ä»¥è¿™æ ·ç®—å¤§è‡´çš„å¤æ‚åº¦åªæ˜¯$\mathcal{O}(n)$ï¼ˆå°±æ˜¯$\boldsymbol{Q}$å·¦ä¹˜é‚£ä¸€æ­¥å ä¸»å¯¼ï¼‰ã€‚</p>
<p>ä¹Ÿå°±æ˜¯è¯´ï¼Œå»æ‰Softmaxçš„Attentionçš„å¤æ‚åº¦å¯ä»¥é™åˆ°æœ€ç†æƒ³çš„çº¿æ€§çº§åˆ«$\mathcal{O}(n)$ï¼è¿™æ˜¾ç„¶å°±æ˜¯æˆ‘ä»¬çš„ç»ˆæè¿½æ±‚ï¼šLinear Attentionï¼Œå¤æ‚åº¦ä¸ºçº¿æ€§çº§åˆ«çš„Attentionã€‚æ‰€ä»¥ï¼Œæœ¬æ–‡çš„ä¸»é¢˜å°±æ˜¯æ¢ç©¶æ‘˜æ‰Softmaxåçš„çº¿å½¢Attentionã€‚</p>
<h3 id="_1">ä¸€èˆ¬çš„å®šä¹‰</h3>
<p>é—®é¢˜æ˜¯ï¼Œç›´æ¥å»æ‰Softmaxè¿˜èƒ½ç®—æ˜¯Attentionå—ï¼Ÿå®ƒè¿˜èƒ½æœ‰æ ‡å‡†çš„Attentionçš„æ•ˆæœå—ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å…ˆå°†Scaled-Dot Attentionçš„å®šä¹‰$\eqref{eq:std-att}$ç­‰ä»·åœ°æ”¹å†™ä¸ºï¼ˆæœ¬æ–‡çš„å‘é‡éƒ½æ˜¯åˆ—å‘é‡ï¼‰<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})<em j="1">i = \frac{\sum\limits</em>}^n e^{\boldsymbol{q<em j="1">i^{\top}\boldsymbol{k}_j}\boldsymbol{v}_j}{\sum\limits</em>}^n e^{\boldsymbol{q<em j="1">i^{\top}\boldsymbol{k}_j}}\label{eq:std-att-2}\end{equation}<br />
æ‰€ä»¥ï¼ŒScaled-Dot Attentionå…¶å®å°±æ˜¯ä»¥$e^{\boldsymbol{q}_i^{\top}\boldsymbol{k}_j}$ä¸ºæƒé‡å¯¹$\boldsymbol{v}_j$åšåŠ æƒå¹³å‡ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æå‡ºä¸€ä¸ªAttentionçš„ä¸€èˆ¬åŒ–å®šä¹‰<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})_i = \frac{\sum\limits</em>}^n \text{sim}(\boldsymbol{q<em j="1">i, \boldsymbol{k}_j)\boldsymbol{v}_j}{\sum\limits</em>}^n \text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)}\label{eq:gen-att}\end{equation<br />
ä¹Ÿå°±æ˜¯æŠŠ$e^{\boldsymbol{q}_i^{\top}\boldsymbol{k}_j}$æ¢æˆ$\boldsymbol{q}_i, \boldsymbol{k}_j$çš„ä¸€èˆ¬å‡½æ•°$\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)$ï¼Œä¸ºäº†ä¿ç•™Attentionç›¸ä¼¼çš„åˆ†å¸ƒç‰¹æ€§ï¼Œæˆ‘ä»¬è¦æ±‚$\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)\geq 0$æ’æˆç«‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¦‚æœè¦å®šä¹‰æ–°å¼çš„Attentionï¼Œé‚£ä¹ˆè¦ä¿ç•™å¼$\eqref{eq:gen-att}$çš„å½¢å¼ï¼Œå¹¶ä¸”æ»¡è¶³$\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)\geq 0$ã€‚</p>
<p>è¿™ç§ä¸€èˆ¬å½¢å¼çš„Attentionåœ¨CVä¸­ä¹Ÿè¢«ç§°ä¸ºNon-Localç½‘ç»œï¼Œå‡ºè‡ªè®ºæ–‡<a href="1711.07971">ã€ŠNon-local Neural Networksã€‹</a>ã€‚</p>
<h2 id="_2">å‡ ä¸ªä¾‹å­</h2>
<p>å¦‚æœç›´æ¥å»æ‰Softmaxï¼Œé‚£ä¹ˆå°±æ˜¯$\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = \boldsymbol{q}_i^{\top}\boldsymbol{k}_j$ï¼Œé—®é¢˜æ˜¯å†…ç§¯æ— æ³•ä¿è¯éè´Ÿæ€§ï¼Œæ‰€ä»¥è¿™è¿˜ä¸æ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚ä¸‹é¢æˆ‘ä»¬ç®€å•ä»‹ç»å‡ ç§å¯å–çš„æ–¹æ¡ˆã€‚</p>
<p>å€¼å¾—æŒ‡å‡ºçš„æ˜¯ï¼Œä¸‹é¢ä»‹ç»çš„è¿™å‡ ç§Linear Attentionï¼Œå‰ä¸¤ç§æ¥è‡ªCVé¢†åŸŸï¼Œç¬¬ä¸‰ç§æ˜¯ç¬”è€…è‡ªå·±æ„æ€çš„ï¼Œæ‰€ä»¥éƒ½è¿˜æ²¡æœ‰åœ¨NLPä»»åŠ¡ä¸Šåšè¿‡ä»€ä¹ˆå®éªŒï¼Œå„ä½åšæ¨¡å‹æ”¹è¿›çš„NLPerä»¬å°±æœ‰å®éªŒæ–¹å‘äº†ï¼ˆ^_^ï¼‰ï½ï½é¡ºä¾¿è¯´ä¸€ä¸‹ï¼ŒCVé¢†åŸŸæœ‰ä¸å°‘å¯¹Attentionçš„æ”¹è¿›å·¥ä½œï¼ˆé™¤äº†ä¸‹é¢ä»‹ç»çš„å¤–ï¼Œè¿˜æœ‰<a href="https://papers.cool/arxiv/1907.13426">EMANet</a>ç­‰ï¼‰ï¼Œå¾ˆå¤šå†…å®¹éƒ½å€¼å¾—åšNLPçš„æˆ‘ä»¬å‚è€ƒé˜…è¯»ã€‚</p>
<h3 id="_3">æ ¸å‡½æ•°å½¢å¼</h3>
<p>ä¸€ä¸ªè‡ªç„¶çš„æƒ³æ³•æ˜¯ï¼šå¦‚æœ$\boldsymbol{q}_i,\boldsymbol{k}_j$çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯éè´Ÿçš„ï¼Œé‚£ä¹ˆå†…ç§¯è‡ªç„¶ä¹Ÿå°±æ˜¯éè´Ÿçš„ã€‚ä¸ºäº†å®Œæˆè¿™ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ç»™$\boldsymbol{q}_i,\boldsymbol{k}_j$å„è‡ªåŠ ä¸ªæ¿€æ´»å‡½æ•°$\phi,\varphi$ï¼Œå³<br />
\begin{equation}\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = \phi(\boldsymbol{q}_i)^{\top} \varphi(\boldsymbol{k}_j)\label{eq:gen-att-2}\end{equation}<br />
å…¶ä¸­$\phi(\cdot),\varphi(\cdot)$æ˜¯å€¼åŸŸéè´Ÿçš„æ¿€æ´»å‡½æ•°ã€‚æœ¬æ–‡å¼€å¤´æåˆ°çš„è®ºæ–‡<a href="https://papers.cool/arxiv/2006.16236">ã€ŠTransformers are RNNs: Fast Autoregressive Transformers with Linear Attentionã€‹</a>é€‰æ‹©çš„æ˜¯$\phi(x)=\varphi(x)=\text{elu}(x)+1$ã€‚</p>
<p>éè¦è®²æ•…äº‹çš„è¯ï¼Œå¼$\eqref{eq:gen-att-2}$å¯ä»¥è”æƒ³åˆ°â€œæ ¸æ–¹æ³•ï¼ˆkernal methodï¼‰â€ï¼Œå°¤å…¶æ˜¯$\phi=\varphi$æ—¶$\phi$å°±ç›¸å½“äºä¸€ä¸ªæ ¸å‡½æ•°ï¼Œè€Œ$\langle \phi(\boldsymbol{q}_i), \phi(\boldsymbol{k}_j)\rangle$å°±æ˜¯é€šè¿‡æ ¸å‡½æ•°æ‰€å®šä¹‰çš„å†…ç§¯ã€‚è¿™æ–¹é¢çš„æ€è€ƒå¯ä»¥å‚è€ƒè®ºæ–‡<a href="https://papers.cool/arxiv/1908.11775">ã€ŠTransformer dissection: An unified understanding for transformerâ€™s attention via the lens of kernelã€‹</a>ï¼Œæ­¤å¤„ä¸åšè¿‡å¤šå»¶ä¼¸ã€‚</p>
<h3 id="softmax_1">å¦™ç”¨Softmax</h3>
<p>å¦ä¸€ç¯‡æ›´æ—©çš„æ–‡ç« <a href="https://papers.cool/arxiv/1812.01243">ã€ŠEfficient Attention: Attention with Linear Complexitiesã€‹</a>åˆ™ç»™å‡ºäº†ä¸€ä¸ªæ›´æœ‰æ„æ€çš„é€‰æ‹©ã€‚å®ƒç•™æ„åˆ°åœ¨$\boldsymbol{Q}\boldsymbol{K}^{\top}$ä¸­ï¼Œ$\boldsymbol{Q}, \boldsymbol{K}, \in\mathbb{R}^{n\times d}$ï¼Œå¦‚æœâ€œ$\boldsymbol{Q}$åœ¨$d$é‚£ä¸€ç»´æ˜¯å½’ä¸€åŒ–çš„ã€å¹¶ä¸”$\boldsymbol{K}$åœ¨$n$é‚£ä¸€ç»´æ˜¯å½’ä¸€åŒ–çš„â€ï¼Œé‚£ä¹ˆ$\boldsymbol{Q}\boldsymbol{K}^{\top}$å°±æ˜¯è‡ªåŠ¨æ»¡è¶³å½’ä¸€åŒ–äº†ï¼Œæ‰€ä»¥å®ƒç»™å‡ºçš„é€‰æ‹©æ˜¯ï¼š<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax_2\left(\boldsymbol{Q}\right)softmax_1(\boldsymbol{K})^{\top}\boldsymbol{V}\end{equation}<br />
å…¶ä¸­$softmax_1$ã€$softmax_2$åˆ†åˆ«æŒ‡åœ¨ç¬¬ä¸€ä¸ªï¼ˆ$n$ï¼‰ã€ç¬¬äºŒä¸ªç»´åº¦ï¼ˆ$d$ï¼‰è¿›è¡ŒSoftmaxè¿ç®—ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™æ—¶å€™æˆ‘ä»¬æ˜¯å„è‡ªç»™$\boldsymbol{Q},\boldsymbol{K}$åŠ Softmaxï¼Œè€Œä¸æ˜¯$\boldsymbol{Q}\boldsymbol{K}^{\top}$ç®—å®Œä¹‹åæ‰åŠ Softmaxã€‚</p>
<p>å¦‚æœç›´æ¥å–$\phi(\boldsymbol{q}_i)=softmax(\boldsymbol{q}_i),\varphi(\boldsymbol{k}_j)=softmax(\boldsymbol{k}_j)$ï¼Œé‚£ä¹ˆå¾ˆæ˜¾ç„¶è¿™ä¸ªå½¢å¼ä¹Ÿæ˜¯å¼$\eqref{eq:gen-att-2}$çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚å¦å¤–è¿™ä¸ªè®¾è®¡åœ¨CVä¸­å‡ºç°è¿‡ä¸æ­¢ä¸€æ¬¡ï¼Œæ¯”å¦‚<a href="https://papers.nips.cc/paper/7318-a2-nets-double-attention-networks.pdf">A2-Nets</a>ä¹ŸåŒ…å«äº†åŒæ ·çš„åšæ³•ã€‚</p>
<h3 id="_4">è‡ªå·±çš„æ„æ€</h3>
<p>åœ¨è¿™é‡Œï¼Œç¬”è€…ç»™å‡ºè‡ªå·±çš„ä¸€ç§æ„æ€ã€‚è¿™ä¸ªæ„æ€çš„å‡ºå‘ç‚¹ä¸å†æ˜¯å¼$\eqref{eq:gen-att-2}$ï¼Œè€Œæ˜¯æºäºæˆ‘ä»¬å¯¹åŸå§‹å®šä¹‰$\eqref{eq:std-att-2}$çš„è¿‘ä¼¼ã€‚ç”±æ³°å‹’å±•å¼€æˆ‘ä»¬æœ‰<br />
\begin{equation}e^{\boldsymbol{q}_i^{\top}\boldsymbol{k}_j} \approx 1 + \boldsymbol{q}_i^{\top}\boldsymbol{k}_j\end{equation}<br />
å¦‚æœ$\boldsymbol{q}_i^{\top}\boldsymbol{k}_j\geq -1$ï¼Œé‚£ä¹ˆå°±å¯ä»¥ä¿è¯å³ç«¯çš„éè´Ÿæ€§ï¼Œè€Œä»å¯ä»¥è®©$\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)=1 + \boldsymbol{q}_i^{\top}\boldsymbol{k}_j$ã€‚åˆ°è¿™é‡Œè¯»è€…å¯èƒ½å·²ç»æƒ³åˆ°äº†ï¼Œæƒ³è¦ä¿è¯$\boldsymbol{q}_i^{\top}\boldsymbol{k}_j\geq -1$ï¼Œåªéœ€è¦åˆ†åˆ«å¯¹$\boldsymbol{q}_i,\boldsymbol{k}_j$åš$l_2$å½’ä¸€åŒ–ã€‚æ‰€ä»¥ï¼Œç¬”è€…æœ€ç»ˆæå‡ºçš„æ–¹æ¡ˆå°±æ˜¯ï¼š<br />
\begin{equation}\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = 1 + \left( \frac{\boldsymbol{q}_i}{\Vert \boldsymbol{q}_i\Vert}\right)^{\top}\left(\frac{\boldsymbol{k}_j}{\Vert \boldsymbol{k}_j\Vert}\right)\end{equation}<br />
è¿™ä¸åŒäºå½¢å¼$\eqref{eq:gen-att-2}$ï¼Œä½†ç†è®ºä¸Šå®ƒæ›´åŠ æ¥è¿‘åŸå§‹çš„Scaled-Dot Attentionã€‚</p>
<h2 id="_5">ç›¸å…³å·¥ä½œ</h2>
<p>é€šè¿‡ä¿®æ”¹Attentionçš„å½¢å¼æ¥é™ä½å®ƒçš„è®¡ç®—å¤æ‚åº¦ï¼Œç›¸å…³çš„å·¥ä½œæœ‰å¾ˆå¤šï¼Œè¿™é‡Œç®€è¦åˆ—ä¸¾ä¸€äº›ã€‚</p>
<h3 id="attention_1">ç¨€ç–Attention</h3>
<p>æˆ‘ä»¬ä¹‹å‰ä»‹ç»è¿‡OpenAIçš„<a href="/archives/6853#Sparse%20Self%20Attention">Sparse Attention</a>ï¼Œé€šè¿‡â€œåªä¿ç•™å°åŒºåŸŸå†…çš„æ•°å€¼ã€å¼ºåˆ¶è®©å¤§éƒ¨åˆ†æ³¨æ„åŠ›ä¸ºé›¶â€çš„æ–¹å¼ï¼Œæ¥å‡å°‘Attentionçš„è®¡ç®—é‡ã€‚ç»è¿‡ç‰¹æ®Šè®¾è®¡ä¹‹åï¼ŒAttentionçŸ©é˜µçš„å¤§éƒ¨åˆ†å…ƒç´ éƒ½æ˜¯0ï¼Œå› æ­¤ç†è®ºä¸Šå®ƒä¹Ÿèƒ½èŠ‚çœæ˜¾å­˜å ç”¨é‡å’Œè®¡ç®—é‡ã€‚åç»­ç±»ä¼¼å·¥ä½œè¿˜æœ‰<a href="https://papers.cool/arxiv/1912.11637">ã€ŠExplicit Sparse Transformer: Concentrated Attention Through Explicit Selectionã€‹</a>ã€<a href="https://papers.cool/arxiv/2004.05150">ã€ŠLongformer: The Long-Document Transformerã€‹</a>ç­‰ã€‚</p>
<p>ä½†æ˜¯å¾ˆæ˜æ˜¾ï¼Œè¿™ç§æ€è·¯æœ‰ä¸¤ä¸ªä¸è¶³ä¹‹å¤„ï¼š</p>
<blockquote>
<p>1ã€å¦‚ä½•é€‰æ‹©è¦ä¿ç•™çš„æ³¨æ„åŠ›åŒºåŸŸï¼Œè¿™æ˜¯äººå·¥ä¸»è§‚å†³å®šçš„ï¼Œå¸¦æœ‰å¾ˆå¤§çš„ä¸æ™ºèƒ½æ€§ï¼›</p>
<p>2ã€å®ƒéœ€è¦ä»ç¼–ç¨‹ä¸Šè¿›è¡Œç‰¹å®šçš„è®¾è®¡ä¼˜åŒ–ï¼Œæ‰èƒ½å¾—åˆ°ä¸€ä¸ªé«˜æ•ˆçš„å®ç°ï¼Œæ‰€ä»¥å®ƒä¸å®¹æ˜“æ¨å¹¿ã€‚</p>
</blockquote>
<h3 id="reformer">Reformer</h3>
<p><a href="https://papers.cool/arxiv/2001.04451">Reformer</a>ä¹Ÿæ˜¯æœ‰ä»£è¡¨æ€§çš„æ”¹è¿›å·¥ä½œï¼Œå®ƒå°†Attentionçš„å¤æ‚åº¦é™åˆ°äº†$\mathcal{O}(n\log n)$ã€‚æŸç§æ„ä¹‰ä¸Šæ¥è¯´ï¼ŒReformerä¹Ÿæ˜¯ç¨€ç–Attentionçš„ä¸€ç§ï¼Œåªä¸è¿‡å®ƒçš„ç¨€ç–Patternä¸æ˜¯äº‹å…ˆæŒ‡å®šçš„ï¼Œè€Œæ˜¯é€šè¿‡LSHï¼ˆLocality Sensitive Hashingï¼‰æŠ€æœ¯ï¼ˆè¿‘ä¼¼åœ°ï¼‰å¿«é€Ÿåœ°æ‰¾åˆ°æœ€å¤§çš„è‹¥å¹²ä¸ªAttentionå€¼ï¼Œç„¶ååªå»è®¡ç®—é‚£è‹¥å¹²ä¸ªå€¼ã€‚æ­¤å¤–ï¼ŒReformeré€šè¿‡æ„é€ å¯é€†å½¢å¼çš„FFNï¼ˆFeedforward Networkï¼‰æ›¿æ¢æ‰åŸæ¥çš„FFNï¼Œç„¶åé‡æ–°è®¾è®¡åå‘ä¼ æ’­è¿‡ç¨‹ï¼Œä»è€Œé™ä½äº†æ˜¾å­˜å ç”¨é‡ã€‚</p>
<p>æ‰€ä»¥ï¼Œç›¸æ¯”å‰è¿°ç¨€ç–Attentionï¼ŒReformerè§£å†³äº†å®ƒçš„ç¬¬ä¸€ä¸ªç¼ºç‚¹ï¼Œä½†æ˜¯ä¾ç„¶æœ‰ç¬¬äºŒä¸ªç¼ºç‚¹ï¼šå®ç°èµ·æ¥å¤æ‚åº¦é«˜ã€‚è¦å®ç°LSHå½¢å¼çš„Attentionæ¯”æ ‡å‡†çš„Attentionå¤æ‚å¤šäº†ï¼Œå¯¹å¯é€†ç½‘ç»œé‡å†™åå‘ä¼ æ’­è¿‡ç¨‹å¯¹æ™®é€šè¯»è€…æ¥è¯´æ›´æ˜¯é¥ä¸å¯åŠï½</p>
<h3 id="linformer">Linformer</h3>
<p>è·Ÿæœ¬æ–‡æ‰€ä»‹ç»çš„Linear Attentionå¾ˆç›¸ä¼¼çš„ä¸€ä¸ªå·¥ä½œæ˜¯Facebookæœ€è¿‘æ”¾å‡ºæ¥çš„<a href="https://papers.cool/arxiv/2006.04768">Linformer</a>ï¼Œå®ƒä¾ç„¶ä¿ç•™åŸå§‹çš„Scaled-Dot Attentionå½¢å¼ï¼Œä½†åœ¨è¿›è¡ŒAttentionä¹‹å‰ï¼Œç”¨ä¸¤ä¸ª$m\times n$çš„çŸ©é˜µ$\boldsymbol{E},\boldsymbol{F}$åˆ†åˆ«å¯¹$\boldsymbol{K},\boldsymbol{V}$è¿›è¡ŒæŠ•å½±ï¼Œå³å˜ä¸º<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax\left(\boldsymbol{Q}(\boldsymbol{E}\boldsymbol{K})^{\top}\right)\boldsymbol{F}\boldsymbol{V}\end{equation}<br />
è¿™æ ·ä¸€æ¥ï¼Œ$\boldsymbol{Q}(\boldsymbol{E}\boldsymbol{K})^{\top}$å°±åªæ˜¯ä¸€ä¸ª$n\times m$çš„çŸ©é˜µï¼Œè€Œä½œè€…å£°ç§°å¯¹äºå“ªæ€•å¯¹äºå¾ˆå¤§çš„åºåˆ—é•¿åº¦$n$ï¼Œ$m$ä¹Ÿå¯ä»¥ä¿æŒä¸ºä¸€ä¸ªé€‚ä¸­çš„å¸¸æ•°ï¼Œä»è€Œè¿™ç§Attentionä¹Ÿæ˜¯çº¿æ€§çš„ã€‚è·ŸLinformerç±»ä¼¼çš„æ€è·¯è¿˜å‡ºç°åœ¨æ›´æ—©ä¸€äº›çš„CVè®ºæ–‡<a href="https://papers.cool/arxiv/1907.13426">ã€ŠAsymmetric Non-local Neural Networks for Semantic Segmentationã€‹</a>ä¸­ã€‚</p>
<p>ä½†æ˜¯ï¼Œç¬”è€…è®¤ä¸ºâ€œå¯¹äºè¶…é•¿åºåˆ—$m$å¯ä»¥ä¿æŒä¸å˜â€è¿™ä¸ªç»“è®ºæ˜¯å€¼å¾—è´¨ç–‘çš„ï¼Œå¯¹äºé•¿åºåˆ—åŸè®ºæ–‡åªåšäº†MLMä»»åŠ¡ï¼Œè€Œå¾ˆæ˜æ˜¾MLMå¹¶ä¸é‚£ä¹ˆéœ€è¦é•¿ç¨‹ä¾èµ–ï¼Œæ‰€ä»¥è¿™ä¸ªå®éªŒæ²¡ä»€ä¹ˆè¯´æœåŠ›ã€‚å› æ­¤ï¼ŒLinformeræ˜¯ä¸æ˜¯çœŸçš„Linearï¼Œè¿˜æœ‰å¾…å•†æ¦·ã€‚</p>
<h3 id="_6">è‡ªå›å½’ç”Ÿæˆ</h3>
<p>Linformerçš„å¦ä¸€ä¸ªç¼ºç‚¹æ˜¯$\boldsymbol{E}\boldsymbol{K},\boldsymbol{F}\boldsymbol{V}$è¿™ä¸¤ä¸ªè¿ç®—ç›´æ¥æŠŠæ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ç»™â€œç³…åˆâ€èµ·æ¥äº†ï¼Œæ‰€ä»¥å®ƒæ²¡æ³•ç®€å•åœ°æŠŠå°†æ¥ä¿¡æ¯ç»™Maskæ‰ï¼ˆCausal Maskingï¼‰ï¼Œä»è€Œæ— æ³•åšè¯­è¨€æ¨¡å‹ã€Seq2Seqç­‰è‡ªå›å½’ç”Ÿæˆä»»åŠ¡ï¼Œè¿™ä¹Ÿæ˜¯åˆšæ‰è¯´çš„åŸä½œè€…åªåšäº†MLMä»»åŠ¡çš„åŸå› ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ¬æ–‡ä»‹ç»çš„å‡ ç§Linear Attentionéƒ½èƒ½åšåˆ°è¿™ä¸€ç‚¹ã€‚ä»¥å¼$\eqref{eq:gen-att}$å’Œå¼$\eqref{eq:gen-att-2}$ä¸ºä¾‹ï¼Œå¦‚æœè¦Maskæ‰æœªæ¥ä¿¡æ¯ï¼Œé‚£ä¹ˆåªéœ€è¦æŠŠæ±‚å’Œ$\sum\limits_{j=1}^n$æ”¹ä¸º$\sum\limits_{j=1}^i$ï¼š<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})<em j="1">i = \frac{\sum\limits</em>}^i \left(\phi(\boldsymbol{q<em j="1">i)^{\top} \varphi(\boldsymbol{k}_j)\right)\boldsymbol{v}_j}{\sum\limits</em>}^i \phi(\boldsymbol{q<em j="1">i)^{\top} \varphi(\boldsymbol{k}_j)}=\frac{ \phi(\boldsymbol{q}_i)^{\top} \sum\limits</em>}^i\varphi(\boldsymbol{k<em j="1">j)\boldsymbol{v}_j^{\top}}{ \phi(\boldsymbol{q}_i)^{\top} \sum\limits</em>}^i\varphi(\boldsymbol{k<em j="1">j)}\end{equation}<br />
å®ç°ä¸Šå¼æœ‰ä¸¤ç§æ–¹å¼ï¼šç¬¬ä¸€æ–¹å¼æ˜¯è®¾$\boldsymbol{S}_i=\sum\limits</em>}^i\varphi(\boldsymbol{k<em j="1">j)\boldsymbol{v}_j^{\top}$ä»¥åŠ$\boldsymbol{z}_i=\sum\limits</em>}^i\varphi(\boldsymbol{k<em i-1="i-1">j)$ï¼Œæˆ‘ä»¬æœ‰<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})_i =\frac{ \phi(\boldsymbol{q}_i)^{\top} \boldsymbol{S}_i}{ \phi(\boldsymbol{q}_i)^{\top} \boldsymbol{z}_i},\quad \begin{aligned}&amp;\boldsymbol{S}_i=\boldsymbol{S}</em>}+\varphi(\boldsymbol{k<em i-1="i-1">i)\boldsymbol{v}_i^{\top}\\\<br />
&amp;\boldsymbol{z}_i=\boldsymbol{z}</em>_i)}+\varphi(\boldsymbol{k<br />
\end{aligned}\end{equation}<br />
è¿™è¯´æ˜è¿™ç§Attentionå¯ä»¥ä½œä¸ºä¸€ä¸ªRNNæ¨¡å‹ç”¨é€’å½’çš„æ–¹å¼å®ç°ï¼Œå®ƒçš„ç©ºé—´å¤æ‚åº¦æœ€ä½ï¼Œä½†æ˜¯è¦ä¸²æ€§è®¡ç®—ï¼Œé€‚åˆé¢„æµ‹è§£ç æ—¶ä½¿ç”¨ï¼›ç¬¬äºŒç§æ˜¯ç›´æ¥å°†$\varphi(\boldsymbol{K}),\boldsymbol{V}\in\mathbb{R}^{n\times d}$åšå¤–ç§¯ï¼Œå¾—åˆ°ä¸€ä¸ª$n\times d\times d$çš„çŸ©é˜µï¼Œç„¶åå¯¹$n$é‚£ä¸€ç»´æ‰§è¡Œ$\text{cumsum}$è¿ç®—ï¼Œè¿™æ ·å°±ä¸€æ¬¡æ€§å¾—åˆ°$\boldsymbol{S}_1,\boldsymbol{S}_2,\dots,\boldsymbol{S}_n$äº†ï¼Œå®ƒçš„é€Ÿåº¦æœ€å¿«ï¼Œä½†ç©ºé—´å ç”¨æœ€å¤§ï¼Œé€‚åˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼Œä¸è¿‡å¾ˆå¤šæ—¶å€™éƒ½æœ‰$d^2\gg n$ï¼Œä¸€èˆ¬æƒ…å†µä¸‹è®­ç»ƒæ—¶éƒ½å¾ˆéš¾æ‰¿å—è¿™ä¸ªç©ºé—´å¤æ‚åº¦ï¼Œå› æ­¤å¤šæ•°è¿˜æ˜¯ç”¨RNNå½¢å¼ã€‚</p>
<h3 id="_7">ä¸‹é‡‡æ ·æŠ€æœ¯</h3>
<p>ä»ç»“æœä¸Šæ¥çœ‹ï¼ŒLinformerçš„$\boldsymbol{E}\boldsymbol{K}, \boldsymbol{F}\boldsymbol{V}$å°±æ˜¯å°†åºåˆ—å˜çŸ­ï¼ˆä¸‹é‡‡æ ·ï¼‰äº†ï¼Œè€Œå°†åºåˆ—å˜çŸ­çš„ä¸€ä¸ªæœ€æœ´ç´ çš„æ–¹æ³•å°±æ˜¯Poolingäº†ï¼Œæ‰€ä»¥ç¬”è€…ä¹‹å‰ä¹Ÿå°è¯•è¿‡æŠŠPoolingæŠ€æœ¯å¼•å…¥åˆ°Transformerä¸­å»ã€‚è¿‘æ¥ä¹Ÿæœ‰ç±»ä¼¼çš„å·¥ä½œå‘å‡ºæ¥ï¼Œæ¯”å¦‚IBMçš„<a href="https://papers.cool/arxiv/2001.08950">ã€ŠPoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Eliminationã€‹</a>å’ŒGoogleçš„<a href="https://papers.cool/arxiv/2006.03236">ã€ŠFunnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processingã€‹</a>ã€‚é™¤äº†Poolingä¹‹å¤–ï¼Œå…¶å®è¿˜æœ‰å…¶ä»–çš„ä¸‹é‡‡æ ·æŠ€æœ¯ï¼Œæ¯”å¦‚å¯ä»¥é€šè¿‡stride &gt; 1çš„ä¸€ç»´å·ç§¯æ¥å®ç°ï¼ŒåŸºäºè¿™ä¸ªæ€è·¯ï¼Œæˆ–è®¸æˆ‘ä»¬å¯ä»¥æŠŠFFNé‡Œè¾¹çš„Position-Wiseå…¨è¿æ¥æ¢æˆstride &gt; 1çš„ä¸€ç»´å·ç§¯ï¼Ÿæ€»ä¹‹è¿™æ–¹é¢åº”è¯¥ä¹Ÿèƒ½ç©å‡ºå¾ˆå¤šèŠ±æ ·æ¥ï¼Œä¸è¿‡è·ŸLinformerä¸€æ ·ï¼Œè¿™æ ·ç³…åˆä¹‹ååšè‡ªå›å½’ç”Ÿæˆå°±å¾ˆéš¾äº†ã€‚</p>
<h2 id="_8">æ–‡ç« å°ç»“</h2>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€äº›ä»ç»“æ„ä¸Šå¯¹Attentionè¿›è¡Œä¿®æ”¹ä»è€Œé™ä½å…¶è®¡ç®—å¤æ‚åº¦çš„å·¥ä½œï¼Œå…¶ä¸­æœ€ä¸»è¦çš„ideaæ˜¯å»æ‰æ ‡å‡†Attentionä¸­çš„Softmaxï¼Œå°±å¯ä»¥ä½¿å¾—Attentionçš„å¤æ‚åº¦é€€åŒ–ä¸ºç†æƒ³çš„$\mathcal{O}(n)$çº§åˆ«ï¼ˆLinear Attentionï¼‰ã€‚ç›¸æ¯”äºå…¶ä»–ç±»ä¼¼çš„æ”¹è¿›ç»“æ„çš„å·¥ä½œï¼Œè¿™ç§ä¿®æ”¹èƒ½åœ¨æŠŠå¤æ‚åº¦é™åˆ°$\mathcal{O}(n)$çš„åŒæ—¶ï¼Œä¾ç„¶ä¿ç•™æ‰€æœ‰çš„â€œtoken-tokenâ€œçš„æ³¨æ„åŠ›ï¼ŒåŒæ—¶è¿˜èƒ½ä¿ç•™ç”¨äºåšè‡ªå›å½’ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚</p>
<p><em><strong>è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š</strong><a href="https://spaces.ac.cn/archives/7546">https://spaces.ac.cn/archives/7546</a></em></p>
<p><em><strong>æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="ã€Šç§‘å­¦ç©ºé—´FAQã€‹">ã€Šç§‘å­¦ç©ºé—´FAQã€‹</a></p>
<p><strong>å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚</strong></p>
<p><strong>å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼</strong></p>
<p>æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>å¾®ä¿¡æ‰“èµ</p>
<p><img alt="ç§‘å­¦ç©ºé—´" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>æ”¯ä»˜å®æ‰“èµ</p>
<p>å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>ç‚¹å‡»è¿™é‡Œ</strong></a>æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚</p>
<p><strong>å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š</strong></p>
<p>è‹å‰‘æ—. (Jul. 04, 2020). ã€Šçº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿ ã€‹[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/7546">https://spaces.ac.cn/archives/7546</a></p>
<p>@online{kexuefm-7546,<br />
title={çº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿ},<br />
author={è‹å‰‘æ—},<br />
year={2020},<br />
month={Jul},<br />
url={\url{https://spaces.ac.cn/archives/7546}},<br />
} </p>
<hr />
<h2 id="_9">å…¬å¼æ¨å¯¼ä¸æ³¨é‡Š</h2>
<p>TODO: æ·»åŠ è¯¦ç»†çš„æ•°å­¦å…¬å¼æ¨å¯¼å’Œæ³¨é‡Š</p>
        </div>
    </div>
</body>
</html>