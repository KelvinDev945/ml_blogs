#!/usr/bin/env python3
"""
è‡ªåŠ¨ä¸ºæ–°æ–‡ç« åˆ†ç±»
æ ¹æ®æ–‡ä»¶åå’Œæ–‡ç« å†…å®¹ä¸­çš„å…³é”®è¯ï¼Œå°†æœªåˆ†ç±»çš„æ–‡ç« å½’ç±»åˆ°åˆé€‚çš„ä¸»é¢˜
"""

import json
from pathlib import Path
import re

# ä¸»é¢˜å…³é”®è¯ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
TOPIC_KEYWORDS = {
    "æ‰©æ•£æ¨¡å‹": {
        "keywords": ["æ‰©æ•£æ¨¡å‹", "ddpm", "ddim", "æ‰©æ•£", "diffusion", "å¾—åˆ†åŒ¹é…", "score matching",
                    "ode", "sde", "reflow", "ä¸€è‡´æ€§æ¨¡å‹", "consistency", "fokker-planck"],
        "priority": 10
    },
    "Transformer": {
        "keywords": ["transformer", "attention", "æ³¨æ„åŠ›", "rope", "ä½ç½®ç¼–ç ", "positional",
                    "å¤–æ¨", "mla", "gqa", "mha", "mqa", "flash", "softmax", "gau"],
        "priority": 9
    },
    "ä¼˜åŒ–ç†è®º": {
        "keywords": ["ä¼˜åŒ–å™¨", "optimizer", "adam", "sgd", "muon", "lion", "tiger", "adafactor",
                    "å­¦ä¹ ç‡", "learning rate", "batch size", "æ¢¯åº¦ä¸‹é™", "momentum", "amos"],
        "priority": 8
    },
    "çŸ©é˜µç†è®º": {
        "keywords": ["çŸ©é˜µ", "matrix", "svd", "ä½ç§©", "low-rank", "msign", "ç‰¹å¾å€¼", "eigenvalue",
                    "å¥‡å¼‚å€¼", "singular", "ä¼ªé€†", "pseudoinverse", "æ­£äº¤", "orthogonal", "monarch",
                    "newton-schulz", "hippo"],
        "priority": 7
    },
    "éšæœºçŸ©é˜µ/æ¦‚ç‡": {
        "keywords": ["æ¦‚ç‡", "probability", "éšæœº", "random", "æ­£æ€", "gaussian", "åˆ†å¸ƒ", "distribution",
                    "æœŸæœ›", "expectation", "æ–¹å·®", "variance", "è´å¶æ–¯", "bayes", "ç»Ÿè®¡"],
        "priority": 6
    },
    "æŸå¤±å‡½æ•°": {
        "keywords": ["æŸå¤±", "loss", "äº¤å‰ç†µ", "cross entropy", "kl", "emo", "å¯¹æ¯”å­¦ä¹ ",
                    "contrastive", "cosent", "circle loss"],
        "priority": 5
    },
    "æ¢¯åº¦åˆ†æ": {
        "keywords": ["æ¢¯åº¦", "gradient", "åå‘ä¼ æ’­", "backprop", "æ¢¯åº¦è£å‰ª", "gradient clip",
                    "æ¢¯åº¦æƒ©ç½š", "gradient penalty", "lora", "å¾®è°ƒ", "fine-tuning"],
        "priority": 4
    },
    "RNN/SSM": {
        "keywords": ["rnn", "lstm", "gru", "ssm", "state space", "s4", "çº¿æ€§æ³¨æ„åŠ›",
                    "linear attention", "hippo", "mamba"],
        "priority": 3
    },
    "VQ/é‡åŒ–": {
        "keywords": ["vq", "vector quantization", "é‡åŒ–", "quantization", "ç¼–ç ", "codebook",
                    "fsq", "ç¦»æ•£", "discrete"],
        "priority": 2
    },
    "è¯­è¨€æ¨¡å‹": {
        "keywords": ["è¯­è¨€æ¨¡å‹", "language model", "llm", "bert", "gpt", "t5", "pegasus",
                    "é¢„è®­ç»ƒ", "pretrain", "tokenizer", "embedding", "decoder"],
        "priority": 1
    },
}

def normalize_text(text):
    """è§„èŒƒåŒ–æ–‡æœ¬ï¼šè½¬å°å†™ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦"""
    text = text.lower()
    text = re.sub(r'[_\-]', ' ', text)
    return text

def classify_article(slug, content_sample=""):
    """
    æ ¹æ®slugå’Œå†…å®¹æ ·æœ¬å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»
    è¿”å›: (ä¸»é¢˜, ç½®ä¿¡åº¦åˆ†æ•°)
    """
    text = normalize_text(slug + " " + content_sample)

    # ç»Ÿè®¡æ¯ä¸ªä¸»é¢˜çš„åŒ¹é…åˆ†æ•°
    topic_scores = {}

    for topic, config in TOPIC_KEYWORDS.items():
        score = 0
        matched_keywords = []

        for keyword in config["keywords"]:
            keyword_normalized = normalize_text(keyword)
            if keyword_normalized in text:
                # å…³é”®è¯åŒ¹é…å¾—åˆ†
                keyword_score = len(keyword.split())  # å¤šè¯å…³é”®è¯å¾—åˆ†æ›´é«˜
                score += keyword_score
                matched_keywords.append(keyword)

        # ä¼˜å…ˆçº§åŠ æƒ
        if score > 0:
            score *= config["priority"]
            topic_scores[topic] = {
                "score": score,
                "matched_keywords": matched_keywords
            }

    # è¿”å›å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜
    if topic_scores:
        best_topic = max(topic_scores.items(), key=lambda x: x[1]["score"])
        return best_topic[0], best_topic[1]

    return "å…¶ä»–", {"score": 0, "matched_keywords": []}

def read_article_preview(filepath, lines=50):
    """è¯»å–æ–‡ç« å‰å‡ è¡Œä½œä¸ºé¢„è§ˆ"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            preview = ''.join(f.readlines()[:lines])
        return preview
    except Exception as e:
        print(f"âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶ {filepath}: {e}")
        return ""

def auto_classify_new_articles():
    """è‡ªåŠ¨åˆ†ç±»æ‰€æœ‰æœªåˆ†ç±»çš„æ–‡ç« """

    # è¯»å–ç°æœ‰åˆ†ç±»
    with open('topic_classification.json', 'r', encoding='utf-8') as f:
        classified = json.load(f)

    # è·å–å·²åˆ†ç±»æ–‡ç« çš„slug
    classified_slugs = set()
    max_number = 0
    for topic_articles in classified.values():
        for article in topic_articles:
            classified_slugs.add(article['slug'])
            max_number = max(max_number, article.get('number', 0))

    # è·å–æ‰€æœ‰æ–‡ç« 
    all_files = list(Path('blogs_raw').glob('*.md'))

    # æ‰¾å‡ºæœªåˆ†ç±»çš„æ–‡ç« 
    unclassified_files = [f for f in all_files if f.stem not in classified_slugs]

    print(f"å‘ç° {len(unclassified_files)} ç¯‡æœªåˆ†ç±»æ–‡ç« ")
    print("=" * 100)

    # å‡†å¤‡æ–°åˆ†ç±»
    new_classifications = {topic: [] for topic in TOPIC_KEYWORDS.keys()}
    new_classifications["å…¶ä»–"] = []

    # åˆ†ç±»æ¯ç¯‡æ–‡ç« 
    for i, filepath in enumerate(unclassified_files, 1):
        slug = filepath.stem

        # è¯»å–æ–‡ç« é¢„è§ˆä»¥æé«˜åˆ†ç±»å‡†ç¡®æ€§
        preview = read_article_preview(filepath)

        # åˆ†ç±»
        topic, info = classify_article(slug, preview)

        # åˆ›å»ºæ–‡ç« æ¡ç›®
        article_entry = {
            "title": slug.replace('-', ' ').replace('_', ' '),  # ç®€å•å¤„ç†æ ‡é¢˜
            "slug": slug,
            "number": max_number + i,  # åˆ†é…æ–°çš„ç¼–å·
            "matched_keywords": info["matched_keywords"][:5],  # ä¿ç•™å‰5ä¸ªåŒ¹é…çš„å…³é”®è¯
            "confidence": info["score"]
        }

        new_classifications[topic].append(article_entry)

        # æ‰“å°è¿›åº¦
        if i % 10 == 0:
            print(f"è¿›åº¦: {i}/{len(unclassified_files)} ({i*100//len(unclassified_files)}%)")

    # æ‰“å°åˆ†ç±»ç»“æœæ‘˜è¦
    print("\n" + "=" * 100)
    print("åˆ†ç±»ç»“æœæ‘˜è¦:")
    print("=" * 100)

    for topic, articles in sorted(new_classifications.items(), key=lambda x: -len(x[1])):
        if articles:
            print(f"\nğŸ“š {topic}: {len(articles)} ç¯‡")
            # æ˜¾ç¤ºå‰3ç¯‡ä½œä¸ºç¤ºä¾‹
            for article in articles[:3]:
                keywords_str = ", ".join(article["matched_keywords"][:3])
                print(f"  â€¢ {article['slug'][:60]}... (å…³é”®è¯: {keywords_str})")
            if len(articles) > 3:
                print(f"  ... è¿˜æœ‰ {len(articles) - 3} ç¯‡")

    # ä¿å­˜è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    with open('new_articles_classification_report.json', 'w', encoding='utf-8') as f:
        json.dump(new_classifications, f, ensure_ascii=False, indent=2)

    print("\n" + "=" * 100)
    print("âœ… è¯¦ç»†åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°: new_articles_classification_report.json")

    # è¯¢é—®æ˜¯å¦åˆå¹¶åˆ°ä¸»åˆ†ç±»æ–‡ä»¶
    print("\næ˜¯å¦è¦å°†æ–°åˆ†ç±»åˆå¹¶åˆ° topic_classification.json?")
    print("è¿™å°†è‡ªåŠ¨æ‰§è¡Œï¼Œæˆ–è€…ä½ å¯ä»¥æ‰‹åŠ¨å®¡æ ¸ new_articles_classification_report.json åå†åˆå¹¶ã€‚")

    return new_classifications, classified

def merge_classifications(new_classifications, existing_classifications):
    """åˆå¹¶æ–°åˆ†ç±»åˆ°ç°æœ‰åˆ†ç±»"""

    merged = dict(existing_classifications)

    for topic, articles in new_classifications.items():
        if topic not in merged:
            merged[topic] = []

        # ç§»é™¤ä¸´æ—¶å­—æ®µ
        clean_articles = []
        for article in articles:
            clean_article = {
                "title": article["title"],
                "slug": article["slug"],
                "number": article["number"]
            }
            clean_articles.append(clean_article)

        merged[topic].extend(clean_articles)

    # æŒ‰numberæ’åºæ¯ä¸ªä¸»é¢˜çš„æ–‡ç« 
    for topic in merged:
        merged[topic] = sorted(merged[topic], key=lambda x: x.get('number', 0))

    return merged

if __name__ == "__main__":
    new_classifications, existing = auto_classify_new_articles()

    print("\n" + "=" * 100)
    response = input("æ˜¯å¦åˆå¹¶åˆ° topic_classification.json? (yes/no): ").strip().lower()

    if response in ['yes', 'y']:
        merged = merge_classifications(new_classifications, existing)

        # å¤‡ä»½åŸæ–‡ä»¶
        import shutil
        shutil.copy('topic_classification.json', 'topic_classification.json.backup')
        print("âœ… å·²å¤‡ä»½åŸæ–‡ä»¶åˆ°: topic_classification.json.backup")

        # ä¿å­˜åˆå¹¶åçš„åˆ†ç±»
        with open('topic_classification.json', 'w', encoding='utf-8') as f:
            json.dump(merged, f, ensure_ascii=False, indent=2)

        print("âœ… æ–°åˆ†ç±»å·²åˆå¹¶åˆ° topic_classification.json")

        # æ‰“å°ç»Ÿè®¡
        print("\n" + "=" * 100)
        print("æ›´æ–°åçš„ç»Ÿè®¡:")
        print("=" * 100)
        total = 0
        for topic, articles in merged.items():
            count = len(articles)
            total += count
            print(f"{topic}: {count} ç¯‡")
        print(f"\næ€»è®¡: {total} ç¯‡æ–‡ç« ")
    else:
        print("âŒ æœªåˆå¹¶ã€‚è¯·æ‰‹åŠ¨å®¡æ ¸ new_articles_classification_report.json")
