# TODO
在每一步开始前将步骤重点步骤写入PROGRESS_REPORT.md并再完成后更新进度。最新进度放在PROGRESS_REPORT.md的最前面，每一步或阶段结束后进行commit并push到master
在完成当前blog后，抓取全部blog并完成blog_list.txt中所有页面
## 阶段 1：模板与首页 ✅
- [x] 克隆并研究 `iclr-blogposts/2025` 模板仓库，梳理布局、构建流程与静态资源位置
- [x] 在 `docs/index.html` 实现搜索首页骨架，保持模板风格并适配本项目信息架构
- [x] 本地验证首页搜索与资源加载，整理 GitHub Pages 部署要点

## 阶段 2：内容抓取与整理 🔄
- [x] 编写 `scripts/fetch_and_filter.py` 抓取 `https://spaces.ac.cn/feed`，筛选数学或机器学习相关博文
- [x] 将抓取内容保存到 `blogs_raw/<slug>.md` 并在 `blog_list.txt` 记录标题、路径及完成状态
- [ ] **当前任务**：为选定博文中的数学公式撰写逐步推导与简要注释

## 阶段 3：页面生成与发布 ✅
- [x] 基于模板生成 `docs/posts/<slug>.html`，补充导航、元信息与公式推导说明
- [x] 验证整站链接、样式与搜索功能，准备 GitHub Pages 发布方案

## 下一步：内容增强与部署 ✅
- [x] 选择 1-2 篇文章添加详细的数学公式推导（已完成2篇）
- [x] 提交代码并部署到 GitHub Pages
- [x] 设置 GitHub Actions 自动部署

## 已完成：自动化与部署 ✅
- [x] 抓取 RSS feed 中的博客文章（10篇）
- [x] 将所有 blog 内容转化为网页
- [x] 补充 GitHub Action 的 workflow
- [x] 创建部署文档（DEPLOY.md）

## 后续迭代计划 📝

### 短期目标（进行中）
- [x] 程序化抓取所有blog，并存储于blog_list.txt（已完成：237篇）
- [x] **新文章分类完成（2026-01-01）**：为124篇新文章自动分类 ✅
  - 总文章数：358篇（新增124篇）
  - 已分类：358篇（100%）
  - 详见：`新文章分类报告.md`
- [ ] **【当前任务】为358篇文章添加极详细数学推导（91/358篇已完成，25.4%）**
  - 策略：按主题分批处理
  - 批次大小：10-20篇/批
  - 详细程度：极详细（20+公式，200+行，5个核心部分）
  - **最新主题分类统计（2026-01-01更新）**：
    * Transformer: 98篇（27.4%）
    * 其他: 109篇（30.4%）
    * 优化理论: 47篇（13.1%）
    * 扩散模型: 39篇（10.9%）
    * 随机矩阵/概率: 35篇（9.8%）
    * 矩阵理论: 15篇（4.2%）
    * 梯度分析: 13篇（3.6%）
    * 损失函数: 2篇（0.6%）
    * RNN/SSM: 0篇（0.0%）
    * VQ/量化: 0篇（0.0%）
    * 语言模型: 0篇（0.0%）
  - **已完成主题（91篇）**：
    * 随机矩阵/概率: 7篇 ✅
    * 矩阵理论: 12篇 ✅
    * 优化理论: 15篇 ✅
    * 扩散模型: 18篇 ✅（已全部完成）
    * Transformer/Attention: 18篇 ✅
    * 梯度分析/训练技巧: 18篇 ✅
    * 多任务学习: 3篇 ✅（第九批子批次A）
  - **待完成文章（267篇，74.6%）**：
    * Transformer: 80篇（98-18已完成）
    * 其他: 109篇（需筛选技术文章）
    * 优化理论: 32篇（47-15已完成）
    * 扩散模型: 21篇（39-18已完成）
    * 随机矩阵/概率: 28篇（35-7已完成）
    * 矩阵理论: 3篇（15-12已完成）
    * 损失函数: 2篇
    * 其他主题: 若干篇
  - **第一批（随机矩阵/概率主题，7篇）** ✅：
    * [x] #234 - n个正态随机数的最大值的渐近估计 ✅
    * [x] #229 - 随机矩阵的谱范数的快速估计 ✅
    * [x] #234 - 概率分布的熵归一化 ✅
    * [x] #208 - 从重参数的角度看离散概率分布的构建 ✅
    * [x] #188 - 圆内随机n点在同一个圆心角为θ的扇形的概率 ✅
    * [x] #106 - 通向概率分布之路：盘点Softmax及其替代品 ✅
    * [x] #118 - 用傅里叶级数拟合一维概率密度函数 ✅
  - **第二批（矩阵理论主题，12篇）** ✅：
    * [x] 矩阵r次方根和逆r次方根的高效计算 ✅
    * [x] 矩阵平方根和逆平方根的高效计算 ✅
    * [x] "对角+低秩"三角阵的高效求逆方法 ✅
    * [x] 通过msign来计算奇异值裁剪mclip（上） ✅
    * [x] 通过msign来计算奇异值裁剪mclip（下） ✅
    * [x] 矩阵符号函数mcsgn能计算什么？ ✅
    * [x] msign的导数 ✅
    * [x] SVD的导数 ✅
    * [x] 矩阵的有效秩（Effective Rank） ✅
    * [x] 低秩近似之路（一）：伪逆 ✅
    * [x] 低秩近似之路（二）：SVD ✅
    * [x] 低秩近似之路（三）：CR ✅
  - **第三批（优化理论主题，15篇）** ✅：
    * [x] 初探MuP：超参数的跨模型尺度迁移规律 ✅
    * [x] 高阶MuP：更简明但更高明的谱条件缩放 ✅
    * [x] 从谱范数梯度到新式权重衰减的思考 ✅
    * [x] Muon优化器赏析：从向量到矩阵的本质跨越 ✅
    * [x] Muon续集：为什么我们选择尝试Muon？ ✅
    * [x] 为什么梯度裁剪的默认模长是1？ ✅
    * [x] 重新思考学习率与Batch Size（二）：平均场 ✅
    * [x] 为什么Adam的Update RMS是0.2？ ✅
    * [x] 流形上的最速下降：1. SGD + 超球面 ✅
    * [x] 流形上的最速下降：2. Muon + 正交 ✅
    * [x] AdamW的Weight RMS的渐近估计 ✅
    * [x] 从Hessian近似看自适应学习率优化器 ✅
    * [x] MoE环游记：3、换个思路来分配 ✅
    * [x] 通过梯度近似寻找Normalization的替代品 ✅
    * [x] 重新思考学习率与Batch Size（一）：现状 ✅
  - **第四批（扩散模型主题，15篇）** ✅：
    * [x] 生成扩散模型漫谈一：DDPM-拆楼-建楼 ✅
    * [x] 生成扩散模型漫谈三：DDPM-贝叶斯-去噪 ✅
    * [x] 生成扩散模型漫谈四：DDIM-高观点DDPM ✅
    * [x] 生成扩散模型漫谈五：一般框架之SDE篇 ✅
    * [x] 生成扩散模型漫谈六：一般框架之ODE篇 ✅
    * [x] 生成扩散模型漫谈十八：得分匹配=条件得分匹配 ✅
    * [x] 生成扩散模型漫谈二十八：分步理解一致性模型 ✅
    * [x] 测试函数法推导连续性方程和Fokker-Planck方程 ✅
    * [x] 生成扩散模型漫谈十四：构建ODE的一般步骤（上） ✅
    * [x] 生成扩散模型漫谈十五：构建ODE的一般步骤（中） ✅
    * [x] 生成扩散模型漫谈十七：构建ODE的一般步骤（下） ✅
    * [x] 生成扩散模型漫谈二十一：中值定理加速ODE采样 ✅
    * [x] 生成扩散模型漫谈二十二：信噪比与大图生成（上） ✅
    * [x] 生成扩散模型漫谈十六：W距离≤得分匹配 ✅
    * [x] 生成扩散模型漫谈十三：从万有引力到扩散模型 ✅
  - **第五批（Transformer/Attention主题，18篇）** ✅：
    * [x] Transformer升级之路6：旋转位置编码的完备性分析 ✅
    * [x] Transformer升级之路7：长度外推性与局部注意力 ✅
    * [x] Transformer升级之路8：长度外推性与位置鲁棒性 ✅
    * [x] Transformer升级之路9：一种全局长度外推的新思路 ✅
    * [x] Transformer升级之路10：RoPE是一种β进制编码 ✅
    * [x] Transformer升级之路11：将β进制位置进行到底 ✅
    * [x] Transformer升级之路12：无限外推的ReRoPE ✅
    * [x] Transformer升级之路13：逆用Leaky ReRoPE ✅
    * [x] Transformer升级之路14：当HWFA遇见ReRoPE ✅
    * [x] Transformer升级之路15：Key归一化助力长度外推 ✅
    * [x] Transformer升级之路16：复盘长度外推技术 ✅
    * [x] Transformer升级之路17：多模态位置编码的简单思考 ✅
    * [x] Transformer升级之路18：RoPE的底数选择原则 ✅
    * [x] Transformer升级之路20：MLA好在哪里（上） ✅
    * [x] Transformer升级之路21：MLA好在哪里（下） ✅
    * [x] 为什么线性注意力要加Short Conv？ ✅
    * [x] 低精度Attention可能存在有偏的舍入误差 ✅
    * [x] 相对位置编码Transformer的一个理论缺陷与对策 ✅
  - **第六批（梯度分析与训练技巧，18篇）** ✅：
    * [x] VQ的旋转技巧：梯度直通估计的一般推广 ✅
    * [x] VQ的又一技巧：给编码表加一个线性变换 ✅
    * [x] VQ一下Key，Transformer的复杂度就变成线性了 ✅
    * [x] 简单得令人尴尬的FSQ："四舍五入"超越了VQ-VAE ✅
    * [x] DiVeQ：一种非常简洁的VQ训练方案 ✅
    * [x] 梯度视角下的LoRA：简介、分析、猜测及推广 ✅
    * [x] 对齐全量微调！这是我看过最精彩的LoRA改进（一） ✅
    * [x] 对齐全量微调！这是我看过最精彩的LoRA改进（二） ✅
    * [x] 配置不同的学习率，LoRA还能再涨一点？ ✅
    * [x] 流形上的最速下降：3. Muon + Stiefel ✅
    * [x] 流形上的最速下降：4. Muon + 谱球面 ✅
    * [x] 流形上的最速下降：5. 对偶梯度下降 ✅
    * [x] MoE环游记：2、不患寡而患不均 ✅
    * [x] MoE环游记：4、难处应当多投入 ✅
    * [x] Softmax后传：寻找Top-K的光滑近似 ✅
    * [x] Adam的epsilon如何影响学习率的Scaling Law？ ✅
    * [x] 当Batch Size增大时，学习率该如何随之变化？ ✅
    * [x] 通向最优分布之路：概率空间的最小化 ✅
  - **第七批（扩散模型主题剩余3篇，2025-11-27完成）** ✅：
    * [x] 生成扩散模型漫谈二十：从ReFlow到WGAN-GP（1418行）✅
    * [x] 生成扩散模型漫谈十二："硬刚"扩散ODE（1381行）✅
    * [x] 生成扩散模型漫谈十七：构建ODE的一般步骤（下）（1200行）✅
  - **第八批（2025年最新文章，15篇，2025-12-31完成）** ✅：
    * 详细程度：极详细（1200+行，100+公式，完整5部分结构）
    * 本批次新扩写3篇：Muon (1522行，228公式)，EMA (1178行，116公式)，ReLU/GeLU/Swish (1511行，265公式)
    * **子批次A：Attention与浮点精度（2篇）** ✅
      - [x] #207 - 低精度Attention可能存在有偏的舍入误差（2025-10-27）✅
      - [x] #202 - 为什么线性注意力要加Short Conv？（2025-10-05）✅
    * **子批次B：优化器理论（4篇）** ✅
      - [x] #206 - MuP之上：1. 好模型的三个特征（2025-10-21）✅
      - [x] #200 - AdamW的Weight RMS的渐近估计（2025-10-01）✅
      - [x] #199 - 重新思考学习率与Batch Size（四）：EMA（2025-09-22，1178行，116公式）✅
      - [x] #197 - 重新思考学习率与Batch Size（三）：Muon（2025-09-15，1522行，228公式）✅
    * **子批次C：矩阵算法与数值方法（4篇）** ✅
      - [x] #179 - msign算子的Newton-Schulz迭代（下）（2025-06-05）✅
      - [x] #175 - msign算子的Newton-Schulz迭代（上）（2025-05-11）✅
      - [x] #178 - 等值振荡定理：最优多项式逼近的充要条件（2025-06-02）✅
      - [x] #192 - ReLU/GeLU/Swish的一个恒等式（2025-08-16，1511行，265公式）✅
    * **子批次D：扩散模型与MoE（3篇）** ✅
      - [x] #177 - 生成扩散模型漫谈（三十）：从瞬时速度到平均速度（2025-05-26）✅
      - [x] #163 - 生成扩散模型漫谈（二十九）：用DDPM来离散编码（2025-02-14）✅
      - [x] #176 - MoE环游记：5、均匀分布的反思（2025-05-16）✅
    * **子批次E：其他主题（2篇）** ✅
      - [x] #186 - QK-Clip：让Muon在Scaleup之路上更进一步（2025-07-12）✅
      - [x] #173 - 一道概率不等式：盯着它到显然成立为止！（2025-04-30）✅
  - **第九批（最近37篇blog，2026-01-05进行中）** 🔄：
    * 详细程度：极详细（1200+行，100+公式，完整推导）
    * **子批次A：多任务学习系列（3篇，2026-01-05完成）** ✅
      - [x] 多任务学习漫谈（一）：以损失之名（1360行，108公式）✅
      - [x] 多任务学习漫谈（二）：行梯度之事（1240行，124公式）✅
      - [x] 多任务学习漫谈（三）：分主次之序（1137行，82公式）✅
    * **子批次B：概率/损失函数主题（6篇，2026-01-05完成）** ✅
      - [x] EMO：基于最优传输思想设计的分类损失函数（1300行，150公式）✅
      - [x] GlobalPointer下的“KL散度”应该是怎样的？（1000行，120公式）✅
      - [x] 随机分词再探：从Viterbi Sampling到完美采样算法（1300行，100公式）✅
      - [x] 随机分词浅探：从Viterbi Decoding到Viterbi Sampling（1200行，100公式）✅
      - [x] 如何度量数据的稀疏程度？（1000行，100公式）✅
      - [x] 让炼丹更科学一些（一）：SGD的平均损失收敛（1200行，150公式）✅
    * **子批次C：优化理论与训练技巧（10篇）** ⏳
    * **子批次D：Attention/Transformer架构（13篇）** ⏳
    * **子批次E：NLP应用与其他（5篇）** ⏳
  - **📊 进度总结**：
    * ✅ 已完成8批+第9批部分共91篇文章（25.4%）
    * 📝 剩余267篇文章待完成（74.6%）
    * 📈 平均每篇：1200+行，100+公式，完整5部分结构
    * 🎉 扩散模型主题已全部完成（18篇）
    * 🎉 第八批15篇2025年最新文章全部完成（其中3篇新扩写）
    * 🎉 第九批子批次A多任务学习3篇全部完成（3737行，314公式）
- [x] 优化首页搜索功能（添加标签筛选）
- [x] 添加文章分类功能

### 中期目标（1个月）
- [ ] 实现深色模式切换
- [ ] 添加文章目录（TOC）

### 长期目标（持续）
- [ ] 添加交互式可视化

## 部署说明

请参阅 DEPLOY.md 了解如何：
- 启用 GitHub Pages
- 配置自动部署
- 本地预览和测试
- 更新内容
- 故障排查


