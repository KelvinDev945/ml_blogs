[
  {
    "title": "n个正态随机数的最大值的渐近估计",
    "slug": "n个正态随机数的最大值的渐近估计",
    "path": "blogs_raw/n个正态随机数的最大值的渐近估计.md",
    "status": "completed",
    "date": "2025-11-06",
    "tags": [
      "数学"
    ],
    "description": "设$z_1,z_2,\\cdots,z_n$是$n$个从标准正态分布中独立重复采样出来的随机数，由此我们可以构造出很多衍生随机变量，比如$z_1+z_2+\\cdots+z_n$，它依旧服从正态分布，又比如$z_1^2+z_2^2+\\cdots+z_n^2$，它服从卡方分布。这篇文章我们来关心它的最大值$z_{\\max} = \\max\\\\{z_1,z_2,\\cdots,z_n\\\\}$的分布信息，尤其是..."
  },
  {
    "title": "流形上的最速下降：5. 对偶梯度下降",
    "slug": "流形上的最速下降5-对偶梯度下降",
    "path": "blogs_raw/流形上的最速下降5-对偶梯度下降.md",
    "status": "completed",
    "date": "2025-11-03",
    "tags": [
      "机器学习"
    ],
    "description": "前四篇文章我们求解了几个具体的给参数加等式约束的最速下降问题，其中第三、四篇的问题没法找到解析解，所以笔者提出了相应的不动点迭代法。其中的其中，第三篇文章[《流形上的最速下降：3. Muon + Stiefel》](https://kexue.fm/archives/11221)所研究的“Stiefel流形上的Muon”，问题提出自Jeremy Bernstein的[《Orthogonal man..."
  },
  {
    "title": "低精度Attention可能存在有偏的舍入误差",
    "slug": "低精度attention可能存在有偏的舍入误差",
    "path": "blogs_raw/低精度attention可能存在有偏的舍入误差.md",
    "status": "completed",
    "date": "2025-10-27",
    "tags": [
      "机器学习"
    ],
    "description": "前段时间笔者在arXiv上刷到了论文[《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》](https://arxiv.org/abs/2510.04212)，里面描述的实验现象跟我们在训练[Kimi K2](https://arxiv.org/abs/2507.20534)时出现的一些现象很..."
  },
  {
    "title": "MuP之上：1. 好模型的三个特征",
    "slug": "mup之上1-好模型的三个特征",
    "path": "blogs_raw/mup之上1-好模型的三个特征.md",
    "status": "completed",
    "date": "2025-10-21",
    "tags": [
      "优化"
    ],
    "description": "不知道大家有没有发现一个有趣的细节，Muon和MuP都是“Mu”开头，但两个“Mu”的原意完全不一样，前者是“**M** oment**U** m Orthogonalized by Newton-Schulz”，后者是“**M** aximal **U** pdate Parametrization”，可它们俩之间确实有着非常深刻的联系。也就是说，Muon和MuP有着截然不同的出发点，但最终都走..."
  },
  {
    "title": "随机矩阵的谱范数的快速估计",
    "slug": "随机矩阵的谱范数的快速估计",
    "path": "blogs_raw/随机矩阵的谱范数的快速估计.md",
    "status": "completed",
    "date": "2025-10-12",
    "tags": [
      "机器学习"
    ],
    "description": "在[《高阶MuP：更简明但更高明的谱条件缩放》](https://kexue.fm/archives/10795)的“近似估计”一节中，我们曾“预支”了一个结论：“一个服从标准正态分布的$n\\times m$大小的随机矩阵，它的谱范数大致是$\\sqrt{n}+\\sqrt{m}$。”  这篇文章我们来补充讨论这个结论，给出随机矩阵谱范数的快速估计方法。  ## 随机矩阵论  设有随机矩阵$\\bold..."
  },
  {
    "title": "DiVeQ：一种非常简洁的VQ训练方案",
    "slug": "diveq一种非常简洁的vq训练方案",
    "path": "blogs_raw/diveq一种非常简洁的vq训练方案.md",
    "status": "completed",
    "date": "2025-10-08",
    "tags": [
      "机器学习"
    ],
    "description": "对于坚持离散化路线的研究人员来说，VQ（Vector Quantization）是视觉理解和生成的关键部分，担任着视觉中的“Tokenizer”的角色。它提出在2017年的论文[《Neural Discrete Representation Learning》](https://arxiv.org/abs/1711.00937)，笔者在2019年的博客[《VQ-VAE的简明介绍：量子化自编码器》]..."
  },
  {
    "title": "为什么线性注意力要加Short Conv？",
    "slug": "为什么线性注意力要加short-conv",
    "path": "blogs_raw/为什么线性注意力要加short-conv.md",
    "status": "completed",
    "date": "2025-10-05",
    "tags": [
      "机器学习"
    ],
    "description": "如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考[《线性注意力简史：从模仿、创新到反哺》](https://kexue.fm/archives/11033)）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short Conv，比如下图所示的[DeltaNet](https://arxiv.org/abs..."
  },
  {
    "title": "AdamW的Weight RMS的渐近估计",
    "slug": "adamw的weight-rms的渐近估计",
    "path": "blogs_raw/adamw的weight-rms的渐近估计.md",
    "status": "completed",
    "date": "2025-10-01",
    "tags": [
      "优化"
    ],
    "description": "在[《为什么Adam的Update RMS是0.2？》](https://kexue.fm/archives/11267)中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 [@EIFY](https://x.com/EIFY/status/1965888629814988984) 指出相同的结果已经出现在论文[《Rotational Equilibrium: How Weig..."
  },
  {
    "title": "重新思考学习率与Batch Size（四）：EMA",
    "slug": "重新思考学习率与batch-size四ema",
    "path": "blogs_raw/重新思考学习率与batch-size四ema.md",
    "status": "completed",
    "date": "2025-09-22",
    "tags": [
      "优化"
    ],
    "description": "我们在[《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在[《配置不同的学习率，LoRA还能再涨一点？》](https://kexue.fm/archives/10001)、[《初探MuP..."
  },
  {
    "title": "重新思考学习率与Batch Size（三）：Muon",
    "slug": "重新思考学习率与batch-size三muon",
    "path": "blogs_raw/重新思考学习率与batch-size三muon.md",
    "status": "completed",
    "date": "2025-09-15",
    "tags": [
      "优化"
    ],
    "description": "前两篇文章[《重新思考学习率与Batch Size（一）：现状》](https://kexue.fm/archives/11260)和[《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)中，我们主要是提出了平均场方法，用以简化学习率与Batch Size的相关计算。当时我们分析的优化器是SGD、SignSGD和SoftSign..."
  },
  {
    "title": "n个正态随机数的最大值的渐近估计",
    "slug": "n个正态随机数的最大值的渐近估计-1",
    "path": "blogs_raw/n个正态随机数的最大值的渐近估计-1.md",
    "status": "completed",
    "date": "2025-11-06",
    "tags": [
      "数学"
    ],
    "description": "设$z_1,z_2,\\cdots,z_n$是$n$个从标准正态分布中独立重复采样出来的随机数，由此我们可以构造出很多衍生随机变量，比如$z_1+z_2+\\cdots+z_n$，它依旧服从正态分布，又比如$z_1^2+z_2^2+\\cdots+z_n^2$，它服从卡方分布。这篇文章我们来关心它的最大值$z_{\\max} = \\max\\\\{z_1,z_2,\\cdots,z_n\\\\}$的分布信息，尤其是..."
  },
  {
    "title": "流形上的最速下降：5. 对偶梯度下降",
    "slug": "流形上的最速下降5-对偶梯度下降-1",
    "path": "blogs_raw/流形上的最速下降5-对偶梯度下降-1.md",
    "status": "completed",
    "date": "2025-11-03",
    "tags": [
      "机器学习"
    ],
    "description": "前四篇文章我们求解了几个具体的给参数加等式约束的最速下降问题，其中第三、四篇的问题没法找到解析解，所以笔者提出了相应的不动点迭代法。其中的其中，第三篇文章[《流形上的最速下降：3. Muon + Stiefel》](https://kexue.fm/archives/11221)所研究的“Stiefel流形上的Muon”，问题提出自Jeremy Bernstein的[《Orthogonal man..."
  },
  {
    "title": "低精度Attention可能存在有偏的舍入误差",
    "slug": "低精度attention可能存在有偏的舍入误差-1",
    "path": "blogs_raw/低精度attention可能存在有偏的舍入误差-1.md",
    "status": "completed",
    "date": "2025-10-27",
    "tags": [
      "机器学习"
    ],
    "description": "前段时间笔者在arXiv上刷到了论文[《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》](https://arxiv.org/abs/2510.04212)，里面描述的实验现象跟我们在训练[Kimi K2](https://arxiv.org/abs/2507.20534)时出现的一些现象很..."
  },
  {
    "title": "MuP之上：1. 好模型的三个特征",
    "slug": "mup之上1-好模型的三个特征-1",
    "path": "blogs_raw/mup之上1-好模型的三个特征-1.md",
    "status": "completed",
    "date": "2025-10-21",
    "tags": [
      "优化"
    ],
    "description": "不知道大家有没有发现一个有趣的细节，Muon和MuP都是“Mu”开头，但两个“Mu”的原意完全不一样，前者是“**M** oment**U** m Orthogonalized by Newton-Schulz”，后者是“**M** aximal **U** pdate Parametrization”，可它们俩之间确实有着非常深刻的联系。也就是说，Muon和MuP有着截然不同的出发点，但最终都走..."
  },
  {
    "title": "随机矩阵的谱范数的快速估计",
    "slug": "随机矩阵的谱范数的快速估计-1",
    "path": "blogs_raw/随机矩阵的谱范数的快速估计-1.md",
    "status": "completed",
    "date": "2025-10-12",
    "tags": [
      "机器学习"
    ],
    "description": "在[《高阶MuP：更简明但更高明的谱条件缩放》](https://kexue.fm/archives/10795)的“近似估计”一节中，我们曾“预支”了一个结论：“一个服从标准正态分布的$n\\times m$大小的随机矩阵，它的谱范数大致是$\\sqrt{n}+\\sqrt{m}$。”  这篇文章我们来补充讨论这个结论，给出随机矩阵谱范数的快速估计方法。  ## 随机矩阵论  设有随机矩阵$\\bold..."
  },
  {
    "title": "DiVeQ：一种非常简洁的VQ训练方案",
    "slug": "diveq一种非常简洁的vq训练方案-1",
    "path": "blogs_raw/diveq一种非常简洁的vq训练方案-1.md",
    "status": "completed",
    "date": "2025-10-08",
    "tags": [
      "机器学习"
    ],
    "description": "对于坚持离散化路线的研究人员来说，VQ（Vector Quantization）是视觉理解和生成的关键部分，担任着视觉中的“Tokenizer”的角色。它提出在2017年的论文[《Neural Discrete Representation Learning》](https://arxiv.org/abs/1711.00937)，笔者在2019年的博客[《VQ-VAE的简明介绍：量子化自编码器》]..."
  },
  {
    "title": "为什么线性注意力要加Short Conv？",
    "slug": "为什么线性注意力要加short-conv-1",
    "path": "blogs_raw/为什么线性注意力要加short-conv-1.md",
    "status": "completed",
    "date": "2025-10-05",
    "tags": [
      "机器学习"
    ],
    "description": "如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考[《线性注意力简史：从模仿、创新到反哺》](https://kexue.fm/archives/11033)）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short Conv，比如下图所示的[DeltaNet](https://arxiv.org/abs..."
  },
  {
    "title": "AdamW的Weight RMS的渐近估计",
    "slug": "adamw的weight-rms的渐近估计-1",
    "path": "blogs_raw/adamw的weight-rms的渐近估计-1.md",
    "status": "completed",
    "date": "2025-10-01",
    "tags": [
      "优化"
    ],
    "description": "在[《为什么Adam的Update RMS是0.2？》](https://kexue.fm/archives/11267)中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 [@EIFY](https://x.com/EIFY/status/1965888629814988984) 指出相同的结果已经出现在论文[《Rotational Equilibrium: How Weig..."
  },
  {
    "title": "重新思考学习率与Batch Size（四）：EMA",
    "slug": "重新思考学习率与batch-size四ema-1",
    "path": "blogs_raw/重新思考学习率与batch-size四ema-1.md",
    "status": "completed",
    "date": "2025-09-22",
    "tags": [
      "优化"
    ],
    "description": "我们在[《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在[《配置不同的学习率，LoRA还能再涨一点？》](https://kexue.fm/archives/10001)、[《初探MuP..."
  },
  {
    "title": "重新思考学习率与Batch Size（三）：Muon",
    "slug": "重新思考学习率与batch-size三muon-1",
    "path": "blogs_raw/重新思考学习率与batch-size三muon-1.md",
    "status": "completed",
    "date": "2025-09-15",
    "tags": [
      "优化"
    ],
    "description": "前两篇文章[《重新思考学习率与Batch Size（一）：现状》](https://kexue.fm/archives/11260)和[《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)中，我们主要是提出了平均场方法，用以简化学习率与Batch Size的相关计算。当时我们分析的优化器是SGD、SignSGD和SoftSign..."
  },
  {
    "title": "低精度Attention可能存在有...",
    "slug": "低精度attention可能存在有",
    "path": "blogs_raw/低精度attention可能存在有.md",
    "status": "completed",
    "date": "",
    "tags": [
      "近似",
      "分析",
      "优化",
      "attention",
      "生成模型"
    ],
    "description": "前段时间笔者在arXiv上刷到了论文[《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》](https://papers.cool/arxiv/2510.04212)，里面描述的实验现象跟我们在训练[Kimi K2](https://papers.cool/arxiv/2507.20534)时..."
  },
  {
    "title": "为什么线性注意力要加Short C...",
    "slug": "为什么线性注意力要加short-c",
    "path": "blogs_raw/为什么线性注意力要加short-c.md",
    "status": "completed",
    "date": "",
    "tags": [
      "线性",
      "RNN",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "description": "如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考[《线性注意力简史：从模仿、创新到反哺》](/archives/11033)）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short Conv，比如下图所示的[DeltaNet](https://papers.cool/arxiv/2406.06484)..."
  },
  {
    "title": "AdamW的Weight RMS的...",
    "slug": "adamw的weight-rms的",
    "path": "blogs_raw/adamw的weight-rms的.md",
    "status": "completed",
    "date": "",
    "tags": [
      "估计",
      "梯度",
      "优化器",
      "平均场",
      "生成模型"
    ],
    "description": "在[《为什么Adam的Update RMS是0.2？》](/archives/11267)中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 [@EIFY](https://x.com/EIFY/status/1965888629814988984) 指出相同的结果已经出现在论文[《Rotational Equilibrium: How Weight Decay Balance..."
  },
  {
    "title": "重新思考学习率与Batch Siz...",
    "slug": "重新思考学习率与batch-siz",
    "path": "blogs_raw/重新思考学习率与batch-siz.md",
    "status": "completed",
    "date": "",
    "tags": [
      "学习率",
      "优化器",
      "尺度定律",
      "平均场",
      "生成模型"
    ],
    "description": "我们在[《重新思考学习率与Batch Size（二）：平均场》](/archives/11280)中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在[《配置不同的学习率，LoRA还能再涨一点？》](/archives/10001)、[《初探MuP：超参数的跨模型尺度迁移规律》](/archives/10770..."
  },
  {
    "title": "重新思考学习率与Batch Siz...",
    "slug": "重新思考学习率与batch-siz",
    "path": "blogs_raw/重新思考学习率与batch-siz.md",
    "status": "existing",
    "date": "",
    "tags": [
      "机器学习"
    ],
    "description": ""
  },
  {
    "title": "苏剑林: 我的pretrain的小模型，暂时没有链接。",
    "slug": "苏剑林-我的pretrain的小模型暂时没有链接",
    "path": "blogs_raw/苏剑林-我的pretrain的小模型暂时没有链接.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "rope",
      "生成模型"
    ],
    "description": "持续将“Transformer升级之路”系列关注到本篇的读者，想必都已经对[旋转位置编码（RoPE）](/archives/8265)有所了解。简单来说，RoPE是施加在Attention的Query（$\\boldsymbol{Q}$）和Key（$\\boldsymbol{K}$）上的旋转变换，形式上属于绝对位置编码，但结合Attention的内积（Dot-Product）特性，能够自动实现相对位置..."
  },
  {
    "title": "苏剑林: 就是反向构造出来的。",
    "slug": "苏剑林-就是反向构造出来的",
    "path": "blogs_raw/苏剑林-就是反向构造出来的.md",
    "status": "completed",
    "date": "",
    "tags": [
      "SSM",
      "SSM",
      "SSM",
      "线性",
      "RNN"
    ],
    "description": "在中文圈，本站应该算是比较早关注线性Attention的了，在2020年写首篇相关博客[《线性Attention的探索：Attention必须有个Softmax吗？》](/archives/7546)时，大家主要讨论的还是BERT相关的Softmax Attention。事后来看，在BERT时代考虑线性Attention并不是太明智，因为当时训练长度比较短，且模型主要还是Encoder，用线性At..."
  },
  {
    "title": "个性邮箱",
    "slug": "个性邮箱",
    "path": "blogs_raw/个性邮箱.md",
    "status": "completed",
    "date": "",
    "tags": [
      "邮箱",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "description": "**注：目前QQ域名邮箱已经不允许新增新账号，因此暂停申请。（2020年02月17日）**  ### 简介 #  科学空间与腾讯公司联合推出以@spaces.ac.cn为后缀的QQ邮箱，欢迎QQ用户申请注册。  腾讯公司可以说越来越强大了，之前已经提供了即时通讯软件(QQ)、电子邮箱、个人空间等让人喜爱的功能，现在还提供了个性化域名邮箱服务，只要有域名，就可以使用自己域名的邮箱了，而这个邮箱空间就..."
  },
  {
    "title": "观测ISS",
    "slug": "观测iss",
    "path": "blogs_raw/观测iss.md",
    "status": "completed",
    "date": "",
    "tags": [
      "国际空间站",
      "ISS",
      "观测",
      "生成模型",
      "attention"
    ],
    "description": "如果你在夜晚看见一颗明亮的星星在移动着，也许你会猜测那是一架飞机，不过，现在请你再加上一个假设：它或许就是“国际空间站（ISS）”！   ![](/usr/uploads/2009/07/f200978163239.jpg)   上一次已经发表过关于国际空间站的观测了，文章是[http://kexue.fm/archives/14/](/archives/14/)。不过上一次注重的是文章的翻译，这..."
  },
  {
    "title": "重新思考学习率与Batch Size（二）：平均场",
    "slug": "重新思考学习率与batch-size二平均场",
    "path": "blogs_raw/重新思考学习率与batch-size二平均场.md",
    "status": "completed",
    "date": "",
    "tags": [
      "学习率",
      "优化器",
      "尺度定律",
      "平均场",
      "生成模型"
    ],
    "description": "上文[《重新思考学习率与Batch Size（一）：现状》](/archives/11260)末尾我们说到，对于SignSGD、SoftSignSGD等$\\tilde{\\boldsymbol{\\varphi}}_B$非线性依赖于$\\tilde{\\boldsymbol{g}}_B$的情形，计算过程的心智负担相当沉重，并且面临难以推广的困境。为此，笔者投入了一些精力去尝试简化其中的推导，万幸有些许收获..."
  },
  {
    "title": "为什么Adam的Update RMS是0.2？",
    "slug": "为什么adam的update-rms是02",
    "path": "blogs_raw/为什么adam的update-rms是02.md",
    "status": "completed",
    "date": "",
    "tags": [
      "分析",
      "梯度",
      "优化器",
      "平均场",
      "生成模型"
    ],
    "description": "众所周知，我们很早就开始尝试将Muon用于大规模LLM的训练。特别地，在[《Muon续集：为什么我们选择尝试Muon？》](/archives/10739)中，我们提出了“Match Adam Update RMS”的技巧，以便快速从Adam迁移到Muon上，这个技巧同样用到了Kimi K2的训练中。该技巧是指将Muon的Update RMS统一成0.2，这使得我们复用Adam的学习率和权重衰减率..."
  },
  {
    "title": "重新思考学习率与Batch Size（一）：现状",
    "slug": "重新思考学习率与batch-size一现状",
    "path": "blogs_raw/重新思考学习率与batch-size一现状.md",
    "status": "completed",
    "date": "",
    "tags": [
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "description": "在之前的文章[《当Batch Size增大时，学习率该如何随之变化？》](/archives/10542)和[《Adam的epsilon如何影响学习率的Scaling Law？》](/archives/10563)中，我们从理论上讨论了学习率随Batch Size的变化规律，其中比较经典的部分是由OpenAI提出的展开到二阶的分析。然而，当我们要处理非SGD优化器时，这套分析方法的计算过程往往会相..."
  },
  {
    "title": "Cool Papers更新：简单适配Zotero Connector",
    "slug": "cool-papers更新简单适配zotero-connector",
    "path": "blogs_raw/cool-papers更新简单适配zotero-connector.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "description": "很早之前就有读者提出希望可以给[Cool Papers](https://papers.cool/)增加导入Zotero的功能，但由于笔者没用Zotero，加上又比较懒，所以一直没提上日程。这个周末刚好有点时间，研究了一下，做了个简单的适配。  ## 单篇导入 #  首先，我们需要安装[Zotero](https://www.zotero.org/)（这是废话），然后需要给所用浏览器安装[Zote..."
  },
  {
    "title": "流形上的最速下降：4. Muon + 谱球面",
    "slug": "流形上的最速下降4-muon-谱球面",
    "path": "blogs_raw/流形上的最速下降4-muon-谱球面.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "description": "看完了前三篇的读者，想必已经熟悉本系列的“套路”——先提出更新量的约束，寻找最速下降方向，接着再给参数也加上约束，寻找新的最速下降方向。在求解参数约束问题时，我们采用的是“一阶近似够用”原则来简化约束形式，这在几何上对应于“切空间”。然后，我们用待定系数法转化无约束形式来写出解析解，最后再数值求解待定系数。  这篇文章我们再来求解一个新例子——谱球面约束下的Muon——它是第一篇文章[《流形上的最..."
  },
  {
    "title": "ReLU/GeLU/Swish的一个恒等式",
    "slug": "relugeluswish的一个恒等式",
    "path": "blogs_raw/relugeluswish的一个恒等式.md",
    "status": "completed",
    "date": "",
    "tags": [
      "分析",
      "神经网络",
      "恒等式",
      "生成模型",
      "attention"
    ],
    "description": "今天水一点轻松的内容，它基于笔者这两天意识到的一个恒等式。这个恒等式实际上很简单，但初看之下会有点意料之外的感觉，所以来记录一下。  ## 基本结果 #  我们知道$\\newcommand{relu}{\\mathop{\\text{relu}}}\\relu(x) = \\max(x, 0)$，容易证明如下恒等式   \\begin{equation}x = \\relu(x) - \\relu(-x)\\en..."
  },
  {
    "title": "流形上的最速下降：3. Muon + Stiefel",
    "slug": "流形上的最速下降3-muon-stiefel",
    "path": "blogs_raw/流形上的最速下降3-muon-stiefel.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "description": "上回说到，当我们把优化对象从向量参数转移到矩阵参数，并选用更适合矩阵的谱范数约束后，Muon优化器便自然而然地出现了。进一步地，我们考虑了给参数加上正交约束后的最速下降方向，这其中又分方阵和非方阵两部分讨论，其中方阵的求解我们在上一篇文章已经完成，但非方阵部分依然悬而未决。  本文的目标，则是把非方阵部分的求解补上，使得正交约束下的优化得以完全解决。  ## 任务信息 #  先简单回顾一下上文[《..."
  },
  {
    "title": "流形上的最速下降：2. Muon + 正交",
    "slug": "流形上的最速下降2-muon-正交",
    "path": "blogs_raw/流形上的最速下降2-muon-正交.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "description": "本文继续我们的约束优化系列。在上文[《流形上的最速下降：1. SGD + 超球面》](/archives/11196)中，我们重温了优化器的“最小作用量”原理，提出不同优化器的核心差异在于给更新量施加的不同约束，如果这个约束是欧几里得范数，那么对应的最速下降便是SGD。进一步地，我们还讨论了同时给参数增加模长约束后的结果，这构成了超球面流形上的最速下降。  不过，上文只能算是“热身”，因为它处理的..."
  },
  {
    "title": "基于树莓派Zero2W搭建一个随身旁路由",
    "slug": "基于树莓派zero2w搭建一个随身旁路由",
    "path": "blogs_raw/基于树莓派zero2w搭建一个随身旁路由.md",
    "status": "completed",
    "date": "",
    "tags": [
      "linux",
      "网络",
      "路由器",
      "智能家居",
      "生成模型"
    ],
    "description": "前段时间搞了个很迷你的开发板树莓派Zero2W（下面简称“**Pi** ”），还搭配了个USB Key转接板，这几天折腾了一下，用于实现一个随身的旁路由。本文记录一下关键技术点，供有同样需求的读者参考。  [![树莓派Zero2W](/usr/uploads/2025/08/1443796894.jpeg)](/usr/uploads/2025/08/1443796894.jpeg \"点击查看原图..."
  },
  {
    "title": "流形上的最速下降：1.  SGD + 超球面",
    "slug": "流形上的最速下降1-sgd-超球面",
    "path": "blogs_raw/流形上的最速下降1-sgd-超球面.md",
    "status": "completed",
    "date": "",
    "tags": [
      "不等式",
      "优化器",
      "约束",
      "最速下降",
      "生成模型"
    ],
    "description": "类似“梯度的反方向是下降最快的方向”的描述，经常用于介绍梯度下降（SGD）的原理。然而，这句话是有条件的，比如“方向”在数学上是单位向量，它依赖于“范数（模长）”的定义，不同范数的结论也不同，[Muon](/archives/10592)实际上就是给矩阵参数换了个谱范数，从而得到了新的下降方向。又比如，当我们从无约束优化转移到约束优化时，下降最快的方向也未必是梯度的反方向。  为此，在这篇文章中，..."
  },
  {
    "title": "矩阵r次方根和逆r次方根的高效计算",
    "slug": "矩阵r次方根和逆r次方根的高效计算",
    "path": "blogs_raw/矩阵r次方根和逆r次方根的高效计算.md",
    "status": "completed",
    "date": "",
    "tags": [
      "代数",
      "迭代",
      "矩阵",
      "线性",
      "生成模型"
    ],
    "description": "上一篇文章[《矩阵平方根和逆平方根的高效计算》](/archives/11158)中，笔者从$\\newcommand{mcsgn}{\\mathop{\\text{mcsgn}}}\\mcsgn$算子出发，提出了一种很漂亮的矩阵平方根和逆平方根的计算方法。比较神奇的是，该方案经过化简之后，最终公式已经看不到最初$\\mcsgn$形式的样子。这不禁引发了更深层的思考：该方案更本质的工作原理是什么？是否有推广..."
  },
  {
    "title": "矩阵平方根和逆平方根的高效计算",
    "slug": "矩阵平方根和逆平方根的高效计算",
    "path": "blogs_raw/矩阵平方根和逆平方根的高效计算.md",
    "status": "completed",
    "date": "",
    "tags": [
      "代数",
      "迭代",
      "矩阵",
      "线性",
      "生成模型"
    ],
    "description": "设$\\boldsymbol{P}\\in\\mathbb{R}^{n\\times n}$是一个特征值都是非负实数的$n$阶方阵，本文来讨论它的平方根$\\boldsymbol{P}^{1/2}$和逆平方根$\\boldsymbol{P}^{-1/2}$的计算。  ## 基本概念 #  矩阵$\\boldsymbol{P}$的平方根，指的是满足$\\boldsymbol{X}^2=\\boldsymbol{P}$..."
  },
  {
    "title": "QK-Clip：让Muon在Scaleup之路上更进一步",
    "slug": "qk-clip让muon在scaleup之路上更进一步",
    "path": "blogs_raw/qk-clip让muon在scaleup之路上更进一步.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "attention",
      "优化器",
      "muon",
      "生成模型"
    ],
    "description": "四个月前，我们发布了[Moonlight](/archives/10739)，在16B的MoE模型上验证了[Muon](/archives/10592)优化器的有效性。在Moonlight中，我们确认了给Muon添加Weight Decay的必要性，同时提出了通过Update RMS对齐来迁移Adam超参的技巧，这使得Muon可以快速应用于LLM的训练。然而，当我们尝试将Muon进一步拓展到千亿参..."
  },
  {
    "title": "Transformer升级之路：21、MLA好在哪里?（下）",
    "slug": "transformer升级之路21mla好在哪里下",
    "path": "blogs_raw/transformer升级之路21mla好在哪里下.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "description": "在文章[《Transformer升级之路：20、MLA好在哪里?（上）》](/archives/10907)中，我们对[MLA](/archives/10091)相比常见MHA、GQA、MQA的一些变化分别做了消融实验，其中的变化包括“增大head_dims”、“Partial RoPE”和“KV共享”，实验的初步结果是这三个变化很可能都是MLA效果优异的原因。  本文我们将从一个更加偏理论的角度..."
  },
  {
    "title": "“对角+低秩”三角阵的高效求逆方法",
    "slug": "对角低秩三角阵的高效求逆方法",
    "path": "blogs_raw/对角低秩三角阵的高效求逆方法.md",
    "status": "completed",
    "date": "",
    "tags": [
      "计算",
      "矩阵",
      "RNN",
      "attention",
      "生成模型"
    ],
    "description": "从文章[《线性注意力简史：从模仿、创新到反哺》](/archives/11033)我们可以发现，DeltaNet及其后的线性Attention模型，基本上都关联到了逆矩阵$(\\boldsymbol{I} + \\boldsymbol{K}\\boldsymbol{K}^{\\top}\\odot\\boldsymbol{M}^-)^{-1}$。本文就专门来探讨一下这类具有“对角+低秩”特点的三角矩阵的逆矩阵..."
  },
  {
    "title": "通过msign来计算奇异值裁剪mclip（下）",
    "slug": "通过msign来计算奇异值裁剪mclip下",
    "path": "blogs_raw/通过msign来计算奇异值裁剪mclip下.md",
    "status": "completed",
    "date": "",
    "tags": [
      "迭代",
      "近似",
      "矩阵",
      "SVD",
      "muon"
    ],
    "description": "前面我们在[《通过msign来计算奇异值裁剪mclip（上）》](/archives/11006)讨论了奇异值裁剪$\\newcommand{mclip}{\\mathop{\\text{mclip}}}\\mclip$的数值计算，核心思路来自 [@leloykun](https://x.com/leloykun) 的文章[《Numerically Stable Spectral Clipping Via..."
  },
  {
    "title": "矩阵符号函数mcsgn能计算什么？",
    "slug": "矩阵符号函数mcsgn能计算什么",
    "path": "blogs_raw/矩阵符号函数mcsgn能计算什么.md",
    "status": "completed",
    "date": "",
    "tags": [
      "代数",
      "矩阵",
      "线性",
      "生成模型",
      "attention"
    ],
    "description": "在[《msign的导数》](/archives/11025)一文中，我们正式引入了两种矩阵符号函数$\\newcommand{msign}{\\mathop{\\text{msign}}}\\msign$和$\\newcommand{mcsgn}{\\mathop{\\text{mcsgn}}}\\mcsgn$，其中$\\msign$是Muon的核心运算，而$\\mcsgn$则是用来解[Sylvester方程]( h..."
  },
  {
    "title": "msign的导数",
    "slug": "msign的导数",
    "path": "blogs_raw/msign的导数.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微积分",
      "矩阵",
      "梯度",
      "muon",
      "生成模型"
    ],
    "description": "这篇文章我们来推导$\\newcommand{msign}{\\mathop{\\text{msign}}}\\msign$算子的求导公式。如果读者想要像[《Test-Time Training Done Right》](https://papers.cool/arxiv/2505.23884)一样，将[TTT](https://papers.cool/arxiv/2407.04620)和[Muon](/..."
  },
  {
    "title": "通过msign来计算奇异值裁剪mclip（上）",
    "slug": "通过msign来计算奇异值裁剪mclip上",
    "path": "blogs_raw/通过msign来计算奇异值裁剪mclip上.md",
    "status": "completed",
    "date": "",
    "tags": [
      "迭代",
      "近似",
      "矩阵",
      "SVD",
      "muon"
    ],
    "description": "前面我们用了两篇文章[《msign算子的Newton-Schulz迭代（上）》](/archives/10922)和[《msign算子的Newton-Schulz迭代（下）》](/archives/10996)讨论了矩阵的$\\newcommand{msign}{\\mathop{\\text{msign}}}\\newcommand{sign}{\\mathop{\\text{sign}}}\\newcomm..."
  },
  {
    "title": "msign算子的Newton-Schulz迭代（下）",
    "slug": "msign算子的newton-schulz迭代下",
    "path": "blogs_raw/msign算子的newton-schulz迭代下.md",
    "status": "completed",
    "date": "",
    "tags": [
      "迭代",
      "近似",
      "优化器",
      "muon",
      "生成模型"
    ],
    "description": "在上文[《msign算子的Newton-Schulz迭代（上）》](/archives/10922)中，我们试图为$\\mathop{\\text{msign}}$算子寻找更好的Newton-Schulz迭代，以期在有限迭代步数内能达到尽可能高的近似程度，这一过程又可以转化为标量函数$\\mathop{\\text{sign}}(x)$寻找同样形式的多项式迭代。当时，我们的求解思路是用Adam优化器端到端..."
  },
  {
    "title": "等值振荡定理：最优多项式逼近的充要条件",
    "slug": "等值振荡定理最优多项式逼近的充要条件",
    "path": "blogs_raw/等值振荡定理最优多项式逼近的充要条件.md",
    "status": "completed",
    "date": "",
    "tags": [
      "导数",
      "近似",
      "最优",
      "分析",
      "生成模型"
    ],
    "description": "最近在阅读时，遇到了一个关于最优多项式逼近的“[等值振荡定理（Equioscillation Theorem）](https://en.wikipedia.org/wiki/Equioscillation_theorem)”，证明过程还涉及到无穷范数求导，感觉结论和证明都颇为新奇，特来记录一番。  参考资料：[《Notes on how to prove Chebyshev’s equioscil..."
  },
  {
    "title": "生成扩散模型漫谈（三十）：从瞬时速度到平均速度",
    "slug": "生成扩散模型漫谈三十从瞬时速度到平均速度",
    "path": "blogs_raw/生成扩散模型漫谈三十从瞬时速度到平均速度.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "description": "众所周知，生成速度慢是扩散模型一直以来的痛点，而为了解决这个问题，大家可谓“八仙过海，各显神通”，提出了各式各样的解决方案，然而长久以来并没一项工作能够脱颖而出，成为标配。什么样的工作能够达到这个标准呢？在笔者看来，它至少满足几个条件：  > 1、数学原理清晰，能够揭示出快速生成的本质所在； >  > 2、能够单目标从零训练，不需要对抗、蒸馏等额外手段； >  > 3、单步生成接近SOTA，可以通..."
  },
  {
    "title": "MoE环游记：5、均匀分布的反思",
    "slug": "moe环游记5均匀分布的反思",
    "path": "blogs_raw/moe环游记5均匀分布的反思.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "稀疏",
      "moe",
      "生成模型",
      "attention"
    ],
    "description": "如果说Meta的LLAMA系列为Dense模型确立了标准架构，那么DeepSeek或许就是MoE标准架构的奠基者。当然，这并非指DeepSeek首创了MoE，也不是说它的MoE不可超越，而是指DeepSeek对MoE所提的一些改进，很可能都是效果增益比较显著的方向，从而逐渐成为MoE的标配。这其中，包括我们在[《MoE环游记：3、换个思路来分配》](/archives/10757)介绍的Loss-..."
  },
  {
    "title": "msign算子的Newton-Schulz迭代（上）",
    "slug": "msign算子的newton-schulz迭代上",
    "path": "blogs_raw/msign算子的newton-schulz迭代上.md",
    "status": "completed",
    "date": "",
    "tags": [
      "迭代",
      "近似",
      "优化器",
      "muon",
      "生成模型"
    ],
    "description": "在之前的[《Muon优化器赏析：从向量到矩阵的本质跨越》](/archives/10592)、[《Muon续集：为什么我们选择尝试Muon？》](/archives/10739)等文章中，我们介绍了一个极具潜力、有望替代Adam的新兴优化器——“Muon”。随着相关研究的不断深入，Muon优化器受到的关注度也在日益增加。  了解过Muon的读者都知道，Muon的核心运算是$\\newcommand{..."
  },
  {
    "title": "Transformer升级之路：20、MLA好在哪里?（上）",
    "slug": "transformer升级之路20mla好在哪里上",
    "path": "blogs_raw/transformer升级之路20mla好在哪里上.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "description": "自从DeepSeek爆火后，它所提的Attention变体MLA（**M** ulti-head **L** atent **A** ttention）也愈发受到关注。MLA通过巧妙的设计实现了MHA与MQA的自由切换，使得模型可以根据训练和推理的不同特性（Compute-Bound or Memory-Bound）选择最佳的形式，尽可能地达到效率最大化。  诚然，MLA很有效，但也有观点认为它不..."
  },
  {
    "title": "一道概率不等式：盯着它到显然成立为止！",
    "slug": "一道概率不等式盯着它到显然成立为止",
    "path": "blogs_raw/一道概率不等式盯着它到显然成立为止.md",
    "status": "completed",
    "date": "",
    "tags": [
      "不等式",
      "概率",
      "显然成立",
      "生成模型",
      "attention"
    ],
    "description": "前两天，QQ群里有群友抛出了一道不等式求证：     [![一道概率相关的不等式，出自《There is no fast single hashing algorithm》](/usr/uploads/2025/04/3939521975.jpg)](/usr/uploads/2025/04/3939521975.jpg \"点击查看原图\")  一道概率相关的不等式，出自《There is no f..."
  },
  {
    "title": "SVD的导数",
    "slug": "svd的导数",
    "path": "blogs_raw/svd的导数.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微积分",
      "分析",
      "矩阵",
      "SVD",
      "梯度"
    ],
    "description": "SVD（Singular Value Decomposition，奇异值分解）是常见的矩阵分解算法，相信很多读者都已经对它有所了解，此前我们在[《低秩近似之路（二）：SVD》](/archives/10407)也专门介绍过它。然而，读者是否想到，SVD竟然还可以求导呢？笔者刚了解到这一结论时也颇感意外，因为直觉上“分解”往往都是不可导的。但事实是，SVD在一般情况下确实可导，这意味着理论上我们可以..."
  },
  {
    "title": "智能家居之手搓一套能接入米家的零冷水装置",
    "slug": "智能家居之手搓一套能接入米家的零冷水装置",
    "path": "blogs_raw/智能家居之手搓一套能接入米家的零冷水装置.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生活",
      "智能家居",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "之前在[《智能家居之热水器零冷水技术原理浅析》](/archives/9405)，我们详细介绍过零冷水的原理，最后指出当时市面上只有名为“爱喜易”的设备实现了文章介绍的理想设计，笔者前两年也一直在用它。然而，笔者的该套装置最近出现了故障，加之无法接入米家，所以也不大想修了，另外“爱喜易”的新版设备也越来越贵，颇有一种“屠龙少年终成恶龙”的感觉。  所以，笔者决定按照相同的原理，手搓一套能接入米家的..."
  },
  {
    "title": "矩阵的有效秩（Effective Rank）",
    "slug": "矩阵的有效秩effective-rank",
    "path": "blogs_raw/矩阵的有效秩effective-rank.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "熵",
      "稀疏",
      "低秩",
      "生成模型"
    ],
    "description": "秩（Rank）是线性代数中的重要概念，它代表了矩阵的内在维度。然而，数学上对秩的严格定义，很多时候并不完全适用于数值计算场景，因为秩等于非零奇异值的个数，而数学上对“等于零”这件事的理解跟数值计算有所不同，数学上的“等于零”是绝对地、严格地等于零，哪怕是$10^{-100}$也是不等于零，但数值计算不一样，很多时候$10^{-10}$就可以当零看待。  因此，我们希望将秩的概念推广到更符合数值计算..."
  },
  {
    "title": "通过梯度近似寻找Normalization的替代品",
    "slug": "通过梯度近似寻找normalization的替代品",
    "path": "blogs_raw/通过梯度近似寻找normalization的替代品.md",
    "status": "completed",
    "date": "",
    "tags": [
      "函数",
      "分析",
      "梯度",
      "光滑",
      "生成模型"
    ],
    "description": "不知道大家有没有留意到前段时间的[《Transformers without Normalization》](https://papers.cool/arxiv/2503.10622)？这篇论文试图将Transformer模型中的Normalization层用一个Element-wise的运算DyT替代，以期能提高速度并保持效果。这种基础架构的主题本身自带一点吸引力，加之Kaiming He和Ya..."
  },
  {
    "title": "MoE环游记：4、难处应当多投入",
    "slug": "moe环游记4难处应当多投入",
    "path": "blogs_raw/moe环游记4难处应当多投入.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "梯度",
      "moe",
      "动态",
      "生成模型"
    ],
    "description": "前两篇文章我们都在讨论负载均衡，其中在[《MoE环游记：3、换个思路来分配》](/archives/10757)介绍Loss-Free方案时，笔者留了一个悬念：它引入的Bias项有一个冗余的自由度，这个自由度可以用来做另外有趣的事情。这篇文章我们就来讨论这件事。  我们知道，MoE是为每个Token只选择最匹配的$k$个Expert来进行计算，从而在增大参数量的同时还节省了计算量。然而，当我们仔细..."
  },
  {
    "title": "高阶MuP：更简明但更高明的谱条件缩放",
    "slug": "高阶mup更简明但更高明的谱条件缩放",
    "path": "blogs_raw/高阶mup更简明但更高明的谱条件缩放.md",
    "status": "completed",
    "date": "",
    "tags": [
      "LoRA",
      "梯度",
      "优化器",
      "尺度定律",
      "谱范数"
    ],
    "description": "在文章[《初探MuP：超参数的跨模型尺度迁移规律》](/archives/10770)中，我们基于前向传播、反向传播、损失增量和特征变化的尺度不变性推导了MuP（Maximal Update Parametrization）。可能对于部分读者来说，这一过程还是显得有些繁琐，但实际上它比原始论文已经明显简化。要知道，我们是在单篇文章内相对完整地介绍的MuP，而MuP的论文实际上是作者Tensor P..."
  },
  {
    "title": "初探MuP：超参数的跨模型尺度迁移规律",
    "slug": "初探mup超参数的跨模型尺度迁移规律",
    "path": "blogs_raw/初探mup超参数的跨模型尺度迁移规律.md",
    "status": "completed",
    "date": "",
    "tags": [
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "description": "众所周知，完整训练一次大型LLM的成本是昂贵的，这就决定了我们不可能直接在大型LLM上反复测试超参数。一个很自然的想法是希望可以在同结构的小模型上仔细搜索超参数，找到最优组合后直接迁移到大模型上。尽管这个想法很朴素，但要实现它并不平凡，它需要我们了解常见的超参数与模型尺度之间的缩放规律，而MuP正是这个想法的一个实践。  MuP，有时也写$\\mu P$，全名是Maximal Update Para..."
  },
  {
    "title": "MoE环游记：3、换个思路来分配",
    "slug": "moe环游记3换个思路来分配",
    "path": "blogs_raw/moe环游记3换个思路来分配.md",
    "status": "completed",
    "date": "",
    "tags": [
      "最优",
      "损失函数",
      "梯度",
      "moe",
      "生成模型"
    ],
    "description": "这篇文章我们继续探讨MoE的负载均衡问题。在上一篇文章[《MoE环游记：2、不患寡而患不均》](/archives/10735)中，我们主要讨论了通过Aux Loss来促进负载均衡的思路。Aux Loss固然简单直观，但它也有一个明显的缺点——权重不好调——调低了无法促进均衡，调高了容易损害LM Loss，所以业界一直有寻找替代方案的尝试。  本文要分享的是名为“Loss-Free”的方案，由De..."
  },
  {
    "title": "Muon续集：为什么我们选择尝试Muon？",
    "slug": "muon续集为什么我们选择尝试muon",
    "path": "blogs_raw/muon续集为什么我们选择尝试muon.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "梯度",
      "优化器",
      "谱范数",
      "muon"
    ],
    "description": "本文解读一下我们最新的技术报告[《Muon is Scalable for LLM Training》](https://papers.cool/arxiv/2502.16982)，里边分享了我们之前在[《Muon优化器赏析：从向量到矩阵的本质跨越》](/archives/10592)介绍过的Muon优化器的一次较大规模的实践，并开源了相应的模型（我们称之为“[Moonlight](https:/..."
  },
  {
    "title": "MoE环游记：2、不患寡而患不均",
    "slug": "moe环游记2不患寡而患不均",
    "path": "blogs_raw/moe环游记2不患寡而患不均.md",
    "status": "completed",
    "date": "",
    "tags": [
      "损失函数",
      "梯度",
      "稀疏",
      "moe",
      "生成模型"
    ],
    "description": "在上一篇文章[《MoE环游记：1、从几何意义出发》](/archives/10699)中，我们介绍了MoE的一个几何诠释，旨在通过Dense模型的最佳逼近出发来推导和理解MoE。同时在文末我们也说了，给出MoE的计算公式仅仅是开始，训练一个实际有效的MoE模型还有很多细节补，比如本文要讨论的负载均衡（Load Balance）问题。  负载均衡，即“不患寡而患不均”，说白了就是让每个Expert都..."
  },
  {
    "title": "生成扩散模型漫谈（二十九）：用DDPM来离散编码",
    "slug": "生成扩散模型漫谈二十九用ddpm来离散编码",
    "path": "blogs_raw/生成扩散模型漫谈二十九用ddpm来离散编码.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成模型",
      "编码",
      "DDPM",
      "扩散",
      "离散化"
    ],
    "description": "笔者前两天在arXiv刷到了一篇新论文[《Compressed Image Generation with Denoising Diffusion Codebook Models》](https://papers.cool/arxiv/2502.01189)，实在为作者的天马行空所叹服，忍不住来跟大家分享一番。  如本文标题所述，作者提出了一个叫DDCM（**D** enoising **D**..."
  },
  {
    "title": "MoE环游记：1、从几何意义出发",
    "slug": "moe环游记1从几何意义出发",
    "path": "blogs_raw/moe环游记1从几何意义出发.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "几何",
      "稀疏",
      "moe",
      "生成模型"
    ],
    "description": "前两年福至心灵之下，开了一个“[Transformer升级之路](/search/Transformer%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF/)”系列，陆续分享了主流Transformer架构的一些改进工作和个人思考，得到了部份读者的认可。这篇文章开始，我们沿着同样的风格，介绍当前另一个主流架构MoE（Mixture of Experts）。  MoE的流行自不..."
  },
  {
    "title": "三个球的交点坐标（三球交会定位）",
    "slug": "三个球的交点坐标三球交会定位",
    "path": "blogs_raw/三个球的交点坐标三球交会定位.md",
    "status": "completed",
    "date": "",
    "tags": [
      "方程",
      "几何",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "前几天笔者在思考一个问题时，联想到了三球交点问题，即给定三个球的球心坐标和半径，求这三个球的交点坐标。按理说这是一个定义清晰且简明的问题，并且具有鲜明的应用背景（比如卫星定位），应该早已有人给出“标准答案”才对。但笔者搜了一圈，发现不管是英文资料还是中文资料，都没有找到标准的求解流程。  当然，这并不是说这个问题有多难以至于没人能求解出来，事实上这是个早已被人解决的经典问题，笔者只是意外于似乎没有..."
  },
  {
    "title": "细水长flow之TARFLOW：流模型满血归来？",
    "slug": "细水长flow之tarflow流模型满血归来",
    "path": "blogs_raw/细水长flow之tarflow流模型满血归来.md",
    "status": "completed",
    "date": "",
    "tags": [
      "流模型",
      "flow",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "description": "不知道还有没有读者对这个系列有印象？这个系列取名“细水长flow”，主要介绍flow模型的相关工作，起因是当年（2018年）OpenAI发布了一个新的流模型[Glow](/archives/5807)，在以GAN为主流的当时来说着实让人惊艳了一番。但惊艳归惊艳，事实上在相当长的时间内，Glow及后期的一些改进在生成效果方面都是比不上GAN的，更不用说现在主流的扩散模型了。  不过局面可能要改变了，..."
  },
  {
    "title": "低秩近似之路（五）：CUR",
    "slug": "低秩近似之路五cur",
    "path": "blogs_raw/低秩近似之路五cur.md",
    "status": "completed",
    "date": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "description": "再次回到低秩近似之路上。在[《低秩近似之路（四）：ID》](/archives/10501)中，我们介绍了“插值分解（Interpolative Decomposition，ID）”，这是为矩阵$\\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}$寻找$\\boldsymbol{C}\\boldsymbol{Z}$形式的近似的过程，其中$\\boldsymbol{C}\\in\\..."
  },
  {
    "title": "为什么梯度裁剪的默认模长是1？",
    "slug": "为什么梯度裁剪的默认模长是1",
    "path": "blogs_raw/为什么梯度裁剪的默认模长是1.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "梯度",
      "学习率",
      "优化器",
      "生成模型"
    ],
    "description": "我们知道，梯度裁剪（Gradient Clipping）是让模型训练更加平稳的常用技巧。常用的梯度裁剪是根据所有参数的梯度总模长来对梯度进行裁剪，其运算可以表示为   \\begin{equation}\\text{clip}(\\boldsymbol{g},\\tau)=\\left\\\\{\\begin{aligned}&\\boldsymbol{g}, &\\Vert\\boldsymbol{g}\\Vert\\l..."
  },
  {
    "title": "从谱范数梯度到新式权重衰减的思考",
    "slug": "从谱范数梯度到新式权重衰减的思考",
    "path": "blogs_raw/从谱范数梯度到新式权重衰减的思考.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "优化",
      "梯度",
      "优化器",
      "谱范数"
    ],
    "description": "在文章[《Muon优化器赏析：从向量到矩阵的本质跨越》](/archives/10592)中，我们介绍了一个名为“Muon”的新优化器，其中一个理解视角是作为谱范数正则下的最速梯度下降，这似乎揭示了矩阵参数的更本质的优化方向。众所周知，对于矩阵参数我们经常也会加权重衰减（Weight Decay），它可以理解为$F$范数平方的梯度，那么从Muon的视角看，通过谱范数平方的梯度来构建新的权重衰减，会..."
  },
  {
    "title": "生成扩散模型漫谈（二十八）：分步理解一致性模型",
    "slug": "生成扩散模型漫谈二十八分步理解一致性模型",
    "path": "blogs_raw/生成扩散模型漫谈二十八分步理解一致性模型.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "description": "书接上文，在[《生成扩散模型漫谈（二十七）：将步长作为条件输入》](/archives/10617)中，我们介绍了加速采样的Shortcut模型，其对比的模型之一就是“[一致性模型（Consistency Models）](https://papers.cool/arxiv/2303.01469)”。事实上，早在[《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》](/archives/94..."
  },
  {
    "title": "生成扩散模型漫谈（二十七）：将步长作为条件输入",
    "slug": "生成扩散模型漫谈二十七将步长作为条件输入",
    "path": "blogs_raw/生成扩散模型漫谈二十七将步长作为条件输入.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "description": "这篇文章我们再次聚焦于扩散模型的采样加速。众所周知，扩散模型的采样加速主要有两种思路，一是开发更高效的求解器，二是事后蒸馏。然而，据笔者观察，除了上两篇文章介绍过的[SiD](/archives/10085)外，这两种方案都鲜有能将生成步数降低到一步的结果。虽然SiD能做到单步生成，但它需要额外的蒸馏成本，并且蒸馏过程中用到了类似GAN的交替训练过程，总让人感觉差点意思。  本文要介绍的是[《On..."
  },
  {
    "title": "Muon优化器赏析：从向量到矩阵的本质跨越",
    "slug": "muon优化器赏析从向量到矩阵的本质跨越",
    "path": "blogs_raw/muon优化器赏析从向量到矩阵的本质跨越.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "梯度",
      "优化器",
      "谱范数",
      "muon"
    ],
    "description": "随着LLM时代的到来，学术界对于优化器的研究热情似乎有所减退。这主要是因为目前主流的AdamW已经能够满足大多数需求，而如果对优化器“大动干戈”，那么需要巨大的验证成本。因此，当前优化器的变化，多数都只是工业界根据自己的训练经验来对AdamW打的一些小补丁。  不过，最近推特上一个名为“[Muon](https://github.com/KellerJordan/Muon)”的优化器颇为热闹，它声..."
  },
  {
    "title": "从Hessian近似看自适应学习率优化器",
    "slug": "从hessian近似看自适应学习率优化器",
    "path": "blogs_raw/从hessian近似看自适应学习率优化器.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "梯度",
      "学习率",
      "优化器",
      "生成模型"
    ],
    "description": "这几天在重温去年的Meta的一篇论文[《A Theory on Adam Instability in Large-Scale Machine Learning》](https://papers.cool/arxiv/2304.09871)，里边给出了看待Adam等自适应学习率优化器的新视角：它指出梯度平方的滑动平均某种程度上近似于在估计Hessian矩阵的平方，从而Adam、RMSprop等优化..."
  },
  {
    "title": "生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）",
    "slug": "生成扩散模型漫谈二十六基于恒等式的蒸馏下",
    "path": "blogs_raw/生成扩散模型漫谈二十六基于恒等式的蒸馏下.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成模型",
      "梯度",
      "扩散",
      "去噪",
      "生成模型"
    ],
    "description": "继续回到我们的扩散系列。在[《生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）》](/archives/10085)中，我们介绍了SiD（Score identity Distillation），这是一种不需要真实数据、也不需要从教师模型采样的扩散模型蒸馏方案，其形式类似GAN，但有着比GAN更好的训练稳定性。  SiD的核心是通过恒等变换来为学生模型构建更好的损失函数，这一点是开创性的，同时也..."
  },
  {
    "title": "Adam的epsilon如何影响学习率的Scaling Law？",
    "slug": "adam的epsilon如何影响学习率的scaling-law",
    "path": "blogs_raw/adam的epsilon如何影响学习率的scaling-law.md",
    "status": "completed",
    "date": "",
    "tags": [
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "description": "上一篇文章[《当Batch Size增大时，学习率该如何随之变化？》](/archives/10542)我们从多个角度讨论了学习率与Batch Size之间的缩放规律，其中对于Adam优化器我们采用了SignSGD近似，这是分析Adam优化器常用的手段。那么一个很自然的问题就是：用SignSGD来近似Adam究竟有多科学呢？  我们知道，Adam优化器的更新量分母会带有一个$\\epsilon$，初..."
  },
  {
    "title": "当Batch Size增大时，学习率该如何随之变化？",
    "slug": "当batch-size增大时学习率该如何随之变化",
    "path": "blogs_raw/当batch-size增大时学习率该如何随之变化.md",
    "status": "completed",
    "date": "",
    "tags": [
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "description": "随着算力的飞速进步，有越多越多的场景希望能够实现“算力换时间”，即通过堆砌算力来缩短模型训练时间。理想情况下，我们希望投入$n$倍的算力，那么达到同样效果的时间则缩短为$1/n$，此时总的算力成本是一致的。这个“希望”看上去很合理和自然，但实际上并不平凡，即便我们不考虑通信之类的瓶颈，当算力超过一定规模或者模型小于一定规模时，增加算力往往只能增大Batch Size。然而，增大Batch Size..."
  },
  {
    "title": "VQ的又一技巧：给编码表加一个线性变换",
    "slug": "vq的又一技巧给编码表加一个线性变换",
    "path": "blogs_raw/vq的又一技巧给编码表加一个线性变换.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成模型",
      "编码",
      "梯度",
      "离散化",
      "生成模型"
    ],
    "description": "在[《VQ的旋转技巧：梯度直通估计的一般推广》](/archives/10489)中，我们介绍了VQ（Vector Quantization）的Rotation Trick，它的思想是通过推广VQ的STE（Straight-Through Estimator）来为VQ设计更好的梯度，从而缓解VQ的编码表坍缩、编码表利用率低等问题。  无独有偶，昨天发布在arXiv上的论文[《Addressing..."
  },
  {
    "title": "低秩近似之路（四）：ID",
    "slug": "低秩近似之路四id",
    "path": "blogs_raw/低秩近似之路四id.md",
    "status": "completed",
    "date": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "description": "这篇文章的主角是ID（Interpolative Decomposition），中文可以称之为“插值分解”，它同样可以理解为是一种具有特定结构的低秩分解，其中的一侧是该矩阵的若干列（当然如果你偏好于行，那么选择行也没什么问题），换句话说，ID试图从一个矩阵中找出若干关键列作为“骨架”（通常也称作“草图”）来逼近原始矩阵。  可能很多读者都未曾听说过ID，即便维基百科也只有几句语焉不详的介绍（[链接..."
  },
  {
    "title": "VQ的旋转技巧：梯度直通估计的一般推广",
    "slug": "vq的旋转技巧梯度直通估计的一般推广",
    "path": "blogs_raw/vq的旋转技巧梯度直通估计的一般推广.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成模型",
      "编码",
      "梯度",
      "离散化",
      "生成模型"
    ],
    "description": "随着多模态LLM的方兴未艾，VQ（Vector Quantization）的地位也“水涨船高”，它可以作为视觉乃至任意模态的Tokenizer，将多模态数据统一到自回归生成框架中。遗憾的是，自[VQ-VAE](/archives/6760)首次提出VQ以来，其理论并没有显著进步，像编码表的坍缩或利用率低等问题至今仍亟待解决，取而代之的是[FSQ](/archives/9826)等替代方案被提出，成..."
  },
  {
    "title": "Cool Papers浏览器扩展升级至v0.2.0",
    "slug": "cool-papers浏览器扩展升级至v020",
    "path": "blogs_raw/cool-papers浏览器扩展升级至v020.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "description": "年初，我们在[《更便捷的Cool Papers打开方式：Chrome重定向扩展》](/archives/9978)中发布了一个Chrome浏览器插件（Cool Papers Redirector v0.1.0），可以通过右击菜单从任意页面中重定向到Cool Papers中，让大家更方便地获取Kimi对论文的理解。前几天我们把该插件升级到了v0.2.0，并顺利上架到了Chrome应用商店中，遂在此向..."
  },
  {
    "title": "让MathJax的数学公式随窗口大小自动缩放",
    "slug": "让mathjax的数学公式随窗口大小自动缩放",
    "path": "blogs_raw/让mathjax的数学公式随窗口大小自动缩放.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "latex",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "随着MathJax的出现和流行，在网页上显示数学公式便逐渐有了标准答案。然而，MathJax（包括其竞品KaTeX）只是负责将网页LaTeX代码转化为数学公式，对于自适应分辨率方面依然没有太好的办法。像本站一些数学文章，因为是在PC端排版好的，所以在PC端浏览效果尚可，但转到手机上看就可能有点难以入目了。  经过测试，笔者得到了一个方案，让MathJax的数学公式也能像图片一样，随着窗口大小而自适..."
  },
  {
    "title": "低秩近似之路（三）：CR",
    "slug": "低秩近似之路三cr",
    "path": "blogs_raw/低秩近似之路三cr.md",
    "status": "completed",
    "date": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "description": "在[《低秩近似之路（二）：SVD》](/archives/10407)中，我们证明了SVD可以给出任意矩阵的最优低秩近似。那里的最优近似是无约束的，也就是说SVD给出的结果只管误差上的最小，不在乎矩阵的具体结构，而在很多应用场景中，出于可解释性或者非线性处理等需求，我们往往希望得到具有某些特殊结构的近似分解。  因此，从这篇文章开始，我们将探究一些具有特定结构的低秩近似，而本文将聚焦于其中的CR近..."
  },
  {
    "title": "低秩近似之路（二）：SVD",
    "slug": "低秩近似之路二svd",
    "path": "blogs_raw/低秩近似之路二svd.md",
    "status": "completed",
    "date": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "SVD",
      "低秩"
    ],
    "description": "上一篇文章中我们介绍了“[伪逆](/archives/10366)”，它关系到给定矩阵$\\boldsymbol{M}$和$\\boldsymbol{A}$（或$\\boldsymbol{B}$）时优化目标$\\Vert \\boldsymbol{A}\\boldsymbol{B} - \\boldsymbol{M}\\Vert_F^2$的最优解。这篇文章我们来关注$\\boldsymbol{A},\\boldsym..."
  },
  {
    "title": "利用“熄火保护 + 通断器”实现燃气灶智能关火",
    "slug": "利用熄火保护-通断器实现燃气灶智能关火",
    "path": "blogs_raw/利用熄火保护-通断器实现燃气灶智能关火.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生活",
      "智能家居",
      "米家",
      "生成模型",
      "attention"
    ],
    "description": "燃气灶智能化主要有两个方向：一是检测开关火状态，实现跟抽油烟机等其他设备的联动；二是实现智能关火，这包括定时关火以及接入米家（或者其他智能家居）实现语音关火、远程关火等。目前带有这两点功能的燃气灶选择并不多，并且相比普通燃气灶贵不少，单纯为了这两点功能而换一个新燃气灶并不划算，所以就出现了一些将普通燃气灶智能化的的魔改方案。  [![接入方案示意图](/usr/uploads/2024/09/26..."
  },
  {
    "title": "Softmax后传：寻找Top-K的光滑近似",
    "slug": "softmax后传寻找top-k的光滑近似",
    "path": "blogs_raw/softmax后传寻找top-k的光滑近似.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "近似",
      "梯度",
      "光滑",
      "生成模型"
    ],
    "description": "Softmax，顾名思义是“soft的max”，是$\\max$算子（准确来说是$\\text{argmax}$）的光滑近似，它通过指数归一化将任意向量$\\boldsymbol{x}\\in\\mathbb{R}^n$转化为分量非负且和为1的新向量，并允许我们通过温度参数来调节它与$\\text{argmax}$（的one hot形式）的近似程度。除了指数归一化外，我们此前在[《通向概率分布之路：盘点Sof..."
  },
  {
    "title": "低秩近似之路（一）：伪逆",
    "slug": "低秩近似之路一伪逆",
    "path": "blogs_raw/低秩近似之路一伪逆.md",
    "status": "completed",
    "date": "",
    "tags": [
      "近似",
      "矩阵",
      "低秩",
      "生成模型",
      "attention"
    ],
    "description": "可能很多读者跟笔者一样，对矩阵的低秩近似有种熟悉而又陌生的感觉。熟悉是因为，低秩近似的概念和意义都不难理解，加之目前诸如LoRA等基于低秩近似的微调技术遍地开花，让低秩近似的概念在耳濡目染间就已经深入人心；然而，低秩近似所覆盖的内容非常广，在低秩近似相关的论文中时常能看到一些不熟悉但又让我们叹为观止的新技巧，这就导致了一种似懂非懂的陌生感。  因此，在这个系列文章中，笔者将试图系统梳理一下矩阵低秩..."
  },
  {
    "title": "“闭门造车”之多模态思路浅谈（三）：位置编码",
    "slug": "闭门造车之多模态思路浅谈三位置编码",
    "path": "blogs_raw/闭门造车之多模态思路浅谈三位置编码.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "多模态",
      "生成模型",
      "attention"
    ],
    "description": "在前面的文章中，我们曾表达过这样的观点：多模态LLM相比纯文本LLM的主要差异在于，前者甚至还没有形成一个公认为标准的方法论。这里的方法论，不仅包括之前讨论的生成和训练策略，还包括一些基础架构的设计，比如本文要谈的“多模态位置编码”。  对于这个主题，我们之前在[《Transformer升级之路：17、多模态位置编码的简单思考》](/archives/10040)就已经讨论过一遍，并且提出了一个方..."
  },
  {
    "title": "Decoder-only的LLM为什么需要位置编码？",
    "slug": "decoder-only的llm为什么需要位置编码",
    "path": "blogs_raw/decoder-only的llm为什么需要位置编码.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "生成模型",
      "attention"
    ],
    "description": "众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在[《为什么现在的LLM都是Decoder-only的架构？》](/archives/9529)也有过相关讨论），而对于Causal Attention，已经有不少工作表明它不需要额外的位置编码（简称NoPE）就可以取得非平凡的结果。然而，事实是主流的Decoder-only LLM都还是加..."
  },
  {
    "title": "近乎完美地解决MathJax与Marked的冲突",
    "slug": "近乎完美地解决mathjax与marked的冲突",
    "path": "blogs_raw/近乎完美地解决mathjax与marked的冲突.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "latex",
      "论文",
      "酷论文",
      "生成模型"
    ],
    "description": "在[《让MathJax更好地兼容谷歌翻译和延时加载》](/archives/10320)我们提到[Cool Papers](https://papers.cool/)加入了MathJax来解析LaTeX公式，不过万万没想到引发了诸多兼容性问题，虽然部分问题纯粹是笔者的强迫症作祟，但一个尽可能完美的解决方案终究是让人赏心悦目的，所以还是愿意在上面花一点心思。  上一篇文章我们已经解决了MathJax..."
  },
  {
    "title": "让MathJax更好地兼容谷歌翻译和延时加载",
    "slug": "让mathjax更好地兼容谷歌翻译和延时加载",
    "path": "blogs_raw/让mathjax更好地兼容谷歌翻译和延时加载.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "latex",
      "论文",
      "酷论文",
      "生成模型"
    ],
    "description": "很早之前，就有读者提出希望把[Cool Papers](https://papers.cool/)上面的数学公式渲染一下，因为很多偏数学的论文，它们的摘要甚至标题上都带有LaTeX代码写的数学公式，如果不把这些公式渲染出来，那么看上去就像是一堆乱码，确实会比较影响阅读体验。然而，之前的测试显示，负责渲染公式的MathJax跟谷歌翻译和延时加载都不大兼容，所以尽管需求存在已久，但笔者一直没有把它加上..."
  },
  {
    "title": "“Cool Papers + 站内搜索”的一些新尝试",
    "slug": "cool-papers-站内搜索的一些新尝试",
    "path": "blogs_raw/cool-papers-站内搜索的一些新尝试.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "description": "在[《Cool Papers更新：简单搭建了一个站内检索系统》](/archives/10088)这篇文章中，我们介绍了[Cool Papers](https://papers.cool/)新增的站内搜索系统。搜索系统的目的，自然希望能够帮助用户快速找到他们需要的论文。然而，如何高效地检索到对自己有价值的结果，并不是一件简单的事情，这里边往往需要一些技巧，比如精准提炼关键词。  这时候算法的价值就..."
  },
  {
    "title": "通向最优分布之路：概率空间的最小化",
    "slug": "通向最优分布之路概率空间的最小化",
    "path": "blogs_raw/通向最优分布之路概率空间的最小化.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "优化",
      "梯度",
      "扩散",
      "生成模型"
    ],
    "description": "当要求函数的最小值时，我们通常会先求导函数然后寻找其零点，比较幸运的情况下，这些零点之一正好是原函数的最小值点。如果是向量函数，则将导数改为梯度并求其零点。当梯度零点不易求得时，我们可以使用梯度下降来逐渐逼近最小值点。  以上这些都是无约束优化的基础结果，相信不少读者都有所了解。然而，本文的主题是概率空间中的优化，即目标函数的输入是一个概率分布，这类目标的优化更为复杂，因为它的搜索空间不再是无约束..."
  },
  {
    "title": "对齐全量微调！这是我看过最精彩的LoRA改进（二）",
    "slug": "对齐全量微调这是我看过最精彩的lora改进二",
    "path": "blogs_raw/对齐全量微调这是我看过最精彩的lora改进二.md",
    "status": "completed",
    "date": "",
    "tags": [
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "description": "前两周笔者写了[《对齐全量微调！这是我看过最精彩的LoRA（一）》](/archives/10226)（当时还没有编号“一”），里边介绍了一个名为“LoRA-GA”的LoRA变体，它通过梯度SVD来改进LoRA的初始化，从而实现LoRA与全量微调的对齐。当然，从理论上来讲，这样做也只能尽量对齐第一步更新后的$W_1$，所以当时就有读者提出了“后面的$W_2,W_3,\\cdots$不管了吗？”的疑问..."
  },
  {
    "title": "Monarch矩阵：计算高效的稀疏型矩阵分解",
    "slug": "monarch矩阵计算高效的稀疏型矩阵分解",
    "path": "blogs_raw/monarch矩阵计算高效的稀疏型矩阵分解.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "语言模型",
      "稀疏",
      "低秩",
      "生成模型"
    ],
    "description": "在矩阵压缩这个问题上，我们通常有两个策略可以选择，分别是**低秩化** 和**稀疏化** 。低秩化通过寻找矩阵的低秩近似来减少矩阵尺寸，而稀疏化则是通过减少矩阵中的非零元素来降低矩阵的复杂性。如果说SVD是奔着矩阵的低秩近似去的，那么相应地寻找矩阵稀疏近似的算法又是什么呢？  接下来我们要学习的是论文[《Monarch: Expressive Structured Matrices for Eff..."
  },
  {
    "title": "【生活杂记】用电饭锅来煮米汤",
    "slug": "生活杂记用电饭锅来煮米汤",
    "path": "blogs_raw/生活杂记用电饭锅来煮米汤.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生活",
      "情感",
      "怀念",
      "生成模型",
      "attention"
    ],
    "description": "前段时间，笔者无意看到了一个“低糖电饭锅”的概念（也叫“低淀粉电饭锅”），开始以为是什么新科技产物，再仔细一看之后才发现，原来就是煮饭的同时沥出一点米汤，米汤中包含了一点淀粉，如果把米汤倒掉，那么就等于少吃了一点淀粉，即所谓的低糖/低淀粉。虽然这种产品看起来就一副智商税的模样（靠这个减糖还不如少吃半口饭），但它却勾起了笔者童年时做饭的回忆，以及对米汤的怀念。  [![经典柴火灶（来源于网络）](/..."
  },
  {
    "title": "对齐全量微调！这是我看过最精彩的LoRA改进（一）",
    "slug": "对齐全量微调这是我看过最精彩的lora改进一",
    "path": "blogs_raw/对齐全量微调这是我看过最精彩的lora改进一.md",
    "status": "completed",
    "date": "",
    "tags": [
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "description": "众所周知，LoRA是一种常见的参数高效的微调方法，我们在[《梯度视角下的LoRA：简介、分析、猜测及推广》](/archives/9590)做过简单介绍。LoRA利用低秩分解来降低微调参数量，节省微调显存，同时训练好的权重可以合并到原始权重上，推理架构不需要作出改变，是一种训练和推理都比较友好的微调方案。此外，我们在[《配置不同的学习率，LoRA还能再涨一点？》](/archives/10001)..."
  },
  {
    "title": "“闭门造车”之多模态思路浅谈（二）：自回归",
    "slug": "闭门造车之多模态思路浅谈二自回归",
    "path": "blogs_raw/闭门造车之多模态思路浅谈二自回归.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成模型",
      "扩散",
      "多模态",
      "自回归",
      "生成模型"
    ],
    "description": "这篇文章我们继续来闭门造车，分享一下笔者最近对多模态学习的一些新理解。  在前文[《“闭门造车”之多模态思路浅谈（一）：无损输入》](/archives/9984)中，我们强调了无损输入对于理想的多模型模态的重要性。如果这个观点成立，那么当前基于VQ-VAE、VQ-GAN等将图像离散化的主流思路就存在能力瓶颈，因为只需要简单计算一下信息熵就可以表明离散化必然会有严重的信息损失，所以更有前景或者说更..."
  },
  {
    "title": "重温SSM（四）：有理生成函数的新视角",
    "slug": "重温ssm四有理生成函数的新视角",
    "path": "blogs_raw/重温ssm四有理生成函数的新视角.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成函数",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "description": "在前三篇文章中，我们较为详细地讨论了HiPPO和S4的大部分数学细节。那么，对于接下来的第四篇文章，大家预期我们会讨论什么工作呢？S5、Mamba乃至Mamba2？都不是。本系列文章主要关心SSM的数学基础，旨在了解SSM的同时也补充自己的数学能力。而在上一篇文章我们简单提过S5和Mamba，S5是S4的简化版，相比S4基本上没有引入新的数学技巧，而Mamba系列虽然表现优异，但它已经将$A$简化..."
  },
  {
    "title": "重温SSM（三）：HiPPO的高效计算（S4）",
    "slug": "重温ssm三hippo的高效计算s4",
    "path": "blogs_raw/重温ssm三hippo的高效计算s4.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "description": "前面我们用两篇文章[《重温SSM（一）：线性系统和HiPPO矩阵》](/archives/10114)和[《重温SSM（二）：HiPPO的一些遗留问题》](/archives/10137)介绍了HiPPO的思想和推导——通过正交函数基对持续更新的函数进行实时逼近，其拟合系数的动力学正好可以表示为一个线性ODE系统，并且对于特定的基底以及逼近方式，我们可以将线性系统的关键矩阵精确地算出来。此外，我们..."
  },
  {
    "title": "通向概率分布之路：盘点Softmax及其替代品",
    "slug": "通向概率分布之路盘点softmax及其替代品",
    "path": "blogs_raw/通向概率分布之路盘点softmax及其替代品.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "分析",
      "损失函数",
      "梯度",
      "生成模型"
    ],
    "description": "不论是在基础的分类任务中，还是如今无处不在的注意力机制中，概率分布的构建都是一个关键步骤。具体来说，就是将一个$n$维的任意向量，转换为一个$n$元的离散型概率分布。众所周知，这个问题的标准答案是Softmax，它是指数归一化的形式，相对来说比较简单直观，同时也伴有很多优良性质，从而成为大部分场景下的“标配”。  尽管如此，Softmax在某些场景下也有一些不如人意之处，比如不够稀疏、无法绝对等于..."
  },
  {
    "title": "重温SSM（二）：HiPPO的一些遗留问题",
    "slug": "重温ssm二hippo的一些遗留问题",
    "path": "blogs_raw/重温ssm二hippo的一些遗留问题.md",
    "status": "completed",
    "date": "",
    "tags": [
      "线性",
      "差分",
      "RNN",
      "梯度",
      "ssm"
    ],
    "description": "书接上文，在上一篇文章[《重温SSM（一）：线性系统和HiPPO矩阵》](/archives/10114)中，我们详细讨论了HiPPO逼近框架其HiPPO矩阵的推导，其原理是通过正交函数基来动态地逼近一个实时更新的函数，其投影系数的动力学正好是一个线性系统，而如果以正交多项式为基，那么线性系统的核心矩阵我们可以解析地求解出来，该矩阵就称为HiPPO矩阵。  当然，上一篇文章侧重于HiPPO矩阵的推..."
  },
  {
    "title": "Transformer升级之路：18、RoPE的底数选择原则",
    "slug": "transformer升级之路18rope的底数选择原则",
    "path": "blogs_raw/transformer升级之路18rope的底数选择原则.md",
    "status": "completed",
    "date": "",
    "tags": [
      "不等式",
      "attention",
      "位置编码",
      "rope",
      "生成模型"
    ],
    "description": "我们知道，在[RoPE](/archives/8265)中频率的计算公式为$\\theta_i = b^{-2i/d}$，底数$b$默认值为10000。目前Long Context的主流做法之一是，先在$b=10000$上用短文本预训练，然后调大$b$并在长文本微调，其出发点是[《Transformer升级之路：10、RoPE是一种β进制编码》](/archives/9675)里介绍的NTK-RoP..."
  },
  {
    "title": "重温SSM（一）：线性系统和HiPPO矩阵",
    "slug": "重温ssm一线性系统和hippo矩阵",
    "path": "blogs_raw/重温ssm一线性系统和hippo矩阵.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "description": "前几天，笔者看了几篇介绍SSM（State Space Model）的文章，才发现原来自己从未认真了解过SSM，于是打算认真去学习一下SSM的相关内容，顺便开了这个新坑，记录一下学习所得。  SSM的概念由来已久，但这里我们特指深度学习中的SSM，一般认为其开篇之作是2021年的[S4](https://papers.cool/arxiv/2111.00396)，不算太老，而SSM最新最火的变体大..."
  },
  {
    "title": "缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA",
    "slug": "缓存与效果的极限拉扯从mhamqagqa到mla",
    "path": "blogs_raw/缓存与效果的极限拉扯从mhamqagqa到mla.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "description": "前几天，幻方发布的[DeepSeek-V2](https://papers.cool/arxiv/2405.04434)引起了大家的热烈讨论。首先，最让人哗然的是1块钱100万token的价格，普遍比现有的各种竞品API便宜了两个数量级，以至于有人调侃“这个价格哪怕它输出乱码，我也会认为这个乱码是一种艺术”；其次，从模型的技术报告看，如此便宜的价格背后的关键技术之一是它新提出的MLA（**M**..."
  },
  {
    "title": "Cool Papers更新：简单搭建了一个站内检索系统",
    "slug": "cool-papers更新简单搭建了一个站内检索系统",
    "path": "blogs_raw/cool-papers更新简单搭建了一个站内检索系统.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "description": "自从[《更便捷的Cool Papers打开方式：Chrome重定向扩展》](/archives/9978)之后，[Cool Papers](http://papers.cool)有两次比较大的变化，一次是引入了venue分支，逐步收录了一些会议历年的论文集，如ICLR、ICML等，这部分是动态人工扩充的，欢迎有心仪的会议的读者提更多需求；另一次就是本文的主题，前天新增加的站内检索功能。  本文将简..."
  },
  {
    "title": "生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）",
    "slug": "生成扩散模型漫谈二十五基于恒等式的蒸馏上",
    "path": "blogs_raw/生成扩散模型漫谈二十五基于恒等式的蒸馏上.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成模型",
      "梯度",
      "扩散",
      "去噪",
      "生成模型"
    ],
    "description": "今天我们分享一下论文[《Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation》](https://papers.cool/arxiv/2404.04057)，顾名思义，这是一篇探讨如何更快更好地蒸馏扩散模型的新论文。  即..."
  },
  {
    "title": "生成扩散模型漫谈（二十四）：少走捷径，更快到达",
    "slug": "生成扩散模型漫谈二十四少走捷径更快到达",
    "path": "blogs_raw/生成扩散模型漫谈二十四少走捷径更快到达.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型",
      "attention"
    ],
    "description": "如何减少采样步数同时保证生成质量，是扩散模型应用层面的一个关键问题。其中，[《生成扩散模型漫谈（四）：DDIM = 高观点DDPM》](/archives/9181)介绍的DDIM可谓是加速采样的第一次尝试。后来，[《生成扩散模型漫谈（五）：一般框架之SDE篇》](/archives/9209)、[《生成扩散模型漫谈（五）：一般框架之ODE篇》](/archives/9228)等所介绍的工作将扩散..."
  },
  {
    "title": "生成扩散模型漫谈（二十三）：信噪比与大图生成（下）",
    "slug": "生成扩散模型漫谈二十三信噪比与大图生成下",
    "path": "blogs_raw/生成扩散模型漫谈二十三信噪比与大图生成下.md",
    "status": "completed",
    "date": "",
    "tags": [
      "无监督",
      "生成模型",
      "扩散",
      "信噪比",
      "生成模型"
    ],
    "description": "上一篇文章[《生成扩散模型漫谈（二十二）：信噪比与大图生成（上）》](/archives/10047)中，我们介绍了通过对齐低分辨率的信噪比来改进noise schedule，从而改善直接在像素空间训练的高分辨率图像生成（大图生成）的扩散模型效果。而这篇文章的主角同样是信噪比和大图生成，但做到了更加让人惊叹的事情——直接将训练好低分辨率图像的扩散模型用于高分辨率图像生成，不用额外的训练，并且效果和..."
  },
  {
    "title": "生成扩散模型漫谈（二十二）：信噪比与大图生成（上）",
    "slug": "生成扩散模型漫谈二十二信噪比与大图生成上",
    "path": "blogs_raw/生成扩散模型漫谈二十二信噪比与大图生成上.md",
    "status": "completed",
    "date": "",
    "tags": [
      "损失函数",
      "生成模型",
      "扩散",
      "信噪比",
      "生成模型"
    ],
    "description": "盘点主流的图像扩散模型作品，我们会发现一个特点：当前多数做高分辨率图像生成（下面简称“大图生成”）的工作，都是先通过Encoder变换到Latent空间进行的（即LDM，[Latent Diffusion Model](https://papers.cool/arxiv/2112.10752)），直接在原始Pixel空间训练的扩散模型，大多数分辨率都不超过64*64，而恰好，LDM通过AutoEn..."
  },
  {
    "title": "Transformer升级之路：17、多模态位置编码的简单思考",
    "slug": "transformer升级之路17多模态位置编码的简单思考",
    "path": "blogs_raw/transformer升级之路17多模态位置编码的简单思考.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "rope",
      "多模态",
      "生成模型"
    ],
    "description": "在这个系列的第二篇文章[《Transformer升级之路：2、博采众长的旋转式位置编码》](/archives/8265)中，笔者提出了旋转位置编码（RoPE）——通过绝对位置的形式实现相对位置编码的方案。一开始RoPE是针对一维序列如文本、音频等设计的（RoPE-1D），后来在[《Transformer升级之路：4、二维位置的旋转式位置编码》](/archives/8397)中我们将它推广到了二..."
  },
  {
    "title": "时空之章：将Attention视为平方复杂度的RNN",
    "slug": "时空之章将attention视为平方复杂度的rnn",
    "path": "blogs_raw/时空之章将attention视为平方复杂度的rnn.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "RNN",
      "attention",
      "复杂度",
      "生成模型"
    ],
    "description": "近年来，RNN由于其线性的训练和推理效率，重新吸引了不少研究人员和用户的兴趣，隐约有“文艺复兴”之势，其代表作有[RWKV](https://papers.cool/arxiv/2305.13048)、[RetNet](https://papers.cool/arxiv/2307.08621)、[Mamba](https://papers.cool/arxiv/2312.00752)等。当将RNN..."
  },
  {
    "title": "用傅里叶级数拟合一维概率密度函数",
    "slug": "用傅里叶级数拟合一维概率密度函数",
    "path": "blogs_raw/用傅里叶级数拟合一维概率密度函数.md",
    "status": "completed",
    "date": "",
    "tags": [
      "级数",
      "概率",
      "分析",
      "逼近",
      "生成模型"
    ],
    "description": "在[《“闭门造车”之多模态思路浅谈（一）：无损输入》](/archives/9984)中我们曾提到，图像生成的本质困难是没有一个连续型概率密度的万能拟合器。当然，也不能说完全没有，比如高斯混合模型（GMM）理论上就是可以拟合任意概率密度，就连GAN本质上也可以理解为混合了无限个高斯模型的GMM。然而，GMM尽管理论上的能力是足够的，但它的最大似然估计会很困难，尤其是通常不适用基于梯度的优化器，这限..."
  },
  {
    "title": "配置不同的学习率，LoRA还能再涨一点？",
    "slug": "配置不同的学习率lora还能再涨一点",
    "path": "blogs_raw/配置不同的学习率lora还能再涨一点.md",
    "status": "completed",
    "date": "",
    "tags": [
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "description": "LoRA（Low-Rank Adaptation）是当前LLM的参数高效微调手段之一，此前我们在[《梯度视角下的LoRA：简介、分析、猜测及推广》](/archives/9590)也有过简单讨论。这篇文章我们来学习LoRA的一个新结论：  > **给LoRA的两个矩阵分配不同的学习率，LoRA的效果还能进一步提升。**  该结论出自最近的论文[《LoRA+: Efficient Low Rank..."
  },
  {
    "title": "“闭门造车”之多模态思路浅谈（一）：无损输入",
    "slug": "闭门造车之多模态思路浅谈一无损输入",
    "path": "blogs_raw/闭门造车之多模态思路浅谈一无损输入.md",
    "status": "completed",
    "date": "",
    "tags": [
      "VAE",
      "GAN",
      "Flow",
      "Diffusion",
      "生成模型"
    ],
    "description": "这篇文章分享一下笔者关于多模态模型架构的一些闭门造车的想法，或者说一些猜测。  最近Google的[Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)和OpenAI的[Sora](https://openai.com/sora)再次点燃了不少人对多模态的热..."
  },
  {
    "title": "更便捷的Cool Papers打开方式：Chrome重定向扩展",
    "slug": "更便捷的cool-papers打开方式chrome重定向扩展",
    "path": "blogs_raw/更便捷的cool-papers打开方式chrome重定向扩展.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "description": "## 一些铺垫 #  自Cool Papers上线以来，很多用户就建议笔者加入搜索功能，后面也确实在前端用JS简单做了个页面内搜索，解决了部分用户的需求，但仍有读者希望引入更完整的全局搜索。诚然，笔者理解这个需求确实是存在，但Cool Papers的数据是逐天累积的，目前才上线一个月，论文数并不多，建立一个大而全的搜索引擎意义不大，其次做搜索也不是笔者的强项，以及并没有很好的利用LLM优化搜索的思..."
  },
  {
    "title": "幂等生成网络IGN：试图将判别和生成合二为一的GAN",
    "slug": "幂等生成网络ign试图将判别和生成合二为一的gan",
    "path": "blogs_raw/幂等生成网络ign试图将判别和生成合二为一的gan.md",
    "status": "completed",
    "date": "",
    "tags": [
      "GAN",
      "GAN",
      "生成模型",
      "对抗",
      "生成模型"
    ],
    "description": "前段时间，一个名为“[幂等生成网络（Idempotent Generative Network，IGN）](https://papers.cool/arxiv/2311.01462)”的生成模型引起了一定的关注。它自称是一种独立于已有的VAE、GAN、flow、Diffusion之外的新型生成模型，并且具有单步采样的特点。也许是大家苦于当前主流的扩散模型的多步采样生成过程久矣，因此任何声称可以实现..."
  },
  {
    "title": "Transformer升级之路：16、“复盘”长度外推技术",
    "slug": "transformer升级之路16复盘长度外推技术",
    "path": "blogs_raw/transformer升级之路16复盘长度外推技术.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "description": "回过头来看，才发现从第7篇[《Transformer升级之路：7、长度外推性与局部注意力》](/archives/9431)开始，“[Transformer升级之路](/search/Transformer%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF/)”这个系列就跟长度外推“杠”上了，接连9篇文章（不算本文）都是围绕长度外推展开的。如今，距离第7篇文章刚好是一年多一点..."
  },
  {
    "title": "旁门左道之如何让Python的重试代码更加优雅",
    "slug": "旁门左道之如何让python的重试代码更加优雅",
    "path": "blogs_raw/旁门左道之如何让python的重试代码更加优雅.md",
    "status": "completed",
    "date": "",
    "tags": [
      "编程",
      "代码",
      "python",
      "优化",
      "生成模型"
    ],
    "description": "这篇文章我们讨论一个编程题：如何更优雅地在Python中实现重试。  在文章[《新年快乐！记录一下 Cool Papers 的开发体验》](/archives/9920)中，笔者分享了开发[Cool Papers](https://papers.cool/)的一些经验，其中就提到了Cool Papers所需要的一些网络通信步骤。但凡涉及到网络通信，就有失败的风险（谁也无法保证网络不会间歇性抽风），..."
  },
  {
    "title": "局部余弦相似度大，全局余弦相似度一定也大吗？",
    "slug": "局部余弦相似度大全局余弦相似度一定也大吗",
    "path": "blogs_raw/局部余弦相似度大全局余弦相似度一定也大吗.md",
    "status": "completed",
    "date": "",
    "tags": [
      "不等式",
      "相似度",
      "悖论",
      "生成模型",
      "attention"
    ],
    "description": "在分析模型的参数时，有些情况下我们会将模型的所有参数当成一个整体的向量，有些情况下我们则会将不同的参数拆开来看。比如，一个7B大小的LLAMA模型所拥有的70亿参数量，有时候我们会将它当成“一个70亿维的向量”，有时候我们会按照模型的实现方式将它看成“数百个不同维度的向量”，最极端的情况下，我们也会将它看成是“七十亿个1维向量”。既然有不同的看待方式，那么当我们要算一些统计指标时，也就会有不同的计..."
  },
  {
    "title": "新年快乐！记录一下 Cool Papers 的开发体验",
    "slug": "新年快乐记录一下-cool-papers-的开发体验",
    "path": "blogs_raw/新年快乐记录一下-cool-papers-的开发体验.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "description": "上周在[《写了个刷论文的辅助网站：Cool Papers》](/archives/9907)中，笔者分享了一个自己开发的刷论文网站Cool Papers，并得到了一些用户的认可。然而，“使用的人越多，暴露的问题就越多”，当用户量上来后，才感觉到之前写的代码是多么不严谨，于是过去一整周都在不停地修Bug之中，直到今天下午还发现了一个Bug在修。这篇文章简单总结一下笔者在开发和修Bug过程中的感想。..."
  },
  {
    "title": "写了个刷论文的辅助网站：Cool Papers",
    "slug": "写了个刷论文的辅助网站cool-papers",
    "path": "blogs_raw/写了个刷论文的辅助网站cool-papers.md",
    "status": "completed",
    "date": "",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "description": "## 写在开头 #  一直以来，笔者都有日刷Arxiv的习惯，以求尽可能跟上领域内最新成果，并告诫自己“不进则退”。之前也有不少读者问我是怎么刷Arxiv的、有什么辅助工具等，但事实上，在很长的时间里，笔者都是直接刷Arxiv官网，并且没有用任何算法过滤，都是自己一篇篇过的。这个过程很枯燥，但并非不能接受，之所以不用算法初筛，主要还是担心算法漏召，毕竟“刷”就是为了追新，一旦算法漏召就“错失先机”..."
  },
  {
    "title": "让炼丹更科学一些（一）：SGD的平均损失收敛",
    "slug": "让炼丹更科学一些一sgd的平均损失收敛",
    "path": "blogs_raw/让炼丹更科学一些一sgd的平均损失收敛.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化器",
      "不等式",
      "优化器",
      "sgd",
      "炼丹"
    ],
    "description": "很多时候我们将深度学习模型的训练过程戏称为“炼丹”，因为整个过程跟古代的炼丹术一样，看上去有一定的科学依据，但整体却给人一种“玄之又玄”的感觉。尽管本站之前也关注过一些[优化器](/tag/%E4%BC%98%E5%8C%96%E5%99%A8/)相关的工作，甚至也写过[《从动力学角度看优化算法》](/search/%E4%BB%8E%E5%8A%A8%E5%8A%9B%E5%AD%A6%E8%A..."
  },
  {
    "title": "注意力机制真的可以“集中注意力”吗？",
    "slug": "注意力机制真的可以集中注意力吗",
    "path": "blogs_raw/注意力机制真的可以集中注意力吗.md",
    "status": "completed",
    "date": "",
    "tags": [
      "熵",
      "稀疏",
      "attention",
      "秩",
      "生成模型"
    ],
    "description": "之前在[《Transformer升级之路：3、从Performer到线性Attention》](/archives/8338#%E4%BD%8E%E7%A7%A9%E9%97%AE%E9%A2%98)、[《为什么现在的LLM都是Decoder-only的架构？》](/archives/9529)等文章中，我们从Attention矩阵的“秩”的角度探讨了Attention机制，并曾经判断线性Atte..."
  },
  {
    "title": "生成扩散模型漫谈（二十一）：中值定理加速ODE采样",
    "slug": "生成扩散模型漫谈二十一中值定理加速ode采样",
    "path": "blogs_raw/生成扩散模型漫谈二十一中值定理加速ode采样.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型",
      "attention"
    ],
    "description": "在生成扩散模型的发展史上，[DDIM](/archives/9181)和同期Song Yang的[扩散SDE](/archives/9209)都称得上是里程碑式的工作，因为它们建立起了扩散模型与随机微分方程（SDE）、常微分方程（ODE）这两个数学领域的紧密联系，从而允许我们可以利用SDE、ODE已有的各种数学工具来对分析、求解和拓展扩散模型，比如后续大量的加速采样工作都以此为基础，可以说这打开了..."
  },
  {
    "title": "我在Performer中发现了Transformer-VQ的踪迹",
    "slug": "我在performer中发现了transformer-vq的踪迹",
    "path": "blogs_raw/我在performer中发现了transformer-vq的踪迹.md",
    "status": "completed",
    "date": "",
    "tags": [
      "量子化",
      "语言模型",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "前些天我们在[《VQ一下Key，Transformer的复杂度就变成线性了》](/archives/9844)介绍了“Transformer-VQ”，这是通过将Key序列做VQ（Vector Quantize）变换来实现Attention复杂度线性化的方案。诚然，Transformer-VQ提供了标准Attention到线性Attentino的一个非常漂亮的过渡，给人一种“大道至简”的美感，但熟悉..."
  },
  {
    "title": "Transformer升级之路：15、Key归一化助力长度外推",
    "slug": "transformer升级之路15key归一化助力长度外推",
    "path": "blogs_raw/transformer升级之路15key归一化助力长度外推.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "生成模型"
    ],
    "description": "大体上，我们可以将目前Transformer的长度外推技术分为两类：一类是事后修改，比如[NTK-RoPE](/archives/9675)、[YaRN](https://papers.cool/arxiv/2309.00071)、[ReRoPE](/archives/9708)等，这类方法的特点是直接修改推理模型，无需微调就能达到一定的长度外推效果，但缺点是它们都无法保持模型在训练长度内的恒等性..."
  },
  {
    "title": "【生活杂记】炒锅的尽头是铁锅",
    "slug": "生活杂记炒锅的尽头是铁锅",
    "path": "blogs_raw/生活杂记炒锅的尽头是铁锅.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生活",
      "厨房",
      "美食",
      "生成模型",
      "attention"
    ],
    "description": "[![铁锅（网络图）](/usr/uploads/2023/11/3429860203.jpeg)](/usr/uploads/2023/11/3429860203.jpeg \"点击查看原图\")  铁锅（网络图）  很多会下厨的同学估计都纠结过一件事情，那就是炒锅的选择。  对于炒锅的纠结，归根结底是不粘与方便的权衡。最简单的不粘锅自然是带涂层的不粘锅，如果家里的热源只有电磁炉，并且炒菜习惯比较温..."
  },
  {
    "title": "VQ一下Key，Transformer的复杂度就变成线性了",
    "slug": "vq一下keytransformer的复杂度就变成线性了",
    "path": "blogs_raw/vq一下keytransformer的复杂度就变成线性了.md",
    "status": "completed",
    "date": "",
    "tags": [
      "量子化",
      "编码",
      "梯度",
      "attention",
      "生成模型"
    ],
    "description": "Efficient Transformer，泛指一切致力于降低Transformer的二次复杂度的工作，开始特指针对Attention的改进，后来更一般的思路，如傅里叶变换、线性RNN等，也被归入这个范畴。不得不说，为了降低Transformer的二次复杂度，各路大牛可谓是“八仙过海，各显神通”，各种神奇的思路“百花齐放”，笔者也从中学习到了不少理论知识。然而，尽管Efficient Transf..."
  },
  {
    "title": "简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE",
    "slug": "简单得令人尴尬的fsq四舍五入超越了vq-vae",
    "path": "blogs_raw/简单得令人尴尬的fsq四舍五入超越了vq-vae.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成模型",
      "编码",
      "梯度",
      "离散化",
      "生成模型"
    ],
    "description": "正如“XXX is all you need”一样，有不少论文都以“简单得令人尴尬”命名（An Embarrassingly Simple XXX），但在笔者看来，这些论文大多数都是噱头多于实力。不过，笔者最近阅读到的一篇论文，真的让人不由得发出“简单得令人尴尬”的感叹～  论文的标题是[《Finite Scalar Quantization: VQ-VAE Made Simple》](https..."
  },
  {
    "title": "从梯度最大化看Attention的Scale操作",
    "slug": "从梯度最大化看attention的scale操作",
    "path": "blogs_raw/从梯度最大化看attention的scale操作.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "梯度",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "我们知道，[Scaled Dot-Product Attention](/archives/4765)的Scale因子是$\\frac{1}{\\sqrt{d}}$，其中$d$是$\\boldsymbol{q},\\boldsymbol{k}$的维度。这个Scale因子的一般解释是：如果不除以$\\sqrt{d}$，那么初始的Attention就会很接近one hot分布，这会造成梯度消失，导致模型训练不起..."
  },
  {
    "title": "随机分词再探：从Viterbi Sampling到完美采样算法",
    "slug": "随机分词再探从viterbi-sampling到完美采样算法",
    "path": "blogs_raw/随机分词再探从viterbi-sampling到完美采样算法.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "随机",
      "优化",
      "分词",
      "采样"
    ],
    "description": "在文章[《随机分词浅探：从Viterbi Decoding到Viterbi Sampling》](/archives/9768)中，笔者提出了一种名为“Viterbi Sampling”的随机分词算法，它只是在求最优解的Viterbi Decoding基础上进行小修改，保留了Viterbi算法的简单快速的特点，相比于已有的[Subword Regularization](https://paper..."
  },
  {
    "title": "EMO：基于最优传输思想设计的分类损失函数",
    "slug": "emo基于最优传输思想设计的分类损失函数",
    "path": "blogs_raw/emo基于最优传输思想设计的分类损失函数.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "优化",
      "损失函数",
      "最优传输",
      "生成模型"
    ],
    "description": "众所周知，分类任务的标准损失是交叉熵（Cross Entropy，等价于最大似然MLE，即Maximum Likelihood Estimation），它有着简单高效的特点，但在某些场景下也暴露出一些问题，如偏离评价指标、过度自信等，相应的改进工作也有很多，此前我们也介绍过一些，比如[《再谈类别不平衡问题：调节权重与魔改Loss的对比联系》](/archives/7708)、[《如何训练你的准确率..."
  },
  {
    "title": "预训练一下，Transformer的长序列成绩还能涨不少！",
    "slug": "预训练一下transformer的长序列成绩还能涨不少",
    "path": "blogs_raw/预训练一下transformer的长序列成绩还能涨不少.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "作为LLM的主流模型架构，Transformer在各类任务上的总体表现都出色，大多数情况下，Transformer的槽点只是它的平方复杂度，而不是效果——除了一个名为Long Range Arena（下面简称LRA）的Benchmark。一直以来，LRA一直是线性RNN类模型的“主场”，与之相比Transformer在上面有明显的差距，以至于让人怀疑这是否就是Transformer的固有缺陷。..."
  },
  {
    "title": "脑洞大开：非线性RNN居然也可以并行计算？",
    "slug": "脑洞大开非线性rnn居然也可以并行计算",
    "path": "blogs_raw/脑洞大开非线性rnn居然也可以并行计算.md",
    "status": "completed",
    "date": "",
    "tags": [
      "摄动",
      "方程",
      "迭代",
      "语言模型",
      "RNN"
    ],
    "description": "近年来，线性RNN由于其可并行训练以及常数推理成本等特性，吸引了一定研究人员的关注（例如笔者之前写的[《Google新作试图“复活”RNN：RNN能否再次辉煌？》](/archives/9554)），这让RNN在Transformer遍地开花的潮流中仍有“一席之地”。然而，目前看来这“一席之地”只属于线性RNN，因为非线性RNN无法高效地并行训练，所以在架构之争中是“心有余而力不足”。  不过，一..."
  },
  {
    "title": "自然数集中 N = ab + c 时 a + b + c 的最小值",
    "slug": "自然数集中-n-ab-c-时-a-b-c-的最小值",
    "path": "blogs_raw/自然数集中-n-ab-c-时-a-b-c-的最小值.md",
    "status": "completed",
    "date": "",
    "tags": [
      "最优",
      "问题",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "前天晚上微信群里有群友提出了一个问题：  > 对于一个任意整数$N > 100$，求一个近似算法，使得$N=a\\times b+c$（其中$a,b,c$都是非负整数），并且令$a+b+c$尽量地小。  初看这道题，笔者第一感觉就是“这还需要算法？”，因为看上去自由度太大了，应该能求出个解析解才对，于是简单分析了一下之后就给出了个“答案”，结果很快就有群友给出了反例。这时，笔者才意识到这题并非那么平..."
  },
  {
    "title": "随机分词浅探：从Viterbi Decoding到Viterbi Sampling",
    "slug": "随机分词浅探从viterbi-decoding到viterbi-sampling",
    "path": "blogs_raw/随机分词浅探从viterbi-decoding到viterbi-sampling.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "随机",
      "分词",
      "新词发现",
      "生成模型"
    ],
    "description": "上一篇文章[《大词表语言模型在续写任务上的一个问题及对策》](/archives/9762)发布后，很快就有读者指出可以在训练阶段引入带有随机性的分词结果来解决同样的问题，并且已经有论文和实现。经过进一步查阅学习，笔者发现这是一个名为[Subword Regularization](https://papers.cool/arxiv/1804.10959)的技巧，最早应用在NMT（机器翻译）中，目..."
  },
  {
    "title": "大词表语言模型在续写任务上的一个问题及对策",
    "slug": "大词表语言模型在续写任务上的一个问题及对策",
    "path": "blogs_raw/大词表语言模型在续写任务上的一个问题及对策.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "问题",
      "语言模型",
      "生成模型",
      "attention"
    ],
    "description": "对于LLM来说，通过增大Tokenizer的词表来提高压缩率，从而缩短序列长度、降低解码成本，是大家都喜闻乐见的事情。毕竟增大词表只需要增大Embedding层和输出的Dense层，这部分增加的计算量几乎不可感知，但缩短序列长度之后带来的解码速度提升却是实打实的。当然，增加词表大小也可能会对模型效果带来一些负面影响，所以也不能无节制地增加词表大小。本文就来分析增大词表后语言模型在续写任务上会出现的..."
  },
  {
    "title": "BytePiece：更纯粹、更高压缩率的Tokenizer",
    "slug": "bytepiece更纯粹更高压缩率的tokenizer",
    "path": "blogs_raw/bytepiece更纯粹更高压缩率的tokenizer.md",
    "status": "completed",
    "date": "",
    "tags": [
      "最小熵",
      "分词",
      "无监督",
      "新词发现",
      "生成模型"
    ],
    "description": "目前在LLM中最流行的Tokenizer（分词器）应该是Google的[SentencePiece](https://github.com/google/sentencepiece)了，因为它符合Tokenizer的一些理想特性，比如语言无关、数据驱动等，并且由于它是C++写的，所以Tokenize（分词）的速度很快，非常适合追求效率的场景。然而，它也有一些明显的缺点，比如训练速度慢（BPE算法）..."
  },
  {
    "title": "Lion/Tiger优化器训练下的Embedding异常和对策",
    "slug": "liontiger优化器训练下的embedding异常和对策",
    "path": "blogs_raw/liontiger优化器训练下的embedding异常和对策.md",
    "status": "completed",
    "date": "",
    "tags": [
      "问题",
      "梯度",
      "优化器",
      "生成模型",
      "attention"
    ],
    "description": "打从在[《Tiger：一个“抠”到极致的优化器》](/archives/9512)提出了Tiger优化器之后，Tiger就一直成为了我训练模型的“标配”优化器。最近笔者已经尝试将Tiger用到了70亿参数模型的预训练之中，前期效果看上来尚可，初步说明Tiger也是能Scale Up的。不过，在查看训练好的模型权重时，笔者发现Embedding出现了一些异常值，有些Embedding的分量达到了$\\..."
  },
  {
    "title": "Transformer升级之路：14、当HWFA遇见ReRoPE",
    "slug": "transformer升级之路14当hwfa遇见rerope",
    "path": "blogs_raw/transformer升级之路14当hwfa遇见rerope.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "外推",
      "rope",
      "生成模型"
    ],
    "description": "在上一篇文章[《Transformer升级之路：13、逆用Leaky ReRoPE》](/archives/9728)中，笔者尝试通过在训练阶段逆用Leaky ReRoPE的思路，使得推理阶段的位置编码变为正常的RoPE，从而在达到长度外推的同时解决ReRoPE推理变慢的缺点。遗憾的是，从实验结果来看，“Leaky ReRoPE → RoPE”的效果并不如“RoPE → ReRoPE/Leaky..."
  },
  {
    "title": "Transformer升级之路：13、逆用Leaky ReRoPE",
    "slug": "transformer升级之路13逆用leaky-rerope",
    "path": "blogs_raw/transformer升级之路13逆用leaky-rerope.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "description": "上周在[《Transformer升级之路：12、无限外推的ReRoPE？》](/archives/9708)中，笔者提出了ReRoPE和Leaky ReRoPE，诸多实验结果表明，它们能够在几乎不损失训练效果的情况下免微调地扩展LLM的Context长度，并且实现了“longer context, lower loss”的理想特性，此外跟NTK-aware Scaled RoPE不同的是，其中Re..."
  },
  {
    "title": "Transformer升级之路：12、无限外推的ReRoPE？",
    "slug": "transformer升级之路12无限外推的rerope",
    "path": "blogs_raw/transformer升级之路12无限外推的rerope.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "description": "自从在[《Transformer升级之路：11、将β进制位置进行到底》](/archives/9706)中引入混合进制的思路进一步推广了NTK-aware Scaled RoPE后，笔者感觉类似思路的效果已经达到了上限，想要更大幅度的提升就必须另辟蹊径了。这时候笔者想起了此前构思过的一个思路，该思路由于复杂度较高所以被搁置下了，既然现在已经遇到了瓶颈，那么“唯一的办法就是最好的办法”，于是便将它重..."
  },
  {
    "title": "Transformer升级之路：11、将β进制位置进行到底",
    "slug": "transformer升级之路11将β进制位置进行到底",
    "path": "blogs_raw/transformer升级之路11将β进制位置进行到底.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "description": "在文章[《Transformer升级之路：10、RoPE是一种β进制编码》](/archives/9675)中，我们给出了RoPE的$\\beta$进制诠释，并基于进制转化的思路推导了能够在不微调的情况下就可以扩展Context长度的[NTK-aware Scaled RoPE](/archives/9675#%E8%BF%BD%E6%A0%B9%E6%BA%AF%E6%BA%90)。不得不说，通过..."
  },
  {
    "title": "语言模型输出端共享Embedding的重新探索",
    "slug": "语言模型输出端共享embedding的重新探索",
    "path": "blogs_raw/语言模型输出端共享embedding的重新探索.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "初始化",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "预训练刚兴起时，在语言模型的输出端重用Embedding权重是很常见的操作，比如BERT、第一版的T5、早期的GPT，都使用了这个操作，这是因为当模型主干部分不大且词表很大时，Embedding层的参数量很可观，如果输出端再新增一个独立的同样大小的权重矩阵的话，会导致显存消耗的激增。不过随着模型参数规模的增大，Embedding层的占比相对变小了，加之[《Rethinking embedding..."
  },
  {
    "title": "当生成模型肆虐：互联网将有“疯牛病”之忧？",
    "slug": "当生成模型肆虐互联网将有疯牛病之忧",
    "path": "blogs_raw/当生成模型肆虐互联网将有疯牛病之忧.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生成模型",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "description": "众所周知，不管是文本还是视觉领域，各种生成模型正在以无法阻挡的势头“肆虐”互联网。虽然大家都明白，实现真正的通用人工智能（AGI）还有很长的路要走，但这并不妨碍人们越来越频繁地利用生成模型来创作和分享内容。君不见，很多网络文章已经配上了Stable Diffusion模型生成的插图；君不见，很多新闻风格已经越来越显现出ChatGPT的影子。看似无害的这种趋势，正悄然引发了一个问题：我们是否应该对互..."
  },
  {
    "title": "Transformer升级之路：10、RoPE是一种β进制编码",
    "slug": "transformer升级之路10rope是一种β进制编码",
    "path": "blogs_raw/transformer升级之路10rope是一种β进制编码.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "description": "对关心如何扩展LLM的Context长度的读者来说，上周无疑是激动人心的一周，开源社区接连不断地出现令人振奋的成果。首先，网友[@kaiokendev](https://www.reddit.com/user/kaiokendev)在他的项目[SuperHOT](https://kaiokendev.github.io/til#extending-context-to-8k)中实验了“位置线性内插..."
  },
  {
    "title": "生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP",
    "slug": "生成扩散模型漫谈二十从reflow到wgan-gp",
    "path": "blogs_raw/生成扩散模型漫谈二十从reflow到wgan-gp.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "GAN",
      "梯度",
      "扩散",
      "生成模型"
    ],
    "description": "上一篇文章[《生成扩散模型漫谈（十九）：作为扩散ODE的GAN》](/archives/9662)中，我们介绍了如何将GAN理解为在另一个时间维度上的扩散ODE，简而言之，GAN实际上就是将扩散模型中样本的运动转化为生成器参数的运动！然而，该文章的推导过程依赖于Wasserstein梯度流等相对复杂和独立的内容，没法很好地跟扩散系列前面的文章连接起来，技术上显得有些“断层”。  在笔者看来，[《生..."
  },
  {
    "title": "生成扩散模型漫谈（十九）：作为扩散ODE的GAN",
    "slug": "生成扩散模型漫谈十九作为扩散ode的gan",
    "path": "blogs_raw/生成扩散模型漫谈十九作为扩散ode的gan.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "GAN",
      "扩散",
      "生成模型",
      "attention"
    ],
    "description": "在文章[《生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配》](/archives/9467)中，我们推导了Wasserstein距离与扩散模型得分匹配损失之间的一个不等式，表明扩散模型的优化目标与WGAN的优化目标在某种程度上具有相似性。而在本文，我们将探讨[《MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserste..."
  },
  {
    "title": "梯度流：探索通向最小值之路",
    "slug": "梯度流探索通向最小值之路",
    "path": "blogs_raw/梯度流探索通向最小值之路.md",
    "status": "completed",
    "date": "",
    "tags": [
      "泛函",
      "动力学",
      "优化",
      "梯度",
      "生成模型"
    ],
    "description": "在这篇文章中，我们将探讨一个被称为“梯度流（Gradient Flow）”的概念。简单来说，梯度流是将我们在用梯度下降法中寻找最小值的过程中的各个点连接起来，形成一条随（虚拟的）时间变化的轨迹，这条轨迹便被称作“梯度流”。在文章的后半部分，我们将重点讨论如何将梯度流的概念扩展到概率空间，从而形成“Wasserstein梯度流”，为我们理解连续性方程、Fokker-Planck方程等内容提供一个新的..."
  },
  {
    "title": "Naive Bayes is all you need ?",
    "slug": "naive-bayes-is-all-you-need",
    "path": "blogs_raw/naive-bayes-is-all-you-need.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "description": "很抱歉，起了这么个具有标题党特征的题目。在写完[《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》](/archives/9617)之后，笔者就觉得朴素贝叶斯（Naive Bayes）跟Attention机制有很多相同的特征，后来再推导了一下发现， _Attention机制其实可以看成是一种 广义的、参数化的朴素贝叶斯_。既然如此，“[Attention is All You Need..."
  },
  {
    "title": "关于NBCE方法的一些补充说明和分析",
    "slug": "关于nbce方法的一些补充说明和分析",
    "path": "blogs_raw/关于nbce方法的一些补充说明和分析.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "外推",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "description": "上周在[《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》](/archives/9617)中，我们介绍了一种基于朴素贝叶斯来扩展LLM的Context长度的方案NBCE（Naive Bayes-based Context Extension）。由于它有着即插即用、模型无关、不用微调等优点，也获得了一些读者的认可，总的来说目前大家反馈的测试效果还算可以。  当然，部分读者在使用的时候也..."
  },
  {
    "title": "NBCE：使用朴素贝叶斯扩展LLM的Context处理长度",
    "slug": "nbce使用朴素贝叶斯扩展llm的context处理长度",
    "path": "blogs_raw/nbce使用朴素贝叶斯扩展llm的context处理长度.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "外推",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "description": "> 在LLM时代还玩朴素贝叶斯（Naive Bayes）？  这可能是许多读者在看到标题后的首个想法。确实如此，当古老的朴素贝叶斯与前沿的LLM相遇时，产生了令人惊讶的效果——我们可以直接扩展现有LLM模型的Context处理长度，无需对模型进行微调，也不依赖于模型架构，具有线性效率，而且效果看起来还不错——这就是本文所提出的NBCE（**N** aive **B** ayes-based **C..."
  },
  {
    "title": "基于量子化假设推导模型的尺度定律（Scaling Law）",
    "slug": "基于量子化假设推导模型的尺度定律scaling-law",
    "path": "blogs_raw/基于量子化假设推导模型的尺度定律scaling-law.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "分析",
      "量子",
      "尺度定律",
      "生成模型"
    ],
    "description": "尺度定律（Scaling Law），指的是模型能力与模型尺度之间的渐近关系。具体来说，模型能力我们可以简单理解为模型的损失函数，模型尺度可以指模型参数量、训练数据量、训练步数等，所谓尺度定律，就是研究损失函数跟参数量、数据量、训练步数等变量的大致关系。[《Scaling Laws for Neural Language Models》](https://papers.cool/arxiv/2001..."
  },
  {
    "title": "Transformer升级之路：9、一种全局长度外推的新思路",
    "slug": "transformer升级之路9一种全局长度外推的新思路",
    "path": "blogs_raw/transformer升级之路9一种全局长度外推的新思路.md",
    "status": "completed",
    "date": "",
    "tags": [
      "attention",
      "泛化",
      "外推",
      "生成模型",
      "attention"
    ],
    "description": "说到Transformer无法处理超长序列的原因，大家的第一反应通常都是Self Attention的二次复杂度。但事实上，即便忽略算力限制，常规的Transformer也无法处理超长序列，因为它们的长度外推性（Length Extrapolation）并不好，具体表现为当输入序列明显超过训练长度时，模型的效果通常会严重下降。  尽管已有一些相关工作，但长度外推问题离实际解决还比较远。本文介绍笔者..."
  },
  {
    "title": "如何度量数据的稀疏程度？",
    "slug": "如何度量数据的稀疏程度",
    "path": "blogs_raw/如何度量数据的稀疏程度.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "熵",
      "度量",
      "稀疏",
      "生成模型"
    ],
    "description": "在机器学习中，我们经常会谈到稀疏性，比如我们经常说注意力矩阵通常是很稀疏的。然而，不知道大家发现没有，我们似乎从没有给出过度量稀疏程度的标准方法。也就是说，以往我们关于稀疏性的讨论，仅仅是直观层面的感觉，并没有过定量分析。那么问题来了，稀疏性的度量有标准方法了吗？  经过搜索，笔者发现确实是有一些可用的指标，比如$l_1/l_2$、熵等，但由于关注视角的不同，在稀疏性度量方面并没有标准答案。本文简..."
  },
  {
    "title": "注意力和Softmax的两点有趣发现：鲁棒性和信息量",
    "slug": "注意力和softmax的两点有趣发现鲁棒性和信息量",
    "path": "blogs_raw/注意力和softmax的两点有趣发现鲁棒性和信息量.md",
    "status": "completed",
    "date": "",
    "tags": [
      "信息",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "最近几周笔者一直都在思考注意力机制的相关性质，在这个过程中对注意力及Softmax有了更深刻的理解。在这篇文章中，笔者简单分享其中的两点：  > 1、Softmax注意力天然能够抵御一定的噪声扰动； >  > 2、从信息熵角度也可以对初始化问题形成直观理解。  ## 鲁棒性 #  基于Softmax归一化的注意力机制，可以写为   \\begin{equation}o = \\frac{\\sum\\li..."
  },
  {
    "title": "梯度视角下的LoRA：简介、分析、猜测及推广",
    "slug": "梯度视角下的lora简介分析猜测及推广",
    "path": "blogs_raw/梯度视角下的lora简介分析猜测及推广.md",
    "status": "completed",
    "date": "",
    "tags": [
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "description": "随着ChatGPT及其平替的火热，各种参数高效（Parameter-Efficient）的微调方法也“水涨船高”，其中最流行的方案之一就是本文的主角**LoRA** 了，它出自论文[《LoRA: Low-Rank Adaptation of Large Language Models》](https://papers.cool/arxiv/2106.09685)。LoRA方法上比较简单直接，而且也..."
  },
  {
    "title": "从JL引理看熵不变性Attention",
    "slug": "从jl引理看熵不变性attention",
    "path": "blogs_raw/从jl引理看熵不变性attention.md",
    "status": "completed",
    "date": "",
    "tags": [
      "熵",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "在[《从熵不变性看Attention的Scale操作》](/archives/8823)、[《熵不变性Softmax的一个快速推导》](/archives/9034)中笔者提出了熵不变性Softmax，简单来说就是往Softmax之前的Attention矩阵多乘上一个$\\log n$，理论上有助于增强长度外推性，其中$n$是序列长度。$\\log n$这个因子让笔者联系到了JL引理（[Johnson..."
  },
  {
    "title": "Bias项的神奇作用：RoPE + Bias = 更好的长度外推性",
    "slug": "bias项的神奇作用rope-bias-更好的长度外推性",
    "path": "blogs_raw/bias项的神奇作用rope-bias-更好的长度外推性.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "rope"
    ],
    "description": "万万没想到，Bias项能跟Transformer的长度外推性联系在一起！  长度外推性是我们希望Transformer具有的一个理想性质，笔者曾在[《Transformer升级之路：7、长度外推性与局部注意力》](/archives/9431)、[《Transformer升级之路：8、长度外推性与位置鲁棒性》](/archives/9444)系统地介绍过这一问题。至于Bias项（偏置项），目前的主..."
  },
  {
    "title": "Google新作试图“复活”RNN：RNN能否再次辉煌？",
    "slug": "google新作试图复活rnnrnn能否再次辉煌",
    "path": "blogs_raw/google新作试图复活rnnrnn能否再次辉煌.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "RNN",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "description": "当前，像ChatGPT之类的LLM可谓是“风靡全球”。有读者留意到，几乎所有LLM都还是用最初的[Multi-Head Scaled-Dot Attention](/archives/4765)，近年来大量的Efficient工作如[线性Attention](/archives/7546)、[FLASH](/archives/8934)等均未被采用。是它们版本效果太差，还是根本没有必要考虑效率？其..."
  },
  {
    "title": "《为什么现在的LLM都是Decoder-only的架构？》FAQ",
    "slug": "为什么现在的llm都是decoder-only的架构faq",
    "path": "blogs_raw/为什么现在的llm都是decoder-only的架构faq.md",
    "status": "completed",
    "date": "",
    "tags": [
      "问答",
      "语言模型",
      "文本生成",
      "attention",
      "生成模型"
    ],
    "description": "上周笔者写了[《为什么现在的LLM都是Decoder-only的架构？》](/archives/9529)，总结了一下我在这个问题上的一些实验结论和猜测。果然是热点问题流量大，paperweekly的转发没多久阅读量就破万了，知乎上点赞数也不少。在几个平台上，陆陆续续收到了读者的一些意见或者疑问，总结了其中一些有代表性的问题，做成了本篇FAQ，希望能进一步帮助大家解决疑惑。  ## 回顾 #  在..."
  },
  {
    "title": "为什么现在的LLM都是Decoder-only的架构？",
    "slug": "为什么现在的llm都是decoder-only的架构",
    "path": "blogs_raw/为什么现在的llm都是decoder-only的架构.md",
    "status": "completed",
    "date": "",
    "tags": [
      "分析",
      "语言模型",
      "文本生成",
      "attention",
      "生成模型"
    ],
    "description": "LLM是“Large Language Model”的简写，目前一般指百亿参数以上的语言模型，主要面向**文本生成** 任务。跟小尺度模型（10亿或以内量级）的“百花齐放”不同，目前LLM的一个现状是Decoder-only架构的研究居多，像OpenAI一直坚持Decoder-only的GPT系列就不说了，即便是Google这样的并非全部押注在Decoder-only的公司，也确实投入了不少的精力..."
  },
  {
    "title": "缓解交叉熵过度自信的一个简明方案",
    "slug": "缓解交叉熵过度自信的一个简明方案",
    "path": "blogs_raw/缓解交叉熵过度自信的一个简明方案.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "损失函数",
      "光滑",
      "生成模型",
      "attention"
    ],
    "description": "众所周知，分类问题的常规评估指标是正确率，而标准的损失函数则是交叉熵，交叉熵有着收敛快的优点，但它并非是正确率的光滑近似，这就带来了训练和预测的不一致性问题。另一方面，当训练样本的预测概率很低时，交叉熵会给出一个非常巨大的损失（趋于$-\\log 0^{+}=\\infty$），这意味着交叉熵会特别关注预测概率低的样本——哪怕这个样本可能是“脏数据”。所以，交叉熵训练出来的模型往往有过度自信现象，即每..."
  },
  {
    "title": "Tiger：一个“抠”到极致的优化器",
    "slug": "tiger一个抠到极致的优化器",
    "path": "blogs_raw/tiger一个抠到极致的优化器.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "优化",
      "优化器",
      "生成模型",
      "attention"
    ],
    "description": "这段时间笔者一直在实验[《Google新搜出的优化器Lion：效率与效果兼得的“训练狮”》](/archives/9473)所介绍的Lion优化器。之所以对Lion饶有兴致，是因为它跟笔者之前的关于理想优化器的一些想法不谋而合，但当时笔者没有调出好的效果，而Lion则做好了。  相比标准的Lion，笔者更感兴趣的是它在$\\beta_1=\\beta_2$时的特殊例子，这里称之为“**Tiger**..."
  },
  {
    "title": "生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配",
    "slug": "生成扩散模型漫谈十八得分匹配-条件得分匹配",
    "path": "blogs_raw/生成扩散模型漫谈十八得分匹配-条件得分匹配.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "分析",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "description": "在前面的介绍中，我们多次提及“得分匹配”和“条件得分匹配”，它们是扩散模型、能量模型等经常出现的概念，特别是很多文章直接说扩散模型的训练目标是“得分匹配”，但事实上当前主流的扩散模型如DDPM的训练目标是“条件得分匹配”才对。  那么“得分匹配”与“条件得分匹配”具体是什么关系呢？它们两者是否等价呢？本文详细讨论这个问题。  ## 得分匹配 #  首先，得分匹配（Score Matching）是指..."
  },
  {
    "title": "生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）",
    "slug": "生成扩散模型漫谈十七构建ode的一般步骤下",
    "path": "blogs_raw/生成扩散模型漫谈十七构建ode的一般步骤下.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "description": "历史总是惊人地相似。当初笔者在写[《生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）》](/archives/9370)（当时还没有“上”这个后缀）时，以为自己已经搞清楚了构建ODE式扩散的一般步骤，结果读者 [@gaohuazuo](/archives/9370#comment-20572) 就给出了一个新的直观有效的方案，这直接导致了后续[《生成扩散模型漫谈（十四）：构建ODE的一般步骤（..."
  },
  {
    "title": "Google新搜出的优化器Lion：效率与效果兼得的“训练狮”",
    "slug": "google新搜出的优化器lion效率与效果兼得的训练狮",
    "path": "blogs_raw/google新搜出的优化器lion效率与效果兼得的训练狮.md",
    "status": "completed",
    "date": "",
    "tags": [
      "分析",
      "优化",
      "优化器",
      "生成模型",
      "attention"
    ],
    "description": "昨天在Arixv上发现了Google新发的一篇论文[《Symbolic Discovery of Optimization Algorithms》](https://papers.cool/arxiv/2302.06675)，主要是讲自动搜索优化器的，咋看上去没啥意思，因为类似的工作也有不少，大多数结果都索然无味。然而，细读之下才发现别有洞天，原来作者们通过数千TPU小时的算力搜索并结合人工干预，..."
  },
  {
    "title": "生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配",
    "slug": "生成扩散模型漫谈十六w距离-得分匹配",
    "path": "blogs_raw/生成扩散模型漫谈十六w距离-得分匹配.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "GAN",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "description": "Wasserstein距离（下面简称“W距离”），是基于最优传输思想来度量两个概率分布差异程度的距离函数，笔者之前在[《从Wasserstein距离、对偶理论到WGAN》](/archives/6280)等博文中也做过介绍。对于很多读者来说，第一次听说W距离，是因为2017年出世的[WGAN](https://papers.cool/arxiv/1701.07875)，它开创了从最优传输视角来理解..."
  },
  {
    "title": "测试函数法推导连续性方程和Fokker-Planck方程",
    "slug": "测试函数法推导连续性方程和fokker-planck方程",
    "path": "blogs_raw/测试函数法推导连续性方程和fokker-planck方程.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "微分方程",
      "随机",
      "扩散",
      "生成模型"
    ],
    "description": "在文章[《生成扩散模型漫谈（六）：一般框架之ODE篇》](/archives/9228)中，我们推导了SDE的Fokker-Planck方程；而在[《生成扩散模型漫谈（十二）：“硬刚”扩散ODE》](/archives/9280)中，我们单独推导了ODE的连续性方程。它们都是描述随机变量沿着SDE/ODE演化的分布变化方程，连续性方程是Fokker-Planck方程的特例。在推导Fokker-Pl..."
  },
  {
    "title": "Transformer升级之路：8、长度外推性与位置鲁棒性",
    "slug": "transformer升级之路8长度外推性与位置鲁棒性",
    "path": "blogs_raw/transformer升级之路8长度外推性与位置鲁棒性.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "生成模型"
    ],
    "description": "上一篇文章[《Transformer升级之路：7、长度外推性与局部注意力》](/archives/9431)我们讨论了Transformer的长度外推性，得出的结论是长度外推性是一个训练和预测的不一致问题，而解决这个不一致的主要思路是将注意力局部化，很多外推性好的改进某种意义上都是局部注意力的变体。诚然，目前语言模型的诸多指标看来局部注意力的思路确实能解决长度外推问题，但这种“强行截断”的做法也许..."
  },
  {
    "title": "Transformer升级之路：7、长度外推性与局部注意力",
    "slug": "transformer升级之路7长度外推性与局部注意力",
    "path": "blogs_raw/transformer升级之路7长度外推性与局部注意力.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "生成模型"
    ],
    "description": "对于Transformer模型来说，其长度的外推性是我们一直在追求的良好性质，它是指我们在短序列上训练的模型，能否不用微调地用到长序列上并依然保持不错的效果。之所以追求长度外推性，一方面是理论的完备性，觉得这是一个理想模型应当具备的性质，另一方面也是训练的实用性，允许我们以较低成本（在较短序列上）训练出一个长序列可用的模型。  下面我们来分析一下加强Transformer长度外推性的关键思路，并由..."
  },
  {
    "title": "智能家居之热水器零冷水技术原理浅析",
    "slug": "智能家居之热水器零冷水技术原理浅析",
    "path": "blogs_raw/智能家居之热水器零冷水技术原理浅析.md",
    "status": "completed",
    "date": "",
    "tags": [
      "智能家居",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "description": "如果家庭使用单一的热水器集中供热水，那么当我们想要用热水时，往往需要先放一段时间的冷水，而如果放冷水时间比较长的话，就会比较影响体验。所谓零冷水，实际上就是想办法提前把热水管中的冷水排放掉，以达到（几乎）瞬间出热水的效果。事实上，零冷水并不是什么高大上的技术，但可能由于观念没跟上、理解上有误等原因，零冷水技术还没有在家庭中得到普及，不过随着大家对生活品质的要求越来越高，零冷水确实在慢慢流行起来了。..."
  },
  {
    "title": "Transformer升级之路：6、旋转位置编码的完备性分析",
    "slug": "transformer升级之路6旋转位置编码的完备性分析",
    "path": "blogs_raw/transformer升级之路6旋转位置编码的完备性分析.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "attention",
      "位置编码",
      "rope",
      "生成模型"
    ],
    "description": "在去年的文章[《Transformer升级之路：2、博采众长的旋转式位置编码》](/archives/8265)中，笔者提出了旋转位置编码（RoPE），当时的出发点只是觉得用绝对位置来实现相对位置是一件“很好玩的事情”，并没料到其实际效果还相当不错，并为大家所接受，不得不说这真是一个意外之喜。后来，在[《Transformer升级之路：4、二维位置的旋转式位置编码》](/archives/8397..."
  },
  {
    "title": "生成扩散模型漫谈（十五）：构建ODE的一般步骤（中）",
    "slug": "生成扩散模型漫谈十五构建ode的一般步骤中",
    "path": "blogs_raw/生成扩散模型漫谈十五构建ode的一般步骤中.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "格林函数",
      "生成模型"
    ],
    "description": "上周笔者写了[《生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）》](/archives/9370)（当时还没有“上”这个后缀），本以为已经窥见了构建ODE扩散模型的一般规律，结果不久后评论区大神 [@gaohuazuo](/archives/9370#comment-20572) 就给出了一个构建格林函数更高效、更直观的方案，让笔者自愧不如。再联想起之前大神之前在[《生成扩散模型漫谈（十二）..."
  },
  {
    "title": "生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）",
    "slug": "生成扩散模型漫谈十四构建ode的一般步骤上",
    "path": "blogs_raw/生成扩散模型漫谈十四构建ode的一般步骤上.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "格林函数",
      "生成模型"
    ],
    "description": "书接上文，在[《生成扩散模型漫谈（十三）：从万有引力到扩散模型》](/archives/9305)中，我们介绍了一个由万有引力启发的、几何意义非常清晰的ODE式生成扩散模型。有的读者看了之后就疑问：似乎“万有引力”并不是唯一的选择，其他形式的力是否可以由同样的物理绘景构建扩散模型？另一方面，该模型在物理上确实很直观，但还欠缺从数学上证明最后确实能学习到数据分布。  本文就尝试从数学角度比较精确地回..."
  },
  {
    "title": "从局部到全局：语义相似度的测地线距离",
    "slug": "从局部到全局语义相似度的测地线距离",
    "path": "blogs_raw/从局部到全局语义相似度的测地线距离.md",
    "status": "completed",
    "date": "",
    "tags": [
      "黎曼几何",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "description": "前段时间在最近的一篇论文[《Unsupervised Opinion Summarization Using Approximate Geodesics》](https://papers.cool/arxiv/2209.07496)中学到了一个新的概念，叫做“测地线距离（Geodesic Distance）”，感觉有点意思，特来跟大家分享一下。  对笔者来说，“新”的不是测地线距离概念本身（以前学..."
  },
  {
    "title": "智能家居之小爱同学控制极米投影仪的简单方案",
    "slug": "智能家居之小爱同学控制极米投影仪的简单方案",
    "path": "blogs_raw/智能家居之小爱同学控制极米投影仪的简单方案.md",
    "status": "completed",
    "date": "",
    "tags": [
      "生活",
      "智能家居",
      "米家",
      "生成模型",
      "attention"
    ],
    "description": "前段时间买了一个极米投影仪，开始折腾才发现极米跟小米基本没啥关系，它根本无法跟小爱同学互动。在众多名字带“米”的品牌中，极米是为数不多的无法接入米家生态的品牌，想必有不少用户开始都会被极米这个名字误导，关键是极米投影仪还在小米商城上有得卖（捂脸）。  买都买了，还过了七天无理由，退是退不成了，只能试着折腾一下，看看能不能强行互动。  ## 现有方案 #  首先网上搜了一下，网友给出的参考方案大体上..."
  },
  {
    "title": "用热传导方程来指导自监督学习",
    "slug": "用热传导方程来指导自监督学习",
    "path": "blogs_raw/用热传导方程来指导自监督学习.md",
    "status": "completed",
    "date": "",
    "tags": [
      "物理",
      "无监督",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "用理论物理来卷机器学习已经不是什么新鲜事了，比如上个月介绍的[《生成扩散模型漫谈（十三）：从万有引力到扩散模型》](/archives/9305)就是经典一例。最近一篇新出的论文[《Self-Supervised Learning based on Heat Equation》](https://papers.cool/arxiv/2211.13228)，顾名思义，用热传导方程来做（图像领域的）自..."
  },
  {
    "title": "基于Amos优化器思想推导出来的一些“炼丹策略”",
    "slug": "基于amos优化器思想推导出来的一些炼丹策略",
    "path": "blogs_raw/基于amos优化器思想推导出来的一些炼丹策略.md",
    "status": "completed",
    "date": "",
    "tags": [
      "分析",
      "优化",
      "渐近",
      "优化器",
      "生成模型"
    ],
    "description": "如果将训练模型比喻为“炼丹”，那么“炼丹炉”显然就是优化器了。据传AdamW优化器是当前训练神经网络最快的方案，这一点笔者也没有一一对比过，具体情况如何不得而知，不过目前做预训练时多数都用AdamW或其变种LAMB倒是真的。然而，正如有了炼丹炉也未必能炼出好丹，即便我们确定了选择AdamW优化器，依然有很多问题还没有确定的答案，比如：  > 1、学习率如何适应不同初始化和参数化？ >  > 2、权..."
  },
  {
    "title": "CoSENT（三）：作为交互式相似度的损失函数",
    "slug": "cosent三作为交互式相似度的损失函数",
    "path": "blogs_raw/cosent三作为交互式相似度的损失函数.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型",
      "attention"
    ],
    "description": "在[《CoSENT（一）：比Sentence-BERT更有效的句向量方案》](/archives/8847)中，笔者提出了名为“CoSENT”的有监督句向量方案，由于它是直接训练cos相似度的，跟评测目标更相关，因此通常能有着比Sentence-BERT更好的效果以及更快的收敛速度。在[《CoSENT（二）：特征式匹配与交互式匹配有多大差距？》](/archives/8860)中我们还比较过它跟交..."
  },
  {
    "title": "利用CUR分解加速交互式相似度模型的检索",
    "slug": "利用cur分解加速交互式相似度模型的检索",
    "path": "blogs_raw/利用cur分解加速交互式相似度模型的检索.md",
    "status": "completed",
    "date": "",
    "tags": [
      "矩阵",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "description": "文本相似度有“交互式”和“特征式”两种做法，想必很多读者对此已经不陌生，之前笔者也写过一篇文章[《CoSENT（二）：特征式匹配与交互式匹配有多大差距？》](/archives/8860)来对比两者的效果。总的来说，交互式相似度效果通常会好些，但直接用它来做大规模检索是不现实的，而特征式相似度则有着更快的检索速度，以及稍逊一筹的效果。  因此，如何在保证交互式相似度效果的前提下提高它的检索速度，是..."
  },
  {
    "title": "圆内随机n点在同一个圆心角为θ的扇形的概率",
    "slug": "圆内随机n点在同一个圆心角为θ的扇形的概率",
    "path": "blogs_raw/圆内随机n点在同一个圆心角为θ的扇形的概率.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "竞赛",
      "随机",
      "生成模型",
      "attention"
    ],
    "description": "这几天网上热传了一道“四鸭共半圆”题目：     [![四鸭共半圆问题](/usr/uploads/2022/10/3296253276.png)](/usr/uploads/2022/10/3296253276.png \"点击查看原图\")  四鸭共半圆问题  可能有不少读者看到后也尝试做过，就连李永乐老师也专门开了一节课讲这道题（参考[《圆形水池四只鸭子在同一个半圆里，概率有多大？》](http..."
  },
  {
    "title": "生成扩散模型漫谈（十三）：从万有引力到扩散模型",
    "slug": "生成扩散模型漫谈十三从万有引力到扩散模型",
    "path": "blogs_raw/生成扩散模型漫谈十三从万有引力到扩散模型.md",
    "status": "completed",
    "date": "",
    "tags": [
      "引力",
      "场论",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "description": "对于很多读者来说，生成扩散模型可能是他们遇到的第一个能够将如此多的数学工具用到深度学习上的模型。在这个系列文章中，我们已经展示了扩散模型与数学分析、概率统计、常微分方程、随机微分方程乃至偏微分方程等内容的深刻联系，可以说，即便是做数学物理方程的纯理论研究的同学，大概率也可以在扩散模型中找到自己的用武之地。  在这篇文章中，我们再介绍一个同样与数学物理有深刻联系的扩散模型——由“万有引力定律”启发的..."
  },
  {
    "title": "“十字架”组合计数问题浅试",
    "slug": "十字架组合计数问题浅试",
    "path": "blogs_raw/十字架组合计数问题浅试.md",
    "status": "completed",
    "date": "",
    "tags": [
      "证明",
      "数学",
      "组合数学",
      "生成模型",
      "attention"
    ],
    "description": "昨天在[这个公众号文章](https://mp.weixin.qq.com/s/cuT6c8gQnpwIrJrYpAo8jQ)看到了一道据说答案有争议的“十字架”组合计数问题：  > 一个正方形中，如果四条边有两条是$i$色，另外两条是其他两种不同颜色，那么称这个正方形是“$i$色主导”的。考虑如下由16条线段、5个正方形组成的“十字架”图形，每条边染上红、黄、蓝三色之一，使得横向和竖向三个正方形..."
  },
  {
    "title": "生成扩散模型漫谈（十二）：“硬刚”扩散ODE",
    "slug": "生成扩散模型漫谈十二硬刚扩散ode",
    "path": "blogs_raw/生成扩散模型漫谈十二硬刚扩散ode.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型",
      "attention"
    ],
    "description": "在[《生成扩散模型漫谈（五）：一般框架之SDE篇》](/archives/9209)中，我们从SDE的角度理解了生成扩散模型，然后在[《生成扩散模型漫谈（六）：一般框架之ODE篇》](/archives/9228)中，我们知道SDE对应的扩散模型中，实际上隐含了一个ODE模型。无独有偶，在[《生成扩散模型漫谈（四）：DDIM = 高观点DDPM》](/archives/9181)中我们也知道原本随..."
  },
  {
    "title": "生成扩散模型漫谈（十一）：统一扩散模型（应用篇）",
    "slug": "生成扩散模型漫谈十一统一扩散模型应用篇",
    "path": "blogs_raw/生成扩散模型漫谈十一统一扩散模型应用篇.md",
    "status": "completed",
    "date": "",
    "tags": [
      "统一",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "在[《生成扩散模型漫谈（十）：统一扩散模型（理论篇）》](/archives/9262)中，笔者自称构建了一个统一的模型框架（Unified Diffusion Model，UDM），它允许更一般的扩散方式和数据类型。那么UDM框架究竟能否实现如期目的呢？本文通过一些具体例子来演示其一般性。  ## 框架回顾 #  首先，UDM通过选择噪声分布$q(\\boldsymbol{\\varepsilon}..."
  },
  {
    "title": "生成扩散模型漫谈（十）：统一扩散模型（理论篇）",
    "slug": "生成扩散模型漫谈十统一扩散模型理论篇",
    "path": "blogs_raw/生成扩散模型漫谈十统一扩散模型理论篇.md",
    "status": "completed",
    "date": "",
    "tags": [
      "统一",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "老读者也许会发现，相比之前的更新频率，这篇文章可谓是“姗姗来迟”，因为这篇文章“想得太多”了。  通过前面九篇文章，我们已经对生成扩散模型做了一个相对全面的介绍。虽然理论内容很多，但我们可以发现，前面介绍的扩散模型处理的都是连续型对象，并且都是基于正态噪声来构建前向过程。而“想得太多”的本文，则希望能够构建一个能突破以上限制的扩散模型统一框架（Unified Diffusion Model，UDM..."
  },
  {
    "title": "生成扩散模型漫谈（九）：条件控制生成结果",
    "slug": "生成扩散模型漫谈九条件控制生成结果",
    "path": "blogs_raw/生成扩散模型漫谈九条件控制生成结果.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "前面的几篇文章都是比较偏理论的结果，这篇文章我们来讨论一个比较有实用价值的主题——条件控制生成。  作为生成模型，扩散模型跟VAE、GAN、flow等模型的发展史很相似，都是先出来了无条件生成，然后有条件生成就紧接而来。无条件生成往往是为了探索效果上限，而有条件生成则更多是应用层面的内容，因为它可以实现根据我们的意愿来控制输出结果。从DDPM至今，已经出来了很多条件扩散模型的工作，甚至可以说真正带..."
  },
  {
    "title": "生成扩散模型漫谈（八）：最优扩散方差估计（下）",
    "slug": "生成扩散模型漫谈八最优扩散方差估计下",
    "path": "blogs_raw/生成扩散模型漫谈八最优扩散方差估计下.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "在上一篇文章[《生成扩散模型漫谈（七）：最优扩散方差估计（上）》](/archives/9245)中，我们介绍并推导了Analytic-DPM中的扩散模型最优方差估计结果，它是直接给出了已经训练好的生成扩散模型的最优方差的一个解析估计，实验显示该估计结果确实能有效提高扩散模型的生成质量。  这篇文章我们继续介绍Analytic-DPM的升级版，出自同一作者团队的论文[《Estimating the..."
  },
  {
    "title": "生成扩散模型漫谈（七）：最优扩散方差估计（上）",
    "slug": "生成扩散模型漫谈七最优扩散方差估计上",
    "path": "blogs_raw/生成扩散模型漫谈七最优扩散方差估计上.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "对于生成扩散模型来说，一个很关键的问题是生成过程的方差应该怎么选择，因为不同的方差会明显影响生成效果。  在[《生成扩散模型漫谈（二）：DDPM = 自回归式VAE》](/archives/9152)我们提到，DDPM分别假设数据服从两种特殊分布推出了两个可用的结果；[《生成扩散模型漫谈（四）：DDIM = 高观点DDPM》](/archives/9181)中的DDIM则调整了生成过程，将方差变为..."
  },
  {
    "title": "生成扩散模型漫谈（六）：一般框架之ODE篇",
    "slug": "生成扩散模型漫谈六一般框架之ode篇",
    "path": "blogs_raw/生成扩散模型漫谈六一般框架之ode篇.md",
    "status": "completed",
    "date": "",
    "tags": [
      "flow模型",
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散"
    ],
    "description": "上一篇文章[《生成扩散模型漫谈（五）：一般框架之SDE篇》](/archives/9209)中，我们对宋飏博士的论文[《Score-Based Generative Modeling through Stochastic Differential Equations》](https://papers.cool/arxiv/2011.13456)做了基本的介绍和推导。然而，顾名思义，上一篇文章主要涉..."
  },
  {
    "title": "生成扩散模型漫谈（五）：一般框架之SDE篇",
    "slug": "生成扩散模型漫谈五一般框架之sde篇",
    "path": "blogs_raw/生成扩散模型漫谈五一般框架之sde篇.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "在写[生成扩散模型](/search/%E7%94%9F%E6%88%90%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/)的第一篇文章时，就有读者在评论区推荐了宋飏博士的论文[《Score-Based Generative Modeling through Stochastic Differential Equations》](https://papers.cool/a..."
  },
  {
    "title": "生成扩散模型漫谈（四）：DDIM = 高观点DDPM",
    "slug": "生成扩散模型漫谈四ddim-高观点ddpm",
    "path": "blogs_raw/生成扩散模型漫谈四ddim-高观点ddpm.md",
    "status": "completed",
    "date": "",
    "tags": [
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "相信很多读者都听说过甚至读过克莱因的[《高观点下的初等数学》](https://book.douban.com/subject/3249247/)这套书，顾名思义，这是在学到了更深入、更完备的数学知识后，从更高的视角重新审视过往学过的初等数学，以得到更全面的认知，甚至达到温故而知新的效果。类似的书籍还有很多，比如[《重温微积分》](https://book.douban.com/subject/1..."
  },
  {
    "title": "生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪",
    "slug": "生成扩散模型漫谈三ddpm-贝叶斯-去噪",
    "path": "blogs_raw/生成扩散模型漫谈三ddpm-贝叶斯-去噪.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "到目前为止，笔者给出了生成扩散模型DDPM的两种推导，分别是[《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》](/archives/9119)中的通俗类比方案和[《生成扩散模型漫谈（二）：DDPM = 自回归式VAE》](/archives/9152)中的变分自编码器方案。两种方案可谓各有特点，前者更为直白易懂，但无法做更多的理论延伸和定量理解，后者理论分析上更加完备一些，但稍显形式化，..."
  },
  {
    "title": "不成功的尝试：将多标签交叉熵推广到“n个m分类”上去",
    "slug": "不成功的尝试将多标签交叉熵推广到n个m分类上去",
    "path": "blogs_raw/不成功的尝试将多标签交叉熵推广到n个m分类上去.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "损失函数",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "可能有读者留意到，这次更新相对来说隔得比较久了。事实上，在上周末时就开始准备这篇文章了，然而笔者低估了这个问题的难度，几乎推导了整整一周，仍然还没得到一个完善的结果出来。目前发出来的，仍然只是一个失败的结果，希望有经验的读者可以指点指点。  在文章[《将“Softmax+交叉熵”推广到多标签分类问题》](/archives/7359)中，我们提出了一个多标签分类损失函数，它能自动调节正负类的不平衡..."
  },
  {
    "title": "生成扩散模型漫谈（二）：DDPM = 自回归式VAE",
    "slug": "生成扩散模型漫谈二ddpm-自回归式vae",
    "path": "blogs_raw/生成扩散模型漫谈二ddpm-自回归式vae.md",
    "status": "completed",
    "date": "",
    "tags": [
      "vae",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "description": "在文章[《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》](/archives/9119)中，我们为生成扩散模型DDPM构建了“拆楼-建楼”的通俗类比，并且借助该类比完整地推导了生成扩散模型DDPM的理论形式。在该文章中，我们还指出DDPM本质上已经不是传统的扩散模型了，它更多的是一个变分自编码器VAE，实际上DDPM的原论文中也是将它按照VAE的思路进行推导的。  所以，本文就从VAE的..."
  },
  {
    "title": "“维度灾难”之Hubness现象浅析",
    "slug": "维度灾难之hubness现象浅析",
    "path": "blogs_raw/维度灾难之hubness现象浅析.md",
    "status": "completed",
    "date": "",
    "tags": [
      "维度",
      "GAN",
      "生成模型",
      "生成模型",
      "attention"
    ],
    "description": "这几天读到论文[《Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling》](https://papers.cool/arxiv/2206.06014)，了解到了一个新的名词“Hubness现象”，说的是高维空间中的一种聚集效应，本质上是“维度灾难”的体现之一。论文借助Hubness的概念得到了一个提..."
  },
  {
    "title": "Ladder Side-Tuning：预训练模型的“过墙梯”",
    "slug": "ladder-side-tuning预训练模型的过墙梯",
    "path": "blogs_raw/ladder-side-tuning预训练模型的过墙梯.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "预训练",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "如果说大型的预训练模型是自然语言处理的“张良计”，那么对应的“过墙梯”是什么呢？笔者认为是高效地微调这些大模型到特定任务上的各种技巧。除了直接微调全部参数外，还有像[Adapter](https://papers.cool/arxiv/1902.00751)、[P-Tuning](/archives/8295)等很多参数高效的微调技巧，它们能够通过只微调很少的参数来达到接近全量参数微调的效果。然而..."
  },
  {
    "title": "生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼",
    "slug": "生成扩散模型漫谈一ddpm-拆楼-建楼",
    "path": "blogs_raw/生成扩散模型漫谈一ddpm-拆楼-建楼.md",
    "status": "completed",
    "date": "",
    "tags": [
      "VAE",
      "GAN",
      "flow模型",
      "概率",
      "生成模型"
    ],
    "description": "说到生成模型，[VAE](/tag/vae/)、[GAN](/tag/GAN/)可谓是“如雷贯耳”，本站也有过多次分享。此外，还有一些比较小众的选择，如[flow模型](/tag/flow/)、[VQ-VAE](/archives/6760)等，也颇有人气，尤其是VQ-VAE及其变体[VQ-GAN](https://papers.cool/arxiv/2012.09841)，近期已经逐渐发展到“图..."
  },
  {
    "title": "相对位置编码Transformer的一个理论缺陷与对策",
    "slug": "相对位置编码transformer的一个理论缺陷与对策",
    "path": "blogs_raw/相对位置编码transformer的一个理论缺陷与对策.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "生成模型",
      "attention"
    ],
    "description": "位置编码是Transformer中很重要的一环，在[《让研究人员绞尽脑汁的Transformer位置编码》](/archives/8130)中我们就总结了一些常见的位置编码设计。大体上，我们将Transformer的位置编码分为“绝对位置编码”和“相对位置编码”两类，其中“相对位置编码”在众多NLP/CV的实验表现相对来说更加好些。  然而，我们可以发现，目前相对位置编码几乎都是在Softmax之..."
  },
  {
    "title": "如何训练你的准确率？",
    "slug": "如何训练你的准确率",
    "path": "blogs_raw/如何训练你的准确率.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "优化",
      "损失函数",
      "生成模型",
      "attention"
    ],
    "description": "最近Arxiv上的一篇论文[《EXACT: How to Train Your Accuracy》](https://papers.cool/arxiv/2205.09615)引起了笔者的兴趣，顾名思义这是介绍如何直接以准确率为训练目标来训练模型的。正好笔者之前也对此有过一些分析，如[《函数光滑化杂谈：不可导函数的可导逼近》](/archives/6620)、[《再谈类别不平衡问题：调节权重与魔改..."
  },
  {
    "title": "从重参数的角度看离散概率分布的构建",
    "slug": "从重参数的角度看离散概率分布的构建",
    "path": "blogs_raw/从重参数的角度看离散概率分布的构建.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "重参数",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "一般来说，神经网络的输出都是无约束的，也就是值域为$\\mathbb{R}$，而为了得到有约束的输出，通常是采用加激活函数的方式。例如，如果我们想要输出一个概率分布来代表每个类别的概率，那么通常在最后加上Softmax作为激活函数。那么一个紧接着的疑问就是：除了Softmax，还有什么别的操作能生成一个概率分布吗？  在[《漫谈重参数：从正态分布到Gumbel Softmax》](/archives..."
  },
  {
    "title": "当BERT-whitening引入超参数：总有一款适合你",
    "slug": "当bert-whitening引入超参数总有一款适合你",
    "path": "blogs_raw/当bert-whitening引入超参数总有一款适合你.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "description": "在[《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》](/archives/8069)中，笔者提出了BERT-whitening，验证了一个线性变换就能媲美当时的SOTA方法BERT-flow。此外，BERT-whitening还可以对句向量进行降维，带来更低的内存占用和更快的检索速度。然而，在[《无监督语义相似度哪家强？我们做了个比较全面的评测》](/archives/83..."
  },
  {
    "title": "logsumexp运算的几个不等式",
    "slug": "logsumexp运算的几个不等式",
    "path": "blogs_raw/logsumexp运算的几个不等式.md",
    "status": "completed",
    "date": "",
    "tags": [
      "不等式",
      "函数",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "$\\text{logsumexp}$是机器学习经常遇到的运算，尤其是交叉熵的相关实现和推导中都会经常出现，同时它还是$\\max$的光滑近似（参考[《寻求一个光滑的最大值函数》](/archives/3290)）。设$x=(x_1,x_2,\\cdots,x_n)$，$\\text{logsumexp}$定义为   \\begin{equation}\\text{logsumexp}(x)=\\log\\sum..."
  },
  {
    "title": "多标签“Softmax+交叉熵”的软标签版本",
    "slug": "多标签softmax交叉熵的软标签版本",
    "path": "blogs_raw/多标签softmax交叉熵的软标签版本.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "损失函数",
      "光滑",
      "生成模型",
      "attention"
    ],
    "description": "**（注：本文的相关内容已整理成论文[《ZLPR: A Novel Loss for Multi-label Classification》](https://papers.cool/arxiv/2208.02955)，如需引用可以直接引用英文论文，谢谢。）**  在[《将“Softmax+交叉熵”推广到多标签分类问题》](/archives/7359)中，我们提出了一个用于多标签分类的损失函数：..."
  },
  {
    "title": "在bert4keras中使用混合精度和XLA加速训练",
    "slug": "在bert4keras中使用混合精度和xla加速训练",
    "path": "blogs_raw/在bert4keras中使用混合精度和xla加速训练.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention"
    ],
    "description": "之前笔者一直都是聚焦于模型的构思和实现，鲜有关注模型的训练加速，像混合精度和XLA这些技术，虽然也有听过，但没真正去实践过。这两天折腾了一番，成功在bert4keras中使用了混合精度和XLA来加速训练，在此做个简单的总结，供大家参考。  本文的多数经验结论并不只限于bert4keras中使用，之所以在标题中强调bert4keras，只不过bert4keras中的模型实现相对较为规整，因此启动这些..."
  },
  {
    "title": "GAU-α：尝鲜体验快好省的下一代Attention",
    "slug": "gau-α尝鲜体验快好省的下一代attention",
    "path": "blogs_raw/gau-α尝鲜体验快好省的下一代attention.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "attention",
      "预训练",
      "生成模型",
      "attention"
    ],
    "description": "在[《FLASH：可能是近来最有意思的高效Transformer设计》](/archives/8934)中，我们介绍了GAU（Gated Attention Unit，门控线性单元），在这里笔者愿意称之为“目前最有潜力的下一代Attention设计”，因为它真正达到了“更快（速度）、更好（效果）、更省（显存）”的特点。  然而，有些读者在自己的测试中得到了相反的结果，比如收敛更慢、效果更差等，这与..."
  },
  {
    "title": "你的语言模型有没有“无法预测的词”？",
    "slug": "你的语言模型有没有无法预测的词",
    "path": "blogs_raw/你的语言模型有没有无法预测的词.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "多任务",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不可能成为概率最大的那个。  当然，这种情况一般只出现在类别数远远超过编码向量维度的场景，常规的分类问题很少这么极端的。然而，我们知道语言模型本质上也..."
  },
  {
    "title": "GlobalPointer下的“KL散度”应该是怎样的？",
    "slug": "globalpointer下的kl散度应该是怎样的",
    "path": "blogs_raw/globalpointer下的kl散度应该是怎样的.md",
    "status": "completed",
    "date": "",
    "tags": [
      "损失函数",
      "对抗训练",
      "NER",
      "正则化",
      "生成模型"
    ],
    "description": "最近有读者提到想测试一下[GlobalPointer](/archives/8373)与[R-Drop](/archives/8496)结合的效果，但不知道GlobalPointer下的KL散度该怎么算。像R-Drop或者[虚拟对抗训练](/archives/7466)这些正则化手段，里边都需要算概率分布的KL散度，但GlobalPointer的预测结果并非一个概率分布，因此无法直接进行计算。..."
  },
  {
    "title": "熵不变性Softmax的一个快速推导",
    "slug": "熵不变性softmax的一个快速推导",
    "path": "blogs_raw/熵不变性softmax的一个快速推导.md",
    "status": "completed",
    "date": "",
    "tags": [
      "近似",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "在文章[《从熵不变性看Attention的Scale操作》](/archives/8823)中，我们推导了一版具有熵不变性质的注意力机制：   \\begin{equation}Attention(Q,K,V) = softmax\\left(\\frac{\\kappa \\log n}{d}QK^{\\top}\\right)V\\label{eq:a}\\end{equation}   可以观察到，它主要是往..."
  },
  {
    "title": "听说Attention与Softmax更配哦～",
    "slug": "听说attention与softmax更配哦",
    "path": "blogs_raw/听说attention与softmax更配哦.md",
    "status": "completed",
    "date": "",
    "tags": [
      "熵",
      "语言模型",
      "attention",
      "预训练",
      "生成模型"
    ],
    "description": "不知道大家留意到一个细节没有，就是当前NLP主流的预训练模式都是在一个固定长度（比如512）上进行，然后直接将预训练好的模型用于不同长度的任务中。大家似乎也没有对这种模式有过怀疑，仿佛模型可以自动泛化到不同长度是一个“理所应当”的能力。  当然，笔者此前同样也没有过类似的质疑，直到前几天笔者做了Base版的GAU实验后才发现GAU的长度泛化能力并不如想象中好。经过进一步分析后，笔者才明白原来这种长..."
  },
  {
    "title": "为什么Pre Norm的效果不如Post Norm？",
    "slug": "为什么pre-norm的效果不如post-norm",
    "path": "blogs_raw/为什么pre-norm的效果不如post-norm.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "梯度",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "Pre Norm与Post Norm之间的对比是一个“老生常谈”的话题了，本博客就多次讨论过这个问题，比如文章[《浅谈Transformer的初始化、参数化与标准化》](/archives/8620)、[《模型优化漫谈：BERT的初始标准差为什么是0.02？》](/archives/8747)等。目前比较明确的结论是：同一设置之下，Pre Norm结构往往更容易训练，但最终效果通常不如Post N..."
  },
  {
    "title": "RoFormerV2：自然语言理解的极限探索",
    "slug": "roformerv2自然语言理解的极限探索",
    "path": "blogs_raw/roformerv2自然语言理解的极限探索.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "预训练",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "大概在1年前，我们提出了[旋转位置编码（RoPE）](/archives/8265)，并发布了对应的预训练模型[RoFormer](https://github.com/ZhuiyiTechnology/roformer)。随着时间的推移，RoFormer非常幸运地得到了越来越多的关注和认可，比如EleutherAI新发布的[60亿](https://github.com/kingoflolz/m..."
  },
  {
    "title": "为什么需要残差？一个来自DeepNet的视角",
    "slug": "为什么需要残差一个来自deepnet的视角",
    "path": "blogs_raw/为什么需要残差一个来自deepnet的视角.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "优化",
      "深度学习",
      "梯度",
      "生成模型"
    ],
    "description": "在[《训练1000层的Transformer究竟有什么困难？》](/archives/8978)中我们介绍了微软提出的能训练1000层Transformer的DeepNet技术。而对于DeepNet，读者一般也有两种反应，一是为此感到惊叹而点赞，另一则是觉得新瓶装旧酒没意思。出现后一种反应的读者，往往是因为DeepNet所提出的两个改进点——增大恒等路径权重和降低残差分支初始化——实在过于稀松平常..."
  },
  {
    "title": "门控注意力单元（GAU）还需要Warmup吗？",
    "slug": "门控注意力单元gau还需要warmup吗",
    "path": "blogs_raw/门控注意力单元gau还需要warmup吗.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "优化",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "在文章[《训练1000层的Transformer究竟有什么困难？》](/archives/8978)发布之后，很快就有读者问到如果将其用到[《FLASH：可能是近来最有意思的高效Transformer设计》](/archives/8934)中的“门控注意力单元（GAU）”，那结果是怎样的？跟标准Transformer的结果有何不同？本文就来讨论这个问题。  ## 先说结论 #  事实上，GAU是非..."
  },
  {
    "title": "训练1000层的Transformer究竟有什么困难？",
    "slug": "训练1000层的transformer究竟有什么困难",
    "path": "blogs_raw/训练1000层的transformer究竟有什么困难.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "梯度",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "众所周知，现在的Transformer越做越大，但这个“大”通常是“宽”而不是“深”，像GPT-3虽然参数有上千亿，但也只是一个96层的Transformer模型，与我们能想象的深度相差甚远。是什么限制了Transformer往“深”发展呢？可能有的读者认为是算力，但“宽而浅”的模型所需的算力不会比“窄而深”的模型少多少，所以算力并非主要限制，归根结底还是Transformer固有的训练困难。一般..."
  },
  {
    "title": "指数梯度下降 + 元学习 = 自适应学习率",
    "slug": "指数梯度下降-元学习-自适应学习率",
    "path": "blogs_raw/指数梯度下降-元学习-自适应学习率.md",
    "status": "completed",
    "date": "",
    "tags": [
      "优化",
      "梯度",
      "优化器",
      "生成模型",
      "attention"
    ],
    "description": "前两天刷到了Google的一篇论文[《Step-size Adaptation Using Exponentiated Gradient Updates》](https://papers.cool/arxiv/2202.00145)，在其中学到了一些新的概念，所以在此记录分享一下。主要的内容有两个，一是非负优化的指数梯度下降，二是基于元学习思想的学习率调整算法，两者都颇有意思，有兴趣的读者也可以了..."
  },
  {
    "title": "FLASH：可能是近来最有意思的高效Transformer设计",
    "slug": "flash可能是近来最有意思的高效transformer设计",
    "path": "blogs_raw/flash可能是近来最有意思的高效transformer设计.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语言模型",
      "生成模型",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "高效Transformer，泛指所有概率Transformer效率的工作，笔者算是关注得比较早了，最早的博客可以追溯到2019年的[《为节约而生：从标准Attention到稀疏Attention》](/archives/6853)，当时做这块的工作很少。后来，这类工作逐渐多了，笔者也跟进了一些，比如[线性Attention](/archives/7546)、[Performer](/archive..."
  },
  {
    "title": "GPLinker：基于GlobalPointer的事件联合抽取",
    "slug": "gplinker基于globalpointer的事件联合抽取",
    "path": "blogs_raw/gplinker基于globalpointer的事件联合抽取.md",
    "status": "completed",
    "date": "",
    "tags": [
      "NLP",
      "信息抽取",
      "NER",
      "生成模型",
      "attention"
    ],
    "description": "大约两年前，笔者在百度的“[2020语言与智能技术竞赛](http://lic2020.cipsc.org.cn/)”中首次接触到了事件抽取任务，并在文章[《bert4keras在手，baseline我有：百度LIC2020》](/archives/7321)中分享了一个转化为BERT+CRF做NER的简单baseline。不过，当时的baseline更像是一个用来凑数的半成品，算不上一个完整的事..."
  },
  {
    "title": "多任务学习漫谈（三）：分主次之序",
    "slug": "多任务学习漫谈三分主次之序",
    "path": "blogs_raw/多任务学习漫谈三分主次之序.md",
    "status": "completed",
    "date": "",
    "tags": [
      "深度学习",
      "损失函数",
      "梯度",
      "多任务",
      "生成模型"
    ],
    "description": "多任务学习是一个很宽泛的命题，不同场景下多任务学习的目标不尽相同。在[《多任务学习漫谈（一）：以损失之名》](/archives/8870)和[《多任务学习漫谈（二）：行梯度之事》](/archives/8896)中，我们将多任务学习的目标理解为“做好每一个任务”，具体表现是“尽量平等地处理每一个任务”，我们可以称之为“平行型多任务学习”。然而，并不是所有多任务学习的目标都是如此，在很多场景下，我..."
  },
  {
    "title": "多任务学习漫谈（二）：行梯度之事",
    "slug": "多任务学习漫谈二行梯度之事",
    "path": "blogs_raw/多任务学习漫谈二行梯度之事.md",
    "status": "completed",
    "date": "",
    "tags": [
      "深度学习",
      "损失函数",
      "梯度",
      "多任务",
      "生成模型"
    ],
    "description": "在[《多任务学习漫谈（一）：以损失之名》](/archives/8870)中，我们从损失函数的角度初步探讨了多任务学习问题，最终发现如果想要结果同时具有缩放不变性和平移不变性，那么用梯度的模长倒数作为任务的权重是一个比较简单的选择。我们继而分析了，该设计等价于将每个任务的梯度单独进行归一化后再相加，这意味着多任务的“战场”从损失函数转移到了梯度之上：看似在设计损失函数，实则在设计更好的梯度，所谓“..."
  },
  {
    "title": "GPLinker：基于GlobalPointer的实体关系联合抽取",
    "slug": "gplinker基于globalpointer的实体关系联合抽取",
    "path": "blogs_raw/gplinker基于globalpointer的实体关系联合抽取.md",
    "status": "completed",
    "date": "",
    "tags": [
      "NLP",
      "信息抽取",
      "NER",
      "生成模型",
      "attention"
    ],
    "description": "在将近三年前的百度“2019语言与智能技术竞赛”（下称LIC2019）中，笔者提出了一个新的关系抽取模型（参考[《基于DGCNN和概率图的轻量级信息抽取模型》](/archives/6671)），后被进一步发表和命名为“[CasRel](https://papers.cool/arxiv/1909.03227)”，算是当时关系抽取的SOTA。然而，CasRel提出时笔者其实也是首次接触该领域，所以..."
  },
  {
    "title": "Efficient GlobalPointer：少点参数，多点效果",
    "slug": "efficient-globalpointer少点参数多点效果",
    "path": "blogs_raw/efficient-globalpointer少点参数多点效果.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "NLP",
      "NER",
      "生成模型",
      "attention"
    ],
    "description": "在[《GlobalPointer：用统一的方式处理嵌套和非嵌套NER》](/archives/8373)中，我们提出了名为“GlobalPointer”的token-pair识别模块，当它用于NER时，能统一处理嵌套和非嵌套任务，并在非嵌套场景有着比CRF更快的速度和不逊色于CRF的效果。换言之，就目前的实验结果来看，至少在NER场景，我们可以放心地将CRF替换为GlobalPointer，而不用..."
  },
  {
    "title": "多任务学习漫谈（一）：以损失之名",
    "slug": "多任务学习漫谈一以损失之名",
    "path": "blogs_raw/多任务学习漫谈一以损失之名.md",
    "status": "completed",
    "date": "",
    "tags": [
      "深度学习",
      "损失函数",
      "多任务",
      "生成模型",
      "attention"
    ],
    "description": "能提升模型性能的方法有很多，多任务学习（Multi-Task Learning）也是其中一种。简单来说，多任务学习是希望将多个相关的任务共同训练，希望不同任务之间能够相互补充和促进，从而获得单任务上更好的效果（准确率、鲁棒性等）。然而，多任务学习并不是所有任务堆起来就能生效那么简单，如何平衡每个任务的训练，使得各个任务都尽量获得有益的提升，依然是值得研究的课题。  最近，笔者机缘巧合之下，也进行了..."
  },
  {
    "title": "CoSENT（二）：特征式匹配与交互式匹配有多大差距？",
    "slug": "cosent二特征式匹配与交互式匹配有多大差距",
    "path": "blogs_raw/cosent二特征式匹配与交互式匹配有多大差距.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型",
      "attention"
    ],
    "description": "一般来说，文本匹配有交互式（Interaction-based）和特征式（Representation-based）两种实现方案，其中交互式是指将两个文本拼接在一起当成单文本进行分类，而特征式则是指两个句子分别由编码器编码为句向量后再做简单的融合处理（算cos值或者接一个浅层网络）。通常的结论是，交互式由于使得两个文本能够进行充分的比较，所以它准确性通常较好，但明显的缺点是在检索场景的效率较差；而..."
  },
  {
    "title": "CoSENT（一）：比Sentence-BERT更有效的句向量方案",
    "slug": "cosent一比sentence-bert更有效的句向量方案",
    "path": "blogs_raw/cosent一比sentence-bert更有效的句向量方案.md",
    "status": "completed",
    "date": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型",
      "attention"
    ],
    "description": "学习句向量的方案大致上可以分为无监督和有监督两大类，其中有监督句向量比较主流的方案是Facebook提出的“[InferSent](https://papers.cool/arxiv/1705.02364)”，而后的“[Sentence-BERT](https://papers.cool/arxiv/1908.10084)”进一步在BERT上肯定了它的有效性。然而，不管是InferSent还是Se..."
  },
  {
    "title": "SquarePlus：可能是运算最简单的ReLU光滑近似",
    "slug": "squareplus可能是运算最简单的relu光滑近似",
    "path": "blogs_raw/squareplus可能是运算最简单的relu光滑近似.md",
    "status": "completed",
    "date": "",
    "tags": [
      "函数",
      "近似",
      "分析",
      "生成模型",
      "attention"
    ],
    "description": "ReLU函数，也就是$\\max(x,0)$，是最常见的激活函数之一，然而它在$x=0$处的不可导通常也被视为一个“槽点”。为此，有诸多的光滑近似被提出，比如SoftPlus、GeLU、Swish等，不过这些光滑近似无一例外地至少都使用了指数运算$e^x$（SoftPlus还用到了对数），从“精打细算”的角度来看，计算量还是不小的（虽然当前在GPU加速之下，我们很少去感知这点计算量了）。最近有一篇论..."
  },
  {
    "title": "概率分布的熵归一化（Entropy Normalization）",
    "slug": "概率分布的熵归一化entropy-normalization",
    "path": "blogs_raw/概率分布的熵归一化entropy-normalization.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "熵",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "在上一篇文章[《从熵不变性看Attention的Scale操作》](/archives/8823)中，我们从熵不变性的角度推导了一个新的Attention Scale，并且实验显示具有熵不变性的新Scale确实能使得Attention的外推性能更好。这时候笔者就有一个很自然的疑问：  > 有没有类似L2 Normalization之类的操作，可以直接对概率分布进行变换，使得保持原始分布主要特性的同..."
  },
  {
    "title": "从熵不变性看Attention的Scale操作",
    "slug": "从熵不变性看attention的scale操作",
    "path": "blogs_raw/从熵不变性看attention的scale操作.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "description": "当前Transformer架构用的最多的注意力机制，全称为“Scaled Dot-Product Attention”，其中“Scaled”是因为在$Q,K$转置相乘之后还要除以一个$\\sqrt{d}$再做Softmax（下面均不失一般性地假设$Q,K,V\\in\\mathbb{R}^{n\\times d}$）：   \\begin{equation}Attention(Q,K,V) = softma..."
  },
  {
    "title": "Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）",
    "slug": "seq2seq前缀树检索任务新范式以kgclue为例",
    "path": "blogs_raw/seq2seq前缀树检索任务新范式以kgclue为例.md",
    "status": "completed",
    "date": "",
    "tags": [
      "代码",
      "语义",
      "keras",
      "相似度",
      "生成模型"
    ],
    "description": "两年前，在[《万能的seq2seq：基于seq2seq的阅读理解问答》](/archives/7115)和[《“非自回归”也不差：基于MLM的阅读理解问答》](/archives/7148)中，我们在尝试过分别利用“Seq2Seq+前缀树”和“MLM+前缀树”的方式做抽取式阅读理解任务，并获得了不错的结果。而在去年的ICLR2021上，Facebook的论文[《Autoregressive Ent..."
  },
  {
    "title": "输入梯度惩罚与参数梯度惩罚的一个不等式",
    "slug": "输入梯度惩罚与参数梯度惩罚的一个不等式",
    "path": "blogs_raw/输入梯度惩罚与参数梯度惩罚的一个不等式.md",
    "status": "completed",
    "date": "",
    "tags": [
      "不等式",
      "优化",
      "梯度",
      "泛化",
      "生成模型"
    ],
    "description": "在本博客中，已经多次讨论过梯度惩罚相关内容了。从形式上来看，梯度惩罚项分为两种，一种是关于输入的梯度惩罚$\\Vert\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x};\\boldsymbol{\\theta})\\Vert^2$，在[《对抗训练浅谈：意义、方法和思考（附Keras实现）》](/archives/7234)、[《泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练..."
  },
  {
    "title": "变分自编码器（八）：估计样本概率密度",
    "slug": "变分自编码器八估计样本概率密度",
    "path": "blogs_raw/变分自编码器八估计样本概率密度.md",
    "status": "completed",
    "date": "",
    "tags": [
      "概率",
      "变分",
      "vae",
      "生成模型",
      "生成模型"
    ],
    "description": "在本系列的前面几篇文章中，我们已经从多个角度来理解了VAE，一般来说，用VAE是为了得到一个生成模型，或者是做更好的编码模型，这都是VAE的常规用途。但除了这些常规应用外，还有一些“小众需求”，比如用来估计$x$的概率密度，这在做压缩的时候通常会用到。  本文就从估计概率密度的角度来了解和推导一下VAE模型。  ## 两个问题 #  所谓估计概率密度，就是在已知样本$x_1,x_2,\\cdots,..."
  },
  {
    "title": "开局一段扯，数据全靠编？真被一篇“神论文”气到了",
    "slug": "开局一段扯数据全靠编真被一篇神论文气到了",
    "path": "blogs_raw/开局一段扯数据全靠编真被一篇神论文气到了.md",
    "status": "completed",
    "date": "",
    "tags": [
      "情感",
      "模型",
      "生成模型",
      "attention",
      "优化"
    ],
    "description": "这篇文章谈一下笔者被昨天出来的一篇“神论文”气到了的经历。  这篇“神论文”是[《How not to Lie with a Benchmark: Rearranging NLP Leaderboards》](https://papers.cool/arxiv/2112.01342)，论文的大致内容是说目前很多排行榜算平均都用算术平均，而它认为几何平均与调和平均更加合理。最关键是它还对GLUE、S..."
  },
  {
    "title": "Dropout视角下的MLM和MAE：一些新的启发",
    "slug": "dropout视角下的mlm和mae一些新的启发",
    "path": "blogs_raw/dropout视角下的mlm和mae一些新的启发.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "概率",
      "分析",
      "优化",
      "生成模型"
    ],
    "description": "大家都知道，BERT的MLM（Masked Language Model）任务在预训练和微调时的不一致，也就是预训练出现了[MASK]而下游任务微调时没有[MASK]，是经常被吐槽的问题，很多工作都认为这是影响BERT微调性能的重要原因，并针对性地提出了很多改进，如[XL-NET](https://papers.cool/arxiv/1906.08237)、[ELECTRA](https://pa..."
  },
  {
    "title": "ChildTuning：试试把Dropout加到梯度上去？",
    "slug": "childtuning试试把dropout加到梯度上去",
    "path": "blogs_raw/childtuning试试把dropout加到梯度上去.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention"
    ],
    "description": "Dropout是经典的防止过拟合的思路了，想必很多读者已经了解过它。有意思的是，最近Dropout有点“老树发新芽”的感觉，出现了一些有趣的新玩法，比如最近引起过热议的[SimCSE](/archives/8348)和[R-Drop](/archives/8496)，尤其是在文章[《又是Dropout两次！这次它做到了有监督任务的SOTA》](/archives/8496)中，我们发现简单的R-D..."
  },
  {
    "title": "WGAN新方案：通过梯度归一化来实现L约束",
    "slug": "wgan新方案通过梯度归一化来实现l约束",
    "path": "blogs_raw/wgan新方案通过梯度归一化来实现l约束.md",
    "status": "completed",
    "date": "",
    "tags": [
      "无监督",
      "GAN",
      "生成模型",
      "生成模型",
      "attention"
    ],
    "description": "当前，WGAN主流的实现方式包括参数裁剪（Weight Clipping）、谱归一化（Spectral Normalization）、梯度惩罚（Gradient Penalty），本来则来介绍一种新的实现方案：梯度归一化（Gradient Normalization），该方案出自两篇有意思的论文，分别是[《Gradient Normalization for Generative Adversar..."
  },
  {
    "title": "模型优化漫谈：BERT的初始标准差为什么是0.02？",
    "slug": "模型优化漫谈bert的初始标准差为什么是002",
    "path": "blogs_raw/模型优化漫谈bert的初始标准差为什么是002.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "分析",
      "优化",
      "梯度",
      "生成模型"
    ],
    "description": "前几天在群里大家讨论到了“Transformer如何解决梯度消失”这个问题，答案有提到残差的，也有提到LN（Layer Norm）的。这些是否都是正确答案呢？事实上这是一个非常有趣而综合的问题，它其实关联到挺多模型细节，比如“BERT为什么要warmup？”、“BERT的初始化标准差为什么是0.02？”、“BERT做MLM预测之前为什么还要多加一层Dense？”，等等。本文就来集中讨论一下这些问题..."
  },
  {
    "title": "bert4keras在手，baseline我有：CLUE基准代码",
    "slug": "bert4keras在手baseline我有clue基准代码",
    "path": "blogs_raw/bert4keras在手baseline我有clue基准代码.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "代码",
      "keras",
      "生成模型",
      "attention"
    ],
    "description": "[CLUE（Chinese GLUE）](https://www.cluebenchmarks.com/)是中文自然语言处理的一个评价基准，目前也已经得到了较多团队的认可。CLUE官方Github提供了tensorflow和pytorch的baseline，但并不易读，而且也不方便调试。事实上，不管是tensorflow还是pytorch，不管是CLUE还是GLUE，笔者认为能找到的baselin..."
  },
  {
    "title": "CAN：借助先验分布提升分类性能的简单后处理技巧",
    "slug": "can借助先验分布提升分类性能的简单后处理技巧",
    "path": "blogs_raw/can借助先验分布提升分类性能的简单后处理技巧.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "概率",
      "分析",
      "技巧",
      "生成模型"
    ],
    "description": "顾名思义，本文将会介绍一种用于分类问题的后处理技巧——CAN（Classification with Alternating Normalization），出自论文[《When in Doubt: Improving Classification Performance with Alternating Normalization》](https://papers.cool/arxiv/2109...."
  },
  {
    "title": "初始化方法中非方阵的维度平均策略思考",
    "slug": "初始化方法中非方阵的维度平均策略思考",
    "path": "blogs_raw/初始化方法中非方阵的维度平均策略思考.md",
    "status": "completed",
    "date": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention"
    ],
    "description": "在[《从几何视角来理解模型参数的初始化策略》](/archives/7180)、[《浅谈Transformer的初始化、参数化与标准化》](/archives/8620)等文章，我们讨论过模型的初始化方法，大致的思路是：如果一个$n\\times n$的方阵用均值为0、方差为$1/n$的独立同分布初始化，那么近似于一个正交矩阵，使得数据二阶矩（或方差）在传播过程中大致保持不变。  那如果是$m\\ti..."
  },
  {
    "title": "用狄拉克函数来构造非光滑函数的光滑近似",
    "slug": "用狄拉克函数来构造非光滑函数的光滑近似",
    "path": "blogs_raw/用狄拉克函数来构造非光滑函数的光滑近似.md",
    "status": "completed",
    "date": "",
    "tags": [
      "函数",
      "近似",
      "分析",
      "光滑",
      "生成模型"
    ],
    "description": "在机器学习中，我们经常会碰到不光滑的函数，但我们的优化方法通常是基于梯度的，这意味着光滑的模型可能更利于优化（梯度是连续的），所以就有了寻找非光滑函数的光滑近似的需求。事实上，本博客已经多次讨论过相关主题，比如[《寻求一个光滑的最大值函数》](/archives/3290)、[《函数光滑化杂谈：不可导函数的可导逼近》](/archives/6620)等，但以往的讨论在方法上并没有什么通用性。  不..."
  },
  {
    "title": "关于WhiteningBERT原创性的疑问和沟通",
    "slug": "关于whiteningbert原创性的疑问和沟通",
    "path": "blogs_raw/关于whiteningbert原创性的疑问和沟通.md",
    "status": "completed",
    "date": "",
    "tags": [
      "情感",
      "模型",
      "工作",
      "生成模型",
      "attention"
    ],
    "description": "在文章[《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》](/archives/8069)中，笔者受到BERT-flow的启发，提出了一种名为BERT-whitening的替代方案，它比BERT-flow更简单，但多数数据集下能取得相近甚至更好的效果，此外它还可以用于对句向量降维以提高检索速度。后来，笔者跟几位合作者一起补充了BERT-whitening的实验，并将其写成了英..."
  }
]