[
  {
    "title": "n个正态随机数的最大值的渐近估计",
    "slug": "n个正态随机数的最大值的渐近估计",
    "path": "blogs_raw/n个正态随机数的最大值的渐近估计.md",
    "status": "completed",
    "date": "2025-11-06",
    "tags": [
      "数学"
    ],
    "description": "设$z_1,z_2,\\cdots,z_n$是$n$个从标准正态分布中独立重复采样出来的随机数，由此我们可以构造出很多衍生随机变量，比如$z_1+z_2+\\cdots+z_n$，它依旧服从正态分布，又比如$z_1^2+z_2^2+\\cdots+z_n^2$，它服从卡方分布。这篇文章我们来关心它的最大值$z_{\\max} = \\max\\\\{z_1,z_2,\\cdots,z_n\\\\}$的分布信息，尤其是..."
  },
  {
    "title": "流形上的最速下降：5. 对偶梯度下降",
    "slug": "流形上的最速下降5-对偶梯度下降",
    "path": "blogs_raw/流形上的最速下降5-对偶梯度下降.md",
    "status": "completed",
    "date": "2025-11-03",
    "tags": [
      "机器学习"
    ],
    "description": "前四篇文章我们求解了几个具体的给参数加等式约束的最速下降问题，其中第三、四篇的问题没法找到解析解，所以笔者提出了相应的不动点迭代法。其中的其中，第三篇文章[《流形上的最速下降：3. Muon + Stiefel》](https://kexue.fm/archives/11221)所研究的“Stiefel流形上的Muon”，问题提出自Jeremy Bernstein的[《Orthogonal man..."
  },
  {
    "title": "低精度Attention可能存在有偏的舍入误差",
    "slug": "低精度attention可能存在有偏的舍入误差",
    "path": "blogs_raw/低精度attention可能存在有偏的舍入误差.md",
    "status": "completed",
    "date": "2025-10-27",
    "tags": [
      "机器学习"
    ],
    "description": "前段时间笔者在arXiv上刷到了论文[《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》](https://arxiv.org/abs/2510.04212)，里面描述的实验现象跟我们在训练[Kimi K2](https://arxiv.org/abs/2507.20534)时出现的一些现象很..."
  },
  {
    "title": "MuP之上：1. 好模型的三个特征",
    "slug": "mup之上1-好模型的三个特征",
    "path": "blogs_raw/mup之上1-好模型的三个特征.md",
    "status": "completed",
    "date": "2025-10-21",
    "tags": [
      "优化"
    ],
    "description": "不知道大家有没有发现一个有趣的细节，Muon和MuP都是“Mu”开头，但两个“Mu”的原意完全不一样，前者是“**M** oment**U** m Orthogonalized by Newton-Schulz”，后者是“**M** aximal **U** pdate Parametrization”，可它们俩之间确实有着非常深刻的联系。也就是说，Muon和MuP有着截然不同的出发点，但最终都走..."
  },
  {
    "title": "随机矩阵的谱范数的快速估计",
    "slug": "随机矩阵的谱范数的快速估计",
    "path": "blogs_raw/随机矩阵的谱范数的快速估计.md",
    "status": "completed",
    "date": "2025-10-12",
    "tags": [
      "机器学习"
    ],
    "description": "在[《高阶MuP：更简明但更高明的谱条件缩放》](https://kexue.fm/archives/10795)的“近似估计”一节中，我们曾“预支”了一个结论：“一个服从标准正态分布的$n\\times m$大小的随机矩阵，它的谱范数大致是$\\sqrt{n}+\\sqrt{m}$。”  这篇文章我们来补充讨论这个结论，给出随机矩阵谱范数的快速估计方法。  ## 随机矩阵论  设有随机矩阵$\\bold..."
  },
  {
    "title": "DiVeQ：一种非常简洁的VQ训练方案",
    "slug": "diveq一种非常简洁的vq训练方案",
    "path": "blogs_raw/diveq一种非常简洁的vq训练方案.md",
    "status": "completed",
    "date": "2025-10-08",
    "tags": [
      "机器学习"
    ],
    "description": "对于坚持离散化路线的研究人员来说，VQ（Vector Quantization）是视觉理解和生成的关键部分，担任着视觉中的“Tokenizer”的角色。它提出在2017年的论文[《Neural Discrete Representation Learning》](https://arxiv.org/abs/1711.00937)，笔者在2019年的博客[《VQ-VAE的简明介绍：量子化自编码器》]..."
  },
  {
    "title": "为什么线性注意力要加Short Conv？",
    "slug": "为什么线性注意力要加short-conv",
    "path": "blogs_raw/为什么线性注意力要加short-conv.md",
    "status": "completed",
    "date": "2025-10-05",
    "tags": [
      "机器学习"
    ],
    "description": "如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考[《线性注意力简史：从模仿、创新到反哺》](https://kexue.fm/archives/11033)）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short Conv，比如下图所示的[DeltaNet](https://arxiv.org/abs..."
  },
  {
    "title": "AdamW的Weight RMS的渐近估计",
    "slug": "adamw的weight-rms的渐近估计",
    "path": "blogs_raw/adamw的weight-rms的渐近估计.md",
    "status": "completed",
    "date": "2025-10-01",
    "tags": [
      "优化"
    ],
    "description": "在[《为什么Adam的Update RMS是0.2？》](https://kexue.fm/archives/11267)中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 [@EIFY](https://x.com/EIFY/status/1965888629814988984) 指出相同的结果已经出现在论文[《Rotational Equilibrium: How Weig..."
  },
  {
    "title": "重新思考学习率与Batch Size（四）：EMA",
    "slug": "重新思考学习率与batch-size四ema",
    "path": "blogs_raw/重新思考学习率与batch-size四ema.md",
    "status": "completed",
    "date": "2025-09-22",
    "tags": [
      "优化"
    ],
    "description": "我们在[《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在[《配置不同的学习率，LoRA还能再涨一点？》](https://kexue.fm/archives/10001)、[《初探MuP..."
  },
  {
    "title": "重新思考学习率与Batch Size（三）：Muon",
    "slug": "重新思考学习率与batch-size三muon",
    "path": "blogs_raw/重新思考学习率与batch-size三muon.md",
    "status": "completed",
    "date": "2025-09-15",
    "tags": [
      "优化"
    ],
    "description": "前两篇文章[《重新思考学习率与Batch Size（一）：现状》](https://kexue.fm/archives/11260)和[《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)中，我们主要是提出了平均场方法，用以简化学习率与Batch Size的相关计算。当时我们分析的优化器是SGD、SignSGD和SoftSign..."
  },
  {
    "title": "n个正态随机数的最大值的渐近估计",
    "slug": "n个正态随机数的最大值的渐近估计-1",
    "path": "blogs_raw/n个正态随机数的最大值的渐近估计-1.md",
    "status": "completed",
    "date": "2025-11-06",
    "tags": [
      "数学"
    ],
    "description": "设$z_1,z_2,\\cdots,z_n$是$n$个从标准正态分布中独立重复采样出来的随机数，由此我们可以构造出很多衍生随机变量，比如$z_1+z_2+\\cdots+z_n$，它依旧服从正态分布，又比如$z_1^2+z_2^2+\\cdots+z_n^2$，它服从卡方分布。这篇文章我们来关心它的最大值$z_{\\max} = \\max\\\\{z_1,z_2,\\cdots,z_n\\\\}$的分布信息，尤其是..."
  },
  {
    "title": "流形上的最速下降：5. 对偶梯度下降",
    "slug": "流形上的最速下降5-对偶梯度下降-1",
    "path": "blogs_raw/流形上的最速下降5-对偶梯度下降-1.md",
    "status": "completed",
    "date": "2025-11-03",
    "tags": [
      "机器学习"
    ],
    "description": "前四篇文章我们求解了几个具体的给参数加等式约束的最速下降问题，其中第三、四篇的问题没法找到解析解，所以笔者提出了相应的不动点迭代法。其中的其中，第三篇文章[《流形上的最速下降：3. Muon + Stiefel》](https://kexue.fm/archives/11221)所研究的“Stiefel流形上的Muon”，问题提出自Jeremy Bernstein的[《Orthogonal man..."
  },
  {
    "title": "低精度Attention可能存在有偏的舍入误差",
    "slug": "低精度attention可能存在有偏的舍入误差-1",
    "path": "blogs_raw/低精度attention可能存在有偏的舍入误差-1.md",
    "status": "completed",
    "date": "2025-10-27",
    "tags": [
      "机器学习"
    ],
    "description": "前段时间笔者在arXiv上刷到了论文[《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》](https://arxiv.org/abs/2510.04212)，里面描述的实验现象跟我们在训练[Kimi K2](https://arxiv.org/abs/2507.20534)时出现的一些现象很..."
  },
  {
    "title": "MuP之上：1. 好模型的三个特征",
    "slug": "mup之上1-好模型的三个特征-1",
    "path": "blogs_raw/mup之上1-好模型的三个特征-1.md",
    "status": "completed",
    "date": "2025-10-21",
    "tags": [
      "优化"
    ],
    "description": "不知道大家有没有发现一个有趣的细节，Muon和MuP都是“Mu”开头，但两个“Mu”的原意完全不一样，前者是“**M** oment**U** m Orthogonalized by Newton-Schulz”，后者是“**M** aximal **U** pdate Parametrization”，可它们俩之间确实有着非常深刻的联系。也就是说，Muon和MuP有着截然不同的出发点，但最终都走..."
  },
  {
    "title": "随机矩阵的谱范数的快速估计",
    "slug": "随机矩阵的谱范数的快速估计-1",
    "path": "blogs_raw/随机矩阵的谱范数的快速估计-1.md",
    "status": "completed",
    "date": "2025-10-12",
    "tags": [
      "机器学习"
    ],
    "description": "在[《高阶MuP：更简明但更高明的谱条件缩放》](https://kexue.fm/archives/10795)的“近似估计”一节中，我们曾“预支”了一个结论：“一个服从标准正态分布的$n\\times m$大小的随机矩阵，它的谱范数大致是$\\sqrt{n}+\\sqrt{m}$。”  这篇文章我们来补充讨论这个结论，给出随机矩阵谱范数的快速估计方法。  ## 随机矩阵论  设有随机矩阵$\\bold..."
  },
  {
    "title": "DiVeQ：一种非常简洁的VQ训练方案",
    "slug": "diveq一种非常简洁的vq训练方案-1",
    "path": "blogs_raw/diveq一种非常简洁的vq训练方案-1.md",
    "status": "completed",
    "date": "2025-10-08",
    "tags": [
      "机器学习"
    ],
    "description": "对于坚持离散化路线的研究人员来说，VQ（Vector Quantization）是视觉理解和生成的关键部分，担任着视觉中的“Tokenizer”的角色。它提出在2017年的论文[《Neural Discrete Representation Learning》](https://arxiv.org/abs/1711.00937)，笔者在2019年的博客[《VQ-VAE的简明介绍：量子化自编码器》]..."
  },
  {
    "title": "为什么线性注意力要加Short Conv？",
    "slug": "为什么线性注意力要加short-conv-1",
    "path": "blogs_raw/为什么线性注意力要加short-conv-1.md",
    "status": "completed",
    "date": "2025-10-05",
    "tags": [
      "机器学习"
    ],
    "description": "如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考[《线性注意力简史：从模仿、创新到反哺》](https://kexue.fm/archives/11033)）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short Conv，比如下图所示的[DeltaNet](https://arxiv.org/abs..."
  },
  {
    "title": "AdamW的Weight RMS的渐近估计",
    "slug": "adamw的weight-rms的渐近估计-1",
    "path": "blogs_raw/adamw的weight-rms的渐近估计-1.md",
    "status": "completed",
    "date": "2025-10-01",
    "tags": [
      "优化"
    ],
    "description": "在[《为什么Adam的Update RMS是0.2？》](https://kexue.fm/archives/11267)中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 [@EIFY](https://x.com/EIFY/status/1965888629814988984) 指出相同的结果已经出现在论文[《Rotational Equilibrium: How Weig..."
  },
  {
    "title": "重新思考学习率与Batch Size（四）：EMA",
    "slug": "重新思考学习率与batch-size四ema-1",
    "path": "blogs_raw/重新思考学习率与batch-size四ema-1.md",
    "status": "completed",
    "date": "2025-09-22",
    "tags": [
      "优化"
    ],
    "description": "我们在[《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在[《配置不同的学习率，LoRA还能再涨一点？》](https://kexue.fm/archives/10001)、[《初探MuP..."
  },
  {
    "title": "重新思考学习率与Batch Size（三）：Muon",
    "slug": "重新思考学习率与batch-size三muon-1",
    "path": "blogs_raw/重新思考学习率与batch-size三muon-1.md",
    "status": "completed",
    "date": "2025-09-15",
    "tags": [
      "优化"
    ],
    "description": "前两篇文章[《重新思考学习率与Batch Size（一）：现状》](https://kexue.fm/archives/11260)和[《重新思考学习率与Batch Size（二）：平均场》](https://kexue.fm/archives/11280)中，我们主要是提出了平均场方法，用以简化学习率与Batch Size的相关计算。当时我们分析的优化器是SGD、SignSGD和SoftSign..."
  }
]