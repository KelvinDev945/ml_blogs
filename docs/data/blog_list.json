[
  {
    "slug": "n个正态随机数的最大值的渐近估计",
    "title": "n个正态随机数的最大值的渐近估计",
    "description": "n个正态随机数的最大值的渐近估计&para;\n设$z_1,z_2,\\cdots,z_n$是$n$个从标准正态分布中独立重复采样出来的随机数，由此我们可以构造出很多衍生随机变量，比如$z_1+z_2+\\cdots+z_n$，它依旧服从正态分布，又比如$z_1^2+z_2^2+\\cdots+z_n^2$，它服从卡方分布。这篇文章我们来关心它的最大值$z_{\\max} = \\max\\{z_1,z_2,\\...",
    "date": "2025-11-06",
    "source": "https://spaces.ac.cn/archives/11390",
    "tags": [
      "数学",
      "概率论",
      "极值统计",
      "渐近分析"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 356
  },
  {
    "slug": "流形上的最速下降5-对偶梯度下降",
    "title": "流形上的最速下降：5. 对偶梯度下降",
    "description": "流形上的最速下降：5. 对偶梯度下降&para;\n原文链接: https://spaces.ac.cn/archives/11388\n发布日期: 2025-11-03\n\n前四篇文章我们求解了几个具体的给参数加等式约束的最速下降问题，其中第三、四篇的问题没法找到解析解，所以笔者提出了相应的不动点迭代法。其中的其中，第三篇文章《流形上的最速下降：3. Muon + Stiefel》所研究的“Stief...",
    "date": "2025-11-03",
    "source": "https://spaces.ac.cn/archives/11388",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 355
  },
  {
    "slug": "低精度attention可能存在有",
    "title": "低精度Attention可能存在有...",
    "description": "低精度Attention可能存在有...&para;\n原文链接: https://spaces.ac.cn/archives/11371\n发布日期: \n\n前段时间笔者在arXiv上刷到了论文《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》，里面描述的实验现象跟我们在训练Kimi K2时出现的...",
    "date": "2025-10-27",
    "source": "",
    "tags": [
      "近似",
      "分析",
      "优化",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 353
  },
  {
    "slug": "低精度attention可能存在有偏的舍入误差",
    "title": "低精度Attention可能存在有偏的舍入误差",
    "description": "低精度Attention可能存在有...&para;\n原文链接: https://spaces.ac.cn/archives/11371\n发布日期: \n\n前段时间笔者在arXiv上刷到了论文《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》，里面描述的实验现象跟我们在训练Kimi K2时出现的...",
    "date": "2025-10-27",
    "source": "https://spaces.ac.cn/archives/11371",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 354
  },
  {
    "slug": "mup之上1-好模型的三个特征",
    "title": "MuP之上：1. 好模型的三个特征",
    "description": "MuP之上：1. 好模型的三个特征&para;\n原文链接: https://spaces.ac.cn/archives/11340\n发布日期: 2025-10-21\n\n📄 引言&para;\n不知道大家有没有发现一个有趣的细节，Muon和MuP都是\"Mu\"开头，但两个\"Mu\"的原意完全不一样，前者是\"MomentUm Orthogonalized by Newton-Schulz\"，后者是\"Maxi...",
    "date": "2025-10-21",
    "source": "https://spaces.ac.cn/archives/11340",
    "tags": [
      "优化",
      "MuP",
      "Muon",
      "Scaling Law",
      "训练稳定性"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 352
  },
  {
    "slug": "随机矩阵的谱范数的快速估计",
    "title": "随机矩阵的谱范数的快速估计",
    "description": "随机矩阵的谱范数的快速估计&para;\n原文链接: https://spaces.ac.cn/archives/11335\n发布日期: 2025-10-12\n\n在《高阶MuP：更简明但更高明的谱条件缩放》的“近似估计”一节中，我们曾“预支”了一个结论：“一个服从标准正态分布的$n\\times m$大小的随机矩阵，它的谱范数大致是$\\sqrt{n}+\\sqrt{m}$。”\n这篇文章我们来补充讨论这个...",
    "date": "2025-10-12",
    "source": "https://spaces.ac.cn/archives/11335",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 351
  },
  {
    "slug": "diveq一种非常简洁的vq训练方案",
    "title": "DiVeQ：一种非常简洁的VQ训练方案",
    "description": "DiVeQ：一种非常简洁的VQ训练方案&para;\n原文链接: https://spaces.ac.cn/archives/11328\n发布日期: 2025-10-08\n\n对于坚持离散化路线的研究人员来说，VQ（Vector Quantization）是视觉理解和生成的关键部分，担任着视觉中的“Tokenizer”的角色。它提出在2017年的论文《Neural Discrete Represent...",
    "date": "2025-10-08",
    "source": "https://spaces.ac.cn/archives/11328",
    "tags": [
      "详细推导",
      "机器学习",
      "向量量化",
      "VQ",
      "重参数化",
      "Gumbel-Softmax",
      "生成模型",
      "离散化",
      "梯度估计"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 350
  },
  {
    "slug": "为什么线性注意力要加short-c",
    "title": "为什么线性注意力要加Short C...",
    "description": "为什么线性注意力要加Short C...&para;\n原文链接: https://spaces.ac.cn/archives/11320\n发布日期: \n\n如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考《线性注意力简史：从模仿、创新到反哺》）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short Co...",
    "date": "2025-10-05",
    "source": "",
    "tags": [
      "线性",
      "RNN",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 348
  },
  {
    "slug": "为什么线性注意力要加short-conv",
    "title": "为什么线性注意力要加Short Conv？",
    "description": "为什么线性注意力要加Short C...&para;\n原文链接: https://spaces.ac.cn/archives/11320\n发布日期: \n\n如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考《线性注意力简史：从模仿、创新到反哺》）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short Co...",
    "date": "2025-10-05",
    "source": "https://spaces.ac.cn/archives/11320",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 349
  },
  {
    "slug": "adamw的weight-rms的",
    "title": "AdamW的Weight RMS的...",
    "description": "AdamW的Weight RMS的...&para;\n原文链接: https://spaces.ac.cn/archives/11307\n发布日期: \n\n在《为什么Adam的Update RMS是0.2？》中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 @EIFY 指出相同的结果已经出现在论文《Rotational Equilibrium: How Weight Decay...",
    "date": "2025-10-01",
    "source": "",
    "tags": [
      "估计",
      "梯度",
      "优化器",
      "平均场",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 346
  },
  {
    "slug": "adamw的weight-rms的渐近估计",
    "title": "AdamW的Weight RMS的渐近估计",
    "description": "AdamW的Weight RMS的...&para;\n原文链接: https://spaces.ac.cn/archives/11307\n发布日期: \n\n在《为什么Adam的Update RMS是0.2？》中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 @EIFY 指出相同的结果已经出现在论文《Rotational Equilibrium: How Weight Decay...",
    "date": "2025-10-01",
    "source": "https://spaces.ac.cn/archives/11307",
    "tags": [
      "详细推导",
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 347
  },
  {
    "slug": "重新思考学习率与batch-size四ema",
    "title": "重新思考学习率与Batch Size（四）：EMA",
    "description": "重新思考学习率与Batch Size（四）：EMA&para;\n原文链接: https://spaces.ac.cn/archives/11301\n发布日期: 2025-09-22\n\n我们在《重新思考学习率与Batch Size（二）：平均场》中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在《配置不同的学习...",
    "date": "2025-09-22",
    "source": "https://spaces.ac.cn/archives/11301",
    "tags": [
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 344
  },
  {
    "slug": "重新思考学习率与batch-siz",
    "title": "重新思考学习率与Batch Siz...",
    "description": "重新思考学习率与Batch Siz...&para;\n原文链接: https://spaces.ac.cn/archives/11301\n发布日期: \n\n我们在《重新思考学习率与Batch Size（二）：平均场》中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在《配置不同的学习率，LoRA还能再涨一点？》、...",
    "date": "2025-09-22",
    "source": "",
    "tags": [
      "学习率",
      "优化器",
      "尺度定律",
      "平均场",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 345
  },
  {
    "slug": "重新思考学习率与batch-size三muon",
    "title": "重新思考学习率与Batch Size（三）：Muon",
    "description": "重新思考学习率与Batch Size（三）：Muon&para;\n原文链接: https://spaces.ac.cn/archives/11285\n发布日期: 2025-09-15\n\n前两篇文章《重新思考学习率与Batch Size（一）：现状》和《重新思考学习率与Batch Size（二）：平均场》中，我们主要是提出了平均场方法，用以简化学习率与Batch Size的相关计算。当时我们分析的优...",
    "date": "2025-09-15",
    "source": "https://spaces.ac.cn/archives/11285",
    "tags": [
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 343
  },
  {
    "slug": "重新思考学习率与batch-size二平均场",
    "title": "重新思考学习率与Batch Size（二）：平均场",
    "description": "重新思考学习率与Batch Size（二）：平均场&para;\n原文链接: https://spaces.ac.cn/archives/11280\n发布日期: \n\n上文《重新思考学习率与Batch Size（一）：现状》末尾我们说到，对于SignSGD、SoftSignSGD等$\\tilde{\\boldsymbol{\\varphi}}_B$非线性依赖于$\\tilde{\\boldsymbol{g}}...",
    "date": "2025-09-10",
    "source": "",
    "tags": [
      "详细推导",
      "学习率",
      "优化器",
      "尺度定律",
      "平均场",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 342
  },
  {
    "slug": "为什么adam的update-rms是02",
    "title": "为什么Adam的Update RMS是0.2？",
    "description": "为什么Adam的Update RMS是0.2？&para;\n原文链接: https://spaces.ac.cn/archives/11267\n发布日期: \n\n众所周知，我们很早就开始尝试将Muon用于大规模LLM的训练。特别地，在《Muon续集：为什么我们选择尝试Muon？》中，我们提出了“Match Adam Update RMS”的技巧，以便快速从Adam迁移到Muon上，这个技巧同样用到了...",
    "date": "2025-09-02",
    "source": "",
    "tags": [
      "详细推导",
      "分析",
      "梯度",
      "优化器",
      "平均场",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 341
  },
  {
    "slug": "重新思考学习率与batch-size一现状",
    "title": "重新思考学习率与Batch Size（一）：现状",
    "description": "重新思考学习率与Batch Size（一）：现状&para;\n原文链接: https://spaces.ac.cn/archives/11260\n发布日期: \n\n在之前的文章《当Batch Size增大时，学习率该如何随之变化？》和《Adam的epsilon如何影响学习率的Scaling Law？》中，我们从理论上讨论了学习率随Batch Size的变化规律，其中比较经典的部分是由OpenAI提出...",
    "date": "2025-09-01",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 340
  },
  {
    "slug": "流形上的最速下降4-muon-谱球面",
    "title": "流形上的最速下降：4. Muon + 谱球面",
    "description": "流形上的最速下降：4. Muon + 谱球面&para;\n原文链接: https://spaces.ac.cn/archives/11241\n发布日期: \n\n看完了前三篇的读者，想必已经熟悉本系列的“套路”——先提出更新量的约束，寻找最速下降方向，接着再给参数也加上约束，寻找新的最速下降方向。在求解参数约束问题时，我们采用的是“一阶近似够用”原则来简化约束形式，这在几何上对应于“切空间”。然后，我...",
    "date": "2025-08-21",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 339
  },
  {
    "slug": "relugeluswish的一个恒等式",
    "title": "ReLU/GeLU/Swish的一个恒等式",
    "description": "ReLU/GeLU/Swish的一个恒等式&para;\n原文链接: https://spaces.ac.cn/archives/11233\n发布日期: \n\n今天水一点轻松的内容，它基于笔者这两天意识到的一个恒等式。这个恒等式实际上很简单，但初看之下会有点意料之外的感觉，所以来记录一下。\n基本结果&para;\n我们知道$\\newcommand{relu}{\\mathop{\\text{relu}}}\\...",
    "date": "2025-08-16",
    "source": "",
    "tags": [
      "分析",
      "神经网络",
      "恒等式",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 338
  },
  {
    "slug": "流形上的最速下降3-muon-stiefel",
    "title": "流形上的最速下降：3. Muon + Stiefel",
    "description": "流形上的最速下降：3. Muon + Stiefel&para;\n原文链接: https://spaces.ac.cn/archives/11221\n发布日期: \n\n上回说到，当我们把优化对象从向量参数转移到矩阵参数，并选用更适合矩阵的谱范数约束后，Muon优化器便自然而然地出现了。进一步地，我们考虑了给参数加上正交约束后的最速下降方向，这其中又分方阵和非方阵两部分讨论，其中方阵的求解我们在上一篇...",
    "date": "2025-08-08",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 337
  },
  {
    "slug": "流形上的最速下降2-muon-正交",
    "title": "流形上的最速下降：2. Muon + 正交",
    "description": "流形上的最速下降：2. Muon + 正交&para;\n原文链接: https://spaces.ac.cn/archives/11215\n发布日期: \n\n本文继续我们的约束优化系列。在上文《流形上的最速下降：1. SGD + 超球面》中，我们重温了优化器的“最小作用量”原理，提出不同优化器的核心差异在于给更新量施加的不同约束，如果这个约束是欧几里得范数，那么对应的最速下降便是SGD。进一步地，我...",
    "date": "2025-08-06",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 336
  },
  {
    "slug": "流形上的最速下降1-sgd-超球面",
    "title": "流形上的最速下降：1.  SGD + 超球面",
    "description": "流形上的最速下降：1.  SGD + 超球面&para;\n原文链接: https://spaces.ac.cn/archives/11196\n发布日期: \n\n类似“梯度的反方向是下降最快的方向”的描述，经常用于介绍梯度下降（SGD）的原理。然而，这句话是有条件的，比如“方向”在数学上是单位向量，它依赖于“范数（模长）”的定义，不同范数的结论也不同，Muon实际上就是给矩阵参数换了个谱范数，从而得到...",
    "date": "2025-08-01",
    "source": "",
    "tags": [
      "详细推导",
      "不等式",
      "优化器",
      "约束",
      "最速下降",
      "生成模型",
      "流形优化",
      "黎曼几何",
      "微分几何",
      "投影梯度",
      "理论分析"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 335
  },
  {
    "slug": "矩阵r次方根和逆r次方根的高效计算",
    "title": "矩阵r次方根和逆r次方根的高效计算",
    "description": "矩阵r次方根和逆r次方根的高效计算&para;\n原文链接: https://spaces.ac.cn/archives/11175\n发布日期: \n\n上一篇文章《矩阵平方根和逆平方根的高效计算》中，笔者从$\\newcommand{mcsgn}{\\mathop{\\text{mcsgn}}}\\mcsgn$算子出发，提出了一种很漂亮的矩阵平方根和逆平方根的计算方法。比较神奇的是，该方案经过化简之后，最终公...",
    "date": "2025-07-21",
    "source": "",
    "tags": [
      "详细推导",
      "代数",
      "迭代",
      "矩阵",
      "线性",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 334
  },
  {
    "slug": "矩阵平方根和逆平方根的高效计算",
    "title": "矩阵平方根和逆平方根的高效计算",
    "description": "矩阵平方根和逆平方根的高效计算&para;\n原文链接: https://spaces.ac.cn/archives/11158\n发布日期: \n\n设$\\boldsymbol{P}\\in\\mathbb{R}^{n\\times n}$是一个特征值都是非负实数的$n$阶方阵，本文来讨论它的平方根$\\boldsymbol{P}^{1/2}$和逆平方根$\\boldsymbol{P}^{-1/2}$的计算。\n基...",
    "date": "2025-07-19",
    "source": "",
    "tags": [
      "详细推导",
      "代数",
      "迭代",
      "矩阵",
      "线性",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 333
  },
  {
    "slug": "qk-clip让muon在scaleup之路上更进一步",
    "title": "QK-Clip：让Muon在Scaleup之路上更进一步",
    "description": "QK-Clip：让Muon在Scaleup之路上更进一步&para;\n原文链接: https://spaces.ac.cn/archives/11126\n发布日期: \n\n四个月前，我们发布了Moonlight，在16B的MoE模型上验证了Muon优化器的有效性。在Moonlight中，我们确认了给Muon添加Weight Decay的必要性，同时提出了通过Update RMS对齐来迁移Adam超参...",
    "date": "2025-07-12",
    "source": "",
    "tags": [
      "优化",
      "attention",
      "优化器",
      "muon",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 332
  },
  {
    "slug": "transformer升级之路21mla好在哪里下",
    "title": "Transformer升级之路：21、MLA好在哪里?（下）",
    "description": "Transformer升级之路：21、MLA好在哪里?（下）&para;\n原文链接: https://spaces.ac.cn/archives/11111\n发布日期: \n\n在文章《Transformer升级之路：20、MLA好在哪里?（上）》中，我们对MLA相比常见MHA、GQA、MQA的一些变化分别做了消融实验，其中的变化包括“增大head_dims”、“Partial RoPE”和“KV共享...",
    "date": "2025-07-10",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 331
  },
  {
    "slug": "对角低秩三角阵的高效求逆方法",
    "title": "“对角+低秩”三角阵的高效求逆方法",
    "description": "“对角+低秩”三角阵的高效求逆方法&para;\n原文链接: https://spaces.ac.cn/archives/11072\n发布日期: \n\n从文章《线性注意力简史：从模仿、创新到反哺》我们可以发现，DeltaNet及其后的线性Attention模型，基本上都关联到了逆矩阵$(\\boldsymbol{I} + \\boldsymbol{K}\\boldsymbol{K}^{\\top}\\odot\\...",
    "date": "2025-07-01",
    "source": "",
    "tags": [
      "详细推导",
      "计算",
      "矩阵",
      "RNN",
      "attention",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 330
  },
  {
    "slug": "通过msign来计算奇异值裁剪mclip下",
    "title": "通过msign来计算奇异值裁剪mclip（下）",
    "description": "通过msign来计算奇异值裁剪mclip（下）&para;\n原文链接: https://spaces.ac.cn/archives/11059\n发布日期: \n\n前面我们在《通过msign来计算奇异值裁剪mclip（上）》讨论了奇异值裁剪$\\newcommand{mclip}{\\mathop{\\text{mclip}}}\\mclip$的数值计算，核心思路来自 @leloykun 的文章《Numeri...",
    "date": "2025-06-23",
    "source": "",
    "tags": [
      "详细推导",
      "迭代",
      "近似",
      "矩阵",
      "SVD",
      "muon"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 328
  },
  {
    "slug": "矩阵符号函数mcsgn能计算什么",
    "title": "矩阵符号函数mcsgn能计算什么？",
    "description": "矩阵符号函数mcsgn能计算什么？&para;\n原文链接: https://spaces.ac.cn/archives/11056\n发布日期: \n\n在《msign的导数》一文中，我们正式引入了两种矩阵符号函数$\\newcommand{msign}{\\mathop{\\text{msign}}}\\msign$和$\\newcommand{mcsgn}{\\mathop{\\text{mcsgn}}}\\mcs...",
    "date": "2025-06-23",
    "source": "",
    "tags": [
      "代数",
      "矩阵",
      "线性",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 329
  },
  {
    "slug": "msign的导数",
    "title": "msign的导数",
    "description": "msign的导数&para;\n原文链接: https://spaces.ac.cn/archives/11025\n发布日期: \n\n这篇文章我们来推导$\\newcommand{msign}{\\mathop{\\text{msign}}}\\msign$算子的求导公式。如果读者想要像《Test-Time Training Done Right》一样，将TTT和Muon结合起来，那么本文可能会对你有帮助。...",
    "date": "2025-06-13",
    "source": "",
    "tags": [
      "微积分",
      "矩阵",
      "梯度",
      "muon",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 327
  },
  {
    "slug": "通过msign来计算奇异值裁剪mclip上",
    "title": "通过msign来计算奇异值裁剪mclip（上）",
    "description": "通过msign来计算奇异值裁剪mclip（上）&para;\n原文链接: https://spaces.ac.cn/archives/11006\n发布日期: \n\n前面我们用了两篇文章《msign算子的Newton-Schulz迭代（上）》和《msign算子的Newton-Schulz迭代（下）》讨论了矩阵的$\\newcommand{msign}{\\mathop{\\text{msign}}}\\newc...",
    "date": "2025-06-07",
    "source": "",
    "tags": [
      "迭代",
      "近似",
      "矩阵",
      "SVD",
      "muon"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 326
  },
  {
    "slug": "msign算子的newton-schulz迭代下",
    "title": "msign算子的Newton-Schulz迭代（下）",
    "description": "msign算子的Newton-Schulz迭代（下）&para;\n原文链接: https://spaces.ac.cn/archives/10996\n发布日期: \n\n在上文《msign算子的Newton-Schulz迭代（上）》中，我们试图为$\\mathop{\\text{msign}}$算子寻找更好的Newton-Schulz迭代，以期在有限迭代步数内能达到尽可能高的近似程度，这一过程又可以转化为...",
    "date": "2025-06-05",
    "source": "",
    "tags": [
      "迭代",
      "近似",
      "优化器",
      "muon",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 325
  },
  {
    "slug": "等值振荡定理最优多项式逼近的充要条件",
    "title": "等值振荡定理：最优多项式逼近的充要条件",
    "description": "等值振荡定理：最优多项式逼近的充要条件&para;\n原文链接: https://spaces.ac.cn/archives/10972\n发布日期: 2025-06-02\n本文深度扩充版本 | 包含理论基础、完整推导、多角度理解、批判性分析、未来方向\n\n摘要&para;\n本文系统阐述了Chebyshev等值振荡定理，一个数学逼近论中最优美的结果之一。等值振荡定理刻画了最优多项式逼近的充要条件：最优逼...",
    "date": "2025-06-02",
    "source": "",
    "tags": [
      "导数",
      "近似",
      "最优",
      "分析",
      "数值逼近",
      "Chebyshev",
      "切比雪夫",
      "逼近论"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 324
  },
  {
    "slug": "生成扩散模型漫谈三十从瞬时速度到平均速度",
    "title": "生成扩散模型漫谈（三十）：从瞬时速度到平均速度",
    "description": "生成扩散模型漫谈（三十）：从瞬时速度到平均速度&para;\n原文链接: https://spaces.ac.cn/archives/10958\n发布日期: \n\n众所周知，生成速度慢是扩散模型一直以来的痛点，而为了解决这个问题，大家可谓“八仙过海，各显神通”，提出了各式各样的解决方案，然而长久以来并没一项工作能够脱颖而出，成为标配。什么样的工作能够达到这个标准呢？在笔者看来，它至少满足几个条件：...",
    "date": "2025-05-26",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 323
  },
  {
    "slug": "moe环游记5均匀分布的反思",
    "title": "MoE环游记：5、均匀分布的反思",
    "description": "MoE环游记：5、均匀分布的反思&para;\n原文链接: https://spaces.ac.cn/archives/10945\n发布日期: \n\n如果说Meta的LLAMA系列为Dense模型确立了标准架构，那么DeepSeek或许就是MoE标准架构的奠基者。当然，这并非指DeepSeek首创了MoE，也不是说它的MoE不可超越，而是指DeepSeek对MoE所提的一些改进，很可能都是效果增益比较...",
    "date": "2025-05-16",
    "source": "",
    "tags": [
      "优化",
      "稀疏",
      "moe",
      "生成模型",
      "attention",
      "Shared Expert",
      "Fine-Grained",
      "非均匀性",
      "Zipf定律",
      "负载均衡",
      "层次化路由"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 322
  },
  {
    "slug": "msign算子的newton-schulz迭代上",
    "title": "msign算子的Newton-Schulz迭代（上）",
    "description": "msign算子的Newton-Schulz迭代（上）&para;\n原文链接: https://spaces.ac.cn/archives/10922\n发布日期: \n\n在之前的《Muon优化器赏析：从向量到矩阵的本质跨越》、《Muon续集：为什么我们选择尝试Muon？》等文章中，我们介绍了一个极具潜力、有望替代Adam的新兴优化器——“Muon”。随着相关研究的不断深入，Muon优化器受到的关注度也...",
    "date": "2025-05-11",
    "source": "",
    "tags": [
      "迭代",
      "近似",
      "优化器",
      "muon",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 321
  },
  {
    "slug": "transformer升级之路20mla好在哪里上",
    "title": "Transformer升级之路：20、MLA好在哪里?（上）",
    "description": "Transformer升级之路：20、MLA好在哪里?（上）&para;\n原文链接: https://spaces.ac.cn/archives/10907\n发布日期: \n\n自从DeepSeek爆火后，它所提的Attention变体MLA（M ulti-head L atent A ttention）也愈发受到关注。MLA通过巧妙的设计实现了MHA与MQA的自由切换，使得模型可以根据训练和推理的不...",
    "date": "2025-05-04",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 320
  },
  {
    "slug": "一道概率不等式盯着它到显然成立为止",
    "title": "一道概率不等式：盯着它到显然成立为止！",
    "description": "一道概率不等式：盯着它到显然成立为止！&para;\n原文链接: https://spaces.ac.cn/archives/10902\n发布日期: \n\n前两天，QQ群里有群友抛出了一道不等式求证：  \n\n一道概率相关的不等式，出自《There is no fast single hashing algorithm》\n简短的题目，加上“easily”的提示，让人觉得这似乎是显然成立的结果，然而提问者...",
    "date": "2025-04-30",
    "source": "",
    "tags": [
      "不等式",
      "概率",
      "显然成立",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 319
  },
  {
    "slug": "svd的导数",
    "title": "SVD的导数",
    "description": "SVD的导数&para;\n原文链接: https://spaces.ac.cn/archives/10878\n发布日期: \n\nSVD（Singular Value Decomposition，奇异值分解）是常见的矩阵分解算法，相信很多读者都已经对它有所了解，此前我们在《低秩近似之路（二）：SVD》也专门介绍过它。然而，读者是否想到，SVD竟然还可以求导呢？笔者刚了解到这一结论时也颇感意外，因为直觉...",
    "date": "2025-04-26",
    "source": "",
    "tags": [
      "详细推导",
      "微积分",
      "分析",
      "矩阵",
      "SVD",
      "梯度"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 318
  },
  {
    "slug": "矩阵的有效秩effective-rank",
    "title": "矩阵的有效秩（Effective Rank）",
    "description": "矩阵的有效秩（Effective Rank）&para;\n原文链接: https://spaces.ac.cn/archives/10847\n发布日期: \n\n秩（Rank）是线性代数中的重要概念，它代表了矩阵的内在维度。然而，数学上对秩的严格定义，很多时候并不完全适用于数值计算场景，因为秩等于非零奇异值的个数，而数学上对“等于零”这件事的理解跟数值计算有所不同，数学上的“等于零”是绝对地、严格地等...",
    "date": "2025-04-10",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "熵",
      "稀疏",
      "低秩",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 317
  },
  {
    "slug": "通过梯度近似寻找normalization的替代品",
    "title": "通过梯度近似寻找Normalization的替代品",
    "description": "通过梯度近似寻找Normalization的替代品&para;\n原文链接: https://spaces.ac.cn/archives/10831\n发布日期: \n\n不知道大家有没有留意到前段时间的《Transformers without Normalization》？这篇论文试图将Transformer模型中的Normalization层用一个Element-wise的运算DyT替代，以期能提高...",
    "date": "2025-04-02",
    "source": "",
    "tags": [
      "详细推导",
      "函数",
      "分析",
      "梯度",
      "光滑",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 316
  },
  {
    "slug": "moe环游记4难处应当多投入",
    "title": "MoE环游记：4、难处应当多投入",
    "description": "MoE环游记：4、难处应当多投入&para;\n原文链接: https://spaces.ac.cn/archives/10815\n发布日期: \n\n前两篇文章我们都在讨论负载均衡，其中在《MoE环游记：3、换个思路来分配》介绍Loss-Free方案时，笔者留了一个悬念：它引入的Bias项有一个冗余的自由度，这个自由度可以用来做另外有趣的事情。这篇文章我们就来讨论这件事。\n我们知道，MoE是为每个To...",
    "date": "2025-03-28",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "梯度",
      "moe",
      "动态",
      "生成模型",
      "难度感知",
      "动态Expert",
      "资源分配",
      "课程学习",
      "预算控制"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 315
  },
  {
    "slug": "高阶mup更简明但更高明的谱条件缩放",
    "title": "高阶MuP：更简明但更高明的谱条件缩放",
    "description": "高阶MuP：更简明但更高明的谱条件缩放&para;\n原文链接: https://spaces.ac.cn/archives/10795\n发布日期: \n\n在文章《初探MuP：超参数的跨模型尺度迁移规律》中，我们基于前向传播、反向传播、损失增量和特征变化的尺度不变性推导了MuP（Maximal Update Parametrization）。可能对于部分读者来说，这一过程还是显得有些繁琐，但实际上它比...",
    "date": "2025-03-24",
    "source": "",
    "tags": [
      "详细推导",
      "LoRA",
      "梯度",
      "优化器",
      "尺度定律",
      "谱范数"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 314
  },
  {
    "slug": "初探mup超参数的跨模型尺度迁移规律",
    "title": "初探MuP：超参数的跨模型尺度迁移规律",
    "description": "初探MuP：超参数的跨模型尺度迁移规律&para;\n原文链接: https://spaces.ac.cn/archives/10770\n发布日期: \n\n众所周知，完整训练一次大型LLM的成本是昂贵的，这就决定了我们不可能直接在大型LLM上反复测试超参数。一个很自然的想法是希望可以在同结构的小模型上仔细搜索超参数，找到最优组合后直接迁移到大模型上。尽管这个想法很朴素，但要实现它并不平凡，它需要我们了...",
    "date": "2025-03-13",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 313
  },
  {
    "slug": "moe环游记3换个思路来分配",
    "title": "MoE环游记：3、换个思路来分配",
    "description": "MoE环游记：3、换个思路来分配&para;\n原文链接: https://spaces.ac.cn/archives/10757\n发布日期: \n\n这篇文章我们继续探讨MoE的负载均衡问题。在上一篇文章《MoE环游记：2、不患寡而患不均》中，我们主要讨论了通过Aux Loss来促进负载均衡的思路。Aux Loss固然简单直观，但它也有一个明显的缺点——权重不好调——调低了无法促进均衡，调高了容易损害...",
    "date": "2025-03-05",
    "source": "",
    "tags": [
      "详细推导",
      "最优",
      "损失函数",
      "梯度",
      "moe",
      "生成模型",
      "负载均衡",
      "Loss-Free",
      "偏置向量",
      "指派问题",
      "最优传输"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 312
  },
  {
    "slug": "muon续集为什么我们选择尝试muon",
    "title": "Muon续集：为什么我们选择尝试Muon？",
    "description": "Muon续集：为什么我们选择尝试Muon？&para;\n原文链接: https://spaces.ac.cn/archives/10739\n发布日期: \n\n本文解读一下我们最新的技术报告《Muon is Scalable for LLM Training》，里边分享了我们之前在《Muon优化器赏析：从向量到矩阵的本质跨越》介绍过的Muon优化器的一次较大规模的实践，并开源了相应的模型（我们称之为“...",
    "date": "2025-02-27",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "梯度",
      "优化器",
      "谱范数",
      "muon"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 311
  },
  {
    "slug": "moe环游记2不患寡而患不均",
    "title": "MoE环游记：2、不患寡而患不均",
    "description": "MoE环游记：2、不患寡而患不均&para;\n原文链接: https://spaces.ac.cn/archives/10735\n发布日期: \n\n在上一篇文章《MoE环游记：1、从几何意义出发》中，我们介绍了MoE的一个几何诠释，旨在通过Dense模型的最佳逼近出发来推导和理解MoE。同时在文末我们也说了，给出MoE的计算公式仅仅是开始，训练一个实际有效的MoE模型还有很多细节补，比如本文要讨论的...",
    "date": "2025-02-21",
    "source": "",
    "tags": [
      "详细推导",
      "损失函数",
      "梯度",
      "稀疏",
      "moe",
      "生成模型",
      "负载均衡",
      "优化",
      "熵",
      "STE",
      "辅助损失"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 310
  },
  {
    "slug": "生成扩散模型漫谈二十九用ddpm来离散编码",
    "title": "生成扩散模型漫谈（二十九）：用DDPM来离散编码",
    "description": "生成扩散模型漫谈（二十九）：用DDPM来离散编码&para;\n原文链接: https://spaces.ac.cn/archives/10711\n发布日期: \n\n笔者前两天在arXiv刷到了一篇新论文《Compressed Image Generation with Denoising Diffusion Codebook Models》，实在为作者的天马行空所叹服，忍不住来跟大家分享一番。\n如本...",
    "date": "2025-02-14",
    "source": "",
    "tags": [
      "生成模型",
      "编码",
      "DDPM",
      "扩散",
      "离散化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 309
  },
  {
    "slug": "moe环游记1从几何意义出发",
    "title": "MoE环游记：1、从几何意义出发",
    "description": "MoE环游记：1、从几何意义出发&para;\n原文链接: https://spaces.ac.cn/archives/10699\n发布日期: \n\n前两年福至心灵之下，开了一个“Transformer升级之路”系列，陆续分享了主流Transformer架构的一些改进工作和个人思考，得到了部份读者的认可。这篇文章开始，我们沿着同样的风格，介绍当前另一个主流架构MoE（Mixture of Expert...",
    "date": "2025-02-08",
    "source": "",
    "tags": [
      "模型",
      "几何",
      "稀疏",
      "moe",
      "生成模型",
      "Transformer",
      "优化",
      "分布式训练",
      "负载均衡"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 308
  },
  {
    "slug": "三个球的交点坐标三球交会定位",
    "title": "三个球的交点坐标（三球交会定位）",
    "description": "三个球的交点坐标（三球交会定位）&para;\n原文链接: https://spaces.ac.cn/archives/10684\n发布日期: \n\n前几天笔者在思考一个问题时，联想到了三球交点问题，即给定三个球的球心坐标和半径，求这三个球的交点坐标。按理说这是一个定义清晰且简明的问题，并且具有鲜明的应用背景（比如卫星定位），应该早已有人给出“标准答案”才对。但笔者搜了一圈，发现不管是英文资料还是中文...",
    "date": "2025-01-28",
    "source": "",
    "tags": [
      "方程",
      "几何",
      "数值分析",
      "GPS定位",
      "三边测量",
      "Gram-Schmidt",
      "坐标变换",
      "误差分析",
      "鲁棒估计"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 307
  },
  {
    "slug": "细水长flow之tarflow流模型满血归来",
    "title": "细水长flow之TARFLOW：流模型满血归来？",
    "description": "细水长flow之TARFLOW：流模型满血归来？&para;\n原文链接: https://spaces.ac.cn/archives/10667\n发布日期: \n\n不知道还有没有读者对这个系列有印象？这个系列取名“细水长flow”，主要介绍flow模型的相关工作，起因是当年（2018年）OpenAI发布了一个新的流模型Glow，在以GAN为主流的当时来说着实让人惊艳了一番。但惊艳归惊艳，事实上在相当...",
    "date": "2025-01-17",
    "source": "",
    "tags": [
      "流模型",
      "flow",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 306
  },
  {
    "slug": "低秩近似之路五cur",
    "title": "低秩近似之路（五）：CUR",
    "description": "低秩近似之路（五）：CUR&para;\n原文链接: https://spaces.ac.cn/archives/10662\n发布日期: \n\n再次回到低秩近似之路上。在《低秩近似之路（四）：ID》中，我们介绍了“插值分解（Interpolative Decomposition，ID）”，这是为矩阵$\\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}$寻找$\\boldsym...",
    "date": "2025-01-12",
    "source": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 305
  },
  {
    "slug": "为什么梯度裁剪的默认模长是1",
    "title": "为什么梯度裁剪的默认模长是1？",
    "description": "为什么梯度裁剪的默认模长是1？&para;\n原文链接: https://spaces.ac.cn/archives/10657\n发布日期: \n\n我们知道，梯度裁剪（Gradient Clipping）是让模型训练更加平稳的常用技巧。常用的梯度裁剪是根据所有参数的梯度总模长来对梯度进行裁剪，其运算可以表示为\n\\begin{equation}\\text{clip}(\\boldsymbol{g},\\ta...",
    "date": "2025-01-02",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "梯度",
      "学习率",
      "优化器",
      "生成模型",
      "Transformer",
      "RNN",
      "LSTM",
      "训练稳定性"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 304
  },
  {
    "slug": "从谱范数梯度到新式权重衰减的思考",
    "title": "从谱范数梯度到新式权重衰减的思考",
    "description": "从谱范数梯度到新式权重衰减的思考&para;\n原文链接: https://spaces.ac.cn/archives/10648\n发布日期: \n\n在文章《Muon优化器赏析：从向量到矩阵的本质跨越》中，我们介绍了一个名为“Muon”的新优化器，其中一个理解视角是作为谱范数正则下的最速梯度下降，这似乎揭示了矩阵参数的更本质的优化方向。众所周知，对于矩阵参数我们经常也会加权重衰减（Weight Dec...",
    "date": "2024-12-25",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "优化",
      "梯度",
      "优化器",
      "谱范数"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 303
  },
  {
    "slug": "生成扩散模型漫谈二十八分步理解一致性模型",
    "title": "生成扩散模型漫谈（二十八）：分步理解一致性模型",
    "description": "生成扩散模型漫谈（二十八）：分步理解一致性模型&para;\n原文链接: https://spaces.ac.cn/archives/10633\n发布日期: \n\n书接上文，在《生成扩散模型漫谈（二十七）：将步长作为条件输入》中，我们介绍了加速采样的Shortcut模型，其对比的模型之一就是“一致性模型（Consistency Models）”。事实上，早在《生成扩散模型漫谈（十七）：构建ODE的一般...",
    "date": "2024-12-18",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 302
  },
  {
    "slug": "生成扩散模型漫谈二十七将步长作为条件输入",
    "title": "生成扩散模型漫谈（二十七）：将步长作为条件输入",
    "description": "生成扩散模型漫谈（二十七）：将步长作为条件输入&para;\n原文链接: https://spaces.ac.cn/archives/10617\n发布日期: \n\n这篇文章我们再次聚焦于扩散模型的采样加速。众所周知，扩散模型的采样加速主要有两种思路，一是开发更高效的求解器，二是事后蒸馏。然而，据笔者观察，除了上两篇文章介绍过的SiD外，这两种方案都鲜有能将生成步数降低到一步的结果。虽然SiD能做到单步...",
    "date": "2024-12-15",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 301
  },
  {
    "slug": "muon优化器赏析从向量到矩阵的本质跨越",
    "title": "Muon优化器赏析：从向量到矩阵的本质跨越",
    "description": "Muon优化器赏析：从向量到矩阵的本质跨越&para;\n原文链接: https://spaces.ac.cn/archives/10592\n发布日期: \n\n随着LLM时代的到来，学术界对于优化器的研究热情似乎有所减退。这主要是因为目前主流的AdamW已经能够满足大多数需求，而如果对优化器“大动干戈”，那么需要巨大的验证成本。因此，当前优化器的变化，多数都只是工业界根据自己的训练经验来对AdamW打...",
    "date": "2024-12-10",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "梯度",
      "优化器",
      "谱范数",
      "muon"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 300
  },
  {
    "slug": "从hessian近似看自适应学习率优化器",
    "title": "从Hessian近似看自适应学习率优化器",
    "description": "从Hessian近似看自适应学习率优化器&para;\n原文链接: https://spaces.ac.cn/archives/10588\n发布日期: \n\n这几天在重温去年的Meta的一篇论文《A Theory on Adam Instability in Large-Scale Machine Learning》，里边给出了看待Adam等自适应学习率优化器的新视角：它指出梯度平方的滑动平均某种程度...",
    "date": "2024-11-29",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "梯度",
      "学习率",
      "优化器",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 299
  },
  {
    "slug": "生成扩散模型漫谈二十六基于恒等式的蒸馏下",
    "title": "生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）",
    "description": "生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）&para;\n原文链接: https://spaces.ac.cn/archives/10567\n发布日期: \n\n继续回到我们的扩散系列。在《生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）》中，我们介绍了SiD（Score identity Distillation），这是一种不需要真实数据、也不需要从教师模型采样的扩散模型蒸馏方案，其形式类似...",
    "date": "2024-11-22",
    "source": "",
    "tags": [
      "生成模型",
      "梯度",
      "扩散",
      "去噪",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 298
  },
  {
    "slug": "adam的epsilon如何影响学习率的scaling-law",
    "title": "Adam的epsilon如何影响学习率的Scaling Law？",
    "description": "Adam的epsilon如何影响学习率的Scaling Law？&para;\n原文链接: https://spaces.ac.cn/archives/10563\n发布日期: \n\n上一篇文章《当Batch Size增大时，学习率该如何随之变化？》我们从多个角度讨论了学习率与Batch Size之间的缩放规律，其中对于Adam优化器我们采用了SignSGD近似，这是分析Adam优化器常用的手段。那么一...",
    "date": "2024-11-18",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 297
  },
  {
    "slug": "当batch-size增大时学习率该如何随之变化",
    "title": "当Batch Size增大时，学习率该如何随之变化？",
    "description": "当Batch Size增大时，学习率该如何随之变化？&para;\n原文链接: https://spaces.ac.cn/archives/10542\n发布日期: \n\n随着算力的飞速进步，有越多越多的场景希望能够实现“算力换时间”，即通过堆砌算力来缩短模型训练时间。理想情况下，我们希望投入$n$倍的算力，那么达到同样效果的时间则缩短为$1/n$，此时总的算力成本是一致的。这个“希望”看上去很合理和自...",
    "date": "2024-11-14",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 296
  },
  {
    "slug": "vq的又一技巧给编码表加一个线性变换",
    "title": "VQ的又一技巧：给编码表加一个线性变换",
    "description": "VQ的又一技巧：给编码表加一个线性变换&para;\n原文链接: https://spaces.ac.cn/archives/10519\n发布日期: \n\n在《VQ的旋转技巧：梯度直通估计的一般推广》中，我们介绍了VQ（Vector Quantization）的Rotation Trick，它的思想是通过推广VQ的STE（Straight-Through Estimator）来为VQ设计更好的梯度，从...",
    "date": "2024-11-06",
    "source": "",
    "tags": [
      "详细推导",
      "生成模型",
      "编码表",
      "梯度共享",
      "过参数化",
      "向量量化",
      "稀疏训练"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 295
  },
  {
    "slug": "低秩近似之路四id",
    "title": "低秩近似之路（四）：ID",
    "description": "低秩近似之路（四）：ID&para;\n原文链接: https://spaces.ac.cn/archives/10501\n发布日期: \n\n这篇文章的主角是ID（Interpolative Decomposition），中文可以称之为“插值分解”，它同样可以理解为是一种具有特定结构的低秩分解，其中的一侧是该矩阵的若干列（当然如果你偏好于行，那么选择行也没什么问题），换句话说，ID试图从一个矩阵中找出...",
    "date": "2024-10-30",
    "source": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 294
  },
  {
    "slug": "vq的旋转技巧梯度直通估计的一般推广",
    "title": "VQ的旋转技巧：梯度直通估计的一般推广",
    "description": "VQ的旋转技巧：梯度直通估计的一般推广&para;\n原文链接: https://spaces.ac.cn/archives/10489\n发布日期: \n\n随着多模态LLM的方兴未艾，VQ（Vector Quantization）的地位也“水涨船高”，它可以作为视觉乃至任意模态的Tokenizer，将多模态数据统一到自回归生成框架中。遗憾的是，自VQ-VAE首次提出VQ以来，其理论并没有显著进步，像编...",
    "date": "2024-10-24",
    "source": "",
    "tags": [
      "详细推导",
      "向量量化",
      "旋转技巧",
      "STE梯度",
      "几何一致性",
      "编码表利用率",
      "梯度估计"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 293
  },
  {
    "slug": "低秩近似之路三cr",
    "title": "低秩近似之路（三）：CR",
    "description": "低秩近似之路（三）：CR&para;\n原文链接: https://spaces.ac.cn/archives/10427\n发布日期: \n\n在《低秩近似之路（二）：SVD》中，我们证明了SVD可以给出任意矩阵的最优低秩近似。那里的最优近似是无约束的，也就是说SVD给出的结果只管误差上的最小，不在乎矩阵的具体结构，而在很多应用场景中，出于可解释性或者非线性处理等需求，我们往往希望得到具有某些特殊结构的...",
    "date": "2024-10-11",
    "source": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 292
  },
  {
    "slug": "低秩近似之路二svd",
    "title": "低秩近似之路（二）：SVD",
    "description": "低秩近似之路（二）：SVD&para;\n原文链接: https://spaces.ac.cn/archives/10407\n发布日期: \n\n上一篇文章中我们介绍了“伪逆”，它关系到给定矩阵$\\boldsymbol{M}$和$\\boldsymbol{A}$（或$\\boldsymbol{B}$）时优化目标$\\Vert \\boldsymbol{A}\\boldsymbol{B} - \\boldsymbol...",
    "date": "2024-10-01",
    "source": "",
    "tags": [
      "详细推导",
      "近似",
      "最优",
      "矩阵",
      "SVD",
      "低秩"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 291
  },
  {
    "slug": "softmax后传寻找top-k的光滑近似",
    "title": "Softmax后传：寻找Top-K的光滑近似",
    "description": "Softmax后传：寻找Top-K的光滑近似&para;\n原文链接: https://spaces.ac.cn/archives/10373\n发布日期: \n\nSoftmax，顾名思义是“soft的max”，是$\\max$算子（准确来说是$\\text{argmax}$）的光滑近似，它通过指数归一化将任意向量$\\boldsymbol{x}\\in\\mathbb{R}^n$转化为分量非负且和为1的新向量，...",
    "date": "2024-09-19",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "近似",
      "梯度",
      "光滑",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 290
  },
  {
    "slug": "低秩近似之路一伪逆",
    "title": "低秩近似之路（一）：伪逆",
    "description": "低秩近似之路（一）：伪逆&para;\n原文链接: https://spaces.ac.cn/archives/10366\n发布日期: \n\n可能很多读者跟笔者一样，对矩阵的低秩近似有种熟悉而又陌生的感觉。熟悉是因为，低秩近似的概念和意义都不难理解，加之目前诸如LoRA等基于低秩近似的微调技术遍地开花，让低秩近似的概念在耳濡目染间就已经深入人心；然而，低秩近似所覆盖的内容非常广，在低秩近似相关的论文中...",
    "date": "2024-09-15",
    "source": "",
    "tags": [
      "近似",
      "矩阵",
      "低秩",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 289
  },
  {
    "slug": "闭门造车之多模态思路浅谈三位置编码",
    "title": "“闭门造车”之多模态思路浅谈（三）：位置编码",
    "description": "“闭门造车”之多模态思路浅谈（三）：位置编码&para;\n原文链接: https://spaces.ac.cn/archives/10352\n发布日期: \n\n在前面的文章中，我们曾表达过这样的观点：多模态LLM相比纯文本LLM的主要差异在于，前者甚至还没有形成一个公认为标准的方法论。这里的方法论，不仅包括之前讨论的生成和训练策略，还包括一些基础架构的设计，比如本文要谈的“多模态位置编码”。\n对于这...",
    "date": "2024-09-06",
    "source": "",
    "tags": [
      "attention",
      "位置编码",
      "多模态",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 288
  },
  {
    "slug": "decoder-only的llm为什么需要位置编码",
    "title": "Decoder-only的LLM为什么需要位置编码？",
    "description": "Decoder-only的LLM为什么需要位置编码？&para;\n原文链接: https://spaces.ac.cn/archives/10347\n发布日期: \n\n众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在《为什么现在的LLM都是Decoder-only的架构？》也有过相关讨论），而对于Causal Attention，已经有不...",
    "date": "2024-09-01",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "生成模型",
      "Transformer",
      "RoPE",
      "ALiBi",
      "NoPE",
      "Causal Attention"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 287
  },
  {
    "slug": "通向最优分布之路概率空间的最小化",
    "title": "通向最优分布之路：概率空间的最小化",
    "description": "通向最优分布之路：概率空间的最小化&para;\n原文链接: https://spaces.ac.cn/archives/10289\n发布日期: \n\n当要求函数的最小值时，我们通常会先求导函数然后寻找其零点，比较幸运的情况下，这些零点之一正好是原函数的最小值点。如果是向量函数，则将导数改为梯度并求其零点。当梯度零点不易求得时，我们可以使用梯度下降来逐渐逼近最小值点。\n以上这些都是无约束优化的基础结果...",
    "date": "2024-08-06",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "优化",
      "梯度",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 286
  },
  {
    "slug": "对齐全量微调这是我看过最精彩的lora改进二",
    "title": "对齐全量微调！这是我看过最精彩的LoRA改进（二）",
    "description": "对齐全量微调！这是我看过最精彩的LoRA改进（二）&para;\n原文链接: https://spaces.ac.cn/archives/10266\n发布日期: \n\n前两周笔者写了《对齐全量微调！这是我看过最精彩的LoRA（一）》（当时还没有编号“一”），里边介绍了一个名为“LoRA-GA”的LoRA变体，它通过梯度SVD来改进LoRA的初始化，从而实现LoRA与全量微调的对齐。当然，从理论上来讲，...",
    "date": "2024-07-29",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型",
      "参数高效微调",
      "优化理论",
      "最小二乘",
      "Sylvester方程",
      "梯度对齐"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 285
  },
  {
    "slug": "monarch矩阵计算高效的稀疏型矩阵分解",
    "title": "Monarch矩阵：计算高效的稀疏型矩阵分解",
    "description": "Monarch矩阵：计算高效的稀疏型矩阵分解&para;\n原文链接: https://spaces.ac.cn/archives/10249\n发布日期:\n\n在矩阵压缩这个问题上,我们通常有两个策略可以选择,分别是低秩化 和稀疏化 。低秩化通过寻找矩阵的低秩近似来减少矩阵尺寸,而稀疏化则是通过减少矩阵中的非零元素来降低矩阵的复杂性。如果说SVD是奔着矩阵的低秩近似去的,那么相应地寻找矩阵稀疏近似的算...",
    "date": "2024-07-24",
    "source": "",
    "tags": [
      "矩阵",
      "语言模型",
      "稀疏",
      "低秩",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 284
  },
  {
    "slug": "对齐全量微调这是我看过最精彩的lora改进一",
    "title": "对齐全量微调！这是我看过最精彩的LoRA改进（一）",
    "description": "对齐全量微调！这是我看过最精彩的LoRA改进（一）&para;\n原文链接: https://spaces.ac.cn/archives/10226\n发布日期: \n\n众所周知，LoRA是一种常见的参数高效的微调方法，我们在《梯度视角下的LoRA：简介、分析、猜测及推广》做过简单介绍。LoRA利用低秩分解来降低微调参数量，节省微调显存，同时训练好的权重可以合并到原始权重上，推理架构不需要作出改变，是一...",
    "date": "2024-07-12",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "参数高效微调",
      "PEFT",
      "低秩分解",
      "SVD",
      "矩阵逼近",
      "Adam优化器",
      "全量微调对齐",
      "投影算子",
      "Frobenius范数"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 283
  },
  {
    "slug": "闭门造车之多模态思路浅谈二自回归",
    "title": "“闭门造车”之多模态思路浅谈（二）：自回归",
    "description": "“闭门造车”之多模态思路浅谈（二）：自回归&para;\n原文链接: https://spaces.ac.cn/archives/10197\n发布日期: \n\n这篇文章我们继续来闭门造车，分享一下笔者最近对多模态学习的一些新理解。\n在前文《“闭门造车”之多模态思路浅谈（一）：无损输入》中，我们强调了无损输入对于理想的多模型模态的重要性。如果这个观点成立，那么当前基于VQ-VAE、VQ-GAN等将图像离...",
    "date": "2024-07-08",
    "source": "",
    "tags": [
      "生成模型",
      "扩散",
      "多模态",
      "自回归",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 282
  },
  {
    "slug": "重温ssm四有理生成函数的新视角",
    "title": "重温SSM（四）：有理生成函数的新视角",
    "description": "重温SSM（四）：有理生成函数的新视角&para;\n原文链接: https://spaces.ac.cn/archives/10180\n发布日期: \n\n在前三篇文章中，我们较为详细地讨论了HiPPO和S4的大部分数学细节。那么，对于接下来的第四篇文章，大家预期我们会讨论什么工作呢？S5、Mamba乃至Mamba2？都不是。本系列文章主要关心SSM的数学基础，旨在了解SSM的同时也补充自己的数学能力...",
    "date": "2024-06-27",
    "source": "",
    "tags": [
      "生成函数",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 281
  },
  {
    "slug": "重温ssm三hippo的高效计算s4",
    "title": "重温SSM（三）：HiPPO的高效计算（S4）",
    "description": "重温SSM（三）：HiPPO的高效计算（S4）&para;\n原文链接: https://spaces.ac.cn/archives/10162\n发布日期: \n\n前面我们用两篇文章《重温SSM（一）：线性系统和HiPPO矩阵》和《重温SSM（二）：HiPPO的一些遗留问题》介绍了HiPPO的思想和推导——通过正交函数基对持续更新的函数进行实时逼近，其拟合系数的动力学正好可以表示为一个线性ODE系统，...",
    "date": "2024-06-20",
    "source": "",
    "tags": [
      "矩阵",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 280
  },
  {
    "slug": "重温ssm二hippo的一些遗留问题",
    "title": "重温SSM（二）：HiPPO的一些遗留问题",
    "description": "重温SSM（二）：HiPPO的一些遗留问题&para;\n原文链接: https://spaces.ac.cn/archives/10137\n发布日期: \n\n书接上文，在上一篇文章《重温SSM（一）：线性系统和HiPPO矩阵》中，我们详细讨论了HiPPO逼近框架其HiPPO矩阵的推导，其原理是通过正交函数基来动态地逼近一个实时更新的函数，其投影系数的动力学正好是一个线性系统，而如果以正交多项式为基，...",
    "date": "2024-06-05",
    "source": "",
    "tags": [
      "线性",
      "差分",
      "RNN",
      "梯度",
      "ssm"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 279
  },
  {
    "slug": "transformer升级之路18rope的底数选择原则",
    "title": "Transformer升级之路：18、RoPE的底数选择原则",
    "description": "Transformer升级之路：18、RoPE的底数选择原则&para;\n原文链接: https://spaces.ac.cn/archives/10122\n发布日期: \n\n我们知道，在RoPE中频率的计算公式为$\\theta_i = b^{-2i/d}$，底数$b$默认值为10000。目前Long Context的主流做法之一是，先在$b=10000$上用短文本预训练，然后调大$b$并在长文本微...",
    "date": "2024-05-29",
    "source": "",
    "tags": [
      "详细推导",
      "不等式",
      "attention",
      "位置编码",
      "rope",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 278
  },
  {
    "slug": "重温ssm一线性系统和hippo矩阵",
    "title": "重温SSM（一）：线性系统和HiPPO矩阵",
    "description": "重温SSM（一）：线性系统和HiPPO矩阵&para;\n原文链接: https://spaces.ac.cn/archives/10114\n发布日期: \n\n前几天，笔者看了几篇介绍SSM（State Space Model）的文章，才发现原来自己从未认真了解过SSM，于是打算认真去学习一下SSM的相关内容，顺便开了这个新坑，记录一下学习所得。\nSSM的概念由来已久，但这里我们特指深度学习中的SSM...",
    "date": "2024-05-24",
    "source": "",
    "tags": [
      "微分方程",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 277
  },
  {
    "slug": "缓存与效果的极限拉扯从mhamqagqa到mla",
    "title": "缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA",
    "description": "缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA&para;\n原文链接: https://spaces.ac.cn/archives/10091\n发布日期: \n\n前几天，幻方发布的DeepSeek-V2引起了大家的热烈讨论。首先，最让人哗然的是1块钱100万token的价格，普遍比现有的各种竞品API便宜了两个数量级，以至于有人调侃“这个价格哪怕它输出乱码，我也会认为这个乱码是一种艺术”；其...",
    "date": "2024-05-13",
    "source": "",
    "tags": [
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 276
  },
  {
    "slug": "生成扩散模型漫谈二十五基于恒等式的蒸馏上",
    "title": "生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）",
    "description": "生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）&para;\n原文链接: https://spaces.ac.cn/archives/10085\n发布日期: \n\n今天我们分享一下论文《Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Ge...",
    "date": "2024-05-01",
    "source": "",
    "tags": [
      "生成模型",
      "梯度",
      "扩散",
      "去噪",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 275
  },
  {
    "slug": "生成扩散模型漫谈二十四少走捷径更快到达",
    "title": "生成扩散模型漫谈（二十四）：少走捷径，更快到达",
    "description": "生成扩散模型漫谈（二十四）：少走捷径，更快到达&para;\n原文链接: https://spaces.ac.cn/archives/10077\n发布日期: \n\n如何减少采样步数同时保证生成质量，是扩散模型应用层面的一个关键问题。其中，《生成扩散模型漫谈（四）：DDIM = 高观点DDPM》介绍的DDIM可谓是加速采样的第一次尝试。后来，《生成扩散模型漫谈（五）：一般框架之SDE篇》、《生成扩散模型...",
    "date": "2024-04-23",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 274
  },
  {
    "slug": "生成扩散模型漫谈二十三信噪比与大图生成下",
    "title": "生成扩散模型漫谈（二十三）：信噪比与大图生成（下）",
    "description": "生成扩散模型漫谈（二十三）：信噪比与大图生成（下）&para;\n原文链接: https://spaces.ac.cn/archives/10055\n发布日期: \n\n上一篇文章《生成扩散模型漫谈（二十二）：信噪比与大图生成（上）》中，我们介绍了通过对齐低分辨率的信噪比来改进noise schedule，从而改善直接在像素空间训练的高分辨率图像生成（大图生成）的扩散模型效果。而这篇文章的主角同样是信噪...",
    "date": "2024-04-17",
    "source": "",
    "tags": [
      "无监督",
      "生成模型",
      "扩散",
      "信噪比",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 273
  },
  {
    "slug": "生成扩散模型漫谈二十二信噪比与大图生成上",
    "title": "生成扩散模型漫谈（二十二）：信噪比与大图生成（上）",
    "description": "生成扩散模型漫谈（二十二）：信噪比与大图生成（上）&para;\n原文链接: https://spaces.ac.cn/archives/10047\n发布日期: \n\n盘点主流的图像扩散模型作品，我们会发现一个特点：当前多数做高分辨率图像生成（下面简称“大图生成”）的工作，都是先通过Encoder变换到Latent空间进行的（即LDM，Latent Diffusion Model），直接在原始Pixe...",
    "date": "2024-04-08",
    "source": "",
    "tags": [
      "详细推导",
      "损失函数",
      "生成模型",
      "扩散",
      "信噪比",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 272
  },
  {
    "slug": "transformer升级之路17多模态位置编码的简单思考",
    "title": "Transformer升级之路：17、多模态位置编码的简单思考",
    "description": "Transformer升级之路：17、多模态位置编码的简单思考&para;\n原文链接: https://spaces.ac.cn/archives/10040\n发布日期: \n\n在这个系列的第二篇文章《Transformer升级之路：2、博采众长的旋转式位置编码》中，笔者提出了旋转位置编码（RoPE）——通过绝对位置的形式实现相对位置编码的方案。一开始RoPE是针对一维序列如文本、音频等设计的（Ro...",
    "date": "2024-03-29",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "rope",
      "多模态",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 271
  },
  {
    "slug": "时空之章将attention视为平方复杂度的rnn",
    "title": "时空之章：将Attention视为平方复杂度的RNN",
    "description": "时空之章：将Attention视为平方复杂度的RNN&para;\n原文链接: https://spaces.ac.cn/archives/10017\n发布日期: \n\n近年来，RNN由于其线性的训练和推理效率，重新吸引了不少研究人员和用户的兴趣，隐约有“文艺复兴”之势，其代表作有RWKV、RetNet、Mamba等。当将RNN用于语言模型时，其典型特点就是每步生成都是常数的空间复杂度和时间复杂度，从...",
    "date": "2024-03-18",
    "source": "",
    "tags": [
      "语言模型",
      "RNN",
      "attention",
      "复杂度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 270
  },
  {
    "slug": "用傅里叶级数拟合一维概率密度函数",
    "title": "用傅里叶级数拟合一维概率密度函数",
    "description": "用傅里叶级数拟合一维概率密度函数&para;\n原文链接: https://spaces.ac.cn/archives/10007\n发布日期: \n\n在《“闭门造车”之多模态思路浅谈（一）：无损输入》中我们曾提到，图像生成的本质困难是没有一个连续型概率密度的万能拟合器。当然，也不能说完全没有，比如高斯混合模型（GMM）理论上就是可以拟合任意概率密度，就连GAN本质上也可以理解为混合了无限个高斯模型的G...",
    "date": "2024-03-07",
    "source": "",
    "tags": [
      "级数",
      "概率",
      "分析",
      "逼近",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 269
  },
  {
    "slug": "配置不同的学习率lora还能再涨一点",
    "title": "配置不同的学习率，LoRA还能再涨一点？",
    "description": "配置不同的学习率，LoRA还能再涨一点？&para;\n原文链接: https://spaces.ac.cn/archives/10001\n发布日期: \n\nLoRA（Low-Rank Adaptation）是当前LLM的参数高效微调手段之一，此前我们在《梯度视角下的LoRA：简介、分析、猜测及推广》也有过简单讨论。这篇文章我们来学习LoRA的一个新结论：\n\n给LoRA的两个矩阵分配不同的学习率，Lo...",
    "date": "2024-02-27",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 268
  },
  {
    "slug": "闭门造车之多模态思路浅谈一无损输入",
    "title": "“闭门造车”之多模态思路浅谈（一）：无损输入",
    "description": "“闭门造车”之多模态思路浅谈（一）：无损输入&para;\n原文链接: https://spaces.ac.cn/archives/9984\n发布日期: \n\n这篇文章分享一下笔者关于多模态模型架构的一些闭门造车的想法，或者说一些猜测。\n最近Google的Gemini 1.5和OpenAI的Sora再次点燃了不少人对多模态的热情，只言片语的技术报告也引起了大家对其背后模型架构的热烈猜测。不过，本文并非...",
    "date": "2024-02-21",
    "source": "",
    "tags": [
      "VAE",
      "GAN",
      "Flow",
      "Diffusion",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 267
  },
  {
    "slug": "幂等生成网络ign试图将判别和生成合二为一的gan",
    "title": "幂等生成网络IGN：试图将判别和生成合二为一的GAN",
    "description": "幂等生成网络IGN：试图将判别和生成合二为一的GAN&para;\n原文链接: https://spaces.ac.cn/archives/9969\n发布日期: \n\n前段时间，一个名为“幂等生成网络（Idempotent Generative Network，IGN）”的生成模型引起了一定的关注。它自称是一种独立于已有的VAE、GAN、flow、Diffusion之外的新型生成模型，并且具有单步采样...",
    "date": "2024-01-31",
    "source": "",
    "tags": [
      "GAN",
      "GAN",
      "生成模型",
      "对抗",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 266
  },
  {
    "slug": "transformer升级之路16复盘长度外推技术",
    "title": "Transformer升级之路：16、“复盘”长度外推技术",
    "description": "Transformer升级之路：16、“复盘”长度外推技术&para;\n原文链接: https://spaces.ac.cn/archives/9948\n发布日期: \n\n回过头来看，才发现从第7篇《Transformer升级之路：7、长度外推性与局部注意力》开始，“Transformer升级之路”这个系列就跟长度外推“杠”上了，接连9篇文章（不算本文）都是围绕长度外推展开的。如今，距离第7篇文章刚...",
    "date": "2024-01-26",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 265
  },
  {
    "slug": "局部余弦相似度大全局余弦相似度一定也大吗",
    "title": "局部余弦相似度大，全局余弦相似度一定也大吗？",
    "description": "局部余弦相似度大，全局余弦相似度一定也大吗？&para;\n原文链接: https://spaces.ac.cn/archives/9931\n发布日期: \n\n在分析模型的参数时，有些情况下我们会将模型的所有参数当成一个整体的向量，有些情况下我们则会将不同的参数拆开来看。比如，一个7B大小的LLAMA模型所拥有的70亿参数量，有时候我们会将它当成“一个70亿维的向量”，有时候我们会按照模型的实现方式将...",
    "date": "2024-01-09",
    "source": "",
    "tags": [
      "不等式",
      "相似度",
      "悖论",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 264
  },
  {
    "slug": "让炼丹更科学一些一sgd的平均损失收敛",
    "title": "让炼丹更科学一些（一）：SGD的平均损失收敛",
    "description": "让炼丹更科学一些（一）：SGD的平均损失收敛&para;\n原文链接: https://spaces.ac.cn/archives/9902\n发布日期: \n\n很多时候我们将深度学习模型的训练过程戏称为“炼丹”，因为整个过程跟古代的炼丹术一样，看上去有一定的科学依据，但整体却给人一种“玄之又玄”的感觉。尽管本站之前也关注过一些优化器相关的工作，甚至也写过《从动力学角度看优化算法》系列，但都是比较表面的...",
    "date": "2023-12-19",
    "source": "",
    "tags": [
      "优化器",
      "不等式",
      "优化器",
      "sgd",
      "炼丹"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 263
  },
  {
    "slug": "注意力机制真的可以集中注意力吗",
    "title": "注意力机制真的可以“集中注意力”吗？",
    "description": "注意力机制真的可以“集中注意力”吗？&para;\n原文链接: https://spaces.ac.cn/archives/9889\n发布日期: \n\n之前在《Transformer升级之路：3、从Performer到线性Attention》、《为什么现在的LLM都是Decoder-only的架构？》等文章中，我们从Attention矩阵的“秩”的角度探讨了Attention机制，并曾经判断线性Att...",
    "date": "2023-12-12",
    "source": "",
    "tags": [
      "熵",
      "稀疏",
      "attention",
      "秩",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 262
  },
  {
    "slug": "通向概率分布之路盘点softmax及其替代品",
    "title": "通向概率分布之路：盘点Softmax及其替代品",
    "description": "通向概率分布之路：盘点Softmax及其替代品&para;\n原文链接: https://spaces.ac.cn/archives/10145\n发布日期: \n\n不论是在基础的分类任务中，还是如今无处不在的注意力机制中，概率分布的构建都是一个关键步骤。具体来说，就是将一个$n$维的任意向量，转换为一个$n$元的离散型概率分布。众所周知，这个问题的标准答案是Softmax，它是指数归一化的形式，相对来...",
    "date": "2023-12-11",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "分析",
      "损失函数",
      "梯度",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 261
  },
  {
    "slug": "生成扩散模型漫谈二十一中值定理加速ode采样",
    "title": "生成扩散模型漫谈（二十一）：中值定理加速ODE采样",
    "description": "生成扩散模型漫谈（二十一）：中值定理加速ODE采样&para;\n原文链接: https://spaces.ac.cn/archives/9881\n发布日期: \n\n在生成扩散模型的发展史上，DDIM和同期Song Yang的扩散SDE都称得上是里程碑式的工作，因为它们建立起了扩散模型与随机微分方程（SDE）、常微分方程（ODE）这两个数学领域的紧密联系，从而允许我们可以利用SDE、ODE已有的各种数...",
    "date": "2023-12-07",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 260
  },
  {
    "slug": "我在performer中发现了transformer-vq的踪迹",
    "title": "我在Performer中发现了Transformer-VQ的踪迹",
    "description": "我在Performer中发现了Transformer-VQ的踪迹&para;\n原文链接: https://spaces.ac.cn/archives/9862\n发布日期: \n\n前些天我们在《VQ一下Key，Transformer的复杂度就变成线性了》介绍了“Transformer-VQ”，这是通过将Key序列做VQ（Vector Quantize）变换来实现Attention复杂度线性化的方案。诚...",
    "date": "2023-11-29",
    "source": "",
    "tags": [
      "量子化",
      "语言模型",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 259
  },
  {
    "slug": "transformer升级之路15key归一化助力长度外推",
    "title": "Transformer升级之路：15、Key归一化助力长度外推",
    "description": "Transformer升级之路：15、Key归一化助力长度外推&para;\n原文链接: https://spaces.ac.cn/archives/9859\n发布日期: \n\n大体上，我们可以将目前Transformer的长度外推技术分为两类：一类是事后修改，比如NTK-RoPE、YaRN、ReRoPE等，这类方法的特点是直接修改推理模型，无需微调就能达到一定的长度外推效果，但缺点是它们都无法保持模...",
    "date": "2023-11-20",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 258
  },
  {
    "slug": "vq一下keytransformer的复杂度就变成线性了",
    "title": "VQ一下Key，Transformer的复杂度就变成线性了",
    "description": "VQ一下Key，Transformer的复杂度就变成线性了&para;\n原文链接: https://spaces.ac.cn/archives/9844\n发布日期: \n\nEfficient Transformer，泛指一切致力于降低Transformer的二次复杂度的工作，开始特指针对Attention的改进，后来更一般的思路，如傅里叶变换、线性RNN等，也被归入这个范畴。不得不说，为了降低Tra...",
    "date": "2023-11-09",
    "source": "",
    "tags": [
      "详细推导",
      "量子化",
      "编码",
      "梯度",
      "attention",
      "生成模型",
      "Transformer",
      "VQ",
      "线性注意力",
      "复杂度优化",
      "Efficient Transformer"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 257
  },
  {
    "slug": "简单得令人尴尬的fsq四舍五入超越了vq-vae",
    "title": "简单得令人尴尬的FSQ：\"四舍五入\"超越了VQ-VAE",
    "description": "简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE&para;\n原文链接: https://spaces.ac.cn/archives/9826\n发布日期: \n\n正如“XXX is all you need”一样，有不少论文都以“简单得令人尴尬”命名（An Embarrassingly Simple XXX），但在笔者看来，这些论文大多数都是噱头多于实力。不过，笔者最近阅读到的一篇论文，真的让...",
    "date": "2023-10-31",
    "source": "",
    "tags": [
      "详细推导",
      "生成模型",
      "向量量化",
      "四舍五入",
      "FSQ",
      "编码表",
      "STE梯度",
      "极简主义"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 256
  },
  {
    "slug": "从梯度最大化看attention的scale操作",
    "title": "从梯度最大化看Attention的Scale操作",
    "description": "从梯度最大化看Attention的Scale操作&para;\n原文链接: https://spaces.ac.cn/archives/9812\n发布日期: \n\n我们知道，Scaled Dot-Product Attention的Scale因子是$\\frac{1}{\\sqrt{d}}$，其中$d$是$\\boldsymbol{q},\\boldsymbol{k}$的维度。这个Scale因子的一般解释是：...",
    "date": "2023-10-22",
    "source": "",
    "tags": [
      "优化",
      "梯度",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 255
  },
  {
    "slug": "随机分词再探从viterbi-sampling到完美采样算法",
    "title": "随机分词再探：从Viterbi Sampling到完美采样算法",
    "description": "随机分词再探：从Viterbi Sampling到完美采样算法&para;\n原文链接: https://spaces.ac.cn/archives/9811\n发布日期: \n\n在文章《随机分词浅探：从Viterbi Decoding到Viterbi Sampling》中，笔者提出了一种名为“Viterbi Sampling”的随机分词算法，它只是在求最优解的Viterbi Decoding基础上进行...",
    "date": "2023-10-16",
    "source": "",
    "tags": [
      "概率",
      "随机",
      "优化",
      "分词",
      "采样"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 254
  },
  {
    "slug": "emo基于最优传输思想设计的分类损失函数",
    "title": "EMO：基于最优传输思想设计的分类损失函数",
    "description": "EMO：基于最优传输思想设计的分类损失函数&para;\n原文链接: https://spaces.ac.cn/archives/9797\n发布日期: \n\n众所周知，分类任务的标准损失是交叉熵（Cross Entropy，等价于最大似然MLE，即Maximum Likelihood Estimation），它有着简单高效的特点，但在某些场景下也暴露出一些问题，如偏离评价指标、过度自信等，相应的改进工...",
    "date": "2023-10-13",
    "source": "",
    "tags": [
      "概率",
      "优化",
      "损失函数",
      "最优传输",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 253
  },
  {
    "slug": "预训练一下transformer的长序列成绩还能涨不少",
    "title": "预训练一下，Transformer的长序列成绩还能涨不少！",
    "description": "预训练一下，Transformer的长序列成绩还能涨不少！&para;\n原文链接: https://spaces.ac.cn/archives/9787\n发布日期: \n\n作为LLM的主流模型架构，Transformer在各类任务上的总体表现都出色，大多数情况下，Transformer的槽点只是它的平方复杂度，而不是效果——除了一个名为Long Range Arena（下面简称LRA）的Benchm...",
    "date": "2023-10-08",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 252
  },
  {
    "slug": "脑洞大开非线性rnn居然也可以并行计算",
    "title": "脑洞大开：非线性RNN居然也可以并行计算？",
    "description": "脑洞大开：非线性RNN居然也可以并行计算？&para;\n原文链接: https://spaces.ac.cn/archives/9783\n发布日期: \n\n近年来，线性RNN由于其可并行训练以及常数推理成本等特性，吸引了一定研究人员的关注（例如笔者之前写的《Google新作试图“复活”RNN：RNN能否再次辉煌？》），这让RNN在Transformer遍地开花的潮流中仍有“一席之地”。然而，目前看来...",
    "date": "2023-09-26",
    "source": "",
    "tags": [
      "摄动",
      "方程",
      "迭代",
      "语言模型",
      "RNN"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 251
  },
  {
    "slug": "自然数集中-n-ab-c-时-a-b-c-的最小值",
    "title": "自然数集中 N = ab + c 时 a + b + c 的最小值",
    "description": "自然数集中 N = ab + c 时 a + b + c 的最小值——组合优化的深度探索&para;\n原文链接: https://spaces.ac.cn/archives/9775\n发布日期: 2023-09-20\n\n第1部分：理论起源、历史发展与设计哲学&para;\n1.1 理论起源与跨域背景&para;\n这个看似简单的组合优化问题，实际上融合了多个深刻的数学和计算机科学领域：\n数论的经典问题...",
    "date": "2023-09-20",
    "source": "",
    "tags": [
      "最优化",
      "组合数论",
      "整数分解",
      "计算复杂度",
      "拉格朗日乘数法",
      "attention机制",
      "并行化",
      "双曲线",
      "参数化"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 250
  },
  {
    "slug": "随机分词浅探从viterbi-decoding到viterbi-sampling",
    "title": "随机分词浅探：从Viterbi Decoding到Viterbi Sampling",
    "description": "随机分词浅探：从Viterbi Decoding到Viterbi Sampling&para;\n原文链接: https://spaces.ac.cn/archives/9768\n发布日期: \n\n上一篇文章《大词表语言模型在续写任务上的一个问题及对策》发布后，很快就有读者指出可以在训练阶段引入带有随机性的分词结果来解决同样的问题，并且已经有论文和实现。经过进一步查阅学习，笔者发现这是一个名为Subw...",
    "date": "2023-09-16",
    "source": "",
    "tags": [
      "概率",
      "随机",
      "分词",
      "新词发现",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 249
  },
  {
    "slug": "大词表语言模型在续写任务上的一个问题及对策",
    "title": "大词表语言模型在续写任务上的一个问题及对策",
    "description": "大词表语言模型在续写任务上的一个问题及对策&para;\n原文链接: https://spaces.ac.cn/archives/9762\n发布日期: \n\n对于LLM来说，通过增大Tokenizer的词表来提高压缩率，从而缩短序列长度、降低解码成本，是大家都喜闻乐见的事情。毕竟增大词表只需要增大Embedding层和输出的Dense层，这部分增加的计算量几乎不可感知，但缩短序列长度之后带来的解码速度...",
    "date": "2023-09-13",
    "source": "",
    "tags": [
      "概率",
      "问题",
      "语言模型",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 248
  },
  {
    "slug": "bytepiece更纯粹更高压缩率的tokenizer",
    "title": "BytePiece：更纯粹、更高压缩率的Tokenizer",
    "description": "BytePiece：更纯粹、更高压缩率的Tokenizer&para;\n原文链接: https://spaces.ac.cn/archives/9752\n发布日期: \n\n目前在LLM中最流行的Tokenizer（分词器）应该是Google的SentencePiece了，因为它符合Tokenizer的一些理想特性，比如语言无关、数据驱动等，并且由于它是C++写的，所以Tokenize（分词）的速度很...",
    "date": "2023-09-07",
    "source": "",
    "tags": [
      "最小熵",
      "分词",
      "无监督",
      "新词发现",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 247
  },
  {
    "slug": "liontiger优化器训练下的embedding异常和对策",
    "title": "Lion/Tiger优化器训练下的Embedding异常和对策",
    "description": "Lion/Tiger优化器训练下的Embedding异常和对策&para;\n原文链接: https://spaces.ac.cn/archives/9736\n发布日期: \n\n打从在《Tiger：一个“抠”到极致的优化器》提出了Tiger优化器之后，Tiger就一直成为了我训练模型的“标配”优化器。最近笔者已经尝试将Tiger用到了70亿参数模型的预训练之中，前期效果看上来尚可，初步说明Tiger也...",
    "date": "2023-08-28",
    "source": "",
    "tags": [
      "问题",
      "梯度",
      "优化器",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 246
  },
  {
    "slug": "transformer升级之路14当hwfa遇见rerope",
    "title": "Transformer升级之路：14、当HWFA遇见ReRoPE",
    "description": "Transformer升级之路：14、当HWFA遇见ReRoPE&para;\n原文链接: https://spaces.ac.cn/archives/9731\n发布日期: \n\n在上一篇文章《Transformer升级之路：13、逆用Leaky ReRoPE》中，笔者尝试通过在训练阶段逆用Leaky ReRoPE的思路，使得推理阶段的位置编码变为正常的RoPE，从而在达到长度外推的同时解决ReRoP...",
    "date": "2023-08-24",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "外推",
      "rope",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 245
  },
  {
    "slug": "transformer升级之路13逆用leaky-rerope",
    "title": "Transformer升级之路：13、逆用Leaky ReRoPE",
    "description": "Transformer升级之路：13、逆用Leaky ReRoPE&para;\n原文链接: https://spaces.ac.cn/archives/9728\n发布日期: \n\n上周在《Transformer升级之路：12、无限外推的ReRoPE？》中，笔者提出了ReRoPE和Leaky ReRoPE，诸多实验结果表明，它们能够在几乎不损失训练效果的情况下免微调地扩展LLM的Context长度，并...",
    "date": "2023-08-14",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 244
  },
  {
    "slug": "transformer升级之路12无限外推的rerope",
    "title": "Transformer升级之路：12、无限外推的ReRoPE？",
    "description": "Transformer升级之路：12、无限外推的ReRoPE？&para;\n原文链接: https://spaces.ac.cn/archives/9708\n发布日期: \n\n自从在《Transformer升级之路：11、将β进制位置进行到底》中引入混合进制的思路进一步推广了NTK-aware Scaled RoPE后，笔者感觉类似思路的效果已经达到了上限，想要更大幅度的提升就必须另辟蹊径了。这时候...",
    "date": "2023-08-07",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 243
  },
  {
    "slug": "transformer升级之路11将β进制位置进行到底",
    "title": "Transformer升级之路：11、将β进制位置进行到底",
    "description": "Transformer升级之路：11、将β进制位置进行到底&para;\n原文链接: https://spaces.ac.cn/archives/9706\n发布日期: \n\n在文章《Transformer升级之路：10、RoPE是一种β进制编码》中，我们给出了RoPE的$\\beta$进制诠释，并基于进制转化的思路推导了能够在不微调的情况下就可以扩展Context长度的NTK-aware Scaled...",
    "date": "2023-07-31",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 242
  },
  {
    "slug": "语言模型输出端共享embedding的重新探索",
    "title": "语言模型输出端共享Embedding的重新探索",
    "description": "语言模型输出端共享Embedding的重新探索&para;\n原文链接: https://spaces.ac.cn/archives/9698\n发布日期: \n\n预训练刚兴起时，在语言模型的输出端重用Embedding权重是很常见的操作，比如BERT、第一版的T5、早期的GPT，都使用了这个操作，这是因为当模型主干部分不大且词表很大时，Embedding层的参数量很可观，如果输出端再新增一个独立的同样...",
    "date": "2023-07-20",
    "source": "",
    "tags": [
      "语言模型",
      "初始化",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 241
  },
  {
    "slug": "当生成模型肆虐互联网将有疯牛病之忧",
    "title": "当生成模型肆虐：互联网将有“疯牛病”之忧？",
    "description": "当生成模型肆虐：互联网将有“疯牛病”之忧？&para;\n原文链接: https://spaces.ac.cn/archives/9687\n发布日期: \n\n众所周知，不管是文本还是视觉领域，各种生成模型正在以无法阻挡的势头“肆虐”互联网。虽然大家都明白，实现真正的通用人工智能（AGI）还有很长的路要走，但这并不妨碍人们越来越频繁地利用生成模型来创作和分享内容。君不见，很多网络文章已经配上了Stabl...",
    "date": "2023-07-14",
    "source": "",
    "tags": [
      "生成模型",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 240
  },
  {
    "slug": "transformer升级之路10rope是一种β进制编码",
    "title": "Transformer升级之路：10、RoPE是一种β进制编码",
    "description": "Transformer升级之路：10、RoPE是一种β进制编码&para;\n原文链接: https://spaces.ac.cn/archives/9675\n发布日期: \n\n对关心如何扩展LLM的Context长度的读者来说，上周无疑是激动人心的一周，开源社区接连不断地出现令人振奋的成果。首先，网友@kaiokendev在他的项目SuperHOT中实验了“位置线性内插”的方案，显示通过非常少的长文...",
    "date": "2023-07-06",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 239
  },
  {
    "slug": "生成扩散模型漫谈二十从reflow到wgan-gp",
    "title": "生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP",
    "description": "生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP&para;\n原文链接: https://spaces.ac.cn/archives/9668\n发布日期: \n\n上一篇文章《生成扩散模型漫谈（十九）：作为扩散ODE的GAN》中，我们介绍了如何将GAN理解为在另一个时间维度上的扩散ODE，简而言之，GAN实际上就是将扩散模型中样本的运动转化为生成器参数的运动！然而，该文章的推导过程依赖于Was...",
    "date": "2023-06-28",
    "source": "",
    "tags": [
      "优化",
      "GAN",
      "梯度",
      "扩散",
      "生成模型",
      "最优传输",
      "WGAN",
      "ReFlow"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 238
  },
  {
    "slug": "生成扩散模型漫谈十九作为扩散ode的gan",
    "title": "生成扩散模型漫谈（十九）：作为扩散ODE的GAN",
    "description": "生成扩散模型漫谈（十九）：作为扩散ODE的GAN&para;\n原文链接: https://spaces.ac.cn/archives/9662\n发布日期: \n\n在文章《生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配》中，我们推导了Wasserstein距离与扩散模型得分匹配损失之间的一个不等式，表明扩散模型的优化目标与WGAN的优化目标在某种程度上具有相似性。而在本文，我们将探讨《MonoFlow...",
    "date": "2023-06-24",
    "source": "",
    "tags": [
      "优化",
      "GAN",
      "扩散",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 237
  },
  {
    "slug": "梯度流探索通向最小值之路",
    "title": "梯度流：探索通向最小值之路",
    "description": "梯度流：探索通向最小值之路&para;\n原文链接: https://spaces.ac.cn/archives/9660\n发布日期: \n\n在这篇文章中，我们将探讨一个被称为“梯度流（Gradient Flow）”的概念。简单来说，梯度流是将我们在用梯度下降法中寻找最小值的过程中的各个点连接起来，形成一条随（虚拟的）时间变化的轨迹，这条轨迹便被称作“梯度流”。在文章的后半部分，我们将重点讨论如何将梯...",
    "date": "2023-06-16",
    "source": "",
    "tags": [
      "泛函",
      "动力学",
      "优化",
      "梯度",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 236
  },
  {
    "slug": "naive-bayes-is-all-you-need",
    "title": "Naive Bayes is all you need ?",
    "description": "Naive Bayes is all you need ?&para;\n原文链接: https://spaces.ac.cn/archives/9648\n发布日期: \n\n很抱歉，起了这么个具有标题党特征的题目。在写完《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》之后，笔者就觉得朴素贝叶斯（Naive Bayes）跟Attention机制有很多相同的特征，后来再推导了一下发现， At...",
    "date": "2023-06-08",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 235
  },
  {
    "slug": "关于nbce方法的一些补充说明和分析",
    "title": "关于NBCE方法的一些补充说明和分析",
    "description": "关于NBCE方法的一些补充说明和分析&para;\n原文链接: https://spaces.ac.cn/archives/9632\n发布日期: \n\n上周在《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》中，我们介绍了一种基于朴素贝叶斯来扩展LLM的Context长度的方案NBCE（Naive Bayes-based Context Extension）。由于它有着即插即用、模型无关、...",
    "date": "2023-05-31",
    "source": "",
    "tags": [
      "语言模型",
      "外推",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 234
  },
  {
    "slug": "nbce使用朴素贝叶斯扩展llm的context处理长度",
    "title": "NBCE：使用朴素贝叶斯扩展LLM的Context处理长度",
    "description": "NBCE：使用朴素贝叶斯扩展LLM的Context处理长度&para;\n原文链接: https://spaces.ac.cn/archives/9617\n发布日期: \n\n\n在LLM时代还玩朴素贝叶斯（Naive Bayes）？\n\n这可能是许多读者在看到标题后的首个想法。确实如此，当古老的朴素贝叶斯与前沿的LLM相遇时，产生了令人惊讶的效果——我们可以直接扩展现有LLM模型的Context处理长度，...",
    "date": "2023-05-23",
    "source": "",
    "tags": [
      "语言模型",
      "外推",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 233
  },
  {
    "slug": "基于量子化假设推导模型的尺度定律scaling-law",
    "title": "基于量子化假设推导模型的尺度定律(Scaling Law)",
    "description": "基于量子化假设推导模型的尺度定律(Scaling Law)&para;\n原文链接: https://spaces.ac.cn/archives/9607\n发布日期: 2023-05-18\n\n📄 引言&para;\n尺度定律（Scaling Law），指的是模型能力与模型尺度之间的渐近关系。具体来说，模型能力我们可以简单理解为模型的损失函数，模型尺度可以指模型参数量、训练数据量、训练步数等，所谓尺度定...",
    "date": "2023-05-18",
    "source": "",
    "tags": [
      "模型",
      "分析",
      "量子",
      "尺度定律",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 232
  },
  {
    "slug": "transformer升级之路9一种全局长度外推的新思路",
    "title": "Transformer升级之路：9、一种全局长度外推的新思路",
    "description": "Transformer升级之路：9、一种全局长度外推的新思路&para;\n原文链接: https://spaces.ac.cn/archives/9603\n发布日期: \n\n说到Transformer无法处理超长序列的原因，大家的第一反应通常都是Self Attention的二次复杂度。但事实上，即便忽略算力限制，常规的Transformer也无法处理超长序列，因为它们的长度外推性（Length E...",
    "date": "2023-05-12",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "泛化",
      "外推",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 231
  },
  {
    "slug": "如何度量数据的稀疏程度",
    "title": "如何度量数据的稀疏程度？",
    "description": "如何度量数据的稀疏程度？&para;\n原文链接: https://spaces.ac.cn/archives/9595\n发布日期: \n\n在机器学习中，我们经常会谈到稀疏性，比如我们经常说注意力矩阵通常是很稀疏的。然而，不知道大家发现没有，我们似乎从没有给出过度量稀疏程度的标准方法。也就是说，以往我们关于稀疏性的讨论，仅仅是直观层面的感觉，并没有过定量分析。那么问题来了，稀疏性的度量有标准方法了吗？...",
    "date": "2023-05-05",
    "source": "",
    "tags": [
      "概率",
      "熵",
      "度量",
      "稀疏",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 230
  },
  {
    "slug": "注意力和softmax的两点有趣发现鲁棒性和信息量",
    "title": "注意力和Softmax的两点有趣发现：鲁棒性和信息量",
    "description": "注意力和Softmax的两点有趣发现：鲁棒性和信息量&para;\n原文链接: https://spaces.ac.cn/archives/9593\n发布日期: \n\n最近几周笔者一直都在思考注意力机制的相关性质，在这个过程中对注意力及Softmax有了更深刻的理解。在这篇文章中，笔者简单分享其中的两点：\n\n1、Softmax注意力天然能够抵御一定的噪声扰动；\n2、从信息熵角度也可以对初始化问题形成直...",
    "date": "2023-04-25",
    "source": "",
    "tags": [
      "信息",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 229
  },
  {
    "slug": "梯度视角下的lora简介分析猜测及推广",
    "title": "梯度视角下的LoRA：简介、分析、猜测及推广",
    "description": "梯度视角下的LoRA：简介、分析、猜测及推广&para;\n原文链接: https://spaces.ac.cn/archives/9590\n发布日期: 2023-04-17\n\n概述与核心贡献&para;\n随着ChatGPT及其平替的火热，各种参数高效（Parameter-Efficient）的微调方法也\"水涨船高\"，其中最流行的方案之一就是本文的主角LoRA 了，它出自论文《LoRA: Low-R...",
    "date": "2023-04-17",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型",
      "参数高效微调",
      "矩阵分解",
      "自适应学习率",
      "梯度投影",
      "流形优化",
      "知识蒸馏",
      "模型压缩"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 228
  },
  {
    "slug": "从jl引理看熵不变性attention",
    "title": "从JL引理看熵不变性Attention",
    "description": "从JL引理看熵不变性Attention&para;\n原文链接: https://spaces.ac.cn/archives/9588\n发布日期: \n\n在《从熵不变性看Attention的Scale操作》、《熵不变性Softmax的一个快速推导》中笔者提出了熵不变性Softmax，简单来说就是往Softmax之前的Attention矩阵多乘上一个$\\log n$，理论上有助于增强长度外推性，其中$n...",
    "date": "2023-04-10",
    "source": "",
    "tags": [
      "熵",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 227
  },
  {
    "slug": "bias项的神奇作用rope-bias-更好的长度外推性",
    "title": "Bias项的神奇作用：RoPE + Bias = 更好的长度外推性",
    "description": "Bias项的神奇作用：RoPE + Bias = 更好的长度外推性&para;\n原文链接: https://spaces.ac.cn/archives/9577\n发布日期: \n\n万万没想到，Bias项能跟Transformer的长度外推性联系在一起！\n长度外推性是我们希望Transformer具有的一个理想性质，笔者曾在《Transformer升级之路：7、长度外推性与局部注意力》、《Transf...",
    "date": "2023-04-03",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "rope"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 226
  },
  {
    "slug": "google新作试图复活rnnrnn能否再次辉煌",
    "title": "Google新作试图“复活”RNN：RNN能否再次辉煌？",
    "description": "Google新作试图“复活”RNN：RNN能否再次辉煌？&para;\n原文链接: https://spaces.ac.cn/archives/9554\n发布日期: \n\n当前，像ChatGPT之类的LLM可谓是“风靡全球”。有读者留意到，几乎所有LLM都还是用最初的Multi-Head Scaled-Dot Attention，近年来大量的Efficient工作如线性Attention、FLASH等...",
    "date": "2023-03-28",
    "source": "",
    "tags": [
      "语言模型",
      "RNN",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 225
  },
  {
    "slug": "为什么现在的llm都是decoder-only的架构faq",
    "title": "《为什么现在的LLM都是Decoder-only的架构？》FAQ",
    "description": "《为什么现在的LLM都是Decoder-only的架构？》FAQ&para;\n原文链接: https://spaces.ac.cn/archives/9547\n发布日期: 2023-03-20\n\n📄 引言&para;\n上周笔者写了《为什么现在的LLM都是Decoder-only的架构？》，总结了一下我在这个问题上的一些实验结论和猜测。果然是热点问题流量大，paperweekly的转发没多久阅读量就...",
    "date": "2023-03-20",
    "source": "",
    "tags": [
      "问答",
      "语言模型",
      "文本生成",
      "attention",
      "生成模型",
      "Decoder-only",
      "Transformer"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 224
  },
  {
    "slug": "为什么现在的llm都是decoder-only的架构",
    "title": "为什么现在的LLM都是Decoder-only的架构？",
    "description": "为什么现在的LLM都是Decoder-only的架构？&para;\n原文链接: https://spaces.ac.cn/archives/9529\n发布日期: \n\nLLM是“Large Language Model”的简写，目前一般指百亿参数以上的语言模型，主要面向文本生成 任务。跟小尺度模型（10亿或以内量级）的“百花齐放”不同，目前LLM的一个现状是Decoder-only架构的研究居多，像...",
    "date": "2023-03-17",
    "source": "",
    "tags": [
      "分析",
      "语言模型",
      "文本生成",
      "attention",
      "生成模型",
      "Transformer",
      "GPT",
      "BERT",
      "T5",
      "矩阵秩",
      "Causal Attention"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 223
  },
  {
    "slug": "缓解交叉熵过度自信的一个简明方案",
    "title": "缓解交叉熵过度自信的一个简明方案",
    "description": "缓解交叉熵过度自信的一个简明方案&para;\n原文链接: https://spaces.ac.cn/archives/9526\n发布日期: \n\n众所周知，分类问题的常规评估指标是正确率，而标准的损失函数则是交叉熵，交叉熵有着收敛快的优点，但它并非是正确率的光滑近似，这就带来了训练和预测的不一致性问题。另一方面，当训练样本的预测概率很低时，交叉熵会给出一个非常巨大的损失（趋于$-\\log 0^{+}...",
    "date": "2023-03-14",
    "source": "",
    "tags": [
      "优化",
      "损失函数",
      "光滑",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 222
  },
  {
    "slug": "tiger一个抠到极致的优化器",
    "title": "Tiger：一个“抠”到极致的优化器",
    "description": "Tiger：一个“抠”到极致的优化器&para;\n原文链接: https://spaces.ac.cn/archives/9512\n发布日期: \n\n这段时间笔者一直在实验《Google新搜出的优化器Lion：效率与效果兼得的“训练狮”》所介绍的Lion优化器。之所以对Lion饶有兴致，是因为它跟笔者之前的关于理想优化器的一些想法不谋而合，但当时笔者没有调出好的效果，而Lion则做好了。\n相比标准的...",
    "date": "2023-03-07",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "优化器",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 221
  },
  {
    "slug": "生成扩散模型漫谈十八得分匹配-条件得分匹配",
    "title": "生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配",
    "description": "生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配&para;\n原文链接: https://spaces.ac.cn/archives/9509\n发布日期: \n\n在前面的介绍中，我们多次提及“得分匹配”和“条件得分匹配”，它们是扩散模型、能量模型等经常出现的概念，特别是很多文章直接说扩散模型的训练目标是“得分匹配”，但事实上当前主流的扩散模型如DDPM的训练目标是“条件得分匹配”才对。\n那么“得...",
    "date": "2023-02-28",
    "source": "",
    "tags": [
      "概率",
      "分析",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 220
  },
  {
    "slug": "生成扩散模型漫谈十七构建ode的一般步骤下",
    "title": "生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）",
    "description": "生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）&para;\n原文链接: https://spaces.ac.cn/archives/9497\n发布日期: 2023-02-23\n\n📌 内容概览&para;\n历史总是惊人地相似。当初笔者在写《生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）》（当时还没有“上”这个后缀）时，以为自己已经搞清楚了构建ODE式扩散的一般步骤，结果读者 @gaohu...",
    "date": "2023-02-23",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "微分方程",
      "生成模型",
      "扩散",
      "ODE",
      "Rectified Flow",
      "最优传输"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 219
  },
  {
    "slug": "google新搜出的优化器lion效率与效果兼得的训练狮",
    "title": "Google新搜出的优化器Lion：效率与效果兼得的\"训练狮\"",
    "description": "Google新搜出的优化器Lion：效率与效果兼得的“训练狮”&para;\n原文链接: https://spaces.ac.cn/archives/9473\n发布日期: \n\n昨天在Arixv上发现了Google新发的一篇论文《Symbolic Discovery of Optimization Algorithms》，主要是讲自动搜索优化器的，咋看上去没啥意思，因为类似的工作也有不少，大多数结果都...",
    "date": "2023-02-16",
    "source": "",
    "tags": [
      "详细推导",
      "优化器",
      "优化",
      "Sign函数",
      "动量",
      "AdamW",
      "泛化性能",
      "内存优化",
      "算法搜索"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 218
  },
  {
    "slug": "生成扩散模型漫谈十六w距离-得分匹配",
    "title": "生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配",
    "description": "生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配&para;\n原文链接: https://spaces.ac.cn/archives/9467\n发布日期: \n\nWasserstein距离（下面简称“W距离”），是基于最优传输思想来度量两个概率分布差异程度的距离函数，笔者之前在《从Wasserstein距离、对偶理论到WGAN》等博文中也做过介绍。对于很多读者来说，第一次听说W距离，是因为2017年出...",
    "date": "2023-02-14",
    "source": "",
    "tags": [
      "详细推导",
      "微分方程",
      "GAN",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 217
  },
  {
    "slug": "测试函数法推导连续性方程和fokker-planck方程",
    "title": "测试函数法推导连续性方程和Fokker-Planck方程",
    "description": "测试函数法推导连续性方程和Fokker-Planck方程&para;\n原文链接: https://spaces.ac.cn/archives/9461\n发布日期: 2023-02-11\n\n引言&para;\n在文章《生成扩散模型漫谈（六）：一般框架之ODE篇》中，我们推导了SDE的Fokker-Planck方程；而在《生成扩散模型漫谈（十二）：\"硬刚\"扩散ODE》中，我们单独推导了ODE的连续性方程...",
    "date": "2023-02-11",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "微分方程",
      "随机",
      "扩散",
      "生成模型",
      "泛函分析",
      "PDE理论",
      "弱解"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 216
  },
  {
    "slug": "transformer升级之路8长度外推性与位置鲁棒性",
    "title": "Transformer升级之路：8、长度外推性与位置鲁棒性",
    "description": "Transformer升级之路：8、长度外推性与位置鲁棒性&para;\n原文链接: https://spaces.ac.cn/archives/9444\n发布日期: \n\n上一篇文章《Transformer升级之路：7、长度外推性与局部注意力》我们讨论了Transformer的长度外推性，得出的结论是长度外推性是一个训练和预测的不一致问题，而解决这个不一致的主要思路是将注意力局部化，很多外推性好的改...",
    "date": "2023-01-31",
    "source": "",
    "tags": [
      "详细推导",
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 215
  },
  {
    "slug": "transformer升级之路7长度外推性与局部注意力",
    "title": "Transformer升级之路：7、长度外推性与局部注意力",
    "description": "Transformer升级之路：7、长度外推性与局部注意力&para;\n原文链接: https://spaces.ac.cn/archives/9431\n发布日期: \n\n对于Transformer模型来说，其长度的外推性是我们一直在追求的良好性质，它是指我们在短序列上训练的模型，能否不用微调地用到长序列上并依然保持不错的效果。之所以追求长度外推性，一方面是理论的完备性，觉得这是一个理想模型应当具备...",
    "date": "2023-01-12",
    "source": "",
    "tags": [
      "详细推导",
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 214
  },
  {
    "slug": "transformer升级之路6旋转位置编码的完备性分析",
    "title": "Transformer升级之路：6、旋转位置编码的完备性分析",
    "description": "Transformer升级之路：6、旋转位置编码的完备性分析&para;\n原文链接: https://spaces.ac.cn/archives/9403\n发布日期: \n\n在去年的文章《Transformer升级之路：2、博采众长的旋转式位置编码》中，笔者提出了旋转位置编码（RoPE），当时的出发点只是觉得用绝对位置来实现相对位置是一件“很好玩的事情”，并没料到其实际效果还相当不错，并为大家所接受...",
    "date": "2022-12-28",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "attention",
      "位置编码",
      "rope",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 213
  },
  {
    "slug": "生成扩散模型漫谈十五构建ode的一般步骤中",
    "title": "生成扩散模型漫谈（十五）：构建ODE的一般步骤（中）",
    "description": "生成扩散模型漫谈（十五）：构建ODE的一般步骤（中）&para;\n原文链接: https://spaces.ac.cn/archives/9379\n发布日期: \n\n上周笔者写了《生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）》（当时还没有“上”这个后缀），本以为已经窥见了构建ODE扩散模型的一般规律，结果不久后评论区大神 @gaohuazuo 就给出了一个构建格林函数更高效、更直观的方案，让...",
    "date": "2022-12-22",
    "source": "",
    "tags": [
      "详细推导",
      "微分方程",
      "生成模型",
      "扩散",
      "格林函数",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 212
  },
  {
    "slug": "生成扩散模型漫谈十四构建ode的一般步骤上",
    "title": "生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）",
    "description": "生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）&para;\n原文链接: https://spaces.ac.cn/archives/9370\n发布日期: \n\n书接上文，在《生成扩散模型漫谈（十三）：从万有引力到扩散模型》中，我们介绍了一个由万有引力启发的、几何意义非常清晰的ODE式生成扩散模型。有的读者看了之后就疑问：似乎“万有引力”并不是唯一的选择，其他形式的力是否可以由同样的物理绘景构建...",
    "date": "2022-12-15",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "格林函数",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 211
  },
  {
    "slug": "从局部到全局语义相似度的测地线距离",
    "title": "从局部到全局：语义相似度的测地线距离",
    "description": "从局部到全局：语义相似度的测地线距离&para;\n原文链接: https://spaces.ac.cn/archives/9368\n发布日期: \n\n前段时间在最近的一篇论文《Unsupervised Opinion Summarization Using Approximate Geodesics》中学到了一个新的概念，叫做“测地线距离（Geodesic Distance）”，感觉有点意思，特来跟...",
    "date": "2022-12-07",
    "source": "",
    "tags": [
      "黎曼几何",
      "语义",
      "语义相似度",
      "生成模型",
      "attention",
      "测地线距离",
      "流形学习",
      "k-NN图",
      "Dijkstra算法"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 210
  },
  {
    "slug": "用热传导方程来指导自监督学习",
    "title": "用热传导方程来指导自监督学习",
    "description": "用热传导方程来指导自监督学习&para;\n原文链接: https://spaces.ac.cn/archives/9359\n发布日期: \n\n用理论物理来卷机器学习已经不是什么新鲜事了，比如上个月介绍的《生成扩散模型漫谈（十三）：从万有引力到扩散模型》就是经典一例。最近一篇新出的论文《Self-Supervised Learning based on Heat Equation》，顾名思义，用热传导...",
    "date": "2022-11-30",
    "source": "",
    "tags": [
      "物理",
      "无监督",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 209
  },
  {
    "slug": "基于amos优化器思想推导出来的一些炼丹策略",
    "title": "基于Amos优化器思想推导出来的一些“炼丹策略”",
    "description": "基于Amos优化器思想推导出来的一些“炼丹策略”&para;\n原文链接: https://spaces.ac.cn/archives/9344\n发布日期: \n\n如果将训练模型比喻为“炼丹”，那么“炼丹炉”显然就是优化器了。据传AdamW优化器是当前训练神经网络最快的方案，这一点笔者也没有一一对比过，具体情况如何不得而知，不过目前做预训练时多数都用AdamW或其变种LAMB倒是真的。然而，正如有了炼...",
    "date": "2022-11-22",
    "source": "",
    "tags": [
      "分析",
      "优化",
      "渐近",
      "优化器",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 208
  },
  {
    "slug": "cosent三作为交互式相似度的损失函数",
    "title": "CoSENT（三）：作为交互式相似度的损失函数",
    "description": "CoSENT（三）：作为交互式相似度的损失函数&para;\n原文链接: https://spaces.ac.cn/archives/9341\n发布日期: 2022-11-09\n\n\n\n### 核心发现\n\n**问题**：CoSENT能否作为交互式相似度模型的损失函数？\n\n**背景**：\n- CoSENT最初设计用于特征式句向量学习（优化余弦相似度）\n- 但本质上它是一个**排序损失**，与具体相似度度...",
    "date": "2022-11-09",
    "source": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型",
      "attention",
      "交互式匹配",
      "排序损失",
      "交叉熵",
      "Spearman",
      "Cross-Encoder"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 207
  },
  {
    "slug": "利用cur分解加速交互式相似度模型的检索",
    "title": "利用CUR分解加速交互式相似度模型的检索",
    "description": "利用CUR分解加速交互式相似度模型的检索&para;\n原文链接: https://spaces.ac.cn/archives/9336\n发布日期: \n\n文本相似度有“交互式”和“特征式”两种做法，想必很多读者对此已经不陌生，之前笔者也写过一篇文章《CoSENT（二）：特征式匹配与交互式匹配有多大差距？》来对比两者的效果。总的来说，交互式相似度效果通常会好些，但直接用它来做大规模检索是不现实的，而特...",
    "date": "2022-11-02",
    "source": "",
    "tags": [
      "矩阵",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 206
  },
  {
    "slug": "圆内随机n点在同一个圆心角为θ的扇形的概率",
    "title": "圆内随机n点在同一个圆心角为θ的扇形的概率",
    "description": "圆内随机n点在同一个圆心角为θ的扇形的概率&para;\n原文链接: https://spaces.ac.cn/archives/9324\n发布日期: \n\n这几天网上热传了一道“四鸭共半圆”题目：  \n\n四鸭共半圆问题\n可能有不少读者看到后也尝试做过，就连李永乐老师也专门开了一节课讲这道题（参考《圆形水池四只鸭子在同一个半圆里，概率有多大？》）。就这道题目本身而言，答案并不算困难，可以有很多方法算出...",
    "date": "2022-10-25",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "竞赛",
      "随机",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 205
  },
  {
    "slug": "生成扩散模型漫谈十三从万有引力到扩散模型",
    "title": "生成扩散模型漫谈（十三）：从万有引力到扩散模型",
    "description": "生成扩散模型漫谈（十三）：从万有引力到扩散模型&para;\n原文链接: https://spaces.ac.cn/archives/9305\n发布日期: \n\n对于很多读者来说，生成扩散模型可能是他们遇到的第一个能够将如此多的数学工具用到深度学习上的模型。在这个系列文章中，我们已经展示了扩散模型与数学分析、概率统计、常微分方程、随机微分方程乃至偏微分方程等内容的深刻联系，可以说，即便是做数学物理方程...",
    "date": "2022-10-18",
    "source": "",
    "tags": [
      "详细推导",
      "引力",
      "场论",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 204
  },
  {
    "slug": "十字架组合计数问题浅试",
    "title": "\"十字架\"组合计数问题浅试",
    "description": "\"十字架\"组合计数问题浅试&para;\n原文链接: https://spaces.ac.cn/archives/9291\n发布日期: 2022-10-09\n\n📄 引言&para;\n昨天在这个公众号文章看到了一道据说答案有争议的\"十字架\"组合计数问题：\n\n一个正方形中，如果四条边有两条是$i$色，另外两条是其他两种不同颜色，那么称这个正方形是\"$i$色主导\"的。考虑如下由16条线段、5个正方形组成的...",
    "date": "2022-10-09",
    "source": "",
    "tags": [
      "证明",
      "数学",
      "组合数学",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 203
  },
  {
    "slug": "生成扩散模型漫谈十二硬刚扩散ode",
    "title": "生成扩散模型漫谈（十二）：“硬刚”扩散ODE",
    "description": "生成扩散模型漫谈（十二）：“硬刚”扩散ODE&para;\n原文链接: https://spaces.ac.cn/archives/9280\n发布日期: 2022-09-28\n\n📌 内容概览&para;\n在《生成扩散模型漫谈（五）：一般框架之SDE篇》中，我们从SDE的角度理解了生成扩散模型，然后在《生成扩散模型漫谈（六）：一般框架之ODE篇》中，我们知道SDE对应的扩散模型中，实际上隐含了一个OD...",
    "date": "2022-09-28",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "ODE",
      "热传导方程",
      "雅可比行列式",
      "得分匹配",
      "Fokker-Planck方程"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 202
  },
  {
    "slug": "生成扩散模型漫谈十一统一扩散模型应用篇",
    "title": "生成扩散模型漫谈（十一）：统一扩散模型（应用篇）",
    "description": "生成扩散模型漫谈（十一）：统一扩散模型（应用篇）&para;\n原文链接: https://spaces.ac.cn/archives/9271\n发布日期: \n\n在《生成扩散模型漫谈（十）：统一扩散模型（理论篇）》中，笔者自称构建了一个统一的模型框架（Unified Diffusion Model，UDM），它允许更一般的扩散方式和数据类型。那么UDM框架究竟能否实现如期目的呢？本文通过一些具体例子...",
    "date": "2022-09-21",
    "source": "",
    "tags": [
      "统一",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 201
  },
  {
    "slug": "生成扩散模型漫谈十统一扩散模型理论篇",
    "title": "生成扩散模型漫谈（十）：统一扩散模型（理论篇）",
    "description": "生成扩散模型漫谈（十）：统一扩散模型（理论篇）&para;\n原文链接: https://spaces.ac.cn/archives/9262\n发布日期: \n\n老读者也许会发现，相比之前的更新频率，这篇文章可谓是“姗姗来迟”，因为这篇文章“想得太多”了。\n通过前面九篇文章，我们已经对生成扩散模型做了一个相对全面的介绍。虽然理论内容很多，但我们可以发现，前面介绍的扩散模型处理的都是连续型对象，并且都是...",
    "date": "2022-09-14",
    "source": "",
    "tags": [
      "统一",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 200
  },
  {
    "slug": "生成扩散模型漫谈九条件控制生成结果",
    "title": "生成扩散模型漫谈（九）：条件控制生成结果",
    "description": "生成扩散模型漫谈（九）：条件控制生成结果&para;\n原文链接: https://spaces.ac.cn/archives/9257\n发布日期: \n\n前面的几篇文章都是比较偏理论的结果，这篇文章我们来讨论一个比较有实用价值的主题——条件控制生成。\n作为生成模型，扩散模型跟VAE、GAN、flow等模型的发展史很相似，都是先出来了无条件生成，然后有条件生成就紧接而来。无条件生成往往是为了探索效果上...",
    "date": "2022-08-30",
    "source": "",
    "tags": [
      "概率",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 199
  },
  {
    "slug": "生成扩散模型漫谈八最优扩散方差估计下",
    "title": "生成扩散模型漫谈（八）：最优扩散方差估计（下）",
    "description": "生成扩散模型漫谈（八）：最优扩散方差估计（下）&para;\n原文链接: https://spaces.ac.cn/archives/9246\n发布日期: \n\n在上一篇文章《生成扩散模型漫谈（七）：最优扩散方差估计（上）》中，我们介绍并推导了Analytic-DPM中的扩散模型最优方差估计结果，它是直接给出了已经训练好的生成扩散模型的最优方差的一个解析估计，实验显示该估计结果确实能有效提高扩散模型的...",
    "date": "2022-08-18",
    "source": "",
    "tags": [
      "优化",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 198
  },
  {
    "slug": "生成扩散模型漫谈七最优扩散方差估计上",
    "title": "生成扩散模型漫谈（七）：最优扩散方差估计（上）",
    "description": "生成扩散模型漫谈（七）：最优扩散方差估计（上）&para;\n原文链接: https://spaces.ac.cn/archives/9245\n发布日期: \n\n对于生成扩散模型来说，一个很关键的问题是生成过程的方差应该怎么选择，因为不同的方差会明显影响生成效果。\n在《生成扩散模型漫谈（二）：DDPM = 自回归式VAE》我们提到，DDPM分别假设数据服从两种特殊分布推出了两个可用的结果；《生成扩散模...",
    "date": "2022-08-12",
    "source": "",
    "tags": [
      "优化",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 197
  },
  {
    "slug": "生成扩散模型漫谈六一般框架之ode篇",
    "title": "生成扩散模型漫谈（六）：一般框架之ODE篇",
    "description": "生成扩散模型漫谈（六）：一般框架之ODE篇&para;\n原文链接: https://spaces.ac.cn/archives/9228\n发布日期: \n\n上一篇文章《生成扩散模型漫谈（五）：一般框架之SDE篇》中，我们对宋飏博士的论文《Score-Based Generative Modeling through Stochastic Differential Equations》做了基本的介绍和...",
    "date": "2022-08-08",
    "source": "",
    "tags": [
      "详细推导",
      "flow模型",
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散",
      "ODE",
      "概率流",
      "Fokker-Planck",
      "连续归一化流"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 196
  },
  {
    "slug": "生成扩散模型漫谈五一般框架之sde篇",
    "title": "生成扩散模型漫谈（五）：一般框架之SDE篇",
    "description": "生成扩散模型漫谈（五）：一般框架之SDE篇&para;\n原文链接: https://spaces.ac.cn/archives/9209\n发布日期: \n\n在写生成扩散模型的第一篇文章时，就有读者在评论区推荐了宋飏博士的论文《Score-Based Generative Modeling through Stochastic Differential Equations》，可以说该论文构建了一个相当...",
    "date": "2022-08-03",
    "source": "",
    "tags": [
      "详细推导",
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 195
  },
  {
    "slug": "生成扩散模型漫谈四ddim-高观点ddpm",
    "title": "生成扩散模型漫谈（四）：DDIM = 高观点DDPM",
    "description": "生成扩散模型漫谈（四）：DDIM = 高观点DDPM&para;\n原文链接: https://spaces.ac.cn/archives/9181\n发布日期: \n\n相信很多读者都听说过甚至读过克莱因的《高观点下的初等数学》这套书，顾名思义，这是在学到了更深入、更完备的数学知识后，从更高的视角重新审视过往学过的初等数学，以得到更全面的认知，甚至达到温故而知新的效果。类似的书籍还有很多，比如《重温微积...",
    "date": "2022-07-27",
    "source": "",
    "tags": [
      "详细推导",
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 194
  },
  {
    "slug": "生成扩散模型漫谈三ddpm-贝叶斯-去噪",
    "title": "生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪",
    "description": "生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪&para;\n原文链接: https://spaces.ac.cn/archives/9164\n发布日期: \n\n到目前为止，笔者给出了生成扩散模型DDPM的两种推导，分别是《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》中的通俗类比方案和《生成扩散模型漫谈（二）：DDPM = 自回归式VAE》中的变分自编码器方案。两种方案可谓各有特点，...",
    "date": "2022-07-19",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 193
  },
  {
    "slug": "不成功的尝试将多标签交叉熵推广到n个m分类上去",
    "title": "不成功的尝试：将多标签交叉熵推广到“n个m分类”上去",
    "description": "不成功的尝试：将多标签交叉熵推广到“n个m分类”上去&para;\n原文链接: https://spaces.ac.cn/archives/9158\n发布日期: \n\n可能有读者留意到，这次更新相对来说隔得比较久了。事实上，在上周末时就开始准备这篇文章了，然而笔者低估了这个问题的难度，几乎推导了整整一周，仍然还没得到一个完善的结果出来。目前发出来的，仍然只是一个失败的结果，希望有经验的读者可以指点指点...",
    "date": "2022-07-15",
    "source": "",
    "tags": [
      "优化",
      "损失函数",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 192
  },
  {
    "slug": "生成扩散模型漫谈二ddpm-自回归式vae",
    "title": "生成扩散模型漫谈（二）：DDPM = 自回归式VAE",
    "description": "生成扩散模型漫谈（二）：DDPM = 自回归式VAE&para;\n原文链接: https://spaces.ac.cn/archives/9152\n发布日期: \n\n在文章《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》中，我们为生成扩散模型DDPM构建了“拆楼-建楼”的通俗类比，并且借助该类比完整地推导了生成扩散模型DDPM的理论形式。在该文章中，我们还指出DDPM本质上已经不是传统的扩散...",
    "date": "2022-07-06",
    "source": "",
    "tags": [
      "vae",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型",
      "变分推断",
      "隐变量模型"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 191
  },
  {
    "slug": "维度灾难之hubness现象浅析",
    "title": "“维度灾难”之Hubness现象浅析",
    "description": "“维度灾难”之Hubness现象浅析&para;\n原文链接: https://spaces.ac.cn/archives/9147\n发布日期: \n\n这几天读到论文《Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling》，了解到了一个新的名词“Hubness现象”，说的是高维空间中的一种聚集效应，本质...",
    "date": "2022-06-28",
    "source": "",
    "tags": [
      "维度",
      "GAN",
      "生成模型",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 190
  },
  {
    "slug": "ladder-side-tuning预训练模型的过墙梯",
    "title": "Ladder Side-Tuning：预训练模型的“过墙梯”",
    "description": "Ladder Side-Tuning：预训练模型的“过墙梯”&para;\n原文链接: https://spaces.ac.cn/archives/9138\n发布日期: \n\n如果说大型的预训练模型是自然语言处理的“张良计”，那么对应的“过墙梯”是什么呢？笔者认为是高效地微调这些大模型到特定任务上的各种技巧。除了直接微调全部参数外，还有像Adapter、P-Tuning等很多参数高效的微调技巧，它们能...",
    "date": "2022-06-20",
    "source": "",
    "tags": [
      "语言模型",
      "预训练",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 189
  },
  {
    "slug": "生成扩散模型漫谈一ddpm-拆楼-建楼",
    "title": "生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼",
    "description": "生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼&para;\n原文链接: https://spaces.ac.cn/archives/9119\n发布日期: \n\n说到生成模型，VAE、GAN可谓是“如雷贯耳”，本站也有过多次分享。此外，还有一些比较小众的选择，如flow模型、VQ-VAE等，也颇有人气，尤其是VQ-VAE及其变体VQ-GAN，近期已经逐渐发展到“图像的Tokenizer”的地位，...",
    "date": "2022-06-13",
    "source": "",
    "tags": [
      "VAE",
      "GAN",
      "flow模型",
      "概率",
      "生成模型",
      "扩散模型",
      "DDPM",
      "生成建模"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 188
  },
  {
    "slug": "相对位置编码transformer的一个理论缺陷与对策",
    "title": "相对位置编码Transformer的一个理论缺陷与对策",
    "description": "相对位置编码Transformer的一个理论缺陷与对策&para;\n原文链接: https://spaces.ac.cn/archives/9105\n发布日期: \n\n位置编码是Transformer中很重要的一环，在《让研究人员绞尽脑汁的Transformer位置编码》中我们就总结了一些常见的位置编码设计。大体上，我们将Transformer的位置编码分为“绝对位置编码”和“相对位置编码”两类，其...",
    "date": "2022-06-07",
    "source": "",
    "tags": [
      "详细推导",
      "语言模型",
      "attention",
      "位置编码",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 187
  },
  {
    "slug": "如何训练你的准确率",
    "title": "如何训练你的准确率？",
    "description": "如何训练你的准确率？&para;\n原文链接: https://spaces.ac.cn/archives/9098\n发布日期: 2022-06-01\n\n📄 引言&para;\n最近Arxiv上的一篇论文《EXACT: How to Train Your Accuracy》引起了笔者的兴趣，顾名思义这是介绍如何直接以准确率为训练目标来训练模型的。正好笔者之前也对此有过一些分析，如《函数光滑化杂谈：不可...",
    "date": "2022-06-01",
    "source": "",
    "tags": [
      "概率",
      "优化",
      "损失函数",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 186
  },
  {
    "slug": "从重参数的角度看离散概率分布的构建",
    "title": "从重参数的角度看离散概率分布的构建",
    "description": "从重参数的角度看离散概率分布的构建&para;\n原文链接: https://spaces.ac.cn/archives/9085\n发布日期: \n\n一般来说，神经网络的输出都是无约束的，也就是值域为$\\mathbb{R}$，而为了得到有约束的输出，通常是采用加激活函数的方式。例如，如果我们想要输出一个概率分布来代表每个类别的概率，那么通常在最后加上Softmax作为激活函数。那么一个紧接着的疑问就是...",
    "date": "2022-05-25",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "重参数",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 185
  },
  {
    "slug": "当bert-whitening引入超参数总有一款适合你",
    "title": "当BERT-whitening引入超参数：总有一款适合你",
    "description": "当BERT-whitening引入超参数：总有一款适合你&para;\n原文链接: https://spaces.ac.cn/archives/9079\n发布日期: \n\n在《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》中，笔者提出了BERT-whitening，验证了一个线性变换就能媲美当时的SOTA方法BERT-flow。此外，BERT-whitening还可以对句向量进行...",
    "date": "2022-05-18",
    "source": "",
    "tags": [
      "语言模型",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 184
  },
  {
    "slug": "logsumexp运算的几个不等式",
    "title": "logsumexp运算的几个不等式",
    "description": "logsumexp运算的几个不等式&para;\n原文链接: https://spaces.ac.cn/archives/9070\n发布日期: \n\n$\\text{logsumexp}$是机器学习经常遇到的运算，尤其是交叉熵的相关实现和推导中都会经常出现，同时它还是$\\max$的光滑近似（参考《寻求一个光滑的最大值函数》）。设$x=(x_1,x_2,\\cdots,x_n)$，$\\text{logsum...",
    "date": "2022-05-10",
    "source": "",
    "tags": [
      "不等式",
      "函数",
      "生成模型",
      "attention",
      "优化",
      "数值稳定性",
      "凸优化",
      "Softmax",
      "Lipschitz",
      "詹森不等式"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 183
  },
  {
    "slug": "多标签softmax交叉熵的软标签版本",
    "title": "多标签“Softmax+交叉熵”的软标签版本",
    "description": "多标签“Softmax+交叉熵”的软标签版本&para;\n原文链接: https://spaces.ac.cn/archives/9064\n发布日期: \n\n（注：本文的相关内容已整理成论文《ZLPR: A Novel Loss for Multi-label Classification》，如需引用可以直接引用英文论文，谢谢。）\n在《将“Softmax+交叉熵”推广到多标签分类问题》中，我们提出了...",
    "date": "2022-05-07",
    "source": "",
    "tags": [
      "优化",
      "损失函数",
      "光滑",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 182
  },
  {
    "slug": "在bert4keras中使用混合精度和xla加速训练",
    "title": "在bert4keras中使用混合精度和XLA加速训练",
    "description": "在bert4keras中使用混合精度和XLA加速训练&para;\n原文链接: https://spaces.ac.cn/archives/9059\n发布日期: \n\n之前笔者一直都是聚焦于模型的构思和实现，鲜有关注模型的训练加速，像混合精度和XLA这些技术，虽然也有听过，但没真正去实践过。这两天折腾了一番，成功在bert4keras中使用了混合精度和XLA来加速训练，在此做个简单的总结，供大家参考。...",
    "date": "2022-04-28",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention",
      "混合精度",
      "XLA",
      "TensorFlow",
      "训练加速",
      "FP16",
      "FP32",
      "Loss Scaling"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 181
  },
  {
    "slug": "gau-α尝鲜体验快好省的下一代attention",
    "title": "GAU-α：尝鲜体验快好省的下一代Attention",
    "description": "GAU-α：尝鲜体验快好省的下一代Attention&para;\n原文链接: https://spaces.ac.cn/archives/9052\n发布日期: \n\n在《FLASH：可能是近来最有意思的高效Transformer设计》中，我们介绍了GAU（Gated Attention Unit，门控线性单元），在这里笔者愿意称之为“目前最有潜力的下一代Attention设计”，因为它真正达到了“更...",
    "date": "2022-04-22",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "预训练",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 180
  },
  {
    "slug": "你的语言模型有没有无法预测的词",
    "title": "你的语言模型有没有“无法预测的词”？",
    "description": "你的语言模型有没有“无法预测的词”？&para;\n原文链接: https://spaces.ac.cn/archives/9046\n发布日期: \n\n众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不...",
    "date": "2022-04-20",
    "source": "",
    "tags": [
      "语言模型",
      "多任务",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 179
  },
  {
    "slug": "globalpointer下的kl散度应该是怎样的",
    "title": "GlobalPointer下的“KL散度”应该是怎样的？",
    "description": "GlobalPointer下的\"KL散度\"应该是怎样的？&para;\n原文链接: https://spaces.ac.cn/archives/9039\n发布日期: 2022-04-15\n\n\n\n### 核心问题\n\n**背景**：\n- [GlobalPointer](/archives/8373)用于命名实体识别等多标签分类任务\n- [R-Drop](/archives/8496)等正则化方法需要计算...",
    "date": "2022-04-15",
    "source": "",
    "tags": [
      "损失函数",
      "对抗训练",
      "NER",
      "正则化",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 178
  },
  {
    "slug": "熵不变性softmax的一个快速推导",
    "title": "熵不变性Softmax的一个快速推导",
    "description": "熵不变性Softmax的一个快速推导&para;\n原文链接: https://spaces.ac.cn/archives/9034\n发布日期: \n\n在文章《从熵不变性看Attention的Scale操作》中，我们推导了一版具有熵不变性质的注意力机制：\n\\begin{equation}Attention(Q,K,V) = softmax\\left(\\frac{\\kappa \\log n}{d}QK^...",
    "date": "2022-04-11",
    "source": "",
    "tags": [
      "近似",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 177
  },
  {
    "slug": "听说attention与softmax更配哦",
    "title": "听说Attention与Softmax更配哦～",
    "description": "听说Attention与Softmax更配哦～&para;\n原文链接: https://spaces.ac.cn/archives/9019\n发布日期: \n\n不知道大家留意到一个细节没有，就是当前NLP主流的预训练模式都是在一个固定长度（比如512）上进行，然后直接将预训练好的模型用于不同长度的任务中。大家似乎也没有对这种模式有过怀疑，仿佛模型可以自动泛化到不同长度是一个“理所应当”的能力。\n当然...",
    "date": "2022-04-07",
    "source": "",
    "tags": [
      "熵",
      "语言模型",
      "attention",
      "预训练",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 176
  },
  {
    "slug": "为什么pre-norm的效果不如post-norm",
    "title": "为什么Pre Norm的效果不如Post Norm？",
    "description": "为什么Pre Norm的效果不如Post Norm？&para;\n原文链接: https://spaces.ac.cn/archives/9009\n发布日期: \n\nPre Norm与Post Norm之间的对比是一个“老生常谈”的话题了，本博客就多次讨论过这个问题，比如文章《浅谈Transformer的初始化、参数化与标准化》、《模型优化漫谈：BERT的初始标准差为什么是0.02？》等。目前比较明...",
    "date": "2022-03-29",
    "source": "",
    "tags": [
      "详细推导",
      "归一化",
      "Layer Norm",
      "残差连接",
      "Transformer",
      "梯度流",
      "深度网络",
      "DeepNet",
      "有效深度"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 175
  },
  {
    "slug": "roformerv2自然语言理解的极限探索",
    "title": "RoFormerV2：自然语言理解的极限探索",
    "description": "RoFormerV2：自然语言理解的极限探索&para;\n原文链接: https://spaces.ac.cn/archives/8998\n发布日期: \n\n大概在1年前，我们提出了旋转位置编码（RoPE），并发布了对应的预训练模型RoFormer。随着时间的推移，RoFormer非常幸运地得到了越来越多的关注和认可，比如EleutherAI新发布的60亿和200亿参数的GPT模型中就用上了RoPE...",
    "date": "2022-03-21",
    "source": "",
    "tags": [
      "语言模型",
      "预训练",
      "生成模型",
      "attention",
      "优化",
      "RoPE",
      "多任务学习",
      "RMS Norm",
      "残差连接"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 174
  },
  {
    "slug": "为什么需要残差一个来自deepnet的视角",
    "title": "为什么需要残差？一个来自DeepNet的视角",
    "description": "为什么需要残差？一个来自DeepNet的视角&para;\n原文链接: https://spaces.ac.cn/archives/8994\n发布日期: \n\n在《训练1000层的Transformer究竟有什么困难？》中我们介绍了微软提出的能训练1000层Transformer的DeepNet技术。而对于DeepNet，读者一般也有两种反应，一是为此感到惊叹而点赞，另一则是觉得新瓶装旧酒没意思。出现...",
    "date": "2022-03-19",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "深度学习",
      "梯度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 173
  },
  {
    "slug": "门控注意力单元gau还需要warmup吗",
    "title": "门控注意力单元（GAU）还需要Warmup吗？",
    "description": "门控注意力单元（GAU）还需要Warmup吗？&para;\n原文链接: https://spaces.ac.cn/archives/8990\n发布日期: \n\n在文章《训练1000层的Transformer究竟有什么困难？》发布之后，很快就有读者问到如果将其用到《FLASH：可能是近来最有意思的高效Transformer设计》中的“门控注意力单元（GAU）”，那结果是怎样的？跟标准Transform...",
    "date": "2022-03-11",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "attention",
      "生成模型",
      "Transformer",
      "初始化",
      "LeCun初始化",
      "Warmup",
      "梯度传播"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 172
  },
  {
    "slug": "训练1000层的transformer究竟有什么困难",
    "title": "训练1000层的Transformer究竟有什么困难？",
    "description": "训练1000层的Transformer究竟有什么困难？&para;\n原文链接: https://spaces.ac.cn/archives/8978\n发布日期: \n\n众所周知，现在的Transformer越做越大，但这个“大”通常是“宽”而不是“深”，像GPT-3虽然参数有上千亿，但也只是一个96层的Transformer模型，与我们能想象的深度相差甚远。是什么限制了Transformer往“深”...",
    "date": "2022-03-09",
    "source": "",
    "tags": [
      "优化",
      "梯度",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 171
  },
  {
    "slug": "指数梯度下降-元学习-自适应学习率",
    "title": "指数梯度下降 + 元学习 = 自适应学习率",
    "description": "指数梯度下降 + 元学习 = 自适应学习率&para;\n原文链接: https://spaces.ac.cn/archives/8968\n发布日期: \n\n前两天刷到了Google的一篇论文《Step-size Adaptation Using Exponentiated Gradient Updates》，在其中学到了一些新的概念，所以在此记录分享一下。主要的内容有两个，一是非负优化的指数梯度下降...",
    "date": "2022-03-03",
    "source": "",
    "tags": [
      "优化",
      "梯度",
      "优化器",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 170
  },
  {
    "slug": "flash可能是近来最有意思的高效transformer设计",
    "title": "FLASH：可能是近来最有意思的高效Transformer设计",
    "description": "FLASH：可能是近来最有意思的高效Transformer设计&para;\n原文链接: https://spaces.ac.cn/archives/8934\n发布日期: \n\n高效Transformer，泛指所有概率Transformer效率的工作，笔者算是关注得比较早了，最早的博客可以追溯到2019年的《为节约而生：从标准Attention到稀疏Attention》，当时做这块的工作很少。后来，这...",
    "date": "2022-02-25",
    "source": "",
    "tags": [
      "语言模型",
      "生成模型",
      "attention",
      "高效Transformer",
      "GAU",
      "门控注意力",
      "线性复杂度",
      "稀疏注意力",
      "FLASH"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 169
  },
  {
    "slug": "gplinker基于globalpointer的事件联合抽取",
    "title": "GPLinker：基于GlobalPointer的事件联合抽取",
    "description": "GPLinker：基于GlobalPointer的事件联合抽取&para;\n原文链接: https://spaces.ac.cn/archives/8926\n发布日期: 2022-02-21\n\n\n\n### 核心创新\n\n**任务**：事件联合抽取（Event Extraction）\n- 识别事件类型、触发词、多个论元及其角色\n- 可能存在多个同类型事件、嵌套实体、共享论元等复杂情况\n\n**传统方法的...",
    "date": "2022-02-21",
    "source": "",
    "tags": [
      "NLP",
      "信息抽取",
      "NER",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 168
  },
  {
    "slug": "多任务学习漫谈三分主次之序",
    "title": "多任务学习漫谈（三）：分主次之序",
    "description": "多任务学习漫谈（三）：分主次之序&para;\n原文链接: https://spaces.ac.cn/archives/8907\n发布日期: \n\n多任务学习是一个很宽泛的命题，不同场景下多任务学习的目标不尽相同。在《多任务学习漫谈（一）：以损失之名》和《多任务学习漫谈（二）：行梯度之事》中，我们将多任务学习的目标理解为“做好每一个任务”，具体表现是“尽量平等地处理每一个任务”，我们可以称之为“平行型...",
    "date": "2022-02-14",
    "source": "",
    "tags": [
      "深度学习",
      "损失函数",
      "梯度",
      "多任务",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 167
  },
  {
    "slug": "多任务学习漫谈二行梯度之事",
    "title": "多任务学习漫谈（二）：行梯度之事",
    "description": "多任务学习漫谈（二）：行梯度之事&para;\n原文链接: https://spaces.ac.cn/archives/8896\n发布日期: \n\n在《多任务学习漫谈（一）：以损失之名》中，我们从损失函数的角度初步探讨了多任务学习问题，最终发现如果想要结果同时具有缩放不变性和平移不变性，那么用梯度的模长倒数作为任务的权重是一个比较简单的选择。我们继而分析了，该设计等价于将每个任务的梯度单独进行归一化后...",
    "date": "2022-02-08",
    "source": "",
    "tags": [
      "深度学习",
      "损失函数",
      "梯度",
      "多任务",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 166
  },
  {
    "slug": "gplinker基于globalpointer的实体关系联合抽取",
    "title": "GPLinker：基于GlobalPointer的实体关系联合抽取",
    "description": "GPLinker：基于GlobalPointer的实体关系联合抽取&para;\n原文链接: https://spaces.ac.cn/archives/8888\n发布日期: \n\n在将近三年前的百度“2019语言与智能技术竞赛”（下称LIC2019）中，笔者提出了一个新的关系抽取模型（参考《基于DGCNN和概率图的轻量级信息抽取模型》），后被进一步发表和命名为“CasRel”，算是当时关系抽取的SO...",
    "date": "2022-01-30",
    "source": "",
    "tags": [
      "NLP",
      "信息抽取",
      "NER",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 165
  },
  {
    "slug": "efficient-globalpointer少点参数多点效果",
    "title": "Efficient GlobalPointer：少点参数，多点效果",
    "description": "Efficient GlobalPointer：少点参数，多点效果&para;\n原文链接: https://spaces.ac.cn/archives/8877\n发布日期: 2022-01-25\n\n\n\n### 核心贡献\n\n提出了 **Efficient GlobalPointer**，通过将实体识别分解为\"抽取\"和\"分类\"两个步骤，显著降低了参数量：\n\n**参数量对比**（BERT base, 1...",
    "date": "2022-01-25",
    "source": "",
    "tags": [
      "模型",
      "NLP",
      "NER",
      "命名实体识别",
      "GlobalPointer",
      "参数效率"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 164
  },
  {
    "slug": "多任务学习漫谈一以损失之名",
    "title": "多任务学习漫谈（一）：以损失之名",
    "description": "多任务学习漫谈（一）：以损失之名&para;\n原文链接: https://spaces.ac.cn/archives/8870\n发布日期: \n\n能提升模型性能的方法有很多，多任务学习（Multi-Task Learning）也是其中一种。简单来说，多任务学习是希望将多个相关的任务共同训练，希望不同任务之间能够相互补充和促进，从而获得单任务上更好的效果（准确率、鲁棒性等）。然而，多任务学习并不是所有...",
    "date": "2022-01-18",
    "source": "",
    "tags": [
      "深度学习",
      "损失函数",
      "多任务",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 163
  },
  {
    "slug": "cosent二特征式匹配与交互式匹配有多大差距",
    "title": "CoSENT（二）：特征式匹配与交互式匹配有多大差距？",
    "description": "CoSENT（二）：特征式匹配与交互式匹配有多大差距？&para;\n原文链接: https://spaces.ac.cn/archives/8860\n发布日期: 2022-01-12\n\n\n\n### 核心问题\n\n文本匹配有两种主流方案：\n\n**特征式（Representation-based）**：\n- 两个句子分别编码为句向量\n- 通过cos或浅层网络融合\n- 优势：效率高，可缓存句向量\n- 劣势...",
    "date": "2022-01-12",
    "source": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "句向量",
      "文本匹配",
      "Powell优化"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 162
  },
  {
    "slug": "cosent一比sentence-bert更有效的句向量方案",
    "title": "CoSENT（一）：比Sentence-BERT更有效的句向量方案",
    "description": "CoSENT（一）：比Sentence-BERT更有效的句向量方案&para;\n原文链接: https://spaces.ac.cn/archives/8847\n发布日期: 2022-01-06\n\n\n\n### 核心创新\n\n**问题背景**：\n- 有监督句向量的主流方案（InferSent、Sentence-BERT）存在**训练-预测不一致**问题\n- 直接优化余弦相似度往往效果很差，甚至不如随机...",
    "date": "2022-01-06",
    "source": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型",
      "attention",
      "Circle Loss",
      "排序损失",
      "Sentence-BERT",
      "困难负样本",
      "Spearman"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 161
  },
  {
    "slug": "squareplus可能是运算最简单的relu光滑近似",
    "title": "SquarePlus：可能是运算最简单的ReLU光滑近似",
    "description": "SquarePlus：可能是运算最简单的ReLU光滑近似&para;\n原文链接: https://spaces.ac.cn/archives/8833\n发布日期: \n\nReLU函数，也就是$\\max(x,0)$，是最常见的激活函数之一，然而它在$x=0$处的不可导通常也被视为一个“槽点”。为此，有诸多的光滑近似被提出，比如SoftPlus、GeLU、Swish等，不过这些光滑近似无一例外地至少都使...",
    "date": "2021-12-29",
    "source": "",
    "tags": [
      "函数",
      "近似",
      "分析",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 160
  },
  {
    "slug": "概率分布的熵归一化entropy-normalization",
    "title": "概率分布的熵归一化（Entropy Normalization）",
    "description": "概率分布的熵归一化（Entropy Normalization）&para;\n原文链接: https://spaces.ac.cn/archives/8829\n发布日期: \n\n在上一篇文章《从熵不变性看Attention的Scale操作》中，我们从熵不变性的角度推导了一个新的Attention Scale，并且实验显示具有熵不变性的新Scale确实能使得Attention的外推性能更好。这时候笔者...",
    "date": "2021-12-24",
    "source": "",
    "tags": [
      "概率",
      "熵",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 159
  },
  {
    "slug": "从熵不变性看attention的scale操作",
    "title": "从熵不变性看Attention的Scale操作",
    "description": "从熵不变性看Attention的Scale操作&para;\n原文链接: https://spaces.ac.cn/archives/8823\n发布日期: \n\n当前Transformer架构用的最多的注意力机制，全称为“Scaled Dot-Product Attention”，其中“Scaled”是因为在$Q,K$转置相乘之后还要除以一个$\\sqrt{d}$再做Softmax（下面均不失一般性地假...",
    "date": "2021-12-21",
    "source": "",
    "tags": [
      "概率",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 158
  },
  {
    "slug": "seq2seq前缀树检索任务新范式以kgclue为例",
    "title": "Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）",
    "description": "Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）&para;\n原文链接: https://spaces.ac.cn/archives/8802\n发布日期: \n\n两年前，在《万能的seq2seq：基于seq2seq的阅读理解问答》和《“非自回归”也不差：基于MLM的阅读理解问答》中，我们在尝试过分别利用“Seq2Seq+前缀树”和“MLM+前缀树”的方式做抽取式阅读理解任务，并获得了不错...",
    "date": "2021-12-17",
    "source": "",
    "tags": [
      "代码",
      "语义",
      "keras",
      "相似度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 157
  },
  {
    "slug": "输入梯度惩罚与参数梯度惩罚的一个不等式",
    "title": "输入梯度惩罚与参数梯度惩罚的一个不等式",
    "description": "输入梯度惩罚与参数梯度惩罚的一个不等式&para;\n原文链接: https://spaces.ac.cn/archives/8796\n发布日期: \n\n在本博客中，已经多次讨论过梯度惩罚相关内容了。从形式上来看，梯度惩罚项分为两种，一种是关于输入的梯度惩罚$\\Vert\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x};\\boldsymbol{\\theta})\\Vert^...",
    "date": "2021-12-11",
    "source": "",
    "tags": [
      "不等式",
      "优化",
      "梯度",
      "泛化",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 156
  },
  {
    "slug": "变分自编码器八估计样本概率密度",
    "title": "变分自编码器（八）：估计样本概率密度",
    "description": "变分自编码器（八）：估计样本概率密度&para;\n原文链接: https://spaces.ac.cn/archives/8791\n发布日期: \n\n在本系列的前面几篇文章中，我们已经从多个角度来理解了VAE，一般来说，用VAE是为了得到一个生成模型，或者是做更好的编码模型，这都是VAE的常规用途。但除了这些常规应用外，还有一些“小众需求”，比如用来估计$x$的概率密度，这在做压缩的时候通常会用到。...",
    "date": "2021-12-09",
    "source": "",
    "tags": [
      "概率",
      "变分",
      "vae",
      "生成模型",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 155
  },
  {
    "slug": "dropout视角下的mlm和mae一些新的启发",
    "title": "Dropout视角下的MLM和MAE：一些新的启发",
    "description": "Dropout视角下的MLM和MAE：一些新的启发&para;\n原文链接: https://spaces.ac.cn/archives/8770\n发布日期: \n\n大家都知道，BERT的MLM（Masked Language Model）任务在预训练和微调时的不一致，也就是预训练出现了[MASK]而下游任务微调时没有[MASK]，是经常被吐槽的问题，很多工作都认为这是影响BERT微调性能的重要原因，...",
    "date": "2021-11-29",
    "source": "",
    "tags": [
      "模型",
      "概率",
      "分析",
      "优化",
      "生成模型",
      "Dropout",
      "MLM",
      "MAE",
      "正则化",
      "BERT"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 154
  },
  {
    "slug": "childtuning试试把dropout加到梯度上去",
    "title": "ChildTuning：试试把Dropout加到梯度上去？",
    "description": "ChildTuning：试试把Dropout加到梯度上去？&para;\n原文链接: https://spaces.ac.cn/archives/8764\n发布日期: \n\nDropout是经典的防止过拟合的思路了，想必很多读者已经了解过它。有意思的是，最近Dropout有点“老树发新芽”的感觉，出现了一些有趣的新玩法，比如最近引起过热议的SimCSE和R-Drop，尤其是在文章《又是Dropout两...",
    "date": "2021-11-22",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention",
      "微调",
      "Fisher信息",
      "正则化",
      "Adam",
      "参数高效"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 153
  },
  {
    "slug": "wgan新方案通过梯度归一化来实现l约束",
    "title": "WGAN新方案：通过梯度归一化来实现L约束",
    "description": "WGAN新方案：通过梯度归一化来实现L约束&para;\n原文链接: https://spaces.ac.cn/archives/8757\n发布日期: \n\n当前，WGAN主流的实现方式包括参数裁剪（Weight Clipping）、谱归一化（Spectral Normalization）、梯度惩罚（Gradient Penalty），本来则来介绍一种新的实现方案：梯度归一化（Gradient Nor...",
    "date": "2021-11-15",
    "source": "",
    "tags": [
      "无监督",
      "GAN",
      "生成模型",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 152
  },
  {
    "slug": "模型优化漫谈bert的初始标准差为什么是002",
    "title": "模型优化漫谈：BERT的初始标准差为什么是0.02？",
    "description": "模型优化漫谈：BERT的初始标准差为什么是0.02？&para;\n原文链接: https://spaces.ac.cn/archives/8747\n发布日期: \n\n前几天在群里大家讨论到了“Transformer如何解决梯度消失”这个问题，答案有提到残差的，也有提到LN（Layer Norm）的。这些是否都是正确答案呢？事实上这是一个非常有趣而综合的问题，它其实关联到挺多模型细节，比如“BERT为...",
    "date": "2021-11-08",
    "source": "",
    "tags": [
      "模型",
      "分析",
      "优化",
      "梯度",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 151
  },
  {
    "slug": "bert4keras在手baseline我有clue基准代码",
    "title": "bert4keras在手，baseline我有：CLUE基准代码",
    "description": "bert4keras在手，baseline我有：CLUE基准代码&para;\n原文链接: https://spaces.ac.cn/archives/8739\n发布日期: \n\nCLUE（Chinese GLUE）是中文自然语言处理的一个评价基准，目前也已经得到了较多团队的认可。CLUE官方Github提供了tensorflow和pytorch的baseline，但并不易读，而且也不方便调试。事实上...",
    "date": "2021-10-31",
    "source": "",
    "tags": [
      "模型",
      "代码",
      "keras",
      "生成模型",
      "attention"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 150
  },
  {
    "slug": "can借助先验分布提升分类性能的简单后处理技巧",
    "title": "CAN：借助先验分布提升分类性能的简单后处理技巧",
    "description": "CAN：借助先验分布提升分类性能的简单后处理技巧&para;\n原文链接: https://spaces.ac.cn/archives/8728\n发布日期: \n\n顾名思义，本文将会介绍一种用于分类问题的后处理技巧——CAN（Classification with Alternating Normalization），出自论文《When in Doubt: Improving Classificati...",
    "date": "2021-10-22",
    "source": "",
    "tags": [
      "模型",
      "概率",
      "分析",
      "技巧",
      "生成模型"
    ],
    "status": "completed",
    "tags_reviewed": false,
    "post_number": 149
  },
  {
    "slug": "初始化方法中非方阵的维度平均策略思考",
    "title": "初始化方法中非方阵的维度平均策略思考",
    "description": "初始化方法中非方阵的维度平均策略思考&para;\n原文链接: https://spaces.ac.cn/archives/8725\n发布日期: 2021-10-18\n\n\n\n### 核心问题\n\n对于 $m \\times n$ 的非方阵权重矩阵，如何选择初始化方差以同时满足：\n- 前向传播时信号方差保持稳定\n- 反向传播时梯度方差保持稳定\n\n**三种候选平均策略**：\n1. **几何平均**：$\\sq...",
    "date": "2021-10-18",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "初始化",
      "Xavier",
      "方差传播"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 148
  },
  {
    "slug": "用狄拉克函数来构造非光滑函数的光滑近似",
    "title": "用狄拉克函数来构造非光滑函数的光滑近似",
    "description": "用狄拉克函数来构造非光滑函数的光滑近似&para;\n原文链接: https://spaces.ac.cn/archives/8718\n发布日期: 2021-10-10\n\n\n\n### 核心思想\n\n通过狄拉克函数的光滑近似与目标函数卷积，可以系统性地构造**任意非光滑函数的光滑近似**。这种方法：\n\n- ✅ **通用性强**：适用于有可数个间断点的函数\n- ✅ **理论保证**：基于狄拉克函数的筛选性...",
    "date": "2021-10-10",
    "source": "",
    "tags": [
      "函数",
      "近似",
      "分析",
      "光滑",
      "生成模型",
      "狄拉克函数",
      "卷积",
      "激活函数"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 147
  },
  {
    "slug": "从emdwmd到wrd文本向量序列的相似度计算",
    "title": "从EMD、WMD到WRD：文本向量序列的相似度计算",
    "description": "从EMD、WMD到WRD：文本向量序列的相似度计算&para;\n原文链接: https://spaces.ac.cn/archives/7388\n发布日期: \n\n在NLP中，我们经常要去比较两个句子的相似度，其标准方法是想办法将句子编码为固定大小的向量，然后用某种几何距离（欧氏距离、$\\cos$距离等）作为相似度。这种方案相对来说比较简单，而且检索起来比较快速，一定程度上能满足工程需求。\n此外，还...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7388",
    "tags": [
      "最优",
      "优化",
      "语义",
      "线性规划",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 1
  },
  {
    "slug": "bert4keras在手baseline我有百度lic2020",
    "title": "bert4keras在手，baseline我有：百度LIC2020",
    "description": "bert4keras在手，baseline我有：百度LIC2020&para;\n原文链接: https://spaces.ac.cn/archives/7321\n发布日期: \n\n百度的“2020语言与智能技术竞赛”开赛了，今年有五个赛道，分别是机器阅读理解、推荐任务对话、语义解析、关系抽取、事件抽取。每个赛道中，主办方都给出了基于PaddlePaddle的baseline模型，这里笔者也基于ber...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7321",
    "tags": [
      "模型",
      "keras",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 2
  },
  {
    "slug": "让mathjax更好地兼容谷歌翻译和延时加载",
    "title": "让MathJax更好地兼容谷歌翻译和延时加载",
    "description": "让MathJax更好地兼容谷歌翻译和延时加载&para;\n原文链接: https://spaces.ac.cn/archives/10320\n发布日期: \n\n很早之前，就有读者提出希望把Cool Papers上面的数学公式渲染一下，因为很多偏数学的论文，它们的摘要甚至标题上都带有LaTeX代码写的数学公式，如果不把这些公式渲染出来，那么看上去就像是一堆乱码，确实会比较影响阅读体验。然而，之前的测试...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10320",
    "tags": [
      "网站",
      "latex",
      "论文",
      "酷论文",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 3
  },
  {
    "slug": "搜狐文本匹配基于条件layernorm的多任务baseline",
    "title": "搜狐文本匹配：基于条件LayerNorm的多任务baseline",
    "description": "搜狐文本匹配：基于条件LayerNorm的多任务baseline&para;\n原文链接: https://spaces.ac.cn/archives/8337\n发布日期: \n\n前段时间看到了“2021搜狐校园文本匹配算法大赛”，觉得赛题颇有意思，便尝试了一下，不过由于比赛本身只是面向在校学生，所以笔者是不能作为正式参赛人员参赛的，因此把自己的做法开源出来，作为比赛baseline供大家参考。\n\nG...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8337",
    "tags": [
      "语言模型",
      "比赛",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 4
  },
  {
    "slug": "让炼丹更科学一些三sgd的终",
    "title": "让炼丹更科学一些（三）：SGD的终点损失收敛",
    "description": "让炼丹更科学一些（三）：SGD的终点损失收敛&para;\n原文链接: https://spaces.ac.cn/archives/11480\n发布日期: \n\n目前我们已经有两篇文章讨论SGD的收敛性质，不过它们都只是损失值的收敛结果，所以它们只保证我们能找到最优的损失值，但不能保证找到最优值的所在位置$\\boldsymbol{\\theta}^$，这是目前的结论跟实践之间的一个显著gap。直觉上，训...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11480",
    "tags": [
      "不等式",
      "优化器",
      "sgd",
      "炼丹",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 5
  },
  {
    "slug": "让炼丹更科学一些二将结论推广",
    "title": "让炼丹更科学一些（二）：将结论推广到无界域",
    "description": "让炼丹更科学一些（二）：将结论推广到无界域&para;\n原文链接: https://spaces.ac.cn/archives/11469\n发布日期: \n\n两年前，笔者打算开一个“科学炼丹”专题，本想着系统整理一下优化器的经典理论结果，但写了第一篇《让炼丹更科学一些（一）：SGD的平均损失收敛》后，就一直搁置至今。主要原因在于，笔者总觉得这些经典优化结论所依赖的条件过于苛刻，跟实际应用相去甚远，尤...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11469",
    "tags": [
      "不等式",
      "优化器",
      "sgd",
      "炼丹",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 6
  },
  {
    "slug": "一个二值化词向量模型是怎么跟果蝇搭上关系的",
    "title": "一个二值化词向量模型，是怎么跟果蝇搭上关系的？",
    "description": "一个二值化词向量模型，是怎么跟果蝇搭上关系的？&para;\n原文链接: https://spaces.ac.cn/archives/8159\n发布日期: \n\n\n果蝇（图片来自Google搜索）\n可能有些读者最近会留意到ICLR 2021的论文《Can a Fruit Fly Learn Word Embeddings?》，文中写到它是基于仿生思想（仿果蝇的嗅觉回路）做出来的一个二值化词向量模型。其...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8159",
    "tags": [
      "自然语言处理",
      "词向量",
      "NLP",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 7
  },
  {
    "slug": "muon优化器指南快速上手与关键细节",
    "title": "Muon优化器指南：快速上手与关键细节",
    "description": "Muon优化器指南：快速上手与关键细节&para;\n原文链接: https://spaces.ac.cn/archives/11416\n发布日期: \n\n这段时间，相信很多读者已经刷到过Muon优化器的相关消息。实际上，Muon的提出时间大致是去年的10月份，由 Keller Jordan 在推特上提出，距今也不过一年多一点。然而，就在这一年里，Muon已经经历了百亿、千亿乃至万亿参数模型的训练考验...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11416",
    "tags": [
      "矩阵",
      "优化",
      "优化器",
      "muon",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 8
  },
  {
    "slug": "提速不掉点基于词颗粒度的中文wobert",
    "title": "提速不掉点：基于词颗粒度的中文WoBERT",
    "description": "提速不掉点：基于词颗粒度的中文WoBERT&para;\n原文链接: https://spaces.ac.cn/archives/7758\n发布日期: \n\n当前，大部分中文预训练模型都是以字为基本单位的，也就是说中文语句会被拆分为一个个字。中文也有一些多颗粒度的语言模型，比如创新工场的ZEN和字节跳动的AMBERT，但这类模型的基本单位还是字，只不过想办法融合了词信息。目前以词为单位的中文预训练模型...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7758",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 9
  },
  {
    "slug": "线性注意力简史从模仿创新到反哺",
    "title": "线性注意力简史：从模仿、创新到反哺",
    "description": "线性注意力简史：从模仿、创新到反哺&para;\n原文链接: https://spaces.ac.cn/archives/11033\n发布日期: \n\n在中文圈，本站应该算是比较早关注线性Attention的了，在2020年写首篇相关博客《线性Attention的探索：Attention必须有个Softmax吗？》时，大家主要讨论的还是BERT相关的Softmax Attention。事后来看，在BE...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11033",
    "tags": [
      "SSM",
      "SSM",
      "SSM",
      "线性",
      "RNN"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 10
  },
  {
    "slug": "对抗训练浅谈意义方法和思考附keras实现",
    "title": "对抗训练浅谈：意义、方法和思考（附Keras实现）",
    "description": "对抗训练浅谈：意义、方法和思考（附Keras实现）&para;\n原文链接: https://spaces.ac.cn/archives/7234\n发布日期: \n\n当前，说到深度学习中的对抗，一般会有两个含义：一个是生成对抗网络（Generative Adversarial Networks，GAN），代表着一大类先进的生成模型；另一个则是跟对抗攻击、对抗样本相关的领域，它跟GAN相关，但又很不一样...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7234",
    "tags": [
      "GAN",
      "keras",
      "对抗训练",
      "泛化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 11
  },
  {
    "slug": "用albert和electra之前请确认你真的了解它们",
    "title": "用ALBERT和ELECTRA之前，请确认你真的了解它们",
    "description": "用ALBERT和ELECTRA之前，请确认你真的了解它们&para;\n原文链接: https://spaces.ac.cn/archives/7846\n发布日期: \n\n在预训练语言模型中，ALBERT和ELECTRA算是继BERT之后的两个“后起之秀”。它们从不同的角度入手对BERT进行了改进，最终提升了效果（至少在不少公开评测数据集上是这样），因此也赢得了一定的口碑。但在平时的交流学习中，笔者发...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7846",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 12
  },
  {
    "slug": "鱼与熊掌兼得融合检索和生成的simbert模型",
    "title": "鱼与熊掌兼得：融合检索和生成的SimBERT模型",
    "description": "鱼与熊掌兼得：融合检索和生成的SimBERT模型&para;\n原文链接: https://spaces.ac.cn/archives/7427\n发布日期: \n\n前段时间我们开放了一个名为SimBERT的模型权重，它是以Google开源的BERT模型为基础，基于微软的UniLM思想设计了融检索与生成于一体的任务，来进一步微调后得到的模型，所以它同时具备相似问生成和相似句检索能力。不过当时除了放出一个...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7427",
    "tags": [
      "语言模型",
      "生成模型",
      "文本生成",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 13
  },
  {
    "slug": "crf用过了不妨再了解下更快的memm",
    "title": "CRF用过了，不妨再了解下更快的MEMM？",
    "description": "CRF用过了，不妨再了解下更快的MEMM？&para;\n原文链接: https://spaces.ac.cn/archives/7213\n发布日期: \n\nHMM、MEMM、CRF被称为是三大经典概率图模型，在深度学习之前的机器学习时代，它们被广泛用于各种序列标注相关的任务中。一个有趣的现象是，到了深度学习时代，HMM和MEMM似乎都“没落”了，舞台上就只留下CRF。相信做NLP的读者朋友们就算没亲...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7213",
    "tags": [
      "模型",
      "概率图",
      "crf",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 14
  },
  {
    "slug": "gelu的两个初等函数近似是怎么来的",
    "title": "GELU的两个初等函数近似是怎么来的",
    "description": "GELU的两个初等函数近似是怎么来的&para;\n原文链接: https://spaces.ac.cn/archives/7309\n发布日期: \n\nGELU，全称为Gaussian Error Linear Unit，也算是RELU的变种，是一个非初等函数形式的激活函数。它由论文《Gaussian Error Linear Units (GELUs)》提出，后来被用到了GPT中，再后来被用在了BE...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7309",
    "tags": [
      "近似",
      "分析",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 15
  },
  {
    "slug": "两个多元正态分布的kl散度巴氏距离和w距离",
    "title": "两个多元正态分布的KL散度、巴氏距离和W距离",
    "description": "两个多元正态分布的KL散度、巴氏距离和W距离&para;\n原文链接: https://spaces.ac.cn/archives/8512\n发布日期: \n\n正态分布是最常见的连续型概率分布之一。它是给定均值和协方差后的最大熵分布（参考《“熵”不起：从熵、最大熵原理到最大熵模型（二）》），也可以看作任意连续型分布的二阶近似，它的地位就相当于一般函数的线性近似。从这个角度来看，正态分布算得上是最简单的...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8512",
    "tags": [
      "概率",
      "矩阵",
      "优化",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 16
  },
  {
    "slug": "中文任务还是sota吗我们给simcse补充了一些实验",
    "title": "中文任务还是SOTA吗？我们给SimCSE补充了一些实验",
    "description": "中文任务还是SOTA吗？我们给SimCSE补充了一些实验&para;\n原文链接: https://spaces.ac.cn/archives/8348\n发布日期: \n\n今年年初，笔者受到BERT-flow的启发，构思了成为“BERT-whitening”的方法，并一度成为了语义相似度的新SOTA（参考《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》，论文为《Whitening...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8348",
    "tags": [
      "语言模型",
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 17
  },
  {
    "slug": "seq2seq重复解码现象的理论分析尝试",
    "title": "Seq2Seq重复解码现象的理论分析尝试",
    "description": "Seq2Seq重复解码现象的理论分析尝试&para;\n原文链接: https://spaces.ac.cn/archives/8128\n发布日期: \n\n去年笔者写过博文《如何应对Seq2Seq中的“根本停不下来”问题？》，里边介绍了一篇论文中对Seq2Seq解码不停止现象的处理，并指出那篇论文只是提了一些应对该问题的策略，并没有提供原理上的理解。近日，笔者在Arixv读到了AAAI 2021的一篇...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8128",
    "tags": [
      "矩阵",
      "语言模型",
      "文本生成",
      "解码",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 18
  },
  {
    "slug": "为什么deltanet要加l2-n",
    "title": "为什么DeltaNet要加L2 Normalize？",
    "description": "为什么DeltaNet要加L2 Normalize？&para;\n原文链接: https://spaces.ac.cn/archives/11486\n发布日期: \n\n在文章《线性注意力简史：从模仿、创新到反哺》中，我们介绍了DeltaNet，它把Delta Rule带进了线性注意力中，成为其强有力的工具之一，并构成GDN、KDA等后续工作的基础。不过，那篇文章我们主要着重于DeltaNet的整体思...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11486",
    "tags": [
      "微分方程",
      "线性",
      "RNN",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 19
  },
  {
    "slug": "动手做个dialogpt基于lm的生成式多轮对话模型",
    "title": "动手做个DialoGPT：基于LM的生成式多轮对话模型",
    "description": "动手做个DialoGPT：基于LM的生成式多轮对话模型&para;\n原文链接: https://spaces.ac.cn/archives/7718\n发布日期: \n\n前段时间刷Arixv的时候，发现清华大学开源了一个大规模的中文闲聊语料库LCCC（论文链接，项目地址），从开源的文件上来看，这可能是目前开源的数量最大、质量最好的闲聊语料库了，而且还包含了部分多轮对话聊天，总的来说可玩性还是蛮强的。笔...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7718",
    "tags": [
      "语言模型",
      "文本生成",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 20
  },
  {
    "slug": "我们真的需要把训练集的损失降低到零吗",
    "title": "我们真的需要把训练集的损失降低到零吗？",
    "description": "我们真的需要把训练集的损失降低到零吗？&para;\n原文链接: https://spaces.ac.cn/archives/7643\n发布日期: \n\n在训练模型的时候，我们需要损失函数一直训练到0吗？显然不用。一般来说，我们是用训练集来训练模型，但希望的是验证集的损失越小越好，而正常来说训练集的损失降低到一定值后，验证集的损失就会开始上升，因此没必要把训练集的损失降低到0。\n既然如此，在已经达到了...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7643",
    "tags": [
      "优化",
      "深度学习",
      "损失函数",
      "泛化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 21
  },
  {
    "slug": "通过互信息思想来缓解类别不平衡问题",
    "title": "通过互信息思想来缓解类别不平衡问题",
    "description": "通过互信息思想来缓解类别不平衡问题&para;\n原文链接: https://spaces.ac.cn/archives/7615\n发布日期: \n\n类别不平衡问题，也叫“长尾问题”，是机器学习面临的常见问题之一，尤其是来源于真实场景下的数据集，几乎都是类别不平衡的。大概在两年前，笔者也思考过这个问题，当时正好对“互信息”相关的内容颇有心得，所以构思了一种基于互信息思想的解决办法，但又想了一下，那思路...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7615",
    "tags": [
      "优化",
      "互信息",
      "损失函数",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 22
  },
  {
    "slug": "殊途同归的策略梯度与零阶优化",
    "title": "殊途同归的策略梯度与零阶优化",
    "description": "殊途同归的策略梯度与零阶优化&para;\n原文链接: https://spaces.ac.cn/archives/7737\n发布日期: \n\n深度学习如此成功的一个巨大原因就是基于梯度的优化算法（SGD、Adam等）能有效地求解大多数神经网络模型。然而，既然是基于梯度，那么就要求模型是可导的，但随着研究的深入，我们时常会有求解不可导模型的需求，典型的例子就是直接优化准确率、F1、BLEU等评测指标，...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7737",
    "tags": [
      "优化",
      "梯度",
      "优化器",
      "强化学习",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 23
  },
  {
    "slug": "cool-papers更新简单适配zotero-connector",
    "title": "Cool Papers更新：简单适配Zotero Connector",
    "description": "Cool Papers更新：简单适配Zotero Connector&para;\n原文链接: https://spaces.ac.cn/archives/11250\n发布日期: \n\n很早之前就有读者提出希望可以给Cool Papers增加导入Zotero的功能，但由于笔者没用Zotero，加上又比较懒，所以一直没提上日程。这个周末刚好有点时间，研究了一下，做了个简单的适配。\n单篇导入&para;...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11250",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 24
  },
  {
    "slug": "基于树莓派zero2w搭建一个随身旁路由",
    "title": "基于树莓派Zero2W搭建一个随身旁路由",
    "description": "基于树莓派Zero2W搭建一个随身旁路由&para;\n原文链接: https://spaces.ac.cn/archives/11206\n发布日期: \n\n前段时间搞了个很迷你的开发板树莓派Zero2W（下面简称“Pi ”），还搭配了个USB Key转接板，这几天折腾了一下，用于实现一个随身的旁路由。本文记录一下关键技术点，供有同样需求的读者参考。\n\n树莓派Zero2W\n背景描述&para;\n折腾过...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11206",
    "tags": [
      "linux",
      "网络",
      "路由器",
      "智能家居",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 25
  },
  {
    "slug": "用开源的人工标注数据来增强roformer-sim",
    "title": "用开源的人工标注数据来增强RoFormer-Sim",
    "description": "用开源的人工标注数据来增强RoFormer-Sim&para;\n原文链接: https://spaces.ac.cn/archives/8541\n发布日期: \n\n大家知道，从SimBERT到SimBERTv2（RoFormer-Sim），我们算是为中文文本相似度任务建立了一个还算不错的基准模型。然而，SimBERT和RoFormer-Sim本质上都只是“弱监督”模型，跟“无监督”类似，我们不能指望...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8541",
    "tags": [
      "语言模型",
      "生成模型",
      "文本生成",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 26
  },
  {
    "slug": "2020年全年天象",
    "title": "2020年全年天象",
    "description": "2020年全年天象&para;\n原文链接: https://spaces.ac.cn/archives/7144\n发布日期: \n\nAstronomy Calendar of Celestial Events\n2020年全年天象\n翻译自NASA：http://eclipse.gsfc.nasa.gov/SKYCAL/SKYCAL.html\n（北京时间）\n2011年版本\n2012年版本\n2013年版本...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7144",
    "tags": [
      "天象",
      "天文",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 27
  },
  {
    "slug": "t5-pegasus开源一个中文生成式预训练模型",
    "title": "T5 PEGASUS：开源一个中文生成式预训练模型",
    "description": "T5 PEGASUS：开源一个中文生成式预训练模型&para;\n原文链接: https://spaces.ac.cn/archives/8209\n发布日期: \n\n去年在文章《那个屠榜的T5模型，现在可以在中文上玩玩了》中我们介绍了Google的多国语言版T5模型（mT5），并给出了用mT5进行中文文本生成任务的例子。诚然，mT5做中文生成任务也是一个可用的方案，但缺乏完全由中文语料训练出来模型总感...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8209",
    "tags": [
      "语言模型",
      "文本生成",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 28
  },
  {
    "slug": "cool-papers-站内搜索的一些新尝试",
    "title": "“Cool Papers + 站内搜索”的一些新尝试",
    "description": "“Cool Papers + 站内搜索”的一些新尝试&para;\n原文链接: https://spaces.ac.cn/archives/10311\n发布日期: \n\n在《Cool Papers更新：简单搭建了一个站内检索系统》这篇文章中，我们介绍了Cool Papers新增的站内搜索系统。搜索系统的目的，自然希望能够帮助用户快速找到他们需要的论文。然而，如何高效地检索到对自己有价值的结果，并不是一...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10311",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 29
  },
  {
    "slug": "对比学习可以使用梯度累积吗",
    "title": "对比学习可以使用梯度累积吗？",
    "description": "对比学习可以使用梯度累积吗？&para;\n原文链接: https://spaces.ac.cn/archives/8471\n发布日期: \n\n在之前的文章《用时间换取效果：Keras梯度累积优化器》中，我们介绍过“梯度累积”，它是在有限显存下实现大batch_size效果的一种技巧。一般来说，梯度累积适用的是loss是独立同分布的场景，换言之每个样本单独计算loss，然后总loss是所有单个loss...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8471",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 30
  },
  {
    "slug": "从动力学角度看优化算法六为什么simsiam不退化",
    "title": "从动力学角度看优化算法（六）：为什么SimSiam不退化？",
    "description": "从动力学角度看优化算法（六）：为什么SimSiam不退化？&para;\n原文链接: https://spaces.ac.cn/archives/7980\n发布日期: \n\n自SimCLR以来，CV中关于无监督特征学习的工作层出不穷，让人眼花缭乱。这些工作大多数都是基于对比学习的，即通过适当的方式构造正负样本进行分类学习的。然而，在众多类似的工作中总有一些特立独行的研究，比如Google的BYOL和最...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7980",
    "tags": [
      "动力学",
      "优化",
      "无监督",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 31
  },
  {
    "slug": "生活杂记炒锅的尽头是铁锅",
    "title": "【生活杂记】炒锅的尽头是铁锅",
    "description": "【生活杂记】炒锅的尽头是铁锅&para;\n原文链接: https://spaces.ac.cn/archives/9855\n发布日期: \n\n\n铁锅（网络图）\n很多会下厨的同学估计都纠结过一件事情，那就是炒锅的选择。\n对于炒锅的纠结，归根结底是不粘与方便的权衡。最简单的不粘锅自然是带涂层的不粘锅，如果家里的热源只有电磁炉，并且炒菜习惯比较温和，那么涂层不粘锅往往是最佳选择了。不过，一旦有了明火的燃气...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/9855",
    "tags": [
      "生活",
      "厨房",
      "美食",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 32
  },
  {
    "slug": "开局一段扯数据全靠编真被一篇神论文气到了",
    "title": "开局一段扯，数据全靠编？真被一篇“神论文”气到了",
    "description": "开局一段扯，数据全靠编？真被一篇“神论文”气到了&para;\n原文链接: https://spaces.ac.cn/archives/8783\n发布日期: \n\n这篇文章谈一下笔者被昨天出来的一篇“神论文”气到了的经历。\n这篇“神论文”是《How not to Lie with a Benchmark: Rearranging NLP Leaderboards》，论文的大致内容是说目前很多排行榜算平...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8783",
    "tags": [
      "情感",
      "模型",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 33
  },
  {
    "slug": "p-tuning自动构建模版释放语言模型潜能",
    "title": "P-tuning：自动构建模版，释放语言模型潜能",
    "description": "P-tuning：自动构建模版，释放语言模型潜能&para;\n原文链接: https://spaces.ac.cn/archives/8295\n发布日期: \n\n在之前的文章《必须要GPT3吗？不，BERT的MLM模型也能小样本学习》中，我们介绍了一种名为Pattern-Exploiting Training（PET）的方法，它通过人工构建的模版与BERT的MLM模型结合，能够起到非常好的零样本、小...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8295",
    "tags": [
      "语言模型",
      "NLP",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 34
  },
  {
    "slug": "wgan的成功可能跟wasserstein距离没啥关系",
    "title": "WGAN的成功，可能跟Wasserstein距离没啥关系",
    "description": "WGAN的成功，可能跟Wasserstein距离没啥关系&para;\n原文链接: https://spaces.ac.cn/archives/8244\n发布日期: \n\nWGAN，即Wasserstein GAN，算是GAN史上一个比较重要的理论突破结果，它将GAN中两个概率分布的度量从f散度改为了Wasserstein距离，从而使得WGAN的训练过程更加稳定，而且生成质量通常也更好。Wassers...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8244",
    "tags": [
      "GAN",
      "优化",
      "GAN",
      "生成模型",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 35
  },
  {
    "slug": "那个屠榜的t5模型现在可以在中文上玩玩了",
    "title": "那个屠榜的T5模型，现在可以在中文上玩玩了",
    "description": "那个屠榜的T5模型，现在可以在中文上玩玩了&para;\n原文链接: https://spaces.ac.cn/archives/7867\n发布日期: \n\n不知道大家对Google去年的屠榜之作T5还有没有印象？就是那个打着“万事皆可Seq2Seq”的旗号、最大搞了110亿参数、一举刷新了GLUE、SuperGLUE等多个NLP榜单的模型，而且过去一年了，T5仍然是SuperGLUE榜单上的第一，目...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7867",
    "tags": [
      "语言模型",
      "文本生成",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 36
  },
  {
    "slug": "曾被嫌弃的预训练任务nsp做出了优秀的zero-shot效果",
    "title": "曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果",
    "description": "曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果&para;\n原文链接: https://spaces.ac.cn/archives/8671\n发布日期: \n\n在五花八门的预训练任务设计中，NSP通常认为是比较糟糕的一种，因为它难度较低，加入到预训练中并没有使下游任务微调时有明显受益，甚至RoBERTa的论文显示它会带来负面效果。所以，后续的预训练工作一般有两种选择：一是像RoBER...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8671",
    "tags": [
      "无监督",
      "语言模型",
      "NLP",
      "模版",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 37
  },
  {
    "slug": "nyströmformer基于矩阵分解的线性化attention方案",
    "title": "Nyströmformer：基于矩阵分解的线性化Attention方案",
    "description": "Nyströmformer：基于矩阵分解的线性化Attention方案&para;\n原文链接: https://spaces.ac.cn/archives/8180\n发布日期: \n\n标准Attention的$\\mathcal{O}(n^2)$复杂度可真是让研究人员头大。前段时间我们在博文《Performer：用随机投影将Attention的复杂度线性化》中介绍了Google的Performer模型...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8180",
    "tags": [
      "矩阵",
      "语言模型",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 38
  },
  {
    "slug": "如何划分一个跟测试集更接近的验证集",
    "title": "如何划分一个跟测试集更接近的验证集？",
    "description": "如何划分一个跟测试集更接近的验证集？&para;\n原文链接: https://spaces.ac.cn/archives/7805\n发布日期: \n\n不管是打比赛、做实验还是搞工程，我们经常会遇到训练集与测试集分布不一致的情况。一般来说我们会从训练集中划分出来一个验证集，通过这个验证集来调整一些超参数（参考《训练集、验证集和测试集的意义》），比如控制模型的训练轮数以防止过拟合。然而，如果验证集本身跟...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7805",
    "tags": [
      "模型",
      "概率",
      "优化",
      "采样",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 39
  },
  {
    "slug": "seq2seq中exposure-bias现象的浅析与对策",
    "title": "Seq2Seq中Exposure Bias现象的浅析与对策",
    "description": "Seq2Seq中Exposure Bias现象的浅析与对策&para;\n原文链接: https://spaces.ac.cn/archives/7259\n发布日期: \n\n前些天笔者写了《CRF用过了，不妨再了解下更快的MEMM？》，里边提到了MEMM的局部归一化和CRF的全局归一化的优劣。同时，笔者联想到了Seq2Seq模型，因为Seq2Seq模型的典型训练方案Teacher Forcing就是一...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7259",
    "tags": [
      "语言模型",
      "文本生成",
      "对抗训练",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 40
  },
  {
    "slug": "google新作synthesizer我们还不够了解自注意力",
    "title": "Google新作Synthesizer：我们还不够了解自注意力",
    "description": "Google新作Synthesizer：我们还不够了解自注意力&para;\n原文链接: https://spaces.ac.cn/archives/7430\n发布日期: \n\n\n深度学习这个箱子，远比我们想象的要黑。\n\n写在开头&para;\n据说物理学家费曼说过一句话[来源]：“谁要是说他懂得量子力学，那他就是真的不懂量子力学。”我现在越来越觉得，这句话中的“量子力学”也可以替换为“深度学习”。尽管...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7430",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 41
  },
  {
    "slug": "cool-papers浏览器扩展升级至v020",
    "title": "Cool Papers浏览器扩展升级至v0.2.0",
    "description": "Cool Papers浏览器扩展升级至v0.2.0&para;\n原文链接: https://spaces.ac.cn/archives/10480\n发布日期: \n\n年初，我们在《更便捷的Cool Papers打开方式：Chrome重定向扩展》中发布了一个Chrome浏览器插件（Cool Papers Redirector v0.1.0），可以通过右击菜单从任意页面中重定向到Cool Papers中...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10480",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 42
  },
  {
    "slug": "突破瓶颈打造更强大的transformer",
    "title": "突破瓶颈，打造更强大的Transformer",
    "description": "突破瓶颈，打造更强大的Transformer&para;\n原文链接: https://spaces.ac.cn/archives/7325\n发布日期: \n\n自《Attention is All You Need》一文发布后，基于Multi-Head Attention的Transformer模型开始流行起来，而去年发布的BERT模型更是将Transformer模型的热度推上了又一个高峰。当然，技术...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7325",
    "tags": [
      "概率",
      "深度学习",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 43
  },
  {
    "slug": "你可能不需要bert-flow一个线性变换媲美bert-flow",
    "title": "你可能不需要BERT-flow：一个线性变换媲美BERT-flow",
    "description": "你可能不需要BERT-flow：一个线性变换媲美BERT-flow&para;\n原文链接: https://spaces.ac.cn/archives/8069\n发布日期: \n\nBERT-flow来自论文《On the Sentence Embeddings from Pre-trained Language Models》，中了EMNLP 2020，主要是用flow模型校正了BERT出来的句向量...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8069",
    "tags": [
      "语言模型",
      "语义",
      "flow",
      "语义相似度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 44
  },
  {
    "slug": "更便捷的cool-papers打开方式chrome重定向扩展",
    "title": "更便捷的Cool Papers打开方式：Chrome重定向扩展",
    "description": "更便捷的Cool Papers打开方式：Chrome重定向扩展&para;\n原文链接: https://spaces.ac.cn/archives/9978\n发布日期: \n\n一些铺垫&para;\n自Cool Papers上线以来，很多用户就建议笔者加入搜索功能，后面也确实在前端用JS简单做了个页面内搜索，解决了部分用户的需求，但仍有读者希望引入更完整的全局搜索。诚然，笔者理解这个需求确实是存在，但...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/9978",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 45
  },
  {
    "slug": "基于conditional-layer-normalization的条件文本生成",
    "title": "基于Conditional Layer Normalization的条件文本生成",
    "description": "基于Conditional Layer Normalization的条件文本生成&para;\n原文链接: https://spaces.ac.cn/archives/7124\n发布日期: \n\n从文章《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》中我们可以知道，只要配合适当的Attention Mask，Bert（或者其他Transformer模型）就可以用来做无条件生成（...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7124",
    "tags": [
      "语言模型",
      "文本生成",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 46
  },
  {
    "slug": "bert可以上几年级了seq2seq硬刚小学数学应用题",
    "title": "BERT可以上几年级了？Seq2Seq“硬刚”小学数学应用题",
    "description": "BERT可以上几年级了？Seq2Seq“硬刚”小学数学应用题&para;\n原文链接: https://spaces.ac.cn/archives/7809\n发布日期: \n\n\n“鸡兔同笼”的那些年\n“盈亏问题”、“年龄问题”、“植树问题”、“牛吃草问题”、“利润问题”...，小学阶段你是否曾被各种花样的数学应用题折磨过呢？没关系，现在机器学习模型也可以帮助我们去解答应用题了，来看看它可以上几年级了？...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7809",
    "tags": [
      "语言模型",
      "文本生成",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 47
  },
  {
    "slug": "个性邮箱",
    "title": "个性邮箱",
    "description": "个性邮箱&para;\n原文链接: https://spaces.ac.cn/archives/119\n发布日期: \n\n注：目前QQ域名邮箱已经不允许新增新账号，因此暂停申请。（2020年02月17日）\n简介&para;\n科学空间与腾讯公司联合推出以@spaces.ac.cn为后缀的QQ邮箱，欢迎QQ用户申请注册。\n腾讯公司可以说越来越强大了，之前已经提供了即时通讯软件(QQ)、电子邮箱、个人空间等...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/119",
    "tags": [
      "邮箱",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 48
  },
  {
    "slug": "变分自编码器六从几何视角来理解vae的尝试",
    "title": "变分自编码器（六）：从几何视角来理解VAE的尝试",
    "description": "变分自编码器（六）：从几何视角来理解VAE的尝试&para;\n原文链接: https://spaces.ac.cn/archives/7725\n发布日期: \n\n前段时间公司组织技术分享，轮到笔者时，大家希望我讲讲VAE。鉴于之前笔者也写过变分自编码器系列，所以对笔者来说应该也不是特别难的事情，因此就答应了下来，后来仔细一想才觉得犯难：怎么讲才好呢？\n对于VAE来说，之前笔者有两篇比较系统的介绍：《...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7725",
    "tags": [
      "变分",
      "无监督",
      "vae",
      "生成模型",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 49
  },
  {
    "slug": "浅谈transformer的初始化参数化与标准化",
    "title": "浅谈Transformer的初始化、参数化与标准化",
    "description": "浅谈Transformer的初始化、参数化与标准化&para;\n原文链接: https://spaces.ac.cn/archives/8620\n发布日期: \n\n前几天在训练一个新的Transformer模型的时候，发现怎么训都不收敛了。经过一番debug，发现是在做Self Attention的时候$\\boldsymbol{Q}\\boldsymbol{K}^{\\top}$之后忘记除以$\\sqrt...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8620",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 50
  },
  {
    "slug": "transformer升级之路5作为无限维的线性attention",
    "title": "Transformer升级之路：5、作为无限维的线性Attention",
    "description": "Transformer升级之路：5、作为无限维的线性Attention&para;\n原文链接: https://spaces.ac.cn/archives/8601\n发布日期: \n\n在《Performer：用随机投影将Attention的复杂度线性化》中我们了解到Google提出的Performer模型，它提出了一种随机投影方案，可以将标准Attention转化为线性Attention，并保持一定...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8601",
    "tags": [
      "语言模型",
      "attention",
      "核方法",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 51
  },
  {
    "slug": "近乎完美地解决mathjax与marked的冲突",
    "title": "近乎完美地解决MathJax与Marked的冲突",
    "description": "近乎完美地解决MathJax与Marked的冲突&para;\n原文链接: https://spaces.ac.cn/archives/10332\n发布日期: \n\n在《让MathJax更好地兼容谷歌翻译和延时加载》我们提到Cool Papers加入了MathJax来解析LaTeX公式，不过万万没想到引发了诸多兼容性问题，虽然部分问题纯粹是笔者的强迫症作祟，但一个尽可能完美的解决方案终究是让人赏心悦目...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10332",
    "tags": [
      "网站",
      "latex",
      "论文",
      "酷论文",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 52
  },
  {
    "slug": "transformer升级之路19第二类旋转位置编码",
    "title": "Transformer升级之路：19、第二类旋转位置编码",
    "description": "Transformer升级之路：19、第二类旋转位置编码&para;\n原文链接: https://spaces.ac.cn/archives/10862\n发布日期: \n\n持续将“Transformer升级之路”系列关注到本篇的读者，想必都已经对旋转位置编码（RoPE）有所了解。简单来说，RoPE是施加在Attention的Query（$\\boldsymbol{Q}$）和Key（$\\boldsymb...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10862",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "rope",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 53
  },
  {
    "slug": "节省显存的重计算技巧也有了keras版了",
    "title": "节省显存的重计算技巧也有了Keras版了",
    "description": "节省显存的重计算技巧也有了Keras版了&para;\n原文链接: https://spaces.ac.cn/archives/7367\n发布日期: \n\n不少读者最近可能留意到了公众号文章《BERT重计算：用22.5%的训练时间节省5倍的显存开销（附代码）》，里边介绍了一个叫做“重计算”的技巧，简单来说就是用来省显存的方法，让平均训练速度慢一点，但batch_size可以增大好几倍。该技巧首先发布于...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7367",
    "tags": [
      "模型",
      "深度学习",
      "keras",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 54
  },
  {
    "slug": "让炼丹更科学一些四新恒等式",
    "title": "让炼丹更科学一些（四）：新恒等式，新学习率",
    "description": "让炼丹更科学一些（四）：新恒等式，新学习率&para;\n原文链接: https://spaces.ac.cn/archives/11494\n发布日期: \n\n上篇文章《让炼丹更科学一些（三）：SGD的终点损失收敛》中我们成功将收敛结论从平均损失转化成终点损失，得到了$\\mathcal{O}(\\sqrt{\\ln T/T})$的收敛速度。然而，仔细思考之下我们会发现这个结果其实不大符合直觉：按照经验，终...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11494",
    "tags": [
      "学习率",
      "优化器",
      "sgd",
      "炼丹",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 55
  },
  {
    "slug": "搜出来的文本三基于bert的文本采样",
    "title": "【搜出来的文本】⋅（三）基于BERT的文本采样",
    "description": "【搜出来的文本】⋅（三）基于BERT的文本采样&para;\n原文链接: https://spaces.ac.cn/archives/8119\n发布日期: \n\n从这一篇开始，我们就将前面所介绍的采样算法应用到具体的文本生成例子中。而作为第一个例子，我们将介绍如何利用BERT来进行文本随机采样。所谓文本随机采样，就是从模型中随机地产生一些自然语言句子出来，通常的观点是这种随机采样是GPT2、GPT3这...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8119",
    "tags": [
      "文本生成",
      "采样",
      "离散优化",
      "MCMC",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 56
  },
  {
    "slug": "关于维度公式n-833-log-n的可用性分析",
    "title": "关于维度公式“n > 8.33 log N”的可用性分析",
    "description": "关于维度公式“n &gt; 8.33 log N”的可用性分析&para;\n原文链接: https://spaces.ac.cn/archives/8711\n发布日期: \n\n在之前的文章《最小熵原理（六）：词向量的维度应该怎么选择？》中，我们基于最小熵思想推导出了一个词向量维度公式“$n &gt; 8.33\\log N$”，然后在《让人惊叹的Johnson-Lindenstrauss引理：应用篇》...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8711",
    "tags": [
      "维度",
      "熵",
      "词向量",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 57
  },
  {
    "slug": "智能家居之热水器零冷水技术原理浅析",
    "title": "智能家居之热水器零冷水技术原理浅析",
    "description": "智能家居之热水器零冷水技术原理浅析&para;\n原文链接: https://spaces.ac.cn/archives/9405\n发布日期: \n\n如果家庭使用单一的热水器集中供热水，那么当我们想要用热水时，往往需要先放一段时间的冷水，而如果放冷水时间比较长的话，就会比较影响体验。所谓零冷水，实际上就是想办法提前把热水管中的冷水排放掉，以达到（几乎）瞬间出热水的效果。事实上，零冷水并不是什么高大上的...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/9405",
    "tags": [
      "智能家居",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 58
  },
  {
    "slug": "智能家居之手搓一套能接入米家的零冷水装置",
    "title": "智能家居之手搓一套能接入米家的零冷水装置",
    "description": "智能家居之手搓一套能接入米家的零冷水装置&para;\n原文链接: https://spaces.ac.cn/archives/10869\n发布日期: \n\n之前在《智能家居之热水器零冷水技术原理浅析》，我们详细介绍过零冷水的原理，最后指出当时市面上只有名为“爱喜易”的设备实现了文章介绍的理想设计，笔者前两年也一直在用它。然而，笔者的该套装置最近出现了故障，加之无法接入米家，所以也不大想修了，另外“爱...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10869",
    "tags": [
      "生活",
      "智能家居",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 59
  },
  {
    "slug": "变分自编码器七球面上的vaevmf-vae",
    "title": "变分自编码器（七）：球面上的VAE（vMF-VAE）",
    "description": "变分自编码器（七）：球面上的VAE（vMF-VAE）&para;\n原文链接: https://spaces.ac.cn/archives/8404\n发布日期: \n\n在《变分自编码器（五）：VAE + BN = 更好的VAE》中，我们讲到了NLP中训练VAE时常见的KL散度消失现象，并且提到了通过BN来使得KL散度项有一个正的下界，从而保证KL散度项不会消失。事实上，早在2018年的时候，就有类似思...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8404",
    "tags": [
      "变分",
      "无监督",
      "vae",
      "生成模型",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 60
  },
  {
    "slug": "写了个刷论文的辅助网站cool-papers",
    "title": "写了个刷论文的辅助网站：Cool Papers",
    "description": "写了个刷论文的辅助网站：Cool Papers&para;\n原文链接: https://spaces.ac.cn/archives/9907\n发布日期: \n\n写在开头&para;\n一直以来，笔者都有日刷Arxiv的习惯，以求尽可能跟上领域内最新成果，并告诫自己“不进则退”。之前也有不少读者问我是怎么刷Arxiv的、有什么辅助工具等，但事实上，在很长的时间里，笔者都是直接刷Arxiv官网，并且没有用...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/9907",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 61
  },
  {
    "slug": "从动力学角度看优化算法五为什么学习率不宜过小",
    "title": "从动力学角度看优化算法（五）：为什么学习率不宜过小？",
    "description": "从动力学角度看优化算法（五）：为什么学习率不宜过小？&para;\n原文链接: https://spaces.ac.cn/archives/7787\n发布日期: \n\n本文的主题是“为什么我们需要有限的学习率”，所谓“有限”，指的是不大也不小，适中即可，太大容易导致算法发散，这不难理解，但为什么太小也不好呢？一个容易理解的答案是，学习率过小需要迭代的步数过多，这是一种没有必要的浪费，因此从“节能”和“...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7787",
    "tags": [
      "摄动",
      "微分方程",
      "动力学",
      "梯度",
      "优化器"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 62
  },
  {
    "slug": "mitchell近似乘法变为加法误差不超过19",
    "title": "Mitchell近似：乘法变为加法，误差不超过1/9",
    "description": "Mitchell近似：乘法变为加法，误差不超过1/9&para;\n原文链接: https://spaces.ac.cn/archives/7991\n发布日期: \n\n今天给大家介绍一篇1962年的论文《Computer Multiplication and Division Using Binary Logarithms》，作者是John N. Mitchell，他在里边提出了一个相当有意思的算法：...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7991",
    "tags": [
      "模型",
      "算法",
      "优化",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 63
  },
  {
    "slug": "又是dropout两次这次它做到了有监督任务的sota",
    "title": "又是Dropout两次！这次它做到了有监督任务的SOTA",
    "description": "又是Dropout两次！这次它做到了有监督任务的SOTA&para;\n原文链接: https://spaces.ac.cn/archives/8496\n发布日期: \n\n关注NLP新进展的读者，想必对四月份发布的SimCSE印象颇深，它通过简单的“Dropout两次”来构造正样本进行对比学习，达到了无监督语义相似度任务的全面SOTA。无独有偶，最近的论文《R-Drop: Regularized Dr...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8496",
    "tags": [
      "优化",
      "损失函数",
      "对抗训练",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 64
  },
  {
    "slug": "层次分解位置编码让bert可以处理超长文本",
    "title": "层次分解位置编码，让BERT可以处理超长文本",
    "description": "层次分解位置编码，让BERT可以处理超长文本&para;\n原文链接: https://spaces.ac.cn/archives/7947\n发布日期: \n\n大家都知道，目前的主流的BERT模型最多能处理512个token的文本。导致这一瓶颈的根本原因是BERT使用了从随机初始化训练出来的绝对位置编码，一般的最大位置设为了512，因此顶多只能处理512个token，多出来的部分就没有位置编码可用了。...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7947",
    "tags": [
      "模型",
      "优化",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 65
  },
  {
    "slug": "从一个单位向量变换到另一个单位向量的正交矩阵",
    "title": "从一个单位向量变换到另一个单位向量的正交矩阵",
    "description": "从一个单位向量变换到另一个单位向量的正交矩阵&para;\n原文链接: https://spaces.ac.cn/archives/8453\n发布日期: \n\n这篇文章我们来讨论一个比较实用的线性代数问题：\n\n给定两个$d$维单位（列）向量$\\boldsymbol{a},\\boldsymbol{b}$，求一个正交矩阵$\\boldsymbol{T}$，使得$\\boldsymbol{b}=\\boldsym...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8453",
    "tags": [
      "变换",
      "向量",
      "矩阵",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 66
  },
  {
    "slug": "必须要gpt3吗不bert的mlm模型也能小样本学习",
    "title": "必须要GPT3吗？不，BERT的MLM模型也能小样本学习",
    "description": "必须要GPT3吗？不，BERT的MLM模型也能小样本学习&para;\n原文链接: https://spaces.ac.cn/archives/7764\n发布日期: \n\n大家都知道现在GPT3风头正盛，然而，到处都是GPT3、GPT3地推，读者是否记得GPT3论文的名字呢？事实上，GPT3的论文叫做《Language Models are Few-Shot Learners》，标题里边已经没有G、P...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7764",
    "tags": [
      "无监督",
      "语言模型",
      "NLP",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 67
  },
  {
    "slug": "非自回归也不差基于mlm的阅读理解问答",
    "title": "“非自回归”也不差：基于MLM的阅读理解问答",
    "description": "“非自回归”也不差：基于MLM的阅读理解问答&para;\n原文链接: https://spaces.ac.cn/archives/7148\n发布日期: \n\n前段时间写了《万能的seq2seq：基于seq2seq的阅读理解问答》，探索了以最通用的seq2seq的方式来做阅读理解式问答，并且取得相当不错的成绩（单模型0.77，超过参加比赛时精调的最佳模型）。这篇文章我们继续做这个任务，不过换一个思路，...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7148",
    "tags": [
      "问答",
      "语言模型",
      "生成模型",
      "文本生成",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 68
  },
  {
    "slug": "teaforn让teacher-forcing更有远见一些",
    "title": "TeaForN：让Teacher Forcing更有“远见”一些",
    "description": "TeaForN：让Teacher Forcing更有“远见”一些&para;\n原文链接: https://spaces.ac.cn/archives/7818\n发布日期: \n\nTeacher Forcing是Seq2Seq模型的经典训练方式，而Exposure Bias则是Teacher Forcing的经典缺陷，这对于搞文本生成的同学来说应该是耳熟能详的事实了。笔者之前也曾写过博文《Seq2Se...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7818",
    "tags": [
      "优化",
      "语言模型",
      "文本生成",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 69
  },
  {
    "slug": "万能的seq2seq基于seq2seq的阅读理解问答",
    "title": "万能的seq2seq：基于seq2seq的阅读理解问答",
    "description": "万能的seq2seq：基于seq2seq的阅读理解问答&para;\n原文链接: https://spaces.ac.cn/archives/7115\n发布日期: \n\n今天给bert4keras新增加了一个例子：阅读理解式问答（task_reading_comprehension_by_seq2seq.py），语料跟之前一样，都是用WebQA和SogouQA，最终的得分在0.77左右（单模型，没精调...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7115",
    "tags": [
      "问答",
      "语言模型",
      "文本生成",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 70
  },
  {
    "slug": "苏剑林-x与单位阵的平均平方误差mse作为它跟单位阵的差距有什",
    "title": "从几何视角来理解模型参数的初始化策略",
    "description": "从几何视角来理解模型参数的初始化策略&para;\n原文链接: https://spaces.ac.cn/archives/7180\n发布日期: \n\n对于复杂模型来说，参数的初始化显得尤为重要。糟糕的初始化，很多时候已经不单是模型效果变差的问题了，还更有可能是模型根本训练不动或者不收敛。在深度学习中常见的自适应初始化策略是Xavier初始化，它是从正态分布$\\mathcal{N}\\left(0,\\f...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7180",
    "tags": [
      "模型",
      "概率",
      "几何",
      "优化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 71
  },
  {
    "slug": "苏剑林-是梯度均值为零的假设这个问题不是在数值模拟一节讨论过了吗",
    "title": "为什么Adam的Update RMS是0.2？",
    "description": "为什么Adam的Update RMS是0.2？&para;\n原文链接: https://spaces.ac.cn/archives/11267\n发布日期: \n\n众所周知，我们很早就开始尝试将Muon用于大规模LLM的训练。特别地，在《Muon续集：为什么我们选择尝试Muon？》中，我们提出了“Match Adam Update RMS”的技巧，以便快速从Adam迁移到Muon上，这个技巧同样用到了...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11267",
    "tags": [
      "分析",
      "梯度",
      "优化器",
      "平均场",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 72
  },
  {
    "slug": "搜出来的文本二从mcmc到模拟退火",
    "title": "【搜出来的文本】⋅（二）从MCMC到模拟退火",
    "description": "【搜出来的文本】⋅（二）从MCMC到模拟退火&para;\n原文链接: https://spaces.ac.cn/archives/8084\n发布日期: \n\n在上一篇文章中，我们介绍了“受限文本生成”这个概念，指出可以通过量化目标并从中采样的方式来无监督地完成某些带条件的文本生成任务。同时，上一篇文章还介绍了“重要性采样”和“拒绝采样”两个方法，并且指出对于高维空间而言，它们所依赖的易于采样的分布往...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8084",
    "tags": [
      "文本生成",
      "采样",
      "离散优化",
      "MCMC",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 73
  },
  {
    "slug": "realformer把残差转移到attention矩阵上面去",
    "title": "RealFormer：把残差转移到Attention矩阵上面去",
    "description": "RealFormer：把残差转移到Attention矩阵上面去&para;\n原文链接: https://spaces.ac.cn/archives/8027\n发布日期: \n\n大家知道Layer Normalization是Transformer模型的重要组成之一，它的用法有PostLN和PreLN两种，论文《On Layer Normalization in the Transformer Arc...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8027",
    "tags": [
      "梯度",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 74
  },
  {
    "slug": "再谈类别不平衡问题调节权重与魔改loss的对比联系",
    "title": "再谈类别不平衡问题：调节权重与魔改Loss的对比联系",
    "description": "再谈类别不平衡问题：调节权重与魔改Loss的对比联系&para;\n原文链接: https://spaces.ac.cn/archives/7708\n发布日期: \n\n类别不平衡问题，也称为长尾分布问题，在本博客里已经有好几次相关讨论了，比如《从loss的硬截断、软化到focal loss》、《将“Softmax+交叉熵”推广到多标签分类问题》、《通过互信息思想来缓解类别不平衡问题》。对于缓解类别不平...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7708",
    "tags": [
      "优化",
      "损失函数",
      "光滑",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 75
  },
  {
    "slug": "搜出来的文本一从文本生成到搜索采样",
    "title": "【搜出来的文本】⋅（一）从文本生成到搜索采样",
    "description": "【搜出来的文本】⋅（一）从文本生成到搜索采样&para;\n原文链接: https://spaces.ac.cn/archives/8062\n发布日期: \n\n最近，笔者入了一个新坑：基于离散优化的思想做一些文本生成任务。简单来说，就是把我们要生成文本的目标量化地写下来，构建一个分布，然后搜索这个分布的最大值点或者从这个分布中进行采样，这个过程通常 不需要标签数据 的训练。由于语言是离散的，因此梯度下...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8062",
    "tags": [
      "文本生成",
      "采样",
      "离散优化",
      "MCMC",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 76
  },
  {
    "slug": "生成扩散模型漫谈三十一预测数",
    "title": "生成扩散模型漫谈（三十一）：预测数据而非噪声",
    "description": "生成扩散模型漫谈（三十一）：预测数据而非噪声&para;\n原文链接: https://spaces.ac.cn/archives/11428\n发布日期: \n\n时至今日，LDM（Latent Diffusion Models）依旧是扩散模型的主流范式。借助Encoder对原始图像进行高倍压缩，LDM能显著减少训练与推理的计算成本，同时还能降低训难度，可谓一举多得。然而，高倍压缩也意味着信息损失，而且...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11428",
    "tags": [
      "损失函数",
      "生成模型",
      "扩散",
      "流形",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 77
  },
  {
    "slug": "线性transformer应该不是你要等的那个模型",
    "title": "线性Transformer应该不是你要等的那个模型",
    "description": "线性Transformer应该不是你要等的那个模型&para;\n原文链接: https://spaces.ac.cn/archives/8610\n发布日期: \n\n在本博客中，我们已经多次讨论过线性Attention的相关内容。介绍线性Attention的逻辑大体上都是：标准Attention具有$\\mathcal{O}(n^2)$的平方复杂度，是其主要的“硬伤”之一，于是我们$\\mathcal{O...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8610",
    "tags": [
      "模型",
      "矩阵",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 78
  },
  {
    "slug": "抛开约束增强模型一行代码提升albert表现",
    "title": "抛开约束，增强模型：一行代码提升albert表现",
    "description": "抛开约束，增强模型：一行代码提升albert表现&para;\n原文链接: https://spaces.ac.cn/archives/7187\n发布日期: \n\n本文标题看起来有点“标题党”了，不过所作改动放到bert4keras框架下，确实是一行代码的变动，至于是否有提升，这个笔者不敢打包票，不过测了几个算是比较有代表性的任务，均显示持平甚至有提升，所以标题说的也基本是事实。\n那究竟是什么改动呢？...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7187",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 79
  },
  {
    "slug": "也来谈谈rnn的梯度消失爆炸问题",
    "title": "也来谈谈RNN的梯度消失/爆炸问题",
    "description": "也来谈谈RNN的梯度消失/爆炸问题&para;\n原文链接: https://spaces.ac.cn/archives/7888\n发布日期: \n\n尽管Transformer类的模型已经攻占了NLP的多数领域，但诸如LSTM、GRU之类的RNN模型依然在某些场景下有它的独特价值，所以RNN依然是值得我们好好学习的模型。而对于RNN梯度的相关分析，则是一个从优化角度思考分析模型的优秀例子，值得大家仔细...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7888",
    "tags": [
      "模型",
      "优化",
      "深度学习",
      "梯度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 80
  },
  {
    "slug": "有限内存下全局打乱几百g文件python",
    "title": "有限内存下全局打乱几百G文件（Python）",
    "description": "有限内存下全局打乱几百G文件（Python）&para;\n原文链接: https://spaces.ac.cn/archives/8662\n发布日期: \n\n这篇文章我们来做一道编程题：\n\n如何在有限内存下全局随机打乱（Shuffle）几百G的文本文件？\n\n题目背景其实很明朗，现在预训练模型动辄就几十甚至几百G语料了，为了让模型能更好地进行预训练，对训练语料进行一次全局的随机打乱是很有必要的。但对于...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8662",
    "tags": [
      "编程",
      "python",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 81
  },
  {
    "slug": "l2正则没有想象那么好可能是权重尺度偏移惹的祸",
    "title": "L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸",
    "description": "L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸&para;\n原文链接: https://spaces.ac.cn/archives/7681\n发布日期: \n\nL2正则是机器学习常用的一种防止过拟合的方法（应该也是一道经常遇到的面试题）。简单来说，它就是希望权重的模长尽可能小一点，从而能抵御的扰动多一点，最终提高模型的泛化性能。但是读者可能也会发现，L2正则的表现通常没有理论上说的那么好，很多...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7681",
    "tags": [
      "模型",
      "优化",
      "深度学习",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 82
  },
  {
    "slug": "观测iss",
    "title": "观测ISS",
    "description": "观测ISS&para;\n原文链接: https://spaces.ac.cn/archives/41\n发布日期: \n\n如果你在夜晚看见一颗明亮的星星在移动着，也许你会猜测那是一架飞机，不过，现在请你再加上一个假设：它或许就是“国际空间站（ISS）”！\n\n上一次已经发表过关于国际空间站的观测了，文章是http://kexue.fm/archives/14/。不过上一次注重的是文章的翻译，这一次才是介...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/41",
    "tags": [
      "国际空间站",
      "ISS",
      "观测",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 83
  },
  {
    "slug": "transformer升级之路3从performer到线性attention",
    "title": "Transformer升级之路：3、从Performer到线性Attention",
    "description": "Transformer升级之路：3、从Performer到线性Attention&para;\n原文链接: https://spaces.ac.cn/archives/8338\n发布日期: \n\n看过笔者之前的文章《线性Attention的探索：Attention必须有个Softmax吗？》和《Performer：用随机投影将Attention的复杂度线性化》的读者，可能会觉得本文的标题有点不自然，因...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8338",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 84
  },
  {
    "slug": "让人惊叹的johnson-lindenstrauss引理应用篇",
    "title": "让人惊叹的Johnson-Lindenstrauss引理：应用篇",
    "description": "让人惊叹的Johnson-Lindenstrauss引理：应用篇&para;\n原文链接: https://spaces.ac.cn/archives/8706\n发布日期: \n\n上一篇文章《让人惊叹的Johnson-Lindenstrauss引理：理论篇》中，我们比较详细地介绍了Johnson-Lindenstrauss引理（JL引理）的理论推导，这一篇我们来关注它的应用。\n作为一个内容上本身就跟降...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8706",
    "tags": [
      "模型",
      "分析",
      "维度",
      "机器学习",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 85
  },
  {
    "slug": "univae基于transformer的单模型多尺度的vae模型",
    "title": "UniVAE：基于Transformer的单模型、多尺度的VAE模型",
    "description": "UniVAE：基于Transformer的单模型、多尺度的VAE模型&para;\n原文链接: https://spaces.ac.cn/archives/8475\n发布日期: \n\n大家都知道，Transformer的$\\mathcal{O}(n^2)$复杂度是它的“硬伤”之一。不过凡事有弊亦有利，$\\mathcal{O}(n^2)$的复杂度也为Transformer带来很大的折腾空间，我们可以灵活...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8475",
    "tags": [
      "变分",
      "无监督",
      "vae",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 86
  },
  {
    "slug": "transformer升级之路4二维位置的旋转式位置编码",
    "title": "Transformer升级之路：4、二维位置的旋转式位置编码",
    "description": "Transformer升级之路：4、二维位置的旋转式位置编码&para;\n原文链接: https://spaces.ac.cn/archives/8397\n发布日期: \n\n在之前的文章《Transformer升级之路：2、博采众长的旋转式位置编码》中我们提出了旋转式位置编码RoPE以及对应的Transformer模型RoFormer。由于笔者主要研究的领域还是NLP，所以本来这个事情对于笔者来说已...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8397",
    "tags": [
      "复数",
      "矩阵",
      "attention",
      "位置编码",
      "rope"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 87
  },
  {
    "slug": "用bert4keras做三元组抽取",
    "title": "用bert4keras做三元组抽取",
    "description": "用bert4keras做三元组抽取&para;\n原文链接: https://spaces.ac.cn/archives/7161\n发布日期: \n\n在开发bert4keras的时候就承诺过，会逐渐将之前用keras-bert实现的例子逐渐迁移到bert4keras来，而那里其中一个例子便是三元组抽取的任务。现在bert4keras的例子已经颇为丰富了，但还没有序列标注和信息抽取相关的任务，而三元组抽...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7161",
    "tags": [
      "语言模型",
      "信息抽取",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 88
  },
  {
    "slug": "让人惊叹的johnson-lindenstrauss引理理论篇",
    "title": "让人惊叹的Johnson-Lindenstrauss引理：理论篇",
    "description": "让人惊叹的Johnson-Lindenstrauss引理：理论篇&para;\n原文链接: https://spaces.ac.cn/archives/8679\n发布日期: \n\n今天我们来学习Johnson-Lindenstrauss引理，由于名字比较长，下面都简称“JL引理”。\n个人认为，JL引理是每一个计算机科学的同学都必须了解的神奇结论之一，它是一个关于降维的著名的结果，它也是高维空间中众多反...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8679",
    "tags": [
      "模型",
      "分析",
      "维度",
      "机器学习",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 89
  },
  {
    "slug": "printemps-苏神怎么看log-linear-attention这种变长的h",
    "title": "为什么线性注意力要加Short Conv？",
    "description": "为什么线性注意力要加Short Conv？&para;\n原文链接: https://spaces.ac.cn/archives/11320\n发布日期: \n\n如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考《线性注意力简史：从模仿、创新到反哺》）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short C...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11320",
    "tags": [
      "线性",
      "RNN",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 90
  },
  {
    "slug": "智能家居之小爱同学控制极米投影仪的简单方案",
    "title": "智能家居之小爱同学控制极米投影仪的简单方案",
    "description": "智能家居之小爱同学控制极米投影仪的简单方案&para;\n原文链接: https://spaces.ac.cn/archives/9365\n发布日期: \n\n前段时间买了一个极米投影仪，开始折腾才发现极米跟小米基本没啥关系，它根本无法跟小爱同学互动。在众多名字带“米”的品牌中，极米是为数不多的无法接入米家生态的品牌，想必有不少用户开始都会被极米这个名字误导，关键是极米投影仪还在小米商城上有得卖（捂脸）...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/9365",
    "tags": [
      "生活",
      "智能家居",
      "米家",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 91
  },
  {
    "slug": "eae自编码器-bn-最大熵-生成模型",
    "title": "EAE：自编码器 + BN + 最大熵  = 生成模型",
    "description": "EAE：自编码器 + BN + 最大熵  = 生成模型&para;\n原文链接: https://spaces.ac.cn/archives/7343\n发布日期: \n\n生成模型一直是笔者比较关注的主题，不管是NLP和CV的生成模型都是如此。这篇文章里，我们介绍一个新颖的生成模型，来自论文《Batch norm with entropic regularization turns determinis...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7343",
    "tags": [
      "最大熵",
      "熵",
      "无监督",
      "生成模型",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 92
  },
  {
    "slug": "designing-gans又一个gan生产车间",
    "title": "Designing GANs：又一个GAN生产车间",
    "description": "Designing GANs：又一个GAN生产车间&para;\n原文链接: https://spaces.ac.cn/archives/7210\n发布日期: \n\n在2018年的文章里《f-GAN简介：GAN模型的生产车间》笔者介绍了f-GAN，并评价其为GAN模型的“生产车间”，顾名思义，这是指它能按照固定的流程构造出很多不同形式的GAN模型来。前几天在arxiv上看到了新出的一篇论文《Desig...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7210",
    "tags": [
      "微积分",
      "GAN",
      "生成模型",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 93
  },
  {
    "slug": "学会提问的bert端到端地从篇章中构建问答对",
    "title": "学会提问的BERT：端到端地从篇章中构建问答对",
    "description": "学会提问的BERT：端到端地从篇章中构建问答对&para;\n原文链接: https://spaces.ac.cn/archives/7630\n发布日期: \n\n机器阅读理解任务，相比不少读者都有所了解了，简单来说就是从给定篇章中寻找给定问题的答案，即“篇章 + 问题 → 答案”这样的流程，笔者之前也写过一些关于阅读理解的文章，比如《基于CNN的阅读理解式问答模型：DGCNN》等。至于问答对构建，则相...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7630",
    "tags": [
      "语言模型",
      "文本生成",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 94
  },
  {
    "slug": "生活杂记用电饭锅来煮米汤",
    "title": "【生活杂记】用电饭锅来煮米汤",
    "description": "【生活杂记】用电饭锅来煮米汤&para;\n原文链接: https://spaces.ac.cn/archives/10240\n发布日期: \n\n前段时间，笔者无意看到了一个“低糖电饭锅”的概念（也叫“低淀粉电饭锅”），开始以为是什么新科技产物，再仔细一看之后才发现，原来就是煮饭的同时沥出一点米汤，米汤中包含了一点淀粉，如果把米汤倒掉，那么就等于少吃了一点淀粉，即所谓的低糖/低淀粉。虽然这种产品看起来...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10240",
    "tags": [
      "生活",
      "情感",
      "怀念",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 95
  },
  {
    "slug": "将softmax交叉熵推广到多标签分类问题",
    "title": "将“Softmax+交叉熵”推广到多标签分类问题",
    "description": "将“Softmax+交叉熵”推广到多标签分类问题&para;\n原文链接: https://spaces.ac.cn/archives/7359\n发布日期: \n\n（注：本文的相关内容已整理成论文《ZLPR: A Novel Loss for Multi-label Classification》，如需引用可以直接引用英文论文，谢谢。）\n一般来说，在处理常规的多分类问题时，我们会在模型的最后用一个全连...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7359",
    "tags": [
      "优化",
      "损失函数",
      "光滑",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 96
  },
  {
    "slug": "泛化性乱弹从随机噪声梯度惩罚到虚拟对抗训练",
    "title": "泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练",
    "description": "泛化性乱弹：从随机噪声、梯度惩罚到虚拟对抗训练&para;\n原文链接: https://spaces.ac.cn/archives/7466\n发布日期: \n\n提高模型的泛化性能是机器学习致力追求的目标之一。常见的提高泛化性的方法主要有两种：第一种是添加噪声，比如往输入添加高斯噪声、中间层增加Dropout以及进来比较热门的对抗训练等，对图像进行随机平移缩放等数据扩增手段某种意义上也属于此列；第二种...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7466",
    "tags": [
      "概率",
      "GAN",
      "对抗训练",
      "泛化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 97
  },
  {
    "slug": "短文本匹配baseline脱敏数据使用预训练模型的尝试",
    "title": "短文本匹配Baseline：脱敏数据使用预训练模型的尝试",
    "description": "短文本匹配Baseline：脱敏数据使用预训练模型的尝试&para;\n原文链接: https://spaces.ac.cn/archives/8213\n发布日期: \n\n最近凑着热闹玩了玩全球人工智能技术创新大赛中的“小布助手对话短文本语义匹配”赛道，其任务就是常规的短文本句子对二分类任务，这任务在如今各种预训练Transformer“横行”的时代已经没啥什么特别的难度了，但有意思的是，这次比赛脱敏...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8213",
    "tags": [
      "语言模型",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 98
  },
  {
    "slug": "我们可以无损放大一个transformer模型吗一",
    "title": "我们可以无损放大一个Transformer模型吗（一）",
    "description": "我们可以无损放大一个Transformer模型吗（一）&para;\n原文链接: https://spaces.ac.cn/archives/8444\n发布日期: \n\n看了标题，可能读者会有疑惑，大家不都想着将大模型缩小吗？怎么你想着将小模型放大了？其实背景是这样的：通常来说更大的模型加更多的数据确实能起得更好的效果，然而算力有限的情况下，从零预训练一个大的模型时间成本太大了，如果还要调试几次参数，...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8444",
    "tags": [
      "模型",
      "优化",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 99
  },
  {
    "slug": "transformer升级之路2博采众长的旋转式位置编码",
    "title": "Transformer升级之路：2、博采众长的旋转式位置编码",
    "description": "Transformer升级之路：2、博采众长的旋转式位置编码&para;\n原文链接: https://spaces.ac.cn/archives/8265\n发布日期: \n\n上一篇文章中，我们对原始的Sinusoidal位置编码做了较为详细的推导和理解，总的感觉是Sinusoidal位置编码是一种“想要成为相对位置编码的绝对位置编码”。一般来说，绝对位置编码具有实现简单、计算速度快等优点，而相对位置...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8265",
    "tags": [
      "复数",
      "语言模型",
      "attention",
      "位置编码",
      "rope"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 100
  },
  {
    "slug": "globalpointer用统一的方式处理嵌套和非嵌套ner",
    "title": "GlobalPointer：用统一的方式处理嵌套和非嵌套NER",
    "description": "GlobalPointer：用统一的方式处理嵌套和非嵌套NER&para;\n原文链接: https://spaces.ac.cn/archives/8373\n发布日期: \n\n（注：本文的相关内容已整理成论文《Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition》，如需引用可以直接引用英文论文...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8373",
    "tags": [
      "模型",
      "NLP",
      "NER",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 101
  },
  {
    "slug": "expx在x0处的偶次泰勒展开式总是正的",
    "title": "exp(x)在x=0处的偶次泰勒展开式总是正的",
    "description": "exp(x)在x=0处的偶次泰勒展开式总是正的&para;\n原文链接: https://spaces.ac.cn/archives/7919\n发布日期: \n\n刚看到一个有意思的结论：\n\n对于任意实数$x$及偶数$n$，总有$\\sum\\limits_{k=0}^n \\frac{x^k}{k!} &gt; 0$，即$e^x$在$x=0$处的偶次泰勒展开式总是正的。\n\n下面我们来看一下这个结论的证明，以...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7919",
    "tags": [
      "导数",
      "概率",
      "分析",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 102
  },
  {
    "slug": "cool-papers更新简单搭建了一个站内检索系统",
    "title": "Cool Papers更新：简单搭建了一个站内检索系统",
    "description": "Cool Papers更新：简单搭建了一个站内检索系统&para;\n原文链接: https://spaces.ac.cn/archives/10088\n发布日期: \n\n自从《更便捷的Cool Papers打开方式：Chrome重定向扩展》之后，Cool Papers有两次比较大的变化，一次是引入了venue分支，逐步收录了一些会议历年的论文集，如ICLR、ICML等，这部分是动态人工扩充的，欢迎有...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10088",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 103
  },
  {
    "slug": "搜出来的文本四通过增删改来用词造句",
    "title": "【搜出来的文本】⋅（四）通过增、删、改来用词造句",
    "description": "【搜出来的文本】⋅（四）通过增、删、改来用词造句&para;\n原文链接: https://spaces.ac.cn/archives/8194\n发布日期: \n\n“用词造句”是小学阶段帮助我们理解和运用词语的一个经典任务，从自然语言处理的角度来看，它是一个句子扩写或者句子补全任务，它其实要求我们具有不定向地进行文本生成的能力。然而，当前主流的语言模型都是单方向生成的（多数是正向的，即从左往右，少数是...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8194",
    "tags": [
      "文本生成",
      "采样",
      "离散优化",
      "MCMC",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 104
  },
  {
    "slug": "spaces抽取-生成式长文本摘要法研杯总结",
    "title": "SPACES：“抽取-生成”式长文本摘要（法研杯总结）",
    "description": "SPACES：“抽取-生成”式长文本摘要（法研杯总结）&para;\n原文链接: https://spaces.ac.cn/archives/8046\n发布日期: \n\n“法研杯”算是近年来比较知名的NLP赛事之一，今年是第三届，包含四个赛道，其中有一个“司法摘要”赛道引起了我们的兴趣。经过了解，这是面向法律领域裁判文书的长文本摘要生成，这应该是国内第一个公开的长文本生成任务和数据集。过去一年多以来，...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8046",
    "tags": [
      "文本生成",
      "稀疏",
      "文本摘要",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 105
  },
  {
    "slug": "旁门左道之如何让python的重试代码更加优雅",
    "title": "旁门左道之如何让Python的重试代码更加优雅",
    "description": "旁门左道之如何让Python的重试代码更加优雅&para;\n原文链接: https://spaces.ac.cn/archives/9938\n发布日期: \n\n这篇文章我们讨论一个编程题：如何更优雅地在Python中实现重试。\n在文章《新年快乐！记录一下 Cool Papers 的开发体验》中，笔者分享了开发Cool Papers的一些经验，其中就提到了Cool Papers所需要的一些网络通信步骤...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/9938",
    "tags": [
      "编程",
      "代码",
      "python",
      "优化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 106
  },
  {
    "slug": "苏剑林-相对位置编码似乎没有太多选择了要不rope这种算是乘性了",
    "title": "让研究人员绞尽脑汁的Transformer位置编码",
    "description": "让研究人员绞尽脑汁的Transformer位置编码&para;\n原文链接: https://spaces.ac.cn/archives/8130\n发布日期: \n\n不同于RNN、CNN等模型，对于Transformer模型来说，位置编码的加入是必不可少的，因为纯粹的Attention模块是无法捕捉输入顺序的，即无法区分不同位置的Token。为此我们大体有两个选择：1、想办法将位置信息融入到输入中，这...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8130",
    "tags": [
      "复数",
      "attention",
      "位置编码",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 107
  },
  {
    "slug": "隐藏在动量中的梯度累积少更新几步效果反而更好",
    "title": "隐藏在动量中的梯度累积：少更新几步，效果反而更好？",
    "description": "隐藏在动量中的梯度累积：少更新几步，效果反而更好？&para;\n原文链接: https://spaces.ac.cn/archives/8634\n发布日期: \n\n我们知道，梯度累积是在有限显存下实现大batch_size训练的常用技巧。在之前的文章《用时间换取效果：Keras梯度累积优化器》中，我们就简单介绍过梯度累积的实现，大致的思路是新增一组参数来缓存梯度，最后用缓存的梯度来更新模型。美中不足...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8634",
    "tags": [
      "模型",
      "分析",
      "梯度",
      "优化器",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 108
  },
  {
    "slug": "从动力学角度看优化算法七sgd-svm",
    "title": "从动力学角度看优化算法（七）：SGD ≈ SVM？",
    "description": "从动力学角度看优化算法（七）：SGD ≈ SVM？&para;\n原文链接: https://spaces.ac.cn/archives/8009\n发布日期: \n\n众所周知，在深度学习之前，机器学习是SVM（Support Vector Machine，支持向量机）的天下，曾经的它可谓红遍机器学习的大江南北，迷倒万千研究人员，直至今日，“手撕SVM”仍然是大厂流行的面试题之一。然而，时过境迁，当深度...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8009",
    "tags": [
      "微分方程",
      "动力学",
      "优化",
      "核方法",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 109
  },
  {
    "slug": "performer用随机投影将attention的复杂度线性化",
    "title": "Performer：用随机投影将Attention的复杂度线性化",
    "description": "Performer：用随机投影将Attention的复杂度线性化&para;\n原文链接: https://spaces.ac.cn/archives/7921\n发布日期: \n\nAttention机制的$\\mathcal{O}(n^2)$复杂度是一个老大难问题了，改变这一复杂度的思路主要有两种：一是走稀疏化的思路，比如我们以往介绍过的Sparse Attention以及Google前几个月搞出来的B...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7921",
    "tags": [
      "优化",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 110
  },
  {
    "slug": "线性attention的探索attention必须有个softmax吗",
    "title": "线性Attention的探索：Attention必须有个Softmax吗？",
    "description": "线性Attention的探索：Attention必须有个Softmax吗？&para;\n原文链接: https://spaces.ac.cn/archives/7546\n发布日期: \n\n众所周知，尽管基于Attention机制的Transformer类模型有着良好的并行性能，但它的空间和时间复杂度都是$\\mathcal{O}(n^2)$级别的，$n$是序列长度，所以当$n$比较大时Transfor...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7546",
    "tags": [
      "模型",
      "文本生成",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 111
  },
  {
    "slug": "从采样看优化可导优化与不可导优化的统一视角",
    "title": "从采样看优化：可导优化与不可导优化的统一视角",
    "description": "从采样看优化：可导优化与不可导优化的统一视角&para;\n原文链接: https://spaces.ac.cn/archives/7521\n发布日期: \n\n不少读者都应该知道，损失函数与评测指标的不一致性是机器学习的典型现象之一，比如分类问题中损失函数用交叉熵，评测指标则是准确率或者F1，又比如文本生成中损失函数是teacher-forcing形式的交叉熵，评测指标则是BLEU、ROUGE等。理想...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7521",
    "tags": [
      "优化",
      "梯度",
      "优化器",
      "采样",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 112
  },
  {
    "slug": "让mathjax的数学公式随窗口大小自动缩放",
    "title": "让MathJax的数学公式随窗口大小自动缩放",
    "description": "让MathJax的数学公式随窗口大小自动缩放&para;\n原文链接: https://spaces.ac.cn/archives/10474\n发布日期: \n\n随着MathJax的出现和流行，在网页上显示数学公式便逐渐有了标准答案。然而，MathJax（包括其竞品KaTeX）只是负责将网页LaTeX代码转化为数学公式，对于自适应分辨率方面依然没有太好的办法。像本站一些数学文章，因为是在PC端排版好的...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10474",
    "tags": [
      "网站",
      "latex",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 113
  },
  {
    "slug": "第1000篇文章",
    "title": "第1000篇文章",
    "description": "第1000篇文章&para;\n原文链接: https://spaces.ac.cn/archives/7782\n发布日期: \n\n\n后台提示，本文是科学空间的第1000篇文章。\n\n本想写下一篇文章的，但是看到这个提示，就先瞎写个水文纪念一下。都说人老了就喜欢各种感叹，这话还真不假。看到别人高考来个感想，博客十周年了来个感想，现在第1000篇文章了也来个感想，似乎总想找点理由感叹一下一样。那今天又能扯...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7782",
    "tags": [
      "生活",
      "节日",
      "情感",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 114
  },
  {
    "slug": "积分梯度一种新颖的神经网络可视化方法",
    "title": "积分梯度：一种新颖的神经网络可视化方法",
    "description": "积分梯度：一种新颖的神经网络可视化方法&para;\n原文链接: https://spaces.ac.cn/archives/7533\n发布日期: \n\n本文介绍一种神经网络的可视化方法：积分梯度（Integrated Gradients），它首先在论文《Gradients of Counterfactuals》中提出，后来《Axiomatic Attribution for Deep Network...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7533",
    "tags": [
      "积分",
      "梯度",
      "可视化",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 115
  },
  {
    "slug": "利用熄火保护-通断器实现燃气灶智能关火",
    "title": "利用“熄火保护 + 通断器”实现燃气灶智能关火",
    "description": "利用“熄火保护 + 通断器”实现燃气灶智能关火&para;\n原文链接: https://spaces.ac.cn/archives/10394\n发布日期: \n\n燃气灶智能化主要有两个方向：一是检测开关火状态，实现跟抽油烟机等其他设备的联动；二是实现智能关火，这包括定时关火以及接入米家（或者其他智能家居）实现语音关火、远程关火等。目前带有这两点功能的燃气灶选择并不多，并且相比普通燃气灶贵不少，单纯为...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/10394",
    "tags": [
      "生活",
      "智能家居",
      "米家",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 116
  },
  {
    "slug": "滑动平均视角下的权重衰减和学习率",
    "title": "滑动平均视角下的权重衰减和学习率",
    "description": "滑动平均视角下的权重衰减和学习率&para;\n原文链接: https://spaces.ac.cn/archives/11459\n发布日期: \n\n权重衰减（Weight Decay）和学习率（Learning Rate）是LLM预训练的重要组成部分，它们的设置是否妥当，是模型最终成败的关键因素之一。自AdamW以来，单独分离出Weight Decay来取代传统的L2正则，基本上已经成为了共识，但在...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11459",
    "tags": [
      "最优",
      "梯度",
      "学习率",
      "平均场",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 117
  },
  {
    "slug": "adafactor优化器浅析附开源实现",
    "title": "AdaFactor优化器浅析（附开源实现）",
    "description": "AdaFactor优化器浅析（附开源实现）&para;\n原文链接: https://spaces.ac.cn/archives/7302\n发布日期: \n\n自从GPT、BERT等预训练模型流行起来后，其中一个明显的趋势是模型越做越大，因为更大的模型配合更充分的预训练通常能更有效地刷榜。不过，理想可以无限远，现实通常很局促，有时候模型太大了，大到哪怕你拥有了大显存的GPU甚至TPU，依然会感到很绝望。...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7302",
    "tags": [
      "分析",
      "keras",
      "优化器",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 118
  },
  {
    "slug": "日食记",
    "title": "日食记",
    "description": "日食记&para;\n原文链接: https://spaces.ac.cn/archives/7515\n发布日期: \n\n\n简单成功的日食观测（2020年6月21日 16:02 深圳宝安沙井）\n转载到请包括本文地址：https://spaces.ac.cn/archives/7515\n更详细的转载事宜请参考：《科学空间FAQ》\n如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。\n如果您觉得本文还不错...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7515",
    "tags": [
      "天象",
      "日食",
      "天文",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 119
  },
  {
    "slug": "从三角不等式到margin-softmax",
    "title": "从三角不等式到Margin Softmax",
    "description": "从三角不等式到Margin Softmax&para;\n原文链接: https://spaces.ac.cn/archives/8656\n发布日期: \n\n在《基于GRU和AM-Softmax的句子相似度模型》中我们介绍了AM-Softmax，它是一种带margin的softmax，通常用于用分类做检索的场景。当时通过图示的方式简单说了一下引入margin是因为“分类与排序的不等价性”，但没有比较定...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8656",
    "tags": [
      "损失函数",
      "相似度",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 120
  },
  {
    "slug": "你的crf层的学习率可能不够大",
    "title": "你的CRF层的学习率可能不够大",
    "description": "你的CRF层的学习率可能不够大&para;\n原文链接: https://spaces.ac.cn/archives/7196\n发布日期: \n\nCRF是做序列标注的经典方法，它理论优雅，实际也很有效，如果还不了解CRF的读者欢迎阅读旧作《简明条件随机场CRF介绍（附带纯Keras实现）》。在BERT模型出来之后，也有不少工作探索了BERT+CRF用于序列标注任务的做法。然而，很多实验结果显示（比如论...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7196",
    "tags": [
      "模型",
      "概率图",
      "crf",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 121
  },
  {
    "slug": "无监督语义相似度哪家强我们做了个比较全面的评测",
    "title": "无监督语义相似度哪家强？我们做了个比较全面的评测",
    "description": "无监督语义相似度哪家强？我们做了个比较全面的评测&para;\n原文链接: https://spaces.ac.cn/archives/8321\n发布日期: \n\n一月份的时候，笔者写了《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》，指出无监督语义相似度的SOTA模型BERT-flow其实可以通过一个简单的线性变换（白化操作，BERT-whitening）达到。随后，我们进一步...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8321",
    "tags": [
      "语言模型",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 122
  },
  {
    "slug": "强大的nvae以后再也不能说vae生成的图像模糊了",
    "title": "强大的NVAE：以后再也不能说VAE生成的图像模糊了",
    "description": "强大的NVAE：以后再也不能说VAE生成的图像模糊了&para;\n原文链接: https://spaces.ac.cn/archives/7574\n发布日期: \n\n昨天早上，笔者在日常刷arixv的时候，然后被一篇新出来的论文震惊了！论文名字叫做《NVAE: A Deep Hierarchical Variational Autoencoder》，顾名思义是做VAE的改进工作的，提出了一个叫NVA...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7574",
    "tags": [
      "变分",
      "vae",
      "生成模型",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 123
  },
  {
    "slug": "龟鱼记全陶粒的同程底滤生态缸",
    "title": "【龟鱼记】全陶粒的同程底滤生态缸",
    "description": "【龟鱼记】全陶粒的同程底滤生态缸&para;\n原文链接: https://spaces.ac.cn/archives/7961\n发布日期: \n\n最近一段时间入了水族的坑，整了个60cm×40cm的超白缸来玩，主要是龟鱼共养。个人比较追求自然仿生，所以希望能在缸里建立一个相对稳定的仿生态环境。当然，其实这都是借口，根本原因是懒得换水，也不想洗过滤棉，所以就想着依靠生态系统自身的净化能力来延长换水时间...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7961",
    "tags": [
      "生活",
      "龟鱼",
      "生态",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 124
  },
  {
    "slug": "self-orthogonality-module一个即插即用的核正交化模块",
    "title": "Self-Orthogonality Module：一个即插即用的核正交化模块",
    "description": "Self-Orthogonality Module：一个即插即用的核正交化模块&para;\n原文链接: https://spaces.ac.cn/archives/7169\n发布日期: \n\n前些天刷Arxiv看到新文章《Self-Orthogonality Module: A Network Architecture Plug-in for Learning Orthogonal Filters》...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7169",
    "tags": [
      "模型",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 125
  },
  {
    "slug": "为什么梯度裁剪能加速训练过程一个简明的分析",
    "title": "为什么梯度裁剪能加速训练过程？一个简明的分析",
    "description": "为什么梯度裁剪能加速训练过程？一个简明的分析&para;\n原文链接: https://spaces.ac.cn/archives/7469\n发布日期: \n\n本文介绍来自MIT的一篇ICLR 2020满分论文《Why gradient clipping accelerates training: A theoretical justification for adaptivity》，顾名思义，这篇论...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7469",
    "tags": [
      "优化",
      "梯度",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 126
  },
  {
    "slug": "simbertv2来了融合检索和生成的roformer-sim模型",
    "title": "SimBERTv2来了！融合检索和生成的RoFormer-Sim模型",
    "description": "SimBERTv2来了！融合检索和生成的RoFormer-Sim模型&para;\n原文链接: https://spaces.ac.cn/archives/8454\n发布日期: \n\n去年我们放出了SimBERT模型，它算是我们开源的比较成功的模型之一，获得了不少读者的认可。简单来说，SimBERT是一个融生成和检索于一体的模型，可以用来作为句向量的一个比较高的baseline，也可以用来实现相似问句...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8454",
    "tags": [
      "语言模型",
      "生成模型",
      "文本生成",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 127
  },
  {
    "slug": "现在可以用keras玩中文gpt2了gpt2-ml",
    "title": "现在可以用Keras玩中文GPT2了（GPT2_ML）",
    "description": "现在可以用Keras玩中文GPT2了（GPT2_ML）&para;\n原文链接: https://spaces.ac.cn/archives/7292\n发布日期: \n\n前段时间留意到有大牛开源了一个中文的GPT2模型，是最大的15亿参数规模的，看作者给的demo，生成效果还是蛮惊艳的，就想着加载到自己的bert4keras来玩玩。不过早期的bert4keras整体架构写得比较“死”，集成多个不同的模...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7292",
    "tags": [
      "语言模型",
      "NLP",
      "文本生成",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 128
  },
  {
    "slug": "flatnce小批次对比学习效果差的原因竟是浮点误差",
    "title": "FlatNCE：小批次对比学习效果差的原因竟是浮点误差？",
    "description": "FlatNCE：小批次对比学习效果差的原因竟是浮点误差？&para;\n原文链接: https://spaces.ac.cn/archives/8586\n发布日期: \n\n自SimCLR在视觉无监督学习大放异彩以来，对比学习逐渐在CV乃至NLP中流行了起来，相关研究和工作越来越多。标准的对比学习的一个广为人知的缺点是需要比较大的batch_size（SimCLR在batch_size=4096时效果最...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8586",
    "tags": [
      "优化",
      "损失函数",
      "对比学习",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 129
  },
  {
    "slug": "adax优化器浅析附开源实现",
    "title": "AdaX优化器浅析（附开源实现）",
    "description": "AdaX优化器浅析（附开源实现）&para;\n原文链接: https://spaces.ac.cn/archives/7387\n发布日期: \n\n这篇文章简单介绍一个叫做AdaX的优化器，来自《AdaX: Adaptive Gradient Descent with Exponential Long Term Memory》。介绍这个优化器的原因是它再次印证了之前在《AdaFactor优化器浅析（附...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7387",
    "tags": [
      "优化器",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 130
  },
  {
    "slug": "adamw的weight-rms的渐近估计上",
    "title": "AdamW的Weight RMS的渐近估计（上）",
    "description": "AdamW的Weight RMS的渐近估计（上）&para;\n原文链接: https://spaces.ac.cn/archives/11307\n发布日期: \n\n在《为什么Adam的Update RMS是0.2？》中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 @EIFY 指出相同的结果已经出现在论文《Rotational Equilibrium: How Weight D...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/11307",
    "tags": [
      "估计",
      "梯度",
      "优化器",
      "平均场",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 131
  },
  {
    "slug": "概率视角下的线性模型逻辑回归有解析解吗",
    "title": "概率视角下的线性模型：逻辑回归有解析解吗？",
    "description": "概率视角下的线性模型：逻辑回归有解析解吗？&para;\n原文链接: https://spaces.ac.cn/archives/8578\n发布日期: \n\n我们知道，线性回归是比较简单的问题，它存在解析解，而它的变体逻辑回归（Logistic Regression）却没有解析解，这不能不说是一个遗憾。因为逻辑回归虽然也叫“回归”，但它实际上是用于分类问题的，而对于很多读者来说分类比回归更加常见。准确...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8578",
    "tags": [
      "模型",
      "概率",
      "优化",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 132
  },
  {
    "slug": "也来扯几句全国青少年科技创新大赛",
    "title": "也来扯几句“全国青少年科技创新大赛”",
    "description": "也来扯几句“全国青少年科技创新大赛”&para;\n原文链接: https://spaces.ac.cn/archives/7611\n发布日期: \n\n最近，“全国青少年科技创新大赛”火了，原因很简单，因为公开的每一篇获奖作品都几乎是硕士乃至博士水平的，甚至相比很多知名期刊上的文章都不遑多让，但这些作品的作者却只是中学生甚至只是小学生，他们迈过了各种“天堑”般的坎，完成对很多人甚至是对很多专业硕士博士...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7611",
    "tags": [
      "情感",
      "时事",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 133
  },
  {
    "slug": "关于whiteningbert原创性的疑问和沟通",
    "title": "关于WhiteningBERT原创性的疑问和沟通",
    "description": "关于WhiteningBERT原创性的疑问和沟通&para;\n原文链接: https://spaces.ac.cn/archives/8715\n发布日期: \n\n在文章《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》中，笔者受到BERT-flow的启发，提出了一种名为BERT-whitening的替代方案，它比BERT-flow更简单，但多数数据集下能取得相近甚至更好的效果，此...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8715",
    "tags": [
      "情感",
      "模型",
      "工作",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 134
  },
  {
    "slug": "也来盘点一些最近的非transformer工作",
    "title": "也来盘点一些最近的非Transformer工作",
    "description": "也来盘点一些最近的非Transformer工作&para;\n原文链接: https://spaces.ac.cn/archives/8431\n发布日期: \n\n大家最近应该多多少少都被各种MLP相关的工作“席卷眼球”了。以Google为主的多个研究机构“奇招频出”，试图从多个维度“打击”Transformer模型，其中势头最猛的就是号称是纯MLP的一系列模型了，让人似乎有种“MLP is all y...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8431",
    "tags": [
      "模型",
      "优化",
      "语言模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 135
  },
  {
    "slug": "变分自编码器五vae-bn-更好的vae",
    "title": "变分自编码器（五）：VAE + BN = 更好的VAE",
    "description": "变分自编码器（五）：VAE + BN = 更好的VAE&para;\n原文链接: https://spaces.ac.cn/archives/7381\n发布日期: \n\n本文我们继续之前的变分自编码器系列，分析一下如何防止NLP中的VAE模型出现“KL散度消失（KL Vanishing）”现象。本文受到参考文献是ACL 2020的论文《A Batch Normalized Inference Netw...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7381",
    "tags": [
      "变分",
      "无监督",
      "vae",
      "生成模型",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 136
  },
  {
    "slug": "苏剑林-从公式12到公式15都在推导和解释你说的这个",
    "title": "生成扩散模型漫谈（五）：一般框架之SDE篇",
    "description": "生成扩散模型漫谈（五）：一般框架之SDE篇&para;\n原文链接: https://spaces.ac.cn/archives/9209\n发布日期: \n\n在写生成扩散模型的第一篇文章时，就有读者在评论区推荐了宋飏博士的论文《Score-Based Generative Modeling through Stochastic Differential Equations》，可以说该论文构建了一个相当...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/9209",
    "tags": [
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 137
  },
  {
    "slug": "当gpt遇上中国象棋写过文章解过题要不再来下盘棋",
    "title": "当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？",
    "description": "当GPT遇上中国象棋：写过文章解过题，要不再来下盘棋？&para;\n原文链接: https://spaces.ac.cn/archives/7877\n发布日期: \n\n\n中国象棋\n不知道读者有没有看过量子位年初的文章《最强写作AI竟然学会象棋和作曲，语言模型跨界操作引热议，在线求战》，里边提到有网友用GPT2模型训练了一个下国际象棋的模型。笔者一直在想，这么有趣的事情怎么可以没有中文版呢？对于国际象...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7877",
    "tags": [
      "中国象棋",
      "语言模型",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 138
  },
  {
    "slug": "无监督分词和句法分析原来bert还可以这样用",
    "title": "无监督分词和句法分析！原来BERT还可以这样用",
    "description": "无监督分词和句法分析！原来BERT还可以这样用&para;\n原文链接: https://spaces.ac.cn/archives/7476\n发布日期: \n\nBERT的一般用法就是加载其预训练权重，再接一小部分新层，然后在下游任务上进行finetune，换句话说一般的用法都是有监督训练的。基于这个流程，我们可以做中文的分词、NER甚至句法分析，这些想必大家就算没做过也会有所听闻。但如果说直接从预训...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7476",
    "tags": [
      "无监督",
      "新词发现",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 139
  },
  {
    "slug": "bert-of-theseus基于模块替换的模型压缩方法",
    "title": "BERT-of-Theseus：基于模块替换的模型压缩方法",
    "description": "BERT-of-Theseus：基于模块替换的模型压缩方法&para;\n原文链接: https://spaces.ac.cn/archives/7575\n发布日期: \n\n最近了解到一种称为“BERT-of-Theseus”的BERT模型压缩方法，来自论文《BERT-of-Theseus: Compressing BERT by Progressive Module Replacing》。这是一种以...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7575",
    "tags": [
      "模型",
      "attention",
      "模型压缩",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 140
  },
  {
    "slug": "如何应对seq2seq中的根本停不下来问题",
    "title": "如何应对Seq2Seq中的“根本停不下来”问题？",
    "description": "如何应对Seq2Seq中的“根本停不下来”问题？&para;\n原文链接: https://spaces.ac.cn/archives/7500\n发布日期: \n\n在Seq2Seq的解码过程中，我们是逐个token地递归生成的，直到出现标记为止，这就是所谓的“自回归”生成模型。然而，研究过Seq2Seq的读者应该都能发现，这种自回归的解码偶尔会出现“根本停不下来”的现象，主要是某个片段反复出现，比如“...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7500",
    "tags": [
      "语言模型",
      "文本生成",
      "解码",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 141
  },
  {
    "slug": "最小熵原理六词向量的维度应该怎么选择",
    "title": "最小熵原理（六）：词向量的维度应该怎么选择？",
    "description": "最小熵原理（六）：词向量的维度应该怎么选择？&para;\n原文链接: https://spaces.ac.cn/archives/7695\n发布日期: \n\n随着NLP的发展，像Word2Vec、Glove这样的词向量模型，正逐渐地被基于Transformer的BERT等模型代替，不过经典始终是经典，词向量模型依然在不少场景发光发热，并且仍有不少值得我们去研究的地方。本文我们来关心一个词向量模型可能...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7695",
    "tags": [
      "最小熵原理",
      "熵",
      "词向量",
      "最小熵",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 142
  },
  {
    "slug": "新年快乐记录一下-cool-papers-的开发体验",
    "title": "新年快乐！记录一下 Cool Papers 的开发体验",
    "description": "新年快乐！记录一下 Cool Papers 的开发体验&para;\n原文链接: https://spaces.ac.cn/archives/9920\n发布日期: \n\n上周在《写了个刷论文的辅助网站：Cool Papers》中，笔者分享了一个自己开发的刷论文网站Cool Papers，并得到了一些用户的认可。然而，“使用的人越多，暴露的问题就越多”，当用户量上来后，才感觉到之前写的代码是多么不严谨，...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/9920",
    "tags": [
      "网站",
      "论文",
      "酷论文",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 143
  },
  {
    "slug": "transformer升级之路1sinusoidal位置编码追根溯源",
    "title": "Transformer升级之路：1、Sinusoidal位置编码追根溯源",
    "description": "Transformer升级之路：1、Sinusoidal位置编码追根溯源&para;\n原文链接: https://spaces.ac.cn/archives/8231\n发布日期: \n\n最近笔者做了一些理解和改进Transformer的尝试，得到了一些似乎还有价值的经验和结论，遂开一个专题总结一下，命名为“Transformer升级之路”，既代表理解上的深入，也代表结果上的改进。\n作为该专题的第一篇...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/8231",
    "tags": [
      "复数",
      "分析",
      "attention",
      "位置编码",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 144
  },
  {
    "slug": "跟风玩玩目前最大的中文gpt2模型bert4keras",
    "title": "跟风玩玩目前最大的中文GPT2模型（bert4keras）",
    "description": "跟风玩玩目前最大的中文GPT2模型（bert4keras）&para;\n原文链接: https://spaces.ac.cn/archives/7912\n发布日期: \n\n相信不少读者这几天都看到了清华大学与智源人工智能研究院一起搞的“清源计划”（相关链接《中文版GPT-3来了？智源研究院发布清源 CPM —— 以中文为核心的大规模预训练模型》），里边开源了目前最大的中文GPT2模型CPM-LM（2...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7912",
    "tags": [
      "语言模型",
      "文本生成",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 145
  },
  {
    "slug": "修改transformer结构设计一个更快更好的mlm模型",
    "title": "修改Transformer结构，设计一个更快更好的MLM模型",
    "description": "修改Transformer结构，设计一个更快更好的MLM模型&para;\n原文链接: https://spaces.ac.cn/archives/7661\n发布日期: \n\n大家都知道，MLM（Masked Language Model）是BERT、RoBERTa的预训练方式，顾名思义，就是mask掉原始序列的一些token，然后让模型去预测这些被mask掉的token。随着研究的深入，大家发现ML...",
    "date": "",
    "source": "https://spaces.ac.cn/archives/7661",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 146
  }
]