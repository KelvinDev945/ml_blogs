[
  {
    "slug": "n个正态随机数的最大值的渐近估计",
    "title": "n个正态随机数的最大值的渐近估计",
    "description": "n个正态随机数的最大值的渐近估计&para;\n设$z_1,z_2,\\cdots,z_n$是$n$个从标准正态分布中独立重复采样出来的随机数，由此我们可以构造出很多衍生随机变量，比如$z_1+z_2+\\cdots+z_n$，它依旧服从正态分布，又比如$z_1^2+z_2^2+\\cdots+z_n^2$，它服从卡方分布。这篇文章我们来关心它的最大值$z_{\\max} = \\max\\{z_1,z_2,\\...",
    "date": "2025-11-06",
    "source": "https://spaces.ac.cn/archives/11390",
    "tags": [
      "数学",
      "概率论",
      "极值统计",
      "渐近分析"
    ],
    "status": "completed",
    "tags_reviewed": true,
    "post_number": 210
  },
  {
    "slug": "流形上的最速下降5-对偶梯度下降",
    "title": "流形上的最速下降：5. 对偶梯度下降",
    "description": "流形上的最速下降：5. 对偶梯度下降&para;\n原文链接: https://spaces.ac.cn/archives/11388\n发布日期: 2025-11-03\n\n前四篇文章我们求解了几个具体的给参数加等式约束的最速下降问题，其中第三、四篇的问题没法找到解析解，所以笔者提出了相应的不动点迭代法。其中的其中，第三篇文章《流形上的最速下降：3. Muon + Stiefel》所研究的“Stief...",
    "date": "2025-11-03",
    "source": "https://spaces.ac.cn/archives/11388",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 209
  },
  {
    "slug": "低精度attention可能存在有",
    "title": "低精度Attention可能存在有...",
    "description": "低精度Attention可能存在有...&para;\n原文链接: https://spaces.ac.cn/archives/11371\n发布日期: \n\n前段时间笔者在arXiv上刷到了论文《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》，里面描述的实验现象跟我们在训练Kimi K2时出现的...",
    "date": "2025-10-27",
    "source": "",
    "tags": [
      "近似",
      "分析",
      "优化",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 207
  },
  {
    "slug": "低精度attention可能存在有偏的舍入误差",
    "title": "低精度Attention可能存在有偏的舍入误差",
    "description": "低精度Attention可能存在有偏的舍入误差&para;\n原文链接: https://spaces.ac.cn/archives/11371\n发布日期: 2025-10-27\n\n前段时间笔者在arXiv上刷到了论文《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》，里面描述的实验现象跟我们在...",
    "date": "2025-10-27",
    "source": "https://spaces.ac.cn/archives/11371",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 208
  },
  {
    "slug": "mup之上1-好模型的三个特征",
    "title": "MuP之上：1. 好模型的三个特征",
    "description": "MuP之上：1. 好模型的三个特征&para;\n原文链接: https://spaces.ac.cn/archives/11340\n发布日期: 2025-10-21\n\n不知道大家有没有发现一个有趣的细节，Muon和MuP都是“Mu”开头，但两个“Mu”的原意完全不一样，前者是“M omentU m Orthogonalized by Newton-Schulz”，后者是“M aximal U pd...",
    "date": "2025-10-21",
    "source": "https://spaces.ac.cn/archives/11340",
    "tags": [
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 206
  },
  {
    "slug": "随机矩阵的谱范数的快速估计",
    "title": "随机矩阵的谱范数的快速估计",
    "description": "随机矩阵的谱范数的快速估计&para;\n原文链接: https://spaces.ac.cn/archives/11335\n发布日期: 2025-10-12\n\n在《高阶MuP：更简明但更高明的谱条件缩放》的“近似估计”一节中，我们曾“预支”了一个结论：“一个服从标准正态分布的$n\\times m$大小的随机矩阵，它的谱范数大致是$\\sqrt{n}+\\sqrt{m}$。”\n这篇文章我们来补充讨论这个...",
    "date": "2025-10-12",
    "source": "https://spaces.ac.cn/archives/11335",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 205
  },
  {
    "slug": "diveq一种非常简洁的vq训练方案",
    "title": "DiVeQ：一种非常简洁的VQ训练方案",
    "description": "DiVeQ：一种非常简洁的VQ训练方案&para;\n原文链接: https://spaces.ac.cn/archives/11328\n发布日期: 2025-10-08\n\n对于坚持离散化路线的研究人员来说，VQ（Vector Quantization）是视觉理解和生成的关键部分，担任着视觉中的“Tokenizer”的角色。它提出在2017年的论文《Neural Discrete Represent...",
    "date": "2025-10-08",
    "source": "https://spaces.ac.cn/archives/11328",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 204
  },
  {
    "slug": "为什么线性注意力要加short-c",
    "title": "为什么线性注意力要加Short C...",
    "description": "为什么线性注意力要加Short C...&para;\n原文链接: https://spaces.ac.cn/archives/11320\n发布日期: \n\n如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考《线性注意力简史：从模仿、创新到反哺》）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$加上了Short Co...",
    "date": "2025-10-05",
    "source": "",
    "tags": [
      "线性",
      "RNN",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 202
  },
  {
    "slug": "为什么线性注意力要加short-conv",
    "title": "为什么线性注意力要加Short Conv？",
    "description": "为什么线性注意力要加Short Conv？&para;\n原文链接: https://spaces.ac.cn/archives/11320\n发布日期: 2025-10-05\n\n如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考《线性注意力简史：从模仿、创新到反哺》）模型都给$\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V}$...",
    "date": "2025-10-05",
    "source": "https://spaces.ac.cn/archives/11320",
    "tags": [
      "详细推导",
      "机器学习"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 203
  },
  {
    "slug": "adamw的weight-rms的",
    "title": "AdamW的Weight RMS的...",
    "description": "AdamW的Weight RMS的...&para;\n原文链接: https://spaces.ac.cn/archives/11307\n发布日期: \n\n在《为什么Adam的Update RMS是0.2？》中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 @EIFY 指出相同的结果已经出现在论文《Rotational Equilibrium: How Weight Decay...",
    "date": "2025-10-01",
    "source": "",
    "tags": [
      "估计",
      "梯度",
      "优化器",
      "平均场",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 200
  },
  {
    "slug": "adamw的weight-rms的渐近估计",
    "title": "AdamW的Weight RMS的渐近估计",
    "description": "AdamW的Weight RMS的渐近估计&para;\n原文链接: https://spaces.ac.cn/archives/11307\n发布日期: 2025-10-01\n\n在《为什么Adam的Update RMS是0.2？》中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 @EIFY 指出相同的结果已经出现在论文《Rotational Equilibrium: How W...",
    "date": "2025-10-01",
    "source": "https://spaces.ac.cn/archives/11307",
    "tags": [
      "详细推导",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 201
  },
  {
    "slug": "重新思考学习率与batch-siz",
    "title": "重新思考学习率与Batch Siz...",
    "description": "重新思考学习率与Batch Siz...&para;\n原文链接: https://spaces.ac.cn/archives/11301\n发布日期: \n\n我们在《重新思考学习率与Batch Size（二）：平均场》中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在《配置不同的学习率，LoRA还能再涨一点？》、...",
    "date": "2025-09-22",
    "source": "",
    "tags": [
      "学习率",
      "优化器",
      "尺度定律",
      "平均场",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 198
  },
  {
    "slug": "重新思考学习率与batch-size四ema",
    "title": "重新思考学习率与Batch Size（四）：EMA",
    "description": "重新思考学习率与Batch Size（四）：EMA&para;\n原文链接: https://spaces.ac.cn/archives/11301\n发布日期: 2025-09-22\n\n我们在《重新思考学习率与Batch Size（二）：平均场》中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在《配置不同的学习...",
    "date": "2025-09-22",
    "source": "https://spaces.ac.cn/archives/11301",
    "tags": [
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 199
  },
  {
    "slug": "重新思考学习率与batch-size三muon",
    "title": "重新思考学习率与Batch Size（三）：Muon",
    "description": "重新思考学习率与Batch Size（三）：Muon&para;\n原文链接: https://spaces.ac.cn/archives/11285\n发布日期: 2025-09-15\n\n前两篇文章《重新思考学习率与Batch Size（一）：现状》和《重新思考学习率与Batch Size（二）：平均场》中，我们主要是提出了平均场方法，用以简化学习率与Batch Size的相关计算。当时我们分析的优...",
    "date": "2025-09-15",
    "source": "https://spaces.ac.cn/archives/11285",
    "tags": [
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 197
  },
  {
    "slug": "重新思考学习率与batch-size二平均场",
    "title": "重新思考学习率与Batch Size（二）：平均场",
    "description": "重新思考学习率与Batch Size（二）：平均场&para;\n原文链接: https://spaces.ac.cn/archives/11280\n发布日期: \n\n上文《重新思考学习率与Batch Size（一）：现状》末尾我们说到，对于SignSGD、SoftSignSGD等$\\tilde{\\boldsymbol{\\varphi}}_B$非线性依赖于$\\tilde{\\boldsymbol{g}}...",
    "date": "2025-09-10",
    "source": "",
    "tags": [
      "详细推导",
      "学习率",
      "优化器",
      "尺度定律",
      "平均场",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 196
  },
  {
    "slug": "为什么adam的update-rms是02",
    "title": "为什么Adam的Update RMS是0.2？",
    "description": "为什么Adam的Update RMS是0.2？&para;\n原文链接: https://spaces.ac.cn/archives/11267\n发布日期: \n\n众所周知，我们很早就开始尝试将Muon用于大规模LLM的训练。特别地，在《Muon续集：为什么我们选择尝试Muon？》中，我们提出了“Match Adam Update RMS”的技巧，以便快速从Adam迁移到Muon上，这个技巧同样用到了...",
    "date": "2025-09-02",
    "source": "",
    "tags": [
      "详细推导",
      "分析",
      "梯度",
      "优化器",
      "平均场",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 195
  },
  {
    "slug": "重新思考学习率与batch-size一现状",
    "title": "重新思考学习率与Batch Size（一）：现状",
    "description": "重新思考学习率与Batch Size（一）：现状&para;\n原文链接: https://spaces.ac.cn/archives/11260\n发布日期: \n\n在之前的文章《当Batch Size增大时，学习率该如何随之变化？》和《Adam的epsilon如何影响学习率的Scaling Law？》中，我们从理论上讨论了学习率随Batch Size的变化规律，其中比较经典的部分是由OpenAI提出...",
    "date": "2025-09-01",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 194
  },
  {
    "slug": "流形上的最速下降4-muon-谱球面",
    "title": "流形上的最速下降：4. Muon + 谱球面",
    "description": "流形上的最速下降：4. Muon + 谱球面&para;\n原文链接: https://spaces.ac.cn/archives/11241\n发布日期: \n\n看完了前三篇的读者，想必已经熟悉本系列的“套路”——先提出更新量的约束，寻找最速下降方向，接着再给参数也加上约束，寻找新的最速下降方向。在求解参数约束问题时，我们采用的是“一阶近似够用”原则来简化约束形式，这在几何上对应于“切空间”。然后，我...",
    "date": "2025-08-21",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 193
  },
  {
    "slug": "relugeluswish的一个恒等式",
    "title": "ReLU/GeLU/Swish的一个恒等式",
    "description": "ReLU/GeLU/Swish的一个恒等式&para;\n原文链接: https://spaces.ac.cn/archives/11233\n发布日期: \n\n今天水一点轻松的内容，它基于笔者这两天意识到的一个恒等式。这个恒等式实际上很简单，但初看之下会有点意料之外的感觉，所以来记录一下。\n基本结果&para;\n我们知道$\\newcommand{relu}{\\mathop{\\text{relu}}}\\...",
    "date": "2025-08-16",
    "source": "",
    "tags": [
      "分析",
      "神经网络",
      "恒等式",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 192
  },
  {
    "slug": "流形上的最速下降3-muon-stiefel",
    "title": "流形上的最速下降：3. Muon + Stiefel",
    "description": "流形上的最速下降：3. Muon + Stiefel&para;\n原文链接: https://spaces.ac.cn/archives/11221\n发布日期: \n\n上回说到，当我们把优化对象从向量参数转移到矩阵参数，并选用更适合矩阵的谱范数约束后，Muon优化器便自然而然地出现了。进一步地，我们考虑了给参数加上正交约束后的最速下降方向，这其中又分方阵和非方阵两部分讨论，其中方阵的求解我们在上一篇...",
    "date": "2025-08-08",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 191
  },
  {
    "slug": "流形上的最速下降2-muon-正交",
    "title": "流形上的最速下降：2. Muon + 正交",
    "description": "流形上的最速下降：2. Muon + 正交&para;\n原文链接: https://spaces.ac.cn/archives/11215\n发布日期: \n\n本文继续我们的约束优化系列。在上文《流形上的最速下降：1. SGD + 超球面》中，我们重温了优化器的“最小作用量”原理，提出不同优化器的核心差异在于给更新量施加的不同约束，如果这个约束是欧几里得范数，那么对应的最速下降便是SGD。进一步地，我...",
    "date": "2025-08-06",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "优化器",
      "muon",
      "约束",
      "最速下降"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 190
  },
  {
    "slug": "流形上的最速下降1-sgd-超球面",
    "title": "流形上的最速下降：1.  SGD + 超球面",
    "description": "流形上的最速下降：1.  SGD + 超球面&para;\n原文链接: https://spaces.ac.cn/archives/11196\n发布日期: \n\n类似“梯度的反方向是下降最快的方向”的描述，经常用于介绍梯度下降（SGD）的原理。然而，这句话是有条件的，比如“方向”在数学上是单位向量，它依赖于“范数（模长）”的定义，不同范数的结论也不同，Muon实际上就是给矩阵参数换了个谱范数，从而得到...",
    "date": "2025-08-01",
    "source": "",
    "tags": [
      "详细推导",
      "不等式",
      "优化器",
      "约束",
      "最速下降",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 189
  },
  {
    "slug": "矩阵r次方根和逆r次方根的高效计算",
    "title": "矩阵r次方根和逆r次方根的高效计算",
    "description": "矩阵r次方根和逆r次方根的高效计算&para;\n原文链接: https://spaces.ac.cn/archives/11175\n发布日期: \n\n上一篇文章《矩阵平方根和逆平方根的高效计算》中，笔者从$\\newcommand{mcsgn}{\\mathop{\\text{mcsgn}}}\\mcsgn$算子出发，提出了一种很漂亮的矩阵平方根和逆平方根的计算方法。比较神奇的是，该方案经过化简之后，最终公...",
    "date": "2025-07-21",
    "source": "",
    "tags": [
      "详细推导",
      "代数",
      "迭代",
      "矩阵",
      "线性",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 188
  },
  {
    "slug": "矩阵平方根和逆平方根的高效计算",
    "title": "矩阵平方根和逆平方根的高效计算",
    "description": "矩阵平方根和逆平方根的高效计算&para;\n原文链接: https://spaces.ac.cn/archives/11158\n发布日期: \n\n设$\\boldsymbol{P}\\in\\mathbb{R}^{n\\times n}$是一个特征值都是非负实数的$n$阶方阵，本文来讨论它的平方根$\\boldsymbol{P}^{1/2}$和逆平方根$\\boldsymbol{P}^{-1/2}$的计算。\n基...",
    "date": "2025-07-19",
    "source": "",
    "tags": [
      "详细推导",
      "代数",
      "迭代",
      "矩阵",
      "线性",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 187
  },
  {
    "slug": "qk-clip让muon在scaleup之路上更进一步",
    "title": "QK-Clip：让Muon在Scaleup之路上更进一步",
    "description": "QK-Clip：让Muon在Scaleup之路上更进一步&para;\n原文链接: https://spaces.ac.cn/archives/11126\n发布日期: \n\n四个月前，我们发布了Moonlight，在16B的MoE模型上验证了Muon优化器的有效性。在Moonlight中，我们确认了给Muon添加Weight Decay的必要性，同时提出了通过Update RMS对齐来迁移Adam超参...",
    "date": "2025-07-12",
    "source": "",
    "tags": [
      "优化",
      "attention",
      "优化器",
      "muon",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 186
  },
  {
    "slug": "transformer升级之路21mla好在哪里下",
    "title": "Transformer升级之路：21、MLA好在哪里?（下）",
    "description": "Transformer升级之路：21、MLA好在哪里?（下）&para;\n原文链接: https://spaces.ac.cn/archives/11111\n发布日期: \n\n在文章《Transformer升级之路：20、MLA好在哪里?（上）》中，我们对MLA相比常见MHA、GQA、MQA的一些变化分别做了消融实验，其中的变化包括“增大head_dims”、“Partial RoPE”和“KV共享...",
    "date": "2025-07-10",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 185
  },
  {
    "slug": "对角低秩三角阵的高效求逆方法",
    "title": "“对角+低秩”三角阵的高效求逆方法",
    "description": "“对角+低秩”三角阵的高效求逆方法&para;\n原文链接: https://spaces.ac.cn/archives/11072\n发布日期: \n\n从文章《线性注意力简史：从模仿、创新到反哺》我们可以发现，DeltaNet及其后的线性Attention模型，基本上都关联到了逆矩阵$(\\boldsymbol{I} + \\boldsymbol{K}\\boldsymbol{K}^{\\top}\\odot\\...",
    "date": "2025-07-01",
    "source": "",
    "tags": [
      "详细推导",
      "计算",
      "矩阵",
      "RNN",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 184
  },
  {
    "slug": "矩阵符号函数mcsgn能计算什么",
    "title": "矩阵符号函数mcsgn能计算什么？",
    "description": "矩阵符号函数mcsgn能计算什么？&para;\n原文链接: https://spaces.ac.cn/archives/11056\n发布日期: \n\n在《msign的导数》一文中，我们正式引入了两种矩阵符号函数$\\newcommand{msign}{\\mathop{\\text{msign}}}\\msign$和$\\newcommand{mcsgn}{\\mathop{\\text{mcsgn}}}\\mcs...",
    "date": "2025-06-23",
    "source": "",
    "tags": [
      "代数",
      "矩阵",
      "线性",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 182
  },
  {
    "slug": "通过msign来计算奇异值裁剪mclip下",
    "title": "通过msign来计算奇异值裁剪mclip（下）",
    "description": "通过msign来计算奇异值裁剪mclip（下）&para;\n原文链接: https://spaces.ac.cn/archives/11059\n发布日期: \n\n前面我们在《通过msign来计算奇异值裁剪mclip（上）》讨论了奇异值裁剪$\\newcommand{mclip}{\\mathop{\\text{mclip}}}\\mclip$的数值计算，核心思路来自 @leloykun 的文章《Numeri...",
    "date": "2025-06-23",
    "source": "",
    "tags": [
      "详细推导",
      "迭代",
      "近似",
      "矩阵",
      "SVD",
      "muon"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 183
  },
  {
    "slug": "msign的导数",
    "title": "msign的导数",
    "description": "msign的导数&para;\n原文链接: https://spaces.ac.cn/archives/11025\n发布日期: \n\n这篇文章我们来推导$\\newcommand{msign}{\\mathop{\\text{msign}}}\\msign$算子的求导公式。如果读者想要像《Test-Time Training Done Right》一样，将TTT和Muon结合起来，那么本文可能会对你有帮助。...",
    "date": "2025-06-13",
    "source": "",
    "tags": [
      "微积分",
      "矩阵",
      "梯度",
      "muon",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 181
  },
  {
    "slug": "通过msign来计算奇异值裁剪mclip上",
    "title": "通过msign来计算奇异值裁剪mclip（上）",
    "description": "通过msign来计算奇异值裁剪mclip（上）&para;\n原文链接: https://spaces.ac.cn/archives/11006\n发布日期: \n\n前面我们用了两篇文章《msign算子的Newton-Schulz迭代（上）》和《msign算子的Newton-Schulz迭代（下）》讨论了矩阵的$\\newcommand{msign}{\\mathop{\\text{msign}}}\\newc...",
    "date": "2025-06-07",
    "source": "",
    "tags": [
      "迭代",
      "近似",
      "矩阵",
      "SVD",
      "muon"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 180
  },
  {
    "slug": "msign算子的newton-schulz迭代下",
    "title": "msign算子的Newton-Schulz迭代（下）",
    "description": "msign算子的Newton-Schulz迭代（下）&para;\n原文链接: https://spaces.ac.cn/archives/10996\n发布日期: \n\n在上文《msign算子的Newton-Schulz迭代（上）》中，我们试图为$\\mathop{\\text{msign}}$算子寻找更好的Newton-Schulz迭代，以期在有限迭代步数内能达到尽可能高的近似程度，这一过程又可以转化为...",
    "date": "2025-06-05",
    "source": "",
    "tags": [
      "迭代",
      "近似",
      "优化器",
      "muon",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 179
  },
  {
    "slug": "等值振荡定理最优多项式逼近的充要条件",
    "title": "等值振荡定理：最优多项式逼近的充要条件",
    "description": "等值振荡定理：最优多项式逼近的充要条件&para;\n原文链接: https://spaces.ac.cn/archives/10972\n发布日期: \n\n最近在阅读时，遇到了一个关于最优多项式逼近的“等值振荡定理（Equioscillation Theorem）”，证明过程还涉及到无穷范数求导，感觉结论和证明都颇为新奇，特来记录一番。\n参考资料：《Notes on how to prove Cheb...",
    "date": "2025-06-02",
    "source": "",
    "tags": [
      "导数",
      "近似",
      "最优",
      "分析",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 178
  },
  {
    "slug": "生成扩散模型漫谈三十从瞬时速度到平均速度",
    "title": "生成扩散模型漫谈（三十）：从瞬时速度到平均速度",
    "description": "生成扩散模型漫谈（三十）：从瞬时速度到平均速度&para;\n原文链接: https://spaces.ac.cn/archives/10958\n发布日期: \n\n众所周知，生成速度慢是扩散模型一直以来的痛点，而为了解决这个问题，大家可谓“八仙过海，各显神通”，提出了各式各样的解决方案，然而长久以来并没一项工作能够脱颖而出，成为标配。什么样的工作能够达到这个标准呢？在笔者看来，它至少满足几个条件：...",
    "date": "2025-05-26",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 177
  },
  {
    "slug": "moe环游记5均匀分布的反思",
    "title": "MoE环游记：5、均匀分布的反思",
    "description": "MoE环游记：5、均匀分布的反思&para;\n原文链接: https://spaces.ac.cn/archives/10945\n发布日期: \n\n如果说Meta的LLAMA系列为Dense模型确立了标准架构，那么DeepSeek或许就是MoE标准架构的奠基者。当然，这并非指DeepSeek首创了MoE，也不是说它的MoE不可超越，而是指DeepSeek对MoE所提的一些改进，很可能都是效果增益比较...",
    "date": "2025-05-16",
    "source": "",
    "tags": [
      "优化",
      "稀疏",
      "moe",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 176
  },
  {
    "slug": "msign算子的newton-schulz迭代上",
    "title": "msign算子的Newton-Schulz迭代（上）",
    "description": "msign算子的Newton-Schulz迭代（上）&para;\n原文链接: https://spaces.ac.cn/archives/10922\n发布日期: \n\n在之前的《Muon优化器赏析：从向量到矩阵的本质跨越》、《Muon续集：为什么我们选择尝试Muon？》等文章中，我们介绍了一个极具潜力、有望替代Adam的新兴优化器——“Muon”。随着相关研究的不断深入，Muon优化器受到的关注度也...",
    "date": "2025-05-11",
    "source": "",
    "tags": [
      "迭代",
      "近似",
      "优化器",
      "muon",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 175
  },
  {
    "slug": "transformer升级之路20mla好在哪里上",
    "title": "Transformer升级之路：20、MLA好在哪里?（上）",
    "description": "Transformer升级之路：20、MLA好在哪里?（上）&para;\n原文链接: https://spaces.ac.cn/archives/10907\n发布日期: \n\n自从DeepSeek爆火后，它所提的Attention变体MLA（M ulti-head L atent A ttention）也愈发受到关注。MLA通过巧妙的设计实现了MHA与MQA的自由切换，使得模型可以根据训练和推理的不...",
    "date": "2025-05-04",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 174
  },
  {
    "slug": "一道概率不等式盯着它到显然成立为止",
    "title": "一道概率不等式：盯着它到显然成立为止！",
    "description": "一道概率不等式：盯着它到显然成立为止！&para;\n原文链接: https://spaces.ac.cn/archives/10902\n发布日期: \n\n前两天，QQ群里有群友抛出了一道不等式求证：  \n\n一道概率相关的不等式，出自《There is no fast single hashing algorithm》\n简短的题目，加上“easily”的提示，让人觉得这似乎是显然成立的结果，然而提问者...",
    "date": "2025-04-30",
    "source": "",
    "tags": [
      "不等式",
      "概率",
      "显然成立",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 173
  },
  {
    "slug": "svd的导数",
    "title": "SVD的导数",
    "description": "SVD的导数&para;\n原文链接: https://spaces.ac.cn/archives/10878\n发布日期: \n\nSVD（Singular Value Decomposition，奇异值分解）是常见的矩阵分解算法，相信很多读者都已经对它有所了解，此前我们在《低秩近似之路（二）：SVD》也专门介绍过它。然而，读者是否想到，SVD竟然还可以求导呢？笔者刚了解到这一结论时也颇感意外，因为直觉...",
    "date": "2025-04-26",
    "source": "",
    "tags": [
      "详细推导",
      "微积分",
      "分析",
      "矩阵",
      "SVD",
      "梯度"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 172
  },
  {
    "slug": "矩阵的有效秩effective-rank",
    "title": "矩阵的有效秩（Effective Rank）",
    "description": "矩阵的有效秩（Effective Rank）&para;\n原文链接: https://spaces.ac.cn/archives/10847\n发布日期: \n\n秩（Rank）是线性代数中的重要概念，它代表了矩阵的内在维度。然而，数学上对秩的严格定义，很多时候并不完全适用于数值计算场景，因为秩等于非零奇异值的个数，而数学上对“等于零”这件事的理解跟数值计算有所不同，数学上的“等于零”是绝对地、严格地等...",
    "date": "2025-04-10",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "熵",
      "稀疏",
      "低秩",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 171
  },
  {
    "slug": "通过梯度近似寻找normalization的替代品",
    "title": "通过梯度近似寻找Normalization的替代品",
    "description": "通过梯度近似寻找Normalization的替代品&para;\n原文链接: https://spaces.ac.cn/archives/10831\n发布日期: \n\n不知道大家有没有留意到前段时间的《Transformers without Normalization》？这篇论文试图将Transformer模型中的Normalization层用一个Element-wise的运算DyT替代，以期能提高...",
    "date": "2025-04-02",
    "source": "",
    "tags": [
      "详细推导",
      "函数",
      "分析",
      "梯度",
      "光滑",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 170
  },
  {
    "slug": "moe环游记4难处应当多投入",
    "title": "MoE环游记：4、难处应当多投入",
    "description": "MoE环游记：4、难处应当多投入&para;\n原文链接: https://spaces.ac.cn/archives/10815\n发布日期: \n\n前两篇文章我们都在讨论负载均衡，其中在《MoE环游记：3、换个思路来分配》介绍Loss-Free方案时，笔者留了一个悬念：它引入的Bias项有一个冗余的自由度，这个自由度可以用来做另外有趣的事情。这篇文章我们就来讨论这件事。\n我们知道，MoE是为每个To...",
    "date": "2025-03-28",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "梯度",
      "moe",
      "动态",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 169
  },
  {
    "slug": "高阶mup更简明但更高明的谱条件缩放",
    "title": "高阶MuP：更简明但更高明的谱条件缩放",
    "description": "高阶MuP：更简明但更高明的谱条件缩放&para;\n原文链接: https://spaces.ac.cn/archives/10795\n发布日期: \n\n在文章《初探MuP：超参数的跨模型尺度迁移规律》中，我们基于前向传播、反向传播、损失增量和特征变化的尺度不变性推导了MuP（Maximal Update Parametrization）。可能对于部分读者来说，这一过程还是显得有些繁琐，但实际上它比...",
    "date": "2025-03-24",
    "source": "",
    "tags": [
      "详细推导",
      "LoRA",
      "梯度",
      "优化器",
      "尺度定律",
      "谱范数"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 168
  },
  {
    "slug": "初探mup超参数的跨模型尺度迁移规律",
    "title": "初探MuP：超参数的跨模型尺度迁移规律",
    "description": "初探MuP：超参数的跨模型尺度迁移规律&para;\n原文链接: https://spaces.ac.cn/archives/10770\n发布日期: \n\n众所周知，完整训练一次大型LLM的成本是昂贵的，这就决定了我们不可能直接在大型LLM上反复测试超参数。一个很自然的想法是希望可以在同结构的小模型上仔细搜索超参数，找到最优组合后直接迁移到大模型上。尽管这个想法很朴素，但要实现它并不平凡，它需要我们了...",
    "date": "2025-03-13",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 167
  },
  {
    "slug": "moe环游记3换个思路来分配",
    "title": "MoE环游记：3、换个思路来分配",
    "description": "MoE环游记：3、换个思路来分配&para;\n原文链接: https://spaces.ac.cn/archives/10757\n发布日期: \n\n这篇文章我们继续探讨MoE的负载均衡问题。在上一篇文章《MoE环游记：2、不患寡而患不均》中，我们主要讨论了通过Aux Loss来促进负载均衡的思路。Aux Loss固然简单直观，但它也有一个明显的缺点——权重不好调——调低了无法促进均衡，调高了容易损害...",
    "date": "2025-03-05",
    "source": "",
    "tags": [
      "详细推导",
      "最优",
      "损失函数",
      "梯度",
      "moe",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 166
  },
  {
    "slug": "muon续集为什么我们选择尝试muon",
    "title": "Muon续集：为什么我们选择尝试Muon？",
    "description": "Muon续集：为什么我们选择尝试Muon？&para;\n原文链接: https://spaces.ac.cn/archives/10739\n发布日期: \n\n本文解读一下我们最新的技术报告《Muon is Scalable for LLM Training》，里边分享了我们之前在《Muon优化器赏析：从向量到矩阵的本质跨越》介绍过的Muon优化器的一次较大规模的实践，并开源了相应的模型（我们称之为“...",
    "date": "2025-02-27",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "梯度",
      "优化器",
      "谱范数",
      "muon"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 165
  },
  {
    "slug": "moe环游记2不患寡而患不均",
    "title": "MoE环游记：2、不患寡而患不均",
    "description": "MoE环游记：2、不患寡而患不均&para;\n原文链接: https://spaces.ac.cn/archives/10735\n发布日期: \n\n在上一篇文章《MoE环游记：1、从几何意义出发》中，我们介绍了MoE的一个几何诠释，旨在通过Dense模型的最佳逼近出发来推导和理解MoE。同时在文末我们也说了，给出MoE的计算公式仅仅是开始，训练一个实际有效的MoE模型还有很多细节补，比如本文要讨论的...",
    "date": "2025-02-21",
    "source": "",
    "tags": [
      "详细推导",
      "损失函数",
      "梯度",
      "稀疏",
      "moe",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 164
  },
  {
    "slug": "生成扩散模型漫谈二十九用ddpm来离散编码",
    "title": "生成扩散模型漫谈（二十九）：用DDPM来离散编码",
    "description": "生成扩散模型漫谈（二十九）：用DDPM来离散编码&para;\n原文链接: https://spaces.ac.cn/archives/10711\n发布日期: \n\n笔者前两天在arXiv刷到了一篇新论文《Compressed Image Generation with Denoising Diffusion Codebook Models》，实在为作者的天马行空所叹服，忍不住来跟大家分享一番。\n如本...",
    "date": "2025-02-14",
    "source": "",
    "tags": [
      "生成模型",
      "编码",
      "DDPM",
      "扩散",
      "离散化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 163
  },
  {
    "slug": "moe环游记1从几何意义出发",
    "title": "MoE环游记：1、从几何意义出发",
    "description": "MoE环游记：1、从几何意义出发&para;\n原文链接: https://spaces.ac.cn/archives/10699\n发布日期: \n\n前两年福至心灵之下，开了一个“Transformer升级之路”系列，陆续分享了主流Transformer架构的一些改进工作和个人思考，得到了部份读者的认可。这篇文章开始，我们沿着同样的风格，介绍当前另一个主流架构MoE（Mixture of Expert...",
    "date": "2025-02-08",
    "source": "",
    "tags": [
      "模型",
      "几何",
      "稀疏",
      "moe",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 162
  },
  {
    "slug": "三个球的交点坐标三球交会定位",
    "title": "三个球的交点坐标（三球交会定位）",
    "description": "三个球的交点坐标（三球交会定位）&para;\n原文链接: https://spaces.ac.cn/archives/10684\n发布日期: \n\n前几天笔者在思考一个问题时，联想到了三球交点问题，即给定三个球的球心坐标和半径，求这三个球的交点坐标。按理说这是一个定义清晰且简明的问题，并且具有鲜明的应用背景（比如卫星定位），应该早已有人给出“标准答案”才对。但笔者搜了一圈，发现不管是英文资料还是中文...",
    "date": "2025-01-28",
    "source": "",
    "tags": [
      "方程",
      "几何",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 161
  },
  {
    "slug": "细水长flow之tarflow流模型满血归来",
    "title": "细水长flow之TARFLOW：流模型满血归来？",
    "description": "细水长flow之TARFLOW：流模型满血归来？&para;\n原文链接: https://spaces.ac.cn/archives/10667\n发布日期: \n\n不知道还有没有读者对这个系列有印象？这个系列取名“细水长flow”，主要介绍flow模型的相关工作，起因是当年（2018年）OpenAI发布了一个新的流模型Glow，在以GAN为主流的当时来说着实让人惊艳了一番。但惊艳归惊艳，事实上在相当...",
    "date": "2025-01-17",
    "source": "",
    "tags": [
      "流模型",
      "flow",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 160
  },
  {
    "slug": "低秩近似之路五cur",
    "title": "低秩近似之路（五）：CUR",
    "description": "低秩近似之路（五）：CUR&para;\n原文链接: https://spaces.ac.cn/archives/10662\n发布日期: \n\n再次回到低秩近似之路上。在《低秩近似之路（四）：ID》中，我们介绍了“插值分解（Interpolative Decomposition，ID）”，这是为矩阵$\\boldsymbol{M}\\in\\mathbb{R}^{n\\times m}$寻找$\\boldsym...",
    "date": "2025-01-12",
    "source": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 159
  },
  {
    "slug": "为什么梯度裁剪的默认模长是1",
    "title": "为什么梯度裁剪的默认模长是1？",
    "description": "为什么梯度裁剪的默认模长是1？&para;\n原文链接: https://spaces.ac.cn/archives/10657\n发布日期: \n\n我们知道，梯度裁剪（Gradient Clipping）是让模型训练更加平稳的常用技巧。常用的梯度裁剪是根据所有参数的梯度总模长来对梯度进行裁剪，其运算可以表示为\n\\begin{equation}\\text{clip}(\\boldsymbol{g},\\ta...",
    "date": "2025-01-02",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "梯度",
      "学习率",
      "优化器",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 158
  },
  {
    "slug": "从谱范数梯度到新式权重衰减的思考",
    "title": "从谱范数梯度到新式权重衰减的思考",
    "description": "从谱范数梯度到新式权重衰减的思考&para;\n原文链接: https://spaces.ac.cn/archives/10648\n发布日期: \n\n在文章《Muon优化器赏析：从向量到矩阵的本质跨越》中，我们介绍了一个名为“Muon”的新优化器，其中一个理解视角是作为谱范数正则下的最速梯度下降，这似乎揭示了矩阵参数的更本质的优化方向。众所周知，对于矩阵参数我们经常也会加权重衰减（Weight Dec...",
    "date": "2024-12-25",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "优化",
      "梯度",
      "优化器",
      "谱范数"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 157
  },
  {
    "slug": "生成扩散模型漫谈二十八分步理解一致性模型",
    "title": "生成扩散模型漫谈（二十八）：分步理解一致性模型",
    "description": "生成扩散模型漫谈（二十八）：分步理解一致性模型&para;\n原文链接: https://spaces.ac.cn/archives/10633\n发布日期: \n\n书接上文，在《生成扩散模型漫谈（二十七）：将步长作为条件输入》中，我们介绍了加速采样的Shortcut模型，其对比的模型之一就是“一致性模型（Consistency Models）”。事实上，早在《生成扩散模型漫谈（十七）：构建ODE的一般...",
    "date": "2024-12-18",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 156
  },
  {
    "slug": "生成扩散模型漫谈二十七将步长作为条件输入",
    "title": "生成扩散模型漫谈（二十七）：将步长作为条件输入",
    "description": "生成扩散模型漫谈（二十七）：将步长作为条件输入&para;\n原文链接: https://spaces.ac.cn/archives/10617\n发布日期: \n\n这篇文章我们再次聚焦于扩散模型的采样加速。众所周知，扩散模型的采样加速主要有两种思路，一是开发更高效的求解器，二是事后蒸馏。然而，据笔者观察，除了上两篇文章介绍过的SiD外，这两种方案都鲜有能将生成步数降低到一步的结果。虽然SiD能做到单步...",
    "date": "2024-12-15",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "采样",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 155
  },
  {
    "slug": "muon优化器赏析从向量到矩阵的本质跨越",
    "title": "Muon优化器赏析：从向量到矩阵的本质跨越",
    "description": "Muon优化器赏析：从向量到矩阵的本质跨越&para;\n原文链接: https://spaces.ac.cn/archives/10592\n发布日期: \n\n随着LLM时代的到来，学术界对于优化器的研究热情似乎有所减退。这主要是因为目前主流的AdamW已经能够满足大多数需求，而如果对优化器“大动干戈”，那么需要巨大的验证成本。因此，当前优化器的变化，多数都只是工业界根据自己的训练经验来对AdamW打...",
    "date": "2024-12-10",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "梯度",
      "优化器",
      "谱范数",
      "muon"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 154
  },
  {
    "slug": "从hessian近似看自适应学习率优化器",
    "title": "从Hessian近似看自适应学习率优化器",
    "description": "从Hessian近似看自适应学习率优化器&para;\n原文链接: https://spaces.ac.cn/archives/10588\n发布日期: \n\n这几天在重温去年的Meta的一篇论文《A Theory on Adam Instability in Large-Scale Machine Learning》，里边给出了看待Adam等自适应学习率优化器的新视角：它指出梯度平方的滑动平均某种程度...",
    "date": "2024-11-29",
    "source": "",
    "tags": [
      "详细推导",
      "优化",
      "梯度",
      "学习率",
      "优化器",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 153
  },
  {
    "slug": "生成扩散模型漫谈二十六基于恒等式的蒸馏下",
    "title": "生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）",
    "description": "生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）&para;\n原文链接: https://spaces.ac.cn/archives/10567\n发布日期: \n\n继续回到我们的扩散系列。在《生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）》中，我们介绍了SiD（Score identity Distillation），这是一种不需要真实数据、也不需要从教师模型采样的扩散模型蒸馏方案，其形式类似...",
    "date": "2024-11-22",
    "source": "",
    "tags": [
      "生成模型",
      "梯度",
      "扩散",
      "去噪",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 152
  },
  {
    "slug": "adam的epsilon如何影响学习率的scaling-law",
    "title": "Adam的epsilon如何影响学习率的Scaling Law？",
    "description": "Adam的epsilon如何影响学习率的Scaling Law？&para;\n原文链接: https://spaces.ac.cn/archives/10563\n发布日期: \n\n上一篇文章《当Batch Size增大时，学习率该如何随之变化？》我们从多个角度讨论了学习率与Batch Size之间的缩放规律，其中对于Adam优化器我们采用了SignSGD近似，这是分析Adam优化器常用的手段。那么一...",
    "date": "2024-11-18",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 151
  },
  {
    "slug": "当batch-size增大时学习率该如何随之变化",
    "title": "当Batch Size增大时，学习率该如何随之变化？",
    "description": "当Batch Size增大时，学习率该如何随之变化？&para;\n原文链接: https://spaces.ac.cn/archives/10542\n发布日期: \n\n随着算力的飞速进步，有越多越多的场景希望能够实现“算力换时间”，即通过堆砌算力来缩短模型训练时间。理想情况下，我们希望投入$n$倍的算力，那么达到同样效果的时间则缩短为$1/n$，此时总的算力成本是一致的。这个“希望”看上去很合理和自...",
    "date": "2024-11-14",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "学习率",
      "优化器",
      "尺度定律",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 150
  },
  {
    "slug": "vq的又一技巧给编码表加一个线性变换",
    "title": "VQ的又一技巧：给编码表加一个线性变换",
    "description": "VQ的又一技巧：给编码表加一个线性变换&para;\n原文链接: https://spaces.ac.cn/archives/10519\n发布日期: \n\n在《VQ的旋转技巧：梯度直通估计的一般推广》中，我们介绍了VQ（Vector Quantization）的Rotation Trick，它的思想是通过推广VQ的STE（Straight-Through Estimator）来为VQ设计更好的梯度，从...",
    "date": "2024-11-06",
    "source": "",
    "tags": [
      "详细推导",
      "生成模型",
      "编码",
      "梯度",
      "离散化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 149
  },
  {
    "slug": "低秩近似之路四id",
    "title": "低秩近似之路（四）：ID",
    "description": "低秩近似之路（四）：ID&para;\n原文链接: https://spaces.ac.cn/archives/10501\n发布日期: \n\n这篇文章的主角是ID（Interpolative Decomposition），中文可以称之为“插值分解”，它同样可以理解为是一种具有特定结构的低秩分解，其中的一侧是该矩阵的若干列（当然如果你偏好于行，那么选择行也没什么问题），换句话说，ID试图从一个矩阵中找出...",
    "date": "2024-10-30",
    "source": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 148
  },
  {
    "slug": "vq的旋转技巧梯度直通估计的一般推广",
    "title": "VQ的旋转技巧：梯度直通估计的一般推广",
    "description": "VQ的旋转技巧：梯度直通估计的一般推广&para;\n原文链接: https://spaces.ac.cn/archives/10489\n发布日期: \n\n随着多模态LLM的方兴未艾，VQ（Vector Quantization）的地位也“水涨船高”，它可以作为视觉乃至任意模态的Tokenizer，将多模态数据统一到自回归生成框架中。遗憾的是，自VQ-VAE首次提出VQ以来，其理论并没有显著进步，像编...",
    "date": "2024-10-24",
    "source": "",
    "tags": [
      "详细推导",
      "生成模型",
      "编码",
      "梯度",
      "离散化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 147
  },
  {
    "slug": "低秩近似之路三cr",
    "title": "低秩近似之路（三）：CR",
    "description": "低秩近似之路（三）：CR&para;\n原文链接: https://spaces.ac.cn/archives/10427\n发布日期: \n\n在《低秩近似之路（二）：SVD》中，我们证明了SVD可以给出任意矩阵的最优低秩近似。那里的最优近似是无约束的，也就是说SVD给出的结果只管误差上的最小，不在乎矩阵的具体结构，而在很多应用场景中，出于可解释性或者非线性处理等需求，我们往往希望得到具有某些特殊结构的...",
    "date": "2024-10-11",
    "source": "",
    "tags": [
      "近似",
      "最优",
      "矩阵",
      "低秩",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 146
  },
  {
    "slug": "低秩近似之路二svd",
    "title": "低秩近似之路（二）：SVD",
    "description": "低秩近似之路（二）：SVD&para;\n原文链接: https://spaces.ac.cn/archives/10407\n发布日期: \n\n上一篇文章中我们介绍了“伪逆”，它关系到给定矩阵$\\boldsymbol{M}$和$\\boldsymbol{A}$（或$\\boldsymbol{B}$）时优化目标$\\Vert \\boldsymbol{A}\\boldsymbol{B} - \\boldsymbol...",
    "date": "2024-10-01",
    "source": "",
    "tags": [
      "详细推导",
      "近似",
      "最优",
      "矩阵",
      "SVD",
      "低秩"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 145
  },
  {
    "slug": "softmax后传寻找top-k的光滑近似",
    "title": "Softmax后传：寻找Top-K的光滑近似",
    "description": "Softmax后传：寻找Top-K的光滑近似&para;\n原文链接: https://spaces.ac.cn/archives/10373\n发布日期: \n\nSoftmax，顾名思义是“soft的max”，是$\\max$算子（准确来说是$\\text{argmax}$）的光滑近似，它通过指数归一化将任意向量$\\boldsymbol{x}\\in\\mathbb{R}^n$转化为分量非负且和为1的新向量，...",
    "date": "2024-09-19",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "近似",
      "梯度",
      "光滑",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 144
  },
  {
    "slug": "低秩近似之路一伪逆",
    "title": "低秩近似之路（一）：伪逆",
    "description": "低秩近似之路（一）：伪逆&para;\n原文链接: https://spaces.ac.cn/archives/10366\n发布日期: \n\n可能很多读者跟笔者一样，对矩阵的低秩近似有种熟悉而又陌生的感觉。熟悉是因为，低秩近似的概念和意义都不难理解，加之目前诸如LoRA等基于低秩近似的微调技术遍地开花，让低秩近似的概念在耳濡目染间就已经深入人心；然而，低秩近似所覆盖的内容非常广，在低秩近似相关的论文中...",
    "date": "2024-09-15",
    "source": "",
    "tags": [
      "近似",
      "矩阵",
      "低秩",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 143
  },
  {
    "slug": "闭门造车之多模态思路浅谈三位置编码",
    "title": "“闭门造车”之多模态思路浅谈（三）：位置编码",
    "description": "“闭门造车”之多模态思路浅谈（三）：位置编码&para;\n原文链接: https://spaces.ac.cn/archives/10352\n发布日期: \n\n在前面的文章中，我们曾表达过这样的观点：多模态LLM相比纯文本LLM的主要差异在于，前者甚至还没有形成一个公认为标准的方法论。这里的方法论，不仅包括之前讨论的生成和训练策略，还包括一些基础架构的设计，比如本文要谈的“多模态位置编码”。\n对于这...",
    "date": "2024-09-06",
    "source": "",
    "tags": [
      "attention",
      "位置编码",
      "多模态",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 142
  },
  {
    "slug": "decoder-only的llm为什么需要位置编码",
    "title": "Decoder-only的LLM为什么需要位置编码？",
    "description": "Decoder-only的LLM为什么需要位置编码？&para;\n原文链接: https://spaces.ac.cn/archives/10347\n发布日期: \n\n众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在《为什么现在的LLM都是Decoder-only的架构？》也有过相关讨论），而对于Causal Attention，已经有不...",
    "date": "2024-09-01",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 141
  },
  {
    "slug": "通向最优分布之路概率空间的最小化",
    "title": "通向最优分布之路：概率空间的最小化",
    "description": "通向最优分布之路：概率空间的最小化&para;\n原文链接: https://spaces.ac.cn/archives/10289\n发布日期: \n\n当要求函数的最小值时，我们通常会先求导函数然后寻找其零点，比较幸运的情况下，这些零点之一正好是原函数的最小值点。如果是向量函数，则将导数改为梯度并求其零点。当梯度零点不易求得时，我们可以使用梯度下降来逐渐逼近最小值点。\n以上这些都是无约束优化的基础结果...",
    "date": "2024-08-06",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "优化",
      "梯度",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 140
  },
  {
    "slug": "对齐全量微调这是我看过最精彩的lora改进二",
    "title": "对齐全量微调！这是我看过最精彩的LoRA改进（二）",
    "description": "对齐全量微调！这是我看过最精彩的LoRA改进（二）&para;\n原文链接: https://spaces.ac.cn/archives/10266\n发布日期: \n\n前两周笔者写了《对齐全量微调！这是我看过最精彩的LoRA（一）》（当时还没有编号“一”），里边介绍了一个名为“LoRA-GA”的LoRA变体，它通过梯度SVD来改进LoRA的初始化，从而实现LoRA与全量微调的对齐。当然，从理论上来讲，...",
    "date": "2024-07-29",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 139
  },
  {
    "slug": "monarch矩阵计算高效的稀疏型矩阵分解",
    "title": "Monarch矩阵：计算高效的稀疏型矩阵分解",
    "description": "Monarch矩阵：计算高效的稀疏型矩阵分解&para;\n原文链接: https://spaces.ac.cn/archives/10249\n发布日期: \n\n在矩阵压缩这个问题上，我们通常有两个策略可以选择，分别是低秩化 和稀疏化 。低秩化通过寻找矩阵的低秩近似来减少矩阵尺寸，而稀疏化则是通过减少矩阵中的非零元素来降低矩阵的复杂性。如果说SVD是奔着矩阵的低秩近似去的，那么相应地寻找矩阵稀疏近似的...",
    "date": "2024-07-24",
    "source": "",
    "tags": [
      "矩阵",
      "语言模型",
      "稀疏",
      "低秩",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 138
  },
  {
    "slug": "对齐全量微调这是我看过最精彩的lora改进一",
    "title": "对齐全量微调！这是我看过最精彩的LoRA改进（一）",
    "description": "对齐全量微调！这是我看过最精彩的LoRA改进（一）&para;\n原文链接: https://spaces.ac.cn/archives/10226\n发布日期: \n\n众所周知，LoRA是一种常见的参数高效的微调方法，我们在《梯度视角下的LoRA：简介、分析、猜测及推广》做过简单介绍。LoRA利用低秩分解来降低微调参数量，节省微调显存，同时训练好的权重可以合并到原始权重上，推理架构不需要作出改变，是一...",
    "date": "2024-07-12",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 137
  },
  {
    "slug": "闭门造车之多模态思路浅谈二自回归",
    "title": "“闭门造车”之多模态思路浅谈（二）：自回归",
    "description": "“闭门造车”之多模态思路浅谈（二）：自回归&para;\n原文链接: https://spaces.ac.cn/archives/10197\n发布日期: \n\n这篇文章我们继续来闭门造车，分享一下笔者最近对多模态学习的一些新理解。\n在前文《“闭门造车”之多模态思路浅谈（一）：无损输入》中，我们强调了无损输入对于理想的多模型模态的重要性。如果这个观点成立，那么当前基于VQ-VAE、VQ-GAN等将图像离...",
    "date": "2024-07-08",
    "source": "",
    "tags": [
      "生成模型",
      "扩散",
      "多模态",
      "自回归",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 136
  },
  {
    "slug": "重温ssm四有理生成函数的新视角",
    "title": "重温SSM（四）：有理生成函数的新视角",
    "description": "重温SSM（四）：有理生成函数的新视角&para;\n原文链接: https://spaces.ac.cn/archives/10180\n发布日期: \n\n在前三篇文章中，我们较为详细地讨论了HiPPO和S4的大部分数学细节。那么，对于接下来的第四篇文章，大家预期我们会讨论什么工作呢？S5、Mamba乃至Mamba2？都不是。本系列文章主要关心SSM的数学基础，旨在了解SSM的同时也补充自己的数学能力...",
    "date": "2024-06-27",
    "source": "",
    "tags": [
      "生成函数",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 135
  },
  {
    "slug": "重温ssm三hippo的高效计算s4",
    "title": "重温SSM（三）：HiPPO的高效计算（S4）",
    "description": "重温SSM（三）：HiPPO的高效计算（S4）&para;\n原文链接: https://spaces.ac.cn/archives/10162\n发布日期: \n\n前面我们用两篇文章《重温SSM（一）：线性系统和HiPPO矩阵》和《重温SSM（二）：HiPPO的一些遗留问题》介绍了HiPPO的思想和推导——通过正交函数基对持续更新的函数进行实时逼近，其拟合系数的动力学正好可以表示为一个线性ODE系统，...",
    "date": "2024-06-20",
    "source": "",
    "tags": [
      "矩阵",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 134
  },
  {
    "slug": "重温ssm二hippo的一些遗留问题",
    "title": "重温SSM（二）：HiPPO的一些遗留问题",
    "description": "重温SSM（二）：HiPPO的一些遗留问题&para;\n原文链接: https://spaces.ac.cn/archives/10137\n发布日期: \n\n书接上文，在上一篇文章《重温SSM（一）：线性系统和HiPPO矩阵》中，我们详细讨论了HiPPO逼近框架其HiPPO矩阵的推导，其原理是通过正交函数基来动态地逼近一个实时更新的函数，其投影系数的动力学正好是一个线性系统，而如果以正交多项式为基，...",
    "date": "2024-06-05",
    "source": "",
    "tags": [
      "线性",
      "差分",
      "RNN",
      "梯度",
      "ssm"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 133
  },
  {
    "slug": "transformer升级之路18rope的底数选择原则",
    "title": "Transformer升级之路：18、RoPE的底数选择原则",
    "description": "Transformer升级之路：18、RoPE的底数选择原则&para;\n原文链接: https://spaces.ac.cn/archives/10122\n发布日期: \n\n我们知道，在RoPE中频率的计算公式为$\\theta_i = b^{-2i/d}$，底数$b$默认值为10000。目前Long Context的主流做法之一是，先在$b=10000$上用短文本预训练，然后调大$b$并在长文本微...",
    "date": "2024-05-29",
    "source": "",
    "tags": [
      "详细推导",
      "不等式",
      "attention",
      "位置编码",
      "rope",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 132
  },
  {
    "slug": "重温ssm一线性系统和hippo矩阵",
    "title": "重温SSM（一）：线性系统和HiPPO矩阵",
    "description": "重温SSM（一）：线性系统和HiPPO矩阵&para;\n原文链接: https://spaces.ac.cn/archives/10114\n发布日期: \n\n前几天，笔者看了几篇介绍SSM（State Space Model）的文章，才发现原来自己从未认真了解过SSM，于是打算认真去学习一下SSM的相关内容，顺便开了这个新坑，记录一下学习所得。\nSSM的概念由来已久，但这里我们特指深度学习中的SSM...",
    "date": "2024-05-24",
    "source": "",
    "tags": [
      "微分方程",
      "线性",
      "RNN",
      "ssm",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 131
  },
  {
    "slug": "缓存与效果的极限拉扯从mhamqagqa到mla",
    "title": "缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA",
    "description": "缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA&para;\n原文链接: https://spaces.ac.cn/archives/10091\n发布日期: \n\n前几天，幻方发布的DeepSeek-V2引起了大家的热烈讨论。首先，最让人哗然的是1块钱100万token的价格，普遍比现有的各种竞品API便宜了两个数量级，以至于有人调侃“这个价格哪怕它输出乱码，我也会认为这个乱码是一种艺术”；其...",
    "date": "2024-05-13",
    "source": "",
    "tags": [
      "优化",
      "语言模型",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 130
  },
  {
    "slug": "生成扩散模型漫谈二十五基于恒等式的蒸馏上",
    "title": "生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）",
    "description": "生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）&para;\n原文链接: https://spaces.ac.cn/archives/10085\n发布日期: \n\n今天我们分享一下论文《Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Ge...",
    "date": "2024-05-01",
    "source": "",
    "tags": [
      "生成模型",
      "梯度",
      "扩散",
      "去噪",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 129
  },
  {
    "slug": "生成扩散模型漫谈二十四少走捷径更快到达",
    "title": "生成扩散模型漫谈（二十四）：少走捷径，更快到达",
    "description": "生成扩散模型漫谈（二十四）：少走捷径，更快到达&para;\n原文链接: https://spaces.ac.cn/archives/10077\n发布日期: \n\n如何减少采样步数同时保证生成质量，是扩散模型应用层面的一个关键问题。其中，《生成扩散模型漫谈（四）：DDIM = 高观点DDPM》介绍的DDIM可谓是加速采样的第一次尝试。后来，《生成扩散模型漫谈（五）：一般框架之SDE篇》、《生成扩散模型...",
    "date": "2024-04-23",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 128
  },
  {
    "slug": "生成扩散模型漫谈二十三信噪比与大图生成下",
    "title": "生成扩散模型漫谈（二十三）：信噪比与大图生成（下）",
    "description": "生成扩散模型漫谈（二十三）：信噪比与大图生成（下）&para;\n原文链接: https://spaces.ac.cn/archives/10055\n发布日期: \n\n上一篇文章《生成扩散模型漫谈（二十二）：信噪比与大图生成（上）》中，我们介绍了通过对齐低分辨率的信噪比来改进noise schedule，从而改善直接在像素空间训练的高分辨率图像生成（大图生成）的扩散模型效果。而这篇文章的主角同样是信噪...",
    "date": "2024-04-17",
    "source": "",
    "tags": [
      "无监督",
      "生成模型",
      "扩散",
      "信噪比",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 127
  },
  {
    "slug": "生成扩散模型漫谈二十二信噪比与大图生成上",
    "title": "生成扩散模型漫谈（二十二）：信噪比与大图生成（上）",
    "description": "生成扩散模型漫谈（二十二）：信噪比与大图生成（上）&para;\n原文链接: https://spaces.ac.cn/archives/10047\n发布日期: \n\n盘点主流的图像扩散模型作品，我们会发现一个特点：当前多数做高分辨率图像生成（下面简称“大图生成”）的工作，都是先通过Encoder变换到Latent空间进行的（即LDM，Latent Diffusion Model），直接在原始Pixe...",
    "date": "2024-04-08",
    "source": "",
    "tags": [
      "详细推导",
      "损失函数",
      "生成模型",
      "扩散",
      "信噪比",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 126
  },
  {
    "slug": "transformer升级之路17多模态位置编码的简单思考",
    "title": "Transformer升级之路：17、多模态位置编码的简单思考",
    "description": "Transformer升级之路：17、多模态位置编码的简单思考&para;\n原文链接: https://spaces.ac.cn/archives/10040\n发布日期: \n\n在这个系列的第二篇文章《Transformer升级之路：2、博采众长的旋转式位置编码》中，笔者提出了旋转位置编码（RoPE）——通过绝对位置的形式实现相对位置编码的方案。一开始RoPE是针对一维序列如文本、音频等设计的（Ro...",
    "date": "2024-03-29",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "rope",
      "多模态",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 125
  },
  {
    "slug": "时空之章将attention视为平方复杂度的rnn",
    "title": "时空之章：将Attention视为平方复杂度的RNN",
    "description": "时空之章：将Attention视为平方复杂度的RNN&para;\n原文链接: https://spaces.ac.cn/archives/10017\n发布日期: \n\n近年来，RNN由于其线性的训练和推理效率，重新吸引了不少研究人员和用户的兴趣，隐约有“文艺复兴”之势，其代表作有RWKV、RetNet、Mamba等。当将RNN用于语言模型时，其典型特点就是每步生成都是常数的空间复杂度和时间复杂度，从...",
    "date": "2024-03-18",
    "source": "",
    "tags": [
      "语言模型",
      "RNN",
      "attention",
      "复杂度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 124
  },
  {
    "slug": "用傅里叶级数拟合一维概率密度函数",
    "title": "用傅里叶级数拟合一维概率密度函数",
    "description": "用傅里叶级数拟合一维概率密度函数&para;\n原文链接: https://spaces.ac.cn/archives/10007\n发布日期: \n\n在《“闭门造车”之多模态思路浅谈（一）：无损输入》中我们曾提到，图像生成的本质困难是没有一个连续型概率密度的万能拟合器。当然，也不能说完全没有，比如高斯混合模型（GMM）理论上就是可以拟合任意概率密度，就连GAN本质上也可以理解为混合了无限个高斯模型的G...",
    "date": "2024-03-07",
    "source": "",
    "tags": [
      "级数",
      "概率",
      "分析",
      "逼近",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 123
  },
  {
    "slug": "配置不同的学习率lora还能再涨一点",
    "title": "配置不同的学习率，LoRA还能再涨一点？",
    "description": "配置不同的学习率，LoRA还能再涨一点？&para;\n原文链接: https://spaces.ac.cn/archives/10001\n发布日期: \n\nLoRA（Low-Rank Adaptation）是当前LLM的参数高效微调手段之一，此前我们在《梯度视角下的LoRA：简介、分析、猜测及推广》也有过简单讨论。这篇文章我们来学习LoRA的一个新结论：\n\n给LoRA的两个矩阵分配不同的学习率，Lo...",
    "date": "2024-02-27",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 122
  },
  {
    "slug": "闭门造车之多模态思路浅谈一无损输入",
    "title": "“闭门造车”之多模态思路浅谈（一）：无损输入",
    "description": "“闭门造车”之多模态思路浅谈（一）：无损输入&para;\n原文链接: https://spaces.ac.cn/archives/9984\n发布日期: \n\n这篇文章分享一下笔者关于多模态模型架构的一些闭门造车的想法，或者说一些猜测。\n最近Google的Gemini 1.5和OpenAI的Sora再次点燃了不少人对多模态的热情，只言片语的技术报告也引起了大家对其背后模型架构的热烈猜测。不过，本文并非...",
    "date": "2024-02-21",
    "source": "",
    "tags": [
      "VAE",
      "GAN",
      "Flow",
      "Diffusion",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 121
  },
  {
    "slug": "幂等生成网络ign试图将判别和生成合二为一的gan",
    "title": "幂等生成网络IGN：试图将判别和生成合二为一的GAN",
    "description": "幂等生成网络IGN：试图将判别和生成合二为一的GAN&para;\n原文链接: https://spaces.ac.cn/archives/9969\n发布日期: \n\n前段时间，一个名为“幂等生成网络（Idempotent Generative Network，IGN）”的生成模型引起了一定的关注。它自称是一种独立于已有的VAE、GAN、flow、Diffusion之外的新型生成模型，并且具有单步采样...",
    "date": "2024-01-31",
    "source": "",
    "tags": [
      "GAN",
      "GAN",
      "生成模型",
      "对抗",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 120
  },
  {
    "slug": "transformer升级之路16复盘长度外推技术",
    "title": "Transformer升级之路：16、“复盘”长度外推技术",
    "description": "Transformer升级之路：16、“复盘”长度外推技术&para;\n原文链接: https://spaces.ac.cn/archives/9948\n发布日期: \n\n回过头来看，才发现从第7篇《Transformer升级之路：7、长度外推性与局部注意力》开始，“Transformer升级之路”这个系列就跟长度外推“杠”上了，接连9篇文章（不算本文）都是围绕长度外推展开的。如今，距离第7篇文章刚...",
    "date": "2024-01-26",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 119
  },
  {
    "slug": "局部余弦相似度大全局余弦相似度一定也大吗",
    "title": "局部余弦相似度大，全局余弦相似度一定也大吗？",
    "description": "局部余弦相似度大，全局余弦相似度一定也大吗？&para;\n原文链接: https://spaces.ac.cn/archives/9931\n发布日期: \n\n在分析模型的参数时，有些情况下我们会将模型的所有参数当成一个整体的向量，有些情况下我们则会将不同的参数拆开来看。比如，一个7B大小的LLAMA模型所拥有的70亿参数量，有时候我们会将它当成“一个70亿维的向量”，有时候我们会按照模型的实现方式将...",
    "date": "2024-01-09",
    "source": "",
    "tags": [
      "不等式",
      "相似度",
      "悖论",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 118
  },
  {
    "slug": "让炼丹更科学一些一sgd的平均损失收敛",
    "title": "让炼丹更科学一些（一）：SGD的平均损失收敛",
    "description": "让炼丹更科学一些（一）：SGD的平均损失收敛&para;\n原文链接: https://spaces.ac.cn/archives/9902\n发布日期: \n\n很多时候我们将深度学习模型的训练过程戏称为“炼丹”，因为整个过程跟古代的炼丹术一样，看上去有一定的科学依据，但整体却给人一种“玄之又玄”的感觉。尽管本站之前也关注过一些优化器相关的工作，甚至也写过《从动力学角度看优化算法》系列，但都是比较表面的...",
    "date": "2023-12-19",
    "source": "",
    "tags": [
      "优化器",
      "不等式",
      "优化器",
      "sgd",
      "炼丹"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 117
  },
  {
    "slug": "注意力机制真的可以集中注意力吗",
    "title": "注意力机制真的可以“集中注意力”吗？",
    "description": "注意力机制真的可以“集中注意力”吗？&para;\n原文链接: https://spaces.ac.cn/archives/9889\n发布日期: \n\n之前在《Transformer升级之路：3、从Performer到线性Attention》、《为什么现在的LLM都是Decoder-only的架构？》等文章中，我们从Attention矩阵的“秩”的角度探讨了Attention机制，并曾经判断线性Att...",
    "date": "2023-12-12",
    "source": "",
    "tags": [
      "熵",
      "稀疏",
      "attention",
      "秩",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 116
  },
  {
    "slug": "通向概率分布之路盘点softmax及其替代品",
    "title": "通向概率分布之路：盘点Softmax及其替代品",
    "description": "通向概率分布之路：盘点Softmax及其替代品&para;\n原文链接: https://spaces.ac.cn/archives/10145\n发布日期: \n\n不论是在基础的分类任务中，还是如今无处不在的注意力机制中，概率分布的构建都是一个关键步骤。具体来说，就是将一个$n$维的任意向量，转换为一个$n$元的离散型概率分布。众所周知，这个问题的标准答案是Softmax，它是指数归一化的形式，相对来...",
    "date": "2023-12-11",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "分析",
      "损失函数",
      "梯度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 115
  },
  {
    "slug": "生成扩散模型漫谈二十一中值定理加速ode采样",
    "title": "生成扩散模型漫谈（二十一）：中值定理加速ODE采样",
    "description": "生成扩散模型漫谈（二十一）：中值定理加速ODE采样&para;\n原文链接: https://spaces.ac.cn/archives/9881\n发布日期: \n\n在生成扩散模型的发展史上，DDIM和同期Song Yang的扩散SDE都称得上是里程碑式的工作，因为它们建立起了扩散模型与随机微分方程（SDE）、常微分方程（ODE）这两个数学领域的紧密联系，从而允许我们可以利用SDE、ODE已有的各种数...",
    "date": "2023-12-07",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 114
  },
  {
    "slug": "我在performer中发现了transformer-vq的踪迹",
    "title": "我在Performer中发现了Transformer-VQ的踪迹",
    "description": "我在Performer中发现了Transformer-VQ的踪迹&para;\n原文链接: https://spaces.ac.cn/archives/9862\n发布日期: \n\n前些天我们在《VQ一下Key，Transformer的复杂度就变成线性了》介绍了“Transformer-VQ”，这是通过将Key序列做VQ（Vector Quantize）变换来实现Attention复杂度线性化的方案。诚...",
    "date": "2023-11-29",
    "source": "",
    "tags": [
      "量子化",
      "语言模型",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 113
  },
  {
    "slug": "transformer升级之路15key归一化助力长度外推",
    "title": "Transformer升级之路：15、Key归一化助力长度外推",
    "description": "Transformer升级之路：15、Key归一化助力长度外推&para;\n原文链接: https://spaces.ac.cn/archives/9859\n发布日期: \n\n大体上，我们可以将目前Transformer的长度外推技术分为两类：一类是事后修改，比如NTK-RoPE、YaRN、ReRoPE等，这类方法的特点是直接修改推理模型，无需微调就能达到一定的长度外推效果，但缺点是它们都无法保持模...",
    "date": "2023-11-20",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 112
  },
  {
    "slug": "vq一下keytransformer的复杂度就变成线性了",
    "title": "VQ一下Key，Transformer的复杂度就变成线性了",
    "description": "VQ一下Key，Transformer的复杂度就变成线性了&para;\n原文链接: https://spaces.ac.cn/archives/9844\n发布日期: \n\nEfficient Transformer，泛指一切致力于降低Transformer的二次复杂度的工作，开始特指针对Attention的改进，后来更一般的思路，如傅里叶变换、线性RNN等，也被归入这个范畴。不得不说，为了降低Tra...",
    "date": "2023-11-09",
    "source": "",
    "tags": [
      "详细推导",
      "量子化",
      "编码",
      "梯度",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 111
  },
  {
    "slug": "简单得令人尴尬的fsq四舍五入超越了vq-vae",
    "title": "简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE",
    "description": "简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE&para;\n原文链接: https://spaces.ac.cn/archives/9826\n发布日期: \n\n正如“XXX is all you need”一样，有不少论文都以“简单得令人尴尬”命名（An Embarrassingly Simple XXX），但在笔者看来，这些论文大多数都是噱头多于实力。不过，笔者最近阅读到的一篇论文，真的让...",
    "date": "2023-10-31",
    "source": "",
    "tags": [
      "详细推导",
      "生成模型",
      "编码",
      "梯度",
      "离散化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 110
  },
  {
    "slug": "从梯度最大化看attention的scale操作",
    "title": "从梯度最大化看Attention的Scale操作",
    "description": "从梯度最大化看Attention的Scale操作&para;\n原文链接: https://spaces.ac.cn/archives/9812\n发布日期: \n\n我们知道，Scaled Dot-Product Attention的Scale因子是$\\frac{1}{\\sqrt{d}}$，其中$d$是$\\boldsymbol{q},\\boldsymbol{k}$的维度。这个Scale因子的一般解释是：...",
    "date": "2023-10-22",
    "source": "",
    "tags": [
      "优化",
      "梯度",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 109
  },
  {
    "slug": "随机分词再探从viterbi-sampling到完美采样算法",
    "title": "随机分词再探：从Viterbi Sampling到完美采样算法",
    "description": "随机分词再探：从Viterbi Sampling到完美采样算法&para;\n原文链接: https://spaces.ac.cn/archives/9811\n发布日期: \n\n在文章《随机分词浅探：从Viterbi Decoding到Viterbi Sampling》中，笔者提出了一种名为“Viterbi Sampling”的随机分词算法，它只是在求最优解的Viterbi Decoding基础上进行...",
    "date": "2023-10-16",
    "source": "",
    "tags": [
      "概率",
      "随机",
      "优化",
      "分词",
      "采样"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 108
  },
  {
    "slug": "emo基于最优传输思想设计的分类损失函数",
    "title": "EMO：基于最优传输思想设计的分类损失函数",
    "description": "EMO：基于最优传输思想设计的分类损失函数&para;\n原文链接: https://spaces.ac.cn/archives/9797\n发布日期: \n\n众所周知，分类任务的标准损失是交叉熵（Cross Entropy，等价于最大似然MLE，即Maximum Likelihood Estimation），它有着简单高效的特点，但在某些场景下也暴露出一些问题，如偏离评价指标、过度自信等，相应的改进工...",
    "date": "2023-10-13",
    "source": "",
    "tags": [
      "概率",
      "优化",
      "损失函数",
      "最优传输",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 107
  },
  {
    "slug": "预训练一下transformer的长序列成绩还能涨不少",
    "title": "预训练一下，Transformer的长序列成绩还能涨不少！",
    "description": "预训练一下，Transformer的长序列成绩还能涨不少！&para;\n原文链接: https://spaces.ac.cn/archives/9787\n发布日期: \n\n作为LLM的主流模型架构，Transformer在各类任务上的总体表现都出色，大多数情况下，Transformer的槽点只是它的平方复杂度，而不是效果——除了一个名为Long Range Arena（下面简称LRA）的Benchm...",
    "date": "2023-10-08",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 106
  },
  {
    "slug": "脑洞大开非线性rnn居然也可以并行计算",
    "title": "脑洞大开：非线性RNN居然也可以并行计算？",
    "description": "脑洞大开：非线性RNN居然也可以并行计算？&para;\n原文链接: https://spaces.ac.cn/archives/9783\n发布日期: \n\n近年来，线性RNN由于其可并行训练以及常数推理成本等特性，吸引了一定研究人员的关注（例如笔者之前写的《Google新作试图“复活”RNN：RNN能否再次辉煌？》），这让RNN在Transformer遍地开花的潮流中仍有“一席之地”。然而，目前看来...",
    "date": "2023-09-26",
    "source": "",
    "tags": [
      "摄动",
      "方程",
      "迭代",
      "语言模型",
      "RNN"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 105
  },
  {
    "slug": "自然数集中-n-ab-c-时-a-b-c-的最小值",
    "title": "自然数集中 N = ab + c 时 a + b + c 的最小值",
    "description": "自然数集中 N = ab + c 时 a + b + c 的最小值&para;\n原文链接: https://spaces.ac.cn/archives/9775\n发布日期: \n\n前天晚上微信群里有群友提出了一个问题：\n\n对于一个任意整数$N &gt; 100$，求一个近似算法，使得$N=a\\times b+c$（其中$a,b,c$都是非负整数），并且令$a+b+c$尽量地小。\n\n初看这道题，笔者第...",
    "date": "2023-09-20",
    "source": "",
    "tags": [
      "最优",
      "问题",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 104
  },
  {
    "slug": "随机分词浅探从viterbi-decoding到viterbi-sampling",
    "title": "随机分词浅探：从Viterbi Decoding到Viterbi Sampling",
    "description": "随机分词浅探：从Viterbi Decoding到Viterbi Sampling&para;\n原文链接: https://spaces.ac.cn/archives/9768\n发布日期: \n\n上一篇文章《大词表语言模型在续写任务上的一个问题及对策》发布后，很快就有读者指出可以在训练阶段引入带有随机性的分词结果来解决同样的问题，并且已经有论文和实现。经过进一步查阅学习，笔者发现这是一个名为Subw...",
    "date": "2023-09-16",
    "source": "",
    "tags": [
      "概率",
      "随机",
      "分词",
      "新词发现",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 103
  },
  {
    "slug": "大词表语言模型在续写任务上的一个问题及对策",
    "title": "大词表语言模型在续写任务上的一个问题及对策",
    "description": "大词表语言模型在续写任务上的一个问题及对策&para;\n原文链接: https://spaces.ac.cn/archives/9762\n发布日期: \n\n对于LLM来说，通过增大Tokenizer的词表来提高压缩率，从而缩短序列长度、降低解码成本，是大家都喜闻乐见的事情。毕竟增大词表只需要增大Embedding层和输出的Dense层，这部分增加的计算量几乎不可感知，但缩短序列长度之后带来的解码速度...",
    "date": "2023-09-13",
    "source": "",
    "tags": [
      "概率",
      "问题",
      "语言模型",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 102
  },
  {
    "slug": "bytepiece更纯粹更高压缩率的tokenizer",
    "title": "BytePiece：更纯粹、更高压缩率的Tokenizer",
    "description": "BytePiece：更纯粹、更高压缩率的Tokenizer&para;\n原文链接: https://spaces.ac.cn/archives/9752\n发布日期: \n\n目前在LLM中最流行的Tokenizer（分词器）应该是Google的SentencePiece了，因为它符合Tokenizer的一些理想特性，比如语言无关、数据驱动等，并且由于它是C++写的，所以Tokenize（分词）的速度很...",
    "date": "2023-09-07",
    "source": "",
    "tags": [
      "最小熵",
      "分词",
      "无监督",
      "新词发现",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 101
  },
  {
    "slug": "liontiger优化器训练下的embedding异常和对策",
    "title": "Lion/Tiger优化器训练下的Embedding异常和对策",
    "description": "Lion/Tiger优化器训练下的Embedding异常和对策&para;\n原文链接: https://spaces.ac.cn/archives/9736\n发布日期: \n\n打从在《Tiger：一个“抠”到极致的优化器》提出了Tiger优化器之后，Tiger就一直成为了我训练模型的“标配”优化器。最近笔者已经尝试将Tiger用到了70亿参数模型的预训练之中，前期效果看上来尚可，初步说明Tiger也...",
    "date": "2023-08-28",
    "source": "",
    "tags": [
      "问题",
      "梯度",
      "优化器",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 100
  },
  {
    "slug": "transformer升级之路14当hwfa遇见rerope",
    "title": "Transformer升级之路：14、当HWFA遇见ReRoPE",
    "description": "Transformer升级之路：14、当HWFA遇见ReRoPE&para;\n原文链接: https://spaces.ac.cn/archives/9731\n发布日期: \n\n在上一篇文章《Transformer升级之路：13、逆用Leaky ReRoPE》中，笔者尝试通过在训练阶段逆用Leaky ReRoPE的思路，使得推理阶段的位置编码变为正常的RoPE，从而在达到长度外推的同时解决ReRoP...",
    "date": "2023-08-24",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "外推",
      "rope",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 99
  },
  {
    "slug": "transformer升级之路13逆用leaky-rerope",
    "title": "Transformer升级之路：13、逆用Leaky ReRoPE",
    "description": "Transformer升级之路：13、逆用Leaky ReRoPE&para;\n原文链接: https://spaces.ac.cn/archives/9728\n发布日期: \n\n上周在《Transformer升级之路：12、无限外推的ReRoPE？》中，笔者提出了ReRoPE和Leaky ReRoPE，诸多实验结果表明，它们能够在几乎不损失训练效果的情况下免微调地扩展LLM的Context长度，并...",
    "date": "2023-08-14",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 98
  },
  {
    "slug": "transformer升级之路12无限外推的rerope",
    "title": "Transformer升级之路：12、无限外推的ReRoPE？",
    "description": "Transformer升级之路：12、无限外推的ReRoPE？&para;\n原文链接: https://spaces.ac.cn/archives/9708\n发布日期: \n\n自从在《Transformer升级之路：11、将β进制位置进行到底》中引入混合进制的思路进一步推广了NTK-aware Scaled RoPE后，笔者感觉类似思路的效果已经达到了上限，想要更大幅度的提升就必须另辟蹊径了。这时候...",
    "date": "2023-08-07",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 97
  },
  {
    "slug": "transformer升级之路11将β进制位置进行到底",
    "title": "Transformer升级之路：11、将β进制位置进行到底",
    "description": "Transformer升级之路：11、将β进制位置进行到底&para;\n原文链接: https://spaces.ac.cn/archives/9706\n发布日期: \n\n在文章《Transformer升级之路：10、RoPE是一种β进制编码》中，我们给出了RoPE的$\\beta$进制诠释，并基于进制转化的思路推导了能够在不微调的情况下就可以扩展Context长度的NTK-aware Scaled...",
    "date": "2023-07-31",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 96
  },
  {
    "slug": "语言模型输出端共享embedding的重新探索",
    "title": "语言模型输出端共享Embedding的重新探索",
    "description": "语言模型输出端共享Embedding的重新探索&para;\n原文链接: https://spaces.ac.cn/archives/9698\n发布日期: \n\n预训练刚兴起时，在语言模型的输出端重用Embedding权重是很常见的操作，比如BERT、第一版的T5、早期的GPT，都使用了这个操作，这是因为当模型主干部分不大且词表很大时，Embedding层的参数量很可观，如果输出端再新增一个独立的同样...",
    "date": "2023-07-20",
    "source": "",
    "tags": [
      "语言模型",
      "初始化",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 95
  },
  {
    "slug": "当生成模型肆虐互联网将有疯牛病之忧",
    "title": "当生成模型肆虐：互联网将有“疯牛病”之忧？",
    "description": "当生成模型肆虐：互联网将有“疯牛病”之忧？&para;\n原文链接: https://spaces.ac.cn/archives/9687\n发布日期: \n\n众所周知，不管是文本还是视觉领域，各种生成模型正在以无法阻挡的势头“肆虐”互联网。虽然大家都明白，实现真正的通用人工智能（AGI）还有很长的路要走，但这并不妨碍人们越来越频繁地利用生成模型来创作和分享内容。君不见，很多网络文章已经配上了Stabl...",
    "date": "2023-07-14",
    "source": "",
    "tags": [
      "生成模型",
      "生成模型",
      "attention",
      "优化",
      "语言模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 94
  },
  {
    "slug": "transformer升级之路10rope是一种β进制编码",
    "title": "Transformer升级之路：10、RoPE是一种β进制编码",
    "description": "Transformer升级之路：10、RoPE是一种β进制编码&para;\n原文链接: https://spaces.ac.cn/archives/9675\n发布日期: \n\n对关心如何扩展LLM的Context长度的读者来说，上周无疑是激动人心的一周，开源社区接连不断地出现令人振奋的成果。首先，网友@kaiokendev在他的项目SuperHOT中实验了“位置线性内插”的方案，显示通过非常少的长文...",
    "date": "2023-07-06",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "位置编码",
      "泛化",
      "外推",
      "rope"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 93
  },
  {
    "slug": "生成扩散模型漫谈二十从reflow到wgan-gp",
    "title": "生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP",
    "description": "生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP&para;\n原文链接: https://spaces.ac.cn/archives/9668\n发布日期: \n\n上一篇文章《生成扩散模型漫谈（十九）：作为扩散ODE的GAN》中，我们介绍了如何将GAN理解为在另一个时间维度上的扩散ODE，简而言之，GAN实际上就是将扩散模型中样本的运动转化为生成器参数的运动！然而，该文章的推导过程依赖于Was...",
    "date": "2023-06-28",
    "source": "",
    "tags": [
      "优化",
      "GAN",
      "梯度",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 92
  },
  {
    "slug": "生成扩散模型漫谈十九作为扩散ode的gan",
    "title": "生成扩散模型漫谈（十九）：作为扩散ODE的GAN",
    "description": "生成扩散模型漫谈（十九）：作为扩散ODE的GAN&para;\n原文链接: https://spaces.ac.cn/archives/9662\n发布日期: \n\n在文章《生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配》中，我们推导了Wasserstein距离与扩散模型得分匹配损失之间的一个不等式，表明扩散模型的优化目标与WGAN的优化目标在某种程度上具有相似性。而在本文，我们将探讨《MonoFlow...",
    "date": "2023-06-24",
    "source": "",
    "tags": [
      "优化",
      "GAN",
      "扩散",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 91
  },
  {
    "slug": "梯度流探索通向最小值之路",
    "title": "梯度流：探索通向最小值之路",
    "description": "梯度流：探索通向最小值之路&para;\n原文链接: https://spaces.ac.cn/archives/9660\n发布日期: \n\n在这篇文章中，我们将探讨一个被称为“梯度流（Gradient Flow）”的概念。简单来说，梯度流是将我们在用梯度下降法中寻找最小值的过程中的各个点连接起来，形成一条随（虚拟的）时间变化的轨迹，这条轨迹便被称作“梯度流”。在文章的后半部分，我们将重点讨论如何将梯...",
    "date": "2023-06-16",
    "source": "",
    "tags": [
      "泛函",
      "动力学",
      "优化",
      "梯度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 90
  },
  {
    "slug": "naive-bayes-is-all-you-need",
    "title": "Naive Bayes is all you need ?",
    "description": "Naive Bayes is all you need ?&para;\n原文链接: https://spaces.ac.cn/archives/9648\n发布日期: \n\n很抱歉，起了这么个具有标题党特征的题目。在写完《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》之后，笔者就觉得朴素贝叶斯（Naive Bayes）跟Attention机制有很多相同的特征，后来再推导了一下发现， At...",
    "date": "2023-06-08",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 89
  },
  {
    "slug": "关于nbce方法的一些补充说明和分析",
    "title": "关于NBCE方法的一些补充说明和分析",
    "description": "关于NBCE方法的一些补充说明和分析&para;\n原文链接: https://spaces.ac.cn/archives/9632\n发布日期: \n\n上周在《NBCE：使用朴素贝叶斯扩展LLM的Context处理长度》中，我们介绍了一种基于朴素贝叶斯来扩展LLM的Context长度的方案NBCE（Naive Bayes-based Context Extension）。由于它有着即插即用、模型无关、...",
    "date": "2023-05-31",
    "source": "",
    "tags": [
      "语言模型",
      "外推",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 88
  },
  {
    "slug": "nbce使用朴素贝叶斯扩展llm的context处理长度",
    "title": "NBCE：使用朴素贝叶斯扩展LLM的Context处理长度",
    "description": "NBCE：使用朴素贝叶斯扩展LLM的Context处理长度&para;\n原文链接: https://spaces.ac.cn/archives/9617\n发布日期: \n\n\n在LLM时代还玩朴素贝叶斯（Naive Bayes）？\n\n这可能是许多读者在看到标题后的首个想法。确实如此，当古老的朴素贝叶斯与前沿的LLM相遇时，产生了令人惊讶的效果——我们可以直接扩展现有LLM模型的Context处理长度，...",
    "date": "2023-05-23",
    "source": "",
    "tags": [
      "语言模型",
      "外推",
      "LLM",
      "贝叶斯",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 87
  },
  {
    "slug": "基于量子化假设推导模型的尺度定律scaling-law",
    "title": "基于量子化假设推导模型的尺度定律（Scaling Law）",
    "description": "基于量子化假设推导模型的尺度定律（Scaling Law）&para;\n原文链接: https://spaces.ac.cn/archives/9607\n发布日期: \n\n尺度定律（Scaling Law），指的是模型能力与模型尺度之间的渐近关系。具体来说，模型能力我们可以简单理解为模型的损失函数，模型尺度可以指模型参数量、训练数据量、训练步数等，所谓尺度定律，就是研究损失函数跟参数量、数据量、训练...",
    "date": "2023-05-18",
    "source": "",
    "tags": [
      "模型",
      "分析",
      "量子",
      "尺度定律",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 86
  },
  {
    "slug": "transformer升级之路9一种全局长度外推的新思路",
    "title": "Transformer升级之路：9、一种全局长度外推的新思路",
    "description": "Transformer升级之路：9、一种全局长度外推的新思路&para;\n原文链接: https://spaces.ac.cn/archives/9603\n发布日期: \n\n说到Transformer无法处理超长序列的原因，大家的第一反应通常都是Self Attention的二次复杂度。但事实上，即便忽略算力限制，常规的Transformer也无法处理超长序列，因为它们的长度外推性（Length E...",
    "date": "2023-05-12",
    "source": "",
    "tags": [
      "详细推导",
      "attention",
      "泛化",
      "外推",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 85
  },
  {
    "slug": "如何度量数据的稀疏程度",
    "title": "如何度量数据的稀疏程度？",
    "description": "如何度量数据的稀疏程度？&para;\n原文链接: https://spaces.ac.cn/archives/9595\n发布日期: \n\n在机器学习中，我们经常会谈到稀疏性，比如我们经常说注意力矩阵通常是很稀疏的。然而，不知道大家发现没有，我们似乎从没有给出过度量稀疏程度的标准方法。也就是说，以往我们关于稀疏性的讨论，仅仅是直观层面的感觉，并没有过定量分析。那么问题来了，稀疏性的度量有标准方法了吗？...",
    "date": "2023-05-05",
    "source": "",
    "tags": [
      "概率",
      "熵",
      "度量",
      "稀疏",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 84
  },
  {
    "slug": "注意力和softmax的两点有趣发现鲁棒性和信息量",
    "title": "注意力和Softmax的两点有趣发现：鲁棒性和信息量",
    "description": "注意力和Softmax的两点有趣发现：鲁棒性和信息量&para;\n原文链接: https://spaces.ac.cn/archives/9593\n发布日期: \n\n最近几周笔者一直都在思考注意力机制的相关性质，在这个过程中对注意力及Softmax有了更深刻的理解。在这篇文章中，笔者简单分享其中的两点：\n\n1、Softmax注意力天然能够抵御一定的噪声扰动；\n2、从信息熵角度也可以对初始化问题形成直...",
    "date": "2023-04-25",
    "source": "",
    "tags": [
      "信息",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 83
  },
  {
    "slug": "梯度视角下的lora简介分析猜测及推广",
    "title": "梯度视角下的LoRA：简介、分析、猜测及推广",
    "description": "梯度视角下的LoRA：简介、分析、猜测及推广&para;\n原文链接: https://spaces.ac.cn/archives/9590\n发布日期: \n\n随着ChatGPT及其平替的火热，各种参数高效（Parameter-Efficient）的微调方法也“水涨船高”，其中最流行的方案之一就是本文的主角LoRA 了，它出自论文《LoRA: Low-Rank Adaptation of Large...",
    "date": "2023-04-17",
    "source": "",
    "tags": [
      "详细推导",
      "梯度",
      "优化器",
      "低秩",
      "lora",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 82
  },
  {
    "slug": "从jl引理看熵不变性attention",
    "title": "从JL引理看熵不变性Attention",
    "description": "从JL引理看熵不变性Attention&para;\n原文链接: https://spaces.ac.cn/archives/9588\n发布日期: \n\n在《从熵不变性看Attention的Scale操作》、《熵不变性Softmax的一个快速推导》中笔者提出了熵不变性Softmax，简单来说就是往Softmax之前的Attention矩阵多乘上一个$\\log n$，理论上有助于增强长度外推性，其中$n...",
    "date": "2023-04-10",
    "source": "",
    "tags": [
      "熵",
      "attention",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 81
  },
  {
    "slug": "bias项的神奇作用rope-bias-更好的长度外推性",
    "title": "Bias项的神奇作用：RoPE + Bias = 更好的长度外推性",
    "description": "Bias项的神奇作用：RoPE + Bias = 更好的长度外推性&para;\n原文链接: https://spaces.ac.cn/archives/9577\n发布日期: \n\n万万没想到，Bias项能跟Transformer的长度外推性联系在一起！\n长度外推性是我们希望Transformer具有的一个理想性质，笔者曾在《Transformer升级之路：7、长度外推性与局部注意力》、《Transf...",
    "date": "2023-04-03",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "rope"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 80
  },
  {
    "slug": "google新作试图复活rnnrnn能否再次辉煌",
    "title": "Google新作试图“复活”RNN：RNN能否再次辉煌？",
    "description": "Google新作试图“复活”RNN：RNN能否再次辉煌？&para;\n原文链接: https://spaces.ac.cn/archives/9554\n发布日期: \n\n当前，像ChatGPT之类的LLM可谓是“风靡全球”。有读者留意到，几乎所有LLM都还是用最初的Multi-Head Scaled-Dot Attention，近年来大量的Efficient工作如线性Attention、FLASH等...",
    "date": "2023-03-28",
    "source": "",
    "tags": [
      "语言模型",
      "RNN",
      "生成模型",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 79
  },
  {
    "slug": "为什么现在的llm都是decoder-only的架构faq",
    "title": "《为什么现在的LLM都是Decoder-only的架构？》FAQ",
    "description": "《为什么现在的LLM都是Decoder-only的架构？》FAQ&para;\n原文链接: https://spaces.ac.cn/archives/9547\n发布日期: \n\n上周笔者写了《为什么现在的LLM都是Decoder-only的架构？》，总结了一下我在这个问题上的一些实验结论和猜测。果然是热点问题流量大，paperweekly的转发没多久阅读量就破万了，知乎上点赞数也不少。在几个平台上，...",
    "date": "2023-03-20",
    "source": "",
    "tags": [
      "问答",
      "语言模型",
      "文本生成",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 78
  },
  {
    "slug": "为什么现在的llm都是decoder-only的架构",
    "title": "为什么现在的LLM都是Decoder-only的架构？",
    "description": "为什么现在的LLM都是Decoder-only的架构？&para;\n原文链接: https://spaces.ac.cn/archives/9529\n发布日期: \n\nLLM是“Large Language Model”的简写，目前一般指百亿参数以上的语言模型，主要面向文本生成 任务。跟小尺度模型（10亿或以内量级）的“百花齐放”不同，目前LLM的一个现状是Decoder-only架构的研究居多，像...",
    "date": "2023-03-17",
    "source": "",
    "tags": [
      "分析",
      "语言模型",
      "文本生成",
      "attention",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 77
  },
  {
    "slug": "缓解交叉熵过度自信的一个简明方案",
    "title": "缓解交叉熵过度自信的一个简明方案",
    "description": "缓解交叉熵过度自信的一个简明方案&para;\n原文链接: https://spaces.ac.cn/archives/9526\n发布日期: \n\n众所周知，分类问题的常规评估指标是正确率，而标准的损失函数则是交叉熵，交叉熵有着收敛快的优点，但它并非是正确率的光滑近似，这就带来了训练和预测的不一致性问题。另一方面，当训练样本的预测概率很低时，交叉熵会给出一个非常巨大的损失（趋于$-\\log 0^{+}...",
    "date": "2023-03-14",
    "source": "",
    "tags": [
      "优化",
      "损失函数",
      "光滑",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 76
  },
  {
    "slug": "tiger一个抠到极致的优化器",
    "title": "Tiger：一个“抠”到极致的优化器",
    "description": "Tiger：一个“抠”到极致的优化器&para;\n原文链接: https://spaces.ac.cn/archives/9512\n发布日期: \n\n这段时间笔者一直在实验《Google新搜出的优化器Lion：效率与效果兼得的“训练狮”》所介绍的Lion优化器。之所以对Lion饶有兴致，是因为它跟笔者之前的关于理想优化器的一些想法不谋而合，但当时笔者没有调出好的效果，而Lion则做好了。\n相比标准的...",
    "date": "2023-03-07",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "优化器",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 75
  },
  {
    "slug": "生成扩散模型漫谈十八得分匹配-条件得分匹配",
    "title": "生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配",
    "description": "生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配&para;\n原文链接: https://spaces.ac.cn/archives/9509\n发布日期: \n\n在前面的介绍中，我们多次提及“得分匹配”和“条件得分匹配”，它们是扩散模型、能量模型等经常出现的概念，特别是很多文章直接说扩散模型的训练目标是“得分匹配”，但事实上当前主流的扩散模型如DDPM的训练目标是“条件得分匹配”才对。\n那么“得...",
    "date": "2023-02-28",
    "source": "",
    "tags": [
      "概率",
      "分析",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 74
  },
  {
    "slug": "生成扩散模型漫谈十七构建ode的一般步骤下",
    "title": "生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）",
    "description": "生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）&para;\n原文链接: https://spaces.ac.cn/archives/9497\n发布日期: \n\n历史总是惊人地相似。当初笔者在写《生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）》（当时还没有“上”这个后缀）时，以为自己已经搞清楚了构建ODE式扩散的一般步骤，结果读者 @gaohuazuo 就给出了一个新的直观有效的方案，这直...",
    "date": "2023-02-23",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 73
  },
  {
    "slug": "google新搜出的优化器lion效率与效果兼得的训练狮",
    "title": "Google新搜出的优化器Lion：效率与效果兼得的“训练狮”",
    "description": "Google新搜出的优化器Lion：效率与效果兼得的“训练狮”&para;\n原文链接: https://spaces.ac.cn/archives/9473\n发布日期: \n\n昨天在Arixv上发现了Google新发的一篇论文《Symbolic Discovery of Optimization Algorithms》，主要是讲自动搜索优化器的，咋看上去没啥意思，因为类似的工作也有不少，大多数结果都...",
    "date": "2023-02-16",
    "source": "",
    "tags": [
      "分析",
      "优化",
      "优化器",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 72
  },
  {
    "slug": "生成扩散模型漫谈十六w距离-得分匹配",
    "title": "生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配",
    "description": "生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配&para;\n原文链接: https://spaces.ac.cn/archives/9467\n发布日期: \n\nWasserstein距离（下面简称“W距离”），是基于最优传输思想来度量两个概率分布差异程度的距离函数，笔者之前在《从Wasserstein距离、对偶理论到WGAN》等博文中也做过介绍。对于很多读者来说，第一次听说W距离，是因为2017年出...",
    "date": "2023-02-14",
    "source": "",
    "tags": [
      "详细推导",
      "微分方程",
      "GAN",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 71
  },
  {
    "slug": "测试函数法推导连续性方程和fokker-planck方程",
    "title": "测试函数法推导连续性方程和Fokker-Planck方程",
    "description": "测试函数法推导连续性方程和Fokker-Planck方程&para;\n原文链接: https://spaces.ac.cn/archives/9461\n发布日期: \n\n在文章《生成扩散模型漫谈（六）：一般框架之ODE篇》中，我们推导了SDE的Fokker-Planck方程；而在《生成扩散模型漫谈（十二）：“硬刚”扩散ODE》中，我们单独推导了ODE的连续性方程。它们都是描述随机变量沿着SDE/OD...",
    "date": "2023-02-11",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "微分方程",
      "随机",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 70
  },
  {
    "slug": "transformer升级之路8长度外推性与位置鲁棒性",
    "title": "Transformer升级之路：8、长度外推性与位置鲁棒性",
    "description": "Transformer升级之路：8、长度外推性与位置鲁棒性&para;\n原文链接: https://spaces.ac.cn/archives/9444\n发布日期: \n\n上一篇文章《Transformer升级之路：7、长度外推性与局部注意力》我们讨论了Transformer的长度外推性，得出的结论是长度外推性是一个训练和预测的不一致问题，而解决这个不一致的主要思路是将注意力局部化，很多外推性好的改...",
    "date": "2023-01-31",
    "source": "",
    "tags": [
      "详细推导",
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 69
  },
  {
    "slug": "transformer升级之路7长度外推性与局部注意力",
    "title": "Transformer升级之路：7、长度外推性与局部注意力",
    "description": "Transformer升级之路：7、长度外推性与局部注意力&para;\n原文链接: https://spaces.ac.cn/archives/9431\n发布日期: \n\n对于Transformer模型来说，其长度的外推性是我们一直在追求的良好性质，它是指我们在短序列上训练的模型，能否不用微调地用到长序列上并依然保持不错的效果。之所以追求长度外推性，一方面是理论的完备性，觉得这是一个理想模型应当具备...",
    "date": "2023-01-12",
    "source": "",
    "tags": [
      "详细推导",
      "语言模型",
      "attention",
      "位置编码",
      "外推",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 68
  },
  {
    "slug": "transformer升级之路6旋转位置编码的完备性分析",
    "title": "Transformer升级之路：6、旋转位置编码的完备性分析",
    "description": "Transformer升级之路：6、旋转位置编码的完备性分析&para;\n原文链接: https://spaces.ac.cn/archives/9403\n发布日期: \n\n在去年的文章《Transformer升级之路：2、博采众长的旋转式位置编码》中，笔者提出了旋转位置编码（RoPE），当时的出发点只是觉得用绝对位置来实现相对位置是一件“很好玩的事情”，并没料到其实际效果还相当不错，并为大家所接受...",
    "date": "2022-12-28",
    "source": "",
    "tags": [
      "详细推导",
      "矩阵",
      "attention",
      "位置编码",
      "rope",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 67
  },
  {
    "slug": "生成扩散模型漫谈十五构建ode的一般步骤中",
    "title": "生成扩散模型漫谈（十五）：构建ODE的一般步骤（中）",
    "description": "生成扩散模型漫谈（十五）：构建ODE的一般步骤（中）&para;\n原文链接: https://spaces.ac.cn/archives/9379\n发布日期: \n\n上周笔者写了《生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）》（当时还没有“上”这个后缀），本以为已经窥见了构建ODE扩散模型的一般规律，结果不久后评论区大神 @gaohuazuo 就给出了一个构建格林函数更高效、更直观的方案，让...",
    "date": "2022-12-22",
    "source": "",
    "tags": [
      "详细推导",
      "微分方程",
      "生成模型",
      "扩散",
      "格林函数",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 66
  },
  {
    "slug": "生成扩散模型漫谈十四构建ode的一般步骤上",
    "title": "生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）",
    "description": "生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）&para;\n原文链接: https://spaces.ac.cn/archives/9370\n发布日期: \n\n书接上文，在《生成扩散模型漫谈（十三）：从万有引力到扩散模型》中，我们介绍了一个由万有引力启发的、几何意义非常清晰的ODE式生成扩散模型。有的读者看了之后就疑问：似乎“万有引力”并不是唯一的选择，其他形式的力是否可以由同样的物理绘景构建...",
    "date": "2022-12-15",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "格林函数",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 65
  },
  {
    "slug": "从局部到全局语义相似度的测地线距离",
    "title": "从局部到全局：语义相似度的测地线距离",
    "description": "从局部到全局：语义相似度的测地线距离&para;\n原文链接: https://spaces.ac.cn/archives/9368\n发布日期: \n\n前段时间在最近的一篇论文《Unsupervised Opinion Summarization Using Approximate Geodesics》中学到了一个新的概念，叫做“测地线距离（Geodesic Distance）”，感觉有点意思，特来跟...",
    "date": "2022-12-07",
    "source": "",
    "tags": [
      "黎曼几何",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 64
  },
  {
    "slug": "用热传导方程来指导自监督学习",
    "title": "用热传导方程来指导自监督学习",
    "description": "用热传导方程来指导自监督学习&para;\n原文链接: https://spaces.ac.cn/archives/9359\n发布日期: \n\n用理论物理来卷机器学习已经不是什么新鲜事了，比如上个月介绍的《生成扩散模型漫谈（十三）：从万有引力到扩散模型》就是经典一例。最近一篇新出的论文《Self-Supervised Learning based on Heat Equation》，顾名思义，用热传导...",
    "date": "2022-11-30",
    "source": "",
    "tags": [
      "物理",
      "无监督",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 63
  },
  {
    "slug": "基于amos优化器思想推导出来的一些炼丹策略",
    "title": "基于Amos优化器思想推导出来的一些“炼丹策略”",
    "description": "基于Amos优化器思想推导出来的一些“炼丹策略”&para;\n原文链接: https://spaces.ac.cn/archives/9344\n发布日期: \n\n如果将训练模型比喻为“炼丹”，那么“炼丹炉”显然就是优化器了。据传AdamW优化器是当前训练神经网络最快的方案，这一点笔者也没有一一对比过，具体情况如何不得而知，不过目前做预训练时多数都用AdamW或其变种LAMB倒是真的。然而，正如有了炼...",
    "date": "2022-11-22",
    "source": "",
    "tags": [
      "分析",
      "优化",
      "渐近",
      "优化器",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 62
  },
  {
    "slug": "cosent三作为交互式相似度的损失函数",
    "title": "CoSENT（三）：作为交互式相似度的损失函数",
    "description": "CoSENT（三）：作为交互式相似度的损失函数&para;\n原文链接: https://spaces.ac.cn/archives/9341\n发布日期: \n\n在《CoSENT（一）：比Sentence-BERT更有效的句向量方案》中，笔者提出了名为“CoSENT”的有监督句向量方案，由于它是直接训练cos相似度的，跟评测目标更相关，因此通常能有着比Sentence-BERT更好的效果以及更快的收敛...",
    "date": "2022-11-09",
    "source": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 61
  },
  {
    "slug": "利用cur分解加速交互式相似度模型的检索",
    "title": "利用CUR分解加速交互式相似度模型的检索",
    "description": "利用CUR分解加速交互式相似度模型的检索&para;\n原文链接: https://spaces.ac.cn/archives/9336\n发布日期: \n\n文本相似度有“交互式”和“特征式”两种做法，想必很多读者对此已经不陌生，之前笔者也写过一篇文章《CoSENT（二）：特征式匹配与交互式匹配有多大差距？》来对比两者的效果。总的来说，交互式相似度效果通常会好些，但直接用它来做大规模检索是不现实的，而特...",
    "date": "2022-11-02",
    "source": "",
    "tags": [
      "矩阵",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 60
  },
  {
    "slug": "圆内随机n点在同一个圆心角为θ的扇形的概率",
    "title": "圆内随机n点在同一个圆心角为θ的扇形的概率",
    "description": "圆内随机n点在同一个圆心角为θ的扇形的概率&para;\n原文链接: https://spaces.ac.cn/archives/9324\n发布日期: \n\n这几天网上热传了一道“四鸭共半圆”题目：  \n\n四鸭共半圆问题\n可能有不少读者看到后也尝试做过，就连李永乐老师也专门开了一节课讲这道题（参考《圆形水池四只鸭子在同一个半圆里，概率有多大？》）。就这道题目本身而言，答案并不算困难，可以有很多方法算出...",
    "date": "2022-10-25",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "竞赛",
      "随机",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 59
  },
  {
    "slug": "生成扩散模型漫谈十三从万有引力到扩散模型",
    "title": "生成扩散模型漫谈（十三）：从万有引力到扩散模型",
    "description": "生成扩散模型漫谈（十三）：从万有引力到扩散模型&para;\n原文链接: https://spaces.ac.cn/archives/9305\n发布日期: \n\n对于很多读者来说，生成扩散模型可能是他们遇到的第一个能够将如此多的数学工具用到深度学习上的模型。在这个系列文章中，我们已经展示了扩散模型与数学分析、概率统计、常微分方程、随机微分方程乃至偏微分方程等内容的深刻联系，可以说，即便是做数学物理方程...",
    "date": "2022-10-18",
    "source": "",
    "tags": [
      "详细推导",
      "引力",
      "场论",
      "生成模型",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 58
  },
  {
    "slug": "十字架组合计数问题浅试",
    "title": "“十字架”组合计数问题浅试",
    "description": "“十字架”组合计数问题浅试&para;\n原文链接: https://spaces.ac.cn/archives/9291\n发布日期: \n\n昨天在这个公众号文章看到了一道据说答案有争议的“十字架”组合计数问题：\n\n一个正方形中，如果四条边有两条是$i$色，另外两条是其他两种不同颜色，那么称这个正方形是“$i$色主导”的。考虑如下由16条线段、5个正方形组成的“十字架”图形，每条边染上红、黄、蓝三色之...",
    "date": "2022-10-09",
    "source": "",
    "tags": [
      "证明",
      "数学",
      "组合数学",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 57
  },
  {
    "slug": "生成扩散模型漫谈十二硬刚扩散ode",
    "title": "生成扩散模型漫谈（十二）：“硬刚”扩散ODE",
    "description": "生成扩散模型漫谈（十二）：“硬刚”扩散ODE&para;\n原文链接: https://spaces.ac.cn/archives/9280\n发布日期: \n\n在《生成扩散模型漫谈（五）：一般框架之SDE篇》中，我们从SDE的角度理解了生成扩散模型，然后在《生成扩散模型漫谈（六）：一般框架之ODE篇》中，我们知道SDE对应的扩散模型中，实际上隐含了一个ODE模型。无独有偶，在《生成扩散模型漫谈（四）：...",
    "date": "2022-09-28",
    "source": "",
    "tags": [
      "微分方程",
      "生成模型",
      "扩散",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 56
  },
  {
    "slug": "生成扩散模型漫谈十一统一扩散模型应用篇",
    "title": "生成扩散模型漫谈（十一）：统一扩散模型（应用篇）",
    "description": "生成扩散模型漫谈（十一）：统一扩散模型（应用篇）&para;\n原文链接: https://spaces.ac.cn/archives/9271\n发布日期: \n\n在《生成扩散模型漫谈（十）：统一扩散模型（理论篇）》中，笔者自称构建了一个统一的模型框架（Unified Diffusion Model，UDM），它允许更一般的扩散方式和数据类型。那么UDM框架究竟能否实现如期目的呢？本文通过一些具体例子...",
    "date": "2022-09-21",
    "source": "",
    "tags": [
      "统一",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 55
  },
  {
    "slug": "生成扩散模型漫谈十统一扩散模型理论篇",
    "title": "生成扩散模型漫谈（十）：统一扩散模型（理论篇）",
    "description": "生成扩散模型漫谈（十）：统一扩散模型（理论篇）&para;\n原文链接: https://spaces.ac.cn/archives/9262\n发布日期: \n\n老读者也许会发现，相比之前的更新频率，这篇文章可谓是“姗姗来迟”，因为这篇文章“想得太多”了。\n通过前面九篇文章，我们已经对生成扩散模型做了一个相对全面的介绍。虽然理论内容很多，但我们可以发现，前面介绍的扩散模型处理的都是连续型对象，并且都是...",
    "date": "2022-09-14",
    "source": "",
    "tags": [
      "统一",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 54
  },
  {
    "slug": "生成扩散模型漫谈九条件控制生成结果",
    "title": "生成扩散模型漫谈（九）：条件控制生成结果",
    "description": "生成扩散模型漫谈（九）：条件控制生成结果&para;\n原文链接: https://spaces.ac.cn/archives/9257\n发布日期: \n\n前面的几篇文章都是比较偏理论的结果，这篇文章我们来讨论一个比较有实用价值的主题——条件控制生成。\n作为生成模型，扩散模型跟VAE、GAN、flow等模型的发展史很相似，都是先出来了无条件生成，然后有条件生成就紧接而来。无条件生成往往是为了探索效果上...",
    "date": "2022-08-30",
    "source": "",
    "tags": [
      "概率",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 53
  },
  {
    "slug": "生成扩散模型漫谈八最优扩散方差估计下",
    "title": "生成扩散模型漫谈（八）：最优扩散方差估计（下）",
    "description": "生成扩散模型漫谈（八）：最优扩散方差估计（下）&para;\n原文链接: https://spaces.ac.cn/archives/9246\n发布日期: \n\n在上一篇文章《生成扩散模型漫谈（七）：最优扩散方差估计（上）》中，我们介绍并推导了Analytic-DPM中的扩散模型最优方差估计结果，它是直接给出了已经训练好的生成扩散模型的最优方差的一个解析估计，实验显示该估计结果确实能有效提高扩散模型的...",
    "date": "2022-08-18",
    "source": "",
    "tags": [
      "优化",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 52
  },
  {
    "slug": "生成扩散模型漫谈七最优扩散方差估计上",
    "title": "生成扩散模型漫谈（七）：最优扩散方差估计（上）",
    "description": "生成扩散模型漫谈（七）：最优扩散方差估计（上）&para;\n原文链接: https://spaces.ac.cn/archives/9245\n发布日期: \n\n对于生成扩散模型来说，一个很关键的问题是生成过程的方差应该怎么选择，因为不同的方差会明显影响生成效果。\n在《生成扩散模型漫谈（二）：DDPM = 自回归式VAE》我们提到，DDPM分别假设数据服从两种特殊分布推出了两个可用的结果；《生成扩散模...",
    "date": "2022-08-12",
    "source": "",
    "tags": [
      "优化",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 51
  },
  {
    "slug": "生成扩散模型漫谈六一般框架之ode篇",
    "title": "生成扩散模型漫谈（六）：一般框架之ODE篇",
    "description": "生成扩散模型漫谈（六）：一般框架之ODE篇&para;\n原文链接: https://spaces.ac.cn/archives/9228\n发布日期: \n\n上一篇文章《生成扩散模型漫谈（五）：一般框架之SDE篇》中，我们对宋飏博士的论文《Score-Based Generative Modeling through Stochastic Differential Equations》做了基本的介绍和...",
    "date": "2022-08-08",
    "source": "",
    "tags": [
      "详细推导",
      "flow模型",
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 50
  },
  {
    "slug": "生成扩散模型漫谈五一般框架之sde篇",
    "title": "生成扩散模型漫谈（五）：一般框架之SDE篇",
    "description": "生成扩散模型漫谈（五）：一般框架之SDE篇&para;\n原文链接: https://spaces.ac.cn/archives/9209\n发布日期: \n\n在写生成扩散模型的第一篇文章时，就有读者在评论区推荐了宋飏博士的论文《Score-Based Generative Modeling through Stochastic Differential Equations》，可以说该论文构建了一个相当...",
    "date": "2022-08-03",
    "source": "",
    "tags": [
      "详细推导",
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 49
  },
  {
    "slug": "生成扩散模型漫谈四ddim-高观点ddpm",
    "title": "生成扩散模型漫谈（四）：DDIM = 高观点DDPM",
    "description": "生成扩散模型漫谈（四）：DDIM = 高观点DDPM&para;\n原文链接: https://spaces.ac.cn/archives/9181\n发布日期: \n\n相信很多读者都听说过甚至读过克莱因的《高观点下的初等数学》这套书，顾名思义，这是在学到了更深入、更完备的数学知识后，从更高的视角重新审视过往学过的初等数学，以得到更全面的认知，甚至达到温故而知新的效果。类似的书籍还有很多，比如《重温微积...",
    "date": "2022-07-27",
    "source": "",
    "tags": [
      "详细推导",
      "微分方程",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 48
  },
  {
    "slug": "生成扩散模型漫谈三ddpm-贝叶斯-去噪",
    "title": "生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪",
    "description": "生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪&para;\n原文链接: https://spaces.ac.cn/archives/9164\n发布日期: \n\n到目前为止，笔者给出了生成扩散模型DDPM的两种推导，分别是《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》中的通俗类比方案和《生成扩散模型漫谈（二）：DDPM = 自回归式VAE》中的变分自编码器方案。两种方案可谓各有特点，...",
    "date": "2022-07-19",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 47
  },
  {
    "slug": "不成功的尝试将多标签交叉熵推广到n个m分类上去",
    "title": "不成功的尝试：将多标签交叉熵推广到“n个m分类”上去",
    "description": "不成功的尝试：将多标签交叉熵推广到“n个m分类”上去&para;\n原文链接: https://spaces.ac.cn/archives/9158\n发布日期: \n\n可能有读者留意到，这次更新相对来说隔得比较久了。事实上，在上周末时就开始准备这篇文章了，然而笔者低估了这个问题的难度，几乎推导了整整一周，仍然还没得到一个完善的结果出来。目前发出来的，仍然只是一个失败的结果，希望有经验的读者可以指点指点...",
    "date": "2022-07-15",
    "source": "",
    "tags": [
      "优化",
      "损失函数",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 46
  },
  {
    "slug": "生成扩散模型漫谈二ddpm-自回归式vae",
    "title": "生成扩散模型漫谈（二）：DDPM = 自回归式VAE",
    "description": "生成扩散模型漫谈（二）：DDPM = 自回归式VAE&para;\n原文链接: https://spaces.ac.cn/archives/9152\n发布日期: \n\n在文章《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》中，我们为生成扩散模型DDPM构建了“拆楼-建楼”的通俗类比，并且借助该类比完整地推导了生成扩散模型DDPM的理论形式。在该文章中，我们还指出DDPM本质上已经不是传统的扩散...",
    "date": "2022-07-06",
    "source": "",
    "tags": [
      "vae",
      "生成模型",
      "DDPM",
      "扩散",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 45
  },
  {
    "slug": "维度灾难之hubness现象浅析",
    "title": "“维度灾难”之Hubness现象浅析",
    "description": "“维度灾难”之Hubness现象浅析&para;\n原文链接: https://spaces.ac.cn/archives/9147\n发布日期: \n\n这几天读到论文《Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling》，了解到了一个新的名词“Hubness现象”，说的是高维空间中的一种聚集效应，本质...",
    "date": "2022-06-28",
    "source": "",
    "tags": [
      "维度",
      "GAN",
      "生成模型",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 44
  },
  {
    "slug": "ladder-side-tuning预训练模型的过墙梯",
    "title": "Ladder Side-Tuning：预训练模型的“过墙梯”",
    "description": "Ladder Side-Tuning：预训练模型的“过墙梯”&para;\n原文链接: https://spaces.ac.cn/archives/9138\n发布日期: \n\n如果说大型的预训练模型是自然语言处理的“张良计”，那么对应的“过墙梯”是什么呢？笔者认为是高效地微调这些大模型到特定任务上的各种技巧。除了直接微调全部参数外，还有像Adapter、P-Tuning等很多参数高效的微调技巧，它们能...",
    "date": "2022-06-20",
    "source": "",
    "tags": [
      "语言模型",
      "预训练",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 43
  },
  {
    "slug": "生成扩散模型漫谈一ddpm-拆楼-建楼",
    "title": "生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼",
    "description": "生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼&para;\n原文链接: https://spaces.ac.cn/archives/9119\n发布日期: \n\n说到生成模型，VAE、GAN可谓是“如雷贯耳”，本站也有过多次分享。此外，还有一些比较小众的选择，如flow模型、VQ-VAE等，也颇有人气，尤其是VQ-VAE及其变体VQ-GAN，近期已经逐渐发展到“图像的Tokenizer”的地位，...",
    "date": "2022-06-13",
    "source": "",
    "tags": [
      "VAE",
      "GAN",
      "flow模型",
      "概率",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 42
  },
  {
    "slug": "相对位置编码transformer的一个理论缺陷与对策",
    "title": "相对位置编码Transformer的一个理论缺陷与对策",
    "description": "相对位置编码Transformer的一个理论缺陷与对策&para;\n原文链接: https://spaces.ac.cn/archives/9105\n发布日期: \n\n位置编码是Transformer中很重要的一环，在《让研究人员绞尽脑汁的Transformer位置编码》中我们就总结了一些常见的位置编码设计。大体上，我们将Transformer的位置编码分为“绝对位置编码”和“相对位置编码”两类，其...",
    "date": "2022-06-07",
    "source": "",
    "tags": [
      "详细推导",
      "语言模型",
      "attention",
      "位置编码",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 41
  },
  {
    "slug": "如何训练你的准确率",
    "title": "如何训练你的准确率？",
    "description": "如何训练你的准确率？&para;\n原文链接: https://spaces.ac.cn/archives/9098\n发布日期: \n\n最近Arxiv上的一篇论文《EXACT: How to Train Your Accuracy》引起了笔者的兴趣，顾名思义这是介绍如何直接以准确率为训练目标来训练模型的。正好笔者之前也对此有过一些分析，如《函数光滑化杂谈：不可导函数的可导逼近》、《再谈类别不平衡问题：...",
    "date": "2022-06-01",
    "source": "",
    "tags": [
      "概率",
      "优化",
      "损失函数",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 40
  },
  {
    "slug": "从重参数的角度看离散概率分布的构建",
    "title": "从重参数的角度看离散概率分布的构建",
    "description": "从重参数的角度看离散概率分布的构建&para;\n原文链接: https://spaces.ac.cn/archives/9085\n发布日期: \n\n一般来说，神经网络的输出都是无约束的，也就是值域为$\\mathbb{R}$，而为了得到有约束的输出，通常是采用加激活函数的方式。例如，如果我们想要输出一个概率分布来代表每个类别的概率，那么通常在最后加上Softmax作为激活函数。那么一个紧接着的疑问就是...",
    "date": "2022-05-25",
    "source": "",
    "tags": [
      "详细推导",
      "概率",
      "重参数",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 39
  },
  {
    "slug": "当bert-whitening引入超参数总有一款适合你",
    "title": "当BERT-whitening引入超参数：总有一款适合你",
    "description": "当BERT-whitening引入超参数：总有一款适合你&para;\n原文链接: https://spaces.ac.cn/archives/9079\n发布日期: \n\n在《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》中，笔者提出了BERT-whitening，验证了一个线性变换就能媲美当时的SOTA方法BERT-flow。此外，BERT-whitening还可以对句向量进行...",
    "date": "2022-05-18",
    "source": "",
    "tags": [
      "语言模型",
      "语义",
      "语义相似度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 38
  },
  {
    "slug": "logsumexp运算的几个不等式",
    "title": "logsumexp运算的几个不等式",
    "description": "logsumexp运算的几个不等式&para;\n原文链接: https://spaces.ac.cn/archives/9070\n发布日期: \n\n$\\text{logsumexp}$是机器学习经常遇到的运算，尤其是交叉熵的相关实现和推导中都会经常出现，同时它还是$\\max$的光滑近似（参考《寻求一个光滑的最大值函数》）。设$x=(x_1,x_2,\\cdots,x_n)$，$\\text{logsum...",
    "date": "2022-05-10",
    "source": "",
    "tags": [
      "不等式",
      "函数",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 37
  },
  {
    "slug": "多标签softmax交叉熵的软标签版本",
    "title": "多标签“Softmax+交叉熵”的软标签版本",
    "description": "多标签“Softmax+交叉熵”的软标签版本&para;\n原文链接: https://spaces.ac.cn/archives/9064\n发布日期: \n\n（注：本文的相关内容已整理成论文《ZLPR: A Novel Loss for Multi-label Classification》，如需引用可以直接引用英文论文，谢谢。）\n在《将“Softmax+交叉熵”推广到多标签分类问题》中，我们提出了...",
    "date": "2022-05-07",
    "source": "",
    "tags": [
      "优化",
      "损失函数",
      "光滑",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 36
  },
  {
    "slug": "在bert4keras中使用混合精度和xla加速训练",
    "title": "在bert4keras中使用混合精度和XLA加速训练",
    "description": "在bert4keras中使用混合精度和XLA加速训练&para;\n原文链接: https://spaces.ac.cn/archives/9059\n发布日期: \n\n之前笔者一直都是聚焦于模型的构思和实现，鲜有关注模型的训练加速，像混合精度和XLA这些技术，虽然也有听过，但没真正去实践过。这两天折腾了一番，成功在bert4keras中使用了混合精度和XLA来加速训练，在此做个简单的总结，供大家参考。...",
    "date": "2022-04-28",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 35
  },
  {
    "slug": "gau-α尝鲜体验快好省的下一代attention",
    "title": "GAU-α：尝鲜体验快好省的下一代Attention",
    "description": "GAU-α：尝鲜体验快好省的下一代Attention&para;\n原文链接: https://spaces.ac.cn/archives/9052\n发布日期: \n\n在《FLASH：可能是近来最有意思的高效Transformer设计》中，我们介绍了GAU（Gated Attention Unit，门控线性单元），在这里笔者愿意称之为“目前最有潜力的下一代Attention设计”，因为它真正达到了“更...",
    "date": "2022-04-22",
    "source": "",
    "tags": [
      "语言模型",
      "attention",
      "预训练",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 34
  },
  {
    "slug": "你的语言模型有没有无法预测的词",
    "title": "你的语言模型有没有“无法预测的词”？",
    "description": "你的语言模型有没有“无法预测的词”？&para;\n原文链接: https://spaces.ac.cn/archives/9046\n发布日期: \n\n众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不...",
    "date": "2022-04-20",
    "source": "",
    "tags": [
      "语言模型",
      "多任务",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 33
  },
  {
    "slug": "globalpointer下的kl散度应该是怎样的",
    "title": "GlobalPointer下的“KL散度”应该是怎样的？",
    "description": "GlobalPointer下的“KL散度”应该是怎样的？&para;\n原文链接: https://spaces.ac.cn/archives/9039\n发布日期: \n\n最近有读者提到想测试一下GlobalPointer与R-Drop结合的效果，但不知道GlobalPointer下的KL散度该怎么算。像R-Drop或者虚拟对抗训练这些正则化手段，里边都需要算概率分布的KL散度，但GlobalPoin...",
    "date": "2022-04-15",
    "source": "",
    "tags": [
      "损失函数",
      "对抗训练",
      "NER",
      "正则化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 32
  },
  {
    "slug": "熵不变性softmax的一个快速推导",
    "title": "熵不变性Softmax的一个快速推导",
    "description": "熵不变性Softmax的一个快速推导&para;\n原文链接: https://spaces.ac.cn/archives/9034\n发布日期: \n\n在文章《从熵不变性看Attention的Scale操作》中，我们推导了一版具有熵不变性质的注意力机制：\n\\begin{equation}Attention(Q,K,V) = softmax\\left(\\frac{\\kappa \\log n}{d}QK^...",
    "date": "2022-04-11",
    "source": "",
    "tags": [
      "近似",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 31
  },
  {
    "slug": "听说attention与softmax更配哦",
    "title": "听说Attention与Softmax更配哦～",
    "description": "听说Attention与Softmax更配哦～&para;\n原文链接: https://spaces.ac.cn/archives/9019\n发布日期: \n\n不知道大家留意到一个细节没有，就是当前NLP主流的预训练模式都是在一个固定长度（比如512）上进行，然后直接将预训练好的模型用于不同长度的任务中。大家似乎也没有对这种模式有过怀疑，仿佛模型可以自动泛化到不同长度是一个“理所应当”的能力。\n当然...",
    "date": "2022-04-07",
    "source": "",
    "tags": [
      "熵",
      "语言模型",
      "attention",
      "预训练",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 30
  },
  {
    "slug": "为什么pre-norm的效果不如post-norm",
    "title": "为什么Pre Norm的效果不如Post Norm？",
    "description": "为什么Pre Norm的效果不如Post Norm？&para;\n原文链接: https://spaces.ac.cn/archives/9009\n发布日期: \n\nPre Norm与Post Norm之间的对比是一个“老生常谈”的话题了，本博客就多次讨论过这个问题，比如文章《浅谈Transformer的初始化、参数化与标准化》、《模型优化漫谈：BERT的初始标准差为什么是0.02？》等。目前比较明...",
    "date": "2022-03-29",
    "source": "",
    "tags": [
      "优化",
      "梯度",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 29
  },
  {
    "slug": "roformerv2自然语言理解的极限探索",
    "title": "RoFormerV2：自然语言理解的极限探索",
    "description": "RoFormerV2：自然语言理解的极限探索&para;\n原文链接: https://spaces.ac.cn/archives/8998\n发布日期: \n\n大概在1年前，我们提出了旋转位置编码（RoPE），并发布了对应的预训练模型RoFormer。随着时间的推移，RoFormer非常幸运地得到了越来越多的关注和认可，比如EleutherAI新发布的60亿和200亿参数的GPT模型中就用上了RoPE...",
    "date": "2022-03-21",
    "source": "",
    "tags": [
      "语言模型",
      "预训练",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 28
  },
  {
    "slug": "为什么需要残差一个来自deepnet的视角",
    "title": "为什么需要残差？一个来自DeepNet的视角",
    "description": "为什么需要残差？一个来自DeepNet的视角&para;\n原文链接: https://spaces.ac.cn/archives/8994\n发布日期: \n\n在《训练1000层的Transformer究竟有什么困难？》中我们介绍了微软提出的能训练1000层Transformer的DeepNet技术。而对于DeepNet，读者一般也有两种反应，一是为此感到惊叹而点赞，另一则是觉得新瓶装旧酒没意思。出现...",
    "date": "2022-03-19",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "深度学习",
      "梯度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 27
  },
  {
    "slug": "门控注意力单元gau还需要warmup吗",
    "title": "门控注意力单元（GAU）还需要Warmup吗？",
    "description": "门控注意力单元（GAU）还需要Warmup吗？&para;\n原文链接: https://spaces.ac.cn/archives/8990\n发布日期: \n\n在文章《训练1000层的Transformer究竟有什么困难？》发布之后，很快就有读者问到如果将其用到《FLASH：可能是近来最有意思的高效Transformer设计》中的“门控注意力单元（GAU）”，那结果是怎样的？跟标准Transform...",
    "date": "2022-03-11",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 26
  },
  {
    "slug": "训练1000层的transformer究竟有什么困难",
    "title": "训练1000层的Transformer究竟有什么困难？",
    "description": "训练1000层的Transformer究竟有什么困难？&para;\n原文链接: https://spaces.ac.cn/archives/8978\n发布日期: \n\n众所周知，现在的Transformer越做越大，但这个“大”通常是“宽”而不是“深”，像GPT-3虽然参数有上千亿，但也只是一个96层的Transformer模型，与我们能想象的深度相差甚远。是什么限制了Transformer往“深”...",
    "date": "2022-03-09",
    "source": "",
    "tags": [
      "优化",
      "梯度",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 25
  },
  {
    "slug": "指数梯度下降-元学习-自适应学习率",
    "title": "指数梯度下降 + 元学习 = 自适应学习率",
    "description": "指数梯度下降 + 元学习 = 自适应学习率&para;\n原文链接: https://spaces.ac.cn/archives/8968\n发布日期: \n\n前两天刷到了Google的一篇论文《Step-size Adaptation Using Exponentiated Gradient Updates》，在其中学到了一些新的概念，所以在此记录分享一下。主要的内容有两个，一是非负优化的指数梯度下降...",
    "date": "2022-03-03",
    "source": "",
    "tags": [
      "优化",
      "梯度",
      "优化器",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 24
  },
  {
    "slug": "flash可能是近来最有意思的高效transformer设计",
    "title": "FLASH：可能是近来最有意思的高效Transformer设计",
    "description": "FLASH：可能是近来最有意思的高效Transformer设计&para;\n原文链接: https://spaces.ac.cn/archives/8934\n发布日期: \n\n高效Transformer，泛指所有概率Transformer效率的工作，笔者算是关注得比较早了，最早的博客可以追溯到2019年的《为节约而生：从标准Attention到稀疏Attention》，当时做这块的工作很少。后来，这...",
    "date": "2022-02-25",
    "source": "",
    "tags": [
      "语言模型",
      "生成模型",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 23
  },
  {
    "slug": "gplinker基于globalpointer的事件联合抽取",
    "title": "GPLinker：基于GlobalPointer的事件联合抽取",
    "description": "GPLinker：基于GlobalPointer的事件联合抽取&para;\n原文链接: https://spaces.ac.cn/archives/8926\n发布日期: \n\n大约两年前，笔者在百度的“2020语言与智能技术竞赛”中首次接触到了事件抽取任务，并在文章《bert4keras在手，baseline我有：百度LIC2020》中分享了一个转化为BERT+CRF做NER的简单baseline。...",
    "date": "2022-02-21",
    "source": "",
    "tags": [
      "NLP",
      "信息抽取",
      "NER",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 22
  },
  {
    "slug": "多任务学习漫谈三分主次之序",
    "title": "多任务学习漫谈（三）：分主次之序",
    "description": "多任务学习漫谈（三）：分主次之序&para;\n原文链接: https://spaces.ac.cn/archives/8907\n发布日期: \n\n多任务学习是一个很宽泛的命题，不同场景下多任务学习的目标不尽相同。在《多任务学习漫谈（一）：以损失之名》和《多任务学习漫谈（二）：行梯度之事》中，我们将多任务学习的目标理解为“做好每一个任务”，具体表现是“尽量平等地处理每一个任务”，我们可以称之为“平行型...",
    "date": "2022-02-14",
    "source": "",
    "tags": [
      "深度学习",
      "损失函数",
      "梯度",
      "多任务",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 21
  },
  {
    "slug": "多任务学习漫谈二行梯度之事",
    "title": "多任务学习漫谈（二）：行梯度之事",
    "description": "多任务学习漫谈（二）：行梯度之事&para;\n原文链接: https://spaces.ac.cn/archives/8896\n发布日期: \n\n在《多任务学习漫谈（一）：以损失之名》中，我们从损失函数的角度初步探讨了多任务学习问题，最终发现如果想要结果同时具有缩放不变性和平移不变性，那么用梯度的模长倒数作为任务的权重是一个比较简单的选择。我们继而分析了，该设计等价于将每个任务的梯度单独进行归一化后...",
    "date": "2022-02-08",
    "source": "",
    "tags": [
      "深度学习",
      "损失函数",
      "梯度",
      "多任务",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 20
  },
  {
    "slug": "gplinker基于globalpointer的实体关系联合抽取",
    "title": "GPLinker：基于GlobalPointer的实体关系联合抽取",
    "description": "GPLinker：基于GlobalPointer的实体关系联合抽取&para;\n原文链接: https://spaces.ac.cn/archives/8888\n发布日期: \n\n在将近三年前的百度“2019语言与智能技术竞赛”（下称LIC2019）中，笔者提出了一个新的关系抽取模型（参考《基于DGCNN和概率图的轻量级信息抽取模型》），后被进一步发表和命名为“CasRel”，算是当时关系抽取的SO...",
    "date": "2022-01-30",
    "source": "",
    "tags": [
      "NLP",
      "信息抽取",
      "NER",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 19
  },
  {
    "slug": "efficient-globalpointer少点参数多点效果",
    "title": "Efficient GlobalPointer：少点参数，多点效果",
    "description": "Efficient GlobalPointer：少点参数，多点效果&para;\n原文链接: https://spaces.ac.cn/archives/8877\n发布日期: \n\n在《GlobalPointer：用统一的方式处理嵌套和非嵌套NER》中，我们提出了名为“GlobalPointer”的token-pair识别模块，当它用于NER时，能统一处理嵌套和非嵌套任务，并在非嵌套场景有着比CRF更...",
    "date": "2022-01-25",
    "source": "",
    "tags": [
      "模型",
      "NLP",
      "NER",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 18
  },
  {
    "slug": "多任务学习漫谈一以损失之名",
    "title": "多任务学习漫谈（一）：以损失之名",
    "description": "多任务学习漫谈（一）：以损失之名&para;\n原文链接: https://spaces.ac.cn/archives/8870\n发布日期: \n\n能提升模型性能的方法有很多，多任务学习（Multi-Task Learning）也是其中一种。简单来说，多任务学习是希望将多个相关的任务共同训练，希望不同任务之间能够相互补充和促进，从而获得单任务上更好的效果（准确率、鲁棒性等）。然而，多任务学习并不是所有...",
    "date": "2022-01-18",
    "source": "",
    "tags": [
      "深度学习",
      "损失函数",
      "多任务",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 17
  },
  {
    "slug": "cosent二特征式匹配与交互式匹配有多大差距",
    "title": "CoSENT（二）：特征式匹配与交互式匹配有多大差距？",
    "description": "CoSENT（二）：特征式匹配与交互式匹配有多大差距？&para;\n原文链接: https://spaces.ac.cn/archives/8860\n发布日期: \n\n一般来说，文本匹配有交互式（Interaction-based）和特征式（Representation-based）两种实现方案，其中交互式是指将两个文本拼接在一起当成单文本进行分类，而特征式则是指两个句子分别由编码器编码为句向量后再...",
    "date": "2022-01-12",
    "source": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 16
  },
  {
    "slug": "cosent一比sentence-bert更有效的句向量方案",
    "title": "CoSENT（一）：比Sentence-BERT更有效的句向量方案",
    "description": "CoSENT（一）：比Sentence-BERT更有效的句向量方案&para;\n原文链接: https://spaces.ac.cn/archives/8847\n发布日期: \n\n学习句向量的方案大致上可以分为无监督和有监督两大类，其中有监督句向量比较主流的方案是Facebook提出的“InferSent”，而后的“Sentence-BERT”进一步在BERT上肯定了它的有效性。然而，不管是Infe...",
    "date": "2022-01-06",
    "source": "",
    "tags": [
      "语义",
      "语义相似度",
      "对比学习",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 15
  },
  {
    "slug": "squareplus可能是运算最简单的relu光滑近似",
    "title": "SquarePlus：可能是运算最简单的ReLU光滑近似",
    "description": "SquarePlus：可能是运算最简单的ReLU光滑近似&para;\n原文链接: https://spaces.ac.cn/archives/8833\n发布日期: \n\nReLU函数，也就是$\\max(x,0)$，是最常见的激活函数之一，然而它在$x=0$处的不可导通常也被视为一个“槽点”。为此，有诸多的光滑近似被提出，比如SoftPlus、GeLU、Swish等，不过这些光滑近似无一例外地至少都使...",
    "date": "2021-12-29",
    "source": "",
    "tags": [
      "函数",
      "近似",
      "分析",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 14
  },
  {
    "slug": "概率分布的熵归一化entropy-normalization",
    "title": "概率分布的熵归一化（Entropy Normalization）",
    "description": "概率分布的熵归一化（Entropy Normalization）&para;\n原文链接: https://spaces.ac.cn/archives/8829\n发布日期: \n\n在上一篇文章《从熵不变性看Attention的Scale操作》中，我们从熵不变性的角度推导了一个新的Attention Scale，并且实验显示具有熵不变性的新Scale确实能使得Attention的外推性能更好。这时候笔者...",
    "date": "2021-12-24",
    "source": "",
    "tags": [
      "概率",
      "熵",
      "生成模型",
      "attention",
      "优化"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 13
  },
  {
    "slug": "从熵不变性看attention的scale操作",
    "title": "从熵不变性看Attention的Scale操作",
    "description": "从熵不变性看Attention的Scale操作&para;\n原文链接: https://spaces.ac.cn/archives/8823\n发布日期: \n\n当前Transformer架构用的最多的注意力机制，全称为“Scaled Dot-Product Attention”，其中“Scaled”是因为在$Q,K$转置相乘之后还要除以一个$\\sqrt{d}$再做Softmax（下面均不失一般性地假...",
    "date": "2021-12-21",
    "source": "",
    "tags": [
      "概率",
      "熵",
      "attention",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 12
  },
  {
    "slug": "seq2seq前缀树检索任务新范式以kgclue为例",
    "title": "Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）",
    "description": "Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）&para;\n原文链接: https://spaces.ac.cn/archives/8802\n发布日期: \n\n两年前，在《万能的seq2seq：基于seq2seq的阅读理解问答》和《“非自回归”也不差：基于MLM的阅读理解问答》中，我们在尝试过分别利用“Seq2Seq+前缀树”和“MLM+前缀树”的方式做抽取式阅读理解任务，并获得了不错...",
    "date": "2021-12-17",
    "source": "",
    "tags": [
      "代码",
      "语义",
      "keras",
      "相似度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 11
  },
  {
    "slug": "输入梯度惩罚与参数梯度惩罚的一个不等式",
    "title": "输入梯度惩罚与参数梯度惩罚的一个不等式",
    "description": "输入梯度惩罚与参数梯度惩罚的一个不等式&para;\n原文链接: https://spaces.ac.cn/archives/8796\n发布日期: \n\n在本博客中，已经多次讨论过梯度惩罚相关内容了。从形式上来看，梯度惩罚项分为两种，一种是关于输入的梯度惩罚$\\Vert\\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x};\\boldsymbol{\\theta})\\Vert^...",
    "date": "2021-12-11",
    "source": "",
    "tags": [
      "不等式",
      "优化",
      "梯度",
      "泛化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 10
  },
  {
    "slug": "变分自编码器八估计样本概率密度",
    "title": "变分自编码器（八）：估计样本概率密度",
    "description": "变分自编码器（八）：估计样本概率密度&para;\n原文链接: https://spaces.ac.cn/archives/8791\n发布日期: \n\n在本系列的前面几篇文章中，我们已经从多个角度来理解了VAE，一般来说，用VAE是为了得到一个生成模型，或者是做更好的编码模型，这都是VAE的常规用途。但除了这些常规应用外，还有一些“小众需求”，比如用来估计$x$的概率密度，这在做压缩的时候通常会用到。...",
    "date": "2021-12-09",
    "source": "",
    "tags": [
      "概率",
      "变分",
      "vae",
      "生成模型",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 9
  },
  {
    "slug": "dropout视角下的mlm和mae一些新的启发",
    "title": "Dropout视角下的MLM和MAE：一些新的启发",
    "description": "Dropout视角下的MLM和MAE：一些新的启发&para;\n原文链接: https://spaces.ac.cn/archives/8770\n发布日期: \n\n大家都知道，BERT的MLM（Masked Language Model）任务在预训练和微调时的不一致，也就是预训练出现了[MASK]而下游任务微调时没有[MASK]，是经常被吐槽的问题，很多工作都认为这是影响BERT微调性能的重要原因，...",
    "date": "2021-11-29",
    "source": "",
    "tags": [
      "模型",
      "概率",
      "分析",
      "优化",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 8
  },
  {
    "slug": "childtuning试试把dropout加到梯度上去",
    "title": "ChildTuning：试试把Dropout加到梯度上去？",
    "description": "ChildTuning：试试把Dropout加到梯度上去？&para;\n原文链接: https://spaces.ac.cn/archives/8764\n发布日期: \n\nDropout是经典的防止过拟合的思路了，想必很多读者已经了解过它。有意思的是，最近Dropout有点“老树发新芽”的感觉，出现了一些有趣的新玩法，比如最近引起过热议的SimCSE和R-Drop，尤其是在文章《又是Dropout两...",
    "date": "2021-11-22",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 7
  },
  {
    "slug": "wgan新方案通过梯度归一化来实现l约束",
    "title": "WGAN新方案：通过梯度归一化来实现L约束",
    "description": "WGAN新方案：通过梯度归一化来实现L约束&para;\n原文链接: https://spaces.ac.cn/archives/8757\n发布日期: \n\n当前，WGAN主流的实现方式包括参数裁剪（Weight Clipping）、谱归一化（Spectral Normalization）、梯度惩罚（Gradient Penalty），本来则来介绍一种新的实现方案：梯度归一化（Gradient Nor...",
    "date": "2021-11-15",
    "source": "",
    "tags": [
      "无监督",
      "GAN",
      "生成模型",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 6
  },
  {
    "slug": "模型优化漫谈bert的初始标准差为什么是002",
    "title": "模型优化漫谈：BERT的初始标准差为什么是0.02？",
    "description": "模型优化漫谈：BERT的初始标准差为什么是0.02？&para;\n原文链接: https://spaces.ac.cn/archives/8747\n发布日期: \n\n前几天在群里大家讨论到了“Transformer如何解决梯度消失”这个问题，答案有提到残差的，也有提到LN（Layer Norm）的。这些是否都是正确答案呢？事实上这是一个非常有趣而综合的问题，它其实关联到挺多模型细节，比如“BERT为...",
    "date": "2021-11-08",
    "source": "",
    "tags": [
      "模型",
      "分析",
      "优化",
      "梯度",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 5
  },
  {
    "slug": "bert4keras在手baseline我有clue基准代码",
    "title": "bert4keras在手，baseline我有：CLUE基准代码",
    "description": "bert4keras在手，baseline我有：CLUE基准代码&para;\n原文链接: https://spaces.ac.cn/archives/8739\n发布日期: \n\nCLUE（Chinese GLUE）是中文自然语言处理的一个评价基准，目前也已经得到了较多团队的认可。CLUE官方Github提供了tensorflow和pytorch的baseline，但并不易读，而且也不方便调试。事实上...",
    "date": "2021-10-31",
    "source": "",
    "tags": [
      "模型",
      "代码",
      "keras",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 4
  },
  {
    "slug": "can借助先验分布提升分类性能的简单后处理技巧",
    "title": "CAN：借助先验分布提升分类性能的简单后处理技巧",
    "description": "CAN：借助先验分布提升分类性能的简单后处理技巧&para;\n原文链接: https://spaces.ac.cn/archives/8728\n发布日期: \n\n顾名思义，本文将会介绍一种用于分类问题的后处理技巧——CAN（Classification with Alternating Normalization），出自论文《When in Doubt: Improving Classificati...",
    "date": "2021-10-22",
    "source": "",
    "tags": [
      "模型",
      "概率",
      "分析",
      "技巧",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 3
  },
  {
    "slug": "初始化方法中非方阵的维度平均策略思考",
    "title": "初始化方法中非方阵的维度平均策略思考",
    "description": "初始化方法中非方阵的维度平均策略思考&para;\n原文链接: https://spaces.ac.cn/archives/8725\n发布日期: \n\n在《从几何视角来理解模型参数的初始化策略》、《浅谈Transformer的初始化、参数化与标准化》等文章，我们讨论过模型的初始化方法，大致的思路是：如果一个$n\\times n$的方阵用均值为0、方差为$1/n$的独立同分布初始化，那么近似于一个正交矩...",
    "date": "2021-10-18",
    "source": "",
    "tags": [
      "模型",
      "优化",
      "梯度",
      "生成模型",
      "attention"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 2
  },
  {
    "slug": "用狄拉克函数来构造非光滑函数的光滑近似",
    "title": "用狄拉克函数来构造非光滑函数的光滑近似",
    "description": "用狄拉克函数来构造非光滑函数的光滑近似&para;\n原文链接: https://spaces.ac.cn/archives/8718\n发布日期: \n\n在机器学习中，我们经常会碰到不光滑的函数，但我们的优化方法通常是基于梯度的，这意味着光滑的模型可能更利于优化（梯度是连续的），所以就有了寻找非光滑函数的光滑近似的需求。事实上，本博客已经多次讨论过相关主题，比如《寻求一个光滑的最大值函数》、《函数光滑...",
    "date": "2021-10-10",
    "source": "",
    "tags": [
      "函数",
      "近似",
      "分析",
      "光滑",
      "生成模型"
    ],
    "status": "pending",
    "tags_reviewed": false,
    "post_number": 1
  }
]