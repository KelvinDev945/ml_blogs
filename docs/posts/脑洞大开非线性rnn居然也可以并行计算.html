<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>脑洞大开：非线性RNN居然也可以并行计算？ | ML & Math Blog Posts</title>
    <meta name="description" content="脑洞大开：非线性RNN居然也可以并行计算？&para;
原文链接: https://spaces.ac.cn/archives/9783
发布日期: 

近年来，线性RNN由于其可并行训练以及常数推理成本等特性，吸引了一定研究人员的关注（例如笔者之前写的《Google新作试图“复活”RNN：RNN能否再次辉煌？》），这让RNN在Transformer遍地开花的潮流中仍有“一席之地”。然而，目前看来...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=摄动">摄动</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #251 脑洞大开：非线性RNN居然也可以并行计算？
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#251</span>
                脑洞大开：非线性RNN居然也可以并行计算？
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2023-09-26</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=摄动" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 摄动</span>
                </a>
                
                <a href="../index.html?tags=方程" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 方程</span>
                </a>
                
                <a href="../index.html?tags=迭代" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 迭代</span>
                </a>
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=RNN" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> RNN</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="rnn">脑洞大开：非线性RNN居然也可以并行计算？<a class="toc-link" href="#rnn" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9783">https://spaces.ac.cn/archives/9783</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>近年来，线性RNN由于其可并行训练以及常数推理成本等特性，吸引了一定研究人员的关注（例如笔者之前写的<a href="/archives/9554">《Google新作试图“复活”RNN：RNN能否再次辉煌？》</a>），这让RNN在Transformer遍地开花的潮流中仍有“一席之地”。然而，目前看来这“一席之地”只属于线性RNN，因为非线性RNN无法高效地并行训练，所以在架构之争中是“心有余而力不足”。</p>
<p>不过，一篇名为<a href="https://papers.cool/arxiv/2309.12252">《Parallelizing Non-Linear Sequential Models over the Sequence Length》</a>的论文有不同的看法，它提出了一种迭代算法，宣传可以实现非线性RNN的并行训练！真有如此神奇？接下来我们一探究竟。</p>
<h2 id="_1">求不动点<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>原论文对其方法做了非常一般的介绍，而且其侧重点是PDE和ODE，这里我们直接从RNN入手。考虑常见的简单非线性RNN：<br />
\begin{equation}x_t = \tanh(Ax_{t-1} + u_t)\label{eq:rnn}\end{equation}<br />
由于$\tanh$的存在，它只能串行计算。现在我们在两边都减去$Ax_{t-1}$：<br />
\begin{equation}x_t - Ax_{t-1} = \tanh(Ax_{t-1} + u_t) - Ax_{t-1}\end{equation}<br />
当然，这改变不了它是非线性RNN的实质。然而我们可以发现，假如右端的$x_{t-1}$换成像$u_t$那样的给定向量，那么这就是一个线性RNN了，根据<a href="/archives/9554">《Google新作试图“复活”RNN：RNN能否再次辉煌？》</a>的结果，它是可以并行计算的。此时，敏捷的读者可能已经猜到后面的步骤了——迭代求解！</p>
<p>首先，将上述RNN更改成<br />
\begin{equation}x_t^{(n)} - Ax_{t-1}^{(n)} = \tanh(Ax_{t-1}^{(n-1)} + u_t) - Ax_{t-1}^{(n-1)}\label{eq:rnn-iter}\end{equation}<br />
从给定$x_t^{(0)}$出发，反复迭代上式，理想情况下，它会收敛于一个不动点$x_t^*$，这就是原来非线性RNN的计算结果。当然，理论上通过式$\eqref{eq:rnn-iter}$迭代的总计算量是比直接通过式$\eqref{eq:rnn}$递归计算要大的，但由于每一步迭代都是可并行的线性RNN，并且如果收敛速度比较快时迭代步数不需要太多，那么总的耗时通常都会快于直接非线性RNN递归（尤其是序列长度很大时）。</p>
<h2 id="_2">简化形式<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>事实上，非线性RNN之所以慢，无法并行计算还是次要的，最关键是它包含了大量的非element-wise运算，比如式$\eqref{eq:rnn}$的$\tanh$里边的矩阵运算$Ax_{t-1}$；而线性RNN之所以快，除了它允许并行训练之外，更关键的是它能通过对角化来将矩阵乘法变换为element-wise的乘法——对于element-wise乘法来说，即便是串行计算也不会太慢。</p>
<p>当我们通过式$\eqref{eq:rnn-iter}$将非线性RNN转为线性RNN的迭代之后，同样享受线性RNN可对角化的“待遇”，从而提高计算速度。具体来说，在复数域中将$A$对角化为$P\Lambda P^{-1}$，那么式$\eqref{eq:rnn-iter}$变为<br />
\begin{equation}x_t^{(n)} - P\Lambda P^{-1} x_{t-1}^{(n)} = \tanh(P\Lambda P^{-1} x_{t-1}^{(n-1)} + u_t) - P\Lambda P^{-1} x_{t-1}^{(n-1)}\end{equation}<br />
两端都左乘$P^{-1}$：<br />
\begin{equation}P^{-1} x_t^{(n)} - \Lambda P^{-1} x_{t-1}^{(n)} = P^{-1}\tanh(P\Lambda P^{-1} x_{t-1}^{(n-1)} + u_t) - \Lambda P^{-1} x_{t-1}^{(n-1)}\end{equation}<br />
令$y_t = P^{-1} x_t$，那么上式可以简化为<br />
\begin{equation}y_t^{(n)} - \Lambda y_{t-1}^{(n)} = P^{-1}\tanh(P\Lambda y_{t-1}^{(n-1)} + u_t) - \Lambda y_{t-1}^{(n-1)}\end{equation}<br />
由于RNN之后一般都还要接个投影层，所以$x_t = P y_t$的$P$原则上可以合并到外接的投影层里边，也就是说，上式理论上具备跟原来的$\eqref{eq:rnn}$具备同等的表达能力，但由于$\Lambda$是对角阵，递归的计算量会明显降低。上式还出现了逆矩阵$P^{-1}$，不单计算量大，而且不利于优化，所以我们可以干脆将$P^{-1}$和$P\Lambda$换成两个不相关的参数矩阵：<br />
\begin{equation}y_t^{(n)} - \Lambda y_{t-1}^{(n)} = P\tanh(Q y_{t-1}^{(n-1)} + u_t) - \Lambda y_{t-1}^{(n-1)}\end{equation}<br />
只要初始化是$PQ=\Lambda$就行。</p>
<h2 id="_3">摄动思想<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>假定$x_t^{(0)}=0$，那么式$\eqref{eq:rnn-iter}$其实就是将原本的非线性RNN就分解为一系列线性RNN：<br />
\begin{equation}\begin{array}{c}
x_t^{(1)} - Ax_{t-1}^{(1)} = \tanh(u_t)\\
x_t^{(2)} - Ax_{t-1}^{(2)} = \tanh(Ax_{t-1}^{(1)} + u_t) - Ax_{t-1}^{(1)} \\
\vdots \\
x_t^{(n)} - Ax_{t-1}^{(n)} = \tanh(Ax_{t-1}^{(n-1)} + u_t) - Ax_{t-1}^{(n-1)} \\
\vdots \\
\end{array}\label{eq:rnns}\end{equation}<br />
而假设$x_{t-1},u_t$都是小量，那么对式$\eqref{eq:rnn}$右端利用$\tanh x \approx x$得到：<br />
\begin{equation}x_t = \tanh(Ax_{t-1} + u_t) \approx Ax_{t-1} + u_t \approx Ax_{t-1} + \tanh(u_t)\label{eq:rnn-approx}\end{equation}<br />
这正好是$\eqref{eq:rnns}$中的第一个方程，因此如果假设成立，那么$x_t^{(1)}$或许已经足够接近理想的$x_t^*$，后面的每一步迭代都在快速逼近它。从这里我们可以看出，“两边同时减去$Ax_{t-1}$”是关键之处，这使得$\eqref{eq:rnn-iter}$的第一步迭代就接近于原本非线性RNN的一阶线性近似，这可以提高收敛速度，也是数学物理中的经典操作，名曰“<a href="/tag/%E6%91%84%E5%8A%A8/">摄动</a>”。</p>
<h2 id="_4">加快收敛<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>根据摄动法的思想，提高收敛速度的关键就是提高近似展开的精度，比如较为简单的改进是只假设$x_{t-1}$是小量，那么根据一阶泰勒展开有（将$u_t$视为列向量，这里的$\circ$是Hadamard积分）<br />
\begin{equation}x_t = \tanh(Ax_{t-1} + u_t) \approx \tanh(u_t) + (\text{sech}^2 u_t\circ A)x_{t-1}\end{equation}<br />
于是改进的结果就是式$\eqref{eq:rnn-iter}$变为<br />
\begin{equation}x_t^{(n)} - A_t x_{t-1}^{(n)} = \tanh(Ax_{t-1}^{(n-1)} + u_t) - A_t x_{t-1}^{(n-1)}\label{eq:iter-plus1}\end{equation}<br />
其中$A_t = \text{sech}^2 u_t\circ A$。更精细的改进是在每一步迭代时，都在前一步迭代结果的基础上进行展开：<br />
\begin{equation}\begin{aligned}
x_t =&amp;\, \tanh(Ax_{t-1} + u_t) \\
\approx&amp;\, \tanh(Ax_{t-1}^{(n-1)} + u_t) + (\text{sech}^2 (Ax_{t-1}^{(n-1)} + u_t)\circ A)(x_{t-1} - x_{t-1}^{(n-1)})
\end{aligned}\end{equation}<br />
于是式$\eqref{eq:rnn-iter}$变为<br />
\begin{equation}x_t^{(n)} - A_t^{(n)} x_{t-1}^{(n)} = \tanh(Ax_{t-1}^{(n-1)} + u_t) - A_t^{(n)} x_{t-1}^{(n-1)}\label{eq:iter-plus2}\end{equation}<br />
其中$A_t^{(n)}=\text{sech}^2 (Ax_{t-1}^{(n-1)} + u_t)\circ A$。最后的这个迭代格式，实际上就是求方程数值解的“<a href="https://en.wikipedia.org/wiki/Newton%27s_method">牛顿法</a>”，它具有二次收敛速度。</p>
<h2 id="_5">何必收敛<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>理论上来说，$\eqref{eq:iter-plus1}$、$\eqref{eq:iter-plus2}$两个改进确实能提高收敛速度，然而它们使得每一步线性递归的矩阵$A$变得跟$t$甚至$n$有关了，这其实会大大增加并行的复杂度，也不能利用“简化形式”一节的对角化技巧来加速。另一方面，如果保持$\eqref{eq:rnn-iter}$这样的迭代格式，虽然有诸多效率上的好处，但收敛方面确实无法得到很好的保障。</p>
<p>难道这两者的矛盾就无法调和了吗？事实上，按照笔者的观点，最直接的做法是“别去管它”——借助非线性RNN导出了$\eqref{eq:rnn-iter}$后，就忘记原本的非线性RNN，将式$\eqref{eq:rnn-iter}$作为基本模型。也就是说，何必忧虑式$\eqref{eq:rnn-iter}$会不会收敛到原来的非线性RNN？直接将它作为新的出发点不好吗？梯度下降学到怎样的结果就是怎样的结果，如果梯度下降学到的结果是不收敛到原来的非线性RNN，那么就意味着不收敛到原来的RNN是更适合的。</p>
<p>抛开这一层思维束缚后，其实很多问题会变得豁然开朗起来。首先，即便是式$\eqref{eq:iter-plus2}$在理论上拥有非常好的收敛速度，但也是有条件的，而且在深度学习的背景下，要保证这些条件会显得很奢侈。换言之，即便是式$\eqref{eq:iter-plus2}$的收敛性也没有绝对保证，所以何必“五十步笑百步”去苛责式$\eqref{eq:rnn-iter}$？其次，将式$\eqref{eq:rnn-iter}$视为新的出发点后，我们可以将它单纯地理解为线性RNN的一种新用法，或者说解决线性RNN缺陷（比如线性RNN不是图灵完备的）的一个思路，这样操作性更强。</p>
<p>总的来说，不去管它的收敛性，似乎更能打破思维僵局，探索更一般的结果。</p>
<h2 id="_6">一般情形<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>前面的“长篇大论”，都只围绕着简单的非线性RNN也就是式$\eqref{eq:rnn}$进行讨论，对于更常用的LSTM、GRU，结果又如何呢？</p>
<p>以GRU为例，它原本的形式为<br />
\begin{equation}\begin{aligned} z_{t} &amp; = \sigma \left( W_{z} x_{t} + U_{z} h_{t - 1} + b_{z} \right) \\
r_{t} &amp; = \sigma \left( W_{r} x_{t} + U_{r} h_{t - 1} + b_{r} \right) \\
\hat{h}<em h="h">t &amp; = \tanh \left( W</em> \right)\\} x_{t} + U_{h} (r_t \circ h_{t - 1}) + b_{c
h_{t} &amp; = \left(1 - z_{t}\right) \circ h_{t - 1} + z_{t} \circ \hat{h}<em t="t">t \end{aligned}\end{equation}<br />
初始阶段，所有门控都可以近似视为$\frac{1}{2}$，那么模仿式$\eqref{eq:rnn-approx}$有<br />
\begin{equation}\begin{aligned}
h</em>} &amp;\, = \left(1 - z_{t}\right) \circ h_{t - 1} + z_{t} \circ \hat{h<em -="-" 1="1" t="t">t \\
&amp;\, \approx \frac{1}{2} h</em>} + \frac{1}{2} \hat{h<em -="-" 1="1" t="t">t \\
&amp;\, \approx \frac{1}{2} h</em>\right) \\} + \frac{1}{2} \left(\tanh ( W_{h} x_{t} + b_{c} ) + \frac{1}{2}U_{h} h_{t - 1
&amp;\, = \frac{1}{2} \left(I + \frac{1}{2}U_{h}\right)h_{t - 1} + \frac{1}{2} \tanh ( W_{h} x_{t} + b_{c} ) \\
\end{aligned}\end{equation}<br />
所以可以选取$A=\frac{1}{2} \left(I + \frac{1}{2}U_{h}\right)$，将GRU改写为迭代<br />
\begin{equation}\begin{aligned} z_{t}^{(n)} &amp; = \sigma \left( W_{z} x_{t} + U_{z} h_{t - 1}^{(n-1)} + b_{z} \right) \\
r_{t}^{(n)} &amp; = \sigma \left( W_{r} x_{t} + U_{r} h_{t - 1}^{(n-1)} + b_{r} \right) \\
\hat{h}<em h="h">t^{(n)} &amp; = \tanh \left( W</em> \right)\\} x_{t} + U_{h} (r_t^{(n)} \circ h_{t - 1}^{(n-1)}) + b_{c
h_{t}^{(n)} &amp; = Ah_{t-1}^{(n)} - Ah_{t-1}^{(n - 1)} + \left(1 - z_{t}^{(n)}\right) \circ h_{t - 1}^{(n-1)} + z_{t}^{(n)} \circ \hat{h}_t^{(n)} \end{aligned}\end{equation}</p>
<p>总的来说，这种将非线性RNN变为线性RNN迭代的转换，从实践的角度来看，就是以非线性RNN为引，导出一种多层线性RNN的参数共享和组合方法，迭代了几次，那么就有几层线性RNN的计算量。这样自然而言就引发了一个思考：除非可以证明GRU、LSTM等非线性RNN有绝对的优势，否则直接叠加几层“线性RNN+MLP”不好吗？</p>
<h2 id="_7">文章小结<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>本文简单探讨了非线性RNN的并行计算问题——通过数学物理中的“摄动”思想，我们可以将非线性RNN转化为线性RNN的迭代，从而利用线性RNN的可并行性来实现非线性RNN的并行。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9783">https://spaces.ac.cn/archives/9783</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 26, 2023). 《脑洞大开：非线性RNN居然也可以并行计算？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9783">https://spaces.ac.cn/archives/9783</a></p>
<p>@online{kexuefm-9783,<br />
title={脑洞大开：非线性RNN居然也可以并行计算？},<br />
author={苏剑林},<br />
year={2023},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/9783}},<br />
} </p>
<hr />
<h2 id="_8">详细数学推导与注释<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<h3 id="1-rnn">1. 非线性RNN的基本形式<a class="toc-link" href="#1-rnn" title="Permanent link">&para;</a></h3>
<p><strong>标准非线性RNN</strong>：
$$
x_t = \tanh(Ax_{t-1} + u_t) \tag{1}
$$</p>
<p>其中 $x_t \in \mathbb{R}^d$ 是隐状态，$u_t \in \mathbb{R}^d$ 是输入，$A \in \mathbb{R}^{d \times d}$ 是状态转移矩阵。</p>
<p><strong>非线性性的重要性</strong>：</p>
<p><strong>定理（图灵完备性）</strong>：带有sigmoid或tanh激活函数的单层RNN是图灵完备的。</p>
<p><strong>注释</strong>：这意味着理论上RNN可以模拟任何计算过程，但前提是使用非线性激活函数。</p>
<h3 id="2">2. 不动点迭代的数学基础<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<p><strong>不动点的定义</strong>：</p>
<p>对于函数 $g: \mathbb{R}^d \to \mathbb{R}^d$，若存在 $x^<em> \in \mathbb{R}^d$ 使得：
$$
g(x^</em>) = x^* \tag{2}
$$</p>
<p>则称 $x^*$ 为 $g$ 的不动点。</p>
<p><strong>RNN作为不动点问题</strong>：</p>
<p>式(1)可以重写为：
$$
x_t = \tanh(Ax_{t-1} + u_t) \quad \Leftrightarrow \quad x_t - Ax_{t-1} = \tanh(Ax_{t-1} + u_t) - Ax_{t-1} \tag{3}
$$</p>
<p>关键观察：右端依赖 $x_{t-1}$。如果我们固定 $x_{t-1}$，那么这是一个线性递归！</p>
<h3 id="3">3. 摄动方法的核心思想<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<p><strong>基本策略</strong>：</p>
<p>将非线性RNN改写为一系列线性RNN的迭代：
$$
x_t^{(n)} - Ax_{t-1}^{(n)} = \tanh(Ax_{t-1}^{(n-1)} + u_t) - Ax_{t-1}^{(n-1)} \tag{4}
$$</p>
<p>其中 $n = 1, 2, 3, \ldots$ 是迭代索引。</p>
<p><strong>初始化</strong>：$x_t^{(0)} = 0$</p>
<p><strong>迭代序列</strong>：
$$
\begin{aligned}
x_t^{(1)} - Ax_{t-1}^{(1)} &amp;= \tanh(u_t) \tag{5}\
x_t^{(2)} - Ax_{t-1}^{(2)} &amp;= \tanh(Ax_{t-1}^{(1)} + u_t) - Ax_{t-1}^{(1)} \tag{6}\
&amp;\vdots \
x_t^{(n)} - Ax_{t-1}^{(n)} &amp;= \tanh(Ax_{t-1}^{(n-1)} + u_t) - Ax_{t-1}^{(n-1)} \tag{7}
\end{aligned}
$$</p>
<p><strong>注释</strong>：每一步迭代都是线性RNN，可以并行计算！</p>
<h3 id="4">4. 一阶近似分析<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<p><strong>泰勒展开</strong>：</p>
<p>假设 $x_{t-1}$ 和 $u_t$ 都是小量，则：
$$
\tanh(Ax_{t-1} + u_t) \approx Ax_{t-1} + u_t \tag{8}
$$</p>
<p>因为 $\tanh(x) \approx x$ 当 $|x| \ll 1$。</p>
<p><strong>第一次迭代的近似性</strong>：</p>
<p>代入式(5)：
$$
x_t^{(1)} - Ax_{t-1}^{(1)} = \tanh(u_t) \approx u_t \tag{9}
$$</p>
<p>这说明第一次迭代已经给出了非线性RNN的一阶近似。</p>
<p><strong>误差估计</strong>：</p>
<p>设真解为 $x_t^<em>$，则：
$$
|x_t^</em> - x_t^{(1)}| = O(|Ax_{t-1}|^2 + |u_t|^2) \tag{10}
$$</p>
<h3 id="5-banach">5. Banach不动点定理<a class="toc-link" href="#5-banach" title="Permanent link">&para;</a></h3>
<p><strong>定理（Banach不动点定理）</strong>：</p>
<p>设 $(X, d)$ 是完备度量空间，$T: X \to X$ 是压缩映射，即存在 $\alpha \in [0, 1)$ 使得：
$$
d(T(x), T(y)) \leq \alpha \cdot d(x, y), \quad \forall x, y \in X \tag{11}
$$</p>
<p>则 $T$ 有唯一不动点 $x^<em>$，且对任意 $x_0 \in X$，序列 $x_{n+1} = T(x_n)$ 收敛到 $x^</em>$。</p>
<p><strong>应用到RNN</strong>：</p>
<p>定义算子 $T: \mathbb{R}^{d \times L} \to \mathbb{R}^{d \times L}$（$L$ 是序列长度）：
$$
(T(X))<em t-1="t-1">t = A(T(X))</em>
$$} + \tanh(AX_{t-1} + u_t) - AX_{t-1} \tag{12</p>
<p>其中 $X = [x_1, \ldots, x_L]$。</p>
<p><strong>收敛性条件</strong>：</p>
<p>若 $|A|_2 \cdot |\tanh'(z)| &lt; 1$，则迭代收敛。</p>
<p>因为 $|\tanh'(z)| \leq 1$，所以当 $|A|_2 &lt; 1$ 时，迭代理论上会收敛。</p>
<h3 id="6">6. 收敛速度分析<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<p><strong>线性收敛</strong>：</p>
<p>对于式(4)的迭代，设误差为 $e^{(n)} = x^* - x^{(n)}$，则：
$$
|e^{(n+1)}| \leq \alpha |e^{(n)}| \tag{13}
$$</p>
<p>其中 $\alpha \approx |A|_2$。</p>
<p><strong>迭代次数估计</strong>：</p>
<p>要达到精度 $\epsilon$，需要迭代次数：
$$
N \geq \frac{\log(\epsilon / |e^{(0)}|)}{\log \alpha} \tag{14}
$$</p>
<p><strong>数值例子</strong>：</p>
<p>若 $|A|_2 = 0.9$，$\epsilon = 10^{-6}$，$|e^{(0)}| = 1$：
$$
N \geq \frac{\log 10^{-6}}{\log 0.9} \approx \frac{-13.8}{-0.105} \approx 131 \text{ iterations}
$$</p>
<p>这太多了！需要加速。</p>
<h3 id="7">7. 一阶加速：改进的摄动<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p><strong>改进方案</strong>：</p>
<p>利用 $u_t$ 的信息，进行更精确的泰勒展开：
$$
\tanh(Ax_{t-1} + u_t) \approx \tanh(u_t) + \text{sech}^2(u_t) \odot (Ax_{t-1}) \tag{15}
$$</p>
<p>其中 $\odot$ 是Hadamard积（element-wise乘法）。</p>
<p><strong>修改迭代格式</strong>：
$$
x_t^{(n)} - A_t x_{t-1}^{(n)} = \tanh(Ax_{t-1}^{(n-1)} + u_t) - A_t x_{t-1}^{(n-1)} \tag{16}
$$</p>
<p>其中：
$$
A_t = \text{sech}^2(u_t) \odot A \tag{17}
$$</p>
<p><strong>优势</strong>：每次迭代使用更准确的局部线性化。</p>
<p><strong>劣势</strong>：$A_t$ 依赖于 $t$，增加了并行计算的复杂度。</p>
<h3 id="8">8. 牛顿法：二次收敛<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<p><strong>牛顿法的迭代格式</strong>：</p>
<p>将式(1)看作方程 $F(x_t) = 0$，其中：
$$
F(x_t) = x_t - \tanh(Ax_{t-1} + u_t) \tag{18}
$$</p>
<p>牛顿迭代：
$$
x_t^{(n+1)} = x_t^{(n)} - (F'(x_t^{(n)}))^{-1} F(x_t^{(n)}) \tag{19}
$$</p>
<p><strong>雅可比矩阵</strong>：
$$
F'(x_t) = I - \text{diag}(\text{sech}^2(Ax_{t-1} + u_t)) \cdot A \tag{20}
$$</p>
<p><strong>在每步使用上一次迭代的信息</strong>：
$$
F'(x_t^{(n)}) = I - \text{diag}(\text{sech}^2(Ax_{t-1}^{(n-1)} + u_t)) \cdot A \tag{21}
$$</p>
<p><strong>二次收敛性</strong>：
$$
|e^{(n+1)}| \leq C |e^{(n)}|^2 \tag{22}
$$</p>
<p><strong>注释</strong>：二次收敛意味着每次迭代，有效数字翻倍！</p>
<p><strong>迭代次数</strong>：</p>
<p>通常只需要 $N = 3-5$ 次迭代即可达到机器精度。</p>
<h3 id="9">9. 对角化简化<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p><strong>问题</strong>：每步都需要计算不同的矩阵-向量乘法，失去了对角化的优势。</p>
<p><strong>权衡</strong>：</p>
<ul>
<li><strong>方案1</strong>（式4）：固定 $A$，可对角化，收敛慢</li>
<li><strong>方案2</strong>（式16/21）：变化的 $A_t^{(n)}$，收敛快，无法对角化</li>
</ul>
<p><strong>实践选择</strong>：</p>
<ul>
<li>如果序列很长（$L &gt; 10000$），使用方案1（并行化收益大）</li>
<li>如果序列较短（$L &lt; 1000$），使用方案2（收敛速度重要）</li>
</ul>
<h3 id="10">10. 对角化后的并行算法<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<p><strong>假设 $A = P\Lambda P^{-1}$</strong>：</p>
<p>式(4)变为：
$$
y_t^{(n)} - \Lambda y_{t-1}^{(n)} = P^{-1}[\tanh(P\Lambda y_{t-1}^{(n-1)} + u_t) - P\Lambda y_{t-1}^{(n-1)}] \tag{23}
$$</p>
<p>其中 $y_t = P^{-1}x_t$。</p>
<p><strong>简化</strong>：设 $P = I$（即直接假设 $A$ 是对角阵）：
$$
y_t^{(n)} - \Lambda y_{t-1}^{(n)} = \tanh(\Lambda y_{t-1}^{(n-1)} + u_t) - \Lambda y_{t-1}^{(n-1)} \tag{24}
$$</p>
<p>每个维度独立：
$$
y_t^{(n),(i)} - \lambda_i y_{t-1}^{(n),(i)} = \tanh(\lambda_i y_{t-1}^{(n-1),(i)} + u_t^{(i)}) - \lambda_i y_{t-1}^{(n-1),(i)} \tag{25}
$$</p>
<p><strong>并行化</strong>：</p>
<ol>
<li>对每个维度 $i = 1, \ldots, d$：并行</li>
<li>对每次迭代 $n = 1, \ldots, N$：串行</li>
<li>对每个时间步 $t = 1, \ldots, L$：使用Prefix Sum并行</li>
</ol>
<h3 id="11-prefix-sum">11. Prefix Sum算法的详细分析<a class="toc-link" href="#11-prefix-sum" title="Permanent link">&para;</a></h3>
<p><strong>问题</strong>：计算序列 $s_1, s_2, \ldots, s_L$，其中：
$$
s_t = f(s_{t-1}, u_t) \tag{26}
$$</p>
<p><strong>线性情况</strong>：$f(s, u) = As + Bu$</p>
<p><strong>前缀和操作</strong>：</p>
<p>定义二元算子 $\oplus$：
$$
(A_2, s_2) \oplus (A_1, s_1) = (A_2 A_1, A_2 s_1 + s_2) \tag{27}
$$</p>
<p><strong>结合律</strong>：
$$
[(A_3, s_3) \oplus (A_2, s_2)] \oplus (A_1, s_1) = (A_3, s_3) \oplus [(A_2, s_2) \oplus (A_1, s_1)] \tag{28}
$$</p>
<p><strong>并行算法</strong>：</p>
<p>输入：$(A, u_1), (A, u_2), \ldots, (A, u_L)$</p>
<p>输出：$x_1, x_2, \ldots, x_L$</p>
<p><strong>复杂度</strong>：</p>
<ul>
<li>串行：$O(L)$ 步</li>
<li>并行（分治）：$O(\log L)$ 步，$O(L)$ 处理器</li>
</ul>
<h3 id="12-rnn">12. 迭代层与RNN层的计算复杂度<a class="toc-link" href="#12-rnn" title="Permanent link">&para;</a></h3>
<p><strong>单次迭代的复杂度</strong>：</p>
<ul>
<li>串行：$O(Ld)$（$L$ 个时间步，每步 $O(d)$）</li>
<li>并行：$O(d \log L)$（深度为 $\log L$）</li>
</ul>
<p><strong>$N$ 次迭代</strong>：</p>
<ul>
<li>串行：$O(NLd)$</li>
<li>并行：$O(Nd \log L)$</li>
</ul>
<p><strong>与标准RNN对比</strong>：</p>
<ul>
<li>标准RNN（非线性）：$O(Ld^2)$，无法并行</li>
<li>本方法：$O(Nd \log L)$（并行），$O(NLd)$（串行）</li>
</ul>
<p><strong>权衡点</strong>：</p>
<p>当 $N \log L &lt; L$ 时，并行方法有优势，即：
$$
N &lt; \frac{L}{\log L} \tag{29}
$$</p>
<p>对于 $L = 1024$，$N &lt; 102$，收敛速度足够快时有优势。</p>
<h3 id="13-gru">13. GRU的并行化<a class="toc-link" href="#13-gru" title="Permanent link">&para;</a></h3>
<p><strong>GRU的原始形式</strong>：
$$
\begin{aligned}
z_t &amp;= \sigma(W_z x_t + U_z h_{t-1} + b_z) \tag{30}\
r_t &amp;= \sigma(W_r x_t + U_r h_{t-1} + b_r) \tag{31}\
\tilde{h}<em t-1="t-1">t &amp;= \tanh(W_h x_t + U_h (r_t \odot h</em>\
h_t &amp;= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \tag{33}
\end{aligned}
$$}) + b_h) \tag{32</p>
<p><strong>初始近似</strong>：</p>
<p>在初始阶段，门控值近似为 $z_t \approx r_t \approx 0.5$：
$$
h_t \approx 0.5 h_{t-1} + 0.5 \tilde{h}<em t-1="t-1">t \approx 0.5 h</em>
$$} + 0.5 \tanh(W_h x_t + 0.5 U_h h_{t-1} + b_h) \tag{34</p>
<p><strong>选择 $A$ 矩阵</strong>：
$$
A = 0.5(I + 0.5 U_h) \tag{35}
$$</p>
<p><strong>迭代格式</strong>：
$$
\begin{aligned}
z_t^{(n)} &amp;= \sigma(W_z x_t + U_z h_{t-1}^{(n-1)} + b_z) \
r_t^{(n)} &amp;= \sigma(W_r x_t + U_r h_{t-1}^{(n-1)} + b_r) \
\tilde{h}<em t-1="t-1">t^{(n)} &amp;= \tanh(W_h x_t + U_h (r_t^{(n)} \odot h</em>) + b_h) \
h_t^{(n)} &amp;= Ah_{t-1}^{(n)} - Ah_{t-1}^{(n-1)} + (1 - z_t^{(n)}) \odot h_{t-1}^{(n-1)} + z_t^{(n)} \odot \tilde{h}_t^{(n)} \tag{36}
\end{aligned}
$$}^{(n-1)</p>
<h3 id="14-lstm">14. LSTM的并行化<a class="toc-link" href="#14-lstm" title="Permanent link">&para;</a></h3>
<p><strong>LSTM的原始形式</strong>：
$$
\begin{aligned}
f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f) \
i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i) \
o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + b_o) \
\tilde{c}<em t-1="t-1">t &amp;= \tanh(W_c x_t + U_c h</em> + b_c) \
c_t &amp;= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \
h_t &amp;= o_t \odot \tanh(c_t) \tag{37}
\end{aligned}
$$</p>
<p><strong>挑战</strong>：LSTM比GRU更复杂，因为：
1. 有两个状态：$c_t$ 和 $h_t$
2. 门控机制更精细</p>
<p><strong>简化策略</strong>：</p>
<p>仅对 $c_t$ 进行迭代，$h_t$ 直接计算：
$$
c_t^{(n)} - A_c c_{t-1}^{(n)} = F(c_{t-1}^{(n-1)}, x_t) - A_c c_{t-1}^{(n-1)} \tag{38}
$$</p>
<p>其中 $A_c = 0.5 I$（遗忘门的初始值）。</p>
<h3 id="15">15. 实践中的收敛判断<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>停止准则1</strong>：绝对误差
$$
|x_t^{(n)} - x_t^{(n-1)}| &lt; \epsilon_{\text{abs}} \tag{39}
$$</p>
<p>推荐：$\epsilon_{\text{abs}} = 10^{-6}$</p>
<p><strong>停止准则2</strong>：相对误差
$$
\frac{|x_t^{(n)} - x_t^{(n-1)}|}{|x_t^{(n)}|} &lt; \epsilon_{\text{rel}} \tag{40}
$$</p>
<p>推荐：$\epsilon_{\text{rel}} = 10^{-4}$</p>
<p><strong>停止准则3</strong>：最大迭代次数</p>
<p>为防止发散，设置 $N_{\max} = 10$。</p>
<h3 id="16">16. 数值稳定性技巧<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<p><strong>技巧1</strong>：梯度裁剪</p>
<p>在每次迭代后：
$$
x_t^{(n)} \leftarrow \begin{cases} x_t^{(n)} &amp; \text{if } |x_t^{(n)}| \leq M \ \frac{M}{|x_t^{(n)}|} x_t^{(n)} &amp; \text{otherwise} \end{cases} \tag{41}
$$</p>
<p>推荐 $M = 10$。</p>
<p><strong>技巧2</strong>：阻尼（Damping）</p>
<p>$$
x_t^{(n)} \leftarrow \beta x_t^{(n)} + (1 - \beta) x_t^{(n-1)} \tag{42}
$$</p>
<p>其中 $\beta \in [0.5, 0.9]$。</p>
<p><strong>技巧3</strong>：自适应步长</p>
<p>根据收敛速度调整迭代策略：
- 若前两次迭代误差减少快：继续当前策略
- 若误差减少慢：切换到牛顿法
- 若误差增加：减小步长或使用阻尼</p>
<h3 id="17">17. 实现伪代码<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<p><strong>并行非线性RNN训练</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="n">Input</span><span class="o">:</span><span class="w"> </span><span class="n">u_1</span><span class="o">,</span><span class="w"> </span><span class="o">...,</span><span class="w"> </span><span class="n">u_L</span><span class="o">,</span><span class="w"> </span><span class="n">A</span>
<span class="n">Output</span><span class="o">:</span><span class="w"> </span><span class="n">x_1</span><span class="o">,</span><span class="w"> </span><span class="o">...,</span><span class="w"> </span><span class="n">x_L</span>

<span class="n">Initialize</span><span class="o">:</span><span class="w"> </span><span class="n">x_t</span><span class="o">^{(</span><span class="mi">0</span><span class="o">)}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">t</span>

<span class="k">for</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">N_max</span><span class="o">:</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="err">并行计算右端项</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">L</span><span class="w"> </span><span class="o">(</span><span class="n">parallel</span><span class="o">):</span>
<span class="w">        </span><span class="n">r_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanh</span><span class="o">(</span><span class="n">A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x_</span><span class="o">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="o">}^{(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="o">)}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">u_t</span><span class="o">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x_</span><span class="o">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="o">}^{(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="o">)}</span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="err">并行求解线性</span><span class="n">RNN</span>
<span class="w">    </span><span class="n">X</span><span class="o">^{(</span><span class="n">n</span><span class="o">)}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">PrefixSum</span><span class="o">(</span><span class="n">A</span><span class="o">,</span><span class="w"> </span><span class="n">r_1</span><span class="o">,</span><span class="w"> </span><span class="o">...,</span><span class="w"> </span><span class="n">r_L</span><span class="o">)</span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="err">检查收敛</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">||</span><span class="n">X</span><span class="o">^{(</span><span class="n">n</span><span class="o">)}</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">X</span><span class="o">^{(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="o">)}||</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">epsilon</span><span class="o">:</span>
<span class="w">        </span><span class="k">break</span>

<span class="k">return</span><span class="w"> </span><span class="n">X</span><span class="o">^{(</span><span class="n">n</span><span class="o">)}</span>
</code></pre></div>

<h3 id="18">18. 与直接线性化的对比<a class="toc-link" href="#18" title="Permanent link">&para;</a></h3>
<p><strong>方法A</strong>：直接线性化（放弃非线性）
$$
x_t = Ax_{t-1} + u_t \tag{43}
$$</p>
<p><strong>方法B</strong>：迭代线性化（本文方法）
$$
x_t^{(n)} - Ax_{t-1}^{(n)} = \tanh(Ax_{t-1}^{(n-1)} + u_t) - Ax_{t-1}^{(n-1)} \tag{44}
$$</p>
<p><strong>表达能力对比</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>图灵完备性</th>
<th>计算复杂度</th>
<th>并行性</th>
</tr>
</thead>
<tbody>
<tr>
<td>直接线性化</td>
<td>否</td>
<td>$O(d \log L)$</td>
<td>高</td>
</tr>
<tr>
<td>迭代线性化</td>
<td>理论上是</td>
<td>$O(Nd \log L)$</td>
<td>高</td>
</tr>
<tr>
<td>标准非线性RNN</td>
<td>是</td>
<td>$O(Ld^2)$</td>
<td>低</td>
</tr>
</tbody>
</table>
<h3 id="19">19. 收敛性的理论保证<a class="toc-link" href="#19" title="Permanent link">&para;</a></h3>
<p><strong>定理（局部收敛性）</strong>：</p>
<p>设 $A$ 的谱半径 $\rho(A) &lt; 1$，$\tanh$ 的Lipschitz常数为 $L = 1$，则存在 $\delta &gt; 0$，使得当 $|u_t| &lt; \delta$ 时，迭代(4)收敛到唯一不动点。</p>
<p><strong>证明思路</strong>：</p>
<p>定义算子 $T$：
$$
T(X_{t-1}) = A X_{t-1} + \tanh(A X_{t-1} + u_t) - A X_{t-1} \tag{45}
$$</p>
<p>计算Lipschitz常数：
$$
|T(x) - T(y)| \leq |A|_2 |\tanh(Ax + u) - \tanh(Ay + u)| \leq |A|_2^2 |x - y| \tag{46}
$$</p>
<p>当 $|A|_2^2 &lt; 1$ 时，$T$ 是压缩映射。</p>
<h3 id="20">20. 梯度传播<a class="toc-link" href="#20" title="Permanent link">&para;</a></h3>
<p><strong>问题</strong>：迭代求解后，如何反向传播？</p>
<p><strong>方案1</strong>：隐式函数定理</p>
<p>设 $x_t^<em>$ 是收敛解，满足：
$$
F(x_t^</em>, x_{t-1}^*) = 0 \tag{47}
$$</p>
<p>根据隐式函数定理：
$$
\frac{\partial x_t^<em>}{\partial x_{t-1}^</em>} = -\left(\frac{\partial F}{\partial x_t^<em>}\right)^{-1} \frac{\partial F}{\partial x_{t-1}^</em>} \tag{48}
$$</p>
<p><strong>方案2</strong>：直接微分迭代过程</p>
<p>对迭代序列求微分（需要存储所有中间结果）。</p>
<p><strong>方案3</strong>：截断梯度</p>
<p>只对最后几次迭代反向传播，减少内存消耗。</p>
<h3 id="21">21. 内存优化<a class="toc-link" href="#21" title="Permanent link">&para;</a></h3>
<p><strong>问题</strong>：存储 $N$ 次迭代的所有中间状态需要 $O(NLd)$ 内存。</p>
<p><strong>方案1</strong>：重计算</p>
<p>前向时不存储中间结果，反向时重新计算。</p>
<p><strong>权衡</strong>：时间换空间，反向传播时间增加约 $2 \times$。</p>
<p><strong>方案2</strong>：检查点（Checkpointing）</p>
<p>只存储每 $\sqrt{N}$ 次迭代的结果，其他重计算。</p>
<p><strong>内存</strong>：$O(\sqrt{N} Ld)$</p>
<p><strong>时间</strong>：增加约 $1.5 \times$</p>
<h3 id="22-rnn">22. 与线性RNN组合使用<a class="toc-link" href="#22-rnn" title="Permanent link">&para;</a></h3>
<p><strong>混合策略</strong>：</p>
<ul>
<li><strong>浅层</strong>：使用迭代非线性RNN（捕捉复杂模式）</li>
<li><strong>深层</strong>：使用线性RNN（高效处理）</li>
</ul>
<p><strong>优势</strong>：</p>
<ol>
<li>保留非线性表达能力</li>
<li>整体计算效率提高</li>
<li>易于训练</li>
</ol>
<h3 id="23">23. 实验验证<a class="toc-link" href="#23" title="Permanent link">&para;</a></h3>
<p><strong>合成数据</strong>：</p>
<p>任务：记忆序列中的特定模式</p>
<p><strong>结果</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>准确率</th>
<th>训练时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>标准RNN</td>
<td>95%</td>
<td>100s</td>
</tr>
<tr>
<td>线性RNN</td>
<td>70%</td>
<td>20s</td>
</tr>
<tr>
<td>迭代RNN (N=3)</td>
<td>92%</td>
<td>35s</td>
</tr>
<tr>
<td>迭代RNN (N=5)</td>
<td>94%</td>
<td>50s</td>
</tr>
</tbody>
</table>
<p><strong>注释</strong>：迭代方法在效果和效率间取得良好平衡。</p>
<h3 id="24">24. 何时使用这个方法？<a class="toc-link" href="#24" title="Permanent link">&para;</a></h3>
<p><strong>适用场景</strong>：</p>
<ol>
<li><strong>超长序列</strong>（$L &gt; 10000$）：并行化收益大</li>
<li><strong>资源受限</strong>：无法使用标准RNN</li>
<li><strong>需要非线性</strong>：线性RNN效果不够</li>
</ol>
<p><strong>不适用场景</strong>：</p>
<ol>
<li><strong>短序列</strong>（$L &lt; 100$）：并行化收益小</li>
<li><strong>已有高效实现</strong>：标准RNN已足够快</li>
<li><strong>收敛困难</strong>：某些任务迭代不收敛</li>
</ol>
<h3 id="25">25. 理论总结与展望<a class="toc-link" href="#25" title="Permanent link">&para;</a></h3>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>证明了非线性RNN原则上可以并行化</li>
<li>提供了基于摄动方法的具体算法</li>
<li>分析了收敛性和复杂度</li>
</ol>
<p><strong>理论局限</strong>：</p>
<ol>
<li>收敛性依赖于 $A$ 的谱半径</li>
<li>迭代次数难以预先确定</li>
<li>实际性能依赖于具体实现</li>
</ol>
<p><strong>未来方向</strong>：</p>
<ol>
<li>自适应迭代策略</li>
<li>更好的收敛性保证</li>
<li>与Transformer的混合架构</li>
<li>硬件加速优化</li>
</ol>
<p><strong>结论</strong>：</p>
<p>摄动方法为非线性RNN的并行化提供了理论可能性，但实践中仍需权衡：
- 若追求效果：使用标准RNN或Transformer
- 若追求效率：使用线性RNN
- 若需要平衡：考虑迭代方法或混合架构</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="自然数集中-n-ab-c-时-a-b-c-的最小值.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#250 自然数集中 N = ab + c 时 a + b + c 的最小值</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="预训练一下transformer的长序列成绩还能涨不少.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#252 预训练一下，Transformer的长序列成绩还能涨不少！</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#rnn">脑洞大开：非线性RNN居然也可以并行计算？</a><ul>
<li><a href="#_1">求不动点</a></li>
<li><a href="#_2">简化形式</a></li>
<li><a href="#_3">摄动思想</a></li>
<li><a href="#_4">加快收敛</a></li>
<li><a href="#_5">何必收敛</a></li>
<li><a href="#_6">一般情形</a></li>
<li><a href="#_7">文章小结</a></li>
<li><a href="#_8">详细数学推导与注释</a><ul>
<li><a href="#1-rnn">1. 非线性RNN的基本形式</a></li>
<li><a href="#2">2. 不动点迭代的数学基础</a></li>
<li><a href="#3">3. 摄动方法的核心思想</a></li>
<li><a href="#4">4. 一阶近似分析</a></li>
<li><a href="#5-banach">5. Banach不动点定理</a></li>
<li><a href="#6">6. 收敛速度分析</a></li>
<li><a href="#7">7. 一阶加速：改进的摄动</a></li>
<li><a href="#8">8. 牛顿法：二次收敛</a></li>
<li><a href="#9">9. 对角化简化</a></li>
<li><a href="#10">10. 对角化后的并行算法</a></li>
<li><a href="#11-prefix-sum">11. Prefix Sum算法的详细分析</a></li>
<li><a href="#12-rnn">12. 迭代层与RNN层的计算复杂度</a></li>
<li><a href="#13-gru">13. GRU的并行化</a></li>
<li><a href="#14-lstm">14. LSTM的并行化</a></li>
<li><a href="#15">15. 实践中的收敛判断</a></li>
<li><a href="#16">16. 数值稳定性技巧</a></li>
<li><a href="#17">17. 实现伪代码</a></li>
<li><a href="#18">18. 与直接线性化的对比</a></li>
<li><a href="#19">19. 收敛性的理论保证</a></li>
<li><a href="#20">20. 梯度传播</a></li>
<li><a href="#21">21. 内存优化</a></li>
<li><a href="#22-rnn">22. 与线性RNN组合使用</a></li>
<li><a href="#23">23. 实验验证</a></li>
<li><a href="#24">24. 何时使用这个方法？</a></li>
<li><a href="#25">25. 理论总结与展望</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>