<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>中文任务还是SOTA吗？我们给SimCSE补充了一些实验 | ML & Math Blog Posts</title>
    <meta name="description" content="中文任务还是SOTA吗？我们给SimCSE补充了一些实验&para;
原文链接: https://spaces.ac.cn/archives/8348
发布日期: 

今年年初，笔者受到BERT-flow的启发，构思了成为“BERT-whitening”的方法，并一度成为了语义相似度的新SOTA（参考《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》，论文为《Whitening...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #17 中文任务还是SOTA吗？我们给SimCSE补充了一些实验
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#17</span>
                中文任务还是SOTA吗？我们给SimCSE补充了一些实验
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/8348" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=语义" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语义</span>
                </a>
                
                <a href="../index.html?tags=语义相似度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语义相似度</span>
                </a>
                
                <a href="../index.html?tags=对比学习" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 对比学习</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="sotasimcse">中文任务还是SOTA吗？我们给SimCSE补充了一些实验<a class="toc-link" href="#sotasimcse" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8348">https://spaces.ac.cn/archives/8348</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>今年年初，笔者受到BERT-flow的启发，构思了成为“BERT-whitening”的方法，并一度成为了语义相似度的新SOTA（参考<a href="/archives/8069">《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》</a>，论文为<a href="https://papers.cool/arxiv/2103.15316">《Whitening Sentence Representations for Better Semantics and Faster Retrieval》</a>）。然而“好景不长”，在BERT-whitening提交到Arxiv的不久之后，Arxiv上出现了至少有两篇结果明显优于BERT-whitening的新论文。</p>
<p>第一篇是<a href="https://arxiv.org/pdf/2104.07540.pdf">《Generating Datasets with Pretrained Language Models》</a>，这篇借助模板从GPT2_XL中无监督地构造了数据对来训练相似度模型，个人认为虽然有一定的启发而且效果还可以，但是复现的成本和变数都太大。另一篇则是本文的主角<a href="https://papers.cool/arxiv/2104.08821">《SimCSE: Simple Contrastive Learning of Sentence Embeddings》</a>，它提出的SimCSE在英文数据上显著超过了BERT-flow和BERT-whitening，并且方法特别简单～</p>
<p>那么，SimCSE在中文上同样有效吗？能大幅提高中文语义相似度的效果吗？本文就来做些补充实验。</p>
<blockquote>
<p><strong>开源地址：<a href="https://github.com/bojone/SimCSE">https://github.com/bojone/SimCSE</a></strong></p>
</blockquote>
<h2 id="simcse">SimCSE<a class="toc-link" href="#simcse" title="Permanent link">&para;</a></h2>
<p>首先，简单对SimCSE做个介绍。事实上，SimCSE可以看成是SimBERT的简化版（关于SimBERT请阅读<a href="/archives/7427">《鱼与熊掌兼得：融合检索和生成的SimBERT模型》</a>），它简化的部分如下：</p>
<blockquote>
<p>1、SimCSE去掉了SimBERT的生成部分，仅保留检索模型；</p>
<p>2、由于SimCSE没有标签数据，所以把每个句子自身视为相似句传入。</p>
</blockquote>
<p>说白了，本质上来说就是(自己,自己)作为正例、(自己,别人)作为负例来训练对比学习模型。当然，事实上还没那么简单，如果仅仅是完全相同的两个样本作为正例，那么泛化能力会大打折扣。一般来说，我们会使用一些数据扩增手段，让正例的两个样本有所差异，但是在NLP中如何做数据扩增本身又是一个难搞的问题，SimCSE则提出了一个极为简单的方案：直接把Dropout当作数据扩增！</p>
<p>具体来说，$N$个句子经过带Dropout的Encoder得到向量$\boldsymbol{h}^{(0)}<em i="1">1,\boldsymbol{h}^{(0)}_2,\cdots,\boldsymbol{h}^{(0)}_N$，然后让这批句子再重新过一遍Encoder（这时候是另一个随机Dropout）得到向量$\boldsymbol{h}^{(1)}_1,\boldsymbol{h}^{(1)}_2,\cdots,\boldsymbol{h}^{(1)}_N$，我们可以将$(\boldsymbol{h}^{(0)}_i,\boldsymbol{h}^{(1)}_i)$视为一对（略有不同的）正例了，那么训练目标为：<br />
\begin{equation}-\sum</em>}^N\sum_{\alpha=0,1}\log \frac{e^{\cos(\boldsymbol{h}^{(\alpha)<em i="i" j="1,j\neq">i, \boldsymbol{h}^{(1-\alpha)}_i)/\tau}}{\sum\limits</em>}^N e^{\cos(\boldsymbol{h}^{(\alpha)}_i, \boldsymbol{h}^{(\alpha)}_j)/\tau} + \sum\limits_j^N e^{\cos(\boldsymbol{h}^{(\alpha)}_i, \boldsymbol{h}^{(1-\alpha)}_j)/\tau}}\end{equation</p>
<h2 id="_1">英文效果<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>原论文的（英文）实验还是颇为丰富的，读者可以仔细阅读原文。但是要注意的是，原论文正文表格的评测指标跟BERT-flow、BERT-whitening的不一致，指标一致的表格在附录：  </p>
<p><a href="/usr/uploads/2021/04/3065122272.png" title="点击查看原图"><img alt="SimCSE与BERT-flow、BERT-whitening的效果对比" src="/usr/uploads/2021/04/3065122272.png" /></a></p>
<p>SimCSE与BERT-flow、BERT-whitening的效果对比</p>
<p>不管怎样比，SimCSE还是明显优于BERT-flow和BERT-whitening的。那么SimCSE的这个优势是不是普遍的呢？在中文上有没有这个优势呢？我们马上就来做实验。</p>
<h2 id="_2">实验配置<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>我们的中文实验基本与<a href="/archives/8321">《无监督语义相似度哪家强？我们做了个比较全面的评测》</a>对齐，包括之前测试的5个任务、4中Pooling以及所有base、small、tiny版的模型，large没有跑是因为相同配置下large模型OOM了。</p>
<blockquote>
<p><strong>开源地址：<a href="https://github.com/bojone/SimCSE">https://github.com/bojone/SimCSE</a></strong></p>
</blockquote>
<p>经过调参，笔者发现中文任务上SimCSE的最优参数跟原论文中的不完全一致，具体区别如下：</p>
<blockquote>
<p>1、原论文batch_size=512，这里是batch_size=64（实在跑不起这么壕的batch_size）；</p>
<p>2、原论文的学习率是5e-5，这里是1e-5；</p>
<p>3、原论文的最优dropout比例是0.1，这里是0.3；</p>
<p>4、原论文的无监督SimCSE是在额外数据上训练的，这里直接随机选了1万条任务数据训练；</p>
<p>5、原文无监督训练的时候还带了个MLM任务，这里只有SimCSE训练。</p>
</blockquote>
<p>最后一点再说明一下，原论文的无监督SimCSE是从维基百科上挑了100万个句子进行训练的，至于中文实验，为了实验上的方便以及对比上的公平，直接用任务数据训练（只用了句子，没有用标签，还是无监督的）。不过除了PAWSX之外，其他4个任务都不需要全部数据都拿来训练，经过测试，只需要随机选1万个训练样本训练一个epoch即可训练到最优效果（更多样本更少样本效果都变差）。</p>
<h2 id="_3">中文效果<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>SimCSE的所有中文实验结果如下：
$$\small{\begin{array}{l|ccccc}
\hline
&amp; \text{ATEC} &amp; \text{BQ} &amp; \text{LCQMC} &amp; \text{PAWSX} &amp; \text{STS-B} \\
\hline
\text{BERT}\text{-P1} &amp; 16.59 / 20.61 / \color{green}{33.14} &amp; 29.35 / 25.76 / \color{green}{50.67} &amp; 41.71 / 48.92 / \color{green}{69.99} &amp; 15.15 / 17.03 / \color{red}{12.95} &amp; 34.65 / 61.19 / \color{green}{69.04} \\
\text{BERT}\text{-P2} &amp; 9.46 / 22.16 / \color{green}{25.18} &amp; 16.97 / 18.97 / \color{green}{41.19} &amp; 28.42 / 49.61 / \color{green}{56.45} &amp; 13.93 / 16.08 / \color{red}{12.46} &amp; 21.66 / 60.75 / \color{red}{57.63} \\
\text{BERT}\text{-P3} &amp; 20.79 / 18.27 / \color{green}{32.89} &amp; 33.08 / 22.58 / \color{green}{49.58} &amp; 59.22 / 60.12 / \color{green}{71.83} &amp; 16.68 / 18.37 / \color{red}{14.47} &amp; 57.48 / 63.97 / \color{green}{70.08} \\
\text{BERT}\text{-P4} &amp; 24.51 / 27.00 / \color{green}{31.96} &amp; 38.81 / 32.29 / \color{green}{48.40} &amp; 64.75 / 64.75 / \color{green}{71.49} &amp; 15.12 / 17.80 / \color{red}{16.01} &amp; 61.66 / 69.45 / \color{green}{70.03} \\
\hline
\text{RoBERTa}\text{-P1} &amp; 24.61 / 29.59 / \color{green}{32.23} &amp; 40.54 / 28.95 / \color{green}{50.61} &amp; 70.55 / 70.82 / \color{green}{74.22} &amp; 16.23 / 17.99 / \color{red}{12.25} &amp; 66.91 / 69.19 / \color{green}{71.13} \\
\text{RoBERTa}\text{-P2} &amp; 20.61 / 28.91 / \color{red}{20.07} &amp; 31.14 / 27.48 / \color{green}{39.92} &amp; 65.43 / 70.62 / \color{red}{62.65} &amp; 15.71 / 17.30 / \color{red}{12.00} &amp; 59.50 / 70.77 / \color{red}{61.49} \\
\text{RoBERTa}\text{-P3} &amp; 26.94 / 29.94 / \color{green}{32.66} &amp; 40.71 / 30.95 / \color{green}{51.03} &amp; 66.80 / 68.00 / \color{green}{73.15} &amp; 16.08 / 19.01 / \color{red}{16.47} &amp; 61.67 / 66.19 / \color{green}{70.14} \\
\text{RoBERTa}\text{-P4} &amp; 27.94 / 28.33 / \color{green}{32.40} &amp; 43.09 / 33.49 / \color{green}{49.78} &amp; 68.43 / 67.86 / \color{green}{72.74} &amp; 15.02 / 17.91 / \color{red}{16.39} &amp; 64.09 / 69.74 / \color{green}{70.11} \\
\hline
\text{NEZHA}\text{-P1} &amp; 17.39 / 18.83 / \color{green}{32.14} &amp; 29.63 / 21.94 / \color{green}{46.08} &amp; 40.60 / 50.52 / \color{green}{60.38} &amp; 14.90 / 18.15 / \color{red}{16.60} &amp; 35.84 / 60.84 / \color{green}{68.50} \\
\text{NEZHA}\text{-P2} &amp; 10.96 / 23.08 / \color{red}{15.70} &amp; 17.38 / 28.81 / \color{green}{32.20} &amp; 22.66 / 49.12 / \color{red}{21.07} &amp; 13.45 / 18.05 / \color{red}{12.68} &amp; 21.16 / 60.11 / \color{red}{43.35} \\
\text{NEZHA}\text{-P3} &amp; 23.70 / 21.93 / \color{green}{31.47} &amp; 35.44 / 22.44 / \color{green}{46.69} &amp; 60.94 / 62.10 / \color{green}{69.65} &amp; 18.35 / 21.72 / \color{red}{18.17} &amp; 60.35 / 68.57 / \color{green}{70.68} \\
\text{NEZHA}\text{-P4} &amp; 27.72 / 25.31 / \color{green}{30.26} &amp; 44.18 / 31.47 / \color{green}{46.57} &amp; 65.16 / 66.68 / \color{green}{67.21} &amp; 13.98 / 16.66 / \color{red}{14.41} &amp; 61.94 / 69.55 / \color{red}{68.18} \\
\hline
\text{WoBERT}\text{-P1} &amp; 23.88 / 22.45 / \color{green}{32.66} &amp; 43.08 / 32.52 / \color{green}{49.13} &amp; 68.56 / 67.89 / \color{green}{72.99} &amp; 18.15 / 19.92 / \color{red}{12.36} &amp; 64.12 / 66.53 / \color{green}{70.00} \\
\text{WoBERT}\text{-P2} &amp; \text{-} &amp; \text{-} &amp; \text{-} &amp; \text{-} &amp; \text{-} \\
\text{WoBERT}\text{-P3} &amp; 24.62 / 22.74 / \color{green}{34.03} &amp; 40.64 / 28.12 / \color{green}{49.77} &amp; 64.89 / 65.22 / \color{green}{72.44} &amp; 16.83 / 20.56 / \color{red}{14.55} &amp; 59.43 / 66.57 / \color{green}{70.96} \\
\text{WoBERT}\text{-P4} &amp; 25.97 / 27.24 / \color{green}{33.67} &amp; 42.37 / 32.34 / \color{green}{49.09} &amp; 66.53 / 65.62 / \color{green}{71.74} &amp; 15.54 / 18.85 / \color{red}{14.00} &amp; 61.37 / 68.11 / \color{green}{70.00} \\
\hline
\text{RoFormer}\text{-P1} &amp; 24.29 / 26.04 / \color{green}{32.33} &amp; 41.91 / 28.13 / \color{green}{49.13} &amp; 64.87 / 60.92 / \color{green}{71.61} &amp; 20.15 / 23.08 / \color{red}{15.25} &amp; 59.91 / 66.96 / \color{green}{69.45} \\
\text{RoFormer}\text{-P2} &amp; \text{-} &amp; \text{-} &amp; \text{-} &amp; \text{-} &amp; \text{-} \\
\text{RoFormer}\text{-P3} &amp; 24.09 / 28.51 / \color{green}{34.23} &amp; 39.09 / 34.92 / \color{green}{50.01} &amp; 63.55 / 63.85 / \color{green}{72.01} &amp; 16.53 / 18.43 / \color{red}{15.25} &amp; 58.98 / 55.30 / \color{green}{71.44} \\
\text{RoFormer}\text{-P4} &amp; 25.92 / 27.38 / \color{green}{34.10} &amp; 41.75 / 32.36 / \color{green}{49.58} &amp; 66.18 / 65.45 / \color{green}{71.84} &amp; 15.30 / 18.36 / \color{red}{15.17} &amp; 61.40 / 68.02 / \color{green}{71.40} \\
\hline
\text{SimBERT}\text{-P1} &amp; 38.50 / 23.64 / \color{green}{36.98} &amp; 48.54 / 31.78 / \color{green}{51.47} &amp; 76.23 / 75.05 / \color{red}{74.87} &amp; 15.10 / 18.49 / \color{red}{12.66} &amp; 74.14 / 73.37 / \color{green}{75.12} \\
\text{SimBERT}\text{-P2} &amp; 38.93 / 27.06 / \color{green}{37.00} &amp; 49.93 / 35.38 / \color{green}{50.33} &amp; 75.56 / 73.45 / \color{red}{72.61} &amp; 14.52 / 18.51 / \color{green}{19.72} &amp; 73.18 / 73.43 / \color{green}{75.13} \\
\text{SimBERT}\text{-P3} &amp; 36.50 / 31.32 / \color{green}{37.81} &amp; 45.78 / 29.17 / \color{green}{51.24} &amp; 74.42 / 73.79 / \color{green}{73.85} &amp; 15.33 / 18.39 / \color{red}{12.48} &amp; 67.31 / 70.70 / \color{green}{73.18} \\
\text{SimBERT}\text{-P4} &amp; 33.53 / 29.04 / \color{green}{36.93} &amp; 45.28 / 34.70 / \color{green}{50.09} &amp; 73.20 / 71.22 / \color{green}{73.42} &amp; 14.16 / 17.32 / \color{red}{16.59} &amp; 66.98 / 70.55 / \color{green}{72.64} \\
\hline
\text{SimBERT}<em _text_small="\text{small">{\text{small}}\text{-P1} &amp; 30.68 / 27.56 / \color{green}{31.16} &amp; 43.41 / 30.89 / \color{green}{44.80} &amp; 74.73 / 73.21 / \color{green}{74.32} &amp; 15.89 / 17.96 / \color{red}{14.69} &amp; 70.54 / 71.39 / \color{red}{69.85} \\
\text{SimBERT}</em> \\}}\text{-P2} &amp; 31.00 / 29.14 / \color{green}{30.76} &amp; 43.76 / 36.86 / \color{green}{45.50} &amp; 74.21 / 73.14 / \color{green}{74.55} &amp; 16.17 / 18.12 / \color{red}{15.18} &amp; 70.10 / 71.40 / \color{red}{69.18
\text{SimBERT}<em _text_small="\text{small">{\text{small}}\text{-P3} &amp; 30.03 / 21.24 / \color{green}{30.07} &amp; 43.72 / 31.69 / \color{green}{44.27} &amp; 72.12 / 70.27 / \color{green}{71.21} &amp; 16.93 / 21.68 / \color{red}{12.10} &amp; 66.55 / 66.11 / \color{red}{64.95} \\
\text{SimBERT}</em> \\}}\text{-P4} &amp; 29.52 / 28.41 / \color{green}{28.56} &amp; 43.52 / 36.56 / \color{green}{43.38} &amp; 70.33 / 68.75 / \color{red}{68.35} &amp; 15.39 / 21.57 / \color{red}{14.47} &amp; 64.73 / 68.12 / \color{red}{63.23
\hline
\text{SimBERT}<em _text_tiny="\text{tiny">{\text{tiny}}\text{-P1} &amp; 30.51 / 24.67 / \color{green}{30.04} &amp; 44.25 / 31.75 / \color{green}{43.89} &amp; 74.27 / 72.25 / \color{green}{73.47} &amp; 16.01 / 18.07 / \color{red}{12.51} &amp; 70.11 / 66.39 / \color{green}{70.11} \\
\text{SimBERT}</em> \\}}\text{-P2} &amp; 30.01 / 27.66 / \color{green}{29.37} &amp; 44.47 / 37.33 / \color{green}{44.04} &amp; 73.98 / 72.31 / \color{green}{72.93} &amp; 16.55 / 18.15 / \color{red}{13.73} &amp; 70.35 / 70.88 / \color{red}{69.63
\text{SimBERT}<em _text_tiny="\text{tiny">{\text{tiny}}\text{-P3} &amp; 28.47 / 19.68 / \color{green}{28.08} &amp; 42.04 / 29.49 / \color{green}{41.21} &amp; 69.16 / 66.99 / \color{green}{69.85} &amp; 16.18 / 20.11 / \color{red}{12.21} &amp; 64.41 / 66.72 / \color{red}{64.62} \\
\text{SimBERT}</em> \\}}\text{-P4} &amp; 27.77 / 27.67 / \color{red}{26.25} &amp; 41.76 / 37.02 / \color{green}{41.62} &amp; 67.55 / 65.66 / \color{green}{67.34} &amp; 15.06 / 20.49 / \color{red}{13.87} &amp; 62.92 / 66.77 / \color{red}{60.80
\hline
\end{array}}$$</p>
<p>其中每个单元的数据是“a/b/c”的形式，a是不加任何处理的原始结果，b是BERT-whitening的结果（没有降维），c则是SimCSE的结果，如果c &gt; b，那么c显示为绿色，否则为红色，也就是说 <em>绿色越多，说明SimCSE比BERT-whitening好得越多</em> 。关于其他实验细节，可以看原代码以及<a href="/archives/8321">《无监督语义相似度哪家强？我们做了个比较全面的评测》</a>。</p>
<p>注意由于又有Dropout，训练时又是随机采样1万个样本，因此结果具有一定的随机性，重跑代码指标肯定会有波动，请读者知悉。</p>
<h2 id="_4">一些结论<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>从实验结果可以看出，除了PAWSX这个“异类”外，SimCSE相比BERT-whitening确实有压倒性优势，有些任务还能好10个点以上，在BQ上SimCSE还比有监督训练过的SimBERT要好，而且像SimBERT这种已经经过监督训练的模型还能获得进一步的提升，这些都说明确实强大。（至于PAWSX为什么“异”，文章<a href="/archives/8321">《无监督语义相似度哪家强？我们做了个比较全面的评测》</a>已经做过简单分析。）</p>
<p>同时，我们还可以看出在SimCSE之下，在BERT-flow和BERT-whitening中表现较好的first-last-avg这种Pooling方式已经没有任何优势了，反而较好的是直接取[CLS]向量，但让人意外的是，Pooler（取[CLS]的基础上再加个Dense）的表现又比较差，真让人迷惘～</p>
<p>由于BERT-whiteing只是一个线性变换，所以笔者还实验了单靠SimCSE是否能复现这个线性变换的效果。具体来说，就是固定Encoder的权重，然后接一个不加激活函数的Dense层，然后以SimCSE为目标，只训练最后接的Dense层。结果发现这种情况下的SimCSE并不如BERT-whitening。那就意味着，SimCSE要有效必须要把Encoder微调才行，同时也说明BERT-whitening可能包含了SimCSE所没有东西的，也许两者以某种方式进行结合会取得更好的效果（构思中...）。</p>
<h2 id="_5">相关工作<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>简单调研了一下，发现“自己与自己做正样本”这个思想的工作，最近都出现好几篇论文了，除了SimCSE之外，还有<a href="https://papers.cool/arxiv/2010.08240">《Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks》</a>、<a href="https://openreview.net/forum?id=Ov_sMNau-PF">《Semantic Re-tuning with Contrastive Tension》</a>都是极度相似的。其实类似的idea笔者也想过，只不过没想到真的能work（就没去做实验了），也没想到关键点是Dropout，看来还是得多多实验啊～</p>
<h2 id="_6">本文小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文分享了笔者在SimCSE上的中文实验，结果表明不少任务上SimCSE确实相当优秀，能明显优于BERT-whiteining。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8348">https://spaces.ac.cn/archives/8348</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 26, 2021). 《中文任务还是SOTA吗？我们给SimCSE补充了一些实验 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8348">https://spaces.ac.cn/archives/8348</a></p>
<p>@online{kexuefm-8348,<br />
title={中文任务还是SOTA吗？我们给SimCSE补充了一些实验},<br />
author={苏剑林},<br />
year={2021},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/8348}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="两个多元正态分布的kl散度巴氏距离和w距离.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#16 两个多元正态分布的KL散度、巴氏距离和W距离</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="seq2seq重复解码现象的理论分析尝试.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#18 Seq2Seq重复解码现象的理论分析尝试</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#sotasimcse">中文任务还是SOTA吗？我们给SimCSE补充了一些实验</a><ul>
<li><a href="#simcse">SimCSE</a></li>
<li><a href="#_1">英文效果</a></li>
<li><a href="#_2">实验配置</a></li>
<li><a href="#_3">中文效果</a></li>
<li><a href="#_4">一些结论</a></li>
<li><a href="#_5">相关工作</a></li>
<li><a href="#_6">本文小结</a></li>
<li><a href="#_7">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>