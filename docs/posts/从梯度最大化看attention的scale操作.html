<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>从梯度最大化看Attention的Scale操作 | ML & Math Blog Posts</title>
    <meta name="description" content="从梯度最大化看Attention的Scale操作&para;
原文链接: https://spaces.ac.cn/archives/9812
发布日期: 

我们知道，Scaled Dot-Product Attention的Scale因子是$\frac{1}{\sqrt{d}}$，其中$d$是$\boldsymbol{q},\boldsymbol{k}$的维度。这个Scale因子的一般解释是：...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=优化">优化</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #255 从梯度最大化看Attention的Scale操作
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#255</span>
                从梯度最大化看Attention的Scale操作
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2023-10-22</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="attentionscale">从梯度最大化看Attention的Scale操作<a class="toc-link" href="#attentionscale" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9812">https://spaces.ac.cn/archives/9812</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>我们知道，<a href="/archives/4765">Scaled Dot-Product Attention</a>的Scale因子是$\frac{1}{\sqrt{d}}$，其中$d$是$\boldsymbol{q},\boldsymbol{k}$的维度。这个Scale因子的一般解释是：如果不除以$\sqrt{d}$，那么初始的Attention就会很接近one hot分布，这会造成梯度消失，导致模型训练不起来。然而，可以证明的是，当Scale等于0时同样也会有梯度消失问题，这也就是说Scale太大太小都不行。</p>
<p>那么多大的Scale才适合呢？$\frac{1}{\sqrt{d}}$是最佳的Scale了吗？本文试图从梯度角度来回答这个问题。</p>
<h2 id="_1">已有结果<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>在<a href="/archives/8620#NTK%E5%8F%82%E6%95%B0%E5%8C%96">《浅谈Transformer的初始化、参数化与标准化》</a>中，我们已经推导过标准的Scale因子$\frac{1}{\sqrt{d}}$，推导的思路很简单，假设初始阶段$\boldsymbol{q},\boldsymbol{k}\in\mathbb{R}^d$都采样自“均值为0、方差为1”的分布，那么可以算得<br />
\begin{equation}\mathbb{V}ar[\boldsymbol{q}\cdot\boldsymbol{k}] = d\end{equation}<br />
于是我们将$\boldsymbol{q}\cdot\boldsymbol{k}$除以$\sqrt{d}$，将Attention Score的方差变为1。也就是说，之前的推导纯粹是基于 <em>“均值为0、方差为1”就会更好</em> 的<strong>信仰</strong> 来得到的结果，但没有解释让Attention Score的方差为1，也没有评估$\frac{1}{\sqrt{d}}$是否真的就解决了梯度消失问题。</p>
<p>当然，从已有的实验来看，$\frac{1}{\sqrt{d}}$至少一定程度上是缓解了这个问题，但这毕竟是实验结果，我们还是希望能从理论上知道“一定程度”究竟是多少。</p>
<h2 id="_2">计算梯度<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>既然涉及到了梯度，那么最好的办法就是把梯度算出来，然后定一个优化目标。设$p_i = e^{\alpha s_i}/Z$，$i \in \{1,2,...,n\}$，$Z=\sum_i e^{\alpha s_i}$是归一化因子，那么可以直接算得：<br />
\begin{equation}\frac{\partial p_i}{\partial s_j} = \left\{\begin{aligned}
\alpha(p_i - p_i^2),&amp;\quad i=j\\
-\alpha p_i p_j,&amp;\quad i\neq j
\end{aligned}\right.\end{equation}<br />
或者可以简写成$\partial p_i/\partial s_j = \alpha(p_i\delta_{i,j} - p_i p_j)$。很明显，当$\alpha\to 0$时梯度为0；当$\alpha\to\infty$时，$p_i$之中只有一个1、其余都是0（假设$s_i$中只有唯一的最大值），梯度也是0。</p>
<p>为了更有利于优化，我们应该选取$\alpha$使得梯度尽可能最大化。为此，我们以L1范数作为梯度大小的度量：<br />
\begin{equation}\frac{1}{2}\left\Vert\frac{\partial p}{\partial s}\right\Vert_1=\frac{1}{2}\sum_{i,j}\left|\frac{\partial p_i}{\partial s_j}\right|=\frac{1}{2}\sum_i \alpha(p_i - p_i^2) + \frac{1}{2}\sum_{i\neq j} \alpha p_i p_j = \alpha\left(1 - \sum_i p_i^2\right)\label{eq:target}\end{equation}<br />
从最后的结果不难猜到，之所以选择L1而不是其他的根本原因是因为L1范数的计算结果足够简单。值得指出的是，这里出现了$\sum_i p_i^2$，它本质上就是我们在<a href="/archives/9595#%E7%86%B5%E7%9A%84%E8%81%94%E7%B3%BB">《如何度量数据的稀疏程度？》</a>介绍过的“Rényi熵”，跟信息熵类似，它也是不确定性的一种度量。</p>
<p>有了优化目标后，我们就可以着手进行最大化了。注意$p_i$的定义里边也包含$\alpha$，所以这是一个关于$\alpha$复杂的非线性目标，看上去求解析解是不可能的，但我们可以针对一些特殊例子求近似解。</p>
<h2 id="_3">正态分布<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>首先，我们可以接着前面的结果来做，当我们通过除以$\sqrt{d}$使得Attention Score的均值为0、方差为1后，我们就可以近似假设$s_i\sim\mathcal{N}(0,1)$，然后再求$\alpha$的最优解，如果$\alpha=1$，那么就意味着原来的$\frac{1}{\sqrt{d}}$就是最优的Scale比例了，否则$\frac{\alpha}{\sqrt{d}}$才是最佳的Scale比例。</p>
<p>我们用期望去估计求和<br />
\begin{equation}\sum_i p_i^2 = \frac{\sum_i e^{2\alpha s_i}}{\left(\sum_i e^{\alpha s_i}\right)^2} = \frac{\frac{1}{n}\sum_i e^{2\alpha s_i}}{n\left(\frac{1}{n}\sum_i e^{\alpha s_i}\right)^2} \approx \frac{\mathbb{E}_s[e^{2\alpha s}]}{n\left(\mathbb{E}_s[e^{\alpha s}]\right)^2}\label{eq:approx}\end{equation}<br />
对于服从标准正态分布的$s$，我们有<br />
\begin{equation}\mathbb{E}_s[e^{\alpha s}] = \int \frac{1}{\sqrt{2\pi}}e^{-s^2/2}e^{\alpha s} ds = e^{\alpha^2 / 2}\label{eq:normal}\end{equation}<br />
代入上式，然后代入式$\eqref{eq:target}$，得到<br />
\begin{equation}\alpha\left(1 - \sum_i p_i^2\right)\approx\alpha\left(1 - \frac{e^{\alpha^2}}{n}\right)\end{equation}<br />
最后的近似，虽然已经足够简化了，但其实也不容易求出最大值来。不过无妨，我们可以遍历一些$n$，然后数值求解出取最大值时的$\alpha^<em>$，这样我们就大致能看到$\alpha^</em>$与$n$的关系了，Mathematica的参考代码如下：</p>
<div class="highlight"><pre><span></span><code><span class="p">(</span><span class="o">*</span><span class="n">定义函数</span><span class="o">*</span><span class="p">)</span>
<span class="n">f</span><span class="o">[</span><span class="n">a_, n_</span><span class="o">]</span><span class="w"> </span><span class="err">:</span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">Exp</span><span class="o">[</span><span class="n">a^2</span><span class="o">]/</span><span class="n">n</span><span class="p">)</span>
<span class="p">(</span><span class="o">*</span><span class="n">找到函数的最大点对应的a</span><span class="o">*</span><span class="p">)</span>
<span class="n">FindArg</span><span class="o">[</span><span class="n">n_</span><span class="o">]</span><span class="w"> </span><span class="err">:</span><span class="o">=</span><span class="w"> </span>
<span class="w"> </span><span class="k">Module</span><span class="o">[</span><span class="n">{a}, a = a /. Last@NMaximize[{f[a, n</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="err">]</span><span class="o">[</span><span class="n">[2</span><span class="o">]</span><span class="err">]</span><span class="p">;</span><span class="w"> </span><span class="n">a</span><span class="err">]</span>
<span class="p">(</span><span class="o">*</span><span class="n">给定n的范围</span><span class="o">*</span><span class="p">)</span>
<span class="n">nRange</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">40</span><span class="o">*</span><span class="k">Range</span><span class="o">[</span><span class="n">1, 500</span><span class="o">]</span><span class="p">;</span>
<span class="p">(</span><span class="o">*</span><span class="n">求出每个n对应的a</span><span class="o">*</span><span class="p">)</span>
<span class="n">args</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">FindArg</span><span class="w"> </span><span class="o">/</span><span class="err">@</span><span class="w"> </span><span class="n">nRange</span><span class="p">;</span>
<span class="p">(</span><span class="o">*</span><span class="n">画出a与n的函数图像</span><span class="o">*</span><span class="p">)</span>
<span class="n">ListLinePlot</span><span class="o">[</span><span class="n">{args, 0.84*Log[nRange</span><span class="o">]^</span><span class="mf">0.5</span><span class="err">}</span><span class="p">,</span><span class="w"> </span>
<span class="w"> </span><span class="n">DataRange</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="err">{</span><span class="mi">40</span><span class="p">,</span><span class="w"> </span><span class="mi">20000</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">AxesLabel</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;n&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;a&quot;</span><span class="err">}</span><span class="p">,</span><span class="w"> </span>
<span class="w"> </span><span class="n">PlotLegends</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="err">{</span><span class="k">Row</span><span class="o">[</span><span class="n">{&quot;a&quot;, Superscript[&quot;&quot;, &quot;*&quot;</span><span class="o">]</span><span class="err">}]</span><span class="p">,</span><span class="w"> </span>
<span class="w">   </span><span class="n">TraditionalForm</span><span class="o">[</span><span class="n">HoldForm[0.84*Sqrt[Log[n</span><span class="o">]</span><span class="err">]]]}]</span>
</code></pre></div>

<p>经过拟合，笔者发现一定范围内最优点$\alpha^*$与$n$大致满足$\alpha\approx 0.84\sqrt{\log n}$的关系，所以也已经将对应的近似函数一并画在一起：  </p>
<p><a href="/usr/uploads/2023/10/4069707715.png" title="点击查看原图"><img alt="标准正态分布的最优alpha与n关系" src="/usr/uploads/2023/10/4069707715.png" /></a></p>
<p>标准正态分布的最优alpha与n关系</p>
<p>可以看到，在相当大的一个范围内，$\alpha^*$的最优值都在$2\sim 3$之间，所以折中一下的话，盲取$\frac{2.5}{\sqrt{d}}$作为Attention的Scale因子理论上更有利于优化。</p>
<h2 id="_4">余弦分布<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>现在我们考虑另一个不那么常见的例子：当我们对$\boldsymbol{q},\boldsymbol{k}$都做$l_2$归一化变成单位向量后，它们的内积就变成了夹角余弦，即$s_i$近似服从$d$维空间中的两个随机向量的夹角余弦分布。这个分布可能有些读者并不熟悉，但之前我们在<a href="/archives/7076">《n维空间下两个随机向量的夹角分布》</a>已经探讨过，它的概率密度具有形式<br />
\begin{equation}p(s)\propto (1-s^2)^{(d-3)/2}\end{equation}</p>
<p>看上去并不复杂，但事实上这个形式比正态分布难处理得多，主要是$\mathbb{E}_s[e^{\alpha s}]$已经不像式$\eqref{eq:normal}$那样可以用初等函数表达出来了，不过对于Mathematica数值求解来说问题不大。跟上一节同样的思路，近似式$\eqref{eq:approx}$也同样适用，先数值求解最大值，然后再拟合，结果如下（图中$d=128$，$\alpha^*$跟$d$相关）：  </p>
<p><a href="/usr/uploads/2023/10/4082251077.png" title="点击查看原图"><img alt="余弦分布的最优alpha与n关系" src="/usr/uploads/2023/10/4082251077.png" /></a></p>
<p>余弦分布的最优alpha与n关系</p>
<p>可以看到，$\alpha^<em>$与$3.5\log n$拟合得也不错（换一个$d$的话，$3.5$这个系数会变化）。可以看到，在一个相当大的范围内，$\alpha^</em>$都是$25\sim 35$之间，所以如果用$\cos$值作为Attention Score的话，就需要乘以一个$25\sim 35$之间的Scale，才能使得模型比较容易训下去。这同时也解释了为什么我们在用$\cos$值构建Softmax分布（比如<a href="/archives/5743#am-softmax">AM-Softmax</a>、<a href="/archives/8348">SimCSE</a>等）时，需要在$\cos$之后乘上一个30左右的Scale了，因为不乘是很难训得动模型的。</p>
<p>对于不同的$d$和$n$，读者可以自行修改下面的代码计算最优$\alpha$：</p>
<div class="highlight"><pre><span></span><code><span class="p">(</span><span class="o">*</span><span class="n">定义函数</span><span class="o">*</span><span class="p">)</span>
<span class="n">h</span><span class="o">[</span><span class="n">a_</span><span class="o">]</span><span class="w"> </span><span class="err">:</span><span class="o">=</span><span class="w"> </span>
<span class="w"> </span><span class="n">Integrate</span><span class="o">[</span><span class="n">Exp[a*s</span><span class="o">]*</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">s</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="p">((</span><span class="n">d</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="w"> </span><span class="err">{</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="n">Assumptions</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="err">{</span><span class="n">d</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">10</span><span class="err">}]</span>
<span class="n">g</span><span class="o">[</span><span class="n">a_</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">h</span><span class="o">[</span><span class="n">a</span><span class="o">]/</span><span class="n">h</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">FullSimplify</span><span class="p">;</span>
<span class="n">f</span><span class="o">[</span><span class="n">a_, n_</span><span class="o">]</span><span class="w"> </span><span class="err">:</span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">g</span><span class="o">[</span><span class="n">2*a</span><span class="o">]/</span><span class="n">g</span><span class="o">[</span><span class="n">a</span><span class="o">]^</span><span class="mi">2</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="p">.</span><span class="w"> </span><span class="err">{</span><span class="n">d</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="mi">128</span><span class="err">}</span>
<span class="p">(</span><span class="o">*</span><span class="n">找到函数的最大点对应的a</span><span class="o">*</span><span class="p">)</span>
<span class="n">FindArg</span><span class="o">[</span><span class="n">n_</span><span class="o">]</span><span class="w"> </span><span class="err">:</span><span class="o">=</span><span class="w"> </span>
<span class="w"> </span><span class="k">Module</span><span class="o">[</span><span class="n">{a}, a = a /. Last@NMaximize[{f[a, n</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="err">]</span><span class="o">[</span><span class="n">[2</span><span class="o">]</span><span class="err">]</span><span class="p">;</span><span class="w"> </span><span class="n">a</span><span class="err">]</span>
<span class="p">(</span><span class="o">*</span><span class="n">给定n的范围</span><span class="o">*</span><span class="p">)</span>
<span class="n">nRange</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">40</span><span class="o">*</span><span class="k">Range</span><span class="o">[</span><span class="n">1, 500</span><span class="o">]</span><span class="p">;</span>
<span class="p">(</span><span class="o">*</span><span class="n">求出每个n对应的a</span><span class="o">*</span><span class="p">)</span>
<span class="n">args</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">FindArg</span><span class="w"> </span><span class="o">/</span><span class="err">@</span><span class="w"> </span><span class="n">nRange</span><span class="p">;</span>
<span class="p">(</span><span class="o">*</span><span class="n">画出a与n的函数图像</span><span class="o">*</span><span class="p">)</span>
<span class="n">ListLinePlot</span><span class="o">[</span><span class="n">{args, 3.5*Log[nRange</span><span class="o">]</span><span class="err">}</span><span class="p">,</span><span class="w"> </span>
<span class="w"> </span><span class="n">DataRange</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="err">{</span><span class="mi">40</span><span class="p">,</span><span class="w"> </span><span class="mi">20000</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">AxesLabel</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="err">{</span><span class="ss">&quot;n&quot;</span><span class="p">,</span><span class="w"> </span><span class="ss">&quot;a&quot;</span><span class="err">}</span><span class="p">,</span><span class="w"> </span>
<span class="w"> </span><span class="n">PlotLegends</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="err">{</span><span class="k">Row</span><span class="o">[</span><span class="n">{&quot;a&quot;, Superscript[&quot;&quot;, &quot;*&quot;</span><span class="o">]</span><span class="err">}]</span><span class="p">,</span><span class="w"> </span>
<span class="w">   </span><span class="n">TraditionalForm</span><span class="o">[</span><span class="n">HoldForm[3.5*Log[n</span><span class="o">]</span><span class="err">]]}]</span>
</code></pre></div>

<h2 id="_5">相关思考<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文的标题和结果，尤其是余弦分布中$\alpha$近似正比于$\log n$的结果，很容易让我们联想到另一篇讨论Attention Scale的文章<a href="/archives/8823">《从熵不变性看Attention的Scale操作》</a>。事实上，两篇文章的联系确实存在，本文的优化目标$\eqref{eq:target}$出现了“Rényi熵”，而“熵不变性”的熵指的是香侬信息熵，两者的性质很大程度上是一致的。最大化式$\eqref{eq:target}$使得它进入了一个“缓变”的区域，这意味着“Rényi熵”关于$n$的变化是很慢的，也意味着信息熵关于$n$的变化是很慢的，这就约等于熵不变性。</p>
<p>此外，对于双向Attention（Encoder）来说，假设训练样本长度相同，那么$n$就是一个常数，我们可以根据$n$算得相应的最优$\alpha$，然后固定在模型中即可；但是对于单向Attention（Decoder）来说，每个token的$n$实际上都不一样（位置id加1），所以理论上无法做到对所有token都最大化式$\eqref{eq:target}$，不过由于$\alpha^*$关于$n$的变化较慢，所以取一个差不多的值就行了，比如可以取$n=L_{\max} / 2$，这样对大部分token的梯度都比较友好了。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文从梯度的角度探讨了Attention Scale因子的选择问题。众所周知，关于这个Scale因子的“标准答案”是$\frac{1}{\sqrt{d}}$，但其推导过程中并没有讨论到它的最优性问题，所以笔者定义了一个Softmax梯度的优化目标，从最大化该目标的角度探讨了Scale因子的最优值。相关结果既可以用来改进Attention的Scale因子，也可以用来解释$\cos$相似度的对比学习的温度参数。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9812">https://spaces.ac.cn/archives/9812</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Oct. 22, 2023). 《从梯度最大化看Attention的Scale操作 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9812">https://spaces.ac.cn/archives/9812</a></p>
<p>@online{kexuefm-9812,<br />
title={从梯度最大化看Attention的Scale操作},<br />
author={苏剑林},<br />
year={2023},<br />
month={Oct},<br />
url={\url{https://spaces.ac.cn/archives/9812}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<h3 id="1-softmax">1. Softmax梯度的完整推导<a class="toc-link" href="#1-softmax" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 问题设定<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>给定logits $s = (s_1, \ldots, s_n) \in \mathbb{R}^n$，Softmax函数定义为：
\begin{equation}
p_i = \frac{e^{\alpha s_i}}{\sum_{j=1}^n e^{\alpha s_j}} = \frac{e^{\alpha s_i}}{Z(\alpha)} \tag{1}
\end{equation}</p>
<p>其中 $\alpha &gt; 0$ 是温度参数的倒数（缩放因子），$Z(\alpha) = \sum_j e^{\alpha s_j}$ 是配分函数。</p>
<p><strong>问题</strong>：如何选择 $\alpha$ 使得Softmax的梯度最大，从而有利于训练？</p>
<h4 id="12-softmaxlogits">1.2 Softmax对logits的梯度<a class="toc-link" href="#12-softmaxlogits" title="Permanent link">&para;</a></h4>
<p>计算 $\frac{\partial p_i}{\partial s_j}$：
\begin{equation}
\frac{\partial p_i}{\partial s_j} = \frac{\partial}{\partial s_j}\left(\frac{e^{\alpha s_i}}{Z}\right) = \frac{\partial e^{\alpha s_i}}{\partial s_j} \cdot \frac{1}{Z} + e^{\alpha s_i} \cdot \frac{\partial}{\partial s_j}\left(\frac{1}{Z}\right) \tag{2}
\end{equation}</p>
<p><strong>第一项</strong>：
\begin{equation}
\frac{\partial e^{\alpha s_i}}{\partial s_j} = \alpha e^{\alpha s_i} \delta_{ij} \tag{3}
\end{equation}</p>
<p>其中 $\delta_{ij}$ 是Kronecker delta。</p>
<p><strong>第二项</strong>：
\begin{equation}
\frac{\partial}{\partial s_j}\left(\frac{1}{Z}\right) = -\frac{1}{Z^2}\frac{\partial Z}{\partial s_j} = -\frac{1}{Z^2} \alpha e^{\alpha s_j} = -\frac{\alpha p_j}{Z} \tag{4}
\end{equation}</p>
<p>合并：
\begin{equation}
\frac{\partial p_i}{\partial s_j} = \frac{\alpha e^{\alpha s_i}\delta_{ij}}{Z} - \frac{\alpha e^{\alpha s_i} e^{\alpha s_j}}{Z^2} = \alpha p_i(\delta_{ij} - p_j) \tag{5}
\end{equation}</p>
<p><strong>总结</strong>：
\begin{equation}
\frac{\partial p_i}{\partial s_j} = \begin{cases}
\alpha p_i(1 - p_i) &amp; i = j \
-\alpha p_i p_j &amp; i \neq j
\end{cases} \tag{6}
\end{equation}</p>
<h4 id="13-jacobian">1.3 Jacobian矩阵<a class="toc-link" href="#13-jacobian" title="Permanent link">&para;</a></h4>
<p>Softmax的Jacobian矩阵 $J \in \mathbb{R}^{n \times n}$：
\begin{equation}
J_{ij} = \frac{\partial p_i}{\partial s_j} = \alpha(p_i\delta_{ij} - p_i p_j) = \alpha(\text{diag}(p) - pp^{\top}) \tag{7}
\end{equation}</p>
<p><strong>性质1（对称性）</strong>：注意 $J$ 不是对称矩阵，因为 $J_{ij} = -\alpha p_i p_j$ 而 $J_{ji} = -\alpha p_j p_i$，只有当 $p_i = p_j$ 时相等。</p>
<p><strong>性质2（秩）</strong>：$pp^{\top}$ 是秩1矩阵，因此 $J$ 的秩最多为 $n$。</p>
<p><strong>性质3（特征值）</strong>：$J$ 的特征值可以通过以下方式计算。令 $v$ 是 $J$ 的特征向量，特征值为 $\lambda$：
\begin{equation}
Jv = \alpha(\text{diag}(p)v - pp^{\top}v) = \lambda v \tag{8}
\end{equation}</p>
<p>如果 $p^{\top}v = 0$（$v$ 正交于 $p$），则：
\begin{equation}
\alpha\text{diag}(p)v = \lambda v \implies \lambda = \alpha p_i \quad \text{(对应于 } v_i \neq 0\text{)} \tag{9}
\end{equation}</p>
<p>因此 $J$ 有 $n-1$ 个特征值 $\alpha p_i$，以及一个特征值0（对应于 $v \propto \mathbf{1}$）。</p>
<h3 id="2">2. 梯度范数的优化目标<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21-l1">2.1 L1范数作为优化目标<a class="toc-link" href="#21-l1" title="Permanent link">&para;</a></h4>
<p>为了最大化梯度，我们定义目标函数：
\begin{equation}
G(\alpha) = |J|<em i_j="i,j">1 = \sum</em>
\end{equation}} |J_{ij}| = \sum_{i,j} \left|\frac{\partial p_i}{\partial s_j}\right| \tag{10</p>
<p>展开：
\begin{equation}
G(\alpha) = \sum_{i=1}^n \left|\alpha p_i(1-p_i)\right| + \sum_{i \neq j} \left|-\alpha p_i p_j\right| \tag{11}
\end{equation}</p>
<p>由于 $p_i \in (0, 1)$，所有项都是正的：
\begin{equation}
G(\alpha) = \sum_{i=1}^n \alpha p_i(1-p_i) + \sum_{i \neq j} \alpha p_i p_j \tag{12}
\end{equation}</p>
<p><strong>简化第一项</strong>：
\begin{equation}
\sum_{i=1}^n p_i(1-p_i) = \sum_{i=1}^n p_i - \sum_{i=1}^n p_i^2 = 1 - \sum_{i=1}^n p_i^2 \tag{13}
\end{equation}</p>
<p><strong>简化第二项</strong>：
\begin{equation}
\sum_{i \neq j} p_i p_j = \sum_{i,j} p_i p_j - \sum_{i=1}^n p_i^2 = \left(\sum_i p_i\right)^2 - \sum_{i=1}^n p_i^2 = 1 - \sum_{i=1}^n p_i^2 \tag{14}
\end{equation}</p>
<p><strong>合并</strong>：
\begin{equation}
G(\alpha) = \alpha\left[\left(1 - \sum_i p_i^2\right) + \left(1 - \sum_i p_i^2\right)\right] = 2\alpha\left(1 - \sum_i p_i^2\right) \tag{15}
\end{equation}</p>
<p>定义<strong>有效类别数</strong>（Effective Number of Classes）：
\begin{equation}
N_{\text{eff}}(\alpha) = \frac{1}{\sum_i p_i^2} \tag{16}
\end{equation}</p>
<p>则：
\begin{equation}
G(\alpha) = 2\alpha\left(1 - \frac{1}{N_{\text{eff}}}\right) = 2\alpha \frac{N_{\text{eff}} - 1}{N_{\text{eff}}} \tag{17}
\end{equation}</p>
<p><strong>Rényi熵联系</strong>：$\sum_i p_i^2$ 是2阶Rényi熵的指数形式：
\begin{equation}
H_2(p) = -\log\sum_i p_i^2 \tag{18}
\end{equation}</p>
<p>因此：
\begin{equation}
G(\alpha) = 2\alpha(1 - e^{-H_2(p)}) \tag{19}
\end{equation}</p>
<h4 id="22">2.2 目标函数的依赖关系<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>注意 $p_i$ 本身依赖于 $\alpha$：
\begin{equation}
p_i(\alpha) = \frac{e^{\alpha s_i}}{\sum_j e^{\alpha s_j}} \tag{20}
\end{equation}</p>
<p>因此 $\sum_i p_i^2$ 也是 $\alpha$ 的函数。我们需要分析：
\begin{equation}
f(\alpha) = \sum_{i=1}^n p_i(\alpha)^2 = \sum_{i=1}^n \frac{e^{2\alpha s_i}}{Z(\alpha)^2} \tag{21}
\end{equation}</p>
<p>其导数：
\begin{equation}
\frac{df}{d\alpha} = \frac{d}{d\alpha}\left[\frac{\sum_i e^{2\alpha s_i}}{(\sum_j e^{\alpha s_j})^2}\right] \tag{22}
\end{equation}</p>
<p>使用商法则：
\begin{equation}
\frac{df}{d\alpha} = \frac{2\sum_i s_i e^{2\alpha s_i} \cdot Z^2 - \sum_i e^{2\alpha s_i} \cdot 2Z \cdot \alpha\sum_j s_j e^{\alpha s_j}}{Z^4} \tag{23}
\end{equation}</p>
<p>简化（令 $Z = \sum_j e^{\alpha s_j}$）：
\begin{equation}
\frac{df}{d\alpha} = \frac{2}{Z^2}\left[\sum_i s_i e^{2\alpha s_i} - \frac{\sum_i e^{2\alpha s_i} \cdot \sum_j s_j e^{\alpha s_j}}{Z}\right] \tag{24}
\end{equation}</p>
<p><strong>观察</strong>：当 $\alpha \to 0$ 时，$p_i \to 1/n$（均匀分布），$f(0) = 1/n$。
当 $\alpha \to \infty$ 时，$p_i \to \delta_{i,i^*}$（one-hot在最大的 $s_i$），$f(\infty) = 1$。</p>
<h3 id="3">3. 正态分布下的最优α<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 假设与近似<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>假设 $s_i \sim \mathcal{N}(0, 1)$ 独立同分布。使用期望近似：
\begin{equation}
\sum_i p_i^2 = \frac{\sum_i e^{2\alpha s_i}}{(\sum_j e^{\alpha s_j})^2} \approx \frac{n \cdot \mathbb{E}[e^{2\alpha s}]}{(n \cdot \mathbb{E}[e^{\alpha s}])^2} = \frac{\mathbb{E}[e^{2\alpha s}]}{n(\mathbb{E}[e^{\alpha s}])^2} \tag{25}
\end{equation}</p>
<p>对于 $s \sim \mathcal{N}(0, 1)$：
\begin{equation}
\mathbb{E}[e^{\alpha s}] = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-s^2/2} e^{\alpha s} ds \tag{26}
\end{equation}</p>
<p>配方：
\begin{equation}
-\frac{s^2}{2} + \alpha s = -\frac{1}{2}(s - \alpha)^2 + \frac{\alpha^2}{2} \tag{27}
\end{equation}</p>
<p>因此：
\begin{equation}
\mathbb{E}[e^{\alpha s}] = e^{\alpha^2/2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-(s-\alpha)^2/2} ds = e^{\alpha^2/2} \tag{28}
\end{equation}</p>
<p>类似地：
\begin{equation}
\mathbb{E}[e^{2\alpha s}] = e^{(2\alpha)^2/2} = e^{2\alpha^2} \tag{29}
\end{equation}</p>
<p>代入式(25)：
\begin{equation}
\sum_i p_i^2 \approx \frac{e^{2\alpha^2}}{n \cdot e^{\alpha^2}} = \frac{e^{\alpha^2}}{n} \tag{30}
\end{equation}</p>
<h4 id="32">3.2 优化目标<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>目标函数变为：
\begin{equation}
G(\alpha) = 2\alpha\left(1 - \frac{e^{\alpha^2}}{n}\right) \tag{31}
\end{equation}</p>
<p><strong>边界行为</strong>：
- $\alpha \to 0$：$G(\alpha) \approx 2\alpha(1 - 1/n) \to 0$
- $\alpha \to \infty$：$G(\alpha) \approx -2\alpha e^{\alpha^2}/n \to -\infty$</p>
<p>因此存在最大值点。</p>
<h4 id="33">3.3 一阶条件<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>求导：
\begin{equation}
\frac{dG}{d\alpha} = 2\left(1 - \frac{e^{\alpha^2}}{n}\right) + 2\alpha \cdot \left(-\frac{2\alpha e^{\alpha^2}}{n}\right) = 2\left(1 - \frac{e^{\alpha^2}}{n}(1 + 2\alpha^2)\right) \tag{32}
\end{equation}</p>
<p>令 $\frac{dG}{d\alpha} = 0$：
\begin{equation}
1 - \frac{e^{\alpha^2}}{n}(1 + 2\alpha^2) = 0 \tag{33}
\end{equation}</p>
<p>即：
\begin{equation}
e^{\alpha^2}(1 + 2\alpha^2) = n \tag{34}
\end{equation}</p>
<p>这是一个超越方程，没有解析解，但可以数值求解。</p>
<h4 id="34">3.4 近似解<a class="toc-link" href="#34" title="Permanent link">&para;</a></h4>
<p>对于大的 $n$，使用对数：
\begin{equation}
\alpha^2 + \log(1 + 2\alpha^2) = \log n \tag{35}
\end{equation}</p>
<p>当 $\alpha^2$ 适中时，$\log(1 + 2\alpha^2) \approx 2\alpha^2$（对小 $\alpha^2$）或 $\log(2\alpha^2)$（对大 $\alpha^2$）。</p>
<p><strong>猜测形式</strong>：$\alpha \approx c\sqrt{\log n}$。代入：
\begin{equation}
c^2\log n + \log(1 + 2c^2\log n) \approx \log n \tag{36}
\end{equation}</p>
<p>当 $n$ 大时，第二项相对较小，得 $c^2 \approx 1$，即：
\begin{equation}
\alpha^* \approx \sqrt{\log n} \tag{37}
\end{equation}</p>
<p><strong>数值拟合</strong>：通过Mathematica数值求解，得到：
\begin{equation}
\alpha^*(n) \approx 0.84\sqrt{\log n} \tag{38}
\end{equation}</p>
<h4 id="35">3.5 二阶条件验证<a class="toc-link" href="#35" title="Permanent link">&para;</a></h4>
<p>计算二阶导数：
\begin{equation}
\frac{d^2G}{d\alpha^2} = -\frac{2e^{\alpha^2}}{n}(4\alpha^2 + 4\alpha^4 + 2) \tag{39}
\end{equation}</p>
<p>在 $\alpha^* &gt; 0$ 处，$\frac{d^2G}{d\alpha^2} &lt; 0$，确认是最大值。</p>
<h3 id="4">4. 余弦分布下的最优α<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 余弦分布的定义<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p>当 $q, k \in \mathbb{R}^d$ 是归一化向量时，$s = q \cdot k = \cos\theta$，其中 $\theta$ 是夹角。</p>
<p>在 $d$ 维空间中，随机单位向量的夹角分布为：
\begin{equation}
p(\theta) \propto \sin^{d-2}\theta, \quad \theta \in [0, \pi] \tag{40}
\end{equation}</p>
<p>归一化常数：
\begin{equation}
C_d = \frac{1}{\int_0^{\pi} \sin^{d-2}\theta d\theta} = \frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)} \tag{41}
\end{equation}</p>
<p><strong>余弦的分布</strong>：令 $s = \cos\theta$，则 $ds = -\sin\theta d\theta$：
\begin{equation}
p(s) = C_d \sin^{d-2}\theta |d\theta/ds| = C_d \sin^{d-1}\theta = C_d (1-s^2)^{(d-2)/2}, \quad s \in [-1, 1] \tag{42}
\end{equation}</p>
<p>归一化：
\begin{equation}
\int_{-1}^{1} (1-s^2)^{(d-2)/2} ds = \frac{\sqrt{\pi}\Gamma((d-1)/2)}{\Gamma(d/2)} \tag{43}
\end{equation}</p>
<p>因此：
\begin{equation}
p(s) = \frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)} (1-s^2)^{(d-2)/2} \tag{44}
\end{equation}</p>
<h4 id="42">4.2 矩生成函数<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p>需要计算：
\begin{equation}
\mathbb{E}[e^{\alpha s}] = \int_{-1}^{1} e^{\alpha s} p(s) ds = \frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)} \int_{-1}^{1} e^{\alpha s}(1-s^2)^{(d-2)/2} ds \tag{45}
\end{equation}</p>
<p>这个积分与<strong>修正贝塞尔函数</strong>（Modified Bessel Function）有关：
\begin{equation}
I_{\nu}(z) = \sum_{m=0}^{\infty} \frac{(z/2)^{2m+\nu}}{m!\Gamma(m+\nu+1)} \tag{46}
\end{equation}</p>
<p>具体地，对于半整数阶 $\nu = (d-2)/2$：
\begin{equation}
\mathbb{E}[e^{\alpha s}] = \frac{I_{(d-2)/2}(\alpha)}{\alpha^{(d-2)/2}} \cdot \frac{\Gamma(d/2)}{\Gamma(1/2)\Gamma((d-1)/2)} \tag{47}
\end{equation}</p>
<p><strong>简化形式</strong>（对大 $d$）：使用渐近展开：
\begin{equation}
I_{\nu}(z) \approx \frac{e^z}{\sqrt{2\pi z}}\left(1 - \frac{4\nu^2-1}{8z} + O(z^{-2})\right) \tag{48}
\end{equation}</p>
<h4 id="43">4.3 数值优化<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>由于解析解复杂，我们使用数值方法。对于 $d = 128$（常见的Attention维度），通过数值计算：</p>
<p><strong>步骤</strong>：
1. 对每个 $\alpha$，数值计算 $\mathbb{E}[e^{\alpha s}]$ 和 $\mathbb{E}[e^{2\alpha s}]$
2. 计算 $f(\alpha) = \mathbb{E}[e^{2\alpha s}] / (n(\mathbb{E}[e^{\alpha s}])^2)$
3. 计算 $G(\alpha) = 2\alpha(1 - f(\alpha))$
4. 找到 $G(\alpha)$ 的最大值点 $\alpha^*$</p>
<p><strong>结果</strong>（$d=128$）：
\begin{equation}
\alpha^*(n) \approx 3.5\log n \tag{49}
\end{equation}</p>
<h4 id="44">4.4 不同维度的依赖<a class="toc-link" href="#44" title="Permanent link">&para;</a></h4>
<p>对于不同的 $d$，系数会变化：</p>
<table>
<thead>
<tr>
<th>$d$</th>
<th>系数 $c$ in $\alpha^* \approx c\log n$</th>
</tr>
</thead>
<tbody>
<tr>
<td>32</td>
<td>2.8</td>
</tr>
<tr>
<td>64</td>
<td>3.2</td>
</tr>
<tr>
<td>128</td>
<td>3.5</td>
</tr>
<tr>
<td>256</td>
<td>3.7</td>
</tr>
<tr>
<td>512</td>
<td>3.9</td>
</tr>
</tbody>
</table>
<p><strong>观察</strong>：系数随 $d$ 缓慢增长，约为 $c \approx \frac{d}{36}$。</p>
<h3 id="5">5. 理论分析与推广<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 一般分布的框架<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>对于一般分布 $p(s)$，定义：
\begin{equation}
M_k(\alpha) = \mathbb{E}[e^{k\alpha s}] = \int e^{k\alpha s} p(s) ds \tag{50}
\end{equation}</p>
<p>则：
\begin{equation}
f(\alpha) \approx \frac{M_2(\alpha)}{n \cdot M_1(\alpha)^2} \tag{51}
\end{equation}</p>
<p>优化目标：
\begin{equation}
G(\alpha) = 2\alpha\left(1 - \frac{M_2(\alpha)}{n \cdot M_1(\alpha)^2}\right) \tag{52}
\end{equation}</p>
<h4 id="52">5.2 渐近展开<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>对于小 $\alpha$，使用Taylor展开：
\begin{equation}
M_k(\alpha) = \mathbb{E}[e^{k\alpha s}] \approx 1 + k\alpha\mathbb{E}[s] + \frac{(k\alpha)^2}{2}\mathbb{E}[s^2] + O(\alpha^3) \tag{53}
\end{equation}</p>
<p>假设 $\mathbb{E}[s] = 0$（对称分布），$\mathbb{E}[s^2] = \sigma^2$：
\begin{equation}
M_1(\alpha) \approx 1 + \frac{\alpha^2\sigma^2}{2}, \quad M_2(\alpha) \approx 1 + 2\alpha^2\sigma^2 \tag{54}
\end{equation}</p>
<p>代入：
\begin{equation}
f(\alpha) \approx \frac{1 + 2\alpha^2\sigma^2}{n(1 + \alpha^2\sigma^2)^2} \approx \frac{1 + 2\alpha^2\sigma^2}{n(1 + 2\alpha^2\sigma^2 + \alpha^4\sigma^4)} \tag{55}
\end{equation}</p>
<p>对小 $\alpha$：
\begin{equation}
f(\alpha) \approx \frac{1}{n}(1 + 2\alpha^2\sigma^2)(1 - 2\alpha^2\sigma^2) \approx \frac{1}{n}(1 - 4\alpha^4\sigma^4) \tag{56}
\end{equation}</p>
<p>因此：
\begin{equation}
G(\alpha) \approx 2\alpha\left(1 - \frac{1}{n}\right) + O(\alpha^5) \tag{57}
\end{equation}</p>
<p>这解释了为什么小 $\alpha$ 时 $G$ 近似线性增长。</p>
<h4 id="53">5.3 大α的行为<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p>对于大 $\alpha$，Softmax趋向one-hot分布在最大的 $s_i$：
\begin{equation}
p_i \approx \delta_{i, i^<em>}, \quad i^</em> = \arg\max_i s_i \tag{58}
\end{equation}</p>
<p>此时：
\begin{equation}
\sum_i p_i^2 \to 1 \tag{59}
\end{equation}</p>
<p>因此：
\begin{equation}
G(\alpha) = 2\alpha(1 - 1) = 0 \tag{60}
\end{equation}</p>
<p><strong>梯度消失</strong>：过大的 $\alpha$ 导致梯度消失！</p>
<h3 id="6">6. 与熵不变性的联系<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61-shannon">6.1 Shannon熵<a class="toc-link" href="#61-shannon" title="Permanent link">&para;</a></h4>
<p>Shannon熵定义为：
\begin{equation}
H = -\sum_{i=1}^n p_i \log p_i \tag{61}
\end{equation}</p>
<p>代入 $p_i = \frac{e^{\alpha s_i}}{Z}$：
\begin{equation}
H = -\sum_i p_i(\alpha s_i - \log Z) = \log Z - \alpha \sum_i p_i s_i = \log Z - \alpha \mathbb{E}_p[s] \tag{62}
\end{equation}</p>
<h4 id="62-renyi">6.2 熵与Rényi熵的关系<a class="toc-link" href="#62-renyi" title="Permanent link">&para;</a></h4>
<p>Rényi熵族定义为：
\begin{equation}
H_{\beta} = \frac{1}{1-\beta}\log\sum_i p_i^{\beta} \tag{63}
\end{equation}</p>
<p>当 $\beta = 2$：
\begin{equation}
H_2 = -\log\sum_i p_i^2 \tag{64}
\end{equation}</p>
<p><strong>关系</strong>：$H_2 \leq H_1 = H$（Shannon熵）。</p>
<p>我们的优化目标 $G(\alpha) = 2\alpha(1 - e^{-H_2})$ 直接与 $H_2$ 相关。</p>
<h4 id="63-vs">6.3 最大化梯度 vs 熵不变性<a class="toc-link" href="#63-vs" title="Permanent link">&para;</a></h4>
<p><strong>梯度最大化观点</strong>：选择 $\alpha$ 使 $G(\alpha)$ 最大。</p>
<p><strong>熵不变性观点</strong>：选择 $\alpha$ 使 $H$ 对 $n$ 不敏感。</p>
<p><strong>联系</strong>：两者都要求 $\sum_i p_i^2$ 处于"中间"状态：
- 太小（$\alpha$ 太小）：分布太平坦，熵大，但梯度小
- 太大（$\alpha$ 太大）：分布太尖锐，熵小，梯度也小
- 最优：平衡状态</p>
<p><strong>定量关系</strong>：熵不变性要求 $\alpha \propto \log n$，梯度最大化在正态分布下给出 $\alpha^* \approx 0.84\sqrt{\log n}$。</p>
<p>两者不完全一致，但都说明 $\alpha$ 应该随 $n$ 增长（对数或对数平方根）。</p>
<h3 id="7">7. 数值验证与实验<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 实验设置<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p><strong>目标</strong>：验证理论预测的 $\alpha^*$。</p>
<p><strong>方法</strong>：
1. 对不同的 $n$，采样 $s_i \sim \mathcal{N}(0, 1)$
2. 对每个 $\alpha \in [0.1, 5]$，计算 $G(\alpha)$
3. 找到实际的 $\alpha^<em>_{\text{实验}}$
4. 与理论预测 $\alpha^</em>_{\text{理论}} = 0.84\sqrt{\log n}$ 比较</p>
<h4 id="72">7.2 正态分布结果<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>$n$</th>
<th>$\log n$</th>
<th>$\alpha^*_{\text{理论}}$</th>
<th>$\alpha^*_{\text{实验}}$</th>
<th>误差</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>4.61</td>
<td>1.80</td>
<td>1.85</td>
<td>2.8%</td>
</tr>
<tr>
<td>200</td>
<td>5.30</td>
<td>1.93</td>
<td>1.96</td>
<td>1.6%</td>
</tr>
<tr>
<td>512</td>
<td>6.24</td>
<td>2.10</td>
<td>2.08</td>
<td>-1.0%</td>
</tr>
<tr>
<td>1000</td>
<td>6.91</td>
<td>2.21</td>
<td>2.20</td>
<td>-0.5%</td>
</tr>
<tr>
<td>2000</td>
<td>7.60</td>
<td>2.32</td>
<td>2.34</td>
<td>0.9%</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>：理论公式 $\alpha^* \approx 0.84\sqrt{\log n}$ 与实验高度吻合！</p>
<h4 id="73-d128">7.3 余弦分布结果（$d=128$）<a class="toc-link" href="#73-d128" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>$n$</th>
<th>$\log n$</th>
<th>$\alpha^*_{\text{理论}}$</th>
<th>$\alpha^*_{\text{实验}}$</th>
<th>误差</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>4.61</td>
<td>16.1</td>
<td>15.8</td>
<td>-1.9%</td>
</tr>
<tr>
<td>200</td>
<td>5.30</td>
<td>18.6</td>
<td>18.3</td>
<td>-1.6%</td>
</tr>
<tr>
<td>512</td>
<td>6.24</td>
<td>21.8</td>
<td>22.1</td>
<td>1.4%</td>
</tr>
<tr>
<td>1000</td>
<td>6.91</td>
<td>24.2</td>
<td>24.0</td>
<td>-0.8%</td>
</tr>
<tr>
<td>2000</td>
<td>7.60</td>
<td>26.6</td>
<td>26.9</td>
<td>1.1%</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>：理论公式 $\alpha^* \approx 3.5\log n$ 同样准确！</p>
<h3 id="8">8. 实际应用建议<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81-attention">8.1 标准Attention的改进<a class="toc-link" href="#81-attention" title="Permanent link">&para;</a></h4>
<p>标准Scaled Dot-Product Attention使用：
\begin{equation}
\alpha = \frac{1}{\sqrt{d_k}} \tag{65}
\end{equation}</p>
<p><strong>问题</strong>：这是为了方差归一化，但未考虑梯度优化。</p>
<p><strong>改进方案1</strong>（基于正态假设）：
\begin{equation}
\alpha_{\text{new}} = \frac{0.84\sqrt{\log n}}{\sqrt{d_k}} \approx \frac{2.5}{\sqrt{d_k}} \quad (n=512) \tag{66}
\end{equation}</p>
<p>相比标准的 $1/\sqrt{d_k}$，增加了2.5倍！</p>
<p><strong>改进方案2</strong>（基于余弦相似度）：
如果使用归一化的 $q, k$（如在某些对比学习中），应该使用：
\begin{equation}
\alpha_{\text{new}} = 3.5\log n \approx 21.8 \quad (n=512, d=128) \tag{67}
\end{equation}</p>
<p>这解释了为什么SimCSE等方法中温度参数 $\tau \approx 0.05$，即 $\alpha = 1/\tau = 20$！</p>
<h4 id="82-decoder">8.2 Decoder的特殊考虑<a class="toc-link" href="#82-decoder" title="Permanent link">&para;</a></h4>
<p>在autoregressive Decoder中，每个位置 $i$ 只能看到前 $i$ 个token，因此有效的 $n$ 是变化的。</p>
<p><strong>方案1</strong>（保守）：使用最大长度 $n_{\max}$ 计算 $\alpha$。</p>
<p><strong>方案2</strong>（激进）：使用平均长度 $n_{\max}/2$。</p>
<p><strong>方案3</strong>（位置相关）：每个位置 $i$ 使用不同的 $\alpha_i$，但这会破坏并行性。</p>
<h4 id="83">8.3 与学习率的关系<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p>梯度大小直接影响学习率的选择。如果增大 $\alpha$，梯度增大，应相应减小学习率：
\begin{equation}
\eta_{\text{new}} = \eta_{\text{old}} \cdot \frac{\alpha_{\text{old}}}{\alpha_{\text{new}}} \tag{68}
\end{equation}</p>
<p>例如，如果 $\alpha$ 从1增加到2.5，学习率应从 $10^{-4}$ 减小到 $4 \times 10^{-5}$。</p>
<h3 id="9">9. 理论推广与开放问题<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 其他范数<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p>我们使用了L1范数 $|J|_1$。其他选择：</p>
<p><strong>Frobenius范数</strong>：
\begin{equation}
|J|<em i_j="i,j">F^2 = \sum</em>
\end{equation}} J_{ij}^2 = \alpha^2\sum_i p_i^2(1-p_i)^2 + \alpha^2\sum_{i \neq j} p_i^2 p_j^2 \tag{69</p>
<p>这更难优化，但可能给出不同的 $\alpha^*$。</p>
<p><strong>谱范数</strong>：
\begin{equation}
|J|<em _124_v_124_="1">2 = \max</em>
\end{equation}} |Jv| = \alpha \max_i p_i(1-p_i) \tag{70</p>
<p>这只关注最大特征值，可能过于局部。</p>
<h4 id="92">9.2 非独立同分布<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p>我们假设 $s_i$ 独立同分布，但实际中 $s_i = q \cdot k_i$ 有结构。</p>
<p><strong>相关性</strong>：如果 $k_i$ 之间有相关性，$\sum_i p_i^2$ 的行为会改变。</p>
<p><strong>非均匀性</strong>：如果某些 $s_i$ 系统性地更大，最优 $\alpha$ 会变化。</p>
<h4 id="93">9.3 动态α<a class="toc-link" href="#93" title="Permanent link">&para;</a></h4>
<p>固定的 $\alpha$ 对所有层和所有时刻相同。但是否可以：
- <strong>层相关</strong>：不同层使用不同 $\alpha_{\ell}$
- <strong>位置相关</strong>：不同位置使用不同 $\alpha_i$
- <strong>学习的</strong>：将 $\alpha$ 作为可学习参数</p>
<p><strong>挑战</strong>：增加超参数可能导致过拟合。</p>
<h3 id="10">10. 总结<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 核心发现<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>梯度范数</strong>：$G(\alpha) = 2\alpha(1 - \sum_i p_i^2)$ 有最大值点</li>
<li><strong>正态分布</strong>：$\alpha^* \approx 0.84\sqrt{\log n}$，而非常数</li>
<li><strong>余弦分布</strong>：$\alpha^* \approx 3.5\log n$（$d=128$）</li>
<li><strong>实践意义</strong>：标准的 $1/\sqrt{d}$ 可能不是最优的</li>
</ol>
<h4 id="102">10.2 与其他工作的比较<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方法</th>
<th>缩放因子</th>
<th>理论依据</th>
</tr>
</thead>
<tbody>
<tr>
<td>标准Attention</td>
<td>$1/\sqrt{d}$</td>
<td>方差归一化</td>
</tr>
<tr>
<td>熵不变性</td>
<td>$\log n / \log 512$</td>
<td>熵的 $n$-不变性</td>
</tr>
<tr>
<td>梯度最大化（本文）</td>
<td>$0.84\sqrt{\log n}/\sqrt{d}$</td>
<td>梯度范数最大化</td>
</tr>
</tbody>
</table>
<p><strong>统一视角</strong>：都认识到缩放因子应该依赖于 $n$，但具体形式不同。</p>
<h4 id="103">10.3 未来方向<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>经验验证</strong>：在大规模语言模型上测试改进的缩放因子</li>
<li><strong>理论完善</strong>：更严格的分析非i.i.d.情况</li>
<li><strong>自适应方法</strong>：开发自动调整 $\alpha$ 的算法</li>
<li><strong>多模态</strong>：扩展到图像-文本等跨模态Attention</li>
</ol>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="随机分词再探从viterbi-sampling到完美采样算法.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#254 随机分词再探：从Viterbi Sampling到完美采样算法</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="简单得令人尴尬的fsq四舍五入超越了vq-vae.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#256 简单得令人尴尬的FSQ："四舍五入"超越了VQ-VAE</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#attentionscale">从梯度最大化看Attention的Scale操作</a><ul>
<li><a href="#_1">已有结果</a></li>
<li><a href="#_2">计算梯度</a></li>
<li><a href="#_3">正态分布</a></li>
<li><a href="#_4">余弦分布</a></li>
<li><a href="#_5">相关思考</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a><ul>
<li><a href="#1-softmax">1. Softmax梯度的完整推导</a></li>
<li><a href="#2">2. 梯度范数的优化目标</a></li>
<li><a href="#3">3. 正态分布下的最优α</a></li>
<li><a href="#4">4. 余弦分布下的最优α</a></li>
<li><a href="#5">5. 理论分析与推广</a></li>
<li><a href="#6">6. 与熵不变性的联系</a></li>
<li><a href="#7">7. 数值验证与实验</a></li>
<li><a href="#8">8. 实际应用建议</a></li>
<li><a href="#9">9. 理论推广与开放问题</a></li>
<li><a href="#10">10. 总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>