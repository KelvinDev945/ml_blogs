<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Muon优化器赏析：从向量到矩阵的本质跨越 | ML & Math Blog Posts</title>
    <meta name="description" content="Muon优化器赏析：从向量到矩阵的本质跨越
原文链接: https://spaces.ac.cn/archives/10592
发布日期: 

随着LLM时代的到来，学术界对于优化器的研究热情似乎有所减退。这主要是因为目前主流的AdamW已经能够满足大多数需求，而如果对优化器“大动干戈”，那么需要巨大的验证成本。因此，当前优化器的变化，多数都只是工业界根据自己的训练经验来对AdamW打的一些小补丁...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">Muon优化器赏析：从向量到矩阵的本质跨越</h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/10592" target="_blank">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <span class="tag"><i class="fas fa-tag"></i> 矩阵</span>
                <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                <span class="tag"><i class="fas fa-tag"></i> 谱范数</span>
                <span class="tag"><i class="fas fa-tag"></i> muon</span>
                
            </div>
            
        </header>

        <!-- Post Body -->
        <div class="post-content">
            <h1 id="muon">Muon优化器赏析：从向量到矩阵的本质跨越</h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10592">https://spaces.ac.cn/archives/10592</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>随着LLM时代的到来，学术界对于优化器的研究热情似乎有所减退。这主要是因为目前主流的AdamW已经能够满足大多数需求，而如果对优化器“大动干戈”，那么需要巨大的验证成本。因此，当前优化器的变化，多数都只是工业界根据自己的训练经验来对AdamW打的一些小补丁。</p>
<p>不过，最近推特上一个名为“<a href="https://github.com/KellerJordan/Muon">Muon</a>”的优化器颇为热闹，它声称比AdamW更为高效，且并不只是在Adam基础上的“小打小闹”，而是体现了关于向量与矩阵差异的一些值得深思的原理。本文让我们一起赏析一番。</p>
<p><a href="/usr/uploads/2024/12/125501438.jpeg" title="点击查看原图"><img alt="Muon与AdamW效果对比（来源：推特@Yuchenj_UW）" src="/usr/uploads/2024/12/125501438.jpeg" /></a></p>
<p>Muon与AdamW效果对比（来源：推特@Yuchenj_UW）</p>
<h2 id="_1">算法初探</h2>
<p>Muon全称是“MomentUm Orthogonalized by Newton-schulz”，它适用于矩阵参数$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，其更新规则是<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{M}<em t-1="t-1">t =&amp;\, \beta\boldsymbol{M}</em>} + \boldsymbol{G<em t-1="t-1">t \\[5pt]<br />
\boldsymbol{W}_t =&amp;\, \boldsymbol{W}</em>} - \eta_t [\text{msign}(\boldsymbol{M<em t-1="t-1">t) + \lambda \boldsymbol{W}</em>] \\<br />
\end{aligned}\end{equation}<br />
这里$\text{msign}$是<a href="https://en.wikipedia.org/wiki/Matrix_sign_function">矩阵符号函数</a>，它并不是简单地对矩阵每个分量取$\text{sign}$操作，而是$\text{sign}$函数的矩阵化推广，它跟<a href="/archives/10407">SVD</a>的关系是：<br />
\begin{equation}\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}^{\top} = \text{SVD}(\boldsymbol{M}) \quad\Rightarrow\quad \text{msign}(\boldsymbol{M}) = \boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>}^{\top}\end{equation<br />
其中$\boldsymbol{U}\in\mathbb{R}^{n\times n},\boldsymbol{\Sigma}\in\mathbb{R}^{n\times m},\boldsymbol{V}\in\mathbb{R}^{m\times m}$，$r$是$\boldsymbol{M}$的秩。更多的理论细节我们稍后再展开，这里我们先来尝试直观感知如下事实：</p>
<blockquote>
<p>Muon是一个类似于Adam的自适应学习率优化器。</p>
</blockquote>
<p>像Adagrad、RMSprop、Adam等自适应学习率优化器的特点是通过除以 <em>梯度平方的滑动平均的平方根</em> 来调整每个参数的更新量，这达到了两个效果：1、损失函数的常数缩放不影响优化轨迹；2、每个参数分量的更新幅度尽可能一致。Muon正好满足这两个特性：</p>
<blockquote>
<p>1、损失函数乘以$\lambda$，$\boldsymbol{M}$也会乘以$\lambda$，结果是$\boldsymbol{\Sigma}$被乘以$\lambda$，但Muon最后的更新量是将$\boldsymbol{\Sigma}$变为单位阵，所以不影响优化结果；</p>
<p>2、当$\boldsymbol{M}$被SVD为$\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$时，$\boldsymbol{\Sigma}$的不同奇异值体现了$\boldsymbol{M}$的“各向异性”，而将它们都 <em>置一</em> 则更加各向同性，也起到同步更新幅度的作用。</p>
</blockquote>
<p>对了，关于第2点，有没有读者想起了<a href="/archives/8069">BERT-whitening</a>？另外要指出的是，Muon还有个Nesterov版，它只是将更新规则中的$\text{msign}(\boldsymbol{M}_t)$换成$\text{msign}(\beta\boldsymbol{M}_t + \boldsymbol{G}_t)$，其余部份完全一致，简单起见就不展开介绍了。</p>
<p>（考古：事后发现，2015年的论文<a href="https://proceedings.mlr.press/v38/carlson15.html">《Stochastic Spectral Descent for Restricted Boltzmann Machines》</a>已经提出过跟Muon大致相同的优化算法，当时称为“Stochastic Spectral Descent”。）</p>
<h2 id="_2">符号函数</h2>
<p>利用SVD，我们还可以证明恒等式<br />
\begin{equation}\text{msign}(\boldsymbol{M}) = (\boldsymbol{M}\boldsymbol{M}^{\top})^{-1/2}\boldsymbol{M}= \boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2}\label{eq:msign-id}\end{equation}<br />
其中${}^{-1/2}$是矩阵的$1/2$次幂的逆矩阵，如果不可逆的话则取<a href="/archives/10366">伪逆</a>。这个恒等式能让我们更好理解为什么$\text{msign}$是$\text{sign}$的矩阵推广：对于标量$x$我们有$\text{sign}(x)=x(x^2)^{-1/2}$，正是上式的一个特殊情形（当$\boldsymbol{M}$是$1\times 1$矩阵时）。这个特殊例子还可以推广到对角阵$\boldsymbol{M}=\text{diag}(\boldsymbol{m})$：<br />
\begin{equation}\text{msign}(\boldsymbol{M}) = \text{diag}(\boldsymbol{m})[\text{diag}(\boldsymbol{m})^2]^{-1/2} = \text{diag}(\text{sign}(\boldsymbol{m}))=\text{sign}(\boldsymbol{M})\end{equation}<br />
其中$\text{sign}(\boldsymbol{m})$、$\text{sign}(\boldsymbol{M})$是指向量/矩阵的每个分量都取$\text{sign}$。上式意味着，当$\boldsymbol{M}$是对角阵时，Muon就退化为带动量的<a href="/archives/10542#%E8%87%AA%E9%80%82%E5%BA%94%E7%89%88">SignSGD</a>（Signum）或笔者所提的<a href="/archives/9512">Tiger</a>，它们都是Adam的经典近似。反过来说，Muon与Signum、Tiger的区别就是Element-wise的$\text{sign}(\boldsymbol{M})$替换成了矩阵版$\text{msign}(\boldsymbol{M})$。</p>
<p>对于$n$维向量来说，我们还可以视为$n\times 1$的矩阵，此时$\text{msign}(\boldsymbol{m}) = \boldsymbol{m}/\Vert\boldsymbol{m}\Vert_2$正好是$l_2$归一化。所以，在Muon框架下对向量我们有两种视角：一是对角矩阵，如LayerNorm的gamma参数，结果是对动量取$\text{sign}$；二是$n\times 1$的矩阵，结果是对动量做$l_2$归一化。此外，输入和输出的Embedding虽然也是矩阵，但它们使用上是稀疏的，所以更合理的方式也是将它们当成多个向量独立处理。</p>
<p>当$m=n=r$时，$\text{msign}(\boldsymbol{M})$还有一个意义是“最优正交近似”：<br />
\begin{equation}\text{msign}(\boldsymbol{M}) = \mathop{\text{argmin}}<em _boldsymbol_O="\boldsymbol{O">{\boldsymbol{O}^{\top}\boldsymbol{O} = \boldsymbol{I}}\Vert \boldsymbol{M} - \boldsymbol{O}\Vert_F^2 \label{eq:nearest-orth}\end{equation}<br />
类似地，对于$\text{sign}(\boldsymbol{M})$我们可以写出（假设$\boldsymbol{M}$没有零元素）：<br />
\begin{equation}\text{sign}(\boldsymbol{M}) = \mathop{\text{argmin}}</em>}\in\{-1,1\}^{n\times m}}\Vert \boldsymbol{M} - \boldsymbol{O}\Vert_F^2\end{equation<br />
不论是$\boldsymbol{O}^{\top}\boldsymbol{O} = \boldsymbol{I}$还是$\boldsymbol{O}\in\{-1,1\}^{n\times m}$，我们都可以视为对更新量的一种规整化约束，所以Muon和Signum、Tiger可以视作是同一思路下的优化器，它们都以动量$\boldsymbol{M}$为出发点来构建更新量，只是为更新量选择了不同的规整化方法。</p>
<blockquote>
<p><strong>式$\eqref{eq:nearest-orth}$的证明</strong> ：对于正交矩阵$\boldsymbol{O}$，我们有<br />
 \begin{equation}\begin{aligned}<br />
 \Vert \boldsymbol{M} - \boldsymbol{O}\Vert_F^2 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + \Vert \boldsymbol{O}\Vert_F^2 - 2\langle\boldsymbol{M},\boldsymbol{O}\rangle_F \\[5pt]<br />
 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + n - 2\text{Tr}(\boldsymbol{M}\boldsymbol{O}^{\top})\\[5pt]<br />
 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + n - 2\text{Tr}(\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}\boldsymbol{O}^{\top})\\[5pt]<br />
 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + n - 2\text{Tr}(\boldsymbol{\Sigma}\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U})\\<br />
 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + n - 2\sum_{i=1}^n \boldsymbol{\Sigma}<em i_i="i,i">{i,i}(\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U})</em><br />
 \end{aligned}\end{equation}<br />
 其中涉及到的运算规则我们在<a href="/archives/10366">伪逆</a>中已经介绍过。由于$\boldsymbol{U},\boldsymbol{V},\boldsymbol{O}$都是正交矩阵，所以$\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U}$也是正交矩阵，正交矩阵的每个分量必然不超过1，又因为$\boldsymbol{\Sigma}<em i_i="i,i">{i,i} &gt; 0$，所以上式取最小值对应于每个$(\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U})</em>$。}$取最大值，即$(\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U})_{i,i}=1$，这意味着$\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U}=\boldsymbol{I}$，即$\boldsymbol{O}=\boldsymbol{U}\boldsymbol{V}^{\top</p>
<p>该结论还可以仔细地推广到$m,n,r$不相等的情形，但这里不作进一步展开。</p>
</blockquote>
<h2 id="_3">迭代求解</h2>
<p>实践中，如果每一步都对$\boldsymbol{M}$做SVD来求解$\text{msign}(\boldsymbol{M})$的话，那么计算成本还是比较大的，因此作者提出了用Newton-schulz迭代来近似计算$\text{msign}(\boldsymbol{M})$。</p>
<p>迭代的出发点是恒等式$\eqref{eq:msign-id}$，不失一般性，我们假设$n\geq m$，然后考虑在$\boldsymbol{M}^{\top}\boldsymbol{M}=\boldsymbol{I}$处泰勒展开$(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2}$，展开的方式是直接将标量函数$t^{-1/2}$的结果用到矩阵中：<br />
\begin{equation}t^{-1/2} = 1 - \frac{1}{2}(t-1) + \frac{3}{8}(t-1)^2 - \frac{5}{16}(t-1)^3 + \cdots\end{equation}<br />
保留到二阶，结果是$(15 - 10t + 3t^2)/8$，那么我们有<br />
\begin{equation}\text{msign}(\boldsymbol{M}) = \boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2}\approx \frac{15}{8}\boldsymbol{M} - \frac{5}{4}\boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M}) + \frac{3}{8}\boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^2\end{equation}<br />
假如$\boldsymbol{X}<em t_1="t+1">t$是$\text{msign}(\boldsymbol{M})$的某个近似，我们认为将它代入上式后，会得到$\text{msign}(\boldsymbol{M})$的一个更好的近似，于是我们得到一个可用的迭代格式<br />
\begin{equation}\boldsymbol{X}</em>} = \frac{15}{8}\boldsymbol{X}_t - \frac{5}{4}\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t) + \frac{3}{8}\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^2\end{equation<br />
然而，查看Muon的官方代码我们就会发现，它里边的Newton-schulz迭代确实是这个形式，但三个系数却是$(3.4445, -4.7750, 2.0315)$，而且作者没有给出数学推导，只有一段语焉不详的注释：  </p>
<p><a href="/usr/uploads/2024/12/39782973.png" title="点击查看原图"><img alt="Muon优化器的Newton-schulz迭代" src="/usr/uploads/2024/12/39782973.png" /></a></p>
<p>Muon优化器的Newton-schulz迭代</p>
<h2 id="_4">收敛加速</h2>
<p>为了猜测官方迭代算法的来源，我们考虑一般的迭代过程<br />
\begin{equation}\boldsymbol{X}_{t+1} = a\boldsymbol{X}_t + b\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t) + c\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^2\label{eq:iteration}\end{equation}<br />
其中$a,b,c$是三个待求解的系数，如果想要更高阶的迭代算法，我们也可以逐次补充$\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^3$、$\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^4$等项，下面的分析过程是通用的。</p>
<p>我们选择的初始值是$\boldsymbol{X}<em t_1="t+1">0=\boldsymbol{M}/\Vert\boldsymbol{M}\Vert_F$，$\Vert\cdot\Vert_F$是矩阵的$F$范数，选择的依据是除以$\Vert\boldsymbol{M}\Vert_F$不改变SVD的$\boldsymbol{U},\boldsymbol{V}$，但可以让$\boldsymbol{X}_0$的所有奇异值都在$[0,1]$之间，让迭代的初始奇异值更标准一些。现在假设$\boldsymbol{X}_t$可以SVD为$\boldsymbol{U}\boldsymbol{\Sigma}_t\boldsymbol{V}^{\top}$，那么代入上式我们可以得到<br />
\begin{equation}\boldsymbol{X}</em>} = \boldsymbol{U<em t_:r_:r_="t,[:r,:r]">{[:,:r]}(a \boldsymbol{\Sigma}</em>} + b \boldsymbol{\Sigma<em t_:r_:r_="t,[:r,:r]">{t,[:r,:r]}^3 + c \boldsymbol{\Sigma}</em>}^5)\boldsymbol{V<em _:r_:r_="[:r,:r]">{[:,:r]}^{\top}\end{equation}<br />
因此，式$\eqref{eq:iteration}$实际上在迭代奇异值组成的对角阵$\boldsymbol{\Sigma}</em>}$，如果记$\boldsymbol{X<em _:_:r_="[:,:r]">t=\boldsymbol{U}</em>}\boldsymbol{\Sigma<em _:_:r_="[:,:r]">{t,[:r,:r]}\boldsymbol{V}</em>}^{\top}$，那么有$\boldsymbol{\Sigma<em t_:r_:r_="t,[:r,:r]">{t+1,[:r,:r]} = g(\boldsymbol{\Sigma}</em>})$，其中$g(x) = ax + bx^3 + cx^5$。又因为对角阵的幂等于对角线元素各自取幂，所以问题简化成单个奇异值$\sigma$的迭代。我们的目标是计算$\boldsymbol{U<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>}^{\top}$，换言之希望通过迭代将$\boldsymbol{\Sigma<em t_1="t+1">{[:r,:r]}$变为单位阵，这又可以简化为迭代$\sigma</em> = g(\sigma_t)$将单个奇异值变为1。</p>
<p>受<a href="https://x.com/leloykun/status/1846165001746501899">@leloykun</a>启发，我们将$a,b,c$的选择视为一个最优化问题，目标是让迭代过程对于任意初始奇异值都收敛得尽可能快。首先我们将$g(x)$重新参数化为<br />
\begin{equation}g(x) = x + \kappa x(x^2 - x_1^2)(x^2 - x_2^2)\end{equation}<br />
其中$x_1 \leq x_2$。该参数化的好处是直观表示出了迭代的5个不动点$0,\pm x_1,\pm x_2$。由于我们的目标是收敛到1，因此初始化我们选择$x_1 &lt; 1,x_2 &gt; 1$，想法是不管迭代过程往$x_1$走还是往$x_2$走，结果都是1附近。</p>
<p>接下来，我们确定迭代步数$T$，这样迭代过程就称为一个确定性函数，然后我们将矩阵的形状（即$n,m$）确定好，就可以采样一批矩阵，并通过SVD来算奇异值。最后，我们将这些奇异值当成输入，而目标输出则是1，损失函数是平方误差，整个模型完全可导，可以用梯度下降解决（<a href="https://x.com/leloykun/status/1846165001746501899">@leloykun</a>则假设了$x_1 + x_2 = 2$，然后用网格搜索来求解）。</p>
<p>一些计算结果：<br />
\begin{array}{ccc|ccc|ccc|c|c}<br />
\hline<br />
n &amp; m &amp; T &amp; \kappa &amp; x_1 &amp; x_2 &amp; a &amp; b &amp; c &amp; \text{mse} &amp; \text{mse}_{\text{o}}\\<br />
\hline<br />
1024 &amp; 1024 &amp; 3 &amp; 7.020 &amp; 0.830 &amp; 0.830 &amp; 4.328 &amp; -9.666 &amp; 7.020 &amp; 0.10257 &amp; 0.18278 \\<br />
1024 &amp; 1024 &amp; 5 &amp; 1.724 &amp; 0.935 &amp; 1.235 &amp; 3.297 &amp; -4.136 &amp; 1.724 &amp; 0.02733 &amp; 0.04431 \\<br />
2048 &amp; 1024 &amp; 3 &amp; 7.028 &amp; 0.815 &amp; 0.815 &amp; 4.095 &amp; -9.327 &amp; 7.028 &amp; 0.01628 &amp; 0.06171 \\<br />
2048 &amp; 1024 &amp; 5 &amp; 1.476 &amp; 0.983 &amp; 1.074 &amp; 2.644 &amp; -3.128 &amp; 1.476 &amp; 0.00038 &amp; 0.02954 \\<br />
4096 &amp; 1024 &amp; 3 &amp; 6.948 &amp; 0.802 &amp; 0.804 &amp; 3.886 &amp; -8.956 &amp; 6.948 &amp; 0.00371 &amp; 0.02574 \\<br />
4096 &amp; 1024 &amp; 5 &amp; 1.214 &amp; 1.047 &amp; 1.048 &amp; 2.461 &amp; -2.663 &amp; 1.214 &amp; 0.00008 &amp; 0.02563 \\<br />
\hline<br />
2048 &amp; 2048 &amp; 3 &amp; 11.130 &amp; 0.767 &amp; 0.767 &amp; 4.857 &amp; -13.103 &amp; 11.130 &amp; 0.10739 &amp; 0.24410 \\<br />
2048 &amp; 2048 &amp; 5 &amp; 1.779 &amp; 0.921 &amp; 1.243 &amp; 3.333 &amp; -4.259 &amp; 1.779 &amp; 0.03516 &amp; 0.04991 \\<br />
4096 &amp; 4096 &amp; 3 &amp; 18.017 &amp; 0.705 &amp; 0.705 &amp; 5.460 &amp; -17.929 &amp; 18.017 &amp; 0.11303 &amp; 0.33404 \\<br />
4096 &amp; 4096 &amp; 5 &amp; 2.057 &amp; 0.894 &amp; 1.201 &amp; 3.373 &amp; -4.613 &amp; 2.057 &amp; 0.04700 &amp; 0.06372 \\<br />
8192 &amp; 8192 &amp; 3 &amp; 30.147 &amp; 0.643 &amp; 0.643 &amp; 6.139 &amp; -24.893 &amp; 30.147 &amp; 0.11944 &amp; 0.44843 \\<br />
8192 &amp; 8192 &amp; 5 &amp; 2.310 &amp; 0.871 &amp; 1.168 &amp; 3.389 &amp; -4.902 &amp; 2.310 &amp; 0.05869 &amp; 0.07606 \\<br />
\hline<br />
\end{array}</p>
<p>这里的$\text{mse}_{\text{o}}$是有Muon作者的$a,b,c$算出来的结果。从表格可以看出，结果跟矩阵大小、迭代步数都有明显关系；从损失函数来看，非方阵比方阵更容易收敛；Muon作者给出的$a,b,c$，大概是迭代步数为5时方阵的最优解。当迭代步数给定时，结果依赖于矩阵大小，这本质上是依赖于奇异值的分布，关于这个分布有个值得一提的结果是当$n,m\to\infty$时为<a href="https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution">Marchenko–Pastur分布</a>。</p>
<p>参考代码：</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">key</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;SVD&#39;</span><span class="p">):</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">data</span><span class="p">,</span> <span class="n">S</span> <span class="o">/</span> <span class="p">(</span><span class="n">S</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">**</span><span class="mf">0.5</span><span class="p">])</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">k</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">w</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">f_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">w</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">):</span>
    <span class="n">u</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">u</span> <span class="o">+</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>  <span class="c1"># 动量加速</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">u</span>

<span class="n">k</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">w</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">k</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">k</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">x1</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">x2</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">a</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">b</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">c</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">)</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>

<h2 id="_5">一些思考</h2>
<p>如果按照默认选择$T=5$，那么对于一个$n\times n$的矩阵参数，Muon的每一步更新至少需要算15次$n\times n$与$n\times n$的矩阵乘法，这计算量毋庸置疑是比Adam明显大的，由此可能有读者担心Muon实践上是否可行。</p>
<p>事实上，这种担心是多余的，Muon计算虽然比Adam复杂，但每一步增加的时间不多，笔者的结论是5%内，Muon作者则声称能做到2%。这是因为Muon的矩阵乘法发生在当前梯度计算完后、下一梯度计算前，这期间几乎所有的算力都是空闲的，而这些矩阵乘法是静态大小且可以并行，因此不会明显增加时间成本，反而是Muon比Adam少一组缓存变量，显存成本更低。</p>
<p>Muon最值得深思的地方，其实是向量与矩阵的内在区别，以及它对优化的影响。SGD、Adam、Tiger等常见优化器的更新规则是Element-wise的，即不论向量、矩阵参数，实际都视为一个大向量，分量按照相同的规则独立地更新。具备这个特性的优化器往往理论分析起来更加简化，也方便张量并行，因为一个大矩阵切成两个小矩阵独立处理，并不改变优化轨迹。</p>
<p>但Muon不一样，它以矩阵为基本单位，考虑了矩阵的一些独有特性。可能有些读者会奇怪：矩阵和向量不都只是一堆数字的排列吗，能有什么区别？举个例子，矩阵我们有“迹（trace）”这个概念，它是对角线元素之和，这个概念不是瞎定义的，它有一个重要特性是在相似变换下保持不变，它还等于矩阵的所有特征值之和。从这个例子就可以看出，矩阵的对角线元素跟非对角线元素，地位其实是不完全对等的。而Muon正是因为考虑了这种不对等性，才有着更好的效果。</p>
<p>当然，这也会导致一些负面影响。如果一个矩阵被划分到不同设备上，那么用Muon时就需要将它们的梯度就需要汇聚起来再计算更新量了，而不能每个设备独立更新，这增加了通信成本。即便我们不考虑并行方面，这个问题也存在，比如Multi-Head Attention一般是通过单个大矩阵投影到$Q$（$K,V$同理），然后用reshape的方式得到多个Head，这样在模型参数中就只有单个矩阵，但它本质上是多个小矩阵，所以按道理我们需要将大矩阵拆开成多个小矩阵独立更新。</p>
<p>总之，Muon这种非Element-wise的更新规则，在捕捉向量与矩阵的本质差异的同时，也会引入一些小问题，这可能会不满足一些读者的审美。</p>
<p>（补充：几乎在本博客发布的同时，Muon的作者Keller Jordan也发布了自己的一篇博客<a href="https://kellerjordan.github.io/posts/muon/">《Muon: An optimizer for hidden layers in neural networks》</a>。）</p>
<h2 id="_6">范数视角</h2>
<p>从理论上看，Muon捕捉了矩阵的什么关键特性呢？也许接下来的范数视角可以回答我们的问题。</p>
<p>这一节的讨论主要参考了论文<a href="https://ieeexplore.ieee.org/abstract/document/7347351">《Stochastic Spectral Descent for Discrete Graphical Models》</a>和<a href="https://papers.cool/arxiv/2409.20325">《Old Optimizer, New Norm: An Anthology》</a>，特别是后一篇。不过其中的出发点并不是新的，我们在<a href="/archives/9660">《梯度流：探索通向最小值之路》</a>就已经简单涉猎过：对于向量参数$\boldsymbol{w}\in\mathbb{R}^n$，我们将下一步的更新规则定义为<br />
\begin{equation}\boldsymbol{w}<em _boldsymbol_w="\boldsymbol{w">{t+1} = \mathop{\text{argmin}}</em>}} \frac{\Vert\boldsymbol{w} - \boldsymbol{w<em t_1="t+1">t\Vert^2}{2\eta_t} + \mathcal{L}(\boldsymbol{w})\end{equation}<br />
其中$\Vert\Vert$是某个向量范数，这称为在某个范数约束下的“最速梯度下降”。接着假设$\eta_t$足够小，那么第一项占主导，这意味着$\boldsymbol{w}</em>}$与$\boldsymbol{w<em t_1="t+1">t$会很接近，于是我们假设$\mathcal{L}(\boldsymbol{w})$的一阶近似够用了，于是问题简化成<br />
\begin{equation}\boldsymbol{w}</em>} = \mathop{\text{argmin}<em _boldsymbol_w="\boldsymbol{w">{\boldsymbol{w}} \frac{\Vert\boldsymbol{w} - \boldsymbol{w}_t\Vert^2}{2\eta_t} + \mathcal{L}(\boldsymbol{w}_t) + \nabla</em><em t_1="t+1">t}\mathcal{L}(\boldsymbol{w}_t)^{\top}(\boldsymbol{w}-\boldsymbol{w}_t)\end{equation}<br />
记$\Delta\boldsymbol{w}</em>} = \boldsymbol{w<em _boldsymbol_w="\boldsymbol{w">{t+1}-\boldsymbol{w}_t, \boldsymbol{g}_t = \nabla</em><em t_1="t+1">t}\mathcal{L}(\boldsymbol{w}_t)$，那么可以简写成<br />
\begin{equation}\Delta\boldsymbol{w}</em>} = \mathop{\text{argmin}<em t_1="t+1">{\Delta\boldsymbol{w}} \frac{\Vert\Delta\boldsymbol{w}\Vert^2}{2\eta_t} + \boldsymbol{g}_t^{\top}\Delta\boldsymbol{w}\end{equation}<br />
计算$\Delta\boldsymbol{w}</em>$的一般思路是求导，但<a href="https://papers.cool/arxiv/2409.20325">《Old Optimizer, New Norm: An Anthology》</a>提供了一个不用求导的统一方案：将$\Delta\boldsymbol{w}$分解为范数$\gamma = \Vert\Delta\boldsymbol{w}\Vert$和方向向量$\boldsymbol{\varphi} = -\Delta\boldsymbol{w}/\Vert\Delta\boldsymbol{w}\Vert$，于是<br />
\begin{equation}\min_{\Delta\boldsymbol{w}} \frac{\Vert\Delta\boldsymbol{w}\Vert^2}{2\eta_t} + \boldsymbol{g}<em 0_="0," _Vert_boldsymbol_varphi="\Vert\boldsymbol{\varphi" _gamma_geq="\gamma\geq">t^{\top}\Delta\boldsymbol{w} = \min</em>}\Vert=1} \frac{\gamma^2}{2\eta_t} - \gamma\boldsymbol{g<em 0="0" _gamma_geq="\gamma\geq">t^{\top}\boldsymbol{\varphi} = \min</em>} \frac{\gamma^2}{2\eta_t} - \gamma\bigg(\underbrace{\max_{\Vert\boldsymbol{\varphi}\Vert=1}\boldsymbol{g<em _text_记为="\text{记为">t^{\top}\boldsymbol{\varphi}}</em>}\Vert \boldsymbol{g<em t_1="t+1">t\Vert^{\dagger}}\bigg)\end{equation}<br />
$\gamma$只是一个标量，跟学习率类似，容易求得最优值是$\eta_t\Vert \boldsymbol{g}_t\Vert^{\dagger}$，而更新方向则是最大化$\boldsymbol{g}_t^{\top}\boldsymbol{\varphi}$（$\Vert\boldsymbol{\varphi}\Vert=1$）的$\boldsymbol{\varphi}^<em>$。现在代入欧氏范数即$\Vert\boldsymbol{\varphi}\Vert_2 = \sqrt{\boldsymbol{\varphi}^{\top}\boldsymbol{\varphi}}$，我们就有$\Vert \boldsymbol{g}_t\Vert^{\dagger}=\Vert \boldsymbol{g}_t\Vert_2$和$\boldsymbol{\varphi}^</em> = \boldsymbol{g}_t/\Vert\boldsymbol{g}_t\Vert_2$，这样一来$\Delta\boldsymbol{w}</em>}=-\eta_t \boldsymbol{g<em i="1">t$，即梯度下降（SGD）。一般地，对于$p$范数<br />
\begin{equation}\Vert\boldsymbol{\varphi}\Vert_p = \sqrt[\uproot{10}p]{\sum</em>}^n |\varphi_i|^p}\end{equation<a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Hölder不等式</a>给出$\boldsymbol{g}^{\top}\boldsymbol{\varphi} \leq \Vert \boldsymbol{g}\Vert_q \Vert \boldsymbol{\varphi}\Vert_p$，其中$1/p + 1/q = 1$，利用它我们得到<br />
\begin{equation}\max_{\Vert\boldsymbol{\varphi}\Vert_p=1}\boldsymbol{g}^{\top}\boldsymbol{\varphi} = \Vert \boldsymbol{g}\Vert_q\end{equation}<br />
等号成立的条件是<br />
\begin{equation}\boldsymbol{\varphi}^* = \frac{1}{\Vert\boldsymbol{g}\Vert_q^{q/p}}\Big[\text{sign}(g_1) |g_1|^{q/p},\text{sign}(g_2) |g_2|^{q/p},\cdots,\text{sign}(g_n) |g_n|^{q/p}\Big]\end{equation}<br />
以它为方向向量的优化器叫做pbSGD，可参考<a href="https://www.ijcai.org/proceedings/2020/451">《pbSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization》</a>。特别地，当$p\to\infty$时有$q\to 1$和$|g_i|^{q/p}\to 1$，此时退化为SignSGD，即SignSGD实际上是$\Vert\Vert_{\infty}$范数下的最速梯度下降。</p>
<h2 id="_7">矩阵范数</h2>
<p>现在让我们将目光切换到矩阵参数$\boldsymbol{W}\in\mathbb{R}^{n\times m}$。类似地，我们将它的更新规则定义为<br />
\begin{equation}\boldsymbol{W}<em _boldsymbol_W="\boldsymbol{W">{t+1} = \mathop{\text{argmin}}</em>}} \frac{\Vert\boldsymbol{W} - \boldsymbol{W<em t_1="t+1">t\Vert^2}{2\eta_t} + \mathcal{L}(\boldsymbol{W})\end{equation}<br />
此时$\Vert\Vert$是某种矩阵范数。同样使用一阶近似，我们得到<br />
\begin{equation}\Delta\boldsymbol{W}</em>} = \mathop{\text{argmin}<em t_1="t+1">{\Delta\boldsymbol{W}} \frac{\Vert\Delta\boldsymbol{W}\Vert^2}{2\eta_t} + \text{Tr}(\boldsymbol{G}_t^{\top}\Delta\boldsymbol{W})\end{equation}<br />
这里$\Delta\boldsymbol{W}</em>} = \boldsymbol{W<em _boldsymbol_W="\boldsymbol{W">{t+1}-\boldsymbol{W}_t, \boldsymbol{G}_t = \nabla</em><em _Delta_boldsymbol_W="\Delta\boldsymbol{W">t}\mathcal{L}(\boldsymbol{W}_t)$。还是使用“范数-方向”解耦，即设$\gamma = \Vert\Delta\boldsymbol{w}\Vert$和$\boldsymbol{\Phi} = -\Delta\boldsymbol{W}/\Vert\Delta\boldsymbol{W}\Vert$，我们得到<br />
\begin{equation}\min</em>}} \frac{\Vert\Delta\boldsymbol{W}\Vert^2}{2\eta_t} + \text{Tr}(\boldsymbol{G<em 0="0" _gamma_geq="\gamma\geq">t^{\top}\Delta\boldsymbol{W}) = \min</em>} \frac{\gamma^2}{2\eta_t} - \gamma\bigg(\underbrace{\max_{\Vert\boldsymbol{\Phi}\Vert=1}\text{Tr}(\boldsymbol{G<em _text_记为="\text{记为">t^{\top}\boldsymbol{\Phi})}</em>}\Vert \boldsymbol{G<em _Vert="\Vert" _boldsymbol_x="\boldsymbol{x">t\Vert^{\dagger}}\bigg)\end{equation}<br />
然后就是具体范数具体分析了。矩阵常用的范数有两种，一种是<a href="/archives/10366#%E8%8C%83%E6%95%B0%E7%9B%B8%E5%85%B3">F范数</a>，它实际上就是将矩阵展平成向量后算的欧氏范数，这种情况下结论跟向量是一样的，答案就是SGD，这里不再展开；另一种则是由向量范数诱导出来的$2$范数，也称谱范数：<br />
\begin{equation}\Vert \boldsymbol{\Phi}\Vert_2 = \max</em>}\Vert_2 = 1} \Vert \boldsymbol{\Phi}\boldsymbol{x}\Vert_2\end{equation<br />
注意右端出现的$\Vert\Vert_2$的对象都是向量，所以定义是明确的。更多关于$2$范数的讨论可以参考<a href="/archives/6051">《深度学习中的Lipschitz约束：泛化与生成模型》</a>和<a href="/archives/10407#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0">《低秩近似之路（二）：SVD》</a>。由于$2$范数是由“矩阵-向量”乘法诱导出来的，因此它更贴合矩阵乘法，并且还恒成立$\Vert\boldsymbol{\Phi}\Vert_2\leq \Vert\boldsymbol{\Phi}\Vert_F$，即$2$范数相比$F$范数更紧凑。</p>
<p>所以，接下来我们就针对$2$范数进行计算。设$\boldsymbol{G}$的SVD为$\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} = \sum\limits_{i=1}^r \sigma_i \boldsymbol{u}<em i="1">i \boldsymbol{v}_i^{\top}$，我们有<br />
\begin{equation}\text{Tr}(\boldsymbol{G}^{\top}\boldsymbol{\Phi})=\text{Tr}\Big(\sum</em>}^r \sigma_i \boldsymbol{v<em i="1">i \boldsymbol{u}_i^{\top}\boldsymbol{\Phi}\Big) = \sum</em>}^r \sigma_i \boldsymbol{u<em i="1">i^{\top}\boldsymbol{\Phi}\boldsymbol{v}_i\end{equation}<br />
根据定义，当$\Vert\boldsymbol{\Phi}\Vert_2=1$时$\Vert\boldsymbol{\Phi}\boldsymbol{v}_i\Vert_2\leq \Vert\boldsymbol{v}_i\Vert_2=1$，于是$\boldsymbol{u}_i^{\top}\boldsymbol{\Phi}\boldsymbol{v}_i\leq 1$，因此<br />
\begin{equation}\text{Tr}(\boldsymbol{G}^{\top}\boldsymbol{\Phi})\leq \sum</em>}^r \sigma_i\end{equation<br />
等号在所有$\boldsymbol{u}<em i="1">i^{\top}\boldsymbol{\Phi}\boldsymbol{v}_i$都等于1时取到，此时<br />
\begin{equation}\boldsymbol{\Phi} = \sum</em>}^r \boldsymbol{u<em _:_:r_="[:,:r]">i \boldsymbol{v}_i^{\top} = \boldsymbol{U}</em>}\boldsymbol{V}_{[:,:r]}^{\top} = \text{msign}(\boldsymbol{G})\end{equation<br />
至此，我们证明了$2$范数惩罚下的梯度下降正是$\beta=0$时的Muon优化器！当$\beta &gt; 0$时，滑动平均生效，我们可以将它视为梯度的一种更精准的估计，所以改为对动量取$\text{msign}$。总的来说，Muon相当于$2$范数约束下的梯度下降，$2$范数更好地度量了矩阵之间的本质差异，从而使每一步都走得更精准、更本质。</p>
<h2 id="_8">追根溯源</h2>
<p>Muon还有一个更久远的相关工作<a href="https://papers.cool/arxiv/1802.09568">《Shampoo: Preconditioned Stochastic Tensor Optimization》</a>，这是2018年的论文，提出了名为Shampoo的优化器，跟Muon有异曲同工之处。</p>
<p>Adam通过梯度平方的平均来自适应学习率的策略，最早提出自Adagrad的论文<a href="https://jmlr.org/papers/v12/duchi11a.html">《Adaptive Subgradient Methods for Online Learning and Stochastic Optimization》</a>，里边提出的是直接将梯度平方累加的策略，这相当于全局等权平均，后来的RMSProp、Adam则类比动量的设计，改为滑动平均，发现在实践中表现更好。</p>
<p>不仅如此，Adagrad最开始提出的实际是累加外积$\boldsymbol{g}\boldsymbol{g}^{\top}$，只不过缓存外积空间成本太大，所以实践中改为Hadamard积$\boldsymbol{g}\odot\boldsymbol{g}$。那累加外积的理论依据是什么呢？这我们在<a href="/archives/10588">《从Hessian近似看自适应学习率优化器》</a>推导过，答案是“梯度外积的长期平均$\mathbb{E}[\boldsymbol{g}\boldsymbol{g}^{\top}]$近似了Hessian矩阵的平方$\sigma^2\boldsymbol{\mathcal{H}}_{\boldsymbol{\theta}^*}^2$”，所以这实际上在近似二阶的Newton法。</p>
<p>Shampoo传承了Adagrad缓存外积的思想，但考虑到成本问题，取了个折中。跟Muon一样，它同样是针对矩阵（以及高阶张量）进行优化，策略是缓存梯度的矩阵乘积$\boldsymbol{G}\boldsymbol{G}^{\top}$和$\boldsymbol{G}^{\top}\boldsymbol{G}$，而不是外积，这样空间成本是$\mathcal{O}(n^2 + m^2)$而不是$\mathcal{O}(n^2 m^2)$：<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{L}<em t-1="t-1">t =&amp;\, \beta\boldsymbol{L}</em>} + \boldsymbol{G<em t-1="t-1">t\boldsymbol{G}_t^{\top} \\[5pt]<br />
\boldsymbol{R}_t =&amp;\, \beta\boldsymbol{R}</em>} + \boldsymbol{G<em t-1="t-1">t^{\top}\boldsymbol{G}_t \\[5pt]<br />
\boldsymbol{W}_t =&amp;\, \boldsymbol{W}</em> \\} - \eta_t \boldsymbol{L}_t^{-1/4}\boldsymbol{G}_t\boldsymbol{R}_t^{-1/4<br />
\end{aligned}\end{equation}<br />
这里的$\beta$是笔者自己加的，Shampoo默认了$\beta=1$，${}^{-1/4}$同样是矩阵的幂运算，可以用SVD来完成。由于Shampoo没有提出Newton-schulz迭代之类的近似方案，是直接用SVD算的，所以为了节省计算成本，它并没有每一步都计算$\boldsymbol{L}_t^{-1/4}$和$\boldsymbol{R}_t^{-1/4}$，而是间隔一定步数才更新它们的结果。</p>
<p>特别地，当$\beta=0$时，Shampoo的更新向量为$(\boldsymbol{G}\boldsymbol{G}^{\top})^{-1/4}\boldsymbol{G}(\boldsymbol{G}^{\top}\boldsymbol{G})^{-1/4}$，通过对$\boldsymbol{G}$进行SVD我们可以证明<br />
\begin{equation}(\boldsymbol{G}\boldsymbol{G}^{\top})^{-1/4}\boldsymbol{G}(\boldsymbol{G}^{\top}\boldsymbol{G})^{-1/4} = (\boldsymbol{G}\boldsymbol{G}^{\top})^{-1/2}\boldsymbol{G}= \boldsymbol{G}(\boldsymbol{G}^{\top}\boldsymbol{G})^{-1/2}=\text{msign}(\boldsymbol{G})\end{equation}<br />
这表明$\beta=0$时Shampoo和Muon在理论上是等价的！因此，Shampoo与Muon在更新量的设计方面有着相通之处。</p>
<h2 id="_9">文章小结</h2>
<p>本文介绍了最近推特上颇为热闹的Muon优化器，它专门为矩阵参数定制，目前看来比AdamW更高效，并且似乎体现了一些向量化与矩阵化的本质差异，值得学习和思考一番。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10592">https://spaces.ac.cn/archives/10592</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Dec. 10, 2024). 《Muon优化器赏析：从向量到矩阵的本质跨越 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10592">https://spaces.ac.cn/archives/10592</a></p>
<p>@online{kexuefm-10592,<br />
title={Muon优化器赏析：从向量到矩阵的本质跨越},<br />
author={苏剑林},<br />
year={2024},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/10592}},<br />
} </p>
<hr />
<h2 id="_10">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>

        <!-- Back to Home -->
        <div class="text-center mt-5 mb-4">
            <a href="../index.html" class="btn btn-outline-primary">
                <i class="fas fa-arrow-left"></i> 返回首页
            </a>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>
</body>
</html>
