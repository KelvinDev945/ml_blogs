<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Muon优化器赏析：从向量到矩阵的本质跨越 | ML & Math Blog Posts</title>
    <meta name="description" content="Muon优化器赏析：从向量到矩阵的本质跨越&para;
原文链接: https://spaces.ac.cn/archives/10592
发布日期: 

随着LLM时代的到来，学术界对于优化器的研究热情似乎有所减退。这主要是因为目前主流的AdamW已经能够满足大多数需求，而如果对优化器“大动干戈”，那么需要巨大的验证成本。因此，当前优化器的变化，多数都只是工业界根据自己的训练经验来对AdamW打...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #300 Muon优化器赏析：从向量到矩阵的本质跨越
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#300</span>
                Muon优化器赏析：从向量到矩阵的本质跨越
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-12-10</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=矩阵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 矩阵</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=谱范数" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 谱范数</span>
                </a>
                
                <a href="../index.html?tags=muon" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> muon</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="muon">Muon优化器赏析：从向量到矩阵的本质跨越<a class="toc-link" href="#muon" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10592">https://spaces.ac.cn/archives/10592</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>随着LLM时代的到来，学术界对于优化器的研究热情似乎有所减退。这主要是因为目前主流的AdamW已经能够满足大多数需求，而如果对优化器“大动干戈”，那么需要巨大的验证成本。因此，当前优化器的变化，多数都只是工业界根据自己的训练经验来对AdamW打的一些小补丁。</p>
<p>不过，最近推特上一个名为“<a href="https://github.com/KellerJordan/Muon">Muon</a>”的优化器颇为热闹，它声称比AdamW更为高效，且并不只是在Adam基础上的“小打小闹”，而是体现了关于向量与矩阵差异的一些值得深思的原理。本文让我们一起赏析一番。</p>
<p><a href="/usr/uploads/2024/12/125501438.jpeg" title="点击查看原图"><img alt="Muon与AdamW效果对比（来源：推特@Yuchenj_UW）" src="/usr/uploads/2024/12/125501438.jpeg" /></a></p>
<p>Muon与AdamW效果对比（来源：推特@Yuchenj_UW）</p>
<h2 id="_1">算法初探<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>Muon全称是“MomentUm Orthogonalized by Newton-schulz”，它适用于矩阵参数$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，其更新规则是<br />
\begin{equation}\begin{aligned}
\boldsymbol{M}<em t-1="t-1">t =&amp;\, \beta\boldsymbol{M}</em>} + \boldsymbol{G<em t-1="t-1">t \\[5pt]
\boldsymbol{W}_t =&amp;\, \boldsymbol{W}</em>} - \eta_t [\text{msign}(\boldsymbol{M<em t-1="t-1">t) + \lambda \boldsymbol{W}</em>] \\
\end{aligned}\end{equation}<br />
这里$\text{msign}$是<a href="https://en.wikipedia.org/wiki/Matrix_sign_function">矩阵符号函数</a>，它并不是简单地对矩阵每个分量取$\text{sign}$操作，而是$\text{sign}$函数的矩阵化推广，它跟<a href="/archives/10407">SVD</a>的关系是：<br />
\begin{equation}\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}^{\top} = \text{SVD}(\boldsymbol{M}) \quad\Rightarrow\quad \text{msign}(\boldsymbol{M}) = \boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>}^{\top}\end{equation
其中$\boldsymbol{U}\in\mathbb{R}^{n\times n},\boldsymbol{\Sigma}\in\mathbb{R}^{n\times m},\boldsymbol{V}\in\mathbb{R}^{m\times m}$，$r$是$\boldsymbol{M}$的秩。更多的理论细节我们稍后再展开，这里我们先来尝试直观感知如下事实：</p>
<blockquote>
<p>Muon是一个类似于Adam的自适应学习率优化器。</p>
</blockquote>
<p>像Adagrad、RMSprop、Adam等自适应学习率优化器的特点是通过除以 <em>梯度平方的滑动平均的平方根</em> 来调整每个参数的更新量，这达到了两个效果：1、损失函数的常数缩放不影响优化轨迹；2、每个参数分量的更新幅度尽可能一致。Muon正好满足这两个特性：</p>
<blockquote>
<p>1、损失函数乘以$\lambda$，$\boldsymbol{M}$也会乘以$\lambda$，结果是$\boldsymbol{\Sigma}$被乘以$\lambda$，但Muon最后的更新量是将$\boldsymbol{\Sigma}$变为单位阵，所以不影响优化结果；</p>
<p>2、当$\boldsymbol{M}$被SVD为$\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$时，$\boldsymbol{\Sigma}$的不同奇异值体现了$\boldsymbol{M}$的“各向异性”，而将它们都 <em>置一</em> 则更加各向同性，也起到同步更新幅度的作用。</p>
</blockquote>
<p>对了，关于第2点，有没有读者想起了<a href="/archives/8069">BERT-whitening</a>？另外要指出的是，Muon还有个Nesterov版，它只是将更新规则中的$\text{msign}(\boldsymbol{M}_t)$换成$\text{msign}(\beta\boldsymbol{M}_t + \boldsymbol{G}_t)$，其余部份完全一致，简单起见就不展开介绍了。</p>
<p>（考古：事后发现，2015年的论文<a href="https://proceedings.mlr.press/v38/carlson15.html">《Stochastic Spectral Descent for Restricted Boltzmann Machines》</a>已经提出过跟Muon大致相同的优化算法，当时称为“Stochastic Spectral Descent”。）</p>
<h2 id="_2">符号函数<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>利用SVD，我们还可以证明恒等式<br />
\begin{equation}\text{msign}(\boldsymbol{M}) = (\boldsymbol{M}\boldsymbol{M}^{\top})^{-1/2}\boldsymbol{M}= \boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2}\label{eq:msign-id}\end{equation}<br />
其中${}^{-1/2}$是矩阵的$1/2$次幂的逆矩阵，如果不可逆的话则取<a href="/archives/10366">伪逆</a>。这个恒等式能让我们更好理解为什么$\text{msign}$是$\text{sign}$的矩阵推广：对于标量$x$我们有$\text{sign}(x)=x(x^2)^{-1/2}$，正是上式的一个特殊情形（当$\boldsymbol{M}$是$1\times 1$矩阵时）。这个特殊例子还可以推广到对角阵$\boldsymbol{M}=\text{diag}(\boldsymbol{m})$：<br />
\begin{equation}\text{msign}(\boldsymbol{M}) = \text{diag}(\boldsymbol{m})[\text{diag}(\boldsymbol{m})^2]^{-1/2} = \text{diag}(\text{sign}(\boldsymbol{m}))=\text{sign}(\boldsymbol{M})\end{equation}<br />
其中$\text{sign}(\boldsymbol{m})$、$\text{sign}(\boldsymbol{M})$是指向量/矩阵的每个分量都取$\text{sign}$。上式意味着，当$\boldsymbol{M}$是对角阵时，Muon就退化为带动量的<a href="/archives/10542#%E8%87%AA%E9%80%82%E5%BA%94%E7%89%88">SignSGD</a>（Signum）或笔者所提的<a href="/archives/9512">Tiger</a>，它们都是Adam的经典近似。反过来说，Muon与Signum、Tiger的区别就是Element-wise的$\text{sign}(\boldsymbol{M})$替换成了矩阵版$\text{msign}(\boldsymbol{M})$。</p>
<p>对于$n$维向量来说，我们还可以视为$n\times 1$的矩阵，此时$\text{msign}(\boldsymbol{m}) = \boldsymbol{m}/\Vert\boldsymbol{m}\Vert_2$正好是$l_2$归一化。所以，在Muon框架下对向量我们有两种视角：一是对角矩阵，如LayerNorm的gamma参数，结果是对动量取$\text{sign}$；二是$n\times 1$的矩阵，结果是对动量做$l_2$归一化。此外，输入和输出的Embedding虽然也是矩阵，但它们使用上是稀疏的，所以更合理的方式也是将它们当成多个向量独立处理。</p>
<p>当$m=n=r$时，$\text{msign}(\boldsymbol{M})$还有一个意义是“最优正交近似”：<br />
\begin{equation}\text{msign}(\boldsymbol{M}) = \mathop{\text{argmin}}<em _boldsymbol_O="\boldsymbol{O">{\boldsymbol{O}^{\top}\boldsymbol{O} = \boldsymbol{I}}\Vert \boldsymbol{M} - \boldsymbol{O}\Vert_F^2 \label{eq:nearest-orth}\end{equation}<br />
类似地，对于$\text{sign}(\boldsymbol{M})$我们可以写出（假设$\boldsymbol{M}$没有零元素）：<br />
\begin{equation}\text{sign}(\boldsymbol{M}) = \mathop{\text{argmin}}</em>}\in\{-1,1\}^{n\times m}}\Vert \boldsymbol{M} - \boldsymbol{O}\Vert_F^2\end{equation
不论是$\boldsymbol{O}^{\top}\boldsymbol{O} = \boldsymbol{I}$还是$\boldsymbol{O}\in\{-1,1\}^{n\times m}$，我们都可以视为对更新量的一种规整化约束，所以Muon和Signum、Tiger可以视作是同一思路下的优化器，它们都以动量$\boldsymbol{M}$为出发点来构建更新量，只是为更新量选择了不同的规整化方法。</p>
<blockquote>
<p><strong>式$\eqref{eq:nearest-orth}$的证明</strong> ：对于正交矩阵$\boldsymbol{O}$，我们有<br />
 \begin{equation}\begin{aligned}
 \Vert \boldsymbol{M} - \boldsymbol{O}\Vert_F^2 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + \Vert \boldsymbol{O}\Vert_F^2 - 2\langle\boldsymbol{M},\boldsymbol{O}\rangle_F \\[5pt]
 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + n - 2\text{Tr}(\boldsymbol{M}\boldsymbol{O}^{\top})\\[5pt]
 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + n - 2\text{Tr}(\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}\boldsymbol{O}^{\top})\\[5pt]
 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + n - 2\text{Tr}(\boldsymbol{\Sigma}\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U})\\
 =&amp;\, \Vert \boldsymbol{M}\Vert_F^2 + n - 2\sum_{i=1}^n \boldsymbol{\Sigma}<em i_i="i,i">{i,i}(\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U})</em>
 \end{aligned}\end{equation}<br />
 其中涉及到的运算规则我们在<a href="/archives/10366">伪逆</a>中已经介绍过。由于$\boldsymbol{U},\boldsymbol{V},\boldsymbol{O}$都是正交矩阵，所以$\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U}$也是正交矩阵，正交矩阵的每个分量必然不超过1，又因为$\boldsymbol{\Sigma}<em i_i="i,i">{i,i} &gt; 0$，所以上式取最小值对应于每个$(\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U})</em>$。}$取最大值，即$(\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U})_{i,i}=1$，这意味着$\boldsymbol{V}^{\top}\boldsymbol{O}^{\top}\boldsymbol{U}=\boldsymbol{I}$，即$\boldsymbol{O}=\boldsymbol{U}\boldsymbol{V}^{\top</p>
<p>该结论还可以仔细地推广到$m,n,r$不相等的情形，但这里不作进一步展开。</p>
</blockquote>
<h2 id="_3">迭代求解<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>实践中，如果每一步都对$\boldsymbol{M}$做SVD来求解$\text{msign}(\boldsymbol{M})$的话，那么计算成本还是比较大的，因此作者提出了用Newton-schulz迭代来近似计算$\text{msign}(\boldsymbol{M})$。</p>
<p>迭代的出发点是恒等式$\eqref{eq:msign-id}$，不失一般性，我们假设$n\geq m$，然后考虑在$\boldsymbol{M}^{\top}\boldsymbol{M}=\boldsymbol{I}$处泰勒展开$(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2}$，展开的方式是直接将标量函数$t^{-1/2}$的结果用到矩阵中：<br />
\begin{equation}t^{-1/2} = 1 - \frac{1}{2}(t-1) + \frac{3}{8}(t-1)^2 - \frac{5}{16}(t-1)^3 + \cdots\end{equation}<br />
保留到二阶，结果是$(15 - 10t + 3t^2)/8$，那么我们有<br />
\begin{equation}\text{msign}(\boldsymbol{M}) = \boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2}\approx \frac{15}{8}\boldsymbol{M} - \frac{5}{4}\boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M}) + \frac{3}{8}\boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^2\end{equation}<br />
假如$\boldsymbol{X}<em t_1="t+1">t$是$\text{msign}(\boldsymbol{M})$的某个近似，我们认为将它代入上式后，会得到$\text{msign}(\boldsymbol{M})$的一个更好的近似，于是我们得到一个可用的迭代格式<br />
\begin{equation}\boldsymbol{X}</em>} = \frac{15}{8}\boldsymbol{X}_t - \frac{5}{4}\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t) + \frac{3}{8}\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^2\end{equation
然而，查看Muon的官方代码我们就会发现，它里边的Newton-schulz迭代确实是这个形式，但三个系数却是$(3.4445, -4.7750, 2.0315)$，而且作者没有给出数学推导，只有一段语焉不详的注释：  </p>
<p><a href="/usr/uploads/2024/12/39782973.png" title="点击查看原图"><img alt="Muon优化器的Newton-schulz迭代" src="/usr/uploads/2024/12/39782973.png" /></a></p>
<p>Muon优化器的Newton-schulz迭代</p>
<h2 id="_4">收敛加速<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>为了猜测官方迭代算法的来源，我们考虑一般的迭代过程
\begin{equation}\boldsymbol{X}_{t+1} = a\boldsymbol{X}_t + b\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t) + c\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^2\label{eq:iteration}\end{equation}<br />
其中$a,b,c$是三个待求解的系数，如果想要更高阶的迭代算法，我们也可以逐次补充$\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^3$、$\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^4$等项，下面的分析过程是通用的。</p>
<p>我们选择的初始值是$\boldsymbol{X}<em t_1="t+1">0=\boldsymbol{M}/\Vert\boldsymbol{M}\Vert_F$，$\Vert\cdot\Vert_F$是矩阵的$F$范数，选择的依据是除以$\Vert\boldsymbol{M}\Vert_F$不改变SVD的$\boldsymbol{U},\boldsymbol{V}$，但可以让$\boldsymbol{X}_0$的所有奇异值都在$[0,1]$之间，让迭代的初始奇异值更标准一些。现在假设$\boldsymbol{X}_t$可以SVD为$\boldsymbol{U}\boldsymbol{\Sigma}_t\boldsymbol{V}^{\top}$，那么代入上式我们可以得到<br />
\begin{equation}\boldsymbol{X}</em>} = \boldsymbol{U<em t_:r_:r_="t,[:r,:r]">{[:,:r]}(a \boldsymbol{\Sigma}</em>} + b \boldsymbol{\Sigma<em t_:r_:r_="t,[:r,:r]">{t,[:r,:r]}^3 + c \boldsymbol{\Sigma}</em>}^5)\boldsymbol{V<em _:r_:r_="[:r,:r]">{[:,:r]}^{\top}\end{equation}<br />
因此，式$\eqref{eq:iteration}$实际上在迭代奇异值组成的对角阵$\boldsymbol{\Sigma}</em>}$，如果记$\boldsymbol{X<em _:_:r_="[:,:r]">t=\boldsymbol{U}</em>}\boldsymbol{\Sigma<em _:_:r_="[:,:r]">{t,[:r,:r]}\boldsymbol{V}</em>}^{\top}$，那么有$\boldsymbol{\Sigma<em t_:r_:r_="t,[:r,:r]">{t+1,[:r,:r]} = g(\boldsymbol{\Sigma}</em>})$，其中$g(x) = ax + bx^3 + cx^5$。又因为对角阵的幂等于对角线元素各自取幂，所以问题简化成单个奇异值$\sigma$的迭代。我们的目标是计算$\boldsymbol{U<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>}^{\top}$，换言之希望通过迭代将$\boldsymbol{\Sigma<em t_1="t+1">{[:r,:r]}$变为单位阵，这又可以简化为迭代$\sigma</em> = g(\sigma_t)$将单个奇异值变为1。</p>
<p>受<a href="https://x.com/leloykun/status/1846165001746501899">@leloykun</a>启发，我们将$a,b,c$的选择视为一个最优化问题，目标是让迭代过程对于任意初始奇异值都收敛得尽可能快。首先我们将$g(x)$重新参数化为<br />
\begin{equation}g(x) = x + \kappa x(x^2 - x_1^2)(x^2 - x_2^2)\end{equation}<br />
其中$x_1 \leq x_2$。该参数化的好处是直观表示出了迭代的5个不动点$0,\pm x_1,\pm x_2$。由于我们的目标是收敛到1，因此初始化我们选择$x_1 &lt; 1,x_2 &gt; 1$，想法是不管迭代过程往$x_1$走还是往$x_2$走，结果都是1附近。</p>
<p>接下来，我们确定迭代步数$T$，这样迭代过程就称为一个确定性函数，然后我们将矩阵的形状（即$n,m$）确定好，就可以采样一批矩阵，并通过SVD来算奇异值。最后，我们将这些奇异值当成输入，而目标输出则是1，损失函数是平方误差，整个模型完全可导，可以用梯度下降解决（<a href="https://x.com/leloykun/status/1846165001746501899">@leloykun</a>则假设了$x_1 + x_2 = 2$，然后用网格搜索来求解）。</p>
<p>一些计算结果：<br />
\begin{array}{ccc|ccc|ccc|c|c}
\hline
n &amp; m &amp; T &amp; \kappa &amp; x_1 &amp; x_2 &amp; a &amp; b &amp; c &amp; \text{mse} &amp; \text{mse}_{\text{o}}\\
\hline
1024 &amp; 1024 &amp; 3 &amp; 7.020 &amp; 0.830 &amp; 0.830 &amp; 4.328 &amp; -9.666 &amp; 7.020 &amp; 0.10257 &amp; 0.18278 \\
1024 &amp; 1024 &amp; 5 &amp; 1.724 &amp; 0.935 &amp; 1.235 &amp; 3.297 &amp; -4.136 &amp; 1.724 &amp; 0.02733 &amp; 0.04431 \\
2048 &amp; 1024 &amp; 3 &amp; 7.028 &amp; 0.815 &amp; 0.815 &amp; 4.095 &amp; -9.327 &amp; 7.028 &amp; 0.01628 &amp; 0.06171 \\
2048 &amp; 1024 &amp; 5 &amp; 1.476 &amp; 0.983 &amp; 1.074 &amp; 2.644 &amp; -3.128 &amp; 1.476 &amp; 0.00038 &amp; 0.02954 \\
4096 &amp; 1024 &amp; 3 &amp; 6.948 &amp; 0.802 &amp; 0.804 &amp; 3.886 &amp; -8.956 &amp; 6.948 &amp; 0.00371 &amp; 0.02574 \\
4096 &amp; 1024 &amp; 5 &amp; 1.214 &amp; 1.047 &amp; 1.048 &amp; 2.461 &amp; -2.663 &amp; 1.214 &amp; 0.00008 &amp; 0.02563 \\
\hline
2048 &amp; 2048 &amp; 3 &amp; 11.130 &amp; 0.767 &amp; 0.767 &amp; 4.857 &amp; -13.103 &amp; 11.130 &amp; 0.10739 &amp; 0.24410 \\
2048 &amp; 2048 &amp; 5 &amp; 1.779 &amp; 0.921 &amp; 1.243 &amp; 3.333 &amp; -4.259 &amp; 1.779 &amp; 0.03516 &amp; 0.04991 \\
4096 &amp; 4096 &amp; 3 &amp; 18.017 &amp; 0.705 &amp; 0.705 &amp; 5.460 &amp; -17.929 &amp; 18.017 &amp; 0.11303 &amp; 0.33404 \\
4096 &amp; 4096 &amp; 5 &amp; 2.057 &amp; 0.894 &amp; 1.201 &amp; 3.373 &amp; -4.613 &amp; 2.057 &amp; 0.04700 &amp; 0.06372 \\
8192 &amp; 8192 &amp; 3 &amp; 30.147 &amp; 0.643 &amp; 0.643 &amp; 6.139 &amp; -24.893 &amp; 30.147 &amp; 0.11944 &amp; 0.44843 \\
8192 &amp; 8192 &amp; 5 &amp; 2.310 &amp; 0.871 &amp; 1.168 &amp; 3.389 &amp; -4.902 &amp; 2.310 &amp; 0.05869 &amp; 0.07606 \\
\hline
\end{array}</p>
<p>这里的$\text{mse}_{\text{o}}$是有Muon作者的$a,b,c$算出来的结果。从表格可以看出，结果跟矩阵大小、迭代步数都有明显关系；从损失函数来看，非方阵比方阵更容易收敛；Muon作者给出的$a,b,c$，大概是迭代步数为5时方阵的最优解。当迭代步数给定时，结果依赖于矩阵大小，这本质上是依赖于奇异值的分布，关于这个分布有个值得一提的结果是当$n,m\to\infty$时为<a href="https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution">Marchenko–Pastur分布</a>。</p>
<p>参考代码：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">key</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;SVD&#39;</span><span class="p">):</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">data</span><span class="p">,</span> <span class="n">S</span> <span class="o">/</span> <span class="p">(</span><span class="n">S</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">**</span><span class="mf">0.5</span><span class="p">])</span>

<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">k</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">w</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">f_grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">w</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">):</span>
    <span class="n">u</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="o">*</span> <span class="n">u</span> <span class="o">+</span> <span class="n">f_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>  <span class="c1"># 动量加速</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">u</span>

<span class="n">k</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">w</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="n">k</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">k</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">x1</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">x2</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">a</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">b</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">c</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1"> &amp; </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">)</span><span class="si">:</span><span class="s1">.5f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div>

<h2 id="_5">一些思考<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>如果按照默认选择$T=5$，那么对于一个$n\times n$的矩阵参数，Muon的每一步更新至少需要算15次$n\times n$与$n\times n$的矩阵乘法，这计算量毋庸置疑是比Adam明显大的，由此可能有读者担心Muon实践上是否可行。</p>
<p>事实上，这种担心是多余的，Muon计算虽然比Adam复杂，但每一步增加的时间不多，笔者的结论是5%内，Muon作者则声称能做到2%。这是因为Muon的矩阵乘法发生在当前梯度计算完后、下一梯度计算前，这期间几乎所有的算力都是空闲的，而这些矩阵乘法是静态大小且可以并行，因此不会明显增加时间成本，反而是Muon比Adam少一组缓存变量，显存成本更低。</p>
<p>Muon最值得深思的地方，其实是向量与矩阵的内在区别，以及它对优化的影响。SGD、Adam、Tiger等常见优化器的更新规则是Element-wise的，即不论向量、矩阵参数，实际都视为一个大向量，分量按照相同的规则独立地更新。具备这个特性的优化器往往理论分析起来更加简化，也方便张量并行，因为一个大矩阵切成两个小矩阵独立处理，并不改变优化轨迹。</p>
<p>但Muon不一样，它以矩阵为基本单位，考虑了矩阵的一些独有特性。可能有些读者会奇怪：矩阵和向量不都只是一堆数字的排列吗，能有什么区别？举个例子，矩阵我们有“迹（trace）”这个概念，它是对角线元素之和，这个概念不是瞎定义的，它有一个重要特性是在相似变换下保持不变，它还等于矩阵的所有特征值之和。从这个例子就可以看出，矩阵的对角线元素跟非对角线元素，地位其实是不完全对等的。而Muon正是因为考虑了这种不对等性，才有着更好的效果。</p>
<p>当然，这也会导致一些负面影响。如果一个矩阵被划分到不同设备上，那么用Muon时就需要将它们的梯度就需要汇聚起来再计算更新量了，而不能每个设备独立更新，这增加了通信成本。即便我们不考虑并行方面，这个问题也存在，比如Multi-Head Attention一般是通过单个大矩阵投影到$Q$（$K,V$同理），然后用reshape的方式得到多个Head，这样在模型参数中就只有单个矩阵，但它本质上是多个小矩阵，所以按道理我们需要将大矩阵拆开成多个小矩阵独立更新。</p>
<p>总之，Muon这种非Element-wise的更新规则，在捕捉向量与矩阵的本质差异的同时，也会引入一些小问题，这可能会不满足一些读者的审美。</p>
<p>（补充：几乎在本博客发布的同时，Muon的作者Keller Jordan也发布了自己的一篇博客<a href="https://kellerjordan.github.io/posts/muon/">《Muon: An optimizer for hidden layers in neural networks》</a>。）</p>
<h2 id="_6">范数视角<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>从理论上看，Muon捕捉了矩阵的什么关键特性呢？也许接下来的范数视角可以回答我们的问题。</p>
<p>这一节的讨论主要参考了论文<a href="https://ieeexplore.ieee.org/abstract/document/7347351">《Stochastic Spectral Descent for Discrete Graphical Models》</a>和<a href="https://papers.cool/arxiv/2409.20325">《Old Optimizer, New Norm: An Anthology》</a>，特别是后一篇。不过其中的出发点并不是新的，我们在<a href="/archives/9660">《梯度流：探索通向最小值之路》</a>就已经简单涉猎过：对于向量参数$\boldsymbol{w}\in\mathbb{R}^n$，我们将下一步的更新规则定义为<br />
\begin{equation}\boldsymbol{w}<em _boldsymbol_w="\boldsymbol{w">{t+1} = \mathop{\text{argmin}}</em>}} \frac{\Vert\boldsymbol{w} - \boldsymbol{w<em t_1="t+1">t\Vert^2}{2\eta_t} + \mathcal{L}(\boldsymbol{w})\end{equation}<br />
其中$\Vert\Vert$是某个向量范数，这称为在某个范数约束下的“最速梯度下降”。接着假设$\eta_t$足够小，那么第一项占主导，这意味着$\boldsymbol{w}</em>}$与$\boldsymbol{w<em t_1="t+1">t$会很接近，于是我们假设$\mathcal{L}(\boldsymbol{w})$的一阶近似够用了，于是问题简化成<br />
\begin{equation}\boldsymbol{w}</em>} = \mathop{\text{argmin}<em _boldsymbol_w="\boldsymbol{w">{\boldsymbol{w}} \frac{\Vert\boldsymbol{w} - \boldsymbol{w}_t\Vert^2}{2\eta_t} + \mathcal{L}(\boldsymbol{w}_t) + \nabla</em><em t_1="t+1">t}\mathcal{L}(\boldsymbol{w}_t)^{\top}(\boldsymbol{w}-\boldsymbol{w}_t)\end{equation}<br />
记$\Delta\boldsymbol{w}</em>} = \boldsymbol{w<em _boldsymbol_w="\boldsymbol{w">{t+1}-\boldsymbol{w}_t, \boldsymbol{g}_t = \nabla</em><em t_1="t+1">t}\mathcal{L}(\boldsymbol{w}_t)$，那么可以简写成<br />
\begin{equation}\Delta\boldsymbol{w}</em>} = \mathop{\text{argmin}<em t_1="t+1">{\Delta\boldsymbol{w}} \frac{\Vert\Delta\boldsymbol{w}\Vert^2}{2\eta_t} + \boldsymbol{g}_t^{\top}\Delta\boldsymbol{w}\end{equation}<br />
计算$\Delta\boldsymbol{w}</em>$的一般思路是求导，但<a href="https://papers.cool/arxiv/2409.20325">《Old Optimizer, New Norm: An Anthology》</a>提供了一个不用求导的统一方案：将$\Delta\boldsymbol{w}$分解为范数$\gamma = \Vert\Delta\boldsymbol{w}\Vert$和方向向量$\boldsymbol{\varphi} = -\Delta\boldsymbol{w}/\Vert\Delta\boldsymbol{w}\Vert$，于是<br />
\begin{equation}\min_{\Delta\boldsymbol{w}} \frac{\Vert\Delta\boldsymbol{w}\Vert^2}{2\eta_t} + \boldsymbol{g}<em 0_="0," _Vert_boldsymbol_varphi="\Vert\boldsymbol{\varphi" _gamma_geq="\gamma\geq">t^{\top}\Delta\boldsymbol{w} = \min</em>}\Vert=1} \frac{\gamma^2}{2\eta_t} - \gamma\boldsymbol{g<em 0="0" _gamma_geq="\gamma\geq">t^{\top}\boldsymbol{\varphi} = \min</em>} \frac{\gamma^2}{2\eta_t} - \gamma\bigg(\underbrace{\max_{\Vert\boldsymbol{\varphi}\Vert=1}\boldsymbol{g<em _text_记为="\text{记为">t^{\top}\boldsymbol{\varphi}}</em>}\Vert \boldsymbol{g<em t_1="t+1">t\Vert^{\dagger}}\bigg)\end{equation}<br />
$\gamma$只是一个标量，跟学习率类似，容易求得最优值是$\eta_t\Vert \boldsymbol{g}_t\Vert^{\dagger}$，而更新方向则是最大化$\boldsymbol{g}_t^{\top}\boldsymbol{\varphi}$（$\Vert\boldsymbol{\varphi}\Vert=1$）的$\boldsymbol{\varphi}^<em>$。现在代入欧氏范数即$\Vert\boldsymbol{\varphi}\Vert_2 = \sqrt{\boldsymbol{\varphi}^{\top}\boldsymbol{\varphi}}$，我们就有$\Vert \boldsymbol{g}_t\Vert^{\dagger}=\Vert \boldsymbol{g}_t\Vert_2$和$\boldsymbol{\varphi}^</em> = \boldsymbol{g}_t/\Vert\boldsymbol{g}_t\Vert_2$，这样一来$\Delta\boldsymbol{w}</em>}=-\eta_t \boldsymbol{g<em i="1">t$，即梯度下降（SGD）。一般地，对于$p$范数<br />
\begin{equation}\Vert\boldsymbol{\varphi}\Vert_p = \sqrt[\uproot{10}p]{\sum</em>}^n |\varphi_i|^p}\end{equation<a href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Hölder不等式</a>给出$\boldsymbol{g}^{\top}\boldsymbol{\varphi} \leq \Vert \boldsymbol{g}\Vert_q \Vert \boldsymbol{\varphi}\Vert_p$，其中$1/p + 1/q = 1$，利用它我们得到<br />
\begin{equation}\max_{\Vert\boldsymbol{\varphi}\Vert_p=1}\boldsymbol{g}^{\top}\boldsymbol{\varphi} = \Vert \boldsymbol{g}\Vert_q\end{equation}<br />
等号成立的条件是<br />
\begin{equation}\boldsymbol{\varphi}^* = \frac{1}{\Vert\boldsymbol{g}\Vert_q^{q/p}}\Big[\text{sign}(g_1) |g_1|^{q/p},\text{sign}(g_2) |g_2|^{q/p},\cdots,\text{sign}(g_n) |g_n|^{q/p}\Big]\end{equation}<br />
以它为方向向量的优化器叫做pbSGD，可参考<a href="https://www.ijcai.org/proceedings/2020/451">《pbSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization》</a>。特别地，当$p\to\infty$时有$q\to 1$和$|g_i|^{q/p}\to 1$，此时退化为SignSGD，即SignSGD实际上是$\Vert\Vert_{\infty}$范数下的最速梯度下降。</p>
<h2 id="_7">矩阵范数<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>现在让我们将目光切换到矩阵参数$\boldsymbol{W}\in\mathbb{R}^{n\times m}$。类似地，我们将它的更新规则定义为<br />
\begin{equation}\boldsymbol{W}<em _boldsymbol_W="\boldsymbol{W">{t+1} = \mathop{\text{argmin}}</em>}} \frac{\Vert\boldsymbol{W} - \boldsymbol{W<em t_1="t+1">t\Vert^2}{2\eta_t} + \mathcal{L}(\boldsymbol{W})\end{equation}<br />
此时$\Vert\Vert$是某种矩阵范数。同样使用一阶近似，我们得到<br />
\begin{equation}\Delta\boldsymbol{W}</em>} = \mathop{\text{argmin}<em t_1="t+1">{\Delta\boldsymbol{W}} \frac{\Vert\Delta\boldsymbol{W}\Vert^2}{2\eta_t} + \text{Tr}(\boldsymbol{G}_t^{\top}\Delta\boldsymbol{W})\end{equation}<br />
这里$\Delta\boldsymbol{W}</em>} = \boldsymbol{W<em _boldsymbol_W="\boldsymbol{W">{t+1}-\boldsymbol{W}_t, \boldsymbol{G}_t = \nabla</em><em _Delta_boldsymbol_W="\Delta\boldsymbol{W">t}\mathcal{L}(\boldsymbol{W}_t)$。还是使用“范数-方向”解耦，即设$\gamma = \Vert\Delta\boldsymbol{w}\Vert$和$\boldsymbol{\Phi} = -\Delta\boldsymbol{W}/\Vert\Delta\boldsymbol{W}\Vert$，我们得到<br />
\begin{equation}\min</em>}} \frac{\Vert\Delta\boldsymbol{W}\Vert^2}{2\eta_t} + \text{Tr}(\boldsymbol{G<em 0="0" _gamma_geq="\gamma\geq">t^{\top}\Delta\boldsymbol{W}) = \min</em>} \frac{\gamma^2}{2\eta_t} - \gamma\bigg(\underbrace{\max_{\Vert\boldsymbol{\Phi}\Vert=1}\text{Tr}(\boldsymbol{G<em _text_记为="\text{记为">t^{\top}\boldsymbol{\Phi})}</em>}\Vert \boldsymbol{G<em _Vert="\Vert" _boldsymbol_x="\boldsymbol{x">t\Vert^{\dagger}}\bigg)\end{equation}<br />
然后就是具体范数具体分析了。矩阵常用的范数有两种，一种是<a href="/archives/10366#%E8%8C%83%E6%95%B0%E7%9B%B8%E5%85%B3">F范数</a>，它实际上就是将矩阵展平成向量后算的欧氏范数，这种情况下结论跟向量是一样的，答案就是SGD，这里不再展开；另一种则是由向量范数诱导出来的$2$范数，也称谱范数：<br />
\begin{equation}\Vert \boldsymbol{\Phi}\Vert_2 = \max</em>}\Vert_2 = 1} \Vert \boldsymbol{\Phi}\boldsymbol{x}\Vert_2\end{equation
注意右端出现的$\Vert\Vert_2$的对象都是向量，所以定义是明确的。更多关于$2$范数的讨论可以参考<a href="/archives/6051">《深度学习中的Lipschitz约束：泛化与生成模型》</a>和<a href="/archives/10407#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0">《低秩近似之路（二）：SVD》</a>。由于$2$范数是由“矩阵-向量”乘法诱导出来的，因此它更贴合矩阵乘法，并且还恒成立$\Vert\boldsymbol{\Phi}\Vert_2\leq \Vert\boldsymbol{\Phi}\Vert_F$，即$2$范数相比$F$范数更紧凑。</p>
<p>所以，接下来我们就针对$2$范数进行计算。设$\boldsymbol{G}$的SVD为$\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} = \sum\limits_{i=1}^r \sigma_i \boldsymbol{u}<em i="1">i \boldsymbol{v}_i^{\top}$，我们有<br />
\begin{equation}\text{Tr}(\boldsymbol{G}^{\top}\boldsymbol{\Phi})=\text{Tr}\Big(\sum</em>}^r \sigma_i \boldsymbol{v<em i="1">i \boldsymbol{u}_i^{\top}\boldsymbol{\Phi}\Big) = \sum</em>}^r \sigma_i \boldsymbol{u<em i="1">i^{\top}\boldsymbol{\Phi}\boldsymbol{v}_i\end{equation}<br />
根据定义，当$\Vert\boldsymbol{\Phi}\Vert_2=1$时$\Vert\boldsymbol{\Phi}\boldsymbol{v}_i\Vert_2\leq \Vert\boldsymbol{v}_i\Vert_2=1$，于是$\boldsymbol{u}_i^{\top}\boldsymbol{\Phi}\boldsymbol{v}_i\leq 1$，因此<br />
\begin{equation}\text{Tr}(\boldsymbol{G}^{\top}\boldsymbol{\Phi})\leq \sum</em>}^r \sigma_i\end{equation
等号在所有$\boldsymbol{u}<em i="1">i^{\top}\boldsymbol{\Phi}\boldsymbol{v}_i$都等于1时取到，此时<br />
\begin{equation}\boldsymbol{\Phi} = \sum</em>}^r \boldsymbol{u<em _:_:r_="[:,:r]">i \boldsymbol{v}_i^{\top} = \boldsymbol{U}</em>}\boldsymbol{V}_{[:,:r]}^{\top} = \text{msign}(\boldsymbol{G})\end{equation
至此，我们证明了$2$范数惩罚下的梯度下降正是$\beta=0$时的Muon优化器！当$\beta &gt; 0$时，滑动平均生效，我们可以将它视为梯度的一种更精准的估计，所以改为对动量取$\text{msign}$。总的来说，Muon相当于$2$范数约束下的梯度下降，$2$范数更好地度量了矩阵之间的本质差异，从而使每一步都走得更精准、更本质。</p>
<h2 id="_8">追根溯源<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>Muon还有一个更久远的相关工作<a href="https://papers.cool/arxiv/1802.09568">《Shampoo: Preconditioned Stochastic Tensor Optimization》</a>，这是2018年的论文，提出了名为Shampoo的优化器，跟Muon有异曲同工之处。</p>
<p>Adam通过梯度平方的平均来自适应学习率的策略，最早提出自Adagrad的论文<a href="https://jmlr.org/papers/v12/duchi11a.html">《Adaptive Subgradient Methods for Online Learning and Stochastic Optimization》</a>，里边提出的是直接将梯度平方累加的策略，这相当于全局等权平均，后来的RMSProp、Adam则类比动量的设计，改为滑动平均，发现在实践中表现更好。</p>
<p>不仅如此，Adagrad最开始提出的实际是累加外积$\boldsymbol{g}\boldsymbol{g}^{\top}$，只不过缓存外积空间成本太大，所以实践中改为Hadamard积$\boldsymbol{g}\odot\boldsymbol{g}$。那累加外积的理论依据是什么呢？这我们在<a href="/archives/10588">《从Hessian近似看自适应学习率优化器》</a>推导过，答案是“梯度外积的长期平均$\mathbb{E}[\boldsymbol{g}\boldsymbol{g}^{\top}]$近似了Hessian矩阵的平方$\sigma^2\boldsymbol{\mathcal{H}}_{\boldsymbol{\theta}^*}^2$”，所以这实际上在近似二阶的Newton法。</p>
<p>Shampoo传承了Adagrad缓存外积的思想，但考虑到成本问题，取了个折中。跟Muon一样，它同样是针对矩阵（以及高阶张量）进行优化，策略是缓存梯度的矩阵乘积$\boldsymbol{G}\boldsymbol{G}^{\top}$和$\boldsymbol{G}^{\top}\boldsymbol{G}$，而不是外积，这样空间成本是$\mathcal{O}(n^2 + m^2)$而不是$\mathcal{O}(n^2 m^2)$：<br />
\begin{equation}\begin{aligned}
\boldsymbol{L}<em t-1="t-1">t =&amp;\, \beta\boldsymbol{L}</em>} + \boldsymbol{G<em t-1="t-1">t\boldsymbol{G}_t^{\top} \\[5pt]
\boldsymbol{R}_t =&amp;\, \beta\boldsymbol{R}</em>} + \boldsymbol{G<em t-1="t-1">t^{\top}\boldsymbol{G}_t \\[5pt]
\boldsymbol{W}_t =&amp;\, \boldsymbol{W}</em> \\} - \eta_t \boldsymbol{L}_t^{-1/4}\boldsymbol{G}_t\boldsymbol{R}_t^{-1/4
\end{aligned}\end{equation}<br />
这里的$\beta$是笔者自己加的，Shampoo默认了$\beta=1$，${}^{-1/4}$同样是矩阵的幂运算，可以用SVD来完成。由于Shampoo没有提出Newton-schulz迭代之类的近似方案，是直接用SVD算的，所以为了节省计算成本，它并没有每一步都计算$\boldsymbol{L}_t^{-1/4}$和$\boldsymbol{R}_t^{-1/4}$，而是间隔一定步数才更新它们的结果。</p>
<p>特别地，当$\beta=0$时，Shampoo的更新向量为$(\boldsymbol{G}\boldsymbol{G}^{\top})^{-1/4}\boldsymbol{G}(\boldsymbol{G}^{\top}\boldsymbol{G})^{-1/4}$，通过对$\boldsymbol{G}$进行SVD我们可以证明<br />
\begin{equation}(\boldsymbol{G}\boldsymbol{G}^{\top})^{-1/4}\boldsymbol{G}(\boldsymbol{G}^{\top}\boldsymbol{G})^{-1/4} = (\boldsymbol{G}\boldsymbol{G}^{\top})^{-1/2}\boldsymbol{G}= \boldsymbol{G}(\boldsymbol{G}^{\top}\boldsymbol{G})^{-1/2}=\text{msign}(\boldsymbol{G})\end{equation}<br />
这表明$\beta=0$时Shampoo和Muon在理论上是等价的！因此，Shampoo与Muon在更新量的设计方面有着相通之处。</p>
<h2 id="_9">文章小结<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>本文介绍了最近推特上颇为热闹的Muon优化器，它专门为矩阵参数定制，目前看来比AdamW更高效，并且似乎体现了一些向量化与矩阵化的本质差异，值得学习和思考一番。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10592">https://spaces.ac.cn/archives/10592</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Dec. 10, 2024). 《Muon优化器赏析：从向量到矩阵的本质跨越 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10592">https://spaces.ac.cn/archives/10592</a></p>
<p>@online{kexuefm-10592,<br />
title={Muon优化器赏析：从向量到矩阵的本质跨越},<br />
author={苏剑林},<br />
year={2024},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/10592}},<br />
} </p>
<hr />
<h2 id="_10">公式推导与注释<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<h3 id="1-muon">1. Muon优化器的完整数学定义<a class="toc-link" href="#1-muon" title="Permanent link">&para;</a></h3>
<p>Muon优化器针对矩阵参数$\boldsymbol{W}\in\mathbb{R}^{n\times m}$设计，其完整的更新规则可以形式化为：</p>
<p><strong>定义1.1（Muon优化器）</strong>：给定损失函数$\mathcal{L}(\boldsymbol{W})$，参数矩阵$\boldsymbol{W}<em t="1">t\in\mathbb{R}^{n\times m}$，学习率序列$\{\eta_t\}</em>$，动量系数$\beta\in[0,1)$，权重衰减系数$\lambda\geq 0$，Muon优化器的更新规则为：}^{\infty</p>
<p>$$
\begin{equation}
\begin{aligned}
\boldsymbol{G}<em _boldsymbol_W="\boldsymbol{W">t &amp;= \nabla</em><em t-1="t-1">{t-1}}\mathcal{L}(\boldsymbol{W}</em>) \
\boldsymbol{M}<em t-1="t-1">t &amp;= \beta\boldsymbol{M}</em>} + \boldsymbol{G<em t-1="t-1">t \
\boldsymbol{W}_t &amp;= \boldsymbol{W}</em>} - \eta_t[\text{msign}(\boldsymbol{M<em t-1="t-1">t) + \lambda\boldsymbol{W}</em>]
\end{aligned}
\end{equation}
$$</p>
<p>其中$\boldsymbol{M}_0 = \boldsymbol{0}$，矩阵符号函数$\text{msign}:\mathbb{R}^{n\times m}\to\mathbb{R}^{n\times m}$定义为：</p>
<p>$$
\text{msign}(\boldsymbol{M}) = \begin{cases}
\boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em> \
\boldsymbol{0} &amp; \text{if } \boldsymbol{M} = \boldsymbol{0}
\end{cases}
$$}^{\top} &amp; \text{if } \boldsymbol{M} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} \text{ (SVD)</p>
<p>其中$r=\text{rank}(\boldsymbol{M})$是矩阵的秩。</p>
<p><strong>命题1.2（msign的等价定义）</strong>：矩阵符号函数具有以下等价定义：</p>
<p>$$
\text{msign}(\boldsymbol{M}) = (\boldsymbol{M}\boldsymbol{M}^{\top})^{-1/2}\boldsymbol{M} = \boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2}
$$</p>
<p><strong>证明</strong>：设$\boldsymbol{M}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$是完整SVD，其中$\boldsymbol{U}\in\mathbb{R}^{n\times n}$，$\boldsymbol{\Sigma}\in\mathbb{R}^{n\times m}$，$\boldsymbol{V}\in\mathbb{R}^{m\times m}$。记$\boldsymbol{\Sigma}<em _:r_:r_="[:r,:r]">r = \boldsymbol{\Sigma}</em>$为非零奇异值对角矩阵。}\in\mathbb{R}^{r\times r</p>
<p>首先计算$\boldsymbol{M}\boldsymbol{M}^{\top}$：</p>
<p>$$
\boldsymbol{M}\boldsymbol{M}^{\top} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}\boldsymbol{V}\boldsymbol{\Sigma}^{\top}\boldsymbol{U}^{\top} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\top}\boldsymbol{U}^{\top}
$$</p>
<p>注意到$\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\top}\in\mathbb{R}^{n\times n}$是块对角矩阵：</p>
<p>$$
\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\top} = \begin{bmatrix}
\boldsymbol{\Sigma}_r^2 &amp; \boldsymbol{0} \
\boldsymbol{0} &amp; \boldsymbol{0}
\end{bmatrix}
$$</p>
<p>因此：</p>
<p>$$
(\boldsymbol{M}\boldsymbol{M}^{\top})^{-1/2} = \boldsymbol{U}\begin{bmatrix}
\boldsymbol{\Sigma}_r^{-1} &amp; \boldsymbol{0} \
\boldsymbol{0} &amp; \boldsymbol{0}
\end{bmatrix}\boldsymbol{U}^{\top}
$$</p>
<p>代入得：</p>
<p>$$
\begin{aligned}
(\boldsymbol{M}\boldsymbol{M}^{\top})^{-1/2}\boldsymbol{M} &amp;= \boldsymbol{U}\begin{bmatrix}
\boldsymbol{\Sigma}<em _:_:r_="[:,:r]">r^{-1} &amp; \boldsymbol{0} \
\boldsymbol{0} &amp; \boldsymbol{0}
\end{bmatrix}\boldsymbol{U}^{\top}\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} \
&amp;= \boldsymbol{U}\begin{bmatrix}
\boldsymbol{\Sigma}_r^{-1} &amp; \boldsymbol{0} \
\boldsymbol{0} &amp; \boldsymbol{0}
\end{bmatrix}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} \
&amp;= \boldsymbol{U}</em>}\boldsymbol{I<em _:_:r_="[:,:r]">r\boldsymbol{V}</em>}^{\top} = \boldsymbol{U<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>
\end{aligned}
$$}^{\top</p>
<p>同理可证$\boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2} = \boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>$。证毕。}^{\top</p>
<h3 id="2">2. 从向量到矩阵：优化器的演化<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21-sgd">2.1 SGD的向量形式<a class="toc-link" href="#21-sgd" title="Permanent link">&para;</a></h4>
<p>标准SGD针对参数向量$\boldsymbol{\theta}\in\mathbb{R}^d$，更新规则为：</p>
<p>$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t\boldsymbol{g}_t
$$</p>
<p>其中$\boldsymbol{g}<em _boldsymbol_theta="\boldsymbol{\theta">t = \nabla</em>^d$上沿着负梯度方向的移动。}_t}\mathcal{L}(\boldsymbol{\theta}_t)$。这可以视为在欧几里得空间$\mathbb{R</p>
<h4 id="22-adam">2.2 Adam的自适应缩放<a class="toc-link" href="#22-adam" title="Permanent link">&para;</a></h4>
<p>Adam引入了基于梯度二阶矩的自适应学习率：</p>
<p>$$
\begin{aligned}
\boldsymbol{m}<em t-1="t-1">t &amp;= \beta_1\boldsymbol{m}</em>} + (1-\beta_1)\boldsymbol{g<em t-1="t-1">t \
\boldsymbol{v}_t &amp;= \beta_2\boldsymbol{v}</em>} + (1-\beta_2)\boldsymbol{g<em t_1="t+1">t\odot\boldsymbol{g}_t \
\hat{\boldsymbol{m}}_t &amp;= \frac{\boldsymbol{m}_t}{1-\beta_1^t}, \quad \hat{\boldsymbol{v}}_t = \frac{\boldsymbol{v}_t}{1-\beta_2^t} \
\boldsymbol{\theta}</em>
\end{aligned}
$$} &amp;= \boldsymbol{\theta}_t - \eta_t\frac{\hat{\boldsymbol{m}}_t}{\sqrt{\hat{\boldsymbol{v}}_t} + \epsilon</p>
<p>关键观察：Adam对每个分量独立地进行缩放，更新规则是<strong>逐元素（element-wise）</strong>的。</p>
<h4 id="23">2.3 向矩阵更新的范式转变<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：当参数是矩阵$\boldsymbol{W}\in\mathbb{R}^{n\times m}$时，有两种视角：</p>
<ol>
<li><strong>向量视角</strong>：将$\boldsymbol{W}$展平为$nm$维向量，应用逐元素优化器</li>
<li><strong>矩阵视角</strong>：将$\boldsymbol{W}$视为矩阵流形上的点，利用矩阵结构</li>
</ol>
<p><strong>定理2.1（矩阵结构的信息容量）</strong>：对于秩为$r$的矩阵$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，其自由度为$(n+m-r)r$，远小于$nm$。</p>
<p><strong>证明</strong>：矩阵的SVD形式为$\boldsymbol{W}=\boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{\Sigma}_r\boldsymbol{V}</em>$，其中：
- $\boldsymbol{U}}^{\top<em _:_:r_="[:,:r]">{[:,:r]}$占据$nr - \frac{r(r-1)}{2}$个自由度（Stiefel流形）
- $\boldsymbol{\Sigma}_r$占据$r$个自由度（正对角矩阵）
- $\boldsymbol{V}</em>$个自由度}$占据$mr - \frac{r(r-1)}{2</p>
<p>总计：$nr + mr - r(r-1) + r = (n+m-r)r$。证毕。</p>
<p>这表明矩阵的内在维度可能远低于其表观维度，因此利用矩阵结构可以更高效地优化。</p>
<h3 id="3">3. 矩阵符号函数的深层机制<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 从标量到矩阵的函数推广<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>对于标量函数$f:\mathbb{R}\to\mathbb{R}$，其矩阵推广$f:\mathbb{R}^{n\times n}\to\mathbb{R}^{n\times n}$定义为：</p>
<p>$$
f(\boldsymbol{A}) = \boldsymbol{Q}f(\boldsymbol{\Lambda})\boldsymbol{Q}^{\top}
$$</p>
<p>其中$\boldsymbol{A}=\boldsymbol{Q}\boldsymbol{\Lambda}\boldsymbol{Q}^{\top}$是特征值分解，$f(\boldsymbol{\Lambda})=\text{diag}(f(\lambda_1),\ldots,f(\lambda_n))$。</p>
<p>对于非方阵，使用SVD：</p>
<p>$$
f(\boldsymbol{M}) = \boldsymbol{U}f(\boldsymbol{\Sigma})\boldsymbol{V}^{\top}
$$</p>
<p><strong>标量符号函数</strong>：$\text{sign}(x) = \begin{cases}1 &amp; x&gt;0 \ 0 &amp; x=0 \ -1 &amp; x&lt;0\end{cases}$</p>
<p><strong>矩阵符号函数</strong>：$\text{msign}(\boldsymbol{M}) = \boldsymbol{U}\text{sign}(\boldsymbol{\Sigma})\boldsymbol{V}^{\top}$</p>
<p>但注意到$\boldsymbol{\Sigma}$的零奇异值对应的奇异向量是不确定的，因此定义为：</p>
<p>$$
\text{msign}(\boldsymbol{M}) = \boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{I}_r\boldsymbol{V}</em>
$$}^{\top</p>
<h4 id="32">3.2 矩阵符号函数的几何意义<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p><strong>命题3.1（最优正交逼近）</strong>：设$\boldsymbol{M}\in\mathbb{R}^{n\times n}$满秩，则：</p>
<p>$$
\text{msign}(\boldsymbol{M}) = \mathop{\arg\min}_{\boldsymbol{Q}^{\top}\boldsymbol{Q}=\boldsymbol{I}}|\boldsymbol{M}-\boldsymbol{Q}|_F^2
$$</p>
<p><strong>证明</strong>：展开目标函数：</p>
<p>$$
\begin{aligned}
|\boldsymbol{M}-\boldsymbol{Q}|_F^2 &amp;= \text{Tr}[(\boldsymbol{M}-\boldsymbol{Q})^{\top}(\boldsymbol{M}-\boldsymbol{Q})] \
&amp;= \text{Tr}(\boldsymbol{M}^{\top}\boldsymbol{M}) + \text{Tr}(\boldsymbol{Q}^{\top}\boldsymbol{Q}) - 2\text{Tr}(\boldsymbol{M}^{\top}\boldsymbol{Q}) \
&amp;= |\boldsymbol{M}|_F^2 + n - 2\text{Tr}(\boldsymbol{M}^{\top}\boldsymbol{Q})
\end{aligned}
$$</p>
<p>因此最小化等价于最大化$\text{Tr}(\boldsymbol{M}^{\top}\boldsymbol{Q})$。设$\boldsymbol{M}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，则：</p>
<p>$$
\text{Tr}(\boldsymbol{M}^{\top}\boldsymbol{Q}) = \text{Tr}(\boldsymbol{V}\boldsymbol{\Sigma}\boldsymbol{U}^{\top}\boldsymbol{Q}) = \text{Tr}(\boldsymbol{\Sigma}\boldsymbol{U}^{\top}\boldsymbol{Q}\boldsymbol{V}) = \sum_{i=1}^n\sigma_i(\boldsymbol{U}^{\top}\boldsymbol{Q}\boldsymbol{V})_{ii}
$$</p>
<p>由于$\boldsymbol{U}^{\top}\boldsymbol{Q}\boldsymbol{V}$是正交矩阵，其元素满足$|(\boldsymbol{U}^{\top}\boldsymbol{Q}\boldsymbol{V})<em ii="ii">{ij}|\leq 1$。当$(\boldsymbol{U}^{\top}\boldsymbol{Q}\boldsymbol{V})</em>)$。证毕。}=1$对所有$i$成立时，$\boldsymbol{U}^{\top}\boldsymbol{Q}\boldsymbol{V}=\boldsymbol{I}$，即$\boldsymbol{Q}=\boldsymbol{U}\boldsymbol{V}^{\top}=\text{msign}(\boldsymbol{M</p>
<p><strong>几何解释</strong>：$\text{msign}(\boldsymbol{M})$是与$\boldsymbol{M}$最接近的正交矩阵，这类似于将$\boldsymbol{M}$"投影"到正交群$O(n)$上。</p>
<h4 id="33">3.3 谱归一化性质<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p><strong>命题3.2（谱归一化）</strong>：$\text{msign}(\boldsymbol{M})$的所有非零奇异值均为1，即：</p>
<p>$$
|\text{msign}(\boldsymbol{M})|_2 = 1, \quad \text{rank}(\text{msign}(\boldsymbol{M})) = \text{rank}(\boldsymbol{M})
$$</p>
<p><strong>证明</strong>：由定义$\text{msign}(\boldsymbol{M})=\boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>$，其SVD即为其自身，奇异值全为1。证毕。}^{\top</p>
<p>这说明$\text{msign}$操作消除了梯度的尺度信息，只保留了方向信息，这与Adam中除以$\sqrt{\boldsymbol{v}_t}$归一化梯度尺度的思想一致。</p>
<h3 id="4">4. 预条件器的矩阵形式推导<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 预条件梯度下降<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p>一般的预条件梯度下降形式为：</p>
<p>$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t\boldsymbol{P}_t^{-1}\boldsymbol{g}_t
$$</p>
<p>其中$\boldsymbol{P}_t\succ 0$是预条件矩阵。选择$\boldsymbol{P}_t$的目标是近似Hessian矩阵$\boldsymbol{H}_t = \nabla^2\mathcal{L}(\boldsymbol{\theta}_t)$，从而加速收敛。</p>
<h4 id="42-adam">4.2 Adam作为对角预条件器<a class="toc-link" href="#42-adam" title="Permanent link">&para;</a></h4>
<p>Adam的更新可以写为：</p>
<p>$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t\boldsymbol{D}_t^{-1}\boldsymbol{m}_t
$$</p>
<p>其中$\boldsymbol{D}_t = \text{diag}(\sqrt{\boldsymbol{v}_t}+\epsilon)$。这是对角预条件器，假设Hessian近似对角。</p>
<h4 id="43-muon">4.3 Muon的矩阵预条件器<a class="toc-link" href="#43-muon" title="Permanent link">&para;</a></h4>
<p>对于矩阵参数$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，我们可以考虑左右预条件：</p>
<p>$$
\boldsymbol{W}_{t+1} = \boldsymbol{W}_t - \eta_t\boldsymbol{L}_t^{-1}\boldsymbol{G}_t\boldsymbol{R}_t^{-1}
$$</p>
<p><strong>定理4.1（Muon的隐式预条件形式）</strong>：Muon更新等价于预条件器：</p>
<p>$$
\boldsymbol{L}_t = (\boldsymbol{M}_t\boldsymbol{M}_t^{\top})^{1/2}, \quad \boldsymbol{R}_t = (\boldsymbol{M}_t^{\top}\boldsymbol{M}_t)^{1/2}
$$</p>
<p><strong>证明</strong>：由msign的定义：</p>
<p>$$
\begin{aligned}
\text{msign}(\boldsymbol{M}_t) &amp;= (\boldsymbol{M}_t\boldsymbol{M}_t^{\top})^{-1/2}\boldsymbol{M}_t \
&amp;= \boldsymbol{M}_t(\boldsymbol{M}_t^{\top}\boldsymbol{M}_t)^{-1/2} \
&amp;= (\boldsymbol{M}_t\boldsymbol{M}_t^{\top})^{-1/2}\boldsymbol{M}_t(\boldsymbol{M}_t^{\top}\boldsymbol{M}_t)^{-1/2} \cdot (\boldsymbol{M}_t^{\top}\boldsymbol{M}_t)^{1/2} \
&amp;= \boldsymbol{L}_t^{-1}\boldsymbol{M}_t\boldsymbol{R}_t^{-1}
\end{aligned}
$$</p>
<p>这与Shampoo的预条件形式一致（$1/4$次幂 vs $1/2$次幂的差异）。证毕。</p>
<p><strong>物理意义</strong>：$\boldsymbol{M}_t\boldsymbol{M}_t^{\top}$和$\boldsymbol{M}_t^{\top}\boldsymbol{M}_t$分别捕捉了梯度在行空间和列空间的二阶统计信息。</p>
<h3 id="5-natural-gradient">5. 与Natural Gradient的联系<a class="toc-link" href="#5-natural-gradient" title="Permanent link">&para;</a></h3>
<h4 id="51-natural-gradient">5.1 Natural Gradient的定义<a class="toc-link" href="#51-natural-gradient" title="Permanent link">&para;</a></h4>
<p>在参数空间$\Theta$上定义Riemannian度量（Fisher信息矩阵）：</p>
<p>$$
\boldsymbol{F}(\boldsymbol{\theta}) = \mathbb{E}_{p(x|\boldsymbol{\theta})}[\nabla\log p(x|\boldsymbol{\theta})\nabla\log p(x|\boldsymbol{\theta})^{\top}]
$$</p>
<p>Natural Gradient定义为：</p>
<p>$$
\tilde{\boldsymbol{g}}_t = \boldsymbol{F}(\boldsymbol{\theta}_t)^{-1}\boldsymbol{g}_t
$$</p>
<p>更新规则：$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t\tilde{\boldsymbol{g}}_t$</p>
<h4 id="52-fisher">5.2 矩阵参数的Fisher信息<a class="toc-link" href="#52-fisher" title="Permanent link">&para;</a></h4>
<p>对于神经网络层$\boldsymbol{y}=\boldsymbol{W}\boldsymbol{x}$，Fisher信息矩阵（使用Kronecker分解近似）：</p>
<p>$$
\boldsymbol{F} \approx \mathbb{E}[\boldsymbol{x}\boldsymbol{x}^{\top}] \otimes \mathbb{E}[\boldsymbol{\delta}\boldsymbol{\delta}^{\top}]
$$</p>
<p>其中$\boldsymbol{\delta}$是从输出反传的梯度。Natural Gradient更新：</p>
<p>$$
\Delta\boldsymbol{W} = -\eta(\mathbb{E}[\boldsymbol{\delta}\boldsymbol{\delta}^{\top}])^{-1}\boldsymbol{G}(\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^{\top}])^{-1}
$$</p>
<p><strong>连接到Muon</strong>：当我们用梯度的矩阵乘积$\boldsymbol{M}_t\boldsymbol{M}_t^{\top}$和$\boldsymbol{M}_t^{\top}\boldsymbol{M}_t$近似这些二阶矩时，Muon提供了一种实用的Natural Gradient近似。</p>
<p><strong>命题5.1</strong>：在线性近似下，Muon的更新方向与Natural Gradient方向的相关性高于SGD。</p>
<p>（完整证明需要具体的分布假设，此处略）</p>
<h3 id="6">6. 黎曼几何视角<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 矩阵流形的切空间<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>考虑秩$r$矩阵的流形：</p>
<p>$$
\mathcal{M}_r = \{\boldsymbol{W}\in\mathbb{R}^{n\times m} : \text{rank}(\boldsymbol{W}) = r\}
$$</p>
<p>这是一个$(n+m-r)r$维的嵌入子流形。</p>
<p>在点$\boldsymbol{W}\in\mathcal{M}_r$处的切空间：</p>
<p>$$
T_{\boldsymbol{W}}\mathcal{M}<em _perp="\perp">r = \{\boldsymbol{U}\boldsymbol{V}^{\top} + \boldsymbol{U}</em>
$$}\boldsymbol{X}^{\top} + \boldsymbol{Y}\boldsymbol{V}_{\perp}^{\top} : \boldsymbol{X}\in\mathbb{R}^{r\times(m-r)}, \boldsymbol{Y}\in\mathbb{R}^{(n-r)\times r}\</p>
<p>其中$\boldsymbol{W}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$。</p>
<h4 id="62-riemannian">6.2 Riemannian梯度<a class="toc-link" href="#62-riemannian" title="Permanent link">&para;</a></h4>
<p>欧几里得梯度$\boldsymbol{G}$到Riemannian梯度的投影：</p>
<p>$$
\text{grad}<em T__boldsymbol_W="T_{\boldsymbol{W">{\mathcal{M}}\mathcal{L}(\boldsymbol{W}) = \text{Proj}</em>)
$$}}\mathcal{M}}(\boldsymbol{G</p>
<p><strong>定理6.1（msign作为切空间归一化）</strong>：$\text{msign}(\boldsymbol{G})$可以视为将梯度投影到切空间并归一化。</p>
<p><strong>证明思路</strong>：设$\boldsymbol{W}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，$\boldsymbol{G}=\boldsymbol{U}\boldsymbol{G}<em _perp="\perp">U\boldsymbol{V}^{\top} + \boldsymbol{U}</em>}\boldsymbol{G<em _perp="\perp">{\perp}\boldsymbol{V}</em> + \cdots$}^{\top</p>
<p>$\text{msign}$操作保留$\boldsymbol{U},\boldsymbol{V}$空间的成分，丢弃垂直空间，并归一化奇异值。</p>
<h4 id="63-retraction">6.3 指数映射与retraction<a class="toc-link" href="#63-retraction" title="Permanent link">&para;</a></h4>
<p>流形上的指数映射$\exp_{\boldsymbol{W}}:\boldsymbol{T}_{\boldsymbol{W}}\mathcal{M}\to\mathcal{M}$定义为：</p>
<p>$$
\exp_{\boldsymbol{W}}(\boldsymbol{\xi}) = \boldsymbol{W} + \boldsymbol{\xi} + O(|\boldsymbol{\xi}|^2)
$$</p>
<p>Muon的更新可以近似为：</p>
<p>$$
\boldsymbol{W}<em _boldsymbol_W="\boldsymbol{W">{t+1} = \text{Retr}</em>_t))
$$}_t}(-\eta_t\text{msign}(\boldsymbol{M</p>
<p>其中retraction是指数映射的一阶近似。</p>
<p><strong>几何直观</strong>：Muon在矩阵流形上沿着归一化的切向量移动，自动适应流形的弯曲。</p>
<h3 id="7">7. 谱裁剪与梯度裁剪的对比<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 传统梯度裁剪<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p>$$
\boldsymbol{g}_t^{\text{clip}} = \begin{cases}
\boldsymbol{g}_t &amp; |\boldsymbol{g}_t|_2 \leq c \
c\frac{\boldsymbol{g}_t}{|\boldsymbol{g}_t|_2} &amp; |\boldsymbol{g}_t|_2 &gt; c
\end{cases}
$$</p>
<p>这控制了梯度的$\ell_2$范数，但对于矩阵参数，它将矩阵展平处理。</p>
<h4 id="72">7.2 谱裁剪<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p>对于矩阵$\boldsymbol{G}$：</p>
<p>$$
\boldsymbol{G}^{\text{spectral-clip}} = \begin{cases}
\boldsymbol{G} &amp; |\boldsymbol{G}|_2 \leq c \
c\cdot\text{msign}(\boldsymbol{G}) &amp; |\boldsymbol{G}|_2 &gt; c
\end{cases}
$$</p>
<p>其中$|\boldsymbol{G}|<em _max="\max">2 = \sigma</em>)$是谱范数（最大奇异值）。}(\boldsymbol{G</p>
<p><strong>定理7.1（谱裁剪的优势）</strong>：谱裁剪$|\cdot|_2$比Frobenius范数裁剪$|\cdot|_F$更紧：</p>
<p>$$
|\boldsymbol{G}|_2 \leq |\boldsymbol{G}|_F \leq \sqrt{r}|\boldsymbol{G}|_2
$$</p>
<p>其中$r=\text{rank}(\boldsymbol{G})$。</p>
<p><strong>证明</strong>：</p>
<p>$$
|\boldsymbol{G}|<em i="1">F^2 = \sum</em>|_2^2
$$}^r\sigma_i^2 \geq \sigma_{\max}^2 = |\boldsymbol{G</p>
<p>$$
|\boldsymbol{G}|<em i="1">F^2 = \sum</em>|_2^2
$$}^r\sigma_i^2 \leq r\sigma_{\max}^2 = r|\boldsymbol{G</p>
<p>证毕。</p>
<p><strong>实践意义</strong>：谱裁剪避免了单个大奇异值支配更新，Muon通过msign自然实现了这一点。</p>
<h4 id="73">7.3 各向异性分析<a class="toc-link" href="#73" title="Permanent link">&para;</a></h4>
<p>考虑矩阵$\boldsymbol{G}$的条件数：</p>
<p>$$
\kappa(\boldsymbol{G}) = \frac{\sigma_{\max}(\boldsymbol{G})}{\sigma_{\min}(\boldsymbol{G})}
$$</p>
<p><strong>命题7.2</strong>：梯度裁剪保持条件数不变，而msign将条件数降为1。</p>
<p><strong>证明</strong>：
- 梯度裁剪：$\boldsymbol{G}^{\text{clip}} = c\boldsymbol{U}\frac{\boldsymbol{\Sigma}}{|\boldsymbol{\Sigma}|_F}\boldsymbol{V}^{\top}$，奇异值等比例缩放，$\kappa$不变
- msign：$\text{msign}(\boldsymbol{G}) = \boldsymbol{U}\boldsymbol{I}\boldsymbol{V}^{\top}$，所有奇异值为1，$\kappa=1$</p>
<p>证毕。</p>
<p>这解释了为什么Muon在病态问题上表现更好。</p>
<h3 id="8">8. 收敛性分析<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 凸情况下的收敛性<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p><strong>定理8.1（凸函数的收敛率）</strong>：假设$\mathcal{L}:\mathbb{R}^{n\times m}\to\mathbb{R}$是$L$-光滑凸函数，梯度有界$|\boldsymbol{G}_t|_F\leq G$，学习率满足$\eta_t = \frac{\eta}{\sqrt{T}}$，则Muon（$\beta=0$）满足：</p>
<p>$$
\mathbb{E}[\mathcal{L}(\bar{\boldsymbol{W}}_T)] - \mathcal{L}(\boldsymbol{W}^*) = O\left(\frac{1}{\sqrt{T}}\right)
$$</p>
<p>其中$\bar{\boldsymbol{W}}<em t="1">T = \frac{1}{T}\sum</em>_t$。}^T\boldsymbol{W</p>
<p><strong>证明思路</strong>：</p>
<p>设$\boldsymbol{W}^*$是最优解。利用凸性：</p>
<p>$$
\mathcal{L}(\boldsymbol{W}_t) - \mathcal{L}(\boldsymbol{W}^<em>) \leq \langle\boldsymbol{G}_t, \boldsymbol{W}_t - \boldsymbol{W}^</em>\rangle_F
$$</p>
<p>Muon更新：$\boldsymbol{W}_{t+1} = \boldsymbol{W}_t - \eta_t\text{msign}(\boldsymbol{G}_t)$</p>
<p>计算距离变化：</p>
<p>$$
\begin{aligned}
|\boldsymbol{W}_{t+1} - \boldsymbol{W}^<em>|_F^2 &amp;= |\boldsymbol{W}_t - \boldsymbol{W}^</em> - \eta_t\text{msign}(\boldsymbol{G}_t)|_F^2 \
&amp;= |\boldsymbol{W}_t - \boldsymbol{W}^<em>|_F^2 - 2\eta_t\langle\text{msign}(\boldsymbol{G}_t), \boldsymbol{W}_t - \boldsymbol{W}^</em>\rangle_F + \eta_t^2|\text{msign}(\boldsymbol{G}_t)|_F^2
\end{aligned}
$$</p>
<p>由于$|\text{msign}(\boldsymbol{G}_t)|_F^2 = r\leq \min(n,m)$，且：</p>
<p>$$
\langle\text{msign}(\boldsymbol{G}<em i="1">t), \boldsymbol{G}_t\rangle_F = \text{Tr}[\text{msign}(\boldsymbol{G}_t)^{\top}\boldsymbol{G}_t] = \sum</em>}^r\sigma_i(\boldsymbol{G<em _max="\max">t) \geq \sigma</em>_t)
$$}(\boldsymbol{G</p>
<p>通过标准的在线凸优化分析（类似于AdaGrad的分析），可得$O(1/\sqrt{T})$收敛率。</p>
<p>（完整技术证明需要更细致的界，此处给出主要思路）</p>
<h4 id="82">8.2 非凸情况：一阶稳定点<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p><strong>定理8.2（非凸收敛）</strong>：假设$\mathcal{L}$是$L$-光滑非凸函数，梯度有界，$\eta_t = \eta$常数，则：</p>
<p>$$
\min_{t\in[T]}\mathbb{E}[|\nabla\mathcal{L}(\boldsymbol{W}_t)|_F^2] = O\left(\frac{1}{T}\right)
$$</p>
<p><strong>证明思路</strong>：使用Descent Lemma：</p>
<p>$$
\mathcal{L}(\boldsymbol{W}<em t_1="t+1">{t+1}) \leq \mathcal{L}(\boldsymbol{W}_t) + \langle\boldsymbol{G}_t, \boldsymbol{W}</em>} - \boldsymbol{W<em t_1="t+1">t\rangle_F + \frac{L}{2}|\boldsymbol{W}</em>_t|_F^2
$$} - \boldsymbol{W</p>
<p>代入Muon更新：</p>
<p>$$
\begin{aligned}
\mathcal{L}(\boldsymbol{W}<em>{t+1}) &amp;\leq \mathcal{L}(\boldsymbol{W}_t) - \eta\langle\boldsymbol{G}_t, \text{msign}(\boldsymbol{G}_t)\rangle_F + \frac{L\eta^2}{2}|\text{msign}(\boldsymbol{G}_t)|_F^2 \
&amp;\leq \mathcal{L}(\boldsymbol{W}_t) - \eta|\boldsymbol{G}_t|</em>* + \frac{L\eta^2 r}{2}
\end{aligned}
$$</p>
<p>其中$|\boldsymbol{G}<em>t|</em>* = \sum_i\sigma_i(\boldsymbol{G}_t)$是核范数。对于满秩矩阵：</p>
<p>$$
|\boldsymbol{G}<em>t|</em>* \geq \sqrt{r}|\boldsymbol{G}_t|_F
$$</p>
<p>因此：</p>
<p>$$
\mathcal{L}(\boldsymbol{W}_{t+1}) \leq \mathcal{L}(\boldsymbol{W}_t) - \eta\sqrt{r}|\boldsymbol{G}_t|_F + \frac{L\eta^2 r}{2}
$$</p>
<p>求和并平均，可得定理结论。</p>
<h4 id="83-sgdadam">8.3 与SGD/Adam的比较<a class="toc-link" href="#83-sgdadam" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优化器</th>
<th>凸收敛率</th>
<th>非凸收敛率</th>
<th>关键量</th>
</tr>
</thead>
<tbody>
<tr>
<td>SGD</td>
<td>$O(1/\sqrt{T})$</td>
<td>$O(1/T)$</td>
<td>$|\boldsymbol{g}|_2$</td>
</tr>
<tr>
<td>Adam</td>
<td>$O(1/\sqrt{T})$</td>
<td>$O(1/T)$</td>
<td>$|\boldsymbol{g}|_2/\sqrt{\boldsymbol{v}}$</td>
</tr>
<tr>
<td>Muon</td>
<td>$O(1/\sqrt{T})$</td>
<td>$O(1/T)$</td>
<td>$|\boldsymbol{G}|_*$</td>
</tr>
</tbody>
</table>
<p>Muon使用核范数（奇异值和）而非Frobenius范数，在低秩情况下更优。</p>
<h3 id="9">9. 计算复杂度分析<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91-svd">9.1 直接SVD方法<a class="toc-link" href="#91-svd" title="Permanent link">&para;</a></h4>
<p>对于$\boldsymbol{M}\in\mathbb{R}^{n\times m}$（假设$n\geq m$），完整SVD的复杂度：</p>
<p>$$
\mathcal{O}(nm^2) \text{ 时间}, \quad \mathcal{O}(nm + m^2) \text{ 空间}
$$</p>
<p>每步更新需要一次SVD，这在大矩阵时是瓶颈。</p>
<h4 id="92-newton-schulz">9.2 Newton-Schulz迭代<a class="toc-link" href="#92-newton-schulz" title="Permanent link">&para;</a></h4>
<p>迭代公式（$T$步）：</p>
<p>$$
\boldsymbol{X}_{k+1} = a\boldsymbol{X}_k + b\boldsymbol{X}_k(\boldsymbol{X}_k^{\top}\boldsymbol{X}_k) + c\boldsymbol{X}_k(\boldsymbol{X}_k^{\top}\boldsymbol{X}_k)^2
$$</p>
<p><strong>每步迭代的计算量</strong>：
- $\boldsymbol{X}_k^{\top}\boldsymbol{X}_k$: $O(m^2n)$
- $(\boldsymbol{X}_k^{\top}\boldsymbol{X}_k)^2$: $O(m^3)$
- 矩阵乘法$\boldsymbol{X}_k(\cdots)$: $O(nm^2)$</p>
<p>总计：$O(T \cdot nm^2)$，但常数较大（约为$5T$次$nm^2$乘法）。</p>
<p><strong>空间复杂度</strong>：$O(nm + m^2)$，与原矩阵相同。</p>
<h4 id="93-adam">9.3 与Adam的对比<a class="toc-link" href="#93-adam" title="Permanent link">&para;</a></h4>
<p>Adam每步：
- 计算$\boldsymbol{v}<em t-1="t-1">t = \beta_2\boldsymbol{v}</em>_t^2$: $O(nm)$
- 计算$\boldsymbol{G}_t/\sqrt{\boldsymbol{v}_t}$: $O(nm)$
- 总计：$O(nm)$} + (1-\beta_2)\boldsymbol{G</p>
<p><strong>时间倍率</strong>：Muon是Adam的$O(Tm)$倍，对于$m=1024$，$T=5$，约为$5000$倍理论复杂度。</p>
<p><strong>但实际加速</strong>：
1. <strong>并行化</strong>：Newton-Schulz迭代的矩阵乘法高度并行，GPU利用率高
2. <strong>流水线</strong>：在反向传播期间GPU空闲，可异步计算msign
3. <strong>内存带宽</strong>：矩阵乘法是计算密集型（compute-bound），而Adam是内存带宽密集型（memory-bound）</p>
<p><strong>实测结果</strong>：Muon仅增加2-5%的wall-clock时间。</p>
<h4 id="94">9.4 内存优势<a class="toc-link" href="#94" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>优化器</th>
<th>状态变量</th>
<th>空间复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td>Adam</td>
<td>$\boldsymbol{m}_t, \boldsymbol{v}_t$</td>
<td>$2nm$</td>
</tr>
<tr>
<td>Muon</td>
<td>$\boldsymbol{M}_t$</td>
<td>$nm$</td>
</tr>
<tr>
<td>AdamW</td>
<td>$\boldsymbol{m}_t, \boldsymbol{v}_t$</td>
<td>$2nm$</td>
</tr>
</tbody>
</table>
<p>Muon节省50%的优化器状态内存，这在训练大模型时很重要。</p>
<h3 id="10-transformer">10. Transformer训练中的实验结果解释<a class="toc-link" href="#10-transformer" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 训练动态分析<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p>在Transformer训练中观察到的现象：</p>
<p><strong>现象1</strong>：Muon在训练早期收敛更快</p>
<p><strong>解释</strong>：早期梯度各向异性强（条件数大），Muon的谱归一化消除了这种各向异性，使得所有方向的学习速率均衡。</p>
<p>数学上，记$\kappa_t = \frac{\sigma_{\max}(\boldsymbol{G}<em _min="\min">t)}{\sigma</em>$：}(\boldsymbol{G}_t)</p>
<p>$$
\text{Effective learning rate range (Adam)}: \left[\frac{\eta}{\sqrt{v_{\max}}}, \frac{\eta}{\sqrt{v_{\min}}}\right]
$$</p>
<p>$$
\text{Effective learning rate (Muon)}: \eta \quad (\text{all directions})
$$</p>
<p><strong>现象2</strong>：Muon对学习率的鲁棒性更强</p>
<p><strong>解释</strong>：由于msign归一化，更新量的尺度由$\eta$直接控制，而不依赖于梯度尺度。</p>
<p>Adam的更新尺度：$\propto \eta \cdot \frac{|\boldsymbol{m}_t|}{\sqrt{\boldsymbol{v}_t}}$，依赖于梯度统计。</p>
<h4 id="102">10.2 注意力矩阵的优化<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p>对于注意力层的$\boldsymbol{W}_Q, \boldsymbol{W}_K, \boldsymbol{W}_V\in\mathbb{R}^{d\times d}$：</p>
<p><strong>观察</strong>：这些矩阵的梯度通常是低秩的。</p>
<p><strong>定理10.1（低秩梯度的优化）</strong>：当$\text{rank}(\boldsymbol{G}_t) = r \ll d$时，Muon的有效自由度为$O(rd)$，而Adam为$O(d^2)$。</p>
<p><strong>证明</strong>：Muon通过msign将更新投影到秩$r$子空间，只在$r$个奇异方向上更新，每个方向是$d$维向量，故总自由度$O(rd)$。</p>
<p>Adam对所有$d^2$个参数独立更新。</p>
<p><strong>结果</strong>：在低秩梯度情况下，Muon更高效。</p>
<h4 id="103">10.3 层归一化的相互作用<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p>LayerNorm之后的矩阵梯度具有特殊结构：</p>
<p>$$
\boldsymbol{G}_t = \boldsymbol{G}_t^{\text{raw}} - \frac{1}{d}\text{Tr}(\boldsymbol{G}_t^{\text{raw}})\boldsymbol{I} - \frac{1}{d}\boldsymbol{W}_t\boldsymbol{W}_t^{\top}\boldsymbol{G}_t^{\text{raw}}
$$</p>
<p>这导致梯度在某些方向上被抑制。</p>
<p><strong>命题10.2</strong>：Muon的msign操作与LayerNorm的梯度修正自然兼容，不需要额外调整。</p>
<p>（证明略，涉及对称性分析）</p>
<h4 id="104">10.4 数值稳定性<a class="toc-link" href="#104" title="Permanent link">&para;</a></h4>
<p><strong>观察</strong>：Muon在长序列训练中数值更稳定。</p>
<p><strong>解释</strong>：谱范数归一化防止了梯度爆炸。设：</p>
<p>$$
|\boldsymbol{W}_{t+1} - \boldsymbol{W}_t|_2 = \eta|\text{msign}(\boldsymbol{M}_t)|_2 = \eta
$$</p>
<p>更新的谱范数严格受控于学习率，而Adam中：</p>
<p>$$
|\boldsymbol{W}<em _min="\min">{t+1} - \boldsymbol{W}_t|_2 \leq \eta\frac{|\boldsymbol{m}_t|_2}{\sqrt{v</em>
$$}}</p>
<p>当$v_{\min}$很小时，更新可能很大。</p>
<h4 id="105">10.5 实验数据的理论解释<a class="toc-link" href="#105" title="Permanent link">&para;</a></h4>
<p>假设在某个Transformer训练实验中观察到：</p>
<ul>
<li><strong>Muon</strong>在10k步达到loss=2.5</li>
<li><strong>AdamW</strong>在15k步达到loss=2.5</li>
</ul>
<p><strong>理论解释</strong>：</p>
<p>设每步的"有效进展"为$\Delta_{\text{eff}}$，则：</p>
<p>$$
\Delta_{\text{eff}}^{\text{Muon}} \propto \eta|\boldsymbol{G}|<em _text_eff="\text{eff">*, \quad \Delta</em>
$$}}^{\text{Adam}} \propto \eta|\boldsymbol{G}|_F/\sqrt{\mathbb{E}[\boldsymbol{G}^2]</p>
<p>当梯度矩阵低秩时，$|\boldsymbol{G}|_* / |\boldsymbol{G}|_F \approx \sqrt{r/d}$，Muon每步进展更大。</p>
<p><strong>定量估计</strong>：如果平均秩$r\approx d/3$，则Muon每步约$\sqrt{3}\approx 1.7$倍效率，$15k/1.7\approx 8.8k$步，考虑其他因素，10k步合理。</p>
<h3 id="_11">总结<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<p>通过以上详细的数学推导，我们深入理解了Muon优化器的多个方面：</p>
<ol>
<li><strong>矩阵符号函数</strong>作为向量符号函数的自然推广，实现了谱归一化</li>
<li><strong>预条件器形式</strong>揭示了与二阶方法的联系</li>
<li><strong>黎曼几何视角</strong>阐明了在矩阵流形上的优化本质</li>
<li><strong>收敛性分析</strong>建立了理论保证</li>
<li><strong>计算复杂度</strong>分析了实用性</li>
<li><strong>实验现象</strong>得到了理论解释</li>
</ol>
<p>Muon代表了从向量到矩阵的范式转变，充分利用了矩阵参数的结构信息，在理论和实践上都有重要价值。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="从hessian近似看自适应学习率优化器.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#299 从Hessian近似看自适应学习率优化器</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈二十七将步长作为条件输入.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#301 生成扩散模型漫谈（二十七）：将步长作为条件输入</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#muon">Muon优化器赏析：从向量到矩阵的本质跨越</a><ul>
<li><a href="#_1">算法初探</a></li>
<li><a href="#_2">符号函数</a></li>
<li><a href="#_3">迭代求解</a></li>
<li><a href="#_4">收敛加速</a></li>
<li><a href="#_5">一些思考</a></li>
<li><a href="#_6">范数视角</a></li>
<li><a href="#_7">矩阵范数</a></li>
<li><a href="#_8">追根溯源</a></li>
<li><a href="#_9">文章小结</a></li>
<li><a href="#_10">公式推导与注释</a><ul>
<li><a href="#1-muon">1. Muon优化器的完整数学定义</a></li>
<li><a href="#2">2. 从向量到矩阵：优化器的演化</a></li>
<li><a href="#3">3. 矩阵符号函数的深层机制</a></li>
<li><a href="#4">4. 预条件器的矩阵形式推导</a></li>
<li><a href="#5-natural-gradient">5. 与Natural Gradient的联系</a></li>
<li><a href="#6">6. 黎曼几何视角</a></li>
<li><a href="#7">7. 谱裁剪与梯度裁剪的对比</a></li>
<li><a href="#8">8. 收敛性分析</a></li>
<li><a href="#9">9. 计算复杂度分析</a></li>
<li><a href="#10-transformer">10. Transformer训练中的实验结果解释</a></li>
<li><a href="#_11">总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>