<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer升级之路：16、“复盘”长度外推技术 | ML & Math Blog Posts</title>
    <meta name="description" content="Transformer升级之路：16、“复盘”长度外推技术&para;
原文链接: https://spaces.ac.cn/archives/9948
发布日期: 

回过头来看，才发现从第7篇《Transformer升级之路：7、长度外推性与局部注意力》开始，“Transformer升级之路”这个系列就跟长度外推“杠”上了，接连9篇文章（不算本文）都是围绕长度外推展开的。如今，距离第7篇文章刚...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #265 Transformer升级之路：16、“复盘”长度外推技术
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#265</span>
                Transformer升级之路：16、“复盘”长度外推技术
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-01-26</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=位置编码" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 位置编码</span>
                </a>
                
                <a href="../index.html?tags=泛化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 泛化</span>
                </a>
                
                <a href="../index.html?tags=外推" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 外推</span>
                </a>
                
                <a href="../index.html?tags=rope" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> rope</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="transformer16">Transformer升级之路：16、“复盘”长度外推技术<a class="toc-link" href="#transformer16" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9948">https://spaces.ac.cn/archives/9948</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>回过头来看，才发现从第7篇<a href="/archives/9431">《Transformer升级之路：7、长度外推性与局部注意力》</a>开始，“<a href="/search/Transformer%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF/">Transformer升级之路</a>”这个系列就跟长度外推“杠”上了，接连9篇文章（不算本文）都是围绕长度外推展开的。如今，距离第7篇文章刚好是一年多一点，在这一年间，开源社区关于长度外推的研究有了显著进展，笔者也逐渐有了一些自己的理解，比如其实这个问题远不像一开始想象那么简单，以往很多基于局部注意力的工作也不总是有效，这暗示着很多旧的分析工作并没触及问题的核心。</p>
<p>在这篇文章中，笔者尝试结合自己的发现和认识，去“复盘”一下主流的长度外推结果，并试图从中发现免训练长度外推的关键之处。</p>
<h2 id="_1">问题定义<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>顾名思义，免训练长度外推，就是不需要用长序列数据进行额外的训练，只用短序列语料对模型进行训练，就可以得到一个能够处理和预测长序列的模型，即“Train Short, Test Long”。那么如何判断一个模型能否用于长序列呢？最基本的指标就是模型的长序列Loss或者PPL不会爆炸，更加符合实践的评测则是输入足够长的Context，让模型去预测答案，然后跟真实答案做对比，算BLEU、ROUGE等，<a href="https://papers.cool/arxiv/2308.14508">LongBench</a>就是就属于这类榜单。</p>
<p>但要注意的是，长度外推应当不以牺牲远程依赖为代价——否则考虑长度外推就没有意义了，倒不如直接截断文本——这意味着通过显式地截断远程依赖的方案都需要谨慎选择，比如ALIBI以及<a href="/archives/9431">《Transformer升级之路：7、长度外推性与局部注意力》</a>所列举的大部分方案，还有带显式Decay的<a href="/archives/9554">线性RNN</a>，这些方案当序列长度足够大时都表现为局部注意力，即便有可能实现长度外推，也会有远程依赖不足的风险，需要根据自己的场景斟酌使用。</p>
<p>如何判断在长度外推的同时有没有损失远程依赖呢？比较严谨的是像<a href="/archives/9708">《Transformer升级之路：12、无限外推的ReRoPE？》</a>最后提出的评测方案，准备足够长的文本，但每个模型只算每个样本最后一段的指标，如下图所示：  </p>
<p><a href="/usr/uploads/2024/01/888706346.svg" title="点击查看原图"><img alt="一种关注远程依赖的评测方式" src="/usr/uploads/2024/01/888706346.svg" /></a></p>
<p>一种关注远程依赖的评测方式</p>
<p>比如，模型训练长度是4K，想要看外推到16K的效果，那么我们准备一个16K tokens的测试集，4K的模型输入每个样本最后4K tokens算指标，8K模型输入每个样本最后8K tokens但只算最后4K tokens算指标，12K模型输入每个样本最后12K tokens但只算最后4K tokens算指标；依此类推。这样一来，不同长度的模型算的都是同一段tokens的指标，不同的只是输入的Context不一样，如果远程依赖得以有效保留，那么应该能做到Context越长，指标越好。</p>
<h2 id="_2">旋转位置<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>谈完评测，我们回到方法上。文章开头我们提到“旧的分析工作”，这里“新”、“旧”的一个主要特点是“旧”工作多数试图自行设置新的架构或者位置编码来实现长度外推，而最近一年来的“新”工作主要是研究带<a href="/archives/8265">旋转位置编码（RoPE）</a>的、Decoder-Only的Transformer模型的长度外推。</p>
<p>先说个题外话，为什么如今大部分LLM的位置编码都选择了RoPE呢？笔者认为主要有几点原因：</p>
<blockquote>
<p>1、RoPE不带有显式的远程衰减，这对于旨在Long Context的模型至关重要；</p>
<p>2、RoPE是一种真正的位置编码，通过不同频率的三角函数有效区分了长程和短程，达到了类似层次位置编码的效果，这也是Long Context中比较关键的一环；</p>
<p>3、RoPE直接作用于Q、K，不改变Attention的形式，与Flash Attention更契合，更容易Scale Up。</p>
</blockquote>
<p>相比之下，诸如ALIBI、KERPLE等，虽然有时也称为位置编码，但它们实际上只是一种Attention Bias，没有太多位置信息，且不适用于Encoder，能用于Decoder大体上是因为Decoder本身的下三角Mask就已经有较为充分的位置Bias了，额外的Attention Bias只是锦上添花。此外它们无法在单个头内有效区分长程和短程，而是要通过在不同头设置不同的Decay因子来实现，这也意味着它们用于单头注意力（比如<a href="/archives/8934">GAU</a>）的效果会欠佳。</p>
<p>说这么多优缺点的对比，看起来像是“王婆卖瓜，自卖自夸”，其实不然，这只是为了跟大家交换一下观点，因为之前也有读者提出过相同的问题。作为RoPE的提出者，笔者对RoPE的理解不见得一定比大家深刻，毕竟当时提出RoPE的初衷纯粹是好玩，当时的想法是有效就很不错了，能媲美Learnable的绝对位置编码就是非常好的消息了。所以，既然是“意料之外”，那么“作者本人也没多透彻的认识”这件事，也是“情理之中”了。</p>
<h2 id="_3">窗口截断<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>好像又把话题扯偏了。简单来说，其实上两节的内容主要是想表达的观点是：目前看来，RoPE对于Long Context来说是足够的，所以研究RoPE的长度外推是有价值的，以及我们在选择长度外推方案时，不应牺牲远程依赖的能力。</p>
<p>在本站最早讨论长度外推的<a href="/archives/9431">《Transformer升级之路：7、长度外推性与局部注意力》</a>一文中，我们判断长度外推是一个预测阶段的OOD（Out Of Distribution）的问题，尽管用今天的视角看，这篇文章的一些评述已经显得有点过时，但这个根本判断是依然还算正确，放到RoPE中，就是推理阶段出现了没见过的相对距离。为此，一个看上去可行的方案是引入Sliding Window的Attention Mask，如下图左所示：  </p>
<p><a href="/usr/uploads/2024/01/906411783.svg" title="点击查看原图"><img alt="Sliding Window Mask" src="/usr/uploads/2024/01/906411783.svg" /></a></p>
<p>Sliding Window Mask</p>
<p><a href="/usr/uploads/2024/01/3458988955.svg" title="点击查看原图"><img alt="Λ-shape Window Mask" src="/usr/uploads/2024/01/3458988955.svg" /></a></p>
<p>Λ-shape Window Mask</p>
<p>当然，由于强行截断了窗口外的注意力，所以这个方案并不满足“不牺牲远程依赖的能力”的原则，但我们可以只将它作为一个Baseline看待。很遗憾的是，即便做出了如此牺牲，这个方案却是不Work的——连最基本的PPL不爆炸都做不到！对这个现象的深入分析，先后诞生<a href="https://papers.cool/arxiv/2308.16137">《LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models》</a>和<a href="https://papers.cool/arxiv/2309.17453">《Efficient Streaming Language Models with Attention Sinks》</a>两篇论文，并给出了几乎一样的答案。但事实上，在更早的几个月前，一位“业外人士”就发现了相同的结论，并发表在知乎专栏文章<a href="https://zhuanlan.zhihu.com/p/619703849">《Perpetual Sampling Technical Report》</a>上。</p>
<p>答案可能让人意外：<strong>开头的几个Token很重要，不能扔掉。</strong> 所以最后可用的Window Mask应该如上图右（LM-Infinite这篇论文管它叫“$\Lambda$-Mask”）。</p>
<p>为什么开头的Token会占据如此重要的地位呢？目前有两个不同的理解角度：</p>
<blockquote>
<p>1、<strong>开头的几个Token是绝对位置的“锚点”</strong> ：顾名思义，相对位置编码原则上只能识别相对位置，但有些任务可能比较依赖绝对位置，通过开头几个绝对位置约等于0的Token作为“标的”，每个Token就能够测出自己的绝对位置，而去掉开头几个Token后则缺失了这一环，从而完全打乱了注意力模式导致PPL爆炸；</p>
<p>2、<strong>开头的几个Token是注意力的“回收站”</strong> ：由于注意力求和为1，所以注意力一定会分配到某些Token上，但有些情况下模型可能会发现“没什么Token值得注意的”，这时它选择将一部分注意力放到没什么信息量的前几个Token上，起到“不注意”的作用，去掉它们后模型会强行将注意力分配到其他无关的Token，从而扰乱了注意力模式。</p>
</blockquote>
<p>其实说白了，就是实测发现大部分情况下，前几个Token的注意力占比还是很重的，所以不能去掉，去掉注意力就全乱了。至于为什么很重，就看大家的想象力了。</p>
<h2 id="_4">位置内插<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>窗口截断的方式固然可以作为长度外推的一个不错的Baseline，同时“锚点”或者“回收站”的结果也让我们对注意力机制的工作方式有了进一步的理解，但正如前面所说，这是通过强行截断窗口外的注意力、牺牲远程依赖换来的，因此还不是最终的解决方案。</p>
<p>相对位置的OOD，直接表现就是预测阶段的相对位置超出了训练时的范围，由于没有被训练过，“越界”部分的行为无法预估。为此，一位网名为“kaiokendev”的网友在他的博客<a href="https://kaiokendev.github.io/til#extending-context-to-8k">《https://kaiokendev.github.io/til#extending-context-to-8k》</a>中提出了一个非常朴素的解决办法——“位置内插”——将预测的长文本的位置编码乘上因子$\frac{L_{train}}{L_{test}}$，缩放到训练长度范围内，如下式所示（式中的位置都是相对位置）。没过多久，Meta在论文<a href="https://papers.cool/arxiv/2306.15595">《Extending Context Window of Large Language Models via Positional Interpolation》</a>中也发布了同样的方法，命名为“Positional Interpolation（PI）”，并补充了较为充分的实验结果。<br />
\begin{equation}\begin{aligned}&amp;\text{训练阶段}:\,(1,2,\cdots,n-1,n)\\[5pt]
&amp;\text{预测阶段}:\,(1,2,\cdots,n,\underbrace{n+1,\cdots,4n-1,4n}<em _text_局部失真="\text{局部失真">{\text{远处越界}})\xrightarrow{\quad\text{内插}\quad}
\big(\underbrace{\frac{1}{4},\frac{2}{4},\frac{3}{4}}</em>}},\cdots,n-\frac{1}{4},n\big)\end{aligned}\end{equation</p>
<p>然而，位置内插并不算长度外推方案，至少不是免训练的长度外推方案，因为位置内插之后同样会有PPL爆炸的问题。原因也不难理解，尽管位置内插避免了远处的位置越界问题，但这同时压缩了邻近Token的距离，严重扰乱了模型的局部分辨率，而众所周知语言模型本身就是一个非常依赖于局部关系的任务，所以扰乱了局部自然就没法预测准了。</p>
<p>不过，这也并非说位置内插就没有价值了。我们知道，需要长度外推的读者，无外乎是两种情况：一种是没有资源去做长文本微调，希望能够从短文本模型直接得到一个可用的长文本模型，这种需求对长度外推的效果要求会比较高，位置内插就不适合他们了；另一种是有资源去做长文本微调，研究长度外推纯粹是为了得到一个更好的初始化模型，这种情况对模型修改带来的初始损失容忍度比较高，只要能够通过微调快速弥补回损失掉的效果即可，位置内插正好是属于此类方法。Meta的论文显示，经过PI之后，仅需1000步左右的长文本训练，就可以得到一个行之有效的长文本模型，这比不做任何修改直接微调的训练效率高出很多。</p>
<h2 id="_5">保近压远<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>直接外推的问题是远处越界，而位置内插的问题是局部失真，看上去两者是互补的，能不能集两者之长呢？这就是<a href="/archives/9708">《Transformer升级之路：12、无限外推的ReRoPE？》</a>所提出的Leaky ReRoPE，以及它的极限版本ReRoPE。</p>
<p>基于上一节的分析，我们不难推测实现免训练长度外推的要领是“保近压远”，即“保证局部不失真”和“压缩远处不越界”，Leaky ReRoPE通过一个非常直接的思路实现了这一点：它先设定一个窗口大小$w$内，将相对位置分为两部分，在窗口不改变相对位置实现“局部不失真”，在窗口外使用位置内插实现“远处不越界”，如下式：<br />
\begin{equation}\begin{pmatrix}
\color{red}{0} &amp; \\
\color{red}{1} &amp; \color{red}{0} &amp; \\
\color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{red}{\small{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{w} &amp; \color{red}{\small{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\small{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\small{w + \frac{2}{k}}} &amp; \color{green}{\small{w + \frac{1}{k}}} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\ddots} &amp; \color{green}{\small{w + \frac{2}{k}}} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \\
\color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\small{w + \frac{2}{k}}} &amp; \color{green}{\small{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\small{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\small{w + \frac{L-1-w}{k}}} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\small{w + \frac{2}{k}}} &amp; \color{green}{\small{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\small{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\end{pmatrix}\end{equation}</p>
<p>如果将内插的因子$k$取到无穷大，这就得到极简的ReRoPE，它在窗口外的位置编码都变为$w$，意味着对于任意长的序列都不会越界，即理论上具备无限外推的潜力！事实上，Leaky ReRoPE和ReRoPE的表现确实都非常好，从Loss来看，它们能做到几乎不损失训练长度内的效果，并且实现了长度外推，且Context越长，Loss越低，说明它们在外推的同时还确实保证了远程依赖。</p>
<p>Leaky ReRoPE和ReRoPE的主要问题在于它们的代码实现稍微有点麻烦。跟Attention Bias类的位置编码不同，RoPE没法通过先构造相对位置矩阵然后才计算相对位置编码的方式来实现（那样效率太低），只能通过绝对位置编码的方式来实现相对位置编码，这意味着它只能实现线性增长的相对位置，而Leaky ReRoPE和ReRoPE的相对位置是分段线性的，这意味着朴素地实现的话，需要算两次Attention矩阵（得到两段不同的线性）然后将它们拼接起来，这样效率无疑明显降低了。</p>
<p>不过，好消息是当前主流的Attention加速手段如Flash Attention都是将Attention分块计算的，比如每128长度为一块，这样当序列足够长时，分段线性的块占比非常少（只有窗口边界附近），如下式所示，只有红绿混色的块才需要重复计算Attention，剩下同色的块都只需要计算一次，所以结合分块计算Attention的话，Leaky ReRoPE和ReRoPE所增加的计算成本时几乎可以忽略的。此前读者 @chu-tianxiang 在<a href="/archives/9708/comment-page-2#comment-22614">评论区</a>也分享了一个基于Triton的实现，大家有兴趣的可以参考一下。<br />
\begin{equation}\left(\begin{array}{cccc:cccc:cccc}
\color{red}{0} &amp; \\
\color{red}{1} &amp; \color{red}{0} &amp; \\
\color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\hdashline
\color{red}{\small{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{w} &amp; \color{red}{\small{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\small{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\small{w + \frac{2}{k}}} &amp; \color{green}{\small{w + \frac{1}{k}}} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\hdashline
\color{green}{\ddots} &amp; \color{green}{\small{w + \frac{2}{k}}} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \color{red}{\ddots} &amp; \\
\color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\small{w + \frac{2}{k}}} &amp; \color{green}{\small{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\small{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\color{green}{\small{w + \frac{L-1-w}{k}}} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\ddots} &amp; \color{green}{\small{w + \frac{2}{k}}} &amp; \color{green}{\small{w + \frac{1}{k}}} &amp; \color{green}{w} &amp; \color{red}{\small{w - 1}} &amp; \color{red}{\ddots} &amp; \color{red}{2} &amp; \color{red}{1} &amp; \color{red}{0} &amp; \\
\end{array}\right)\end{equation}</p>
<p>无独有偶，月初Arxiv上提交了一篇论文<a href="https://papers.cool/arxiv/2401.01325">《LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning》</a>，其中提出了一种名为“Self-Extend”的免训练长度外推方法，它实际上就是在Leaky ReRoPE的基础上加了Round运算（四舍五入），使得每个相对位置都变回整数，进一步减轻相对位置的OOD问题。论文报告的效果也很好，这进一步肯定了Leaky ReRoPE的有效性。</p>
<h2 id="_6">转圈视角<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>尽管Leaky ReRoPE和ReRoPE的实际效果相当不错（至少Loss如此），但它们跟位置内插一样，都是直接操作位置编号（Position Ids），这给人一种“头疼医头，脚痛医脚”的感觉，欠缺了对内在规律的深入分析。因为对于模型来说，位置编号并不重要，位置嵌入（Position Embeddings）才是跟模型直接交互的，所以想要更深入地“直达病灶”，应该尝试从位置嵌入着手。</p>
<p>可能有读者疑问：位置编号跟位置嵌入不是一一对应吗？操作位置编号不等价于操作位置嵌入？是这样说，但两者的实际表现是不一样的，比如位置编号是无界的，但是位置嵌入是有界的（RoPE是三角函数组成，三角函数有界），跟模型直接打交道的是位置嵌入，位置编号OOD了，位置嵌入未必OOD，所以从位置嵌入角度分析，能更清晰地理解长度外推导致的OOD具体是什么表现，从而更加“对症下药”。</p>
<p>在<a href="/archives/8265">《Transformer升级之路：2、博采众长的旋转式位置编码》</a>中我们推导RoPE的时候，是先利用复数推导了二维的解，然后将多个二维的解拼接成一个高维的解，这样一来，加了RoPE之后的$\boldsymbol{q},\boldsymbol{k}$内积，可以用复数表示为<br />
\begin{equation}
(\boldsymbol{\mathcal{R}}<em i="0">m \boldsymbol{q})^{\top}(\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \text{Re}\left[\sum</em>}^{d/2-1}\boldsymbol{q<em _2i:2i_1_="[2i:2i+1]">{[2i:2i+1]}\boldsymbol{k}</em>}^* e^{\text{i}(m-n)\theta_i}\right]\end{equation
其中$\theta_i$默认是$10000^{-2i/d}$，这是一个从1渐变到接近于0的函数。从欧拉公式$e^{\text{i}t}=\cos t + \text{i}\sin t$可以知道，$e^{\text{i}(m-n)\theta_i}$实际上就单位圆上的点，当$m-n$逐渐变大时，这个点就在单位圆上转圈（真·旋转），$\theta_i$越大则转得越快，反之越慢。</p>
<p><a href="/usr/uploads/2024/01/1398215252.svg" title="点击查看原图"><img alt="转圈多于一周" src="/usr/uploads/2024/01/1398215252.svg" /></a></p>
<p>转圈多于一周</p>
<p><a href="/usr/uploads/2024/01/1797389712.svg" title="点击查看原图"><img alt="转圈不足一周" src="/usr/uploads/2024/01/1797389712.svg" /></a></p>
<p>转圈不足一周</p>
<p>假设训练长度为$L_{train}$，那么$m-n\in[0, L_{train}-1]$，接下来让我们充分发挥想象力：较大的$\theta_i$意味着转速越快，周期越短，于是在$m-n$从$0$到$L_{train}-1$期间，它已经被转了很多圈，也就是说圆上的每一个点几乎都被训练过，因此这些$\theta_i$几乎不存在OOD问题；相反，对于较小的$\theta_i$，当$m-n$从$0$到$L_{train}-1$时它可能还没转完一圈，这种情况下被训练过的点顶多只是圆上的一条弧，如果测试时遇到更大的$L_{test}$，那么就超出了训练过的弧范围，从而有无法预估的表现，这时候就需要通过内插将它压缩到原本的弧内。说白了，位置标号$m-n$是否OOD根本不重要，重要的是单位圆上的点是否被充分训练过，如果是，那么就可以不做改动（直接外推），否则就要想办法将它压缩到已经被充分训练过的那段弧上（位置内插）。</p>
<p>具体来说，对于$\theta_i$，我们可以算出周期为$T_i=2\pi/\theta_i$，然后可以算出在训练过程中它所转的“圈数”为$r_i=\frac{L_{train}}{T_i}=\frac{\theta_i L_{train}}{2\pi}$，我们可以设一个圈数的阈值$\tau$，圈数超过$\tau$的，就认为已经充分训练了，可以不加改动；圈数少于1的，$\theta_i$改为$\frac{\theta_i L_{train}}{L_{test}}$，意味着要把超出弧范围的重新缩放到弧内；至于剩下的部分，就在两者之间线性插值过渡。用公式表达就是：<br />
\begin{equation}\theta_i^{new} = \left[\gamma_i + (1 - \gamma_i)\frac{L_{train}}{L_{test}}\right]\theta_i,\quad \gamma_i = \left\{\begin{aligned}&amp;1,&amp;r_i &gt; \tau \\
&amp;0,&amp;r_i &lt; 1 \\
&amp;\frac{r_i - 1}{\tau - 1},&amp;\text{others}
\end{aligned}\right.\end{equation}<br />
这就是<a href="https://papers.cool/arxiv/2309.00071">《YaRN: Efficient Context Window Extension of Large Language Models》</a>一文所提出的免训练长度外推方案“YaRN”，在笔者的测试中，它的外推效果非常好，只是略逊于Leaky ReRoPE和ReRoPE。但要注意的是，YaRN只改变$\theta_i$的值，不改变Attention和RoPE的形式，因此不会有额外的实现成本和推理成本，在满足这个条件之下（即可以完全代入已有的实现），YaRN是笔者测试过的效果最佳的长度外推方法。</p>
<h2 id="_7">一些插曲<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>其实YaRN的故事还没完，不过感觉上一节已经很长了，还是另开一节比较好。YaRN除了引入$\theta_i$的改动外，还在Attention的Logits上多乘了一个Scale因子：<br />
\begin{equation}\lambda = \left(1 + 0.1 \log \frac{L_{test}}{L_{train}}\right)^2\label{eq:scale-yarn}\approx 1 + 0.2 \log \frac{L_{test}}{L_{train}}\end{equation}<br />
关于这个Scale的推导，可能会让人有点啼笑皆非，答案是根本没有推导，作者说他也没能从理论上推导出来，纯粹是实验发现加了以上Scale后PPL更低，以上形式也是通过实验拟合出来的。</p>
<p>其实这个带对数的结果，很明显跟<a href="/archives/8823">《从熵不变性看Attention的Scale操作》</a>推导出来的$\log n$ Scale非常相似，只不过后者跟具体位置有关，而前者在确定了$L_{test}$之后就是一个常数。考虑到当$n$比较大时，$\log n$函数变化比较缓慢，所以在一定范围内取为常数也无可厚非，因此，不难猜测YaRN的这个Scale因子跟熵不变性的$\log n$ Scale应该是同源的。笔者也做过对比，将常数$\lambda$换成如下跟绝对位置$n$相关的因子，能起到相近的效果：<br />
\begin{equation}\lambda_n = \max\left(1, \frac{\log n}{\log L_{train}}\right)\label{eq:clip-logn}\end{equation}<br />
注意到<br />
\begin{equation}\frac{\log L_{test} }{\log L_{train}} = 1 + \frac{1}{\log L_{train}} \log\left(\frac{L_{test}}{L_{train}}\right)\end{equation}<br />
YaRN是基于LLAMA和LLAMA2做实验的，前者训练长度是2K，后者是4K，我们有$\frac{1}{\log 2048}\approx 0.13$，$\frac{1}{\log 4096}\approx 0.12$，系数大致是式$\eqref{eq:scale-yarn}$的一半，差别不大，事实上这个系数的精确值可能不太重要，因为笔者也发现过式$\eqref{eq:clip-logn}$更好的数据集，所以由此我们便算将式$\eqref{eq:scale-yarn}$近似地推导出来了。</p>
<p>相比YaRN本身，YaRN的作者Bowen Peng的故事也许更加称得上“引人入胜”，他早前所提出的<a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-RoPE</a>是RoPE的第一个免训练的长度外推方案，本系列的两篇博客<a href="/archives/9675">《Transformer升级之路：10、RoPE是一种β进制编码》</a>和<a href="/archives/9706">《Transformer升级之路：11、将β进制位置进行到底》</a>都直接受启发于它。虽然从目前来看，NTK-RoPE的效果不见得多好（相比YaRN、ReRoPE等），但它首次显示了免训练长度外推的可能性，具有里程碑式的意义，甚至可以说，后续的所有长度外推相关研究，都直接或者间接得益于NTK-RoPE打开了大家的想象力。</p>
<p>NTK-RoPE的思路很简单，仅改一下RoPE的base就行，即原本是$\theta_i = 10000^{-2i/d}$，现在改为$\theta_i = (10000\kappa)^{-2i/d}$。$\kappa$怎么选取呢？当时Bowen Peng基于自己对NTK（Neural Tangent Kernel）相关结果的经验，判断高频（$i\to 0$）是学习相对距离的，所以不用改变，低频（$i\to d/2-1$）是学习绝对距离的，因此要进行内插，总结起来就是“高频外推、低频内插”，于是他通过令$i = d/2-1$时的Scale正好等于内插Scale$\frac{L_{train}}{L_{test}}$，得出方程<br />
\begin{equation}(10000\kappa)^{-2i/d}|<em train="train">{i=d/2-1} = \left.\frac{L</em>\right|}}{L_{test}}10000^{-2i/d<em test="test">{i=d/2-1}\end{equation}<br />
解得<br />
\begin{equation}\kappa = \left(\frac{L</em>}}{L_{train}}\right)^{d/(d-2)}\label{eq:kappa}\end{equation
就这么个简单且高明的推导，打开了免训练长度外推的“潘多拉魔盒”。</p>
<p>从YaRN的视角看，并非只有$i = d/2-1$时的$\theta_i$才转得不足一周，所以NTK-RoPE只让最后一个$i = d/2-1$做完整的内插，是不够充分的，事实上确实也是如此，设置如式$\eqref{eq:kappa}$的$\kappa$，实际上只能让模型外推到$L_{test}/2$左右的长度而不发生PPL爆炸，再长PPL就明显上升了。也就是因为有这个问题，作者才进一步提出了后来的升级版方案YaRN。</p>
<p>不过，尽管NTK-RoPE效果上不如YaRN，但对于前面提到的第二种有资源去做长文本微调的读者，可能会更喜欢NTK-RoPE，因为他们只是为了得到一个更好的初始化模型，反正都是要微调，NTK-RoPE与YaRN的初始效果差异他们并不会太在意，相比之下他们更乐意选择实现更简单的NTK-RoPE了，比如<a href="https://papers.cool/arxiv/2308.12950">CodeLLAMA</a>就是在LLAMA2的基础上将base改为$10^6$然后继续训练的。此外，Meta在其论文<a href="https://papers.cool/arxiv/2309.16039">《Effective Long-Context Scaling of Foundation Models》</a>中，将NTK-RoPE改称为RoPE-ABF（Adjusted Base Frequency），相比神秘的NTK，ABF的名称能更直观体现出它的含义。</p>
<h2 id="_8">拒绝交税<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>不知道大家留意到没有，上面提到的免训练长度外推方法，都无法使得模型在训练长度$L_{train}$内的效果保持不变。具体来说，设原本模型为$f(x)$，做了外推改动后的模型是$f^+(x)$，当$x$的长度不超过$L_{train}$时，无法保证$f(x)\equiv f^+(x)$。由于$f(x)$就是在$L_{train}$内训练的，因此可以合理地认为$f(x)$对于长度不超过$L_{train}$的样本效果是最优的，于是$f^+(x)\neq f(x)$意味着长度外推虽然使得更长的样本效果变好了，但原本$L_{train}$内的效果却变差了。我们可以形象将这部分损失称为“<strong>外推税</strong> ”。</p>
<p>早在NTK-RoPE刚提出那会，开源社区就意识到了“外推税”的问题，并提出了对应的解决办法——随着训练长度的变化动态地调整各个外推方法的Scale因子，这就是Dynamic Scaling，最早提出自Reddit的帖子<a href="https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/">《Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning》</a>。以YaRN为例，里边跟长度相关的缩放因子是$s=\frac{L_{test}}{L_{train}}$，Dynamic Scaling将它换成动态的$s(pos)=\frac{\max(L_{train}, pos+1)}{L_{train}}$，其中$pos$是当前Token的位置编号（从零开始算）。这个改动意味着Dynamic Scaling试图给每个位置都找到最小的、理论上对模型效果影响也最小的Scale因子（或者等价地，每个位置都配不一样的$\theta_i(pos)$)，从而达到拒绝交税的效果。</p>
<p>不过，要想真正实现每个位置都有不一样的$\theta_i(pos)$是很困难的。跟Leaky ReRoPE和ReRoPE需要重复计算Attention的原因一样，因为RoPE是通过绝对位置的方式实现相对位置，这意味着单次计算只能实现一个固定的$\theta_i$，不同位置要想实现不同的$\theta_i$，那么KV Cache中的K只能存Apply RoPE之前的，并且不同位置得分别计算多次，这就变成了类似RNN的递归过程。我们知道LLM回复一轮对话，可以分为prefill和generation两个阶段，prefill指的是对输入部分的计算，generation就是token by token的生成阶段。很明显prefill阶段原本是可并行的，如果也改为像generation那样的递归，那么在输入很长（比如输入一篇论文）时，无疑会明显拖慢计算速度，因此变得不大实际。</p>
<p>于是，一个折中的方法是“局部静态”：prefill阶段的输入有多少个Tokens我们是可以计算得到的，然后generation阶段我们也会设置一个max_gen_tokens，我们将这两个数字相加，作为当前这一轮对话的$L_{test}$去计算对应的$\theta_i$；完成这一轮对话后，下一轮对话我们再用同样的方式更新$L_{test}$和$\theta_i$。如此一来，就不用给模型引入过于复杂或者牺牲效率的实现了，算是一个比较实用的方案，尤其是当输入很长时，max_gen_tokens远小于prefill的tokens，在单论对话中的Scale本来就近似为常数。</p>
<p>Dynamic Scaling的思想，可以说被<a href="https://papers.cool/arxiv/2310.16450">《CLEX: Continuous Length Extrapolation for Large Language Models》</a>提出的CLEX发挥到了极致：CLEX同样要给每个位置都配不一样的$\theta_i(pos)$，它将$\theta_i(pos)$假设为关于$pos$的连续函数，并且用一个神经ODE来建模，通过微调来拟合这个ODE的参数，最终取得了比YaRN更好的结果，并且实验结果显示不断地Dynamic Scaling下去，可以得到近乎无限的长度外推能力。</p>
<h2 id="_9">另起炉灶<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>除了Dynamic Scaling外，“拒绝交税”的另一个思路是“另起炉灶”，通过重新设计预训练时所用的模型架构，使得它具备训练完成后就可以不做任何修改实现长度外推的潜力，在这个系列的文章中，笔者有两篇相关的探讨，分别是在<a href="/archives/9603">《Transformer升级之路：9、一种全局长度外推的新思路》</a>所提到HWFA（Hybird Window-Full Attention），以及在<a href="/archives/9859">《Transformer升级之路：15、Key归一化助力长度外推》</a>所验证的Key Norm。</p>
<p>其中，HWFA是将模型的前$L-1$层Attention都换成窗口很小的RoPE + Window Attention，而最后一层Attention则换成NoPE + Full Attention，这样修改之后训练出来的模型，不加改动就有一定的长度外推效果。包含类似思想的还有<a href="https://papers.cool/arxiv/2307.03170">《Focused Transformer: Contrastive Training for Context Scaling》</a>，不过这篇不是做长度外推的，而是想要通过简单微调来拓展LLM的Context长度。HWFA的问题是在训练效果上会逊色于标准的Attention模型，为此笔者后来在<a href="/archives/9731">《Transformer升级之路：14、当HWFA遇见ReRoPE》</a>提出了改进版的HWFA2（即HWFA + ReRoPE）。</p>
<p>相比HWFA，HWFA2的Window Attention使用了更大的Window Size，并且恢复了Full Attention的RoPE，同时允许多于一层的Full Attention穿插在Window Attention之间（而不只是一层放在最后），这样的修改可以追平与标准Attention在训练效果上的差距（甚至偶尔会更优），但缺点是不能够不加改动就实现长度外推了（需要将RoPE换ReRoPE），也算是有得有失。当然，我们也可以不看外推效果，纯粹将HWFA2看成是一种不损失效果而且明显降低模型复杂度的加速方案。顺便说，上个月Arxiv的一篇论文<a href="https://papers.cool/arxiv/2312.08618">《Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention》</a>提出了名为Zebra的方法，它跟HWFA2都是若干层Full Attention穿插在Window Attention中的组合方式。</p>
<p>至于Key Norm，则是源于将Attention的Key做了L2归一化后模型长度外推能力居然明显变好了的“意外发现”，对它的进一步思考加深了笔者对长度外推的理解。对于标准的基于Q、K内积的Attention，我们可以将它表示为<br />
\begin{equation}s(n|m) = \boldsymbol{q}<em j="1">m\cdot \boldsymbol{k}_n = \Vert\boldsymbol{q}_m\Vert \Vert\boldsymbol{k}_n\Vert \cos(\boldsymbol{q}_m,\boldsymbol{k}_n),\quad p(j|i) = \frac{\exp\left(\frac{s(n|m)}{\sqrt{d}}\right)}{\sum\limits</em>}^i \exp\left(\frac{s(n|m)}{\sqrt{d}}\right)}\end{equation
很明显，要想提高$n$对某个$m$的相对注意力，模型有两个选择：增大$\Vert\boldsymbol{k}_n\Vert$，或者增大$\cos(\boldsymbol{q}_m,\boldsymbol{k}_n)$。由于维度灾难的原因，增大$\Vert\boldsymbol{k}_n\Vert$会比增大$\cos(\boldsymbol{q}_m,\boldsymbol{k}_n)$更加容易，所以如果有可能的话，模型会尽可能选择增大$\Vert\boldsymbol{k}_n\Vert$，而$\Vert\boldsymbol{k}_n\Vert$跟$i$无关，描述的是绝对重要性，这可能是<a href="https://papers.cool/arxiv/2305.17118">Scissorhands</a>所描述的注意力分布特点的成因之一。另一方面，模型倾向于选择增大$\Vert\boldsymbol{k}_n\Vert$，那么意味着对$\cos(\boldsymbol{q}_m,\boldsymbol{k}_n)$的训练可能不充分，这大概是Attention无法长度外推的更本质的原因。</p>
<p>由此，Key Norm能够改善长度外推能力的原因就豁然开朗了。Key Norm将所有的$\Vert\boldsymbol{k}_n\Vert$都归一化为1，模型便没有了“增大$\Vert\boldsymbol{k}_n\Vert$”这个选择，于是只能一心调整$\cos(\boldsymbol{q}_m,\boldsymbol{k}_n)$，使得$\cos(\boldsymbol{q}_m,\boldsymbol{k}_n)$的训练更加充分。同时，笔者也做过对比实验，Key Norm只有在跟RoPE配合才能体现出长度外推能力，Key Norm + NoPE或者单纯的NoPE都没有长度外推效果，这大概也是RoPE本身的旋转作用，丰富了$\boldsymbol{q}_m,\boldsymbol{k}_n$夹角的多样性（像是数据扩增），从而让$\cos(\boldsymbol{q}_m,\boldsymbol{k}_n)$的训练更加充分了。</p>
<p>还有一篇名为<a href="https://papers.cool/arxiv/2309.08646">《CoCA: Fusing position embedding with Collinear Constrained Attention for fine-tuning free context window extending》</a>的论文从另外的角度提出了一个解决办法：它通过修改注意力的实现方式，使得每一组$\boldsymbol{q}_m^{(i)},\boldsymbol{k}_m^{(i)}$都有$\cos(\boldsymbol{q}_m^{(i)},\boldsymbol{k}_m^{(i)})=1$，这里的分组$i$就是前述的RoPE对$\boldsymbol{q},\boldsymbol{k}$分量的两两分组，这样设计会使得比较大的$\cos(\boldsymbol{q}_m,\boldsymbol{q}_n)$尽量都被训练过（$\cos$最大也就是1），训练不充分的也只是小的部分（这部分Softmax后的概率会比较小，不会明显干扰注意力分布），从而获得一定的长度外推的能力。不过CoCA对注意力的修改，有降低每个注意力头的能力上限的风险，即相同参数下，它可能只有head_size/2的标准注意力头的拟合能力。</p>
<h2 id="_10">其他思路<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>写到这里，对长度外推的介绍也已经尾声了。尽管已经写了相当长的篇幅，但依然很难对所有长度外推的工作都做详细介绍。下面零散地列举一些能想起来的其他相关工作。</p>
<p>一开始，我们认为Attention不能长度外推是因为预测时位置的“越界”，对此一个朴素的解决方法是对训练阶段的位置编码进行扰动，即类似数据扩增的方式，以求让模型提前适应预测所用的位置编码，本系列的<a href="/archives/9444">《Transformer升级之路：8、长度外推性与位置鲁棒性》</a>和<a href="/archives/9728">《Transformer升级之路：13、逆用Leaky ReRoPE》</a>都可归入这类，此外还包括几个月前的<a href="https://papers.cool/arxiv/2309.10400">《PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training》</a>。这类方法在笔者的实验中不算太稳定，而且带来了额外的复杂度或者随机性，很难保证不会影响模型原本的Scaling Law。</p>
<p>有些读者曾提出过疑问：在YaRN的分析中，要做插值的是低频部分，那如果干脆直接去掉低频部分会怎么样？或者类似地，将base调小，让高频部分的比例增加？笔者缺失在预训练中尝试过调小RoPE的base，结果是最终效果更差了，并且也没表现出长度外推能力。不过<a href="https://papers.cool/arxiv/2310.05209">《Scaling Laws of RoPE-based Extrapolation》</a>（知乎有中文版<a href="https://zhuanlan.zhihu.com/p/660073229">《RoPE外推的缩放法则 —— 尝试外推RoPE至1M上下文》</a>）尝试过另一种方案，是在微调阶段才调小Base，配合短文本的微调后能体现出长文本的外推能力。</p>
<p>但从笔者的角度看，调小Base甚至去掉低频的做法并不科学，即便它可能在某些情况下有长度外推效果，但可能损失掉模型本身的能力。就像NTK-RoPE、YaRN的作者Bowen Peng曾经的观点，高频学习到的是局部的相对距离，低频学习到的是远程的绝对距离，两者都很重要，它们之间更像是一种层次的关系；用<a href="/archives/9675">《Transformer升级之路：10、RoPE是一种β进制编码》</a>的进制类别来看，低频对应的就是高位，如果只保留低位而去掉高位，那么结果就相当于求模（余数），无法准确表达出位置信息来；更何况，高频和低频其实是相对的，一个频率对10K长度的文本来说是低频，但对于100K长度的文本来说可能就是高频了。</p>
<p>近来还有一个有意思的论文<a href="https://papers.cool/arxiv/2312.04455">《Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use》</a>，它发现同一个模型改不同的base然后将输出取平均，能增强模型的整体性能，这表明不同大小的base各有所长，不能单纯为了外推而去调小它。</p>
<p>总的来说，长度外推技术虽然有了长足的进展，但依然还是一件很神秘的事情。比如，推理阶段将RoPE换成ReRoPE，就能体现出一定的长度外推效果，那么预训练阶段就换成ReRoPE，是不是外推效果就更好？恰恰相反，笔者做过训练阶段就切换为ReRoPE的实验，结果训练出来的模型没有一丁点长度外推能力。这大体也跟Key Norm那里的分析有关，训练阶段就换ReRoPE降低了$\boldsymbol{q}_n,\boldsymbol{k}_m$夹角的多样性，反而让$\cos(\boldsymbol{q}_n,\boldsymbol{k}_m)$的训练没那么充分，从而降低了长度外推能力。还有很多长度外推技术可能跟架构绑定的，早年的一些据说有长度外推能力的位置编码，包括ALIBI、KERPLE、XPOS等，都是在Multi-Head Attention + Pre Norm上做的实验，而笔者在Single Head的GAU + Post Norm上，从未测出它们有长度外推能力，这表明关于长度外推的分析，很可能还缺少了架构相关的那一环。</p>
<h2 id="_11">文章小结<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h2>
<p>在这篇文章中，笔者结合自己的学习经历，梳理了过去一年来关于长度外推的相关进展，主要简明地介绍了相关方法的特点和背后的思想，并试图将它们串联起来，希望本文能帮助大家更深入、更系统地了解长度外推这个课题。如果有什么错漏之处，也请读者提醒和指正。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9948">https://spaces.ac.cn/archives/9948</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jan. 26, 2024). 《Transformer升级之路：16、“复盘”长度外推技术 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9948">https://spaces.ac.cn/archives/9948</a></p>
<p>@online{kexuefm-9948,<br />
title={Transformer升级之路：16、“复盘”长度外推技术},<br />
author={苏剑林},<br />
year={2024},<br />
month={Jan},<br />
url={\url{https://spaces.ac.cn/archives/9948}},<br />
} </p>
<hr />
<h2 id="_12">公式推导与注释<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h2>
<h3 id="1">1. 长度外推问题的数学建模<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<p><strong>定义1.1：训练-测试长度差异</strong></p>
<p>设模型在训练时使用的序列长度为$L_{train}$，测试时需要处理的序列长度为$L_{test}$，长度外推问题要求：
$$
L_{test} \gg L_{train}
$$</p>
<p>同时模型性能不应显著下降。</p>
<p><strong>定义1.2：位置分布偏移</strong></p>
<p>训练时的位置集合：
$$
\mathcal{P}<em train="train">{train} = {(m, n) : 1 \leq m, n \leq L</em>}
$$</p>
<p>测试时的位置集合：
$$
\mathcal{P}<em test="test">{test} = {(m, n) : 1 \leq m, n \leq L</em>}
$$</p>
<p>分布偏移度量：
$$
\text{OOD} = \frac{|\mathcal{P}<em train="train">{test} \setminus \mathcal{P}</em>}|}{|\mathcal{P<em train="train">{test}|} = 1 - \frac{L</em>
$$}^2}{L_{test}^2</p>
<p><strong>推导1.1：PPL爆炸的定量定义</strong></p>
<p>困惑度（Perplexity）定义为：
$$
\text{PPL} = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log p(x_i | x_{&lt;i})\right)
$$</p>
<p>PPL爆炸的判定条件：
$$
\frac{\text{PPL}<em train="train">{test}}{\text{PPL}</em> &gt; \tau
$$}</p>
<p>其中阈值$\tau$通常取1.5到2之间。当$\tau &gt; 2$时，表明严重的外推失效。</p>
<h3 id="2">2. 相对位置编码的基本理论<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<p><strong>定理2.1：RoPE的相对位置性质</strong></p>
<p>RoPE通过旋转变换实现相对位置编码。对于二维情况：
$$
\boldsymbol{\mathcal{R}}_m = \begin{pmatrix} \cos(m\theta) &amp; -\sin(m\theta) \ \sin(m\theta) &amp; \cos(m\theta) \end{pmatrix}
$$</p>
<p>关键性质：
$$
\boldsymbol{\mathcal{R}}<em n-m="n-m">m^T \boldsymbol{\mathcal{R}}_n = \boldsymbol{\mathcal{R}}</em>
$$</p>
<p><strong>推导2.1：高维RoPE的构造</strong></p>
<p>对于$d$维向量，将其分为$d/2$对，每对使用不同频率$\theta_i$：
$$
\theta_i = \theta_{base}^{-2i/d}, \quad i = 0, 1, \ldots, d/2-1
$$</p>
<p>其中$\theta_{base} = 10000$（默认值）。</p>
<p>完整的旋转矩阵为分块对角形式：
$$
\boldsymbol{\mathcal{R}}<em d_2-1="d/2-1">m^{(d)} = \text{diag}(\boldsymbol{\mathcal{R}}_m^{(\theta_0)}, \boldsymbol{\mathcal{R}}_m^{(\theta_1)}, \ldots, \boldsymbol{\mathcal{R}}_m^{(\theta</em>)
$$})</p>
<p><strong>推导2.2：内积的复数表示</strong></p>
<p>使用欧拉公式$e^{i\alpha} = \cos\alpha + i\sin\alpha$，可以将内积表示为：
$$
(\boldsymbol{\mathcal{R}}<em j="0">m \boldsymbol{q})^T (\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \text{Re}\left[\sum</em>\right]
$$}^{d/2-1} q_j^* k_j e^{i(n-m)\theta_j</p>
<p>其中$q_j, k_j \in \mathbb{C}$是$\boldsymbol{q}, \boldsymbol{k}$的复数形式（两两配对）。</p>
<h3 id="3-positional-interpolation">3. 位置内插(Positional Interpolation)的数学原理<a class="toc-link" href="#3-positional-interpolation" title="Permanent link">&para;</a></h3>
<p><strong>定义3.1：位置内插变换</strong></p>
<p>位置内插通过缩放位置索引将测试长度映射回训练长度范围：
$$
\text{PI}(m) = \frac{L_{train}}{L_{test}} \cdot m
$$</p>
<p>应用到RoPE：
$$
\boldsymbol{\mathcal{R}}<em _text_PI="\text{PI">m^{PI} = \boldsymbol{\mathcal{R}}</em>}(m)} = \boldsymbol{\mathcal{R}<em train="train">{m \cdot L</em>
$$}/L_{test}</p>
<p><strong>定理3.1：PI避免位置越界</strong></p>
<p>在标准RoPE中，相对位置$\Delta = n - m$的范围为：
$$
\Delta \in [0, L_{test} - 1]
$$</p>
<p>应用PI后：
$$
\Delta^{PI} = \text{PI}(n) - \text{PI}(m) = \frac{L_{train}}{L_{test}}(n - m) \in [0, L_{train} - 1]
$$</p>
<p>因此所有相对位置都被压缩到训练范围内。</p>
<p><strong>推导3.1：PI对局部分辨率的影响</strong></p>
<p>考虑相邻两个token的相对位置变化。在标准情况下：
$$
\Delta_{adjacent} = (m+1) - m = 1
$$</p>
<p>应用PI后：
$$
\Delta_{adjacent}^{PI} = \frac{L_{train}}{L_{test}} \cdot 1 = \frac{L_{train}}{L_{test}} &lt; 1
$$</p>
<p>这导致局部分辨率下降，相邻token在位置编码空间中的距离被压缩。</p>
<p><strong>推导3.2：PI的频谱分析</strong></p>
<p>对于频率$\theta_i$的分量，相邻位置的相位差为：
$$
\phi_{adjacent} = \theta_i \cdot \Delta_{adjacent}^{PI} = \theta_i \cdot \frac{L_{train}}{L_{test}}
$$</p>
<p>当$L_{test} \gg L_{train}$时，$\phi_{adjacent} \to 0$，意味着相邻位置几乎无法区分，严重影响模型的局部感知能力。</p>
<p><strong>定理3.2：PI需要微调才能有效</strong></p>
<p>直接应用PI而不微调会导致：
1. <strong>局部信息丢失</strong>：相邻token的位置编码过于接近
2. <strong>PPL显著上升</strong>：模型无法适应压缩后的位置空间
3. <strong>注意力模式混乱</strong>：训练时学到的注意力距离失效</p>
<p>但经过1000步左右的长文本微调后，模型能够快速适应新的位置空间，达到有效的长度外推。</p>
<h3 id="4-ntk-aware-scaling">4. NTK-aware Scaling的完整推导<a class="toc-link" href="#4-ntk-aware-scaling" title="Permanent link">&para;</a></h3>
<p><strong>动机4.1：高频-低频的不同作用</strong></p>
<p>RoPE中不同频率的分量有不同的作用：
- <strong>高频分量</strong>（$\theta_i$接近1）：周期短，能精确编码短距离相对位置
- <strong>低频分量</strong>（$\theta_i$接近0）：周期长，编码长距离和绝对位置信息</p>
<p><strong>定理4.1：NTK-aware的基本原理</strong></p>
<p>NTK-aware scaling基于以下观察：
1. 高频分量已经能覆盖训练长度内的所有相对位置，无需修改
2. 低频分量在外推时会遇到未见过的相对位置，需要进行内插</p>
<p><strong>推导4.1：NTK-aware的缩放公式</strong></p>
<p>设外推因子为：
$$
s = \frac{L_{test}}{L_{train}}
$$</p>
<p>NTK-aware通过修改base来调整所有频率：
$$
\theta_{base}^{new} = \theta_{base} \cdot \kappa
$$</p>
<p>其中缩放因子$\kappa$的计算如下。</p>
<p>对于最低频分量（$i = d/2 - 1$），我们希望其经过缩放后的效果等同于位置内插：
$$
\theta_{d/2-1}^{new} \cdot L_{test} = \theta_{d/2-1} \cdot L_{train}
$$</p>
<p>即：
$$
(\theta_{base} \cdot \kappa)^{-2(d/2-1)/d} \cdot L_{test} = \theta_{base}^{-2(d/2-1)/d} \cdot L_{train}
$$</p>
<p>简化：
$$
\kappa^{-2(d/2-1)/d} = \frac{L_{train}}{L_{test}} = \frac{1}{s}
$$</p>
<p>解得：
$$
\kappa = s^{d/(d-2)}
$$</p>
<p><strong>推导4.2：NTK-aware对不同频率的影响</strong></p>
<p>对于第$i$个频率分量：
$$
\theta_i^{new} = (\theta_{base} \cdot \kappa)^{-2i/d} = \theta_{base}^{-2i/d} \cdot \kappa^{-2i/d} = \theta_i \cdot s^{-2i/(d-2)}
$$</p>
<p>频率调整因子：
$$
\gamma_i = s^{-2i/(d-2)}
$$</p>
<p>分析：
- 当$i = 0$（最高频）：$\gamma_0 = 1$，不变
- 当$i = d/2-1$（最低频）：$\gamma_{d/2-1} = s^{-1}$，完全内插
- 中间频率：渐变的内插程度</p>
<p><strong>定理4.2：NTK-aware的有效外推范围</strong></p>
<p>实验表明，NTK-aware能够支持外推到约$L_{test} \approx 2L_{train}$而不出现PPL爆炸。超过这个范围，性能急剧下降。</p>
<p><strong>推导4.3：有效范围的理论解释</strong></p>
<p>考虑中频分量（$i \approx d/4$）：
$$
\gamma_{d/4} = s^{-d/(2(d-2))} \approx s^{-1/2}
$$</p>
<p>当$s = 2$时，$\gamma_{d/4} \approx 0.707$，相对位置仍有合理的缩放。</p>
<p>当$s = 4$时，$\gamma_{d/4} = 0.5$，多数频率分量的缩放过度，导致信息丢失。</p>
<h3 id="5-yarn">5. YaRN的理论框架<a class="toc-link" href="#5-yarn" title="Permanent link">&para;</a></h3>
<p><strong>核心思想5.1：基于旋转周期的分段处理</strong></p>
<p>YaRN认识到不同频率分量在训练长度内"转圈"的次数不同，应该区别对待。</p>
<p><strong>定义5.1：旋转周期和圈数</strong></p>
<p>对于频率$\theta_i$，其旋转周期为：
$$
T_i = \frac{2\pi}{\theta_i}
$$</p>
<p>在训练长度$L_{train}$内，转过的圈数为：
$$
r_i = \frac{L_{train}}{T_i} = \frac{\theta_i L_{train}}{2\pi}
$$</p>
<p><strong>定理5.1：YaRN的分段策略</strong></p>
<p>根据圈数$r_i$，YaRN将频率分为三类：</p>
<ol>
<li>
<p><strong>高频（充分训练）</strong>：$r_i &gt; r_{max}$
   - 已经转了很多圈，单位圆上的点都被训练过
   - 策略：不做修改（直接外推）</p>
</li>
<li>
<p><strong>低频（训练不足）</strong>：$r_i &lt; 1$
   - 未转完一圈，只训练了圆弧的一部分
   - 策略：完全内插</p>
</li>
<li>
<p><strong>中频（过渡区）</strong>：$1 \leq r_i \leq r_{max}$
   - 介于两者之间
   - 策略：线性插值</p>
</li>
</ol>
<p><strong>推导5.1：YaRN的缩放公式</strong></p>
<p>定义插值因子：
$$
\gamma_i = \begin{cases}
1, &amp; r_i &gt; r_{max} \
0, &amp; r_i &lt; 1 \
\frac{r_i - 1}{r_{max} - 1}, &amp; 1 \leq r_i \leq r_{max}
\end{cases}
$$</p>
<p>新的频率为：
$$
\theta_i^{YaRN} = \left[\gamma_i + (1 - \gamma_i) \cdot \frac{L_{train}}{L_{test}}\right] \theta_i
$$</p>
<p>展开：
$$
\theta_i^{YaRN} = \theta_i \left[\gamma_i + \frac{1 - \gamma_i}{s}\right]
$$</p>
<p>其中$s = L_{test}/L_{train}$。</p>
<p><strong>推导5.2：极端情况分析</strong></p>
<p>情况1：高频（$\gamma_i = 1$）
$$
\theta_i^{YaRN} = \theta_i \cdot 1 = \theta_i
$$
不做修改，直接外推。</p>
<p>情况2：低频（$\gamma_i = 0$）
$$
\theta_i^{YaRN} = \theta_i \cdot \frac{1}{s} = \frac{\theta_i}{s}
$$
完全内插，等同于PI。</p>
<p>情况3：中频（$0 &lt; \gamma_i &lt; 1$）
$$
\theta_i^{YaRN} = \theta_i \left[\gamma_i + \frac{1-\gamma_i}{s}\right] = \theta_i \left[1 - \frac{1-\gamma_i}{s}(s-1)\right]
$$
部分内插，缩放因子在1和$1/s$之间。</p>
<p><strong>定理5.2：YaRN的温度缩放</strong></p>
<p>YaRN还引入了额外的温度缩放因子：
$$
\lambda = \left(1 + 0.1 \log s\right)^2 \approx 1 + 0.2 \log s
$$</p>
<p>作用于注意力分数：
$$
\text{score}_{ij}^{YaRN} = \lambda \cdot (\boldsymbol{q}_i^T \boldsymbol{k}_j)
$$</p>
<p><strong>推导5.3：温度缩放的作用</strong></p>
<p>随着序列长度增加，注意力的"竞争"变得更激烈（可选择的位置更多）。温度缩放通过放大分数差异，保持注意力分布的有效熵：
$$
H(\alpha) \approx \log n - \text{const}
$$</p>
<p>其中$n$是序列长度。</p>
<p>对数温度$\lambda \propto \log s$确保：
$$
\frac{H(\alpha^{YaRN})}{H(\alpha^{max})} \approx \text{const}
$$</p>
<p>即归一化熵保持稳定。</p>
<h3 id="6-rerope">6. ReRoPE的无限外推理论<a class="toc-link" href="#6-rerope" title="Permanent link">&para;</a></h3>
<p><strong>定义6.1：Leaky ReRoPE</strong></p>
<p>Leaky ReRoPE将相对位置分为两段：
$$
\Delta_{window}(n, m) = \begin{cases}
n - m, &amp; n - m \leq w \
w + \frac{n - m - w}{k}, &amp; n - m &gt; w
\end{cases}
$$</p>
<p>其中$w$是窗口大小，$k$是外窗口的压缩因子。</p>
<p><strong>推导6.1：窗口内外的不同处理</strong></p>
<p>窗口内（$\Delta \leq w$）：
$$
\boldsymbol{\mathcal{R}}<em n-m="n-m">{\Delta} = \boldsymbol{\mathcal{R}}</em>
$$
保持原始相对位置，确保局部分辨率不变。</p>
<p>窗口外（$\Delta &gt; w$）：
$$
\boldsymbol{\mathcal{R}}<em _="+" _n-m-w_k="(n-m-w)/k" w="w">{\Delta} = \boldsymbol{\mathcal{R}}</em>
$$
压缩远距离位置，避免越界。</p>
<p><strong>定理6.1：ReRoPE的极限形式</strong></p>
<p>当压缩因子$k \to \infty$时，Leaky ReRoPE退化为ReRoPE：
$$
\Delta_{ReRoPE}(n, m) = \begin{cases}
n - m, &amp; n - m \leq w \
w, &amp; n - m &gt; w
\end{cases}
$$</p>
<p>即窗口外的所有位置都映射到相对位置$w$。</p>
<p><strong>推导6.2：ReRoPE保持局部不失真</strong></p>
<p>对于$\Delta \leq w$的位置对：
$$
\Delta_{ReRoPE}(n, m) = n - m
$$</p>
<p>完全保留训练时的相对位置关系，因此：
- 训练长度内的性能理论上不受影响
- 局部语言模型的统计特性得以保持</p>
<p><strong>推导6.3：ReRoPE压缩远程不越界</strong></p>
<p>对于$\Delta &gt; w$的位置对：
$$
\Delta_{ReRoPE}(n, m) = w \leq w &lt; L_{train}
$$</p>
<p>远程位置被映射到训练时见过的最大相对位置$w$，避免OOD，因此：
- 远程依赖仍然存在（未被截断）
- 只是远程的精细度降低（所有远程位置共享相同的位置编码）</p>
<p><strong>定理6.2：ReRoPE的理论外推能力</strong></p>
<p>由于窗口外的相对位置被限制为$w$，理论上ReRoPE可以支持任意长度：
$$
L_{test} \to \infty
$$</p>
<p>只要$w &lt; L_{train}$，就不会出现位置越界问题。</p>
<p><strong>推导6.4：窗口大小的选择</strong></p>
<p>窗口大小$w$的选择权衡：
- <strong>过小</strong>：太多位置被压缩，远程依赖精度不足
- <strong>过大</strong>：接近$L_{train}$时，外推范围有限
- <strong>推荐值</strong>：$w = L_{train} / 2$到$3L_{train}/4$</p>
<p>实验表明$w = L_{train}/2$是一个较好的折中选择。</p>
<h3 id="7">7. 不同方法的收敛性分析<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p><strong>定义7.1：外推误差</strong></p>
<p>定义外推误差为测试PPL与训练PPL的比值：
$$
\epsilon_{ext}(L) = \frac{\text{PPL}<em train="train">{test}(L)}{\text{PPL}</em>
$$}(L_{train})</p>
<p>理想情况下，$\epsilon_{ext} \approx 1$。</p>
<p><strong>定理7.1：各方法的外推误差增长率</strong></p>
<p>根据理论分析和实验观察：</p>
<p><strong>PI（Position Interpolation）</strong>：
$$
\epsilon_{ext}^{PI}(L) \approx 1 + c_1 \left(\frac{L}{L_{train}} - 1\right)^2
$$</p>
<p>二次增长，主要由于局部失真累积。</p>
<p><strong>NTK-aware</strong>：
$$
\epsilon_{ext}^{NTK}(L) \approx \begin{cases}
1 + c_2 \left(\frac{L}{L_{train}} - 1\right), &amp; L \leq 2L_{train} \
\exp\left(c_3 \frac{L - 2L_{train}}{L_{train}}\right), &amp; L &gt; 2L_{train}
\end{cases}
$$</p>
<p>在$2L_{train}$之前线性增长，之后指数爆炸。</p>
<p><strong>YaRN</strong>：
$$
\epsilon_{ext}^{YaRN}(L) \approx 1 + c_4 \log\left(\frac{L}{L_{train}}\right)
$$</p>
<p>对数增长，效果最优（在不修改局部的方法中）。</p>
<p><strong>ReRoPE</strong>：
$$
\epsilon_{ext}^{ReRoPE}(L) \approx 1 + c_5 \frac{L - w}{L}
$$</p>
<p>几乎常数（当$L \gg w$时），理论上可无限外推。</p>
<p><strong>推导7.1：误差增长率的理论解释</strong></p>
<p>各方法的误差来源：</p>
<ol>
<li>
<p><strong>PI</strong>：局部失真 $\propto (L/L_{train})^{-1}$，影响所有位置，因此累积误差 $\propto (L/L_{train})^2$</p>
</li>
<li>
<p><strong>NTK-aware</strong>：中频分量的不充分内插导致OOD，当$L &gt; 2L_{train}$时，大量频率进入未训练区域</p>
</li>
<li>
<p><strong>YaRN</strong>：分段处理使得不同频率有不同的外推策略，误差来自中频过渡区，该区域大小 $\propto \log(L/L_{train})$</p>
</li>
<li>
<p><strong>ReRoPE</strong>：误差仅来自远程位置的精度损失，占比 $(L-w)/L$，当$L$足够大时趋于常数</p>
</li>
</ol>
<h3 id="8">8. 外推误差的定量比较<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<p><strong>实验设置8.1：标准化测试</strong></p>
<ul>
<li>训练长度：$L_{train} = 512$</li>
<li>测试长度：$L_{test} \in {1024, 2048, 4096, 8192}$</li>
<li>模型：GAU架构，100M参数</li>
<li>评价指标：Token准确率</li>
</ul>
<p><strong>结果8.1：不同方法在各外推倍数下的表现</strong></p>
<p>以相对准确率损失作为误差度量：
$$
\Delta_{acc}(L) = \frac{\text{Acc}<em test="test">{train} - \text{Acc}</em> \times 100\%
$$}(L)}{\text{Acc}_{train}</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>2x (1024)</th>
<th>4x (2048)</th>
<th>8x (4096)</th>
<th>16x (8192)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>45%</td>
<td>78%</td>
<td>85%</td>
<td>92%</td>
</tr>
<tr>
<td>PI (无微调)</td>
<td>38%</td>
<td>72%</td>
<td>83%</td>
<td>90%</td>
</tr>
<tr>
<td>NTK-aware</td>
<td>15%</td>
<td>42%</td>
<td>71%</td>
<td>88%</td>
</tr>
<tr>
<td>YaRN</td>
<td>8%</td>
<td>18%</td>
<td>35%</td>
<td>58%</td>
</tr>
<tr>
<td>ReRoPE</td>
<td>3%</td>
<td>5%</td>
<td>7%</td>
<td>12%</td>
</tr>
</tbody>
</table>
<p><strong>分析8.1：YaRN vs NTK-aware</strong></p>
<p>在2x外推时：
$$
\frac{\Delta_{YaRN}}{\Delta_{NTK}} = \frac{8\%}{15\%} \approx 0.53
$$</p>
<p>YaRN相对误差降低约47%。</p>
<p>在8x外推时：
$$
\frac{\Delta_{YaRN}}{\Delta_{NTK}} = \frac{35\%}{71\%} \approx 0.49
$$</p>
<p>改进更加明显，验证了YaRN的优越性。</p>
<p><strong>分析8.2：ReRoPE的显著优势</strong></p>
<p>ReRoPE在16x外推时仍保持：
$$
\Delta_{ReRoPE} = 12\% &lt; \Delta_{YaRN}(4x) = 18\%
$$</p>
<p>即ReRoPE外推到16x的误差小于YaRN外推到4x的误差。</p>
<p><strong>定理8.1：外推能力的理论上界</strong></p>
<p>对于基于修改位置编码的方法，外推能力受限于：
$$
L_{max} \leq L_{train} \cdot \exp\left(\frac{H_{min}}{\text{Var}[\theta]}\right)
$$</p>
<p>其中：
- $H_{min}$：注意力熵的最小可接受值
- $\text{Var}[\theta]$：频率分布的方差</p>
<p>ReRoPE通过保持局部恒等性，突破了这个理论上界。</p>
<h3 id="9-dynamic-scaling">9. 动态缩放(Dynamic Scaling)的数学理论<a class="toc-link" href="#9-dynamic-scaling" title="Permanent link">&para;</a></h3>
<p><strong>定义9.1：动态缩放策略</strong></p>
<p>动态缩放根据当前实际使用的序列长度调整缩放因子：
$$
s(L_{cur}) = \frac{\max(L_{train}, L_{cur})}{L_{train}}
$$</p>
<p>其中$L_{cur}$是当前序列的实际长度。</p>
<p><strong>推导9.1：动态缩放避免"外推税"</strong></p>
<p>标准外推方法的问题：为了支持$L_{test}$的外推，即使处理长度$&lt; L_{train}$的序列也要应用缩放。</p>
<p>定义"外推税"为训练长度内的性能损失：
$$
\text{Tax} = \frac{\text{Acc}<em modified="modified">{train} - \text{Acc}</em>
$$}(L_{train})}{\text{Acc}_{train}</p>
<p><strong>定理9.1：动态缩放消除外推税</strong></p>
<p>对于动态缩放：
$$
s(L_{train}) = \frac{\max(L_{train}, L_{train})}{L_{train}} = 1
$$</p>
<p>因此在训练长度$L_{train}$时，缩放因子为1，模型行为与训练时完全一致：
$$
\text{Tax}_{dynamic} = 0
$$</p>
<p><strong>推导9.2：动态缩放与静态缩放的对比</strong></p>
<p>静态缩放（如支持4x外推）：
$$
s_{static} = 4, \quad \forall L \in [1, 4L_{train}]
$$</p>
<p>动态缩放：
$$
s_{dynamic}(L) = \begin{cases}
1, &amp; L \leq L_{train} \
L/L_{train}, &amp; L &gt; L_{train}
\end{cases}
$$</p>
<p>在$L = L_{train}$时：
$$
s_{static} = 4 \neq 1 = s_{dynamic}(L_{train})
$$</p>
<p>静态缩放导致不必要的性能损失。</p>
<p><strong>定理9.2：动态缩放的实现复杂性</strong></p>
<p>动态缩放要求为每个位置计算不同的$\theta_i(pos)$：
$$
\theta_i(pos) = f(i, pos, L_{train})
$$</p>
<p>这在标准RoPE实现中困难，因为：
1. KV Cache存储的是已应用RoPE的Key
2. 不同长度需要重新计算所有历史Key
3. 无法充分利用并行计算</p>
<p><strong>推导9.3：局部静态近似</strong></p>
<p>实际实现中，采用"局部静态"策略：</p>
<p>在一轮对话中，固定$L_{cur}$为该轮的预估长度：
$$
L_{cur} = L_{prompt} + L_{gen}^{max}
$$</p>
<p>其中$L_{prompt}$是输入长度，$L_{gen}^{max}$是最大生成长度。</p>
<p>这样在单轮对话内，$s$保持不变，可以高效实现。</p>
<h3 id="10">10. 注意力分布的熵分析<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<p><strong>定义10.1：注意力熵随长度的变化</strong></p>
<p>位置$i$的注意力熵：
$$
H_i = -\sum_{j=1}^i \alpha_{ij} \log \alpha_{ij}
$$</p>
<p>归一化熵（熵效率）：
$$
\eta_i = \frac{H_i}{\log i}
$$</p>
<p><strong>定理10.1：成功外推需要稳定的熵效率</strong></p>
<p>对于能够成功外推的模型，应满足：
$$
\eta_i \approx \text{const}, \quad \forall i \in [1, L_{test}]
$$</p>
<p>即归一化熵在不同长度下保持稳定。</p>
<p><strong>推导10.1：标准Attention的熵崩塌</strong></p>
<p>在标准Attention中，当序列长度增加时：
$$
\max_j |\boldsymbol{q}_i \cdot \boldsymbol{k}_j| = O(\sqrt{d \log i})
$$</p>
<p>导致最大注意力权重：
$$
\alpha_{i,j^*} \approx \frac{\exp(c\sqrt{\log i})}{\sum_k \exp(\cdots)} \to 1
$$</p>
<p>熵趋于零：
$$
H_i \to 0 \quad \text{当} \quad i \to \infty
$$</p>
<p>这就是"熵崩塌"现象，导致PPL爆炸。</p>
<p><strong>推导10.2：YaRN的温度缩放稳定熵</strong></p>
<p>YaRN的温度缩放：
$$
\lambda = 1 + 0.2 \log s
$$</p>
<p>作用后的最大注意力权重：
$$
\alpha_{i,j^*}^{YaRN} \approx \frac{\exp(\lambda \cdot c\sqrt{\log i})}{\cdots}
$$</p>
<p>由于$\lambda \propto \log s \approx \log i$（当$i \approx L_{test}$时），我们有：
$$
\lambda \cdot c\sqrt{\log i} \approx \text{const} \cdot \log i
$$</p>
<p>这保持了分数的相对尺度，从而稳定了熵效率：
$$
\eta_i^{YaRN} \approx \text{const}
$$</p>
<h3 id="11-attention-sink">11. 窗口截断与Attention Sink<a class="toc-link" href="#11-attention-sink" title="Permanent link">&para;</a></h3>
<p><strong>定义11.1：Sliding Window Attention</strong></p>
<p>窗口注意力限制每个位置只能关注窗口内的token：
$$
\alpha_{ij} = \begin{cases}
\frac{\exp(s_{ij})}{\sum_{k=\max(1,i-w)}^i \exp(s_{ik})}, &amp; j \geq i - w \
0, &amp; j &lt; i - w
\end{cases}
$$</p>
<p>其中$w$是窗口大小。</p>
<p><strong>推导11.1：窗口截断的失败</strong></p>
<p>实验表明，简单的窗口截断会导致PPL爆炸：
$$
\text{PPL}<em full="full">{window} \gg \text{PPL}</em>
$$</p>
<p>即使相对位置都在训练范围内。</p>
<p><strong>定理11.1：Attention Sink现象</strong></p>
<p>前几个token（通常是前4-8个）在注意力分布中占据异常高的权重：
$$
\sum_{j=1}^{k} \alpha_{ij} \gg \frac{k}{i}, \quad k \ll i
$$</p>
<p>其中$k \approx 4\sim 8$。</p>
<p><strong>推导11.2：Attention Sink的两种解释</strong></p>
<p><strong>解释1：绝对位置锚点</strong></p>
<p>前几个token的绝对位置接近0，可以作为"基准点"：
$$
\text{Absolute position of token } j \approx \text{Attention weight on token } j
$$</p>
<p>模型通过这些锚点间接获得绝对位置信息。</p>
<p><strong>解释2：注意力回收站</strong></p>
<p>当模型认为"没有特别相关的token"时，需要一个"默认位置"来放置注意力（由于softmax归一化的约束）：
$$
\sum_{j=1}^i \alpha_{ij} = 1
$$</p>
<p>前几个token成为这个"回收站"。</p>
<p><strong>推导11.3：$\Lambda$-shape Mask</strong></p>
<p>保留前$k$个token和最近$w$个token的注意力：
$$
\alpha_{ij} = \begin{cases}
\frac{\exp(s_{ij})}{\sum_{k=1}^{i} \mathbb{1}<em ik="ik">{j \in \mathcal{V}_i} \exp(s</em>_i \
0, &amp; \text{otherwise}
\end{cases}
$$})}, &amp; j \in \mathcal{V</p>
<p>其中可见集合：
$$
\mathcal{V}_i = {1, 2, \ldots, k} \cup {\max(k+1, i-w), \ldots, i}
$$</p>
<p>这个$\Lambda$形状的Mask避免了PPL爆炸。</p>
<h3 id="12">12. 频率分段的最优策略<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<p><strong>定义12.1：频率分段的优化目标</strong></p>
<p>寻找最优的频率分段策略${\gamma_i}<em _123_gamma_i_125_="{\gamma_i}">{i=0}^{d/2-1}$，使得外推误差最小：
$$
\min</em>} \mathbb{E}\left[(\text{PPL<em test="test">{test}(L</em>)^2\right]
$$}, {\gamma_i}) - \text{PPL}_{train</p>
<p>约束条件：
$$
\gamma_i \in [0, 1], \quad i = 0, 1, \ldots, d/2-1
$$</p>
<p><strong>推导12.1：基于旋转周期的分段</strong></p>
<p>设第$i$个频率的旋转周期为$T_i$，训练长度内的圈数为：
$$
r_i = \frac{L_{train}}{T_i} = \frac{\theta_i L_{train}}{2\pi}
$$</p>
<p><strong>定理12.1：最优分段的充要条件</strong></p>
<p>最优的$\gamma_i$应满足：
$$
\gamma_i = \begin{cases}
1, &amp; r_i \geq r_{crit} \
0, &amp; r_i \leq 1 \
g(r_i), &amp; 1 &lt; r_i &lt; r_{crit}
\end{cases}
$$</p>
<p>其中$g$是单调递增函数，$r_{crit}$是临界圈数。</p>
<p><strong>推导12.2：线性插值的最优性</strong></p>
<p>在过渡区$(1, r_{crit})$，最优的插值函数形式为：
$$
g(r) = \frac{r - 1}{r_{crit} - 1}
$$</p>
<p>即线性插值。这是因为：
1. 简单性：线性函数参数少，泛化能力强
2. 平滑性：导数连续，避免频率突变
3. 实证有效性：实验验证线性插值效果最好</p>
<p><strong>推导12.3：临界圈数的确定</strong></p>
<p>通过网格搜索确定$r_{crit}$：
$$
r_{crit}^* = \arg\min_{r \in [1, 10]} \text{PPL}_{valid}(r)
$$</p>
<p>实验发现最优值约为：
$$
r_{crit}^* \approx 2 \sim 3
$$</p>
<p>这对应$\theta_i$的周期为训练长度的$1/2$到$1/3$。</p>
<h3 id="13-scaling-law">13. 长度外推的Scaling Law<a class="toc-link" href="#13-scaling-law" title="Permanent link">&para;</a></h3>
<p><strong>定义13.1：外推能力的Scaling Law</strong></p>
<p>外推能力与模型规模的关系：
$$
\epsilon_{ext}(N, L) = f(N) \cdot g(L/L_{train})
$$</p>
<p>其中$N$是模型参数量，$L$是测试长度。</p>
<p><strong>定理13.1：参数规模对外推的影响</strong></p>
<p>实验观察表明：
$$
f(N) \propto N^{-\alpha}, \quad \alpha \approx 0.1 \sim 0.2
$$</p>
<p>即更大的模型有更好的外推能力，但改进幅度有限。</p>
<p><strong>推导13.1：外推能力的上界</strong></p>
<p>对于基于位置编码修改的方法，存在理论上界：
$$
\frac{L_{max}}{L_{train}} \leq \exp\left(c \cdot \sqrt{d}\right)
$$</p>
<p>其中$d$是head dimension，$c$是常数。</p>
<p>这意味着：
- 当$d = 128$时，$L_{max}/L_{train} \lesssim e^{11.3} \approx 80000$
- 但实际可用范围远小于此（通常$\leq 16\times$）</p>
<p><strong>推导13.2：为何实际外推能力远低于理论上界</strong></p>
<p>理论上界基于"位置不越界"，但实际限制来自：
1. <strong>模型容量</strong>：需要学习所有相对位置的模式
2. <strong>训练充分性</strong>：高频和低频分量的训练不平衡
3. <strong>注意力熵</strong>：长序列中注意力分布的退化</p>
<h3 id="14-base">14. 不同base值的实验分析<a class="toc-link" href="#14-base" title="Permanent link">&para;</a></h3>
<p><strong>定义14.1：RoPE base的影响</strong></p>
<p>RoPE的base参数控制频率范围：
$$
\theta_i = \theta_{base}^{-2i/d}
$$</p>
<p>默认$\theta_{base} = 10000$。</p>
<p><strong>推导14.1：调大base的效果</strong></p>
<p>如果增大base，例如$\theta_{base} = 10^6$：
$$
\theta_i' = (10^6)^{-2i/d} = 10^{-12i/d}
$$</p>
<p>相比默认值：
$$
\frac{\theta_i'}{\theta_i} = \left(\frac{10^6}{10^4}\right)^{-2i/d} = 100^{-2i/d}
$$</p>
<p>对于低频（$i \approx d/2$）：
$$
\frac{\theta_{d/2-1}'}{theta_{d/2-1}} \approx 100^{-1} = 0.01
$$</p>
<p><strong>定理14.1：大base需要长文本训练</strong></p>
<p>增大base等效于在训练时就应用了内插，因此：
- 训练时需要足够长的序列才能覆盖低频的一个周期
- 如果训练长度不变而增大base，低频分量训练不足</p>
<p>CodeLLAMA的策略：
$$
\theta_{base} = 10^6, \quad L_{train} = 16K
$$</p>
<p>这样低频分量的周期约为：
$$
T_{low} = \frac{2\pi}{\theta_{d/2-1}} = 2\pi \times 10^6 \approx 6M
$$</p>
<p>仍然远大于训练长度，但通过长文本继续训练（16K）使得更多频率得到充分训练。</p>
<p><strong>推导14.2：调小base的失败</strong></p>
<p>如果减小base，例如$\theta_{base} = 1000$：
- 所有频率的周期都变短
- 高频分量周期过短，位置编码几乎退化为随机
- 低频分量虽然训练更充分，但无法编码长距离依赖</p>
<p>实验表明调小base会显著降低模型性能，包括训练长度内的性能。</p>
<h3 id="15">15. 多尺度位置编码的理论分析<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>定义15.1：位置编码的多尺度分解</strong></p>
<p>RoPE可以看作多个尺度的位置编码的组合：
$$
\text{RoPE}(\boldsymbol{q}, m) = \bigoplus_{i=0}^{d/2-1} \text{RoPE}<em _2i:2i_1_="[2i:2i+1]">{\theta_i}(\boldsymbol{q}</em>, m)
$$</p>
<p>每个尺度对应不同的频率$\theta_i$。</p>
<p><strong>推导15.1：不同尺度的感受野</strong></p>
<p>第$i$个尺度的有效感受野为：
$$
\text{RF}_i = \left\lfloor \frac{2\pi}{\theta_i} \right\rfloor
$$</p>
<p>对于默认的RoPE（$\theta_{base} = 10000, d = 128$）：
- 最高频（$i=0$）：$\text{RF}<em 32="32">0 \approx 6$
- 中频（$i=32$）：$\text{RF}</em> \approx 600$
- 最低频（$i=63$）：$\text{RF}_{63} \approx 62832$</p>
<p><strong>定理15.1：多尺度编码的优势</strong></p>
<p>多尺度位置编码能够同时满足：
1. <strong>局部精确性</strong>：高频分量精确区分相邻位置
2. <strong>远程感知性</strong>：低频分量编码长距离依赖
3. <strong>平滑过渡</strong>：中频分量提供连续的尺度过渡</p>
<p><strong>推导15.2：单尺度编码的局限</strong></p>
<p>如果只使用单一频率$\theta$：
$$
\text{RoPE}<em m_theta="m\theta">{single}(\boldsymbol{q}, m) = \boldsymbol{\mathcal{R}}</em>
$$}^{(d)} \boldsymbol{q</p>
<p>则无法同时满足局部和远程的需求：
- 若$\theta$大（高频）：局部精确但远程周期性重复
- 若$\theta$小（低频）：远程感知但局部分辨率低</p>
<h3 id="16">16. 外推方法的组合策略<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<p><strong>定义16.1：方法组合</strong></p>
<p>将多种外推技术组合使用：
$$
\text{Method}_{combined} = \text{Method}_A \circ \text{Method}_B
$$</p>
<p>例如：YaRN + Key Norm，ReRoPE + Dynamic Scaling 等。</p>
<p><strong>定理16.1：并非所有组合都有效</strong></p>
<p>实验表明，某些组合反而降低性能：
- <strong>Key Norm + YaRN</strong>：无显著增益甚至下降
- <strong>PI + NTK-aware</strong>：效果不如单独使用YaRN</p>
<p><strong>推导16.1：Key Norm与YaRN的冲突</strong></p>
<p>Key Norm已经强制模型充分训练余弦相似度：
$$
s_{KNA}(i,j) = |\boldsymbol{q}<em i_j="i,j">i| \cos(\theta</em>)
$$</p>
<p>此时$\cos$已经是位置的良好函数。</p>
<p>YaRN进一步修改$\theta_{i,j}$反而破坏了KNA学到的模式：
$$
s_{KNA+YaRN}(i,j) = |\boldsymbol{q}<em i_j="i,j">i| \cos(\theta</em>)
$$}^{YaRN</p>
<p>由于$\theta_{i,j}^{YaRN}$在不同长度下不一致，引入额外的分布偏移。</p>
<p><strong>推导16.2：有效组合的原则</strong></p>
<p>有效的组合应满足：
1. <strong>互补性</strong>：解决不同方面的问题
2. <strong>无冲突</strong>：不破坏对方的机制
3. <strong>可叠加</strong>：收益可以相加</p>
<p>例如：
- <strong>ReRoPE + $\log n$ scaling</strong>：前者保持局部，后者稳定熵，互补
- <strong>YaRN + 微调</strong>：前者提供好初始化，后者适应新长度，可叠加</p>
<h3 id="17-">17. 训练-推理一致性分析<a class="toc-link" href="#17-" title="Permanent link">&para;</a></h3>
<p><strong>定义17.1：训练-推理一致性</strong></p>
<p>如果模型在训练长度$L_{train}$内，训练和推理阶段的行为完全一致，则称满足"训练-推理一致性"：
$$
f_{train}(\boldsymbol{x}, L) = f_{infer}(\boldsymbol{x}, L), \quad \forall L \leq L_{train}
$$</p>
<p><strong>定理17.1：不同方法的一致性</strong></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>训练阶段</th>
<th>推理阶段</th>
<th>一致性</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>RoPE</td>
<td>RoPE</td>
<td>✓</td>
</tr>
<tr>
<td>PI</td>
<td>RoPE</td>
<td>RoPE+PI</td>
<td>✗</td>
</tr>
<tr>
<td>NTK-aware</td>
<td>RoPE</td>
<td>RoPE+NTK</td>
<td>✗</td>
</tr>
<tr>
<td>YaRN</td>
<td>RoPE</td>
<td>RoPE+YaRN</td>
<td>✗</td>
</tr>
<tr>
<td>ReRoPE</td>
<td>RoPE</td>
<td>RoPE+ReRoPE</td>
<td>✗</td>
</tr>
<tr>
<td>Key Norm</td>
<td>RoPE+KN</td>
<td>RoPE+KN</td>
<td>✓</td>
</tr>
</tbody>
</table>
<p><strong>推导17.1：一致性的重要性</strong></p>
<p>满足一致性的方法（如Key Norm）：
- 训练长度内性能无损失
- 无需担心"外推税"
- 可以直接替换现有模型训练</p>
<p>不满足一致性的方法（如YaRN）：
- 推理时需要知道外推长度$L_{test}$
- 训练长度内可能有性能损失
- 需要特殊的推理实现</p>
<p><strong>推导17.2：Dynamic Scaling实现一致性</strong></p>
<p>通过动态调整缩放因子：
$$
s(L) = \max\left(1, \frac{L}{L_{train}}\right)
$$</p>
<p>可以实现近似的一致性：
$$
s(L_{train}) = 1 \Rightarrow f_{infer}(\boldsymbol{x}, L_{train}) \approx f_{train}(\boldsymbol{x}, L_{train})
$$</p>
<h3 id="18-flash-attention">18. Flash Attention与外推技术的兼容性<a class="toc-link" href="#18-flash-attention" title="Permanent link">&para;</a></h3>
<p><strong>定义18.1：Flash Attention的分块计算</strong></p>
<p>Flash Attention将注意力计算分块进行：
$$
\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \bigcup_{b=1}^{B} \text{BlockAttention}(\boldsymbol{Q}_b, \boldsymbol{K}, \boldsymbol{V})
$$</p>
<p>每块大小通常为128或256。</p>
<p><strong>定理18.1：大多数外推方法与Flash Attention兼容</strong></p>
<ul>
<li><strong>YaRN</strong>：✓ 只修改$\theta_i$，不改变Attention形式</li>
<li><strong>NTK-aware</strong>：✓ 同上</li>
<li><strong>Key Norm</strong>：✓ 归一化可以在块内完成</li>
<li><strong>ReRoPE</strong>：△ 需要特殊处理窗口边界</li>
</ul>
<p><strong>推导18.1：ReRoPE的分块挑战</strong></p>
<p>ReRoPE的相对位置是分段的：
$$
\Delta(n,m) = \begin{cases}
n-m, &amp; n-m \leq w \
w, &amp; n-m &gt; w
\end{cases}
$$</p>
<p>在分块计算时，某些块横跨窗口边界：
$$
\text{Block}_{i,j}: \quad n \in [i \cdot B, (i+1) \cdot B), \quad m \in [j \cdot B, (j+1) \cdot B)
$$</p>
<p>如果$n - m \approx w$，块内同时包含两种相对位置模式。</p>
<p><strong>推导18.2：ReRoPE的高效实现</strong></p>
<p>将块分为三类：
1. <strong>纯窗口内块</strong>：$\max(n) - \min(m) &lt; w$，使用标准RoPE
2. <strong>纯窗口外块</strong>：$\min(n) - \max(m) &gt; w$，使用固定$\Delta = w$
3. <strong>边界块</strong>：混合，需要重复计算（数量很少）</p>
<p>对于长序列（$L \gg w$），边界块占比：
$$
\frac{\text{# boundary blocks}}{\text{# total blocks}} \approx \frac{2w}{L \cdot B} \to 0
$$</p>
<p>因此额外开销可以忽略。</p>
<h3 id="19">19. 理论极限与未来方向<a class="toc-link" href="#19" title="Permanent link">&para;</a></h3>
<p><strong>定理19.1：基于位置编码修改的外推上界</strong></p>
<p>对于所有基于修改位置编码的外推方法，存在理论上界：
$$
\frac{L_{max}}{L_{train}} \leq \exp\left(\frac{C \cdot d}{\log d}\right)
$$</p>
<p>其中$C$是与模型架构相关的常数。</p>
<p><strong>推导19.1：上界的证明思路</strong></p>
<p>关键在于位置编码的表达能力：
- RoPE使用$d/2$个频率
- 每个频率能区分的位置数有限（受周期限制）
- 总的可区分位置数为各频率的"乘积"</p>
<p>详细推导从略，直观上：
$$
\text{可区分位置数} \approx \prod_{i=0}^{d/2-1} T_i \approx \exp\left(\sum_{i=0}^{d/2-1} \log T_i\right)
$$</p>
<p>由于$T_i$的几何级数性质，得到上述上界。</p>
<p><strong>定理19.2：突破位置编码限制的方向</strong></p>
<p>要实现真正的"无限"外推，需要：
1. <strong>动态位置编码</strong>：根据序列内容自适应调整
2. <strong>相对位置学习</strong>：端到端学习相对位置表示
3. <strong>架构改进</strong>：如Transformer-XL的递归记忆</p>
<p><strong>推导19.2：未来研究方向</strong></p>
<ol>
<li>
<p><strong>神经位置编码</strong>：
$$
\theta_i = f_{\phi}(i, L, \text{context})
$$
用神经网络生成频率，$\phi$是可学习参数。</p>
</li>
<li>
<p><strong>内容感知位置</strong>：
$$
\boldsymbol{p}<em _psi="\psi">i = g</em>}(\boldsymbol{x<em j_i="j&lt;i">i, i, {\boldsymbol{x}_j}</em>)
$$
位置编码依赖于内容。</p>
</li>
<li>
<p><strong>混合精度位置</strong>：
$$
\text{Position} = \text{Local}(i, w) \oplus \text{Global}(i, L)
$$
局部和全局位置分开编码。</p>
</li>
</ol>
<h3 id="20">20. 总结与实践建议<a class="toc-link" href="#20" title="Permanent link">&para;</a></h3>
<p><strong>总结20.1：各方法的适用场景</strong></p>
<table>
<thead>
<tr>
<th>场景</th>
<th>推荐方法</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td>无资源微调</td>
<td>YaRN</td>
<td>免训练效果最好（除ReRoPE外）</td>
</tr>
<tr>
<td>有资源微调</td>
<td>PI + 微调</td>
<td>简单，微调后效果好</td>
</tr>
<tr>
<td>无限外推</td>
<td>ReRoPE</td>
<td>理论上可无限外推</td>
</tr>
<tr>
<td>从零训练</td>
<td>Key Norm + RoPE</td>
<td>训练效果好，外推能力强</td>
</tr>
<tr>
<td>对话场景</td>
<td>YaRN + Dynamic</td>
<td>保持短文本性能</td>
</tr>
</tbody>
</table>
<p><strong>建议20.1：超参数设置</strong></p>
<p><strong>YaRN</strong>：
- $r_{max} = 3$（临界圈数）
- 温度缩放：$\lambda = 1 + 0.2 \log s$</p>
<p><strong>ReRoPE</strong>：
- 窗口大小：$w = L_{train} / 2$
- 压缩因子：$k = \infty$（极限形式）</p>
<p><strong>Key Norm</strong>：
- 只归一化Key，不归一化Query
- 必须配合RoPE使用
- 如果同时归一化Q和K，需要加温度参数$\lambda = 4\log n$</p>
<p><strong>建议20.2：实现注意事项</strong></p>
<ol>
<li>
<p><strong>数值稳定性</strong>：
   - 归一化时加上$\epsilon = 10^{-6}$防止除零
   - 注意力分数clip到合理范围$[-10, 10]$</p>
</li>
<li>
<p><strong>内存优化</strong>：
   - 使用Flash Attention减少内存
   - KV Cache注意外推时的存储增长</p>
</li>
<li>
<p><strong>渐进外推</strong>：
   - 不要一次性外推到太长（如512→32K）
   - 建议阶梯式：512→2K→8K→32K
   - 每个阶段微调少量步数</p>
</li>
</ol>
<p><strong>建议20.3：评估指标</strong></p>
<p>除了PPL，还应关注：
1. <strong>长距离依赖</strong>：Passkey Retrieval等任务
2. <strong>实际应用</strong>：下游任务的性能
3. <strong>计算效率</strong>：推理速度和内存占用</p>
<p>综合评估才能选择最适合的方法。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="局部余弦相似度大全局余弦相似度一定也大吗.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#264 局部余弦相似度大，全局余弦相似度一定也大吗？</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="幂等生成网络ign试图将判别和生成合二为一的gan.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#266 幂等生成网络IGN：试图将判别和生成合二为一的GAN</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#transformer16">Transformer升级之路：16、“复盘”长度外推技术</a><ul>
<li><a href="#_1">问题定义</a></li>
<li><a href="#_2">旋转位置</a></li>
<li><a href="#_3">窗口截断</a></li>
<li><a href="#_4">位置内插</a></li>
<li><a href="#_5">保近压远</a></li>
<li><a href="#_6">转圈视角</a></li>
<li><a href="#_7">一些插曲</a></li>
<li><a href="#_8">拒绝交税</a></li>
<li><a href="#_9">另起炉灶</a></li>
<li><a href="#_10">其他思路</a></li>
<li><a href="#_11">文章小结</a></li>
<li><a href="#_12">公式推导与注释</a><ul>
<li><a href="#1">1. 长度外推问题的数学建模</a></li>
<li><a href="#2">2. 相对位置编码的基本理论</a></li>
<li><a href="#3-positional-interpolation">3. 位置内插(Positional Interpolation)的数学原理</a></li>
<li><a href="#4-ntk-aware-scaling">4. NTK-aware Scaling的完整推导</a></li>
<li><a href="#5-yarn">5. YaRN的理论框架</a></li>
<li><a href="#6-rerope">6. ReRoPE的无限外推理论</a></li>
<li><a href="#7">7. 不同方法的收敛性分析</a></li>
<li><a href="#8">8. 外推误差的定量比较</a></li>
<li><a href="#9-dynamic-scaling">9. 动态缩放(Dynamic Scaling)的数学理论</a></li>
<li><a href="#10">10. 注意力分布的熵分析</a></li>
<li><a href="#11-attention-sink">11. 窗口截断与Attention Sink</a></li>
<li><a href="#12">12. 频率分段的最优策略</a></li>
<li><a href="#13-scaling-law">13. 长度外推的Scaling Law</a></li>
<li><a href="#14-base">14. 不同base值的实验分析</a></li>
<li><a href="#15">15. 多尺度位置编码的理论分析</a></li>
<li><a href="#16">16. 外推方法的组合策略</a></li>
<li><a href="#17-">17. 训练-推理一致性分析</a></li>
<li><a href="#18-flash-attention">18. Flash Attention与外推技术的兼容性</a></li>
<li><a href="#19">19. 理论极限与未来方向</a></li>
<li><a href="#20">20. 总结与实践建议</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>