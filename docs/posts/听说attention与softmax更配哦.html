<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>听说Attention与Softmax更配哦～ | ML & Math Blog Posts</title>
    <meta name="description" content="听说Attention与Softmax更配哦～&para;
原文链接: https://spaces.ac.cn/archives/9019
发布日期: 

不知道大家留意到一个细节没有，就是当前NLP主流的预训练模式都是在一个固定长度（比如512）上进行，然后直接将预训练好的模型用于不同长度的任务中。大家似乎也没有对这种模式有过怀疑，仿佛模型可以自动泛化到不同长度是一个“理所应当”的能力。
当然...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=熵">熵</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #176 听说Attention与Softmax更配哦～
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#176</span>
                听说Attention与Softmax更配哦～
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-04-07</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=熵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 熵</span>
                </a>
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=预训练" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 预训练</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="attentionsoftmax">听说Attention与Softmax更配哦～<a class="toc-link" href="#attentionsoftmax" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9019">https://spaces.ac.cn/archives/9019</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>不知道大家留意到一个细节没有，就是当前NLP主流的预训练模式都是在一个固定长度（比如512）上进行，然后直接将预训练好的模型用于不同长度的任务中。大家似乎也没有对这种模式有过怀疑，仿佛模型可以自动泛化到不同长度是一个“理所应当”的能力。</p>
<p>当然，笔者此前同样也没有过类似的质疑，直到前几天笔者做了Base版的GAU实验后才发现GAU的长度泛化能力并不如想象中好。经过进一步分析后，笔者才明白原来这种长度泛化的能力并不是“理所当然”的......</p>
<h2 id="_1">模型回顾<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>在<a href="/archives/8934">《FLASH：可能是近来最有意思的高效Transformer设计》</a>中，我们介绍了“门控注意力单元GAU”，它是一种融合了GLU和Attention的新设计。</p>
<p>除了效果，GAU在设计上给我们带来的冲击主要有两点：一是它显示了单头注意力未必就逊色于多头注意力，这奠定了它“快”、“省”的地位；二是它是显示了注意力未必需要Softmax归一化，可以换成简单的$\text{relu}^2$除以序列长度：<br />
\begin{equation}\boldsymbol{A}=\frac{1}{n}\text{relu}^2\left(\frac{\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}}{\sqrt{s}}\right)=\frac{1}{ns}\text{relu}^2\left(\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}\right)\end{equation}<br />
这个形式导致了一个有意思的问题：如果我们预训练的时候尽量将样本整理成同一长度（比如512），那么在预训练阶段$n$几乎一直就是512，也就是说$n$相当于一个常数，如果我们将它用于其他长度（比如64、128）微调，那么这个$n$究竟要自动改为样本长度，还是保持为512呢？</p>
<p>直觉应该是等于样本长度更加自适应一些，但答案很反直觉：$n$固定为512的微调效果比$n$取样本长度的效果要明显好！这就引人深思了......</p>
<h2 id="_2">问题定位<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>如果单看GAU的预训练效果，它是优于标准Attention的，所以GAU本身的拟合能力应该是没问题的，只是$\frac{1}{n}\text{relu}^2(\cdot)$在样本长度方面的迁移能力不好。为了确认这一点，笔者也尝试了混合不同长度的样本来做GAU的预训练，发现结果会有明显的改善。</p>
<p>那么，可能是GAU的什么地方出了问题呢？其实这不难猜测，GAU的整体运算可以简写成$\boldsymbol{O}=(\boldsymbol{U}\odot\boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o$，其中$\boldsymbol{U},\boldsymbol{V},\boldsymbol{W}_o$都是token-wise的，也就是说它们根本不会受到长度变化的影响，所以问题只能是出现在$\boldsymbol{A}$中。</p>
<p>以前我们用标准的Attention时，并没有出现类似的问题，以至于我们以前都无意识地觉得这是一个“理所当然”的性质。所以，我们需要从GAU的Attention与标准Attention的差异中发现问题。前面说了，两者不同的地方有两点，其一是多头Attention变成单头Attention，但是这顶多会让效果有一定波动，而我们测出来的结果是大幅下降，所以问题就只能出现在另一点，也就是归一化方式上，即Attention的$softmax$换成$\frac{1}{n}\text{relu}^2(\cdot)$所带来的。</p>
<p>验证这个猜测很简单，笔者将GAU中Attention的归一化方式换回Softmax后重新训练一个GAU模型，然后微调测试不同长度的任务，发现其效果比$\frac{1}{n}\text{relu}^2(\cdot)$时明显要好。所以，我们得出结论：Attention还是与Softmax更配～</p>
<h2 id="_3">原因分析<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>为什么更符合直觉的、自适应长度的$n$反而表现不如固定的$n$呢？既然我们已经以往用Softmax是没有这个问题的，所以我们不妨从Softmax出发找找灵感。Softmax的操作是：<br />
\begin{equation}a_{i,j} = \frac{1}{Z_i}\exp\left(\frac{\boldsymbol{q}<em j="1">i\cdot\boldsymbol{k}_j}{\sqrt{d}}\right),\quad Z_i = \sum</em>}^n \exp\left(\frac{\boldsymbol{q}_i\cdot\boldsymbol{k}_j}{\sqrt{d}}\right)\end{equation
一个直接的问题就是：$Z_i$跟$n$的关系是怎样的呢？如果真有$Z_i=\mathcal{O}(n)$，那么理论上将$Z_i$换成$n$应该能取得相近的效果，至少不会是特别差的那种。</p>
<p>然而，我们知道注意力的重点是“注意”，它应该有能力“聚焦”到它认为比较重要的几个token上。同时，以往关于高效Transformer的一些实验结果显示，把标准Attention换成Local Attention后结果并不会明显下降，所以我们可以预计位置为$i$的Attention基本上就聚焦在$i$附近的若干token上，超出一定距离后就基本为0了。事实上，也有很多事后的可视化结果显示训练好的Attention矩阵其实是很稀疏的。</p>
<p>综合这些结果，我们可以得出，存在某个常数$k$，使得$|j-i|\geq k$时$\exp\left(\frac{\boldsymbol{q}_i\cdot\boldsymbol{k}_j}{\sqrt{d}}\right)$都相当接近于0，这样一来$Z_i$应该更接近$\mathcal{O}(k)$而不是$\mathcal{O}(n)$，这就意味着$Z_i$很可能跟$n$是无关的，或者说跟$n$的数量级关系至少是小于$\mathcal{O}(n)$的！因此，我们如果要将$Z_i$替换成别的东西，那应该是一个比$n$的一次方更低阶的函数，甚至是一个常数。</p>
<p>现在回看GAU，它的激活函数换成了$\text{relu}^2(\cdot)$时，其Attention情况是类似的，甚至会更稀疏。这是因为$\text{relu}$操作有直接置零的作用，不像$\exp(\cdot)$总是正的，同时GAU“标配”旋转位置编码RoPE，在<a href="/archives/8265">《Transformer升级之路：2、博采众长的旋转式位置编码》</a>中我们就推导过，RoPE本身自带一定的远程衰减的能力。综合这些条件，GAU的归一化因子也应该是低于$\mathcal{O}(n)$的阶甚至是常数级别的。</p>
<h2 id="_4">熵不变性<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>由此，我们可以总结出GAU的三个解决方案，一是预训练和微调都用同一个固定的$n$；二是依然使用动态的样本长度$n$，但是预训练时需要用不同长度的样本来混合训练，不能只使用单一长度的样本；三就是像Softmax那样补充上一个归一化因子，让模型自己去学：<br />
\begin{equation}a_{i,j} = \frac{1}{Z_i}\text{relu}^2\left(\frac{\boldsymbol{q}<em i="1">i\cdot\boldsymbol{k}_j}{\sqrt{d}}\right),\quad Z_i = \sum</em>}^n \text{relu}^2\left(\frac{\boldsymbol{q}_i\cdot\boldsymbol{k}_j}{\sqrt{d}}\right)\end{equation</p>
<p>既然存在这些解决方案，那为什么我们还说“Attention与Softmax更配”呢？GAU的$\text{relu}^2(\cdot)$哪里不够配呢？首先，我们看GAU原论文的消融实验，显示出$\text{relu}^2(\cdot)$换成Softmax，效果基本是一致的：  </p>
<p><a href="/usr/uploads/2022/04/3734029708.png" title="点击查看原图"><img alt="GAU的squared_relu换成softmax效果是相近的" src="/usr/uploads/2022/04/3734029708.png" /></a></p>
<p>GAU的squared_relu换成softmax效果是相近的</p>
<p>有了这个基本保证之后，我们就可以看Softmax比$\text{relu}^2(\cdot)$好在哪里了。我们看刚才提到的GAU三个解决方案，方案一总让人感觉不够自适应，方案二必须用多种长度训练显得不够优雅，至于方案三补充了归一化因子后形式上相比Softmax反而显得“臃肿”了。所以，总体来说还是用Softmax显得更为优雅有效。</p>
<p>此外，泛化能力可以简单分为“内插”和“外推”两种，在这里内插（外推）指的是测试长度小于（大于）训练长度。我们刚才说归一化因子是常数量级，更多是在内插范围内说的。对于外推来说，如果长度足够长，$\boldsymbol{q}<em 512="512">i,\boldsymbol{k}_j$都“挤”在一起，所以很难保持距离超过某个范围就很接近于0的特性。而如果我们用Softmax的话，就是它可以推导出一个“熵不变性”的版本，来增强模型的外推能力：<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\log</em>} n}{\sqrt{d}}QK^{\top}\right)V\end{equation
在<a href="/archives/8823">《从熵不变性看Attention的Scale操作》</a>中我们做过简单的对比实验，显示该版本确实能提高模型在超出训练长度外的效果。</p>
<p>那么，$\text{relu}^2(\cdot)$能否推一个“熵不变性”的版本呢？答案是不能，因为它相当于是通过温度参数来调节分布的熵，这要求激活函数不能是具备正齐次性，比如对于幂函数有$(\lambda \boldsymbol{q}_i\cdot\boldsymbol{k}_j)^n=\lambda^n (\boldsymbol{q}_i\cdot\boldsymbol{k}_j)^n$，归一化后$\lambda^n$就抵消了，不起作用。激活函数最好比幂函数高一阶，才比较好实现这个调控，而比幂函数高阶的函数，最常见就是指数函数了，而指数归一化正好就是Softmax。</p>
<h2 id="_5">本文小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文分析了GAU在微调效果不佳的原因，发现Attention的归一化因子应该是接近常数量级的，所以GAU用$n$或者$n^2$做归一化因子会表现不佳。总的来说，笔者认为Attention还是跟Softmax更配，它是一个不错的基准，并且还可以通过“熵不变性”的拓展来进一步增强外推能力。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9019">https://spaces.ac.cn/archives/9019</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 07, 2022). 《听说Attention与Softmax更配哦～ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9019">https://spaces.ac.cn/archives/9019</a></p>
<p>@online{kexuefm-9019,<br />
title={听说Attention与Softmax更配哦～},<br />
author={苏剑林},<br />
year={2022},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/9019}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<h3 id="softmax">一、Softmax函数的数学基础<a class="toc-link" href="#softmax" title="Permanent link">&para;</a></h3>
<h4 id="11-softmax">1.1 Softmax函数定义与性质<a class="toc-link" href="#11-softmax" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>: 给定向量 $\boldsymbol{x} = (x_1, x_2, \ldots, x_n) \in \mathbb{R}^n$，Softmax函数定义为：
\begin{equation}
\text{softmax}(\boldsymbol{x})<em j="1">i = \frac{\exp(x_i)}{\sum</em>
\tag{1}
\end{equation}}^n \exp(x_j)} = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}</p>
<p><strong>性质1（概率分布）</strong>: Softmax输出满足概率分布的两个基本条件：
\begin{equation}
\text{softmax}(\boldsymbol{x})<em i="1">i \in (0, 1), \quad \sum</em>)_i = 1
\tag{2}
\end{equation}}^n \text{softmax}(\boldsymbol{x</p>
<p><strong>证明</strong>:
- 非负性显然成立：$\exp(x_i) &gt; 0, \forall x_i \in \mathbb{R}$
- 归一性：$\sum_{i=1}^n \frac{e^{x_i}}{\sum_j e^{x_j}} = \frac{\sum_i e^{x_i}}{\sum_j e^{x_j}} = 1$</p>
<p><strong>性质2（平移不变性）</strong>: 对任意常数 $c \in \mathbb{R}$，有：
\begin{equation}
\text{softmax}(\boldsymbol{x} + c\boldsymbol{1}) = \text{softmax}(\boldsymbol{x})
\tag{3}
\end{equation}</p>
<p><strong>证明</strong>:
\begin{equation}
\text{softmax}(\boldsymbol{x} + c\boldsymbol{1})_i = \frac{e^{x_i+c}}{\sum_j e^{x_j+c}} = \frac{e^c \cdot e^{x_i}}{e^c \cdot \sum_j e^{x_j}} = \frac{e^{x_i}}{\sum_j e^{x_j}} = \text{softmax}(\boldsymbol{x})_i
\tag{4}
\end{equation}</p>
<p>这个性质对数值稳定性至关重要，实践中通常选择 $c = -\max_i x_i$ 以避免指数溢出。</p>
<p><strong>性质3（单调性）</strong>: Softmax保持输入的相对顺序，即：
\begin{equation}
x_i &gt; x_j \Rightarrow \text{softmax}(\boldsymbol{x})_i &gt; \text{softmax}(\boldsymbol{x})_j
\tag{5}
\end{equation}</p>
<p><strong>性质4（温度参数）</strong>: 引入温度参数 $\tau &gt; 0$：
\begin{equation}
\text{softmax}<em j="1">\tau(\boldsymbol{x})_i = \frac{\exp(x_i/\tau)}{\sum</em>
\tag{6}
\end{equation}}^n \exp(x_j/\tau)</p>
<p>当 $\tau \to 0$ 时，Softmax退化为one-hot编码（选择最大值）；当 $\tau \to \infty$ 时，Softmax趋向均匀分布。</p>
<h4 id="12-softmax">1.2 Softmax梯度的完整推导<a class="toc-link" href="#12-softmax" title="Permanent link">&para;</a></h4>
<p><strong>定理</strong>: Softmax函数的Jacobian矩阵为：
\begin{equation}
\frac{\partial \text{softmax}(\boldsymbol{x})<em ij="ij">i}{\partial x_j} = \text{softmax}(\boldsymbol{x})_i \cdot (\delta</em>)} - \text{softmax}(\boldsymbol{x<em ij="ij">j)
\tag{7}
\end{equation}
其中 $\delta</em>$ 是Kronecker delta函数。</p>
<p><strong>详细证明</strong>:</p>
<p>设 $s_i = \text{softmax}(\boldsymbol{x})<em k="1">i = \frac{e^{x_i}}{Z}$，其中 $Z = \sum</em>$}^n e^{x_k</p>
<p><strong>情况1</strong>: 当 $i = j$ 时：
\begin{align}
\frac{\partial s_i}{\partial x_i} &amp;= \frac{\partial}{\partial x_i}\left(\frac{e^{x_i}}{Z}\right) \tag{8}\
&amp;= \frac{e^{x_i} \cdot Z - e^{x_i} \cdot \frac{\partial Z}{\partial x_i}}{Z^2} \tag{9}\
&amp;= \frac{e^{x_i} \cdot Z - e^{x_i} \cdot e^{x_i}}{Z^2} \tag{10}\
&amp;= \frac{e^{x_i}}{Z} \cdot \frac{Z - e^{x_i}}{Z} \tag{11}\
&amp;= s_i \cdot (1 - s_i) \tag{12}
\end{align}</p>
<p><strong>情况2</strong>: 当 $i \neq j$ 时：
\begin{align}
\frac{\partial s_i}{\partial x_j} &amp;= \frac{\partial}{\partial x_j}\left(\frac{e^{x_i}}{Z}\right) \tag{13}\
&amp;= \frac{0 \cdot Z - e^{x_i} \cdot \frac{\partial Z}{\partial x_j}}{Z^2} \tag{14}\
&amp;= \frac{-e^{x_i} \cdot e^{x_j}}{Z^2} \tag{15}\
&amp;= -\frac{e^{x_i}}{Z} \cdot \frac{e^{x_j}}{Z} \tag{16}\
&amp;= -s_i \cdot s_j \tag{17}
\end{align}</p>
<p>综合两种情况，得到：
\begin{equation}
\frac{\partial s_i}{\partial x_j} = s_i \cdot (\delta_{ij} - s_j)
\tag{18}
\end{equation}</p>
<p><strong>矩阵形式</strong>: 令 $\boldsymbol{s} = \text{softmax}(\boldsymbol{x})$，Jacobian矩阵为：
\begin{equation}
\boldsymbol{J} = \text{diag}(\boldsymbol{s}) - \boldsymbol{s}\boldsymbol{s}^{\top}
\tag{19}
\end{equation}</p>
<p>这是一个对称矩阵，且半正定（所有特征值非负）。</p>
<h4 id="13">1.3 反向传播中的梯度计算<a class="toc-link" href="#13" title="Permanent link">&para;</a></h4>
<p>在神经网络中，设损失函数为 $\mathcal{L}$，则通过链式法则：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial x_j} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial s_i} \cdot \frac{\partial s_i}{\partial x_j} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial s_i} \cdot s_i \cdot (\delta_{ij} - s_j)
\tag{20}
\end{equation}</p>
<p>展开得：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial x_j} = \frac{\partial \mathcal{L}}{\partial s_j} \cdot s_j - s_j \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial s_i} \cdot s_i
\tag{21}
\end{equation}</p>
<p><strong>特殊情况（交叉熵损失）</strong>: 当 $\mathcal{L} = -\log s_y$（$y$是真实类别）时：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial s_i} = -\frac{\delta_{iy}}{s_y}
\tag{22}
\end{equation}</p>
<p>代入得：
\begin{align}
\frac{\partial \mathcal{L}}{\partial x_j} &amp;= -\frac{\delta_{jy}}{s_y} \cdot s_j - s_j \sum_{i=1}^n \left(-\frac{\delta_{iy}}{s_y}\right) \cdot s_i \tag{23}\
&amp;= -\delta_{jy} + s_j \tag{24}\
&amp;= s_j - \delta_{jy} \tag{25}
\end{align}</p>
<p>这个简洁的形式是Softmax与交叉熵搭配的一个重要优势。</p>
<h3 id="attentionsoftmax_1">二、Attention机制中的Softmax分析<a class="toc-link" href="#attentionsoftmax_1" title="Permanent link">&para;</a></h3>
<h4 id="21-attention">2.1 标准Attention的数学形式<a class="toc-link" href="#21-attention" title="Permanent link">&para;</a></h4>
<p>给定Query、Key、Value矩阵 $Q, K, V \in \mathbb{R}^{n \times d}$，标准的Scaled Dot-Product Attention定义为：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}\right)V
\tag{26}
\end{equation}</p>
<p>对于第 $i$ 个位置，注意力权重为：
\begin{equation}
a_{i,j} = \frac{\exp\left(\frac{q_i \cdot k_j}{\sqrt{d}}\right)}{\sum_{j'=1}^n \exp\left(\frac{q_i \cdot k_{j'}}{\sqrt{d}}\right)}, \quad j = 1, \ldots, n
\tag{27}
\end{equation}</p>
<p>输出为：
\begin{equation}
o_i = \sum_{j=1}^n a_{i,j} v_j
\tag{28}
\end{equation}</p>
<h4 id="22">2.2 归一化因子的量级分析<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>: 归一化因子 $Z_i = \sum_{j=1}^n \exp\left(\frac{q_i \cdot k_j}{\sqrt{d}}\right)$ 与序列长度 $n$ 的关系？</p>
<p><strong>分析</strong>: 如果Attention具有"聚焦"特性（即只关注少数几个token），存在常数 $K$ 使得：
\begin{equation}
\exp\left(\frac{q_i \cdot k_j}{\sqrt{d}}\right) \approx 0, \quad \text{当 } |i-j| &gt; K
\tag{29}
\end{equation}</p>
<p>则：
\begin{equation}
Z_i \approx \sum_{j:|i-j|\leq K} \exp\left(\frac{q_i \cdot k_j}{\sqrt{d}}\right) = \mathcal{O}(K)
\tag{30}
\end{equation}</p>
<p>因此 $Z_i$ 与 $n$ <strong>无关</strong>或弱相关，这是一个关键观察。</p>
<p><strong>实证支持</strong>:
1. Local Attention的成功表明大部分权重集中在局部
2. Attention矩阵的稀疏性可视化
3. RoPE等位置编码的远程衰减特性</p>
<h4 id="23-softmax-vs">2.3 Softmax vs. 其他归一化方法<a class="toc-link" href="#23-softmax-vs" title="Permanent link">&para;</a></h4>
<p><strong>方案1</strong>: GAU使用的归一化（原文提到）：
\begin{equation}
\boldsymbol{A} = \frac{1}{n}\text{relu}^2\left(\frac{QK^{\top}}{\sqrt{d}}\right)
\tag{31}
\end{equation}</p>
<p><strong>问题分析</strong>:
- 归一化因子 $n$ 是序列长度，与位置无关
- 但实际有效的注意力范围可能远小于 $n$
- 导致长度泛化能力差</p>
<p><strong>方案2</strong>: 动态归一化：
\begin{equation}
a_{i,j} = \frac{\text{relu}^2\left(\frac{q_i \cdot k_j}{\sqrt{d}}\right)}{\sum_{j'=1}^n \text{relu}^2\left(\frac{q_i \cdot k_{j'}}{同\sqrt{d}}\right)}
\tag{32}
\end{equation}</p>
<p>这与Softmax在形式上类似，但：
- $\text{relu}^2$ 具有正齐次性：$\text{relu}^2(\lambda x) = \lambda^2 \text{relu}^2(x)$
- 无法通过温度参数灵活调控</p>
<p><strong>方案3</strong>: Softmax（标准方案）：
\begin{equation}
a_{i,j} = \frac{\exp\left(\frac{q_i \cdot k_j}{\sqrt{d}}\right)}{\sum_{j'=1}^n \exp\left(\frac{q_i \cdot k_{j'}}{\sqrt{d}}\right)}
\tag{33}
\end{equation}</p>
<p>优势：
- 归一化因子自适应于有效注意力范围
- 支持温度参数调控
- 与位置编码配合良好</p>
<h3 id="_7">三、熵不变性理论<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 信息熵的定义<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>给定离散概率分布 $\boldsymbol{p} = (p_1, \ldots, p_n)$，Shannon熵定义为：
\begin{equation}
H(\boldsymbol{p}) = -\sum_{i=1}^n p_i \log p_i
\tag{34}
\end{equation}</p>
<p>对于Softmax输出的分布：
\begin{equation}
H(\text{softmax}(\boldsymbol{x})) = -\sum_{i=1}^n \frac{e^{x_i}}{\sum_j e^{x_j}} \cdot \log\frac{e^{x_i}}{\sum_j e^{x_j}}
\tag{35}
\end{equation}</p>
<p>化简：
\begin{align}
H &amp;= -\sum_{i=1}^n \frac{e^{x_i}}{Z} \cdot \left(x_i - \log Z\right) \tag{36}\
&amp;= -\frac{1}{Z}\sum_{i=1}^n e^{x_i} x_i + \log Z \cdot \frac{1}{Z}\sum_{i=1}^n e^{x_i} \tag{37}\
&amp;= \log Z - \frac{1}{Z}\sum_{i=1}^n e^{x_i} x_i \tag{38}
\end{align}</p>
<p>其中 $Z = \sum_j e^{x_j}$ 是配分函数。</p>
<h4 id="32">3.2 温度参数对熵的影响<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>引入温度参数 $\tau$：
\begin{equation}
H_\tau = \log Z_\tau - \frac{1}{\tau Z_\tau}\sum_{i=1}^n e^{x_i/\tau} x_i
\tag{39}
\end{equation}</p>
<p>其中 $Z_\tau = \sum_j e^{x_j/\tau}$。</p>
<p><strong>定理（熵的单调性）</strong>:
\begin{equation}
\frac{\partial H_\tau}{\partial \tau} &gt; 0
\tag{40}
\end{equation}</p>
<p>即温度越高，熵越大（分布越平滑）。</p>
<p><strong>极限情况</strong>:
- $\tau \to 0$: $H \to 0$（one-hot分布，熵最小）
- $\tau \to \infty$: $H \to \log n$（均匀分布，熵最大）</p>
<h4 id="33">3.3 长度外推的熵不变性方案<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>: 训练长度为 $n_0$，测试长度为 $n$，如何保持熵的一致性？</p>
<p><strong>方案</strong>: 调整温度参数使得期望熵保持不变：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{\log_{n_0} n}{\sqrt{d}} QK^{\top}\right)V
\tag{41}
\end{equation}</p>
<p><strong>理论分析</strong>: 假设注意力分数 $s_{ij} = \frac{q_i \cdot k_j}{\sqrt{d}}$ 的分布与长度无关，则：</p>
<p>当序列长度从 $n_0$ 变为 $n$ 时，有效的非零项数量从 $\mathcal{O}(K)$ 可能增加。为保持熵的量级，我们需要：
\begin{equation}
H_n \approx H_{n_0}
\tag{42}
\end{equation}</p>
<p>通过调整温度 $\tau = \frac{1}{\log_{n_0} n}$，可以部分补偿长度变化带来的熵变化。</p>
<p><strong>数值验证</strong>: 设 $n_0 = 512, n = 2048$，则：
\begin{equation}
\tau = \frac{1}{\log_{512} 2048} = \frac{1}{\frac{\log 2048}{\log 512}} = \frac{\log 512}{\log 2048} = \frac{9\log 2}{11\log 2} \approx 0.818
\tag{43}
\end{equation}</p>
<p>即对于4倍长度，温度缩放到约0.82倍。</p>
<h3 id="_8">四、数值稳定性分析<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<h4 id="41-softmax">4.1 Softmax的数值稳定实现<a class="toc-link" href="#41-softmax" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>: 直接计算 $e^{x_i}$ 可能导致：
- 上溢：$x_i$ 很大时，$e^{x_i} &gt; \text{FLOAT_MAX}$
- 下溢：$x_i$ 很小时，$e^{x_i} \approx 0$</p>
<p><strong>解决方案</strong>: 利用平移不变性 $\eqref{eq:3}$，减去最大值：
\begin{equation}
\tilde{x}_i = x_i - \max_j x_j
\tag{44}
\end{equation}</p>
<p>则：
\begin{equation}
\text{softmax}(\boldsymbol{x})_i = \frac{e^{\tilde{x}_i}}{\sum_j e^{\tilde{x}_j}}
\tag{45}
\end{equation}</p>
<p><strong>数值性质</strong>:
- $\tilde{x}_i \leq 0, \forall i$，避免上溢
- $\max_i \tilde{x}_i = 0$，至少有一项为 $e^0 = 1$，避免全部下溢</p>
<p><strong>伪代码</strong>:</p>
<div class="highlight"><pre><span></span><code>def stable_softmax(x):
    x_max = max(x)
    x_shifted = x - x_max
    exp_x = exp(x_shifted)
    return exp_x / sum(exp_x)
</code></pre></div>

<h4 id="42-log-softmax">4.2 Log-Softmax的稳定计算<a class="toc-link" href="#42-log-softmax" title="Permanent link">&para;</a></h4>
<p>在计算交叉熵损失时，需要 $\log(\text{softmax}(\boldsymbol{x})_i)$：
\begin{align}
\log(\text{softmax}(\boldsymbol{x})_i) &amp;= \log\frac{e^{x_i}}{\sum_j e^{x_j}} \tag{46}\
&amp;= x_i - \log\sum_j e^{x_j} \tag{47}
\end{align}</p>
<p><strong>Log-Sum-Exp技巧</strong>:
\begin{align}
\log\sum_j e^{x_j} &amp;= \log\left(e^{x_{\max}}\sum_j e^{x_j - x_{\max}}\right) \tag{48}\
&amp;= x_{\max} + \log\sum_j e^{x_j - x_{\max}} \tag{49}
\end{align}</p>
<p>其中 $x_{\max} = \max_j x_j$。</p>
<p><strong>完整的稳定实现</strong>:
\begin{equation}
\log(\text{softmax}(\boldsymbol{x})<em _max="\max">i) = (x_i - x</em>
\tag{50}
\end{equation}}) - \log\sum_j e^{x_j - x_{\max}</p>
<h4 id="43">4.3 梯度的数值稳定性<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>Softmax的梯度计算 $\eqref{eq:18}$ 本身是数值稳定的，因为：
- $s_i \in (0, 1)$，不会溢出
- $(1 - s_i) \in (0, 1)$，同样稳定</p>
<p>但在反向传播中，需要注意：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial x_j} = s_j - \delta_{jy}
\tag{51}
\end{equation}</p>
<p>这个形式非常稳定，因为：
- $s_j \in (0, 1)$
- $\delta_{jy} \in {0, 1}$
- 差值在 $(-1, 1)$ 范围内</p>
<h3 id="_9">五、复杂度分析<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<h4 id="51-attention">5.1 标准Attention的复杂度<a class="toc-link" href="#51-attention" title="Permanent link">&para;</a></h4>
<p><strong>时间复杂度</strong>:
\begin{equation}
\mathcal{O}(n^2 d + n^2 + nd) = \mathcal{O}(n^2 d)
\tag{52}
\end{equation}</p>
<p>详细分解：
- 计算 $QK^{\top}$: $\mathcal{O}(n^2 d)$
- 计算Softmax: $\mathcal{O}(n^2)$
- 计算注意力输出: $\mathcal{O}(n^2 d)$</p>
<p><strong>空间复杂度</strong>:
\begin{equation}
\mathcal{O}(n^2 + nd) = \mathcal{O}(n^2)
\tag{53}
\end{equation}</p>
<ul>
<li>存储注意力矩阵: $\mathcal{O}(n^2)$</li>
<li>存储 $Q, K, V$: $\mathcal{O}(nd)$</li>
</ul>
<h4 id="52">5.2 优化方案的复杂度对比<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p><strong>Flash Attention</strong>: 通过分块计算避免存储完整注意力矩阵
- 时间: $\mathcal{O}(n^2 d)$（不变）
- 空间: $\mathcal{O}(nd)$（降为线性！）</p>
<p><strong>Linear Attention</strong>: 使用核方法近似
- 时间: $\mathcal{O}(nd^2)$
- 空间: $\mathcal{O}(nd)$</p>
<p>当 $n \gg d$ 时，Linear Attention更优。</p>
<p><strong>Sparse Attention</strong>: 只计算部分注意力权重
- 时间: $\mathcal{O}(nkd)$，其中 $k \ll n$ 是每个位置关注的token数
- 空间: $\mathcal{O}(nk)$</p>
<h3 id="_10">六、激活函数对比实验<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 理论对比<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>激活函数</th>
<th>公式</th>
<th>归一化性质</th>
<th>温度调控</th>
<th>梯度性质</th>
</tr>
</thead>
<tbody>
<tr>
<td>Softmax</td>
<td>$\frac{e^{x_i}}{\sum_j e^{x_j}}$</td>
<td>自适应</td>
<td>支持</td>
<td>光滑</td>
</tr>
<tr>
<td>ReLU²</td>
<td>$\frac{\text{relu}^2(x_i)}{\sum_j \text{relu}^2(x_j)}$</td>
<td>需手动设置</td>
<td>不支持</td>
<td>非光滑</td>
</tr>
<tr>
<td>Sigmoid</td>
<td>$\sigma(x_i) = \frac{1}{1+e^{-x_i}}$</td>
<td>无（不归一）</td>
<td>支持</td>
<td>光滑但饱和</td>
</tr>
<tr>
<td>Tanh</td>
<td>$\tanh(x_i)$</td>
<td>无（不归一）</td>
<td>支持</td>
<td>光滑但饱和</td>
</tr>
</tbody>
</table>
<h4 id="62">6.2 齐次性分析<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p><strong>Softmax</strong>: 不具有齐次性
\begin{equation}
\text{softmax}(\lambda \boldsymbol{x}) \neq \lambda^k \text{softmax}(\boldsymbol{x})
\tag{54}
\end{equation}</p>
<p>这使得温度参数有效。</p>
<p><strong>ReLU²</strong>: 具有2次齐次性
\begin{equation}
\text{relu}^2(\lambda x) = \lambda^2 \text{relu}^2(x)
\tag{55}
\end{equation}</p>
<p>归一化后：
\begin{equation}
\frac{\text{relu}^2(\lambda x_i)}{\sum_j \text{relu}^2(\lambda x_j)} = \frac{\lambda^2 \text{relu}^2(x_i)}{\sum_j \lambda^2 \text{relu}^2(x_j)} = \frac{\text{relu}^2(x_i)}{\sum_j \text{relu}^2(x_j)}
\tag{56}
\end{equation}</p>
<p>温度参数被抵消，无法调控！</p>
<h4 id="63">6.3 梯度特性对比<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p><strong>Softmax梯度</strong> $\eqref{eq:18}$:
- 处处可导
- 梯度有界：$|s_i(1-s_i)| \leq 1/4$
- 自动归一化（梯度和为0）</p>
<p><strong>ReLU²梯度</strong>:
\begin{equation}
\frac{\partial \text{relu}^2(x)}{\partial x} = \begin{cases}
2x, &amp; x &gt; 0 \
0, &amp; x \leq 0
\end{cases}
\tag{57}
\end{equation}</p>
<ul>
<li>在0点不可导</li>
<li>梯度无界（可能导致梯度爆炸）</li>
<li>稀疏性（负值梯度为0）</li>
</ul>
<h3 id="_11">七、实践建议与超参数选择<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<h4 id="71-attention">7.1 标准Attention的最佳实践<a class="toc-link" href="#71-attention" title="Permanent link">&para;</a></h4>
<p><strong>1. Scale因子选择</strong>:
\begin{equation}
\alpha = \frac{1}{\sqrt{d_k}}
\tag{58}
\end{equation}</p>
<p><strong>理论依据</strong>: 假设 $q_i, k_j$ 的每个分量独立同分布，均值为0，方差为 $\sigma^2$，则：
\begin{equation}
\mathbb{E}[q_i \cdot k_j] = 0, \quad \text{Var}(q_i \cdot k_j) = d_k \sigma^2
\tag{59}
\end{equation}</p>
<p>除以 $\sqrt{d_k}$ 使方差归一化：
\begin{equation}
\text{Var}\left(\frac{q_i \cdot k_j}{\sqrt{d_k}}\right) = \sigma^2
\tag{60}
\end{equation}</p>
<p>这避免了Softmax输入方差随维度增长。</p>
<p><strong>2. 温度参数调整</strong>:</p>
<p>对于长度外推，使用：
\begin{equation}
\tau = \frac{\log n_{\text{train}}}{\log n_{\text{test}}}
\tag{61}
\end{equation}</p>
<p><strong>3. Dropout位置</strong>:</p>
<p>推荐在Attention权重上应用Dropout：
\begin{equation}
\text{Attention}(Q,K,V) = \text{Dropout}(\text{softmax}(QK^{\top}/\sqrt{d}))V
\tag{62}
\end{equation}</p>
<p>典型值：$p_{\text{dropout}} = 0.1$</p>
<h4 id="72">7.2 数值稳定性检查清单<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<ol>
<li>使用Log-Softmax计算交叉熵损失</li>
<li>梯度裁剪：<code>clip_grad_norm_(parameters, max_norm=1.0)</code></li>
<li>混合精度训练时注意Softmax精度</li>
<li>检查Attention矩阵的最大值/最小值</li>
</ol>
<h4 id="73">7.3 性能优化建议<a class="toc-link" href="#73" title="Permanent link">&para;</a></h4>
<p><strong>内存优化</strong>:
- 使用Flash Attention减少显存占用
- Gradient Checkpointing牺牲计算换显存</p>
<p><strong>计算优化</strong>:
- 使用fused kernels（Softmax与Scale合并）
- 利用硬件加速（Tensor Cores）</p>
<p><strong>多头注意力配置</strong>:
\begin{equation}
h \times d_k = d_{\text{model}}
\tag{63}
\end{equation}</p>
<p>常见配置：
- 小模型：$h=8, d_k=64, d_{\text{model}}=512$
- 大模型：$h=16, d_k=64, d_{\text{model}}=1024$</p>
<h3 id="_12">八、总结与展望<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 核心结论<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>Softmax的理论优势</strong>:
   - 自适应的归一化因子，与有效注意力范围匹配
   - 支持温度参数调控，实现熵的灵活控制
   - 数学性质优良（光滑、可导、有界）</p>
</li>
<li>
<p><strong>与其他激活函数的对比</strong>:
   - ReLU²等幂函数因齐次性无法有效调控温度
   - Softmax的指数形式是实现温度调控的最自然选择</p>
</li>
<li>
<p><strong>长度泛化的关键</strong>:
   - 归一化因子应与实际注意力范围匹配
   - 熵不变性提供了一个理论框架</p>
</li>
</ol>
<h4 id="82">8.2 开放问题<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<ol>
<li>是否存在比Softmax更优的归一化方案？</li>
<li>如何理论化地预测最优温度参数？</li>
<li>熵不变性是否是长度外推的充分条件？</li>
</ol>
<h4 id="83">8.3 扩展阅读<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<ul>
<li>Flash Attention: 高效的注意力计算</li>
<li>Linformer: 线性复杂度的近似方法</li>
<li>ALiBi: 基于偏置的位置编码方案</li>
</ul>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="为什么pre-norm的效果不如post-norm.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#175 为什么Pre Norm的效果不如Post Norm？</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="熵不变性softmax的一个快速推导.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#177 熵不变性Softmax的一个快速推导</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#attentionsoftmax">听说Attention与Softmax更配哦～</a><ul>
<li><a href="#_1">模型回顾</a></li>
<li><a href="#_2">问题定位</a></li>
<li><a href="#_3">原因分析</a></li>
<li><a href="#_4">熵不变性</a></li>
<li><a href="#_5">本文小结</a></li>
<li><a href="#_6">公式推导与注释</a><ul>
<li><a href="#softmax">一、Softmax函数的数学基础</a></li>
<li><a href="#attentionsoftmax_1">二、Attention机制中的Softmax分析</a></li>
<li><a href="#_7">三、熵不变性理论</a></li>
<li><a href="#_8">四、数值稳定性分析</a></li>
<li><a href="#_9">五、复杂度分析</a></li>
<li><a href="#_10">六、激活函数对比实验</a></li>
<li><a href="#_11">七、实践建议与超参数选择</a></li>
<li><a href="#_12">八、总结与展望</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>