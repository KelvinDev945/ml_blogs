<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>生成扩散模型漫谈（三十）：从瞬时速度到平均速度 | ML & Math Blog Posts</title>
    <meta name="description" content="生成扩散模型漫谈（三十）：从瞬时速度到平均速度&para;
原文链接: https://spaces.ac.cn/archives/10958
发布日期: 

众所周知，生成速度慢是扩散模型一直以来的痛点，而为了解决这个问题，大家可谓“八仙过海，各显神通”，提出了各式各样的解决方案，然而长久以来并没一项工作能够脱颖而出，成为标配。什么样的工作能够达到这个标准呢？在笔者看来，它至少满足几个条件：...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=微分方程">微分方程</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #323 生成扩散模型漫谈（三十）：从瞬时速度到平均速度
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#323</span>
                生成扩散模型漫谈（三十）：从瞬时速度到平均速度
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-05-26</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=微分方程" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 微分方程</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=采样" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 采样</span>
                </a>
                
                <a href="../index.html?tags=扩散" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 扩散</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">生成扩散模型漫谈（三十）：从瞬时速度到平均速度<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10958">https://spaces.ac.cn/archives/10958</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，生成速度慢是扩散模型一直以来的痛点，而为了解决这个问题，大家可谓“八仙过海，各显神通”，提出了各式各样的解决方案，然而长久以来并没一项工作能够脱颖而出，成为标配。什么样的工作能够达到这个标准呢？在笔者看来，它至少满足几个条件：</p>
<blockquote>
<p>1、数学原理清晰，能够揭示出快速生成的本质所在；</p>
<p>2、能够单目标从零训练，不需要对抗、蒸馏等额外手段；</p>
<p>3、单步生成接近SOTA，可以通过增加步数提升效果。</p>
</blockquote>
<p>根据笔者的阅读经历，几乎没有一项工作能同时满足这三个标准。然而，就在几天前，arXiv出了一篇<a href="https://papers.cool/arxiv/2505.13447">《Mean Flows for One-step Generative Modeling》</a>（简称“MeanFlow”），看上去非常有潜力。接下来，我们将以此为契机，讨论一下相关思路和进展。</p>
<h2 id="_2">现有思路<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>扩散模型的生成加速工作已经有非常多，本博客前面也简单介绍过一些。总的来说，加速思路大体上可以分为三类。</p>
<p>第一，将扩散模型转化为SDE/ODE，然后研究更高效的求解器，代表作是<a href="https://papers.cool/arxiv/2206.00927">DPM-Solver</a>及其一系列后续改进。然而，这个思路通常只能将生成的NFE（Number of Function Evaluations）降到10左右，再低就会明显降低生成质量。这是因为求解器的收敛速度通常都是正比于步长的若干次方，当NFE很小时步长就无法很小，所以收敛不够快以至于没法用。</p>
<p>第二，通过蒸馏将训练好的扩散模型转化为更少步数的生成器，由此衍生出来的工作和方案也非常多，我们此前介绍过其中的一种名为<a href="/archives/10085">SiD</a>的方案。蒸馏算是比较常规和通用的思路，但缺点也是共同的，即需要额外的训练成本，并非从零训练的方案。有些工作为了蒸馏到单步生成器，还加上了对抗训练等多重优化策略，整个方案往往过于复杂。</p>
<p>第二，基于一致性模型（Consistency Model，CM），包括我们在<a href="/archives/10633">《生成扩散模型漫谈（二十八）：分步理解一致性模型》</a>简单介绍的CM、它的连续版本<a href="https://papers.cool/arxiv/2410.11081">sCM</a>以及<a href="https://papers.cool/arxiv/2310.02279">CTM</a>等。CM是自成一派的思路，可以从零训练得到NFE很小的模型，也可以用于蒸馏，但CM的目标依赖于EMA或者stop_gradient运算，意味着它耦合了优化器动力学，这通常给人一种说不清道不明的感觉。</p>
<h2 id="_3">瞬时速度<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>到目前为止，生成NFE最小的扩散模型，基本上都是ODE，因为确定性模型往往更容易分析和求解。本文同样只关注ODE式扩散，所用框架是<a href="/archives/9497">《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》</a>介绍的ReFlow，它跟<a href="https://papers.cool/arxiv/2210.02747">Flow Matching</a>本质是相通的，但更加直观。</p>
<p>ODE式扩散，是希望学习一个ODE<br />
\begin{equation}\frac{d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t}{dt} = \boldsymbol{v}</em>}}(\boldsymbol{x}_t,t)\label{eq:ode}\end{equation
来构建一个$\boldsymbol{x}_1\to \boldsymbol{x}_0$的变换。具体来说，设$\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)$是某个容易采样的随机噪声，$\boldsymbol{x}_0\sim p_0(\boldsymbol{x}_0)$则是目标分布的真实样本，我们希望能够通过上述ODE，实现随机噪声到目标样本的变换，即随机采样一个$\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)$作为初值，求解上述ODE得到的$\boldsymbol{x}_0$就是$p_0(\boldsymbol{x}_0)$的样本。</p>
<p>如果将$t$看成时间，$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t$看成位移，那么$d\boldsymbol{x}_t/dt$就是瞬时速度，所以ODE式扩散就是瞬时速度的建模。那怎么训练$\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)$呢？ReFlow提出了一种非常直观的方法：首先构建$\boldsymbol{x}_0$与$\boldsymbol{x}_1$的任意插值方式，如最简单的线性插值$\boldsymbol{x}_t=(1-t)\boldsymbol{x}_0 + t \boldsymbol{x}_1$，那么对$t$求导得<br />
\begin{equation}\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{x}_1 - \boldsymbol{x}_0\end{equation}<br />
这是个极简单的ODE，但不符合我们的要求，因为$\boldsymbol{x}_0$是我们的目标，它不应该出现在ODE中。对此，ReFlow提出一个非常符合直觉的想法——用$\boldsymbol{v}</em>}}(\boldsymbol{x<em t_boldsymbol_x="t,\boldsymbol{x">t,t)$去逼近$\boldsymbol{x}_1 - \boldsymbol{x}_0$：<br />
\begin{equation}\mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0,\boldsymbol{x}_1}\left[\Vert\boldsymbol{v}</em>}}(\boldsymbol{x}_t,t) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\Vert^2\right]\label{eq:loss-reflow}\end{equation
这就是ReFlow的目标函数。值得指出的是：1）ReFlow理论上允许$\boldsymbol{x}_0$与$\boldsymbol{x}_1$的任意插值方式；2）ReFlow虽然直观，但理论上也是严格的，可以证明它的最优解确实是我们所求的ODE。相关细节大家请参考<a href="/archives/9497">《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》</a>以及原论文。</p>
<h2 id="_4">平均速度<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>然而，ODE仅仅是一个纯数学形式，实际求解还是需要离散化，比如最简单的欧拉格式：<br />
\begin{equation}\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t - \Delta t} = \boldsymbol{x}_t - \boldsymbol{v}</em>}}(\boldsymbol{x}_t,t) \Delta t\end{equation
从$1$到$0$的NFE是$1/\Delta t$，想要NFE小等价于$\Delta t$大。然而，ReFlow的理论基础是精确的ODE，即精确求解ODE时才能实现目标样本的生成，这意味着$\Delta t$越小越好，跟我们的期望相背。尽管ReFlow声称使用直线插值可以让ODE的轨迹变得更直，从而允许更大的$\Delta t$，但实际轨迹终究是弯曲的，$\Delta t$很难接近1，所以ReFlow很难实现一步生成。</p>
<p>归根结底，ODE本来就是$\Delta t\to 0$的东西，我们非要将它用于$\Delta t \to 1$，还要求它效果好，这本身就是“强模型所难”了。所以说，更换建模目标，而不是继续“为难”模型，才是实现更快生成的本质思路。为此，我们考虑对式$\eqref{eq:ode}$两端进行积分<br />
\begin{equation}\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t - \boldsymbol{x}_r = \int_r^t \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{\tau},\tau) d\tau = (t-r)\times \frac{1}{t-r}\int_r^t \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{\tau},\tau) d\tau\end{equation}<br />
如果我们可以建模<br />
\begin{equation} \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) \triangleq \frac{1}{t-r}\int_r^t \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{\tau},\tau) d\tau\end{equation}<br />
那么就有$\boldsymbol{x}_0 = \boldsymbol{x}_1 - \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">1, 0, 1)$，即理论上可以精准地实现一步生成，而不必求诸于近似关系。如果说$\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)$是$t$时刻的瞬时速度，那么很显然$\boldsymbol{u}</em>_t, r, t)$是$[r,t]$时间段内的平均速度。也就是说，为了加速生成甚至一步生成，我们的建模目标应该是平均速度，而不是ODE的瞬时速度。}}(\boldsymbol{x</p>
<h2 id="_5">恒等变换<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>当然，从瞬时速度到平均速度的转变并不难想，真正难的地方是如何给它构建损失函数。ReFlow只告诉我们如何给瞬时速度构建损失函数，对平均速度的训练我们是一无所知。</p>
<p>接下来很自然的想法是“化未知为已知”，即以平均速度$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)$来为出发点来构建瞬时速度$\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)$，然后代入ReFlow的目标函数，这需要我们去推导两者之间的恒等变换。从$\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)$的定义我们得到<br />
\begin{equation} \int_r^t \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{\tau},\tau) d\tau = (t-r)\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) \end{equation}<br />
两边对$t$求导，得到<br />
\begin{equation}\begin{aligned}
\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t) =&amp;\, \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\frac{d}{dt}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) \\
=&amp;\, \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right]
\end{aligned}\label{eq:id1}\end{equation}<br />
这便是$\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)$跟$\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)$的第一个恒等关系。有第一自然就有第二，第二个恒等关系由平均速度的定义得到：<br />
\begin{equation}\boldsymbol{v}</em>}}(\boldsymbol{x<em r_to="r\to" t="t">t,t) = \lim</em>}\frac{1}{t-r}\int_r^t \boldsymbol{v<em _tau="\tau">{\boldsymbol{\theta}}(\boldsymbol{x}</em>},\tau) d\tau = \boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, t)\label{eq:id2}\end{equation
说白了，无限小区间内的平均速度，就等于瞬时速度。</p>
<h2 id="_6">第一目标<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>根据$d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t/dt = \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)$以及恒等式$\eqref{eq:id2}$，我们可以将恒等式$\eqref{eq:id1}$的$d\boldsymbol{x}_t/dt$换成$\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)$或者$\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, t)$，前者是隐式关系，我们后面再谈，我们先看后者，此时有：<br />
\begin{equation}\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t) = \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, t)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right]\end{equation}<br />
代入ReFlow，我们得到可以用来训练$\boldsymbol{u}</em>}}(\boldsymbol{x<em r_t_boldsymbol_x="r,t,\boldsymbol{x">t, r, t)$的第一个目标函数：<br />
\begin{equation}\mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0,\boldsymbol{x}_1}\left[\left\Vert\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, t)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x}_t, r, t)\right] - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right\Vert^2\right]\label{eq:loss-1}\end{equation
这是一个非常理想的结果，它满足我们对生成模型目标函数的所有期望：</p>
<blockquote>
<p>1、单个显式的最小化目标；</p>
<p>2、没有EMA、stop_gradient等运算；</p>
<p>3、理论上有保证（ReFlow）。</p>
</blockquote>
<p>这些特性意味着，不管我们用什么优化算法，只要我们能找到上式的最小值点，那么它就是我们想要的平均速度模型，即理论上能够实现一步生成的生成模型。换句话说，它具备了扩散模型的训练简单和理论保证，又能像GAN那样一步生成，还不用求神拜佛保佑模型别“想不开”而训崩。</p>
<h2 id="jvp">JVP运算<a class="toc-link" href="#jvp" title="Permanent link">&para;</a></h2>
<p>不过，对于部分读者来说，目标函数$\eqref{eq:loss-1}$的实现还是有点困难的，因为它涉及到普通用户比较少见的“雅可比向量积（Jacobian-Vector Product，JVP）”。具体来说，我们可以将目标函数内方括号部分写成：<br />
\begin{equation}\underbrace{\left[\boldsymbol{u}<em _text_向量="\text{向量">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, t),0,1\right] \\[10pt]}</em>}}\cdot\underbrace{\left[\frac{\partial}{\partial \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t), \frac{\partial}{\partial r}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t), \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _text_雅可比矩阵="\text{雅可比矩阵">t, r, t)\right] \\[10pt]}</em>}}\end{equation
即$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)$的雅可比矩阵与给定向量$[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, t),0,1]$的乘法，结果是一个跟$\boldsymbol{u}</em>_t, r, t)$大小一致的向量，这种运算就叫做JVP，在Jax、Torch里边都有现成实现，比如Jax的参考代码是：}}(\boldsymbol{x</p>
<div class="highlight"><pre><span></span><code>u = lambda xt, r, t: diffusion_model(weights, [xt, r, t])
urt, durt = jax.jvp(u, (xt, r, t), (u(xt, t, t), r <span class="gs">* 0, t *</span> 0 + 1))
</code></pre></div>

<p>其中<code>urt</code>就是$\boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)$，而<code>durt</code>就是对应的JVP结果，Torch的用法也类似。了解JVP运算后，目标函数$\eqref{eq:loss-1}$的实现就基本上没有难度了。</p>
<h2 id="_7">第二目标<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>如果要说目标函数$\eqref{eq:loss-1}$的缺点，在笔者看来只有一个，那就是计算量相对偏大。这是因为它要进行两次不同的前向传播$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)$和$\boldsymbol{u}</em>_t, t, t)$，然后JVP求了一次梯度，用基于梯度下降优化时还要再求一次梯度，所以它本质上要求二阶梯度，跟以往的}}(\boldsymbol{x<a href="/archives/4439">WGAN-GP</a>类似。</p>
<p>为了降低计算量，我们可以考虑给JVP部分加上stop_gradient运算（$\newcommand{\sg}[1]{\color{skyblue}{\mathop{\text{sg}}\left[\color{blue}{#1}\right]}}\sg{\cdot}$）：<br />
\begin{equation}\mathbb{E}<em _boldsymbol_theta="\boldsymbol{\theta">{r,t,\boldsymbol{x}_0,\boldsymbol{x}_1}\left[\left\Vert\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\sg{\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, t)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x}_t, r, t)} - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right\Vert^2\right]\label{eq:loss-2}\end{equation
这样就避免了对JVP再次求梯度（但依然需要两次前向传播）。实测结果显示，相比第一目标$\eqref{eq:loss-1}$，上述目标在梯度优化器下训练速度能够快将近一倍，并且效果目测无损。</p>
<p>注意，这里的stop_gradient单纯是出于减少计算量的目的，实际优化方向依然是损失函数值越小越好，这跟CM系列模型尤其是sCM是不一样的，它们的损失函数只是具有等效梯度的等效损失，并不一定是越小越好，它们的stop_gradient往往是必须的，一旦去掉几乎可以肯定会训练崩溃。</p>
<h2 id="_8">第三目标<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>前面我们提到，处理恒等式$\eqref{eq:id1}$中的$d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t/dt$的另一个方案是将其换成$\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)$，这将导致<br />
\begin{equation}\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t) = \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right]\end{equation}<br />
如果要从中解出$\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t)$，结果将是<br />
\begin{equation}\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t,t) = \left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right]\cdot\left[\boldsymbol{I} - (t-r)\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right]^{-1}\end{equation}<br />
这涉及到了非常庞大的矩阵求逆，因此并不现实。MeanFlow给出了一个折中方案：既然$d\boldsymbol{x}_t/dt = \boldsymbol{v}</em>}}(\boldsymbol{x<em r_t_boldsymbol_x="r,t,\boldsymbol{x">t,t)$的回归目标是$\boldsymbol{x}_1-\boldsymbol{x}_0$，那干脆把$d\boldsymbol{x}_t/dt$换成$\boldsymbol{x}_1-\boldsymbol{x}_0$好了，于是目标函数变成<br />
\begin{equation}\mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0,\boldsymbol{x}_1}\left[\left\Vert\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[(\boldsymbol{x}_1-\boldsymbol{x}_0)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right] - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right\Vert^2\right]\end{equation}<br />
然而，此时的$\boldsymbol{x}_1-\boldsymbol{x}_0$既是回归目标，又出现在模型$\boldsymbol{v}</em>}}(\boldsymbol{x<em r_t_boldsymbol_x="r,t,\boldsymbol{x">t,t)$的定义中，难免会有一种“标签泄漏”的感觉。为了避免这个问题，MeanFlow采取的办法同样是给JVP部分加上stop_gradient：<br />
\begin{equation}\mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0,\boldsymbol{x}_1}\left[\left\Vert\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\sg{(\boldsymbol{x}_1-\boldsymbol{x}_0)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)} - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right\Vert^2\right]\label{eq:loss-3}\end{equation}<br />
这就是MeanFlow最终所用的损失函数，这里我们称之为“第三目标”。相比第二目标$\eqref{eq:loss-2}$，它少了一次前向传播$\boldsymbol{u}</em>_t, t, t)$，所以训练速度会更快一些。但此时“标签泄漏”的引入和stop_gradient的对策，使得第三目标的训练跟梯度优化器是耦合的，这就跟CM一样，多了一些说不清道不明的神秘感。}}(\boldsymbol{x</p>
<p>论文实验结果表明，加上$\sg{\cdot}$的目标$\eqref{eq:loss-3}$是能训出合理结果的，那如果去掉它呢？笔者向作者请教过，他表明去掉$\sg{\cdot}$后，训练依然能收敛，能多步生成，但没有一步生成能力了。其实这也不难理解，因为$r=t$时不管有没有$\sg{\cdot}$，目标函数都退化为ReFlow：<br />
\begin{equation}\mathbb{E}<em _boldsymbol_theta="\boldsymbol{\theta">{t,\boldsymbol{x}_0,\boldsymbol{x}_1}\left[\Vert\boldsymbol{u}</em>}}(\boldsymbol{x}_t, t, t) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\Vert^2\right]\label{eq:loss-reflow-2}\end{equation
也就是说MeanFlow总有ReFlow在背后“兜底”，因此怎样也不至于太差。而去掉$\sg{\cdot}$后，“标签泄漏”的负面影响加剧，因此就不如加上它了。</p>
<h2 id="_9">证明一下<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>我们能否像ReFlow一样，从理论上证明第三目标$\eqref{eq:loss-3}$的最优解确实是我们期望的平均速度模型呢？让我们尝试一下。首先我们回顾证明ReFlow的两个关键引理：</p>
<blockquote>
<p>1、$\mathop{\text{argmin}}_{\boldsymbol{\mu}}\mathbb{E}[\Vert\boldsymbol{\mu} - \boldsymbol{x}\Vert^2] = \mathbb{E}[\boldsymbol{x}]$，即最小化$\boldsymbol{\mu}$与$\boldsymbol{x}$的平方误差，最优解是$\boldsymbol{x}$的均值；</p>
<p>2、按照分布轨迹$\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t=(1-t)\boldsymbol{x}_0 + t \boldsymbol{x}_1$将$\boldsymbol{x}_1$变到$\boldsymbol{x}_0$的ODE形式解是$d\boldsymbol{x}_t/dt = \mathbb{E}</em>_0]$。}_0|\boldsymbol{x}_t}[\boldsymbol{x}_1-\boldsymbol{x</p>
</blockquote>
<p>其中，引理1的证明比较简单，直接对$\boldsymbol{\mu}$求梯度得$\mathbb{E}[\boldsymbol{\mu} - \boldsymbol{x}] = \boldsymbol{\mu} - \mathbb{E}[\boldsymbol{x}]$，令它等于零即可；引理2的证明细节则需要看<a href="/archives/9497">《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》</a>，其中$\mathbb{E}_{\boldsymbol{x}_0|\boldsymbol{x}_t}[\boldsymbol{x}_1-\boldsymbol{x}_0]$是需要先利用$\boldsymbol{x}_t=(1-t)\boldsymbol{x}_0 + t \boldsymbol{x}_1$消去$\boldsymbol{x}_1$，得到一个$\boldsymbol{x}_0,\boldsymbol{x}_t$的函数，然后对分布$p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)$求期望，结果是关于$t,\boldsymbol{x}_t$的函数。</p>
<p>利用引理1，我们可以证明ReFlow的目标函数$\eqref{eq:loss-reflow}$的理论最优解就是$\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t,t) = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0|\boldsymbol{x}_t}[\boldsymbol{x}_1-\boldsymbol{x}_0]$，结合引理2就得到$d\boldsymbol{x}_t/dt=\boldsymbol{v}</em>^</em>}(\boldsymbol{x}_t,t)$是我们所求的ODE。第三目标$\eqref{eq:loss-3}$的证明类似，由于里边有$\sg{\cdot}$，对$\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)$求梯度并让它等于零的结果是<br />
\begin{equation}\begin{aligned}
\boldsymbol{0} =&amp;\, \boldsymbol{u}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t, r, t) + \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0|\boldsymbol{x}_t}\left[(t-r)\left[(\boldsymbol{x}_1-\boldsymbol{x}_0)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right] - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right] \\
=&amp;\, \boldsymbol{u}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t, r, t) + (t-r)\left[\mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0|\boldsymbol{x}_t}[\boldsymbol{x}_1-\boldsymbol{x}_0]\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t, r, t)\right] - \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0|\boldsymbol{x}_t}[\boldsymbol{x}_1 - \boldsymbol{x}_0] \\
=&amp;\, \boldsymbol{u}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right] - \frac{d\boldsymbol{x}_t}{dt} \\
=&amp;\, \frac{d}{dt}\left[(t - r) \boldsymbol{u}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) - (\boldsymbol{x}_t - \boldsymbol{x}_r)\right] \\
\end{aligned}\end{equation}<br />
所以在适当的边界条件下就有$\boldsymbol{x}_t - \boldsymbol{x}_r = (t - r) \boldsymbol{u}</em>_t, r, t)$，即我们期望的平均速度模型。}^*}(\boldsymbol{x</p>
<p>这个过程的关键是$\sg{\cdot}$的引入避免了对JVP部分求梯度，从而简化了梯度表达式并得到了正确的结果。如果去掉$\sg{\cdot}$的话，上式右端就要多乘一项JVP部分对$\boldsymbol{u}_{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)$的雅可比矩阵，结果就是最后无法将$\frac{d}{dt}\left[(t - r) \boldsymbol{u}</em>^</em>}(\boldsymbol{x}_t, r, t) - (\boldsymbol{x}_t - \boldsymbol{x}_r)\right]$这一项分离出来，而引入$\sg{\cdot}$的数学意义便是为了解决此问题。</p>
<p>当然，笔者还是那句话，$\sg{\cdot}$的引入也使得整个模型的训练耦合了梯度优化器，多了一丝不清晰的感觉。此时梯度等于零的点，顶多算是一个驻点而非（局部）最小值点，所以稳定性也不明朗，这其实也是所有耦合$\sg{\cdot}$的模型的共性。</p>
<h2 id="_10">相关工作<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>非常有趣的是，我们之前介绍过的两篇加速生成的文章<a href="/archives/9881">《生成扩散模型漫谈（二十一）：中值定理加速ODE采样》</a>和<a href="/archives/10617">《生成扩散模型漫谈（二十七）：将步长作为条件输入》</a>，也都是以平均速度为核心的，并且思想上可以说是一脉相承的。尽管作者之间未必相互有联系，但他们的工作内容上确实给人一种承前启后的连贯感。</p>
<p>在中值定理篇，作者已经意识到了平均速度<br />
\begin{equation}\frac{1}{t-r}\int_r^t \boldsymbol{v}<em _tau="\tau">{\boldsymbol{\theta}}(\boldsymbol{x}</em>},\tau) d\tau\end{equation
的重要性，但他的做法是类比一维函数的积分中值定理，试图寻找$s\in[r,t]$使得$\boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}_s,s)$等于平均速度。这本质上还是寻找高阶Solver的思想，但不再是Training-Free，而是需要少量的蒸馏步骤，对Solver来说算是一个小突破。</p>
<p>而步长输入篇所提的Shortcut模型，则几乎已经触碰到了MeanFlow，因为步长作为额外输入，跟MeanFlow的双时间参数$r,t$实质是等价的，不同的是它是直接以平均速度的性质作为额外的正则项来训练模型。用本文的记号，平均速度应该满足的性质是<br />
\begin{equation}\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t) = \frac{1}{2}\left[\boldsymbol{u}</em>}}\left(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, s, t\right) + \boldsymbol{u}</em>}}\left(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">s, r, s\right)\right]\end{equation}<br />
其中$s = (r+t)/2$。所以Shortcut干脆以它来构建正则项<br />
\begin{equation}\left\Vert\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) - \frac{1}{2}\sg{\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, s, t) + \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">s, r, s)}\right\Vert^2\end{equation}<br />
跟ReFlow的目标$\eqref{eq:loss-reflow-2}$混合训练，实际训练中$\boldsymbol{x}_s = \boldsymbol{x}_t - (t-s)\boldsymbol{u}</em>$的引入在笔者看来主要也是为了节省计算量。Shortcut模型其实比MeanFlow更直观，但由于没有恒等变换和ReFlow带来的严格理论支撑，使得它看上去更多是一个过渡期的经验产物。}}(\boldsymbol{x}_t, s, t)$，$\sg{\cdot</p>
<h2 id="_11">一致模型<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h2>
<p>最后我们再来讨论一下一致性模型。由于CM、sCM珠玉在前，MeanFlow的成功实际上也借鉴了它们的经验，尤其是给JVP加$\sg{\cdot}$的操作，这在原论文中也有提到。当然，MeanFlow作者之一何恺明老师本身也是操控梯度的大师（比如<a href="/archives/7980">SimSiam</a>），所以MeanFlow的出现看起来是非常水到渠成的。</p>
<p>离散的CM我们在<a href="/archives/10633">《生成扩散模型漫谈（二十八）：分步理解一致性模型》</a>仔细分析过，如果将其中CM的EMA算符换成stop_gradient，求梯度并取$\Delta t\to 0$的极限，那么就得到了<a href="https://papers.cool/arxiv/2410.11081">《Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models》</a>中的sCM的目标函数：<br />
\begin{equation}\boldsymbol{f}<em _sg_boldsymbol_theta="\sg{\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\cdot \frac{d}{dt}\boldsymbol{f}</em>}}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) = \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)\cdot\sg{\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) + \frac{\partial}{\partial t}\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)}\label{eq:loss-scm}\end{equation}<br />
如果将$\frac{d\boldsymbol{x}_t}{dt}$换成$\boldsymbol{x}_1 - \boldsymbol{x}_0$，然后记$\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) = \boldsymbol{x}_t - t\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t , 0, t)$，那么它的梯度跟$r=0$时的MeanFlow第三目标$\eqref{eq:loss-3}$是等价的：<br />
\begin{equation}\begin{aligned}
\nabla</em>}}\eqref{eq:loss-scm} =&amp;\, \nabla_{\boldsymbol{\theta}}\boldsymbol{f<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\cdot \left[\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) + \frac{\partial}{\partial t}\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)\right] \\[10pt]
=&amp;\, -t\nabla</em>}}\boldsymbol{u<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, 0, t)\cdot \left[\frac{d\boldsymbol{x}_t}{dt} - t\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t) - \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t) - t\frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t)\right] \\[10pt]
=&amp;\, t\nabla</em>}}\boldsymbol{u<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, 0, t)\cdot \left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t) + t\left[\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t)\right]- \frac{d\boldsymbol{x}_t}{dt}\right] \\[10pt]
=&amp;\, \frac{t}{2}\nabla</em>}}\left\Vert\boldsymbol{u<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, 0, t) + t\sg{\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t)}- \frac{d\boldsymbol{x}_t}{dt}\right\Vert^2 \\[10pt]
\sim &amp;\, \left.\nabla</em>}}\eqref{eq:loss-3}\right|_{r=0
\end{aligned}\end{equation}</p>
<p>所以，从这个角度看，sCM是MeanFlow在$r=0$时的一个特例。正如前面所说，引入另外的时间参数$r$可以让ReFlow给MeanFlow“兜底”（$r=t$时），从而更好地避免训崩，这是它的优点之一。当然，从sCM出发其实也可以引入双时间参数，得到跟第三目标完全相同的结果，但从个人的审美来看，CM、sCM的物理意义终究不如MeanFlow平均速度的诠释直观。</p>
<p>此外，平均速度和ReFlow结合的出发点，还可以得到另外的第一目标$\eqref{eq:loss-1}$和第二目标$\eqref{eq:loss-2}$，这对于像笔者这样的stop_gradient洁癖患者来说是非常舒适和漂亮的结果。在笔者看来，从计算成本出发，我们是可以考虑给损失函数加上stop_gradient，但推导的第一性原理和基本结果不应该跟stop_gradient耦合，否则意味着它跟优化器和动力学是强耦合的，这并不是一个本质结果应有的表现。</p>
<h2 id="_12">文章小结<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h2>
<p>本文以最近出来的MeanFlow为中心，讨论了“平均速度”视角下的扩散模型加速生成思路。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10958">https://spaces.ac.cn/archives/10958</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (May. 26, 2025). 《生成扩散模型漫谈（三十）：从瞬时速度到平均速度 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10958">https://spaces.ac.cn/archives/10958</a></p>
<p>@online{kexuefm-10958,<br />
title={生成扩散模型漫谈（三十）：从瞬时速度到平均速度},<br />
author={苏剑林},<br />
year={2025},<br />
month={May},<br />
url={\url{https://spaces.ac.cn/archives/10958}},<br />
} </p>
<hr />
<h2 id="_13">公式推导与注释<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h2>
<h3 id="1">1. 核心概念与数学框架<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11-ode">1.1 ODE式扩散模型的基本设定<a class="toc-link" href="#11-ode" title="Permanent link">&para;</a></h4>
<p>在扩散模型中,我们的目标是学习一个从简单分布(如标准正态分布)到复杂数据分布的变换。ODE式扩散模型通过以下常微分方程来实现这一变换:</p>
<p>$$\frac{d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t}{dt} = \boldsymbol{v}</em>$$}}(\boldsymbol{x}_t,t) \tag{1</p>
<p><strong>符号说明:</strong>
- $\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t$: 时刻$t$的状态变量,可以理解为图像在加噪过程中的中间状态
- $t \in [0,1]$: 时间参数,通常$t=1$对应纯噪声,$t=0$对应真实数据
- $\boldsymbol{v}</em>_t,t)$: 参数化的速度场,描述状态变化的瞬时速度
- $\boldsymbol{\theta}$: 神经网络参数}}(\boldsymbol{x</p>
<p><strong>物理直觉:</strong> 将$t$看作时间,$\boldsymbol{x}_t$看作位移,则$\frac{d\boldsymbol{x}_t}{dt}$就是瞬时速度。</p>
<h4 id="12-reflow">1.2 ReFlow框架<a class="toc-link" href="#12-reflow" title="Permanent link">&para;</a></h4>
<p>ReFlow提供了一种直观的方法来训练速度场$\boldsymbol{v}_{\boldsymbol{\theta}}$。其核心思想是:</p>
<ol>
<li>
<p><strong>构建插值路径:</strong> 在数据点$\boldsymbol{x}_0$和噪声点$\boldsymbol{x}_1$之间构建插值
   $$\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1 \tag{2}$$</p>
</li>
<li>
<p><strong>计算精确速度:</strong> 对插值路径求导得到精确的速度场
   $$\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{x}_1 - \boldsymbol{x}_0 \tag{3}$$</p>
</li>
<li>
<p><strong>监督学习:</strong> 用神经网络$\boldsymbol{v}<em _text_ReFlow="\text{ReFlow">{\boldsymbol{\theta}}$拟合这个速度场
   $$\mathcal{L}</em>}} = \mathbb{E<em _boldsymbol_theta="\boldsymbol{\theta">{t,\boldsymbol{x}_0,\boldsymbol{x}_1}\left[|\boldsymbol{v}</em>$$}}(\boldsymbol{x}_t,t) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)|^2\right] \tag{4</p>
</li>
</ol>
<p><strong>理论保证:</strong> 可以证明,当损失函数最小化时,学到的ODE确实能将噪声分布$p_1$变换为数据分布$p_0$。</p>
<h3 id="2">2. 从瞬时速度到平均速度的动机<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21-ode">2.1 ODE离散化的问题<a class="toc-link" href="#21-ode" title="Permanent link">&para;</a></h4>
<p>在实际应用中,我们需要对ODE进行离散化求解。最简单的欧拉方法为:</p>
<p>$$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t-\Delta t} = \boldsymbol{x}_t - \boldsymbol{v}</em>$$}}(\boldsymbol{x}_t,t) \Delta t \tag{5</p>
<p><strong>关键矛盾:</strong>
- <strong>理论要求:</strong> ODE的精确求解需要$\Delta t \to 0$,即步长越小越好
- <strong>实际需求:</strong> 为了加速生成,我们希望$\Delta t$尽可能大,最好$\Delta t = 1$实现一步生成
- <strong>性能问题:</strong> ReFlow虽然声称直线插值使轨迹更直,但实际轨迹仍有弯曲,$\Delta t$很难接近1</p>
<h4 id="22">2.2 平均速度的定义<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>为了解决这个矛盾,我们考虑对ODE两端积分:</p>
<p>$$\int_r^t \frac{d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{\tau}}{d\tau} d\tau = \int_r^t \boldsymbol{v}</em>$$}}(\boldsymbol{x}_{\tau},\tau) d\tau \tag{6</p>
<p>左端积分为:
$$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t - \boldsymbol{x}_r = \int_r^t \boldsymbol{v}</em>$$}}(\boldsymbol{x}_{\tau},\tau) d\tau \tag{7</p>
<p><strong>改写为平均速度形式:</strong>
$$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t - \boldsymbol{x}_r = (t-r) \cdot \frac{1}{t-r}\int_r^t \boldsymbol{v}</em>$$}}(\boldsymbol{x}_{\tau},\tau) d\tau \tag{8</p>
<p><strong>定义平均速度:</strong>
$$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t) \triangleq \frac{1}{t-r}\int_r^t \boldsymbol{v}</em>$$}}(\boldsymbol{x}_{\tau},\tau) d\tau \tag{9</p>
<p><strong>核心优势:</strong> 如果能直接建模平均速度$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}$,则有:
$$\boldsymbol{x}_0 = \boldsymbol{x}_1 - \boldsymbol{u}</em>$$}}(\boldsymbol{x}_1, 0, 1) \tag{10</p>
<p>这样就可以<strong>理论上精确地</strong>实现一步生成,无需依赖离散化近似!</p>
<h3 id="3">3. 瞬时速度与平均速度的恒等变换<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 第一个恒等关系的推导<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>从平均速度的定义出发:
$$\int_r^t \boldsymbol{v}<em _tau="\tau">{\boldsymbol{\theta}}(\boldsymbol{x}</em>$$},\tau) d\tau = (t-r)\boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t) \tag{11</p>
<p><strong>两端对$t$求导</strong> (应用Leibniz积分法则):</p>
<p>左端求导:
$$\frac{d}{dt}\left[\int_r^t \boldsymbol{v}<em _tau="\tau">{\boldsymbol{\theta}}(\boldsymbol{x}</em>$$},\tau) d\tau\right] = \boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t) \tag{12</p>
<p>右端求导(应用乘积法则):
$$\frac{d}{dt}\left[(t-r)\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)\right] = \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\frac{d}{dt}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t) \tag{13</p>
<p><strong>对$\boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)$应用链式法则:</strong></p>
<p>$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}$是关于$\boldsymbol{x}_t$和$t$的函数,因此全导数为:
$$\frac{d}{dt}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) = \frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t) \tag{14</p>
<p><strong>第一个恒等关系:</strong>
结合(12)-(14),得到:
$$\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t,t) = \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t)\right] \tag{15</p>
<h4 id="32">3.2 第二个恒等关系的推导<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>从平均速度的定义,考虑极限情况$r \to t$:</p>
<p>$$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t) = \frac{1}{t-r}\int_r^t \boldsymbol{v}</em>$$}}(\boldsymbol{x}_{\tau},\tau) d\tau \tag{16</p>
<p><strong>应用L'Hôpital法则或积分中值定理:</strong></p>
<p>当$r \to t$时,分子分母都趋于0,应用L'Hôpital法则:
$$\lim_{r \to t} \boldsymbol{u}<em _to="\to" r="r" t="t">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t) = \lim</em>} \frac{\int_r^t \boldsymbol{v<em _tau="\tau">{\boldsymbol{\theta}}(\boldsymbol{x}</em>$$},\tau) d\tau}{t-r} \tag{17</p>
<p>对分子求导(对$r$):
$$\frac{\partial}{\partial r}\left[\int_r^t \boldsymbol{v}<em _tau="\tau">{\boldsymbol{\theta}}(\boldsymbol{x}</em>$$},\tau) d\tau\right] = -\boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}_r,r) \tag{18</p>
<p>对分母求导:
$$\frac{\partial}{\partial r}(t-r) = -1 \tag{19}$$</p>
<p>因此:
$$\lim_{r \to t} \boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t) = \boldsymbol{v}</em>$$}}(\boldsymbol{x}_t,t) \tag{20</p>
<p><strong>第二个恒等关系:</strong>
$$\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t,t) = \boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, t, t) \tag{21</p>
<p><strong>物理直觉:</strong> 无限小时间区间内的平均速度就等于瞬时速度。</p>
<h3 id="4-meanflow">4. MeanFlow的三个训练目标<a class="toc-link" href="#4-meanflow" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 第一目标的推导(最理想)<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p><strong>策略:</strong> 用平均速度$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}$来表达瞬时速度$\boldsymbol{v}</em>$,然后代入ReFlow目标。}</p>
<p><strong>步骤1:</strong> 将第二恒等关系(21)代入第一恒等关系(15)中的$\frac{d\boldsymbol{x}_t}{dt}$:</p>
<p>$$\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t,t) = \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, t)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t)\right] \tag{22</p>
<p><strong>步骤2:</strong> 代入ReFlow的损失函数(4):</p>
<p>$$\mathcal{L}<em r_t_boldsymbol_x="r,t,\boldsymbol{x">1 = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0,\boldsymbol{x}_1}\left[\left|\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, t)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t)\right] - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right|^2\right] \tag{23</p>
<p><strong>优点分析:</strong>
1. <strong>单一目标:</strong> 只有一个最小化目标,没有对抗或多目标优化
2. <strong>无EMA:</strong> 不需要指数移动平均,训练更稳定
3. <strong>无stop_gradient:</strong> 不依赖梯度截断技巧
4. <strong>理论保证:</strong> 继承ReFlow的理论保证</p>
<p><strong>计算要求:</strong>
- 需要两次前向传播: $\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)$和$\boldsymbol{u}</em>_t, t, t)$
- 需要计算JVP(雅可比向量积)
- 反向传播时需要二阶梯度}}(\boldsymbol{x</p>
<h4 id="42-jvp">4.2 雅可比向量积(JVP)的理解<a class="toc-link" href="#42-jvp" title="Permanent link">&para;</a></h4>
<p><strong>定义:</strong> 给定函数$f: \mathbb{R}^n \to \mathbb{R}^m$和切向量$\boldsymbol{v} \in \mathbb{R}^n$,JVP计算:</p>
<p>$$\text{JVP}(f, \boldsymbol{v}) = J_f \cdot \boldsymbol{v} = \frac{\partial f}{\partial \boldsymbol{x}} \boldsymbol{v} \tag{24}$$</p>
<p>其中$J_f \in \mathbb{R}^{m \times n}$是雅可比矩阵。</p>
<p><strong>在MeanFlow中的应用:</strong></p>
<p>函数: $f(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) = \boldsymbol{u}</em>_t, r, t)$}}(\boldsymbol{x</p>
<p>输入向量: $(\boldsymbol{x}_t, r, t)$</p>
<p>切向量: $(\boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, t), 0, 1)$</p>
<p>JVP结果:
$$\text{JVP} = \boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, t) \cdot \frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + 0 \cdot \frac{\partial}{\partial r}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + 1 \cdot \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t) \tag{25</p>
<p><strong>计算效率:</strong> JVP的计算复杂度与前向传播相同,远小于完整雅可比矩阵的计算。</p>
<h4 id="43">4.3 第二目标(减少计算量)<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p><strong>问题:</strong> 第一目标需要对JVP再次求梯度,计算量较大。</p>
<p><strong>解决方案:</strong> 对JVP部分添加stop_gradient:</p>
<p>$$\mathcal{L}<em r_t_boldsymbol_x="r,t,\boldsymbol{x">2 = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0,\boldsymbol{x}_1}\left[\left|\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\text{sg}\left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, t)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t)\right] - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right|^2\right] \tag{26</p>
<p>其中$\text{sg}[\cdot]$表示stop_gradient操作。</p>
<p><strong>梯度分析:</strong></p>
<p>对$\boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)$求梯度时:</p>
<p>不含stop_gradient的项:
$$\nabla_{\boldsymbol{\theta}} \boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t) \tag{27}$$</p>
<p>含stop_gradient的项不产生梯度:
$$\nabla_{\boldsymbol{\theta}} \text{sg}[...] = 0 \tag{28}$$</p>
<p><strong>效果:</strong>
- 训练速度提升约2倍
- 实验表明效果无明显损失
- 仍需两次前向传播</p>
<p><strong>与Consistency Model的区别:</strong>
- MeanFlow的stop_gradient是为了<strong>计算效率</strong>
- 损失函数值越小越好(真实的优化目标)
- CM的stop_gradient是为了<strong>算法正确性</strong>
- CM的损失不一定越小越好(仅保证梯度等效)</p>
<h4 id="44-meanflow">4.4 第三目标(MeanFlow论文使用)<a class="toc-link" href="#44-meanflow" title="Permanent link">&para;</a></h4>
<p><strong>动机:</strong> 进一步减少计算量,去掉第二次前向传播$\boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, t)$。</p>
<p><strong>策略:</strong> 用$\boldsymbol{x}_1 - \boldsymbol{x}_0$替换$\frac{d\boldsymbol{x}_t}{dt}$:</p>
<p>从ReFlow我们知道$\frac{d\boldsymbol{x}_t}{dt}$的回归目标就是$\boldsymbol{x}_1 - \boldsymbol{x}_0$,因此在第一恒等关系(15)中:</p>
<p>$$\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t,t) \approx \boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[(\boldsymbol{x}_1-\boldsymbol{x}_0)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t)\right] \tag{29</p>
<p><strong>标签泄漏问题:</strong> $\boldsymbol{x}_1 - \boldsymbol{x}_0$既是回归目标,又出现在模型定义中。</p>
<p><strong>解决方案:</strong> 添加stop_gradient:</p>
<p>$$\mathcal{L}<em r_t_boldsymbol_x="r,t,\boldsymbol{x">3 = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0,\boldsymbol{x}_1}\left[\left|\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\text{sg}\left[(\boldsymbol{x}_1-\boldsymbol{x}_0)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t)\right] - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right|^2\right] \tag{30</p>
<p><strong>特殊情况分析($r=t$):</strong></p>
<p>当$r=t$时,目标退化为:
$$\mathcal{L}<em r="t">3|</em>} = \mathbb{E<em _boldsymbol_theta="\boldsymbol{\theta">{t,\boldsymbol{x}_0,\boldsymbol{x}_1}\left[|\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, t, t) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)|^2\right] \tag{31</p>
<p>这正是ReFlow的目标!因此ReFlow为MeanFlow"兜底",保证训练不会太差。</p>
<p><strong>优缺点:</strong>
- 优点: 只需一次前向传播,最快
- 缺点: 与优化器耦合,有"神秘感"</p>
<h3 id="5">5. 理论证明与收敛性分析<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51-reflow">5.1 ReFlow的理论基础<a class="toc-link" href="#51-reflow" title="Permanent link">&para;</a></h4>
<p><strong>引理1(回归最优解):</strong></p>
<p>$$\mathop{\text{argmin}}_{\boldsymbol{\mu}} \mathbb{E}[|\boldsymbol{\mu} - \boldsymbol{x}|^2] = \mathbb{E}[\boldsymbol{x}] \tag{32}$$</p>
<p><strong>证明:</strong>
对$\boldsymbol{\mu}$求导:
$$\frac{\partial}{\partial \boldsymbol{\mu}} \mathbb{E}[|\boldsymbol{\mu} - \boldsymbol{x}|^2] = 2\mathbb{E}[\boldsymbol{\mu} - \boldsymbol{x}] = 2(\boldsymbol{\mu} - \mathbb{E}[\boldsymbol{x}]) \tag{33}$$</p>
<p>令导数为零:
$$\boldsymbol{\mu} = \mathbb{E}[\boldsymbol{x}] \tag{34}$$</p>
<p><strong>引理2(ODE的概率流形式):</strong></p>
<p>对于插值轨迹$\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$,将$\boldsymbol{x}_1$到$\boldsymbol{x}_0$的最优ODE为:</p>
<p>$$\frac{d\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t}{dt} = \mathbb{E}</em>$$}_0|\boldsymbol{x}_t}[\boldsymbol{x}_1 - \boldsymbol{x}_0] \tag{35</p>
<p><strong>直觉理解:</strong>
- 在$\boldsymbol{x}_t$处,下一步应该朝着期望方向移动
- 期望是对所有可能的$\boldsymbol{x}_0$(给定$\boldsymbol{x}_t$)取平均</p>
<p><strong>ReFlow的收敛性:</strong></p>
<p>最优解满足:
$$\boldsymbol{v}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\theta}^*}(\boldsymbol{x}_t, t) = \mathbb{E}</em>$$}_0|\boldsymbol{x}_t}[\boldsymbol{x}_1 - \boldsymbol{x}_0] \tag{36</p>
<p>代入引理2,得到正确的ODE。</p>
<h4 id="52">5.2 第三目标的理论证明<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p><strong>目标:</strong> 证明$\mathcal{L}_3$的最优解是期望的平均速度模型。</p>
<p><strong>步骤1:</strong> 对$\boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t)$求梯度并令其为零。</p>
<p>由于有stop_gradient,梯度为:
$$\nabla_{\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}} \mathcal{L}_3 = 2\mathbb{E}\left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_u="\boldsymbol{u">t, r, t) + (t-r)\text{sg}[...] - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\right] \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}} \boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, r, t) \tag{37</p>
<p>令梯度为零:
$$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t, r, t) + (t-r)\mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0|\boldsymbol{x}_t}\left[(\boldsymbol{x}_1-\boldsymbol{x}_0)\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>^</em>}(\boldsymbol{x}_t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>}^*}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t, r, t)\right] = \mathbb{E}</em>$$}_0|\boldsymbol{x}_t}[\boldsymbol{x}_1 - \boldsymbol{x}_0] \tag{38</p>
<p><strong>步骤2:</strong> 利用引理2,将$\mathbb{E}_{\boldsymbol{x}_0|\boldsymbol{x}_t}[\boldsymbol{x}_1 - \boldsymbol{x}_0]$替换为$\frac{d\boldsymbol{x}_t}{dt}$:</p>
<p>$$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\left[\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>^</em>}(\boldsymbol{x}_t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}^*}(\boldsymbol{x}_t, r, t)\right] = \frac{d\boldsymbol{x}_t}{dt} \tag{39</p>
<p><strong>步骤3:</strong> 识别全导数结构。</p>
<p>注意到:
$$\frac{d}{dt}\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) = \frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>^</em>}(\boldsymbol{x}_t, r, t) + \frac{\partial}{\partial t}\boldsymbol{u}</em>$$}^*}(\boldsymbol{x}_t, r, t) \tag{40</p>
<p>方程(39)可改写为:
$$\boldsymbol{u}_{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) + (t-r)\frac{d}{dt}\boldsymbol{u}</em>^</em>}(\boldsymbol{x}_t, r, t) = \frac{d\boldsymbol{x}_t}{dt} \tag{41}$$</p>
<p><strong>步骤4:</strong> 应用乘积法则。</p>
<p>注意到:
$$\frac{d}{dt}\left[(t-r)\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t)\right] = \boldsymbol{u}</em>^</em>}(\boldsymbol{x}_t, r, t) + (t-r)\frac{d}{dt}\boldsymbol{u}</em>$$}^*}(\boldsymbol{x}_t, r, t) \tag{42</p>
<p>因此方程(41)等价于:
$$\frac{d}{dt}\left[(t-r)\boldsymbol{u}_{\boldsymbol{\theta}^*}(\boldsymbol{x}_t, r, t)\right] = \frac{d\boldsymbol{x}_t}{dt} \tag{43}$$</p>
<p><strong>步骤5:</strong> 积分得到结果。</p>
<p>对方程(43)从$r$到$t$积分:
$$\int_r^t \frac{d}{d\tau}\left[(\tau-r)\boldsymbol{u}<em _tau="\tau">{\boldsymbol{\theta}^*}(\boldsymbol{x}</em>$$}, r, \tau)\right] d\tau = \int_r^t \frac{d\boldsymbol{x}_{\tau}}{d\tau} d\tau \tag{44</p>
<p>左端:
$$(t-r)\boldsymbol{u}_{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) - 0 = (t-r)\boldsymbol{u}</em>^</em>}(\boldsymbol{x}_t, r, t) \tag{45}$$</p>
<p>右端:
$$\boldsymbol{x}_t - \boldsymbol{x}_r \tag{46}$$</p>
<p><strong>最终结果:</strong>
$$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t - \boldsymbol{x}_r = (t-r)\boldsymbol{u}</em>$$}^*}(\boldsymbol{x}_t, r, t) \tag{47</p>
<p>这正是平均速度的定义!证明完毕。</p>
<p><strong>stop_gradient的必要性分析:</strong></p>
<p>如果去掉stop_gradient,求梯度时会多出JVP部分对$\boldsymbol{u}_{\boldsymbol{\theta}^*}$的雅可比矩阵,导致无法将方程化简为(43)的形式。</p>
<p><strong>数学意义:</strong> stop_gradient的引入避免了对JVP求梯度,使得最优性条件能够化简为微分方程形式。</p>
<h3 id="6">6. 与相关工作的联系<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 与中值定理方法的关系<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p><strong>中值定理方法</strong> (<a href="/archives/9881">《生成扩散模型漫谈(二十一)》</a>) 的核心思想:</p>
<p>根据积分中值定理,存在$s \in [r,t]$使得:
$$\frac{1}{t-r}\int_r^t \boldsymbol{v}<em _tau="\tau">{\boldsymbol{\theta}}(\boldsymbol{x}</em>$$},\tau) d\tau = \boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}_s, s) \tag{48</p>
<p><strong>策略:</strong> 寻找最优的$s$来近似平均速度。</p>
<p><strong>与MeanFlow的区别:</strong>
- 中值定理: 寻找特殊点$s$,仍是求解器思想
- MeanFlow: 直接建模平均速度,训练时学习</p>
<h4 id="62-shortcut">6.2 与Shortcut模型的关系<a class="toc-link" href="#62-shortcut" title="Permanent link">&para;</a></h4>
<p><strong>Shortcut模型</strong> (<a href="/archives/10617">《生成扩散模型漫谈(二十七)》</a>) 将步长作为条件输入。</p>
<p><strong>平均速度的性质:</strong> 应该满足
$$\boldsymbol{u}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, r, t) = \frac{1}{2}\left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, s, t) + \boldsymbol{u}</em>$$}}(\boldsymbol{x}_s, r, s)\right] \tag{49</p>
<p>其中$s = (r+t)/2$。</p>
<p><strong>Shortcut的正则化损失:</strong>
$$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{reg}} = \left|\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, r, t) - \frac{1}{2}\text{sg}\left[\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, s, t) + \boldsymbol{u}</em>$$}}(\boldsymbol{x}_s, r, s)\right]\right|^2 \tag{50</p>
<p>混合ReFlow目标训练。</p>
<p><strong>与MeanFlow的区别:</strong>
- Shortcut: 直观但缺乏理论保证,像经验性探索
- MeanFlow: 通过恒等变换有严格理论支撑</p>
<h4 id="63-consistency-model">6.3 与Consistency Model的关系<a class="toc-link" href="#63-consistency-model" title="Permanent link">&para;</a></h4>
<p><strong>连续一致性模型(sCM)</strong> 的目标函数:
$$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{sCM}} = \boldsymbol{f}</em>}}(\boldsymbol{x<em _text_sg="\text{sg">t, t) \cdot \frac{d}{dt}\boldsymbol{f}</em>$$}[\boldsymbol{\theta}]}(\boldsymbol{x}_t, t) \tag{51</p>
<p>其中$\boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)$是一致性函数。</p>
<p><strong>变换关系:</strong> 令
$$\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) = \boldsymbol{x}_t - t\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, 0, t) \tag{52</p>
<p><strong>梯度等价性推导:</strong></p>
<p>对$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{sCM}}$求梯度:
$$\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">{\text{sCM}} = \nabla</em>}}\boldsymbol{f<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \left[\frac{d\boldsymbol{x}_t}{dt}\cdot\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) + \frac{\partial}{\partial t}\boldsymbol{f}</em>$$}}(\boldsymbol{x}_t, t)\right] \tag{53</p>
<p>代入(52):
$$\nabla_{\boldsymbol{\theta}}\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) = -t\nabla</em>$$}}\boldsymbol{u}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, 0, t) \tag{54</p>
<p>$$\frac{\partial}{\partial \boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t}\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) = \boldsymbol{I} - t\frac{\partial}{\partial \boldsymbol{x}_t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, 0, t) \tag{55</p>
<p>$$\frac{\partial}{\partial t}\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) = -\boldsymbol{u}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, 0, t) - t\frac{\partial}{\partial t}\boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, 0, t) \tag{56</p>
<p>经过代数化简(详细过程略),可以证明:
$$\nabla_{\boldsymbol{\theta}}\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{sCM}} \propto \nabla</em>}}\mathcal{L<em r="0">3|</em>$$} \tag{57</p>
<p><strong>结论:</strong> sCM是MeanFlow在$r=0$时的特例。</p>
<p><strong>MeanFlow的优势:</strong>
1. 引入$r$参数,当$r=t$时退化为ReFlow,提供"兜底"保证
2. 物理意义更清晰(平均速度 vs 一致性)
3. 可以得到不依赖stop_gradient的第一、第二目标</p>
<h3 id="7">7. 实现细节与技术要点<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 采样算法<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p><strong>单步生成:</strong>
$$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">0 = \boldsymbol{x}_1 - \boldsymbol{u}</em>$$}}(\boldsymbol{x}_1, 0, 1) \tag{58</p>
<p><strong>多步生成</strong> (NFE=N):</p>
<p>将$[0,1]$划分为$0=t_0 &lt; t_1 &lt; \cdots &lt; t_N = 1$</p>
<p>从$\boldsymbol{x}<em i-1="i-1">N \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$开始:
$$\boldsymbol{x}</em>} = \boldsymbol{x<em i-1="i-1">i - (t_i - t</em>})\boldsymbol{u<em i-1="i-1">{\boldsymbol{\theta}}(\boldsymbol{x}_i, t</em>$$}, t_i), \quad i=N,N-1,\ldots,1 \tag{59</p>
<p><strong>理论优势:</strong> 多步时是对ODE积分的更精确近似,可以提升质量。</p>
<h4 id="72">7.2 训练算法伪代码<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p><strong>输入:</strong> 数据集${\boldsymbol{x}<em i="1">0^{(i)}}</em>^M$</p>
<p><strong>输出:</strong> 训练好的$\boldsymbol{u}_{\boldsymbol{\theta}}$</p>
<div class="highlight"><pre><span></span><code><span class="k">for</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">max_epochs</span><span class="p">:</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">data_loader</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># 采样数据点</span>
<span class="w">        </span><span class="n">x0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch</span><span class="w">  </span><span class="c1"># 真实数据</span>

<span class="w">        </span><span class="c1"># 采样噪声</span>
<span class="w">        </span><span class="n">x1</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">N</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="p">)</span>

<span class="w">        </span><span class="c1"># 采样时间参数</span>
<span class="w">        </span><span class="n">t</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">        </span><span class="n">r</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">)</span><span class="w">  </span><span class="c1"># 或固定 r=0</span>

<span class="w">        </span><span class="c1"># 构建插值</span>
<span class="w">        </span><span class="n">xt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">t</span><span class="p">)</span><span class="o">*</span><span class="n">x0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">t</span><span class="o">*</span><span class="n">x1</span>

<span class="w">        </span><span class="c1"># 目标速度</span>
<span class="w">        </span><span class="n">target</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x0</span>

<span class="w">        </span><span class="c1"># 计算预测(根据选择的目标)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">using_loss_1</span><span class="p">:</span>
<span class="w">            </span><span class="n">u_rt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u_theta</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">)</span>
<span class="w">            </span><span class="n">u_tt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u_theta</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">)</span>
<span class="w">            </span><span class="n">jvp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compute_jvp</span><span class="p">(</span><span class="n">u_rt</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="n">u_tt</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="w">            </span><span class="n">pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u_rt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="n">r</span><span class="p">)</span><span class="o">*</span><span class="n">jvp</span>
<span class="w">        </span><span class="k">elif</span><span class="w"> </span><span class="n">using_loss_2</span><span class="p">:</span>
<span class="w">            </span><span class="n">u_rt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u_theta</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">)</span>
<span class="w">            </span><span class="n">u_tt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u_theta</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">)</span>
<span class="w">            </span><span class="n">jvp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compute_jvp</span><span class="p">(</span><span class="n">u_rt</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="n">u_tt</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="w">            </span><span class="n">pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u_rt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="n">r</span><span class="p">)</span><span class="o">*</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">jvp</span><span class="p">)</span>
<span class="w">        </span><span class="k">elif</span><span class="w"> </span><span class="n">using_loss_3</span><span class="p">:</span>
<span class="w">            </span><span class="n">u_rt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u_theta</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">)</span>
<span class="w">            </span><span class="n">jvp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compute_jvp</span><span class="p">(</span><span class="n">u_rt</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">xt</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="n">target</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="w">            </span><span class="n">pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u_rt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="n">r</span><span class="p">)</span><span class="o">*</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">jvp</span><span class="p">)</span>

<span class="w">        </span><span class="c1"># 计算损失</span>
<span class="w">        </span><span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">||</span><span class="n">pred</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">target</span><span class="o">||^</span><span class="mi">2</span>

<span class="w">        </span><span class="c1"># 反向传播</span>
<span class="w">        </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></pre></div>

<h4 id="73-jvp">7.3 JVP的实现<a class="toc-link" href="#73-jvp" title="Permanent link">&para;</a></h4>
<p><strong>PyTorch示例:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_jvp</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
    <span class="c1"># func: 要计算JVP的函数</span>
    <span class="c1"># inputs: 输入点 (xt, r, t)</span>
    <span class="c1"># tangents: 切向量 (v_xt, v_r, v_t)</span>

    <span class="n">outputs</span><span class="p">,</span> <span class="n">jvp_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">jvp</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">tangents</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">jvp_out</span>
</code></pre></div>

<p><strong>JAX示例:</strong></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_jvp</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
    <span class="n">primals_out</span><span class="p">,</span> <span class="n">tangents_out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jvp</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">tangents</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">primals_out</span><span class="p">,</span> <span class="n">tangents_out</span>
</code></pre></div>

<h3 id="8">8. 理论深度分析<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 为什么平均速度更适合大步长?<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p><strong>瞬时速度的局限性:</strong></p>
<p>ODE $\frac{d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t}{dt} = \boldsymbol{v}</em>_t, t)$ 描述的是}}(\boldsymbol{x<strong>局部</strong>性质,只在$\Delta t \to 0$时精确。</p>
<p>欧拉法离散化:
$$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t-\Delta t} \approx \boldsymbol{x}_t - \boldsymbol{v}</em>$$}}(\boldsymbol{x}_t, t)\Delta t \tag{60</p>
<p><strong>局部截断误差:</strong> $O(\Delta t^2)$</p>
<p><strong>全局累积误差:</strong> $O(\Delta t)$</p>
<p>当$\Delta t$大时,误差不可接受。</p>
<p><strong>平均速度的优势:</strong></p>
<p>平均速度直接建模<strong>全局</strong>性质:
$$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t-\Delta t} = \boldsymbol{x}_t - \Delta t \cdot \boldsymbol{u}</em>$$}}(\boldsymbol{x}_t, t-\Delta t, t) \tag{61</p>
<p>理论上,当模型完美学习时,这是<strong>精确</strong>的,无近似误差!</p>
<p><strong>类比:</strong>
- 瞬时速度: 用切线近似曲线
- 平均速度: 直接学习曲线的弦</p>
<h4 id="82">8.2 双时间参数的意义<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p><strong>为什么需要$r$和$t$两个参数?</strong></p>
<ol>
<li><strong>灵活性:</strong> 可以建模任意时间区间$[r,t]$的平均速度</li>
<li><strong>ReFlow兜底:</strong> 当$r=t$时,退化为瞬时速度,即ReFlow</li>
<li><strong>渐进学习:</strong> 可以先学小区间,再泛化到大区间</li>
</ol>
<p><strong>训练策略:</strong></p>
<p>可以设计课程学习:
- 早期训练: $r$和$t$接近,学习局部性质
- 后期训练: $r$和$t$相差大,学习全局性质</p>
<h4 id="83">8.3 理论局限性分析<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p><strong>stop_gradient的理论地位:</strong></p>
<p>第三目标中的stop_gradient:
- 数学上: 改变了优化目标的性质
- 优化上: 使最优点变为驻点而非极小值点
- 实践上: 依赖优化器动力学</p>
<p><strong>开放问题:</strong>
1. 能否找到不依赖stop_gradient的全局性建模方法?
2. stop_gradient的稳定性如何理论保证?
3. 不同优化器对结果的影响?</p>
<h3 id="9">9. 实验观察与实践建议<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 三个目标的对比<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p><strong>第一目标($\mathcal{L}_1$):</strong>
- 优点: 理论最清晰,无stop_gradient
- 缺点: 计算量最大(二阶梯度)
- 适用: 理论研究,小规模实验</p>
<p><strong>第二目标($\mathcal{L}_2$):</strong>
- 优点: 平衡理论与效率
- 缺点: 仍需两次前向传播
- 适用: 高质量生成任务</p>
<p><strong>第三目标($\mathcal{L}_3$):</strong>
- 优点: 最快,工程实践友好
- 缺点: 理论上有"神秘感"
- 适用: 大规模生产应用</p>
<h4 id="92">9.2 超参数设置建议<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p><strong>时间采样:</strong>
- $t \sim \text{Uniform}(0, 1)$: 简单有效
- $t \sim \text{Beta}(\alpha, \beta)$: 可以调整难度分布</p>
<p><strong>$r$的选择:</strong>
- $r = 0$: 类似sCM,理论保证强
- $r \sim \text{Uniform}(0, t)$: 更灵活,学习全局</p>
<p><strong>学习率:</strong>
- 建议使用warmup
- AdamW优化器效果较好</p>
<h3 id="10">10. 总结与展望<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 核心贡献<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p>MeanFlow的核心贡献可以总结为:</p>
<ol>
<li><strong>概念创新:</strong> 从瞬时速度转向平均速度,突破ODE的$\Delta t \to 0$限制</li>
<li><strong>理论严格:</strong> 通过恒等变换和ReFlow理论,提供了严格的数学基础</li>
<li><strong>多个目标:</strong> 提供了三个训练目标,平衡理论纯粹性与计算效率</li>
<li><strong>统一框架:</strong> 将sCM、Shortcut等方法统一到平均速度框架下</li>
</ol>
<h4 id="102">10.2 理论意义<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p><strong>对扩散模型的理解:</strong>
- 生成过程不必局限于无限小步长
- 全局性质可以直接建模
- 理论保证与实践效率可以兼顾</p>
<p><strong>对未来研究的启示:</strong>
- 寻找更多的全局性质进行建模
- 探索stop_gradient的替代方案
- 发展更精细的理论分析工具</p>
<h4 id="103">10.3 开放问题<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>最优时间离散化:</strong> 多步生成时,如何选择最优的时间点?</li>
<li><strong>高阶平均速度:</strong> 能否引入加速度等高阶概念?</li>
<li><strong>条件生成:</strong> 如何将平均速度扩展到条件生成?</li>
<li><strong>离散数据:</strong> 能否应用到文本等离散模态?</li>
</ol>
<h4 id="104">10.4 实践展望<a class="toc-link" href="#104" title="Permanent link">&para;</a></h4>
<p>MeanFlow为扩散模型的快速生成开辟了新路径:
- <strong>一步生成:</strong> 理论上可以达到与多步生成相当的质量
- <strong>可扩展性:</strong> 可以通过增加步数提升效果
- <strong>训练简单:</strong> 单一目标,无对抗训练
- <strong>理论清晰:</strong> 便于进一步改进和扩展</p>
<p><strong>未来方向:</strong>
1. 结合其他加速技术(蒸馏、剪枝等)
2. 应用到视频、3D等高维数据
3. 与其他生成模型(VAE、GAN)结合
4. 开发更高效的网络架构</p>
<hr />
<p><strong>参考文献:</strong>
- MeanFlow原论文: <a href="https://papers.cool/arxiv/2505.13447">Mean Flows for One-step Generative Modeling</a>
- ReFlow: <a href="https://papers.cool/arxiv/2209.03003">Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow</a>
- sCM: <a href="https://papers.cool/arxiv/2410.11081">Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models</a></p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="moe环游记5均匀分布的反思.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#322 MoE环游记：5、均匀分布的反思</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="等值振荡定理最优多项式逼近的充要条件.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#324 等值振荡定理：最优多项式逼近的充要条件</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">生成扩散模型漫谈（三十）：从瞬时速度到平均速度</a><ul>
<li><a href="#_2">现有思路</a></li>
<li><a href="#_3">瞬时速度</a></li>
<li><a href="#_4">平均速度</a></li>
<li><a href="#_5">恒等变换</a></li>
<li><a href="#_6">第一目标</a></li>
<li><a href="#jvp">JVP运算</a></li>
<li><a href="#_7">第二目标</a></li>
<li><a href="#_8">第三目标</a></li>
<li><a href="#_9">证明一下</a></li>
<li><a href="#_10">相关工作</a></li>
<li><a href="#_11">一致模型</a></li>
<li><a href="#_12">文章小结</a></li>
<li><a href="#_13">公式推导与注释</a><ul>
<li><a href="#1">1. 核心概念与数学框架</a></li>
<li><a href="#2">2. 从瞬时速度到平均速度的动机</a></li>
<li><a href="#3">3. 瞬时速度与平均速度的恒等变换</a></li>
<li><a href="#4-meanflow">4. MeanFlow的三个训练目标</a></li>
<li><a href="#5">5. 理论证明与收敛性分析</a></li>
<li><a href="#6">6. 与相关工作的联系</a></li>
<li><a href="#7">7. 实现细节与技术要点</a></li>
<li><a href="#8">8. 理论深度分析</a></li>
<li><a href="#9">9. 实验观察与实践建议</a></li>
<li><a href="#10">10. 总结与展望</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>