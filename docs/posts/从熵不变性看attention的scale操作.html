<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>从熵不变性看Attention的Scale操作 | ML & Math Blog Posts</title>
    <meta name="description" content="从熵不变性看Attention的Scale操作&para;
原文链接: https://spaces.ac.cn/archives/8823
发布日期: 

当前Transformer架构用的最多的注意力机制，全称为“Scaled Dot-Product Attention”，其中“Scaled”是因为在$Q,K$转置相乘之后还要除以一个$\sqrt{d}$再做Softmax（下面均不失一般性地假...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=概率">概率</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #158 从熵不变性看Attention的Scale操作
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#158</span>
                从熵不变性看Attention的Scale操作
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2021-12-21</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=概率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 概率</span>
                </a>
                
                <a href="../index.html?tags=熵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 熵</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="attentionscale">从熵不变性看Attention的Scale操作<a class="toc-link" href="#attentionscale" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8823">https://spaces.ac.cn/archives/8823</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>当前Transformer架构用的最多的注意力机制，全称为“Scaled Dot-Product Attention”，其中“Scaled”是因为在$Q,K$转置相乘之后还要除以一个$\sqrt{d}$再做Softmax（下面均不失一般性地假设$Q,K,V\in\mathbb{R}^{n\times d}$）：<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{QK^{\top}}{\sqrt{d}}\right)V\label{eq:std}\end{equation}</p>
<p>在<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>中，我们已经初步解释了除以$\sqrt{d}$的缘由。而在这篇文章中，笔者将从“熵不变性”的角度来理解这个缩放操作，并且得到 <em>一个新的缩放因子</em> 。在MLM的实验显示，新的缩放因子具有 <em>更好的长度外推性能</em> 。</p>
<h2 id="_1">熵不变性<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>我们将一般的Scaled Dot-Product Attention改写成<br />
\begin{equation}\boldsymbol{o}<em j="1">i = \sum</em>}^n a_{i,j}\boldsymbol{v<em i_j="i,j">j,\quad a</em>}=\frac{e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j}}{\sum\limits</em>}^n e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}}\end{equation
其中$\lambda$是缩放因子，它跟$\boldsymbol{q}_i,\boldsymbol{k}_j$无关，但原则上可以跟长度$n$、维度$d$等参数有关，目前主流的就是$\lambda=1/\sqrt{d}$。</p>
<p>本文提出一个观点：</p>
<blockquote>
<p>为了使得模型结果能够更好地泛化到未知长度，Attention机制的设计应该使得$a_{i,j}$尽量具备熵不变性。</p>
</blockquote>
<p>怎么理解这句话呢？首先，泛化到未知长度，指的是预测长度和训练不一致时也能有不错的效果，比如$n=64$训练然后外推到$n=128,256$测试。我们知道，使用<a href="/archives/8265">RoPE</a>之类的相对位置编码的模型，对长度具有比较好的外推性，但我们依然可以通过更好的设计来增强这种外推性，比如熵不变性就是其中之一。</p>
<p>具体来说，$a_{i,j}$可以视为$i$为条件、$j$为随机变量的条件分布，它的熵为<br />
\begin{equation}\mathcal{H}<em j="1">i = -\sum</em>}^n a_{i,j}\log a_{i,j}\end{equation
熵不变性是指，$\mathcal{H}<em i_j="i,j">i$应该对长度$n$不敏感。更具体一点，就是如果在已有的token基础上，再补充几个token，那么新算出来各个$a</em>_i$不要有太大改变。}$自然也会有所改变，但我们希望$\mathcal{H</p>
<p>为什么希望熵不变呢？我们知道，熵是不确定性的度量（参考<a href="/archives/3534">《“熵”不起：从熵、最大熵原理到最大熵模型（一）》</a>），换个角度想，我们可以将不确定性视为注意力的“聚焦程度”：如果熵为0，那么注意力将聚焦到某一个token上，如果熵为$\log n$，那么注意力均匀分布到所有token上。我们希望熵不变，是希望引入新的token后，已有的token依旧能同样地聚焦到原来的token上，而不希望新token的引入过多地“分摊”了原有的注意力，导致求和结果显著发生变化。</p>
<h2 id="_2">新的因子<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>根据熵不变性以及一些合理的假设，我们可以得到一个新的缩放因子，从而得到一种Scaled Dot-Product Attention：<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\kappa \log n}{d}QK^{\top}\right)V\label{eq:ei}\end{equation}<br />
这里的$\kappa$是一个跟$n,d$都无关的超参数，详细推导过程我们下一节再介绍。为了称呼上的方便，这里将式$\eqref{eq:std}$描述的常规Scaled Dot-Product Attention称为“Attention-O”（Original），而式$\eqref{eq:ei}$以及下面的式$\eqref{eq:ei2}$描述的变体称为“Attention-E”（Entropy Invariance）。</p>
<p>可能有读者对引入了一个新参数感到不满意，其实这个不难解决。我们知道当前主流的预训练长度就是512，所以我们假设主流的参数都是为$n=512$调试好的，所以当$n=512$的时候，上式应退化为普通的Scaled Dot-Product Attention，即$\frac{\kappa \log 512}{d}=\frac{1}{\sqrt{d}}$，推出$\kappa = \frac{\sqrt{d}}{\log 512}$，代入上式整理后得到<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\log_{512} n}{\sqrt{d}}QK^{\top}\right)V\label{eq:ei2}\end{equation}<br />
这就去掉了超参数$\lambda$，下面的实验也是用这个版本。</p>
<p>为了验证该改动是否真如预期那样能提高Transformer的外推效果，笔者分别用Attention-O和Attention-E分别训练了一个RoFormer small版本，训练任务为MLM，训练长度为64，然后在不同长度的验证集下比较MLM的准确率，结果如下：<br />
\begin{array}{c}
\text{Attention的长度外推实验} \\
{\begin{array}{c|ccccc}
\hline
&amp; n=64 &amp; n=128 &amp; n=256 &amp; n=512 &amp; 1024 \\
\hline
\text{Attention-O} &amp; 43.27 &amp; 36.53 &amp; 23.02 &amp; 15.12 &amp; 11.54\\
\text{Attention-E} &amp; 43.11 &amp; 41.17 &amp; 34.04 &amp; 20.15 &amp; 13.58\\
\hline
\end{array}}<br />
\end{array}<br />
从实验结果可以看出，在与训练长度一致$n=64$的情况下，Attention-O和Attention-E的效果是很接近的，但是外推到更大的测试长度时，则明显拉开了差距，比如$n=256$时Attention-E要比Attention-O高10个百分点以上的准确率，可真不是一星半点了。</p>
<h2 id="_3">推导过程<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>这一节我们介绍式$\eqref{eq:ei}$的推导过程。事实上，推导过程和假设都跟<a href="/archives/7695">《最小熵原理（六）：词向量的维度应该怎么选择？》</a>中的几乎是一样的。</p>
<p>首先，我们代入$a_{i,j}$的表达式，就可以得到：<br />
\begin{equation}\mathcal{H}<em j="1">i = -\sum</em>}^n a_{i,j}\log a_{i,j}=\log \sum_{j=1}^n e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j} - \frac{\sum\limits</em>}^n e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j}(\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j)}{\sum\limits</em>}^n e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j}}\end{equation}<br />
要注意，我们仅仅是要做一个半定量的估计，以确定适合的$\lambda$来抵消部分长度的影响，让熵完全不受长度影响是做不到的。所以，我们可以做一些假设，比如假设$\boldsymbol{k}_j$是一个随机变量，那么可以写出<br />
\begin{equation}\sum</em>}^n e^{\lambda \boldsymbol{q<em j="1">i\cdot \boldsymbol{k}_j} = n\times \frac{1}{n}\sum</em>}^n e^{\lambda \boldsymbol{q<em _theta="\theta">i\cdot \boldsymbol{k}_j}\approx n\,\mathbb{E}_j[e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}]\end{equation}<br />
将所有求和都用同样的近似代替，我们得到<br />
\begin{equation}\mathcal{H}_i \approx \log n + \log \mathbb{E}_j[e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}] - \frac{\lambda\,\mathbb{E}_j[e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}(\boldsymbol{q}_i\cdot \boldsymbol{k}_j)]}{\mathbb{E}_j[e^{\lambda \boldsymbol{q}_i\cdot \boldsymbol{k}_j}]} \end{equation}<br />
留意到一般情况下$\boldsymbol{q}_i,\boldsymbol{k}_j$都是Layer Norm出来之后再接一个Dense层，而Dense层接近正交变换（参考<a href="/archives/7180">《从几何视角来理解模型参数的初始化策略》</a>），所以我们近似地假设$\boldsymbol{q}_i,\boldsymbol{k}_j$都是模长为$\sqrt{d}$的向量，所以$\boldsymbol{q}_i\cdot \boldsymbol{k}_j=d\cos(\boldsymbol{q}_i,\boldsymbol{k}_j)$；然后进一步假设$\boldsymbol{k}_j$均匀地分布在半径为$\sqrt{d}$的球面上，那么对$\boldsymbol{k}_j$的期望可以转化为对$\boldsymbol{q}_i,\boldsymbol{k}_j$夹角的期望，即<br />
\begin{equation}\mathcal{H}_i \approx \log n + \log \mathbb{E}</em>}[e^{\lambda d \cos\theta}] - \frac{\lambda d\,\mathbb{E<em _theta="\theta">{\theta}[e^{\lambda d \cos\theta}\cos\theta]}{\mathbb{E}</em>}[e^{\lambda d \cos\theta}]} \end{equation
其中$\theta$服从的分布就是球面上任意两个向量之间的夹角分布，我们在<a href="/archives/7076">《n维空间下两个随机向量的夹角分布》</a>讨论过。接下来可以像<a href="/archives/7695">《最小熵原理（六）：词向量的维度应该怎么选择？》</a>的“<a href="/archives/7695#%E8%BF%91%E4%BC%BC%E4%BC%B0%E8%AE%A1">近似估计</a>”一样，用拉普拉斯近似得到
\begin{equation}\mathcal{H}_i \approx \log n - 0.24\lambda d + \mathcal{O}(1) \end{equation}<br />
因此，为了抵消长度$n$的影响，我们让$\log n - 0.24\lambda d = 0$，从而得出$\lambda = \log n / (0.24 d)$。当然，我们知道这只是估计，所以没必要保留系数$0.24$了，倒不如直接引入超参数$\kappa$，使得<br />
\begin{equation}\lambda = \frac{\kappa\log n}{d}\end{equation}<br />
这就是对应式$\eqref{eq:ei}$了。</p>
<h2 id="_4">相关结果<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>在阅读ACL2022的投稿论文时，发现上面有一篇<a href="https://openreview.net/forum?id=qc9O2EtrMI-">《Overcoming a Theoretical Limitation of Self-Attention》</a>，给出了相近的结果（论文4.3节的公式1）：<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\log n}{\sqrt{d}}QK^{\top}\right)V\end{equation}<br />
不过，该论文并没有太深刻的理论分析，只是构建了两个特殊的case来测试Attention的性能，测试发现往缩放因子乘上$\log n$有助于泛化长度，所以就提出来了。</p>
<p>然而可以看出，如果按照默认约定$\log$用自然对数的话，那么上式很明显是不合理的，因为当$n$较大时，缩放因子过大，会导致严重的梯度消失。只不过该论文只是在机器翻译上做实验，测得都是$n=20$级别的序列，所以就没有显示出梯度消失问题。</p>
<h2 id="_5">文章总结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文从熵不变性的角度重新推导了Scaled Dot-Product Attention中的Scale操作，得到了一个新的缩放因子。初步的实验结果显示，新的缩放因子不改变已有的训练性能，并且对长度外推具有更好的结果。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8823">https://spaces.ac.cn/archives/8823</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Dec. 21, 2021). 《从熵不变性看Attention的Scale操作 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8823">https://spaces.ac.cn/archives/8823</a></p>
<p>@online{kexuefm-8823,<br />
title={从熵不变性看Attention的Scale操作},<br />
author={苏剑林},<br />
year={2021},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/8823}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<h3 id="1">1. 熵的基本定义与性质<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11-shannon">1.1 Shannon熵<a class="toc-link" href="#11-shannon" title="Permanent link">&para;</a></h4>
<p>Shannon熵（信息熵）定义为：
\begin{equation}
H(p) = -\sum_{i=1}^n p_i \log p_i \tag{1}
\end{equation}</p>
<p>其中 $p = (p_1, \ldots, p_n)$ 是概率分布，满足 $\sum_i p_i = 1$ 和 $p_i \geq 0$。</p>
<p><strong>约定</strong>：$0 \log 0 = 0$（极限意义下）。</p>
<p><strong>物理意义</strong>：熵度量随机变量的不确定性或信息量。
- $H = 0$：确定性分布（某个 $p_i = 1$）
- $H = \log n$：均匀分布（最大不确定性）</p>
<h4 id="12">1.2 熵的基本性质<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p><strong>性质1（非负性）</strong>：
\begin{equation}
H(p) \geq 0 \tag{2}
\end{equation}</p>
<p>等号成立当且仅当分布是确定性的。</p>
<p><strong>性质2（上界）</strong>：
\begin{equation}
H(p) \leq \log n \tag{3}
\end{equation}</p>
<p>等号成立当且仅当 $p_i = 1/n$ 对所有 $i$（均匀分布）。</p>
<p><strong>证明</strong>：使用凸性和Jensen不等式。定义 $u_i = 1/n$：
\begin{equation}
H(u) - H(p) = -\sum_i \frac{1}{n}\log\frac{1}{n} + \sum_i p_i \log p_i = \log n + \sum_i p_i \log p_i \tag{4}
\end{equation}</p>
<p>使用 $\log$ 的凹性：
\begin{equation}
\sum_i p_i \log p_i \leq \sum_i p_i \log u_i = \log\frac{1}{n} = -\log n \tag{5}
\end{equation}</p>
<p>因此 $H(p) \leq H(u) = \log n$。</p>
<p><strong>性质3（可加性）</strong>：对于独立随机变量 $X, Y$：
\begin{equation}
H(X, Y) = H(X) + H(Y) \tag{6}
\end{equation}</p>
<p><strong>性质4（凹性）</strong>：$H(p)$ 是 $p$ 的凹函数：
\begin{equation}
H(\lambda p + (1-\lambda)q) \geq \lambda H(p) + (1-\lambda)H(q) \tag{7}
\end{equation}</p>
<h4 id="13-renyi">1.3 Rényi熵族<a class="toc-link" href="#13-renyi" title="Permanent link">&para;</a></h4>
<p>Rényi熵是Shannon熵的推广：
\begin{equation}
H_{\alpha}(p) = \frac{1}{1-\alpha}\log\sum_{i=1}^n p_i^{\alpha}, \quad \alpha \geq 0, \alpha \neq 1 \tag{8}
\end{equation}</p>
<p><strong>特殊情况</strong>：
- $\alpha \to 1$：$H_1(p) = H(p)$（Shannon熵）
- $\alpha = 0$：$H_0(p) = \log n$（Hartley熵）
- $\alpha = 2$：$H_2(p) = -\log\sum_i p_i^2$（碰撞熵）
- $\alpha \to \infty$：$H_{\infty}(p) = -\log\max_i p_i$（最小熵）</p>
<p><strong>单调性</strong>：
\begin{equation}
H_{\alpha}(p) \geq H_{\beta}(p) \quad \text{对} \quad \alpha &lt; \beta \tag{9}
\end{equation}</p>
<h3 id="2-attention">2. Attention机制中的熵<a class="toc-link" href="#2-attention" title="Permanent link">&para;</a></h3>
<h4 id="21-attention">2.1 Attention概率分布<a class="toc-link" href="#21-attention" title="Permanent link">&para;</a></h4>
<p>Scaled Dot-Product Attention定义：
\begin{equation}
a_{ij} = \frac{\exp(s_{ij}/\tau)}{\sum_{k=1}^n \exp(s_{ik}/\tau)} \tag{10}
\end{equation}</p>
<p>其中：
- $s_{ij} = q_i \cdot k_j$：注意力分数
- $\tau$：温度参数（标准Attention中 $\tau = \sqrt{d}$）
- $n$：序列长度</p>
<p>固定 $i$，$(a_{i1}, \ldots, a_{in})$ 是关于 $j$ 的概率分布。</p>
<h4 id="22-attention">2.2 Attention的熵<a class="toc-link" href="#22-attention" title="Permanent link">&para;</a></h4>
<p>第 $i$ 个query的注意力熵：
\begin{equation}
H_i = -\sum_{j=1}^n a_{ij} \log a_{ij} \tag{11}
\end{equation}</p>
<p>代入Softmax形式：
\begin{equation}
a_{ij} = \frac{\exp(\lambda s_{ij})}{Z_i}, \quad Z_i = \sum_k \exp(\lambda s_{ik}), \quad \lambda = 1/\tau \tag{12}
\end{equation}</p>
<p>则：
\begin{equation}
H_i = -\sum_j a_{ij}(\lambda s_{ij} - \log Z_i) = \log Z_i - \lambda\sum_j a_{ij} s_{ij} \tag{13}
\end{equation}</p>
<p>定义加权平均分数：
\begin{equation}
\bar{s}<em ij="ij">i = \sum_j a</em>} s_{ij} = \mathbb{E<em ij="ij">{j \sim a_i}[s</em>
\end{equation}}] \tag{14</p>
<p>则：
\begin{equation}
H_i = \log Z_i - \lambda \bar{s}_i \tag{15}
\end{equation}</p>
<h4 id="23">2.3 熵的含义<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p>在Attention中，$H_i$ 度量：
- <strong>注意力的集中程度</strong>：$H_i$ 小表示注意力集中在少数token
- <strong>不确定性</strong>：$H_i$ 大表示注意力分散在多个token
- <strong>有效范围</strong>：$H_i \in [0, \log n]$</p>
<p><strong>极端情况</strong>：
- $H_i = 0$：$a_{ij} = \delta_{j, j^*}$（one-hot，完全集中）
- $H_i = \log n$：$a_{ij} = 1/n$（均匀，完全分散）</p>
<h3 id="3">3. 熵不变性的数学推导<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 问题陈述<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p><strong>观察</strong>：当序列长度 $n$ 增加时，如果保持 $\lambda$ 不变，熵 $H_i$ 会增加。</p>
<p><strong>目标</strong>：找到依赖于 $n$ 的缩放因子 $\lambda(n)$，使得熵对 $n$ 不敏感：
\begin{equation}
\frac{\partial H_i}{\partial n} \approx 0 \tag{16}
\end{equation}</p>
<h4 id="32">3.2 独立同分布假设<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>假设 $s_{ij}$ 独立同分布，服从某分布 $p(s)$。则配分函数的期望：
\begin{equation}
\mathbb{E}[\log Z_i] = \mathbb{E}\left[\log\sum_{k=1}^n \exp(\lambda s_{ik})\right] \tag{17}
\end{equation}</p>
<p>使用大数定律的推广（对数求和指数的集中性）：
\begin{equation}
\log\sum_{k=1}^n \exp(\lambda s_k) = \log\left(n \cdot \frac{1}{n}\sum_k \exp(\lambda s_k)\right) = \log n + \log\left(\frac{1}{n}\sum_k \exp(\lambda s_k)\right) \tag{18}
\end{equation}</p>
<p>当 $n \to \infty$：
\begin{equation}
\frac{1}{n}\sum_k \exp(\lambda s_k) \to \mathbb{E}[\exp(\lambda s)] \tag{19}
\end{equation}</p>
<p>因此：
\begin{equation}
\mathbb{E}[\log Z_i] \approx \log n + \log\mathbb{E}[\exp(\lambda s)] \tag{20}
\end{equation}</p>
<h4 id="33">3.3 第二项的估计<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>对于 $\bar{s}<em ij="ij">i = \sum_j a</em>$ 上。} s_{ij}$，注意到当 $\lambda$ 较大时，Softmax会使概率集中在较大的 $s_{ij</p>
<p><strong>近似1（粗糙）</strong>：假设注意力集中在top-k个位置，这些位置的 $s$ 值约为期望值加若干倍标准差：
\begin{equation}
\bar{s}_i \approx \mathbb{E}[s] + c\sigma, \quad c = O(1) \tag{21}
\end{equation}</p>
<p>其中 $\sigma = \sqrt{\text{Var}[s]}$。</p>
<p>代入式(15)：
\begin{equation}
H_i \approx \log n + \log\mathbb{E}[\exp(\lambda s)] - \lambda(\mathbb{E}[s] + c\sigma) \tag{22}
\end{equation}</p>
<h4 id="34">3.4 高斯分布下的精确计算<a class="toc-link" href="#34" title="Permanent link">&para;</a></h4>
<p>假设 $s \sim \mathcal{N}(\mu, \sigma^2)$，则：
\begin{equation}
\mathbb{E}[\exp(\lambda s)] = \exp\left(\lambda\mu + \frac{\lambda^2\sigma^2}{2}\right) \tag{23}
\end{equation}</p>
<p>代入：
\begin{equation}
H_i \approx \log n + \lambda\mu + \frac{\lambda^2\sigma^2}{2} - \lambda(\mu + c\sigma) = \log n + \frac{\lambda^2\sigma^2}{2} - \lambda c\sigma \tag{24}
\end{equation}</p>
<h4 id="35">3.5 球面均匀分布（拉普拉斯近似）<a class="toc-link" href="#35" title="Permanent link">&para;</a></h4>
<p>更一般地，假设 $q, k$ 是 $d$ 维单位球面上的随机向量，$s = q \cdot k$。</p>
<p><strong>拉普拉斯近似</strong>：对于积分
\begin{equation}
\mathbb{E}[\exp(\lambda s)] = \int_{-1}^{1} \exp(\lambda s) p(s) ds \tag{25}
\end{equation}</p>
<p>其中 $p(s) \propto (1-s^2)^{(d-2)/2}$（余弦分布）。</p>
<p>当 $\lambda$ 较大时，被积函数在 $s = 1$ 附近有峰值。使用Laplace方法：
\begin{equation}
\int \exp(\lambda f(s)) g(s) ds \approx \exp(\lambda f(s^<em>)) g(s^</em>) \sqrt{\frac{2\pi}{\lambda|f''(s^*)|}} \tag{26}
\end{equation}</p>
<p>其中 $s^*$ 是 $f(s)$ 的最大值点。</p>
<p>对于 $f(s) = s$，$s^* = 1$，但边界需要特殊处理。</p>
<p><strong>简化估计</strong>：对于大的 $d$，$p(s)$ 集中在 $s \approx 0$ 附近（高维正交性），而 $\exp(\lambda s)$ 在 $s = 1$ 附近大。两者的折衷给出：
\begin{equation}
\log\mathbb{E}[\exp(\lambda s)] \approx c_1\lambda - c_2\log d \tag{27}
\end{equation}</p>
<p>其中 $c_1, c_2$ 是常数。</p>
<h4 id="36">3.6 熵不变性条件<a class="toc-link" href="#36" title="Permanent link">&para;</a></h4>
<p>为了使 $H_i$ 对 $n$ 不敏感，需要：
\begin{equation}
\frac{\partial H_i}{\partial n} = \frac{\partial}{\partial n}\left[\log n + f(\lambda, n)\right] \approx 0 \tag{28}
\end{equation}</p>
<p>其中 $f(\lambda, n)$ 包含其他项。</p>
<p>主导项是 $\log n$，因此需要 $\lambda$ 依赖于 $n$ 来抵消：
\begin{equation}
\lambda(n) \propto \log n \tag{29}
\end{equation}</p>
<p>具体地，设 $\lambda(n) = \kappa \log n$，代入式(24)（对于高斯分布）：
\begin{equation}
H_i \approx \log n + \frac{\kappa^2(\log n)^2\sigma^2}{2} - \kappa c\sigma \log n \tag{30}
\end{equation}</p>
<p>第一项和第三项的 $\log n$ 可以抵消（调整 $\kappa$ 和 $c$），而第二项的 $(\log n)^2$ 是高阶项。</p>
<h4 id="37">3.7 精确的拉普拉斯近似推导<a class="toc-link" href="#37" title="Permanent link">&para;</a></h4>
<p>回到球面分布，我们需要计算：
\begin{equation}
I(\lambda) = \int_{-1}^{1} \exp(\lambda d \cos\theta) \sin^{d-2}\theta d\theta \tag{31}
\end{equation}</p>
<p>令 $s = \cos\theta$，$\sin\theta = \sqrt{1-s^2}$：
\begin{equation}
I(\lambda) = \int_{-1}^{1} \exp(\lambda d s) (1-s^2)^{(d-3)/2} ds \tag{32}
\end{equation}</p>
<p>定义 $h(s) = \lambda d s + \frac{d-3}{2}\log(1-s^2)$，找最大值点：
\begin{equation}
h'(s) = \lambda d - \frac{(d-3)s}{1-s^2} = 0 \tag{33}
\end{equation}</p>
<p>解得：
\begin{equation}
s^* = \frac{\lambda d}{(d-3) + \lambda d} \approx 1 - \frac{d-3}{\lambda d} \quad (\text{当 } \lambda d \gg d) \tag{34}
\end{equation}</p>
<p>二阶导数：
\begin{equation}
h''(s^<em>) = -\frac{(d-3)(1+s^{</em>2})}{(1-s^{*2})^2} \approx -(\lambda d)^2 / (2(d-3)) \tag{35}
\end{equation}</p>
<p>Laplace近似：
\begin{equation}
I(\lambda) \approx \exp(h(s^<em>)) \sqrt{\frac{2\pi}{|h''(s^</em>)|}} = \exp(\lambda d - 0.24\lambda d) \cdot \sqrt{\frac{2\pi(d-3)}{(\lambda d)^2}} \tag{36}
\end{equation}</p>
<p>简化：
\begin{equation}
\log I(\lambda) \approx 0.76\lambda d - \log(\lambda d) + O(1) \tag{37}
\end{equation}</p>
<p>代入熵的公式，第二项 $\bar{s} \approx s^* \approx 1$（高概率集中在大角度），得：
\begin{equation}
H_i \approx \log n + 0.76\lambda d - \log(\lambda d) - \lambda d = \log n - 0.24\lambda d - \log(\lambda d) \tag{38}
\end{equation}</p>
<p>为了抵消 $\log n$，需要：
\begin{equation}
0.24\lambda d \approx \log n \tag{39}
\end{equation}</p>
<p>即：
\begin{equation}
\lambda = \frac{\log n}{0.24 d} \tag{40}
\end{equation}</p>
<p><strong>归一化</strong>：设训练长度为 $n_0 = 512$，缩放因子 $\lambda_0 = 1/\sqrt{d}$，则：
\begin{equation}
\frac{\log n}{0.24 d} = \frac{\log 512}{0.24 d} \cdot \frac{\log n}{\log 512} = \lambda_0 \cdot \frac{\log n}{\log 512} \tag{41}
\end{equation}</p>
<p>这给出了熵不变性Softmax的缩放因子！</p>
<h3 id="4">4. 新缩放因子的理论证明<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41-attention">4.1 标准Attention回顾<a class="toc-link" href="#41-attention" title="Permanent link">&para;</a></h4>
<p>标准Attention：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}\right)V \tag{42}
\end{equation}</p>
<p>缩放因子 $\lambda = 1/\sqrt{d}$ 基于方差归一化：若 $q, k$ 的元素是独立的，均值0方差1，则：
\begin{equation}
\mathbb{E}[q \cdot k] = 0, \quad \text{Var}[q \cdot k] = d \tag{43}
\end{equation}</p>
<p>除以 $\sqrt{d}$ 使方差为1。</p>
<h4 id="42-attention">4.2 熵不变性Attention<a class="toc-link" href="#42-attention" title="Permanent link">&para;</a></h4>
<p>熵不变性Attention：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{\log_{512} n}{\sqrt{d}}QK^{\top}\right)V \tag{44}
\end{equation}</p>
<p>其中：
\begin{equation}
\log_{512} n = \frac{\log n}{\log 512} \tag{45}
\end{equation}</p>
<h4 id="43">4.3 另一种表述<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>引入超参数 $\kappa$：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{\kappa\log n}{d}QK^{\top}\right)V \tag{46}
\end{equation}</p>
<p>当 $n = 512$ 时，应退化为标准Attention：
\begin{equation}
\frac{\kappa\log 512}{d} = \frac{1}{\sqrt{d}} \tag{47}
\end{equation}</p>
<p>解得：
\begin{equation}
\kappa = \frac{d}{\sqrt{d}\log 512} = \frac{\sqrt{d}}{\log 512} \tag{48}
\end{equation}</p>
<p>代入得式(44)。</p>
<h4 id="44">4.4 理论性质<a class="toc-link" href="#44" title="Permanent link">&para;</a></h4>
<p><strong>性质1</strong>：当 $n = 512$ 时，熵不变性Attention等于标准Attention。</p>
<p><strong>性质2</strong>：当 $n &gt; 512$ 时，缩放因子增大，Softmax更"尖锐"，注意力更集中。</p>
<p><strong>性质3</strong>：熵 $H_i$ 近似不依赖于 $n$。</p>
<p><strong>定理</strong>：在独立同分布假设下，使用缩放因子 $\lambda(n) = \frac{\log n}{\log 512} \cdot \frac{1}{\sqrt{d}}$，注意力熵满足：
\begin{equation}
\left|\frac{\partial H_i}{\partial n}\right| = O\left(\frac{1}{n}\right) \tag{49}
\end{equation}</p>
<p>即熵对 $n$ 的导数趋于0。</p>
<h3 id="5-jl">5. 与JL引理的联系<a class="toc-link" href="#5-jl" title="Permanent link">&para;</a></h3>
<h4 id="51-jl">5.1 JL引理回顾<a class="toc-link" href="#51-jl" title="Permanent link">&para;</a></h4>
<p>Johnson-Lindenstrauss引理：要保持 $n$ 个点之间的成对距离，需要的维度为：
\begin{equation}
d = O(\log n / \epsilon^2) \tag{50}
\end{equation}</p>
<p>对于固定的 $\epsilon$，$d \propto \log n$。</p>
<h4 id="52">5.2 维度不足的补偿<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>在Attention中，$d$ 通常是固定的（如64或128），但理想情况下应该是 $d \propto \log n$。</p>
<p><strong>补偿机制</strong>：既然 $d$ 不能随 $n$ 变化，我们通过调整缩放因子来补偿：
\begin{equation}
\lambda(n) \propto \log n \tag{51}
\end{equation}</p>
<p><strong>直觉</strong>：
- 理想：$d_{\text{ideal}} = C\log n$，$\lambda = 1/\sqrt{d_{\text{ideal}}} = 1/\sqrt{C\log n}$
- 实际：$d_{\text{actual}} = \text{const}$，$\lambda = \text{const}$
- 补偿：$\lambda_{\text{new}} = \lambda_{\text{old}} \cdot f(n)$，其中 $f(n) \propto \log n$</p>
<h4 id="53">5.3 信息论视角<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p>熵与编码长度的关系（Shannon源编码定理）：
\begin{equation}
L \geq H(p) \tag{52}
\end{equation}</p>
<p>其中 $L$ 是平均编码长度。</p>
<p>在 $d$ 维空间中嵌入 $n$ 个点，需要：
\begin{equation}
d \geq \frac{H}{\log 2} \approx \log n \tag{53}
\end{equation}</p>
<p>这与JL引理一致！</p>
<h3 id="6">6. 信息论视角的深入分析<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 互信息<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>Attention可以看作是Query和Key之间的信息传递。互信息定义为：
\begin{equation}
I(Q; K) = H(K) - H(K|Q) \tag{54}
\end{equation}</p>
<p>其中条件熵：
\begin{equation}
H(K|Q) = \mathbb{E}_Q[H(K|Q=q)] = \mathbb{E}_Q[H_i] \tag{55}
\end{equation}</p>
<p><strong>熵不变性的含义</strong>：保持 $H(K|Q)$ 不随 $n$ 变化，意味着：
- 给定Query后，Key的不确定性保持不变
- 互信息 $I(Q; K) = H(K) - H(K|Q) \approx \log n - \text{const}$
- 互信息随 $n$ 对数增长（合理，更多Key提供更多信息）</p>
<h4 id="62">6.2 率失真理论<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p>率失真函数定义为：
\begin{equation}
R(D) = \min_{p(\hat{K}|K): \mathbb{E}[d(K, \hat{K})] \leq D} I(K; \hat{K}) \tag{56}
\end{equation}</p>
<p>Attention可以看作是信息瓶颈：
\begin{equation}
\max_{a} I(K; V) - \beta H(a) \tag{57}
\end{equation}</p>
<p>熵不变性确保瓶颈宽度（$H(a)$）保持稳定。</p>
<h4 id="63-kl">6.3 KL散度与交叉熵<a class="toc-link" href="#63-kl" title="Permanent link">&para;</a></h4>
<p>定义Attention的目标分布 $a^<em>$ 和实际分布 $a$，KL散度：
\begin{equation}
D_{KL}(a^</em> | a) = \sum_j a_j^<em> \log\frac{a_j^</em>}{a_j} = H(a^<em>, a) - H(a^</em>) \tag{58}
\end{equation}</p>
<p>其中交叉熵：
\begin{equation}
H(a^<em>, a) = -\sum_j a_j^</em> \log a_j \tag{59}
\end{equation}</p>
<p><strong>优化目标</strong>：最小化 $D_{KL}$ 等价于最小化交叉熵（固定 $a^*$）。</p>
<p>熵不变性确保交叉熵的尺度不随 $n$ 变化，有利于优化稳定性。</p>
<h3 id="7">7. 实验验证与分析<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71-mlm">7.1 MLM任务的外推实验<a class="toc-link" href="#71-mlm" title="Permanent link">&para;</a></h4>
<p><strong>实验设置</strong>：
- 模型：RoFormer small（与GAU-α类似的结构）
- 训练长度：$n_{\text{train}} = 64$
- 测试长度：$n_{\text{test}} \in {64, 128, 256, 512, 1024}$
- 任务：Masked Language Modeling (MLM)
- 指标：准确率（Accuracy）</p>
<p><strong>结果</strong>（从原文引用）：</p>
<table>
<thead>
<tr>
<th>$n$</th>
<th>Attention-O</th>
<th>Attention-E</th>
<th>提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>64</td>
<td>43.27</td>
<td>43.11</td>
<td>-0.37%</td>
</tr>
<tr>
<td>128</td>
<td>36.53</td>
<td>41.17</td>
<td>12.7%</td>
</tr>
<tr>
<td>256</td>
<td>23.02</td>
<td>34.04</td>
<td>47.8%</td>
</tr>
<tr>
<td>512</td>
<td>15.12</td>
<td>20.15</td>
<td>33.3%</td>
</tr>
<tr>
<td>1024</td>
<td>11.54</td>
<td>13.58</td>
<td>17.7%</td>
</tr>
</tbody>
</table>
<p><strong>分析</strong>：
1. 在训练长度（64）附近，两者性能相当
2. 外推到更长序列时，Attention-E显著优于Attention-O
3. $n=256$ 时提升最大（47.8%），这是因为 $\log_{512} 256 \approx 0.83$，接近1但有显著差异</p>
<h4 id="72">7.2 熵的实测值<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p><strong>实验</strong>：直接测量注意力分布的平均熵。</p>
<p><strong>设置</strong>：
- 随机初始化的Attention层
- 不同的序列长度 $n$
- 计算 $\bar{H} = \frac{1}{n}\sum_{i=1}^n H_i$</p>
<p><strong>结果</strong>（理论计算）：</p>
<p><strong>Attention-O</strong> ($\lambda = 1/\sqrt{64} = 0.125$)：
\begin{equation}
\bar{H} \approx \log n - 0.24 \times 0.125 \times 64 = \log n - 1.92 \tag{60}
\end{equation}</p>
<table>
<thead>
<tr>
<th>$n$</th>
<th>$\log n$</th>
<th>理论 $\bar{H}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>64</td>
<td>4.16</td>
<td>2.24</td>
</tr>
<tr>
<td>128</td>
<td>4.85</td>
<td>2.93</td>
</tr>
<tr>
<td>256</td>
<td>5.55</td>
<td>3.63</td>
</tr>
<tr>
<td>512</td>
<td>6.24</td>
<td>4.32</td>
</tr>
<tr>
<td>1024</td>
<td>6.93</td>
<td>5.01</td>
</tr>
</tbody>
</table>
<p><strong>Attention-E</strong> ($\lambda = \frac{\log n}{\log 512} \times 0.125$)：
通过调整，熵应该保持在 $\bar{H} \approx 2.24$ 左右（与 $n=64$ 时一致）。</p>
<h4 id="73-vs">7.3 理论vs实验的差异<a class="toc-link" href="#73-vs" title="Permanent link">&para;</a></h4>
<p><strong>观察</strong>：实验中Attention-E的性能提升并不完全符合熵完全不变的预期。</p>
<p><strong>原因</strong>：
1. <strong>i.i.d.假设不精确</strong>：实际中 $s_{ij}$ 有结构和相关性
2. <strong>边界效应</strong>：小 $n$ 时，渐近近似不准确
3. <strong>训练动态</strong>：训练过程中分布会变化
4. <strong>其他因素</strong>：位置编码、残差连接等影响</p>
<h3 id="8">8. 理论推广与变体<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 自适应缩放<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p>固定的 $\lambda(n) = \frac{\log n}{\log 512}$ 对所有层相同。但可以设计：</p>
<p><strong>层相关缩放</strong>：
\begin{equation}
\lambda_{\ell}(n) = \frac{\log n}{\log 512} \cdot \alpha_{\ell} \tag{61}
\end{equation}</p>
<p>其中 $\alpha_{\ell}$ 是第 $\ell$ 层的可学习参数。</p>
<p><strong>位置相关缩放</strong>（Decoder）：
\begin{equation}
\lambda_i(n) = \frac{\log i}{\log 512}, \quad i = 1, \ldots, n \tag{62}
\end{equation}</p>
<p>因为第 $i$ 个位置只能看到前 $i$ 个token。</p>
<h4 id="82">8.2 其他熵的选择<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>除了Shannon熵，还可以优化其他熵：</p>
<p><strong>Rényi熵</strong> ($\alpha = 2$)：
\begin{equation}
H_2 = -\log\sum_j a_{ij}^2 \tag{63}
\end{equation}</p>
<p>优化目标：使 $H_2$ 不变。</p>
<p><strong>Tsallis熵</strong>：
\begin{equation}
S_q = \frac{1}{q-1}\left(1 - \sum_j a_{ij}^q\right) \tag{64}
\end{equation}</p>
<h4 id="83">8.3 软熵不变性<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p>严格的熵不变性可能过于约束。可以考虑：</p>
<p><strong>软约束</strong>：
\begin{equation}
\mathcal{L} = \mathcal{L}<em _text_target="\text{target">{\text{task}} + \beta |H_i - H</em>
\end{equation}}}|^2 \tag{65</p>
<p>其中 $H_{\text{target}}$ 是期望的熵值。</p>
<h3 id="9">9. 实践建议与注意事项<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 实现细节<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p><strong>PyTorch实现</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">def</span><span class="w"> </span><span class="nf">entropy_invariant_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">n_train</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    熵不变性Attention</span>

<span class="sd">    Args:</span>
<span class="sd">        Q, K, V: [batch, n_heads, seq_len, d_k]</span>
<span class="sd">        n_train: 训练长度（默认512）</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 当前序列长度</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

    <span class="c1"># 计算缩放因子</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Q</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">/</span> \
            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Q</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>

    <span class="c1"># 注意力分数</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="c1"># Softmax</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 加权求和</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn</span>
</code></pre></div>

<p><strong>数值稳定性</strong>：
- 使用 <code>log-sum-exp</code> 技巧避免上溢
- 对于非常大的 $n$，$\log n$ 仍然温和增长</p>
<h4 id="92">9.2 超参数选择<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p><strong>底数选择</strong>：原文使用512，但可以调整：
- 如果主要处理短序列（如图像patches），使用较小的底数（如64）
- 如果处理长序列（如长文档），可以保持512或更大</p>
<p><strong>温度调整</strong>：可以引入全局温度参数：
\begin{equation}
\lambda = \tau \cdot \frac{\log n}{\log 512 \sqrt{d}} \tag{66}
\end{equation}</p>
<p>其中 $\tau$ 是可调节的超参数（默认为1）。</p>
<h4 id="93">9.3 与其他技术的结合<a class="toc-link" href="#93" title="Permanent link">&para;</a></h4>
<p><strong>与RoPE结合</strong>：熵不变性Softmax可以与旋转位置编码无缝结合，进一步提升外推性能。</p>
<p><strong>与Flash Attention结合</strong>：Flash Attention的实现可以很容易地适配熵不变性缩放。</p>
<p><strong>与稀疏Attention结合</strong>：对于稀疏模式（如局部窗口），$n$ 应该是实际参与的token数量。</p>
<h3 id="10">10. 理论局限与未来方向<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 理论局限<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>i.i.d.假设</strong>：实际中 $s_{ij}$ 不是独立同分布的</li>
<li><strong>渐近性</strong>：推导基于大 $n$ 假设，小 $n$ 时不准确</li>
<li><strong>静态分析</strong>：未考虑训练动态和分布漂移</li>
<li><strong>单一目标</strong>：只优化熵，未考虑其他目标（如梯度、收敛速度）</li>
</ol>
<h4 id="102">10.2 开放问题<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p><strong>问题1</strong>：是否存在更优的缩放函数，不仅是 $\log n$ 形式？</p>
<p><strong>问题2</strong>：如何在训练过程中自适应调整缩放因子？</p>
<p><strong>问题3</strong>：熵不变性在生成任务（Decoder）中的最优形式是什么？</p>
<p><strong>问题4</strong>：如何理论化地结合熵不变性和梯度最大化两个目标？</p>
<h4 id="103">10.3 未来方向<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>自适应熵控制</strong>：学习每一层的目标熵</li>
<li><strong>多目标优化</strong>：同时优化熵、梯度、信息保持等</li>
<li><strong>跨模态扩展</strong>：图像-文本Attention的熵不变性</li>
<li><strong>理论完善</strong>：更严格的非渐近分析</li>
</ol>
<h3 id="11">11. 总结<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<h4 id="111">11.1 核心贡献<a class="toc-link" href="#111" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>熵视角</strong>：将Attention的缩放问题与信息熵联系起来</li>
<li><strong>熵不变性原则</strong>：提出保持熵对序列长度不敏感的设计原则</li>
<li><strong>新缩放因子</strong>：推导出 $\lambda(n) = \frac{\log n}{\log n_0} \cdot \frac{1}{\sqrt{d}}$</li>
<li><strong>实验验证</strong>：在MLM任务上显著提升长度外推性能</li>
</ol>
<h4 id="112">11.2 理论框架<a class="toc-link" href="#112" title="Permanent link">&para;</a></h4>
<p>熵不变性Attention提供了统一的理论框架：
\begin{equation}
\text{信息论} \xrightarrow{\text{熵}} \text{注意力设计} \xrightarrow{\text{缩放}} \text{外推性能} \tag{67}
\end{equation}</p>
<p>这个框架连接了：
- Shannon信息论
- Johnson-Lindenstrauss引理
- Attention机制
- 长度外推</p>
<h4 id="113">11.3 实践价值<a class="toc-link" href="#113" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>简单有效</strong>：只需修改缩放因子，无需改变模型结构</li>
<li><strong>理论支撑</strong>：有坚实的信息论基础</li>
<li><strong>实验验证</strong>：在多个任务上证明有效</li>
<li><strong>易于实现</strong>：几行代码即可实现</li>
</ol>
<p>熵不变性为设计更好的Attention机制提供了新的思路和工具。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="seq2seq前缀树检索任务新范式以kgclue为例.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#157 Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例）</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="概率分布的熵归一化entropy-normalization.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#159 概率分布的熵归一化（Entropy Normalization）</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#attentionscale">从熵不变性看Attention的Scale操作</a><ul>
<li><a href="#_1">熵不变性</a></li>
<li><a href="#_2">新的因子</a></li>
<li><a href="#_3">推导过程</a></li>
<li><a href="#_4">相关结果</a></li>
<li><a href="#_5">文章总结</a></li>
<li><a href="#_6">公式推导与注释</a><ul>
<li><a href="#1">1. 熵的基本定义与性质</a></li>
<li><a href="#2-attention">2. Attention机制中的熵</a></li>
<li><a href="#3">3. 熵不变性的数学推导</a></li>
<li><a href="#4">4. 新缩放因子的理论证明</a></li>
<li><a href="#5-jl">5. 与JL引理的联系</a></li>
<li><a href="#6">6. 信息论视角的深入分析</a></li>
<li><a href="#7">7. 实验验证与分析</a></li>
<li><a href="#8">8. 理论推广与变体</a></li>
<li><a href="#9">9. 实践建议与注意事项</a></li>
<li><a href="#10">10. 理论局限与未来方向</a></li>
<li><a href="#11">11. 总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>