<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>无监督语义相似度哪家强？我们做了个比较全面的评测 | ML & Math Blog Posts</title>
    <meta name="description" content="无监督语义相似度哪家强？我们做了个比较全面的评测&para;
原文链接: https://spaces.ac.cn/archives/8321
发布日期: 

一月份的时候，笔者写了《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》，指出无监督语义相似度的SOTA模型BERT-flow其实可以通过一个简单的线性变换（白化操作，BERT-whitening）达到。随后，我们进一步...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #122 无监督语义相似度哪家强？我们做了个比较全面的评测
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#122</span>
                无监督语义相似度哪家强？我们做了个比较全面的评测
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/8321" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=语义" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语义</span>
                </a>
                
                <a href="../index.html?tags=语义相似度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语义相似度</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">无监督语义相似度哪家强？我们做了个比较全面的评测<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8321">https://spaces.ac.cn/archives/8321</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>一月份的时候，笔者写了<a href="/archives/8069">《你可能不需要BERT-flow：一个线性变换媲美BERT-flow》</a>，指出无监督语义相似度的SOTA模型BERT-flow其实可以通过一个简单的线性变换（白化操作，BERT-whitening）达到。随后，我们进一步完善了实验结果，写成了论文<a href="https://papers.cool/arxiv/2103.15316">《Whitening Sentence Representations for Better Semantics and Faster Retrieval》</a>。这篇博客将对这篇论文的内容做一个基本的梳理，并在5个中文语义相似度任务上进行了补充评测，包含了600多个实验结果。</p>
<blockquote>
<p><strong>Github链接：<a href="https://github.com/bojone/BERT-whitening">https://github.com/bojone/BERT-whitening</a></strong></p>
</blockquote>
<h2 id="_2">方法概要<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>BERT-whitening的思路很简单，就是在得到每个句子的句向量$\{x_i\}_{i=1}^N$后，对这些矩阵进行一个白化（也就是PCA），使得每个维度的均值为0、协方差矩阵为单位阵，然后保留$k$个主成分，流程如下图：  </p>
<p><a href="/usr/uploads/2021/04/4207721528.png" title="点击查看原图"><img alt="BERT-whitening的基本流程" src="/usr/uploads/2021/04/4207721528.png" /></a></p>
<p>BERT-whitening的基本流程</p>
<p>当然，理论上来说，我们也可以将BERT-whitening看成是<a href="https://papers.cool/arxiv/2011.05864">BERT-flow</a>的最简单实现，而之前的博客中已经指出，就是这样简单的实现足以媲美一般的BERT-flow模型，有时候甚至更好。同时，BERT-whitening在变换的同时还对特征重要性进行了排序，因此我们可以对句向量进行降维来提高检索速度。实验结果显示，在多数任务中，降维不但不会带来效果上的下降，反而会带来效果上的提升。</p>
<h2 id="_3">英文任务<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>首先介绍BERT-whitening在英文任务上的测试结果，主要包含三个图表，基本上实现了与BERT-flow进行了严格对照。</p>
<h3 id="_4">纯无监督<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h3>
<p>首先，第一个表格介绍的是在完全无监督的情况下，直接使用预训练的BERT抽取句向量的结果。在BERT-flow的论文中，我们已经确实，如果不加任何后处理手段，那么基于BERT抽取句向量的 <em>最好Pooling方法是BERT的第一层与最后一层的所有token向量的平均</em> ，即fisrt-larst-avg（BERT-flow论文误认为是最后两层的平均，记为了last2avg，实际上是第一层与最后一层）。所以后面的结果，都是以fisrt-larst-avg为基准来加flow或者whitening。</p>
<p><a href="/usr/uploads/2021/04/1712832858.png" title="点击查看原图"><img alt="英文任务上纯无监督语义匹配的评测结果" src="/usr/uploads/2021/04/1712832858.png" /></a></p>
<p>英文任务上纯无监督语义匹配的评测结果</p>
<h3 id="nli">NLI监督<a class="toc-link" href="#nli" title="Permanent link">&para;</a></h3>
<p>然后，第二个表格介绍的是基于NLI数据集微调后的<a href="https://papers.cool/arxiv/1908.10084">Sentence-BERT</a>模型（SBERT）抽取句向量的结果，在此情况下同样是fisrt-larst-avg最好，所以flow和whitening的基准都是fisrt-larst-avg出来的句向量。NLI数据集是自然语言推理数据集，跟语义相似度类似但不等价，它可以作为语义相似度任务的有监督预训练，但由于没有直接用到语义相似度数据，因此相对于语义相似度任务来说依然属于无监督的。</p>
<p><a href="/usr/uploads/2021/04/898602584.png" title="点击查看原图"><img alt="英文任务上基于BERT-NLI的语义匹配的评测结果" src="/usr/uploads/2021/04/898602584.png" /></a></p>
<p>英文任务上基于BERT-NLI的语义匹配的评测结果</p>
<h3 id="-">维度-效果<a class="toc-link" href="#-" title="Permanent link">&para;</a></h3>
<p>在这两个表格中，加粗的是最优结果；绿色箭头$\color{green}{\uparrow}$意味着BERT-whitening的结果优于同样情况下的BERT-flow模型，而红色箭头$\color{red}{\downarrow}$则相反，也就是说，绿色箭头越多意味着BERT-whitening的效果也好；whitening后面接的数字256、384指的是降维后保留的维度。所以，从这两个表格可以看出， <em>BERT-whitening总体而言优于BERT-flow，实现了大多数任务的SOTA，并且多数情况下，降维还能进一步提升效果</em> 。</p>
<p>为了进一步确认降维所带来的效果，我们绘制了如下的保留维度与效果关系图：  </p>
<p><a href="/usr/uploads/2021/04/145472581.png" title="点击查看原图"><img alt="英文各个任务上的“维度-效果”图，图中还标记了最有效果对应的维度，可以看到对于每个任务而言，降维都有可能带来一定的提升。" src="/usr/uploads/2021/04/145472581.png" /></a></p>
<p>英文各个任务上的“维度-效果”图，图中还标记了最有效果对应的维度，可以看到对于每个任务而言，降维都有可能带来一定的提升。</p>
<p>上图演示了各个模型在各个任务上经过whitening之后，保留的维度与评测指标的变化曲线。可以看到，对于每个任务来说，最优效果都不是在全部维度取到，这意味着降维都可以带来一定的效果提升，并且可以看到，不少任务甚至可以降维到原来的1/8甚至更多而保持效果不减甚至增加，这充分显示了BERT-whitening的工程价值，因为降维意味着我们能大大加快检索速度。</p>
<h2 id="_5">中文任务<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>秉承着“没有在中文测试过的模型是没有灵魂的”的理念，笔者整理了一些中文语义相似度数据集，结合不同的中文预训练模型、Pooling方式以及whitening与否进行了评测，结果汇总于此，供大家对比。</p>
<h3 id="_6">评测情况<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h3>
<p>本次评测涉及到<strong>11个模型、5个数据集、4种Pooling方式</strong> ，每种组合比较“不whitening”、“whitening”、“whitening且降维”3种后处理方式的效果，所以共有将近<br />
$$11\times 5\times 4\times 3 = 660$$<br />
个实验结果，算是比较全面了。说“将近”是因为某些Pooling方式在某些模型不可用，所以离660还差一点。由于BERT-flow计算成本明显大于BERT-whitening，因此我们没有复现对比BERT-flow的效果，但是从英文任务上可以看出，BERT-whitening和BERT-flow的效果通常是接近的，并且BERT-whitening通常还优于BERT-flow，因为whitening的效果应该是有代表性的了。</p>
<p>评测指标跟前述英文任务一样，都是用spearman相关系数，这是一个类似AUC的排序指标，只依赖于预测分数的顺序，并且不依赖于阈值，所以用来评测效果是比较适合的。之所以没有用大家更熟悉的准确率（accuracy）指标，一是因为准确率依赖于具体的阈值，二是因为STS-B数据集的标签是1～5的数字，并不是0/1标签，要算准确率也无从算起，因此统一用spearman相关系数了。如果读者非要从准确率角度理解，那么大概可以认为“accuracy ≈ 0.5 + spearman / 2”吧。</p>
<p>其中11个模型如下：</p>
<blockquote>
<p><strong>BERT</strong> ：Google开源的中文BERT base版，<a href="https://github.com/google-research/bert">链接</a>；</p>
<p><strong>RoBERTa</strong> ：哈工大开源的roberta_wwm_ext的base版，<a href="https://github.com/ymcui/Chinese-BERT-wwm">链接</a>；</p>
<p><strong>NEZHA</strong> ：华为开源的相对位置编码的BERT base版（wwm），<a href="https://github.com/huawei-noah/Pretrained-Language-Model">链接</a>；</p>
<p><strong>WoBERT</strong> ：以词为单位的BERT，这里用的是Plus版，<a href="https://github.com/ZhuiyiTechnology/WoBERT">链接</a>；</p>
<p><strong>RoFormer</strong> ：加入了新型位置编码的BERT，<a href="https://github.com/ZhuiyiTechnology/roformer">链接</a>；</p>
<p><strong>BERT large</strong>：腾讯UER开源的BERT large版本，<a href="https://github.com/dbiir/UER-py">链接</a>；</p>
<p><strong>RoBERTa large</strong>：哈工大开源的roberta_wwm_ext的large版，<a href="https://github.com/ymcui/Chinese-BERT-wwm">链接</a>；</p>
<p><strong>NEZHA-large</strong> ：华为开源的相对位置编码的BERT large版（wwm），<a href="https://github.com/huawei-noah/Pretrained-Language-Model">链接</a>；</p>
<p><strong>SimBERT</strong> ：经过相似句训练的BERT base版，<a href="https://github.com/ZhuiyiTechnology/simbert">链接</a>；</p>
<p><strong>SimBERT small</strong>：经过相似句训练的BERT small版，<a href="https://github.com/ZhuiyiTechnology/simbert">链接</a>；</p>
<p><strong>SimBERT tiny</strong>：经过相似句训练的BERT tiny版，<a href="https://github.com/ZhuiyiTechnology/simbert">链接</a>。</p>
</blockquote>
<p>5个任务如下：</p>
<blockquote>
<p><strong>ATEC</strong> ：ATEC语义相似度学习赛数据集，金融领域客服场景，原比赛链接已经失效，当前数据来自<a href="https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC">链接</a>；</p>
<p><strong>BQ</strong> ：哈工大BQ Corpus数据集，银行金融领域的问题匹配，详情可看<a href="http://icrc.hitsz.edu.cn/info/1037/1162.htm">链接</a>；</p>
<p><strong>LCQMC</strong> ：哈工大LCQMC数据集，覆盖多个领域的问题匹配，详情可看<a href="http://icrc.hitsz.edu.cn/Article/show/171.html">链接</a>；</p>
<p><strong>PAWSX</strong> ：谷歌发布的数据集（<a href="https://papers.cool/arxiv/1908.11828">链接</a>），数据集里包含了多语种的释义对和非释义对，即识别一对句子是否具有相同的释义（含义），特点是具有高度重叠词汇，对无监督方法来说算是比较难的任务，这里只保留了中文部分；</p>
<p><strong>STS-B</strong> ：计算两句话之间的相关性，原数据集为英文版，通过翻译加部分人工修正的方法生成中文版，来源<a href="https://github.com/pluto-junzeng/CNSD">链接</a>。</p>
</blockquote>
<p>4种Pooling方式如下：</p>
<blockquote>
<p><strong>P1</strong> ：把encoder的最后一层的[CLS]向量拿出来；</p>
<p><strong>P2</strong> ：把Pooler（BERT用来做NSP任务）对应的向量拿出来，跟P1的区别是多了个线性变换；</p>
<p><strong>P3</strong> ：把encoder的最后一层的所有向量取平均；</p>
<p><strong>P4</strong> ：把encoder的第一层与最后一层的所有向量取平均。</p>
</blockquote>
<h3 id="_7">结果汇总<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<p>所有的实验结果汇总在如下三个表格中。其中表格中的每个元素是$a / b / c$的形式，代表该任务在该模型下“不加whitening”的得分为$a$、“加whitening”的得分为$b$、“加whitening并适当降维”的得分为$c$；如果$b\geq a$，那么$b$显示为绿色，否则显示为红色；如果$c\geq a$，那么$c$显示为绿色，否则显示为红色；所谓“适当降维”，对于base版本的模型是降到256维，对于large版本的模型是降到384维，对于tiny和small版则降到128维。</p>
<p>第一个表格是11个模型中的6个base版模型的对比，其中WoBERT和RoFormer没有NSP任务，所以没有P2的权重，测不了P2：<br />
$$\small{\begin{array}{l|ccccc}
\hline
&amp; \text{ATEC} &amp; \text{BQ} &amp; \text{LCQMC} &amp; \text{PAWSX} &amp; \text{STS-B} \\
\hline
\text{BERT-P1} &amp; 16.59 / \color{green}{20.61} / \color{green}{25.58} &amp; 29.35 / \color{red}{25.76} / \color{green}{34.66} &amp; 41.71 / \color{green}{48.92} / \color{green}{49.18} &amp; 15.15 / \color{green}{17.03} / \color{green}{15.98} &amp; 34.65 / \color{green}{61.19} / \color{green}{60.07} \\
\text{BERT-P2} &amp; 9.46 / \color{green}{22.16} / \color{green}{25.13} &amp; 16.97 / \color{green}{18.97} / \color{green}{33.99} &amp; 28.42 / \color{green}{49.61} / \color{green}{49.59} &amp; 13.93 / \color{green}{16.08} / \color{green}{16.19} &amp; 21.66 / \color{green}{60.75} / \color{green}{60.13} \\
\text{BERT-P3} &amp; 20.79 / \color{red}{18.27} / \color{green}{28.98} &amp; 33.08 / \color{red}{22.58} / \color{green}{38.62} &amp; 59.22 / \color{green}{60.12} / \color{green}{62.00} &amp; 16.68 / \color{green}{18.37} / \color{green}{17.38} &amp; 57.48 / \color{green}{63.97} / \color{green}{68.27} \\
\text{BERT-P4} &amp; 24.51 / \color{green}{27.00} / \color{green}{27.91} &amp; 38.81 / \color{red}{32.29} / \color{red}{37.67} &amp; 64.75 / \color{green}{64.75} / \color{green}{65.65} &amp; 15.12 / \color{green}{17.80} / \color{green}{15.34} &amp; 61.66 / \color{green}{69.45} / \color{green}{69.37} \\
\hline
\text{RoBERTa-P1} &amp; 24.61 / \color{green}{29.59} / \color{green}{29.49} &amp; 40.54 / \color{red}{28.95} / \color{red}{38.35} &amp; 70.55 / \color{green}{70.82} / \color{red}{68.84} &amp; 16.23 / \color{green}{17.99} / \color{green}{16.87} &amp; 66.91 / \color{green}{69.19} / \color{green}{71.16} \\
\text{RoBERTa-P2} &amp; 20.61 / \color{green}{28.91} / \color{green}{29.49} &amp; 31.14 / \color{red}{27.48} / \color{green}{38.46} &amp; 65.43 / \color{green}{70.62} / \color{green}{68.76} &amp; 15.71 / \color{green}{17.30} / \color{green}{17.01} &amp; 59.50 / \color{green}{70.77} / \color{green}{71.16} \\
\text{RoBERTa-P3} &amp; 26.94 / \color{green}{29.94} / \color{green}{30.57} &amp; 40.71 / \color{red}{30.95} / \color{red}{39.89} &amp; 66.80 / \color{green}{68.00} / \color{green}{67.30} &amp; 16.08 / \color{green}{19.01} / \color{green}{16.79} &amp; 61.67 / \color{green}{66.19} / \color{green}{69.36} \\
\text{RoBERTa-P4} &amp; 27.94 / \color{green}{28.33} / \color{green}{29.06} &amp; 43.09 / \color{red}{33.49} / \color{red}{38.83} &amp; 68.43 / \color{red}{67.86} / \color{red}{68.36} &amp; 15.02 / \color{green}{17.91} / \color{green}{15.26} &amp; 64.09 / \color{green}{69.74} / \color{green}{70.09} \\
\hline
\text{NEZHA-P1} &amp; 17.39 / \color{green}{18.83} / \color{green}{24.97} &amp; 29.63 / \color{red}{21.94} / \color{green}{33.65} &amp; 40.60 / \color{green}{50.52} / \color{green}{46.57} &amp; 14.90 / \color{green}{18.15} / \color{green}{16.69} &amp; 35.84 / \color{green}{60.84} / \color{green}{58.98} \\
\text{NEZHA-P2} &amp; 10.96 / \color{green}{23.08} / \color{green}{24.21} &amp; 17.38 / \color{green}{28.81} / \color{green}{32.21} &amp; 22.66 / \color{green}{49.12} / \color{green}{47.03} &amp; 13.45 / \color{green}{18.05} / \color{green}{17.15} &amp; 21.16 / \color{green}{60.11} / \color{green}{58.68} \\
\text{NEZHA-P3} &amp; 23.70 / \color{red}{21.93} / \color{green}{28.65} &amp; 35.44 / \color{red}{22.44} / \color{green}{37.95} &amp; 60.94 / \color{green}{62.10} / \color{green}{62.50} &amp; 18.35 / \color{green}{21.72} / \color{green}{18.78} &amp; 60.35 / \color{green}{68.57} / \color{green}{68.97} \\
\text{NEZHA-P4} &amp; 27.72 / \color{red}{25.31} / \color{red}{26.18} &amp; 44.18 / \color{red}{31.47} / \color{red}{36.02} &amp; 65.16 / \color{green}{66.68} / \color{green}{66.54} &amp; 13.98 / \color{green}{16.66} / \color{green}{14.02} &amp; 61.94 / \color{green}{69.55} / \color{green}{69.14} \\
\hline
\text{WoBERT-P1} &amp; 23.88 / \color{red}{22.45} / \color{green}{27.88} &amp; 43.08 / \color{red}{32.52} / \color{red}{37.54} &amp; 68.56 / \color{red}{67.89} / \color{red}{65.80} &amp; 18.15 / \color{green}{19.92} / \color{green}{18.73} &amp; 64.12 / \color{green}{66.53} / \color{green}{69.03} \\
\text{WoBERT-P2} &amp; \text{-} &amp; \text{-} &amp; \text{-} &amp; \text{-} &amp; \text{-} \\
\text{WoBERT-P3} &amp; 24.62 / \color{red}{22.74} / \color{green}{29.01} &amp; 40.64 / \color{red}{28.12} / \color{red}{38.82} &amp; 64.89 / \color{green}{65.22} / \color{green}{65.14} &amp; 16.83 / \color{green}{20.56} / \color{green}{17.87} &amp; 59.43 / \color{green}{66.57} / \color{green}{67.76} \\
\text{WoBERT-P4} &amp; 25.97 / \color{green}{27.24} / \color{green}{28.38} &amp; 42.37 / \color{red}{32.34} / \color{red}{38.06} &amp; 66.53 / \color{red}{65.62} / \color{red}{66.36} &amp; 15.54 / \color{green}{18.85} / \color{green}{15.98} &amp; 61.37 / \color{green}{68.11} / \color{green}{68.42} \\
\hline
\text{RoFormer-P1} &amp; 24.29 / \color{green}{26.04} / \color{green}{28.20} &amp; 41.91 / \color{red}{28.13} / \color{red}{38.21} &amp; 64.87 / \color{red}{60.92} / \color{red}{60.83} &amp; 20.15 / \color{green}{23.08} / \color{green}{21.30} &amp; 59.91 / \color{green}{66.96} / \color{green}{66.86} \\
\text{RoFormer-P2} &amp; \text{-} &amp; \text{-} &amp; \text{-} &amp; \text{-} &amp; \text{-} \\
\text{RoFormer-P3} &amp; 24.09 / \color{green}{28.51} / \color{green}{29.37} &amp; 39.09 / \color{red}{34.92} / \color{red}{39.05} &amp; 63.55 / \color{green}{63.85} / \color{green}{63.58} &amp; 16.53 / \color{green}{18.43} / \color{green}{17.52} &amp; 58.98 / \color{red}{55.30} / \color{green}{67.32} \\
\text{RoFormer-P4} &amp; 25.92 / \color{green}{27.38} / \color{green}{28.37} &amp; 41.75 / \color{red}{32.36} / \color{red}{38.05} &amp; 66.18 / \color{red}{65.45} / \color{red}{65.63} &amp; 15.30 / \color{green}{18.36} / \color{green}{15.69} &amp; 61.40 / \color{green}{68.02} / \color{green}{68.27} \\
\hline
\text{SimBERT-P1} &amp; 38.50 / \color{red}{23.64} / \color{red}{30.79} &amp; 48.54 / \color{red}{31.78} / \color{red}{40.01} &amp; 76.23 / \color{red}{75.05} / \color{red}{74.50} &amp; 15.10 / \color{green}{18.49} / \color{green}{15.64} &amp; 74.14 / \color{red}{73.37} / \color{green}{75.29} \\
\text{SimBERT-P2} &amp; 38.93 / \color{red}{27.06} / \color{red}{30.79} &amp; 49.93 / \color{red}{35.38} / \color{red}{40.14} &amp; 75.56 / \color{red}{73.45} / \color{red}{74.39} &amp; 14.52 / \color{green}{18.51} / \color{green}{15.74} &amp; 73.18 / \color{green}{73.43} / \color{green}{75.12} \\
\text{SimBERT-P3} &amp; 36.50 / \color{red}{31.32} / \color{red}{31.24} &amp; 45.78 / \color{red}{29.17} / \color{red}{40.98} &amp; 74.42 / \color{red}{73.79} / \color{red}{73.43} &amp; 15.33 / \color{green}{18.39} / \color{green}{15.87} &amp; 67.31 / \color{green}{70.70} / \color{green}{72.00} \\
\text{SimBERT-P4} &amp; 33.53 / \color{red}{29.04} / \color{red}{28.78} &amp; 45.28 / \color{red}{34.70} / \color{red}{39.00} &amp; 73.20 / \color{red}{71.22} / \color{red}{72.09} &amp; 14.16 / \color{green}{17.32} / \color{green}{14.39} &amp; 66.98 / \color{green}{70.55} / \color{green}{71.43} \\
\hline
\end{array}}$$</p>
<p>第二个表格则是3个large版模型的对比：<br />
$$\small{\begin{array}{l|ccccc}
\hline
&amp; \text{ATEC} &amp; \text{BQ} &amp; \text{LCQMC} &amp; \text{PAWSX} &amp; \text{STS-B} \\
\hline
\text{BERT}<em _text_large="\text{large">{\text{large}}\text{-P1} &amp; 13.15 / \color{green}{22.42} / \color{green}{24.32} &amp; 19.81 / \color{red}{17.61} / \color{green}{31.09} &amp; 23.45 / \color{green}{44.31} / \color{green}{41.32} &amp; 16.88 / \color{green}{19.37} / \color{green}{19.87} &amp; 25.93 / \color{green}{52.70} / \color{green}{56.74} \\
\text{BERT}</em> \\}}\text{-P2} &amp; 8.16 / \color{green}{16.57} / \color{green}{24.34} &amp; 9.43 / \color{green}{18.23} / \color{green}{30.91} &amp; 16.66 / \color{green}{39.50} / \color{green}{41.40} &amp; 14.72 / \color{green}{20.00} / \color{green}{19.92} &amp; 15.82 / \color{green}{56.79} / \color{green}{56.73
\text{BERT}<em _text_large="\text{large">{\text{large}}\text{-P3} &amp; 24.31 / \color{red}{18.25} / \color{green}{30.24} &amp; 35.87 / \color{red}{32.56} / \color{green}{37.51} &amp; 59.29 / \color{green}{65.06} / \color{green}{63.78} &amp; 16.94 / \color{green}{20.01} / \color{green}{18.62} &amp; 60.22 / \color{green}{68.07} / \color{green}{68.87} \\
\text{BERT}</em> \\}}\text{-P4} &amp; 25.62 / \color{green}{27.64} / \color{green}{28.15} &amp; 38.45 / \color{red}{31.30} / \color{red}{36.47} &amp; 65.43 / \color{green}{66.54} / \color{green}{67.02} &amp; 15.33 / \color{green}{19.06} / \color{green}{15.95} &amp; 62.02 / \color{green}{69.74} / \color{green}{69.99
\hline
\text{RoBERTa}<em _text_large="\text{large">{\text{large}}\text{-P1} &amp; 19.32 / \color{red}{15.90} / \color{green}{29.32} &amp; 34.21 / \color{red}{23.16} / \color{green}{37.11} &amp; 64.89 / \color{green}{67.05} / \color{green}{66.49} &amp; 17.78 / \color{green}{20.66} / \color{green}{19.73} &amp; 60.16 / \color{green}{69.46} / \color{green}{70.44} \\
\text{RoBERTa}</em> \\}}\text{-P2} &amp; 19.32 / \color{green}{22.16} / \color{green}{29.23} &amp; 34.33 / \color{red}{33.22} / \color{green}{37.10} &amp; 65.00 / \color{green}{67.12} / \color{green}{66.50} &amp; 17.77 / \color{green}{18.90} / \color{green}{19.79} &amp; 60.09 / \color{green}{61.35} / \color{green}{70.32
\text{RoBERTa}<em _text_large="\text{large">{\text{large}}\text{-P3} &amp; 24.83 / \color{red}{21.05} / \color{green}{30.85} &amp; 39.23 / \color{red}{26.85} / \color{red}{38.39} &amp; 66.86 / \color{green}{68.62} / \color{green}{67.25} &amp; 17.67 / \color{green}{20.06} / \color{green}{19.09} &amp; 62.98 / \color{red}{55.75} / \color{green}{69.72} \\
\text{RoBERTa}</em> \\}}\text{-P4} &amp; 25.69 / \color{green}{28.19} / \color{green}{28.39} &amp; 40.18 / \color{red}{32.06} / \color{red}{36.91} &amp; 68.58 / \color{green}{68.74} / \color{green}{68.71} &amp; 16.01 / \color{green}{19.87} / \color{green}{16.50} &amp; 63.75 / \color{green}{70.08} / \color{green}{70.39
\hline
\text{NEZHA}<em _text_large="\text{large">{\text{large}}\text{-P1} &amp; 18.91 / \color{green}{24.98} / \color{green}{25.68} &amp; 30.39 / \color{red}{29.30} / \color{green}{33.29} &amp; 41.68 / \color{green}{52.42} / \color{green}{49.80} &amp; 18.89 / \color{green}{23.31} / \color{green}{21.74} &amp; 39.04 / \color{green}{60.36} / \color{green}{61.13} \\
\text{NEZHA}</em> \\}}\text{-P2} &amp; 7.92 / \color{green}{21.60} / \color{green}{25.33} &amp; 12.03 / \color{green}{24.63} / \color{green}{33.22} &amp; 12.33 / \color{green}{52.40} / \color{green}{49.68} &amp; 16.26 / \color{green}{23.11} / \color{green}{21.95} &amp; 16.59 / \color{green}{57.70} / \color{green}{60.82
\text{NEZHA}<em _text_large="\text{large">{\text{large}}\text{-P3} &amp; 22.74 / \color{green}{25.63} / \color{green}{27.48} &amp; 36.48 / \color{red}{22.33} / \color{red}{35.47} &amp; 59.65 / \color{green}{59.90} / \color{green}{59.94} &amp; 18.09 / \color{green}{23.12} / \color{green}{19.71} &amp; 59.66 / \color{green}{67.80} / \color{green}{68.55} \\
\text{NEZHA}</em> \\}}\text{-P4} &amp; 27.45 / \color{red}{24.83} / \color{red}{24.90} &amp; 44.33 / \color{red}{29.73} / \color{red}{34.05} &amp; 66.19 / \color{green}{66.89} / \color{green}{67.88} &amp; 13.74 / \color{green}{16.66} / \color{green}{13.95} &amp; 62.91 / \color{green}{69.87} / \color{green}{69.71
\hline
\end{array}}$$</p>
<p>第三个表格则是不同大小的SimBERT模型之间的对比：<br />
$$\small{\begin{array}{l|ccccc}
\hline
&amp; \text{ATEC} &amp; \text{BQ} &amp; \text{LCQMC} &amp; \text{PAWSX} &amp; \text{STS-B} \\
\hline
\text{SimBERT}\text{-P1} &amp; 38.50 / \color{red}{23.64} / \color{red}{30.79} &amp; 48.54 / \color{red}{31.78} / \color{red}{40.01} &amp; 76.23 / \color{red}{75.05} / \color{red}{74.50} &amp; 15.10 / \color{green}{18.49} / \color{green}{15.64} &amp; 74.14 / \color{red}{73.37} / \color{green}{75.29} \\
\text{SimBERT}\text{-P2} &amp; 38.93 / \color{red}{27.06} / \color{red}{30.79} &amp; 49.93 / \color{red}{35.38} / \color{red}{40.14} &amp; 75.56 / \color{red}{73.45} / \color{red}{74.39} &amp; 14.52 / \color{green}{18.51} / \color{green}{15.74} &amp; 73.18 / \color{green}{73.43} / \color{green}{75.12} \\
\text{SimBERT}\text{-P3} &amp; 36.50 / \color{red}{31.32} / \color{red}{31.24} &amp; 45.78 / \color{red}{29.17} / \color{red}{40.98} &amp; 74.42 / \color{red}{73.79} / \color{red}{73.43} &amp; 15.33 / \color{green}{18.39} / \color{green}{15.87} &amp; 67.31 / \color{green}{70.70} / \color{green}{72.00} \\
\text{SimBERT}\text{-P4} &amp; 33.53 / \color{red}{29.04} / \color{red}{28.78} &amp; 45.28 / \color{red}{34.70} / \color{red}{39.00} &amp; 73.20 / \color{red}{71.22} / \color{red}{72.09} &amp; 14.16 / \color{green}{17.32} / \color{green}{14.39} &amp; 66.98 / \color{green}{70.55} / \color{green}{71.43} \\
\hline
\text{SimBERT}<em _text_small="\text{small">{\text{small}}\text{-P1} &amp; 30.68 / \color{red}{27.56} / \color{red}{29.07} &amp; 43.41 / \color{red}{30.89} / \color{red}{39.78} &amp; 74.73 / \color{red}{73.21} / \color{red}{73.50} &amp; 15.89 / \color{green}{17.96} / \color{green}{16.75} &amp; 70.54 / \color{green}{71.39} / \color{green}{72.14} \\
\text{SimBERT}</em> \\}}\text{-P2} &amp; 31.00 / \color{red}{29.14} / \color{red}{29.11} &amp; 43.76 / \color{red}{36.86} / \color{red}{39.84} &amp; 74.21 / \color{red}{73.14} / \color{red}{73.67} &amp; 16.17 / \color{green}{18.12} / \color{green}{16.81} &amp; 70.10 / \color{green}{71.40} / \color{green}{72.28
\text{SimBERT}<em _text_small="\text{small">{\text{small}}\text{-P3} &amp; 30.03 / \color{red}{21.24} / \color{red}{29.30} &amp; 43.72 / \color{red}{31.69} / \color{red}{40.81} &amp; 72.12 / \color{red}{70.27} / \color{red}{70.52} &amp; 16.93 / \color{green}{21.68} / \color{green}{18.75} &amp; 66.55 / \color{red}{66.11} / \color{green}{69.19} \\
\text{SimBERT}</em> \\}}\text{-P4} &amp; 29.52 / \color{red}{28.41} / \color{red}{28.57} &amp; 43.52 / \color{red}{36.56} / \color{red}{40.49} &amp; 70.33 / \color{red}{68.75} / \color{red}{69.01} &amp; 15.39 / \color{green}{21.57} / \color{green}{16.34} &amp; 64.73 / \color{green}{68.12} / \color{green}{68.24
\hline
\text{SimBERT}<em _text_tiny="\text{tiny">{\text{tiny}}\text{-P1} &amp; 30.51 / \color{red}{24.67} / \color{red}{27.98} &amp; 44.25 / \color{red}{31.75} / \color{red}{39.42} &amp; 74.27 / \color{red}{72.25} / \color{red}{73.24} &amp; 16.01 / \color{green}{18.07} / \color{green}{17.07} &amp; 70.11 / \color{red}{66.39} / \color{green}{71.92} \\
\text{SimBERT}</em> \\}}\text{-P2} &amp; 30.01 / \color{red}{27.66} / \color{red}{27.92} &amp; 44.47 / \color{red}{37.33} / \color{red}{39.39} &amp; 73.98 / \color{red}{72.31} / \color{red}{73.31} &amp; 16.55 / \color{green}{18.15} / \color{green}{17.14} &amp; 70.35 / \color{green}{70.88} / \color{green}{72.04
\text{SimBERT}<em _text_tiny="\text{tiny">{\text{tiny}}\text{-P3} &amp; 28.47 / \color{red}{19.68} / \color{green}{28.60} &amp; 42.04 / \color{red}{29.49} / \color{red}{40.59} &amp; 69.16 / \color{red}{66.99} / \color{red}{67.74} &amp; 16.18 / \color{green}{20.11} / \color{green}{17.87} &amp; 64.41 / \color{green}{66.72} / \color{green}{67.57} \\
\text{SimBERT}</em> \\}}\text{-P4} &amp; 27.77 / \color{red}{27.67} / \color{green}{28.02} &amp; 41.76 / \color{red}{37.02} / \color{red}{40.19} &amp; 67.55 / \color{red}{65.66} / \color{red}{66.60} &amp; 15.06 / \color{green}{20.49} / \color{green}{16.26} &amp; 62.92 / \color{green}{66.77} / \color{green}{67.01
\hline
\end{array}}$$</p>
<h3 id="_8">实验结论<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<p>跟英文任务的表格类似，绿色意味着whitening操作提升了句向量质量，红色则意味着whitening降低了句向量质量，绿色越多则意味着whitening方法越有效。从上述几个表格中，我们可以得出一些结论：</p>
<blockquote>
<p>1、中文任务的测试结果比英文任务复杂得多，更加不规律，比如在英文任务中，P4这种Pooling方式基本上都比其他方式好，而large模型基本上比base好，但这些情况在中文任务中都不明显；</p>
<p>2、除了SimBERT外，整体而言还是绿色比红色多，所以whitening对句向量的改善基本上还是有正面作用的，特别地，在$a / b / c$中，$c$的绿色明显比$b$的绿色要多，这说明降维还能进一步提升效果，也就是说whitening是真正的提速又提效的算法；</p>
<p>3、在BQ任务中，whitening方法几乎都带来了下降，这跟英文任务中的SICK-R任务类似，这说明“天下没有免费的午餐”，总有一些任务会使得“各向同性”假设失效，这时候不管是BERT-whitening还是BERT-flow都不能带来提升；</p>
<p>4、SimBERT是所有除PAWSX外的任务的SOTA，当然SimBERT算是经过语义相似度任务有监督训练过的了（但理论上训练数据与测试任务没有交集），所以跟其他模型比肯定不是特别公平的，但不管怎样，SimBERT已经开源，大家都可以用，所以可以作为一个baseline对待；</p>
<p>5、SimBERT加whitening，要不会带来性能下降，要不就是有提升也不明显，这说明如果通过有监督方法训练出来的句向量，就没有必要进一步做whitening了，基本上不会带来提升；</p>
<p>6、PAWSX确实很难，语义相似度任务还任重道远...</p>
</blockquote>
<h2 id="_9">本文小结<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>本文介绍了我们在中英文任务上对无监督语义相似度方法的比较全面评测。在英文任务方面，主要复述了我们的BERT-whitening方法的论文中的结果，里边包含了跟BERT-flow一一对齐的比较；在中文方面，我们收集了5个任务，在11个预训练模型、4种Pooling方式、3种后处理方式共600多种组合进行了评测，以提供一个可以方便大家对比的结果。</p>
<p>一句话总结评测结果，那就是：BERT-whitening方法确实达到了当前无监督语义的SOTA，而SimBERT则是中文语义相似度的一个比较高的开源baseline。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8321">https://spaces.ac.cn/archives/8321</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 11, 2021). 《无监督语义相似度哪家强？我们做了个比较全面的评测 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8321">https://spaces.ac.cn/archives/8321</a></p>
<p>@online{kexuefm-8321,<br />
title={无监督语义相似度哪家强？我们做了个比较全面的评测},<br />
author={苏剑林},<br />
year={2021},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/8321}},<br />
} </p>
<hr />
<h2 id="_10">公式推导与注释<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="你的crf层的学习率可能不够大.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#121 你的CRF层的学习率可能不够大</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="强大的nvae以后再也不能说vae生成的图像模糊了.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#123 强大的NVAE：以后再也不能说VAE生成的图像模糊了</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">无监督语义相似度哪家强？我们做了个比较全面的评测</a><ul>
<li><a href="#_2">方法概要</a></li>
<li><a href="#_3">英文任务</a><ul>
<li><a href="#_4">纯无监督</a></li>
<li><a href="#nli">NLI监督</a></li>
<li><a href="#-">维度-效果</a></li>
</ul>
</li>
<li><a href="#_5">中文任务</a><ul>
<li><a href="#_6">评测情况</a></li>
<li><a href="#_7">结果汇总</a></li>
<li><a href="#_8">实验结论</a></li>
</ul>
</li>
<li><a href="#_9">本文小结</a></li>
<li><a href="#_10">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>