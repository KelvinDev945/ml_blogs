<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>突破瓶颈，打造更强大的Transformer | ML & Math Blog Posts</title>
    <meta name="description" content="突破瓶颈，打造更强大的Transformer&para;
原文链接: https://spaces.ac.cn/archives/7325
发布日期: 

自《Attention is All You Need》一文发布后，基于Multi-Head Attention的Transformer模型开始流行起来，而去年发布的BERT模型更是将Transformer模型的热度推上了又一个高峰。当然，技术...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=概率">概率</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #43 突破瓶颈，打造更强大的Transformer
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#43</span>
                突破瓶颈，打造更强大的Transformer
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/7325" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=概率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 概率</span>
                </a>
                
                <a href="../index.html?tags=深度学习" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 深度学习</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="transformer">突破瓶颈，打造更强大的Transformer<a class="toc-link" href="#transformer" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/7325">https://spaces.ac.cn/archives/7325</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>自<a href="https://papers.cool/arxiv/1706.03762">《Attention is All You Need》</a>一文发布后，基于Multi-Head Attention的Transformer模型开始流行起来，而去年发布的BERT模型更是将Transformer模型的热度推上了又一个高峰。当然，技术的探索是无止境的，改进的工作也相继涌现：有改进预训练任务的，比如XLNET的PLM、ALBERT的SOP等；有改进归一化的，比如Post-Norm向Pre-Norm的改变，以及T5中去掉了Layer Norm里边的beta参数等；也有改进模型结构的，比如Transformer-XL等；有改进训练方式的，比如ALBERT的参数共享等；...</p>
<p>以上的这些改动，都是在Attention外部进行改动的，也就是说它们都默认了Attention的合理性，没有对Attention本身进行改动。而本文我们则介绍关于两个新结果：<strong>它们针对Multi-Head Attention中可能存在建模瓶颈，提出了不同的方案来改进Multi-Head Attention。两篇论文都来自Google，并且做了相当充分的实验，因此结果应该是相当有说服力的了。</strong></p>
<h2 id="key_size">再小也不能小key_size<a class="toc-link" href="#key_size" title="Permanent link">&para;</a></h2>
<p>第一个结果来自文章<a href="https://papers.cool/arxiv/2002.07028">《Low-Rank Bottleneck in Multi-head Attention Models》</a>，它明确地指出了Multi-Head Attention里边的表达能力瓶颈，并提出通过 <em>增大key_size</em> 的方法来缓解这个瓶颈。</p>
<h3 id="multi-head-attention">Multi-Head Attention<a class="toc-link" href="#multi-head-attention" title="Permanent link">&para;</a></h3>
<p>首先简单回顾一下Multi-Head Attention，读者也可以翻看旧作<a href="/archives/4765">《Attention is All You Need》浅读（简介+代码）</a>。Multi-Head Attention的基础是自然是Single-Head Attention，也叫Scaled-Dot Attention，定义如下：<br />
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V}\end{equation}<br />
其中$\boldsymbol{Q}\in\mathbb{R}^{n\times d_k}, \boldsymbol{K}\in\mathbb{R}^{m\times d_k}, \boldsymbol{V}\in\mathbb{R}^{m\times d_v}$。而Multi-Head Attention，就是将$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}$分别用$h$个不同的投影矩阵投影$h$次，然后分别做$h$次Single-Head Attention，最后把结果拼接起来，即<br />
\begin{equation}\begin{aligned}&amp;\boldsymbol{Q}^{(1)}=\boldsymbol{Q}\boldsymbol{W}_Q^{(1)},\boldsymbol{K}^{(1)}=\boldsymbol{K}\boldsymbol{W}_K^{(1)},\boldsymbol{V}^{(1)}=\boldsymbol{V}\boldsymbol{W}_V^{(1)},\boldsymbol{O}^{(1)}=Attention\left(\boldsymbol{Q}^{(1)},\boldsymbol{K}^{(1)},\boldsymbol{V}^{(1)}\right)\\
&amp;\boldsymbol{Q}^{(2)}=\boldsymbol{Q}\boldsymbol{W}_Q^{(2)},\boldsymbol{K}^{(2)}=\boldsymbol{K}\boldsymbol{W}_K^{(2)},\boldsymbol{V}^{(2)}=\boldsymbol{V}\boldsymbol{W}_V^{(2)},\boldsymbol{O}^{(2)}=Attention\left(\boldsymbol{Q}^{(2)},\boldsymbol{K}^{(2)},\boldsymbol{V}^{(2)}\right)\\
&amp;\qquad\qquad\qquad\qquad\vdots\\
&amp;\boldsymbol{Q}^{(h)}=\boldsymbol{Q}\boldsymbol{W}_Q^{(h)},\boldsymbol{K}^{(h)}=\boldsymbol{K}\boldsymbol{W}_K^{(h)},\boldsymbol{V}^{(h)}=\boldsymbol{V}\boldsymbol{W}_V^{(h)},\boldsymbol{O}^{(h)}=Attention\left(\boldsymbol{Q}^{(h)},\boldsymbol{K}^{(h)},\boldsymbol{V}^{(h)}\right)\\
&amp;\boldsymbol{O}=\left[\boldsymbol{O}^{(1)},\boldsymbol{O}^{(2)},\dots,\boldsymbol{O}^{(h)}\right]
\end{aligned}\end{equation}</p>
<h3 id="attention">Attention里有个瓶颈<a class="toc-link" href="#attention" title="Permanent link">&para;</a></h3>
<p>在实际使用中，$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}$一般具有相同的特征维度$d_k=d_v=d$（即hidden_size），比如BERT Base里边是768；$h$一般选择12、16、24等，比如BERT base里边是12；确定了$d,h$之后，通常的选择是让投影矩阵$\boldsymbol{W}\in\mathbb{R}^{d\times (d/h)}$，也就是说，每个Attention Head里边，是将原始的$d$维投影到$d/h$维，然后在进行Attention运算，输出也是$d/h$维，最后把$h$个$d/h$维的结果拼接起来，得到一个$d$维的输出。这里的$d/h$我们通常称为head_size。</p>
<p>在Attention中，关键的一步是<br />
\begin{equation}\boldsymbol{P}=softmax\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right)\label{eq:softmax}\end{equation}<br />
这一步是描述了$\boldsymbol{Q}$与$\boldsymbol{K}$的两两向量之间的联系，我们可以将$\boldsymbol{P}$看成一个二元联合分布（实际上是$n$个一元分布，不过这个细节并不重要），如果序列长度都为$n$，也就是每个元有$n$个可能的取值，那么这个分布共有$n^2$个值。</p>
<p>但是，我们将$\boldsymbol{Q},\boldsymbol{K}$分别投影到低维后，各自的参数量只有$n\times (d/h)$，总的参数量是$2nd/h$，所以式$\eqref{eq:softmax}$就相当于用$2nd/h$的参数量去逼近一个本身有$n^2$个值的量，而我们通常有$2nd/h \ll n^2$，尤其是$h$比较大时更是如此，因此这种建模有点“强模型所难”，这就是原论文中的“低秩瓶颈（Low-Rank Bottleneck）”的含义。</p>
<h3 id="key_size_1">不妨试试增大key_size？<a class="toc-link" href="#key_size_1" title="Permanent link">&para;</a></h3>
<p>那么，解决办法是什么呢？直接的想法是让$2nd/h$增大，所以要不就是减少head的数目$h$，要不就是增加hidden_size大小$d$。但是更多的Attention Head本身也能增强模型的表达能力，所以为了缓解低秩瓶颈而减少$h$的做法可能得不偿失；如果增加$d$的话，那自然是能够增强模型整体表达能力的，但整个模型的规模与计算量也会剧增，似乎也不是一个好选择。</p>
<p>那没有其他办法了吗？有！当我们用投影矩阵将$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}$都投影到低维时，前面都是将它们投影到$d/h$维，但其实它们的维度不一定要相等，而是只需要保证$\boldsymbol{Q},\boldsymbol{K}$的维度相等就行了（因为要做内积），为了区别，我们通常称$\boldsymbol{Q},\boldsymbol{K}$的维度为key_size，$\boldsymbol{V}$的维度才叫head_size，改变key_size的大小而不改变head_size的话，也不影响模型的hidden_size。</p>
<p>所以，这篇论文提出来的解决方法就是<strong>增大模型的key_size</strong> ，它能增加Attention的表达能力，并且不改变模型整体的hidden_size，计算量上也只是稍微增加了一点。</p>
<blockquote>
<p><strong>补充说明</strong> ：</p>
<p>事实上原论文考虑的是同时增大key_size和head_size、然后Multi-Head Attention的输出拼接之后再用一个变换矩阵降维，但笔者认为由于拼接降维这一步只是一个线性变换，所以本质上的提升还是来源于增大key_size，所以本文只强调了增大key_size这一步。</p>
<p>此外，如果同时增大key_size和head_size，那么会导致计算量和显存消耗都明显增加，而只增大key_size的话，增加的资源消耗就小很多了。</p>
</blockquote>
<h3 id="_1">来看看实验结果～<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h3>
<p>增大key_size这个想法很简单，也容易实现，但是否真的有效呢？我们来看看原论文的实验结果，其实验都是以BERT为baseline的，实验结果图表很多，大家直接看原论文为好，这里只分享比较有代表性的一个：  </p>
<p><a href="/usr/uploads/2020/04/46441019.png" title="点击查看原图"><img alt="保持一个较大的key_size，能使得模型在同样参数规模的情况下表现更优异" src="/usr/uploads/2020/04/46441019.png" /></a></p>
<p>保持一个较大的key_size，能使得模型在同样参数规模的情况下表现更优异</p>
<p>这个结果显示，如果固定一个比较大的key_size（比如128），那么我们可以调整模型的hidden_size和head数，使得参数量可以跟原始的BERT设计一致，但是效果更优！所以，增加key_size确实是有意义的，哪怕将总体参数量重新调整到原来的一样大，也能一定程度上提升模型的效果。这无疑对我们设计新的Transformer模型（尤其是小规模的模型）有重要的指导作用。</p>
<p>最后，附上我们预训练的两个增大了key_size的RoBERTa小模型，欢迎大家使用（我们称之为<strong>RoBERTa +</strong>）：</p>
<blockquote>
<p><a href="https://github.com/ZhuiyiTechnology/pretrained-models">https://github.com/ZhuiyiTechnology/pretrained-models</a></p>
</blockquote>
<h2 id="talking">再缺也不能缺Talking<a class="toc-link" href="#talking" title="Permanent link">&para;</a></h2>
<p>对Multi-Head Attention改进的第二个结果来自论文<a href="https://papers.cool/arxiv/2003.02436">《Talking-Heads Attention》</a>，这篇论文虽然没有显式地指出它跟前一篇论文的联系，但笔者认为它们事实上在解决同一个问题，只不过思路不一样：它指出当前的Multi-Head Attention每个head的运算是相互孤立的，而通过将它们联系（Talking）起来，则可以得到更强的Attention设计，即标题的“Talking-Heads Attention”。</p>
<h3 id="_2">从单一分布到混合分布<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h3>
<p>在前一篇论文里边，我们提到了低秩瓶颈，也就是由于key_size太小所以$\boldsymbol{Q}^{(i)}{\boldsymbol{K}^{(i)}}^{\top}$表达能力不足，因此softmax之后无法很好地建议完整的二元分布。为了缓解这个问题，除了增大key_size之外，还有没有其他方法呢？有，比如这篇论文使用的混合分布思路。</p>
<p>所谓混合分布，就是多个简单分布的叠加（比如加权平均），它能极大地增强原分布的表达能力。典型的例子是高斯混合模型：我们知道高斯分布只是一个常见的简单分布，但多个高斯分布叠加而成的高斯混合分布（也叫高斯混合模型，GMM）就是一个更强的分布，理论上来说，只要叠加的高斯分布足够多，高斯混合分布能逼近任意概率分布。这个例子告诉我们，想要增加Attention中分布的表达能力，又不想增加key_size，那么可以考虑叠加多个低秩分布。</p>
<p>那么“多个”低秩分布哪里来呢？不是有Multi-Head嘛，每个head都带有一个低秩分布，就直接用它们叠加就行了，这就是Talking-Heads Attention了。具体来说，它的形式是：<br />
\begin{equation}\begin{aligned}&amp;\hat{\boldsymbol{J}}^{(1)}=\boldsymbol{Q}^{(1)}{\boldsymbol{K}^{(1)}}^{\top},\quad\hat{\boldsymbol{J}}^{(2)}=\boldsymbol{Q}^{(2)}{\boldsymbol{K}^{(2)}}^{\top},\quad\cdots,\quad\hat{\boldsymbol{J}}^{(h)}=\boldsymbol{Q}^{(h)}{\boldsymbol{K}^{(h)}}^{\top}\\
&amp;\begin{pmatrix}\boldsymbol{J}^{(1)} \\ \boldsymbol{J}^{(2)} \\ \vdots \\ \boldsymbol{J}^{(h)}\end{pmatrix}=\begin{pmatrix}\lambda_{11} &amp; \lambda_{12}&amp; \cdots &amp; \lambda_{1h}\\
\lambda_{21} &amp; \lambda_{22} &amp; \cdots &amp; \lambda_{2h}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
\lambda_{h1} &amp; \lambda_{h2} &amp; \cdots &amp; \lambda_{hh}
\end{pmatrix}\begin{pmatrix}\hat{\boldsymbol{J}}^{(1)} \\ \hat{\boldsymbol{J}}^{(2)} \\ \vdots \\ \hat{\boldsymbol{J}}^{(h)}\end{pmatrix}\\<br />
&amp;\boldsymbol{P}^{(1)}=softmax\left(\boldsymbol{J}^{(1)}\right),\boldsymbol{P}^{(2)}=softmax\left(\boldsymbol{J}^{(2)}\right),\dots,\boldsymbol{P}^{(h)}=softmax\left(\boldsymbol{J}^{(h)}\right)\\<br />
&amp;\boldsymbol{O}^{(1)}=\boldsymbol{P}^{(1)} \boldsymbol{V}^{(1)},\quad \boldsymbol{O}^{(2)}=\boldsymbol{P}^{(2)} \boldsymbol{V}^{(2)},\quad ,\cdots,\quad\boldsymbol{O}^{(h)}=\boldsymbol{P}^{(h)} \boldsymbol{V}^{(h)}\\<br />
&amp;\boldsymbol{O}=\left[\boldsymbol{O}^{(1)},\boldsymbol{O}^{(2)},\dots,\boldsymbol{O}^{(h)}\right]<br />
\end{aligned}\end{equation}<br />
写起来很复杂，事实上很简单，就是<strong>在“$\boldsymbol{Q}\boldsymbol{K}^{\top}$之后、softmax之前”用一个参数矩阵$\boldsymbol{\lambda}$将各个$\boldsymbol{Q}\boldsymbol{K}^{\top}$的结果叠加一下</strong> 而已。这样就把原本是孤立的各个Attention Head联系了起来，即做了一个简单的Talking。</p>
<p>对上述公式，做两点补充说明：</p>
<blockquote>
<p>1、简单起见，上述公式中笔者省去了缩放因子$\sqrt{d_k}$，如果有需要，读者自行补充上去即可；</p>
<p>2、更一般的Talking-Heads Attention允许可以在$\boldsymbol{J}=\boldsymbol{\lambda}\hat{\boldsymbol{J}}$这一步进行升维，即叠加出多于$h$个混合分布，然后再用另一个参数矩阵降维，但这并不是特别重要的改进，所以不在主要篇幅介绍。</p>
</blockquote>
<h3 id="_3">再来看看实验结果～<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h3>
<p>是不是真的有效，当然还是得靠实验结果来说话。这篇论文的实验阵容可谓空前强大，它同时包含了BERT、ALBERT、T5为baseline的实验结果！众所周知，BERT、ALBERT、T5均是某个时间段的NLP最优模型，尤其是T5还是处在<a href="https://super.gluebenchmark.com/leaderboard">superglue</a>的榜首，并且远超出第二名很多，而这个Talking-Heads Attention则几乎是把它们的辉煌战绩又刷到了一个新高度！</p>
<p>还是那句话，具体的实验结果大家自己看论文去，这里展示一个比较典型的结果：  </p>
<p><a href="/usr/uploads/2020/04/3111635993.png" title="点击查看原图"><img alt="实验结果显示，采用Talking-Head机制的情况下，保持hidden_size不变的情况下，head的数目越大，结果越优" src="/usr/uploads/2020/04/3111635993.png" /></a></p>
<p>实验结果显示，采用Talking-Head机制的情况下，保持hidden_size不变的情况下，head的数目越大，结果越优</p>
<p>这个结果显示，使用Talking-Head Attention情况下，保持hidden_size不变，head数目越大（相应地key_size和head_size都越小），效果越优。这看起来跟前一篇增大key_size的结论矛盾，但事实上这正说明了混合分布对分布拟合能力明显提升作用，能够将key_size缩小时本身变弱的单一分布，叠加成拟合能力更强大的分布。当然，这不能说明就直接设key_size=1就好了，因为key_size=1时计算量会明显大于原始的BERT base，应用时需要根据实际情况平衡效果和计算量。</p>
<p>上述表格只是原论文实验结果的冰山一角，这里再放出一个实验表格，让大家感受感受它的实验阵容：  </p>
<p><a href="/usr/uploads/2020/04/813962011.png" title="点击查看原图"><img alt="T5 + Talking-Heads Attention 在SuperGLUE上的实验结果" src="/usr/uploads/2020/04/813962011.png" /></a></p>
<p>T5 + Talking-Heads Attention 在SuperGLUE上的实验结果</p>
<p>几乎每个任务、每个超参组合都做了实验，并给出实验结果。如此强大的实验阵容，基本上也就只有Google能搞出来了，而且整篇论文明显是浓浓的“T5 Style”（还没看过T5论文的读者，可以去<a href="https://papers.cool/arxiv/1910.10683">《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》</a>感受一下），果不其然，作者之一Noam Shazeer也正是T5的作者之一。</p>
<p>笔者只想说，这种庞大的实验轰炸，仿佛在向我们宣告着：</p>
<blockquote>
<p>不用质疑，该调的参数我们都调了，就我们的Talking-Heads Attention最好～</p>
</blockquote>
<h3 id="_4">插曲：神奇的论文画风<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h3>
<p>话说回来，笔者在Arxiv上首次刷到《Talking-Heads Attention》这篇论文时，第一感觉是一篇垃圾论文。为啥？因为它的画风是这样的：  </p>
<p><a href="/usr/uploads/2020/04/591585191.png" title="点击查看原图"><img alt="《Talking-Heads Attention》里边的伪代码" src="/usr/uploads/2020/04/591585191.png" /></a></p>
<p>《Talking-Heads Attention》里边的伪代码</p>
<p>谁能想象到，一篇如此强大的论文，里边居然没有一条数学公式，取而代之的全是伪代码！！其实伪代码都算不上，感觉更像是直接把实验中的Python代码复制到了论文中，还是复制到论文主体上！笔者印象里，只有那些不入流的水论文才会这样做，所以笔者看到的第一想法就是水文一篇。也就Google的大佬们才能这么任性，要不是耐着心多扫了几眼，要不是不小心扫到了T5等字眼，要不是回去看作者居然清一色是Google的，这篇强大的论文就被笔者当作垃圾论文放到回收站了。</p>
<p>不过，任性还是有任性的代价的，这篇实验阵容这么强大又这么有效的论文，发布至今也有一个多月了，但似乎也没什么反响，估计也跟这个任性的风格有关系～</p>
<h2 id="_5">来自文末的小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文介绍了两个关于Multi-Head Attention的后续改进工作，虽然改进细节不一致，但可以说它们都是针对“低秩瓶颈”这个问题而提出的，有种殊途同归之感。两个工作都来自Google，实验内容都很丰富，所以结果都比较有说服力，正在做模型结构改进工作的读者可以参考参考。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/7325">https://spaces.ac.cn/archives/7325</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 13, 2020). 《突破瓶颈，打造更强大的Transformer 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/7325">https://spaces.ac.cn/archives/7325</a></p>
<p>@online{kexuefm-7325,<br />
title={突破瓶颈，打造更强大的Transformer},<br />
author={苏剑林},<br />
year={2020},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/7325}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="cool-papers浏览器扩展升级至v020.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#42 Cool Papers浏览器扩展升级至v0.2.0</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="你可能不需要bert-flow一个线性变换媲美bert-flow.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#44 你可能不需要BERT-flow：一个线性变换媲美BERT-flow</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#transformer">突破瓶颈，打造更强大的Transformer</a><ul>
<li><a href="#key_size">再小也不能小key_size</a><ul>
<li><a href="#multi-head-attention">Multi-Head Attention</a></li>
<li><a href="#attention">Attention里有个瓶颈</a></li>
<li><a href="#key_size_1">不妨试试增大key_size？</a></li>
<li><a href="#_1">来看看实验结果～</a></li>
</ul>
</li>
<li><a href="#talking">再缺也不能缺Talking</a><ul>
<li><a href="#_2">从单一分布到混合分布</a></li>
<li><a href="#_3">再来看看实验结果～</a></li>
<li><a href="#_4">插曲：神奇的论文画风</a></li>
</ul>
</li>
<li><a href="#_5">来自文末的小结</a></li>
<li><a href="#_6">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>