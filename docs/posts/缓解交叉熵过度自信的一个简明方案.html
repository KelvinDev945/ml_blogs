<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>缓解交叉熵过度自信的一个简明方案 | ML & Math Blog Posts</title>
    <meta name="description" content="缓解交叉熵过度自信的一个简明方案&para;
原文链接: https://spaces.ac.cn/archives/9526
发布日期: 

众所周知，分类问题的常规评估指标是正确率，而标准的损失函数则是交叉熵，交叉熵有着收敛快的优点，但它并非是正确率的光滑近似，这就带来了训练和预测的不一致性问题。另一方面，当训练样本的预测概率很低时，交叉熵会给出一个非常巨大的损失（趋于$-\log 0^{+}...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=优化">优化</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #222 缓解交叉熵过度自信的一个简明方案
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#222</span>
                缓解交叉熵过度自信的一个简明方案
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2023-03-14</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=损失函数" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 损失函数</span>
                </a>
                
                <a href="../index.html?tags=光滑" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 光滑</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">缓解交叉熵过度自信的一个简明方案<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9526">https://spaces.ac.cn/archives/9526</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，分类问题的常规评估指标是正确率，而标准的损失函数则是交叉熵，交叉熵有着收敛快的优点，但它并非是正确率的光滑近似，这就带来了训练和预测的不一致性问题。另一方面，当训练样本的预测概率很低时，交叉熵会给出一个非常巨大的损失（趋于$-\log 0^{+}=\infty$），这意味着交叉熵会特别关注预测概率低的样本——哪怕这个样本可能是“脏数据”。所以，交叉熵训练出来的模型往往有过度自信现象，即每个样本都给出较高的预测概率，这会带来两个副作用：一是对脏数据的过度拟合带来的效果下降，二是预测的概率值无法作为不确定性的良好指标。</p>
<p>围绕交叉熵的改进，学术界一直都有持续输出，目前这方面的研究仍处于“八仙过海，各显神通”的状态，没有标准答案。在这篇文章中，我们来学习一下论文<a href="https://papers.cool/arxiv/2302.13344">《Tailoring Language Generation Models under Total Variation Distance》</a>给出的该问题的又一种简明的候选方案。</p>
<h2 id="_2">结果简介<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>顾名思义，原论文的改动是针对文本生成任务的，理论基础是Total Variation距离（参考<a href="/archives/7210#Total%20Variation">《Designing GANs：又一个GAN生产车间》</a>）。但事实上，经过原论文的一系列放缩和简化后，最终结果已经跟Total Variation距离没有明显联系，并且理论上也不限于文本生成任务。所以，本文将它作为一般分类任务的损失函数来看待。</p>
<p>对于数据对$(x,y)$，交叉熵给出的损失函数为<br />
\begin{equation}-\log p_{\theta}(y|x)\end{equation}<br />
原论文的改动很简单，改为<br />
\begin{equation}-\frac{\log \big[\gamma + (1 - \gamma)p_{\theta}(y|x)\big]}{1-\gamma}\label{eq:gamma-ce}\end{equation}<br />
其中$\gamma\in[0,1]$。当$\gamma=0$时，就是普通的交叉熵；当$\gamma=1$时，按极限来算，结果是$-p_{\theta}(y|x)$。</p>
<p>在原论文的实验中，不同任务的$\gamma$选取差别比较大，比如语言模型任务中取到了$\gamma=10^{-7}$，机器翻译任务中取了$\gamma=0.1$，文本摘要任务中取了$\gamma=0.8$。一个可以参考的规律是，如果是从零训练，那么需要选择比较接近于0的$\gamma$，如果是微调训练，那么可以考虑相对大一点的$\gamma$。此外，还有一种比较直观的方案，就是将$\gamma$视为动态参数，从$\gamma=0$开始，随着训练的推进慢慢转向$\gamma=1$，但这样就多了个schedule要调试。</p>
<p>效果上，由于多了个可调的$\gamma$参数，并且原本的交叉熵也包含在里边，所以只要用心去调，一般总有机会调出比交叉熵更好的结果的，这个倒不用太担心。</p>
<h2 id="_3">个人推导<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>怎么理解式$\eqref{eq:gamma-ce}$呢？在<a href="/archives/6620#%E6%AD%A3%E7%A1%AE%E7%8E%87">《函数光滑化杂谈：不可导函数的可导逼近》</a>中的“正确率”一节，我们推导过正确率的光滑近似是<br />
\begin{equation}\mathbb{E}<em _theta="\theta">{(x,y)\sim \mathcal{D}}[p</em>}(y|x)]\end{equation
所以，如果我们的评估指标是正确率，那么直觉上以$-p_{\theta}(y|x)$为损失函数才对，因为这时候损失函数跟正确率的变化更加同步。然而，事实上是交叉熵的表现往往更好。但交叉熵的出发点只是“更好训练”，所以有时候就会“训过头”了，导致过拟合。所以一个直观的想法就是能否将两个结果“插值”一下，以兼顾两者的优点。</p>
<p>为此，我们考虑两者的梯度【准确率指的是它的负光滑近似$-p_{\theta}(y|x)$】：<br />
\begin{equation}\begin{aligned}
\text{准确率：}&amp;\,\quad-\nabla_{\theta} p_{\theta}(y|x) \\
\text{交叉熵：}&amp;\,\quad-\frac{1}{p_{\theta}(y|x)}\nabla_{\theta} p_{\theta}(y|x)
\end{aligned}\end{equation}<br />
两者就差个$\frac{1}{p_{\theta}(y|x)}$。怎么把$\frac{1}{p_{\theta}(y|x)}$变为1呢？原论文的方案是：<br />
\begin{equation}\frac{1}{\gamma + (1 - \gamma)p_{\theta}(y|x)}\end{equation}<br />
当然这个构造方式不是唯一的，原论文选的这个方式，尽可能地保留了交叉熵的梯度特性，也就尽可能保留了交叉熵收敛快的特点。根据这个构造，我们就希望新损失函数的梯度为<br />
\begin{equation}-\frac{\nabla_{\theta}p_{\theta}(y|x)}{\gamma + (1 - \gamma)p_{\theta}(y|x)} = \nabla_{\theta}\left(-\frac{\log \big[\gamma + (1 - \gamma)p_{\theta}(y|x)\big]}{1-\gamma}\right)\label{eq:gamma-ce-g}\end{equation}<br />
这就找出了损失函数$\eqref{eq:gamma-ce}$，在这个过程中，我们先设计新的梯度，然后通过积分找原函数的方式找到了对应的损失函数。</p>
<h2 id="_4">多扯几句<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>为什么要从梯度角度去设计损失函数呢？大概有两方面的原因。</p>
<p>第一，很多损失函数求了梯度后会得到简化，所以在梯度空间设计，往往有更多的灵感和自由度，比如本文的例子中，在梯度空间设计$\frac{1}{p_{\theta}(y|x)}$与$1$的过渡函数$\frac{1}{\gamma + (1 - \gamma)p_{\theta}(y|x)}$不算太复杂，但如果直接在损失函数空间设计$p_{\theta}(y|x)$和$\log p_{\theta}(y|x)$的过渡函数$\frac{\log \big[\gamma + (1 - \gamma)p_{\theta}(y|x)\big]}{1-\gamma}$就复杂多了。</p>
<p>第二，目前使用的优化器都是基于梯度的，所以很多时候我们设计好梯度就行了，甚至都不必要找出原函数。论文的原始结果实际上就是只给出了梯度：<br />
\begin{equation}-\max\left(b, \frac{p_{\theta}(y|x)}{\gamma + (1 - \gamma)p_{\theta}(y|x)}\right)\nabla_{\theta}\log p_{\theta}(y|x)\end{equation}<br />
当$b=0$时，它就等价于式$\eqref{eq:gamma-ce}$。也就是说，原论文在设计梯度的时候还加了个阈值，这时候就很难写出简单的原函数了。但上式实现上并不困难，只要考虑损失函数<br />
\begin{equation}-\max\left(b, \frac{p_{\theta}(y|x)}{\gamma + (1 - \gamma)p_{\theta}(y|x)}\right)<em _theta="\theta">{\text{stop_grad}}\log p</em>}(y|x)\end{equation
这里边的$\text{stop_grad}$就是直接断掉这部分结果的梯度，在tensorflow中对应着<code>tf.stop_gradient</code>算子。</p>
<h2 id="_5">文章小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文主要介绍了缓解交叉熵过度自信的一个简明方案。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9526">https://spaces.ac.cn/archives/9526</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Mar. 14, 2023). 《缓解交叉熵过度自信的一个简明方案 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9526">https://spaces.ac.cn/archives/9526</a></p>
<p>@online{kexuefm-9526,<br />
title={缓解交叉熵过度自信的一个简明方案},<br />
author={苏剑林},<br />
year={2023},<br />
month={Mar},<br />
url={\url{https://spaces.ac.cn/archives/9526}},<br />
} </p>
<hr />
<h2 id="_6">详细数学推导与理论分析<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<h3 id="1">1. 交叉熵的过度自信问题<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<p><strong>标准交叉熵损失</strong>：</p>
<p>对于分类问题，给定样本$(x, y)$，模型预测概率为$p_\theta(y|x)$，交叉熵损失为：
\begin{equation}
\mathcal{L}<em>{\text{CE}} = -\log p</em>\theta(y|x) \tag{1}
\end{equation}</p>
<p><strong>梯度分析</strong>：</p>
<p>对于模型参数$\theta$，梯度为：
\begin{equation}
\nabla_\theta \mathcal{L}<em>{\text{CE}} = -\frac{1}{p</em>\theta(y|x)} \nabla_\theta p_\theta(y|x) \tag{2}
\end{equation}</p>
<p><strong>关键观察</strong>：当$p_\theta(y|x) \to 0$时，梯度趋向无穷大！</p>
<p>\begin{equation}
p_\theta(y|x) \to 0^+ \Rightarrow |\nabla_\theta \mathcal{L}_{\text{CE}}| \to \infty \tag{3}
\end{equation}</p>
<p>这导致模型对低概率样本给予极大关注，即使这些样本可能是：
- <strong>噪声标签</strong>（label noise）
- <strong>异常值</strong>（outliers）
- <strong>边界样本</strong>（borderline cases）</p>
<p><strong>过度自信现象</strong>：</p>
<p>优化交叉熵会推动模型给出极端的概率预测：
\begin{equation}
p_\theta(y|x) \to 1 \quad \text{对于训练样本} \tag{4}
\end{equation}</p>
<p>这会导致：
1. <strong>校准性差</strong>：预测概率不能很好地反映真实置信度
2. <strong>过拟合</strong>：对脏数据过度拟合
3. <strong>泛化性差</strong>：在测试集上表现下降</p>
<h3 id="2-">2. γ-交叉熵的定义与推导<a class="toc-link" href="#2-" title="Permanent link">&para;</a></h3>
<p><strong>原论文的γ-交叉熵</strong>：</p>
<p>\begin{equation}
\mathcal{L}<em>{\gamma\text{-CE}} = -\frac{\log[\gamma + (1-\gamma)p</em>\theta(y|x)]}{1-\gamma} \tag{5}
\end{equation}</p>
<p>其中$\gamma \in [0, 1]$是超参数。</p>
<p><strong>特殊情况分析</strong>：</p>
<p><strong>情况1</strong>：$\gamma = 0$
\begin{equation}
\mathcal{L}<em>{0\text{-CE}} = -\frac{\log p</em>\theta(y|x)}{1} = -\log p_\theta(y|x) = \mathcal{L}_{\text{CE}} \tag{6}
\end{equation}</p>
<p>退化为标准交叉熵。</p>
<p><strong>情况2</strong>：$\gamma = 1$（利用洛必达法则）
\begin{equation}
\begin{aligned}
\mathcal{L}<em 1="1" _gamma="\gamma" _to="\to">{1\text{-CE}} &amp;= \lim</em> \
&amp;= \lim_{\gamma \to 1} -\frac{\frac{d}{d\gamma}\log[\gamma + (1-\gamma)p_\theta(y|x)]}{\frac{d}{d\gamma}(1-\gamma)} \
&amp;= \lim_{\gamma \to 1} -\frac{\frac{1 - p_\theta(y|x)}{\gamma + (1-\gamma)p_\theta(y|x)}}{-1} \
&amp;= \frac{1 - p_\theta(y|x)}{1} = 1 - p_\theta(y|x)
\end{aligned} \tag{7}
\end{equation}} -\frac{\log[\gamma + (1-\gamma)p_\theta(y|x)]}{1-\gamma</p>
<p>这是<strong>负的准确率平滑近似</strong>！</p>
<p><strong>中间情况</strong>：$\gamma \in (0, 1)$</p>
<p>\begin{equation}
\mathcal{L}<em _text_CE="\text{CE">{\gamma\text{-CE}} \text{ 在 } \mathcal{L}</em>
\end{equation}}} \text{ 和 } 1 - p_\theta(y|x) \text{ 之间插值} \tag{8</p>
<h3 id="3-">3. 从梯度视角理解γ-交叉熵<a class="toc-link" href="#3-" title="Permanent link">&para;</a></h3>
<p><strong>准确率的平滑近似</strong>：</p>
<p>如原文所述，分类准确率的平滑近似为：
\begin{equation}
\text{Accuracy} \approx \mathbb{E}<em>{(x,y)\sim\mathcal{D}}[p</em>\theta(y|x)] \tag{9}
\end{equation}</p>
<p>因此，最小化$-p_\theta(y|x)$等价于最大化准确率。</p>
<p><strong>两种损失的梯度对比</strong>：</p>
<p>准确率损失的梯度：
\begin{equation}
\nabla_\theta[-p_\theta(y|x)] = -\nabla_\theta p_\theta(y|x) \tag{10}
\end{equation}</p>
<p>交叉熵损失的梯度：
\begin{equation}
\nabla_\theta[-\log p_\theta(y|x)] = -\frac{1}{p_\theta(y|x)} \nabla_\theta p_\theta(y|x) \tag{11}
\end{equation}</p>
<p><strong>关键差异</strong>：权重因子$\frac{1}{p_\theta(y|x)}$</p>
<p><strong>设计目标</strong>：找到一个权重函数$w(p)$，使得：
\begin{equation}
w(p) \text{ 在 } 1 \text{ 和 } \frac{1}{p} \text{ 之间插值} \tag{12}
\end{equation}</p>
<p><strong>原论文的选择</strong>：
\begin{equation}
w_\gamma(p) = \frac{1}{\gamma + (1-\gamma)p} \tag{13}
\end{equation}</p>
<p>验证：
- $w_0(p) = \frac{1}{p}$（交叉熵）
- $w_1(p) = 1$（准确率）</p>
<p><strong>寻找对应的损失函数</strong>：</p>
<p>我们希望找到$\mathcal{L}$使得：
\begin{equation}
\nabla_\theta \mathcal{L} = -\frac{1}{\gamma + (1-\gamma)p_\theta(y|x)} \nabla_\theta p_\theta(y|x) \tag{14}
\end{equation}</p>
<p><strong>积分反推</strong>：</p>
<p>记$p = p_\theta(y|x)$，我们需要：
\begin{equation}
\frac{d\mathcal{L}}{dp} = -\frac{1}{\gamma + (1-\gamma)p} \tag{15}
\end{equation}</p>
<p>积分：
\begin{equation}
\begin{aligned}
\mathcal{L} &amp;= -\int \frac{1}{\gamma + (1-\gamma)p} dp \
&amp;= -\frac{1}{1-\gamma} \int \frac{1}{\gamma + (1-\gamma)p} d[\gamma + (1-\gamma)p] \
&amp;= -\frac{1}{1-\gamma} \log[\gamma + (1-\gamma)p] + C
\end{aligned} \tag{16}
\end{equation}</p>
<p>选择常数$C=0$，得到式(5)！</p>
<h3 id="4-">4. γ-交叉熵的性质<a class="toc-link" href="#4-" title="Permanent link">&para;</a></h3>
<p><strong>性质1：单调性</strong></p>
<p>对于固定的$p$，$\mathcal{L}<em _gamma_text_-CE="\gamma\text{-CE">{\gamma\text{-CE}}$关于$\gamma$单调递增：
\begin{equation}
\frac{\partial \mathcal{L}</em>
\end{equation}}}}{\partial \gamma} = \frac{\log[\gamma + (1-\gamma)p]}{(1-\gamma)^2} - \frac{1-p}{(1-\gamma)[\gamma + (1-\gamma)p]} \tag{17</p>
<p>当$p &lt; 1$时，可以证明：
\begin{equation}
\frac{\partial \mathcal{L}_{\gamma\text{-CE}}}{\partial \gamma} &gt; 0 \tag{18}
\end{equation}</p>
<p><strong>性质2：有界性</strong></p>
<p>当$\gamma &gt; 0$时，损失有上界：
\begin{equation}
\mathcal{L}_{\gamma\text{-CE}} \leq -\frac{\log \gamma}{1-\gamma} \tag{19}
\end{equation}</p>
<p>这避免了交叉熵在$p \to 0$时趋向无穷的问题！</p>
<p><strong>性质3：梯度有界性</strong></p>
<p>梯度的大小为：
\begin{equation}
|\nabla_\theta \mathcal{L}<em>{\gamma\text{-CE}}| = \frac{1}{\gamma + (1-\gamma)p</em>\theta(y|x)} |\nabla_\theta p_\theta(y|x)| \tag{20}
\end{equation}</p>
<p>有上界：
\begin{equation}
|\nabla_\theta \mathcal{L}<em>{\gamma\text{-CE}}| \leq \frac{1}{\gamma} |\nabla</em>\theta p_\theta(y|x)| \tag{21}
\end{equation}</p>
<p>而交叉熵的梯度可以无界！</p>
<h3 id="5-label-smoothing">5. 与Label Smoothing的联系<a class="toc-link" href="#5-label-smoothing" title="Permanent link">&para;</a></h3>
<p><strong>标准Label Smoothing</strong>：</p>
<p>对于$K$类分类问题，硬标签为one-hot向量$\boldsymbol{y} = [0, \ldots, 1, \ldots, 0]$。</p>
<p>Label smoothing定义软标签为：
\begin{equation}
\tilde{y}_i = \begin{cases}
1 - \alpha + \frac{\alpha}{K}, &amp; i = y \
\frac{\alpha}{K}, &amp; i \neq y
\end{cases} \tag{22}
\end{equation}</p>
<p>其中$\alpha \in [0, 1]$是平滑参数。</p>
<p><strong>等价写法</strong>：
\begin{equation}
\tilde{\boldsymbol{y}} = (1-\alpha)\boldsymbol{y} + \alpha \boldsymbol{u} \tag{23}
\end{equation}</p>
<p>其中$\boldsymbol{u} = [\frac{1}{K}, \ldots, \frac{1}{K}]$是均匀分布。</p>
<p><strong>Label Smoothing的损失</strong>：
\begin{equation}
\mathcal{L}<em i="1">{\text{LS}} = -\sum</em>}^K \tilde{y<em>i \log p</em>\theta(i|x) \tag{24}
\end{equation}</p>
<p><strong>展开</strong>：
\begin{equation}
\begin{aligned}
\mathcal{L}<em>{\text{LS}} &amp;= -(1-\alpha)\log p</em>\theta(y|x) - \frac{\alpha}{K}\sum_{i=1}^K \log p_\theta(i|x) \
&amp;= (1-\alpha)\mathcal{L}<em _text_unif="\text{unif">{\text{CE}} + \alpha \mathcal{L}</em>
\end{aligned} \tag{25}
\end{equation}}</p>
<p>其中$\mathcal{L}<em>{\text{unif}} = -\frac{1}{K}\sum_i \log p</em>\theta(i|x)$是与均匀分布的交叉熵。</p>
<p><strong>与γ-交叉熵的对比</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>形式</th>
<th>插值对象</th>
</tr>
</thead>
<tbody>
<tr>
<td>Label Smoothing</td>
<td>$(1-\alpha)\mathcal{L}<em _text_unif="\text{unif">{\text{CE}} + \alpha \mathcal{L}</em>$}</td>
<td>损失函数的线性组合</td>
</tr>
<tr>
<td>γ-交叉熵</td>
<td>$-\frac{\log[\gamma + (1-\gamma)p]}{1-\gamma}$</td>
<td>梯度的插值</td>
</tr>
</tbody>
</table>
<p><strong>区别</strong>：
- Label Smoothing在<strong>损失空间</strong>插值
- γ-交叉熵在<strong>梯度空间</strong>插值</p>
<h3 id="6-calibration">6. 校准理论（Calibration）<a class="toc-link" href="#6-calibration" title="Permanent link">&para;</a></h3>
<p><strong>什么是校准？</strong></p>
<p>一个模型是<strong>完美校准</strong>的，如果：
\begin{equation}
\mathbb{P}(Y = y | p_\theta(Y|X) = p) = p \quad \forall p \in [0, 1] \tag{26}
\end{equation}</p>
<p>即：对于所有预测概率为$p$的样本，真实的准确率也是$p$。</p>
<p><strong>Expected Calibration Error (ECE)</strong>：</p>
<p>将预测概率分为$M$个区间$B_1, \ldots, B_M$，ECE定义为：
\begin{equation}
\text{ECE} = \sum_{m=1}^M \frac{|B_m|}{N} |\text{acc}(B_m) - \text{conf}(B_m)| \tag{27}
\end{equation}</p>
<p>其中：
- $\text{conf}(B_m) = \frac{1}{|B_m|} \sum_{i \in B_m} \hat{p}<em B_m="B_m" _in="\in" i="i">i$（平均置信度）
- $\text{acc}(B_m) = \frac{1}{|B_m|} \sum</em>_i = y_i]$（准确率）} \mathbb{1}[\hat{y</p>
<p><strong>可靠性图（Reliability Diagram）</strong>：</p>
<p>横轴为$\text{conf}(B_m)$，纵轴为$\text{acc}(B_m)$。完美校准对应于对角线$y=x$。</p>
<p><strong>交叉熵的校准问题</strong>：</p>
<p>交叉熵训练的模型通常是<strong>过度自信</strong>的（overconfident）：
\begin{equation}
\text{conf}(B_m) &gt; \text{acc}(B_m) \tag{28}
\end{equation}</p>
<p>即：预测概率高于真实准确率。</p>
<p><strong>γ-交叉熵的校准效果</strong>：</p>
<p>通过减小梯度权重，γ-交叉熵缓解了过度自信问题，使得：
\begin{equation}
\text{conf}(B_m) \approx \text{acc}(B_m) \tag{29}
\end{equation}</p>
<h3 id="7">7. 正则化效应的数学证明<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p><strong>定理</strong>：γ-交叉熵等价于在交叉熵上添加一个隐式的正则项。</p>
<p><strong>证明思路</strong>：</p>
<p>Taylor展开式(5)在$p = p_\theta(y|x)$附近：
\begin{equation}
\begin{aligned}
\mathcal{L}_{\gamma\text{-CE}} &amp;= -\frac{\log[\gamma + (1-\gamma)p]}{1-\gamma} \
&amp;\approx -\frac{\log[(1-\gamma)p] + \log[1 + \frac{\gamma}{(1-\gamma)p}]}{1-\gamma}
\end{aligned} \tag{30}
\end{equation}</p>
<p>利用$\log(1+x) \approx x - \frac{x^2}{2}$（当$x$小时）：
\begin{equation}
\log\left[1 + \frac{\gamma}{(1-\gamma)p}\right] \approx \frac{\gamma}{(1-\gamma)p} - \frac{1}{2}\left(\frac{\gamma}{(1-\gamma)p}\right)^2 \tag{31}
\end{equation}</p>
<p>代入：
\begin{equation}
\begin{aligned}
\mathcal{L}_{\gamma\text{-CE}} &amp;\approx -\frac{1}{1-\gamma}\left[\log p + \frac{\gamma}{(1-\gamma)p} - \frac{\gamma^2}{2(1-\gamma)^2 p^2}\right] \
&amp;= -\log p - \frac{\gamma}{(1-\gamma)^2 p} + \frac{\gamma^2}{2(1-\gamma)^3 p^2}
\end{aligned} \tag{32}
\end{equation}</p>
<p>第一项是交叉熵，后面两项是<strong>正则项</strong>！</p>
<p><strong>直觉解释</strong>：
- $-\frac{\gamma}{(1-\gamma)^2 p}$：惩罚小概率（防止过度自信）
- $+\frac{\gamma^2}{2(1-\gamma)^3 p^2}$：二阶修正项</p>
<h3 id="8">8. 与其他鲁棒损失函数的对比<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<p><strong>Mean Absolute Error (MAE)</strong>：
\begin{equation}
\mathcal{L}<em i="1">{\text{MAE}} = \sum</em>
\end{equation}}^K |y_i - p_\theta(i|x)| \tag{33</p>
<p><strong>Focal Loss</strong>：
\begin{equation}
\mathcal{L}<em>{\text{Focal}} = -(1 - p</em>\theta(y|x))^\beta \log p_\theta(y|x) \tag{34}
\end{equation}</p>
<p><strong>Generalized Cross Entropy (GCE)</strong>：
\begin{equation}
\mathcal{L}<em>{\text{GCE}} = \frac{1 - p</em>\theta(y|x)^q}{q} \tag{35}
\end{equation}</p>
<p><strong>对比表</strong>：</p>
<table>
<thead>
<tr>
<th>损失函数</th>
<th>梯度权重</th>
<th>鲁棒性来源</th>
<th>超参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>CE</td>
<td>$\frac{1}{p}$</td>
<td>无</td>
<td>无</td>
</tr>
<tr>
<td>γ-CE</td>
<td>$\frac{1}{\gamma + (1-\gamma)p}$</td>
<td>梯度裁剪</td>
<td>$\gamma$</td>
</tr>
<tr>
<td>Focal</td>
<td>$\frac{(1-p)^\beta}{p}$</td>
<td>降低易分样本权重</td>
<td>$\beta$</td>
</tr>
<tr>
<td>GCE</td>
<td>$p^{q-1}$</td>
<td>有界损失</td>
<td>$q$</td>
</tr>
</tbody>
</table>
<h3 id="9">9. 梯度裁剪的几何解释<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p><strong>梯度空间的视角</strong>：</p>
<p>在参数空间$\Theta$中，梯度$\nabla_\theta \mathcal{L}$定义了一个向量场。</p>
<p>交叉熵的梯度：
\begin{equation}
\boldsymbol{g}<em>{\text{CE}} = -\frac{1}{p</em>\theta(y|x)} \nabla_\theta p_\theta(y|x) \tag{36}
\end{equation}</p>
<p>γ-交叉熵的梯度：
\begin{equation}
\boldsymbol{g}<em>{\gamma\text{-CE}} = -\frac{1}{\gamma + (1-\gamma)p</em>\theta(y|x)} \nabla_\theta p_\theta(y|x) \tag{37}
\end{equation}</p>
<p><strong>比例关系</strong>：
\begin{equation}
\boldsymbol{g}<em>{\gamma\text{-CE}} = \frac{\gamma + (1-\gamma)p</em>\theta(y|x)}{p_\theta(y|x)} \boldsymbol{g}<em>{\text{CE}} = \left[\gamma + (1-\gamma)p</em>\theta(y|x)\right] \frac{\boldsymbol{g}<em>{\text{CE}}}{p</em>\theta(y|x)} \tag{38}
\end{equation}</p>
<p>定义<strong>裁剪因子</strong>：
\begin{equation}
c(\gamma, p) = \gamma + (1-\gamma)p = (1-\gamma)\left(p + \frac{\gamma}{1-\gamma}\right) \tag{39}
\end{equation}</p>
<p><strong>关键性质</strong>：
\begin{equation}
c(\gamma, p) \geq \gamma \quad \forall p \in [0, 1] \tag{40}
\end{equation}</p>
<p>因此：
\begin{equation}
|\boldsymbol{g}<em _text_CE="\text{CE">{\gamma\text{-CE}}| \leq \frac{1}{\gamma} |\boldsymbol{g}</em>
\end{equation}}} \cdot p_\theta(y|x)| = \frac{1}{\gamma} |\nabla_\theta p_\theta(y|x)| \tag{41</p>
<p><strong>几何直觉</strong>：γ-交叉熵相当于对交叉熵的梯度进行了<strong>自适应裁剪</strong>，裁剪强度取决于当前预测概率$p$。</p>
<h3 id="10-hessian">10. Hessian分析与收敛性<a class="toc-link" href="#10-hessian" title="Permanent link">&para;</a></h3>
<p><strong>交叉熵的Hessian</strong>：</p>
<p>对于单个样本，记$\boldsymbol{p} = [p_1, \ldots, p_K]$为预测概率向量（$\sum_i p_i = 1$）。</p>
<p>对于参数$\theta_i$和$\theta_j$：
\begin{equation}
\frac{\partial^2 \mathcal{L}_{\text{CE}}}{\partial \theta_i \partial \theta_j} = -\frac{1}{p_y}\frac{\partial^2 p_y}{\partial \theta_i \partial \theta_j} + \frac{1}{p_y^2}\frac{\partial p_y}{\partial \theta_i}\frac{\partial p_y}{\partial \theta_j} \tag{42}
\end{equation}</p>
<p>当$p_y \to 0$时，Hessian的第二项趋向无穷！</p>
<p><strong>γ-交叉熵的Hessian</strong>：</p>
<p>\begin{equation}
\frac{\partial^2 \mathcal{L}_{\gamma\text{-CE}}}{\partial \theta_i \partial \theta_j} = -\frac{1}{\gamma + (1-\gamma)p_y}\frac{\partial^2 p_y}{\partial \theta_i \partial \theta_j} + \frac{1-\gamma}{[\gamma + (1-\gamma)p_y]^2}\frac{\partial p_y}{\partial \theta_i}\frac{\partial p_y}{\partial \theta_j} \tag{43}
\end{equation}</p>
<p><strong>关键区别</strong>：第二项被$[\gamma + (1-\gamma)p_y]^2 \geq \gamma^2$界住，因此Hessian是<strong>有界的</strong>！</p>
<p><strong>收敛速度</strong>：</p>
<p>在凸优化中，Hessian的条件数决定了收敛速度。交叉熵的Hessian条件数可能很大（当$p_y$很小时），而γ-交叉熵的Hessian条件数更加温和。</p>
<h3 id="11">11. 贝叶斯视角<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>最大后验估计（MAP）</strong>：</p>
<p>假设模型参数$\theta$有先验分布$p(\theta)$，MAP估计为：
\begin{equation}
\hat{\theta}<em>{\text{MAP}} = \arg\max</em>\theta p(\theta | \mathcal{D}) = \arg\max_\theta p(\mathcal{D} | \theta) p(\theta) \tag{44}
\end{equation}</p>
<p>等价于最小化：
\begin{equation}
\mathcal{L}_{\text{MAP}} = -\log p(\mathcal{D} | \theta) - \log p(\theta) \tag{45}
\end{equation}</p>
<p><strong>γ-交叉熵的先验解释</strong>：</p>
<p>可以证明，γ-交叉熵等价于使用某种隐式先验的MAP估计。</p>
<p>具体地，如果我们假设：
\begin{equation}
p(\theta) \propto \exp\left(-\lambda \sum_i \frac{1}{p_\theta(y_i | x_i)}\right) \tag{46}
\end{equation}</p>
<p>那么MAP等价于最小化：
\begin{equation}
-\log p(\mathcal{D} | \theta) + \lambda \sum_i \frac{1}{p_\theta(y_i | x_i)} \tag{47}
\end{equation}</p>
<p>这与γ-交叉熵的形式类似（见式32）。</p>
<h3 id="12">12. 信息论视角<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<p><strong>KL散度分解</strong>：</p>
<p>交叉熵可以分解为：
\begin{equation}
\mathcal{L}<em>{\text{CE}} = H(q, p</em>\theta) = H(q) + D_{\text{KL}}(q | p_\theta) \tag{48}
\end{equation}</p>
<p>其中$q$是真实分布（one-hot），$H(q) = 0$，所以：
\begin{equation}
\mathcal{L}<em _text_KL="\text{KL">{\text{CE}} = D</em>
\end{equation}}}(q | p_\theta) \tag{49</p>
<p><strong>γ-交叉熵的信息论解释</strong>：</p>
<p>γ-交叉熵可以理解为最小化一个<strong>修正的KL散度</strong>：
\begin{equation}
D_{\gamma\text{-KL}}(q | p_\theta) = -\frac{1}{1-\gamma}\sum_i q_i \log[\gamma + (1-\gamma)p_{\theta,i}] \tag{50}
\end{equation}</p>
<p>这不是标准的Bregman散度，但具有类似的性质。</p>
<p><strong>互信息最大化</strong>：</p>
<p>从信息论角度，学习目标是最大化输入$X$和标签$Y$之间的互信息：
\begin{equation}
I(X; Y) = H(Y) - H(Y|X) \tag{51}
\end{equation}</p>
<p>交叉熵直接最小化条件熵$H(Y|X)$，而γ-交叉熵通过正则化防止过度减小$H(Y|X)$，从而保持一定的不确定性。</p>
<h3 id="13-mixupcutout">13. 与Mixup和Cutout的关系<a class="toc-link" href="#13-mixupcutout" title="Permanent link">&para;</a></h3>
<p><strong>Mixup</strong>：</p>
<p>对于两个样本$(x_1, y_1)$和$(x_2, y_2)$，生成：
\begin{equation}
\tilde{x} = \lambda x_1 + (1-\lambda)x_2, \quad \tilde{y} = \lambda y_1 + (1-\lambda)y_2 \tag{52}
\end{equation}</p>
<p>其中$\lambda \sim \text{Beta}(\alpha, \alpha)$。</p>
<p><strong>Mixup的损失</strong>：
\begin{equation}
\mathcal{L}<em>{\text{Mixup}} = -\tilde{y} \log p</em>\theta(\tilde{y} | \tilde{x}) - (1-\tilde{y})\log(1 - p_\theta(\tilde{y} | \tilde{x})) \tag{53}
\end{equation}</p>
<p>这是一个<strong>软标签</strong>损失！</p>
<p><strong>与γ-交叉熵结合</strong>：</p>
<p>可以将两者结合：
\begin{equation}
\mathcal{L}<em>{\gamma\text{-Mixup}} = -\frac{\log[\gamma + (1-\gamma)\tilde{y} p</em>\theta(\tilde{y} | \tilde{x})]}{1-\gamma} \tag{54}
\end{equation}</p>
<p><strong>Cutout</strong>：</p>
<p>Cutout随机遮挡输入图像的一部分。这可以理解为一种<strong>数据增强</strong>，增加了输入的不确定性。</p>
<p>与γ-交叉熵的协同效应：
- Cutout增加输入不确定性
- γ-交叉熵保持输出不确定性</p>
<p>两者互补，进一步提高泛化性能。</p>
<h3 id="14">14. 超参数选择策略<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<p><strong>γ的选择依据</strong>：</p>
<ol>
<li>
<p><strong>任务类型</strong>：
   - 从零训练：$\gamma \in [0, 10^{-4}]$（接近标准交叉熵）
   - 微调预训练模型：$\gamma \in [0.1, 0.8]$（更大的平滑）</p>
</li>
<li>
<p><strong>数据质量</strong>：
   - 干净标签：小$\gamma$
   - 噪声标签：大$\gamma$（如$\gamma \geq 0.5$）</p>
</li>
<li>
<p><strong>模型容量</strong>：
   - 大模型：大$\gamma$（防止过拟合）
   - 小模型：小$\gamma$（保持拟合能力）</p>
</li>
</ol>
<p><strong>自适应γ策略</strong>：</p>
<p>定义<strong>动态γ</strong>：
\begin{equation}
\gamma(t) = \gamma_{\min} + (\gamma_{\max} - \gamma_{\min}) \cdot \frac{t}{T} \tag{55}
\end{equation}</p>
<p>其中$t$是当前epoch，$T$是总epoch数。</p>
<p><strong>直觉</strong>：开始时用小$\gamma$快速收敛，后期用大$\gamma$提高泛化。</p>
<p><strong>自适应调整（基于验证集）</strong>：
\begin{equation}
\gamma \leftarrow \gamma + \eta \cdot \text{sign}(\text{ECE}_{\text{val}}) \tag{56}
\end{equation}</p>
<p>其中ECE是验证集上的校准误差。</p>
<h3 id="15">15. 梯度累积与批量大小的影响<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>批量梯度</strong>：</p>
<p>对于batch $\mathcal{B}$，总梯度为：
\begin{equation}
\nabla_\theta \mathcal{L}<em _in="\in" _mathcal_B="\mathcal{B" i="i">{\text{batch}} = \frac{1}{|\mathcal{B}|} \sum</em>
\end{equation}}} \nabla_\theta \mathcal{L}_{\gamma\text{-CE}}(x_i, y_i) \tag{57</p>
<p><strong>大batch vs. 小batch</strong>：</p>
<ul>
<li><strong>大batch</strong>：梯度更稳定，但可能陷入尖锐的极小值（泛化差）</li>
<li><strong>小batch</strong>：梯度有噪声，倾向于平坦的极小值（泛化好）</li>
</ul>
<p><strong>γ-交叉熵的影响</strong>：</p>
<p>γ-交叉熵通过减小梯度幅度，起到了类似<strong>小batch</strong>的正则化效果，即使在大batch下也能找到平坦的极小值。</p>
<h3 id="16">16. 温度缩放与后处理校准<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<p><strong>Temperature Scaling</strong>：</p>
<p>对于已训练的模型，可以通过温度参数$T$校准输出：
\begin{equation}
p_T(y|x) = \frac{e^{z_y / T}}{\sum_i e^{z_i / T}} \tag{58}
\end{equation}</p>
<p>其中$z_i$是logits。通过最小化验证集上的负对数似然来优化$T$。</p>
<p><strong>与γ-交叉熵的比较</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>时机</th>
<th>额外参数</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>γ-交叉熵</td>
<td>训练时</td>
<td>$\gamma$（每个任务）</td>
<td>同时提高准确率和校准</td>
</tr>
<tr>
<td>Temperature Scaling</td>
<td>后处理</td>
<td>$T$（单个标量）</td>
<td>只改善校准，不改变准确率</td>
</tr>
</tbody>
</table>
<p><strong>结合使用</strong>：</p>
<p>可以先用γ-交叉熵训练，再用Temperature Scaling微调，进一步提高校准性能。</p>
<h3 id="17">17. 理论收敛性分析<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<p><strong>假设</strong>：损失函数$\mathcal{L}$是$L$-smooth的，即：
\begin{equation}
|\nabla \mathcal{L}(\theta_1) - \nabla \mathcal{L}(\theta_2)| \leq L |\theta_1 - \theta_2| \tag{59}
\end{equation}</p>
<p><strong>定理</strong>（SGD的收敛）：</p>
<p>对于$L$-smooth且凸的损失函数，使用学习率$\eta &lt; \frac{1}{L}$的SGD，经过$T$步后：
\begin{equation}
\mathbb{E}[\mathcal{L}(\bar{\theta}_T)] - \mathcal{L}(\theta^<em>) \leq \frac{L |\theta_0 - \theta^</em>|^2}{2T} + \frac{\eta \sigma^2}{2} \tag{60}
\end{equation}</p>
<p>其中$\bar{\theta}<em t="1">T = \frac{1}{T}\sum</em>^T \theta_t$，$\sigma^2$是梯度方差。</p>
<p><strong>γ-交叉熵的优势</strong>：</p>
<p>由于梯度有界（式21），$\sigma^2$更小，因此收敛误差更小！</p>
<h3 id="18">18. 数值稳定性实现<a class="toc-link" href="#18" title="Permanent link">&para;</a></h3>
<p><strong>LogSumExp技巧</strong>：</p>
<p>对于softmax，标准实现：
\begin{equation}
p_i = \frac{e^{z_i}}{\sum_j e^{z_j}} \tag{61}
\end{equation}</p>
<p>数值稳定版本：
\begin{equation}
p_i = \frac{e^{z_i - z_{\max}}}{\sum_j e^{z_j - z_{\max}}} \tag{62}
\end{equation}</p>
<p><strong>γ-交叉熵的实现</strong>：</p>
<p>\begin{equation}
\mathcal{L}_{\gamma\text{-CE}} = -\frac{\log[\gamma + (1-\gamma)p_y]}{1-\gamma} \tag{63}
\end{equation}</p>
<p>当$\gamma \to 0$时，分子分母都趋向0，需要特殊处理：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gamma_cross_entropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    y_true: 真实标签 (batch_size,)</span>
<span class="sd">    y_pred: 预测logits (batch_size, num_classes)</span>
<span class="sd">    gamma: 平滑参数</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 计算softmax概率</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 获取真实类别的概率</span>
    <span class="n">p_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y_true</span><span class="p">)[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y_true</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># 避免数值问题</span>
    <span class="n">p_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">p_y</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">gamma</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="c1"># 当gamma接近0时，使用标准交叉熵</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># γ-交叉熵</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">gamma</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">*</span> <span class="n">p_y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</code></pre></div>

<h3 id="19">19. 实验设计与消融研究<a class="toc-link" href="#19" title="Permanent link">&para;</a></h3>
<p><strong>消融实验设置</strong>：</p>
<ol>
<li><strong>Baseline</strong>: 标准交叉熵（$\gamma = 0$）</li>
<li><strong>变体1</strong>: 小γ（$\gamma = 0.01$）</li>
<li><strong>变体2</strong>: 中γ（$\gamma = 0.1$）</li>
<li><strong>变体3</strong>: 大γ（$\gamma = 0.5$）</li>
<li><strong>变体4</strong>: 动态γ（从0到0.5）</li>
</ol>
<p><strong>评估指标</strong>：</p>
<ol>
<li><strong>准确率</strong>：$\text{Acc} = \frac{1}{N}\sum_i \mathbb{1}[y_i = \hat{y}_i]$</li>
<li><strong>ECE</strong>：预期校准误差（式27）</li>
<li><strong>负对数似然</strong>：$\text{NLL} = -\frac{1}{N}\sum_i \log p_\theta(y_i | x_i)$</li>
<li><strong>Brier分数</strong>：$\text{BS} = \frac{1}{N}\sum_i |\boldsymbol{y}_i - \boldsymbol{p}_i|^2$</li>
</ol>
<p><strong>可视化</strong>：</p>
<ol>
<li><strong>可靠性图</strong>：置信度vs.准确率</li>
<li><strong>损失曲线</strong>：训练/验证损失随epoch变化</li>
<li><strong>梯度范数</strong>：$|\nabla_\theta \mathcal{L}|$随训练变化</li>
</ol>
<h3 id="20">20. 理论开放问题<a class="toc-link" href="#20" title="Permanent link">&para;</a></h3>
<p><strong>问题1</strong>：γ-交叉熵的最优$\gamma$选择</p>
<p>是否存在理论最优的$\gamma^*$作为数据分布和模型架构的函数？</p>
<p><strong>问题2</strong>：与其他散度的关系</p>
<p>γ-交叉熵是否可以理解为某种f-散度或Bregman散度的特例？</p>
<p><strong>问题3</strong>：泛化界</p>
<p>能否为γ-交叉熵推导更紧的泛化界（generalization bound）？</p>
<p><strong>问题4</strong>：多任务学习</p>
<p>在多任务学习中，不同任务是否应该使用不同的$\gamma$？如何自动确定？</p>
<h3 id="21">21. 扩展：多分类到多标签<a class="toc-link" href="#21" title="Permanent link">&para;</a></h3>
<p><strong>多标签γ-交叉熵</strong>：</p>
<p>对于多标签分类，每个类别独立，可以定义：
\begin{equation}
\mathcal{L}<em i="1">{\gamma\text{-BCE}} = -\sum</em>
\end{equation}}^K \frac{\log[\gamma + (1-\gamma)\sigma(z_i)^{y_i}(1-\sigma(z_i))^{1-y_i}]}{1-\gamma} \tag{64</p>
<p>其中$\sigma(z_i)$是第$i$个类别的sigmoid输出。</p>
<p><strong>简化形式</strong>：</p>
<p>对于每个类别$i$：
\begin{equation}
\mathcal{L}_{\gamma\text{-BCE},i} = \begin{cases}
-\frac{\log[\gamma + (1-\gamma)\sigma(z_i)]}{1-\gamma}, &amp; y_i = 1 \
-\frac{\log[\gamma + (1-\gamma)(1-\sigma(z_i))]}{1-\gamma}, &amp; y_i = 0
\end{cases} \tag{65}
\end{equation}</p>
<h3 id="22-stop_gradient-">22. 连接：从stop_gradient到γ-交叉熵<a class="toc-link" href="#22-stop_gradient-" title="Permanent link">&para;</a></h3>
<p><strong>原论文的stop_gradient版本</strong>：</p>
<p>\begin{equation}
\mathcal{L}<em>{\text{sg}} = -\underbrace{\frac{p</em>\theta(y|x)}{\gamma + (1-\gamma)p_\theta(y|x)}}<em>{\text{stop_grad}} \log p</em>\theta(y|x) \tag{66}
\end{equation}</p>
<p><strong>梯度</strong>：
\begin{equation}
\nabla_\theta \mathcal{L}<em>{\text{sg}} = -\frac{p</em>\theta(y|x)}{\gamma + (1-\gamma)p_\theta(y|x)} \cdot \frac{1}{p_\theta(y|x)} \nabla_\theta p_\theta(y|x) = -\frac{\nabla_\theta p_\theta(y|x)}{\gamma + (1-\gamma)p_\theta(y|x)} \tag{67}
\end{equation}</p>
<p>这正是式(14)！</p>
<p><strong>与γ-交叉熵的区别</strong>：
- stop_gradient版本：直接定义梯度，损失函数本身没有意义
- γ-交叉熵：损失函数有明确意义，梯度是自然导出的</p>
<h3 id="23">23. 实践建议总结<a class="toc-link" href="#23" title="Permanent link">&para;</a></h3>
<p><strong>推荐配置</strong>：</p>
<ol>
<li>
<p><strong>图像分类（从零训练）</strong>：
   - $\gamma \in [0, 0.01]$
   - 结合Label Smoothing ($\alpha = 0.1$)</p>
</li>
<li>
<p><strong>图像分类（微调）</strong>：
   - $\gamma \in [0.1, 0.3]$
   - 可选：动态γ schedule</p>
</li>
<li>
<p><strong>NLP任务</strong>：
   - 语言模型：$\gamma \approx 10^{-7}$（接近标准CE）
   - 机器翻译：$\gamma \approx 0.1$
   - 文本摘要：$\gamma \approx 0.8$</p>
</li>
<li>
<p><strong>噪声标签场景</strong>：
   - 轻度噪声（&lt;10%）：$\gamma \approx 0.2$
   - 中度噪声（10-30%）：$\gamma \approx 0.5$
   - 重度噪声（&gt;30%）：$\gamma \approx 0.7$</p>
</li>
</ol>
<p><strong>调试checklist</strong>：
- [ ] 验证损失单调下降
- [ ] 检查梯度范数是否有界
- [ ] 在验证集上评估ECE
- [ ] 绘制可靠性图
- [ ] 与baseline（$\gamma=0$）对比</p>
<hr />
<h2 id="_7">参考文献<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<ol>
<li>Müller et al., "When Does Label Smoothing Help?", NeurIPS 2019</li>
<li>Guo et al., "On Calibration of Modern Neural Networks", ICML 2017</li>
<li>Zhang &amp; Sabuncu, "Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels", NeurIPS 2018</li>
<li>Pereyra et al., "Regularizing Neural Networks by Penalizing Confident Output Distributions", ICLR 2017</li>
<li>Szegedy et al., "Rethinking the Inception Architecture for Computer Vision", CVPR 2016</li>
</ol>
<h2 id="_8">总结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>本文深入分析了γ-交叉熵损失函数，关键洞察包括：</p>
<ol>
<li><strong>梯度视角</strong>：γ-CE在交叉熵和准确率的梯度之间插值</li>
<li><strong>正则化</strong>：等价于添加隐式正则项，防止过度自信</li>
<li><strong>校准性</strong>：改善模型的概率校准，降低ECE</li>
<li><strong>鲁棒性</strong>：梯度和Hessian有界，对噪声标签更鲁棒</li>
<li><strong>灵活性</strong>：超参数$\gamma$可根据任务和数据质量调整</li>
</ol>
<p>γ-交叉熵是一个简洁而强大的工具，在实践中值得尝试！</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="tiger一个抠到极致的优化器.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#221 Tiger：一个“抠”到极致的优化器</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="为什么现在的llm都是decoder-only的架构.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#223 为什么现在的LLM都是Decoder-only的架构？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">缓解交叉熵过度自信的一个简明方案</a><ul>
<li><a href="#_2">结果简介</a></li>
<li><a href="#_3">个人推导</a></li>
<li><a href="#_4">多扯几句</a></li>
<li><a href="#_5">文章小结</a></li>
<li><a href="#_6">详细数学推导与理论分析</a><ul>
<li><a href="#1">1. 交叉熵的过度自信问题</a></li>
<li><a href="#2-">2. γ-交叉熵的定义与推导</a></li>
<li><a href="#3-">3. 从梯度视角理解γ-交叉熵</a></li>
<li><a href="#4-">4. γ-交叉熵的性质</a></li>
<li><a href="#5-label-smoothing">5. 与Label Smoothing的联系</a></li>
<li><a href="#6-calibration">6. 校准理论（Calibration）</a></li>
<li><a href="#7">7. 正则化效应的数学证明</a></li>
<li><a href="#8">8. 与其他鲁棒损失函数的对比</a></li>
<li><a href="#9">9. 梯度裁剪的几何解释</a></li>
<li><a href="#10-hessian">10. Hessian分析与收敛性</a></li>
<li><a href="#11">11. 贝叶斯视角</a></li>
<li><a href="#12">12. 信息论视角</a></li>
<li><a href="#13-mixupcutout">13. 与Mixup和Cutout的关系</a></li>
<li><a href="#14">14. 超参数选择策略</a></li>
<li><a href="#15">15. 梯度累积与批量大小的影响</a></li>
<li><a href="#16">16. 温度缩放与后处理校准</a></li>
<li><a href="#17">17. 理论收敛性分析</a></li>
<li><a href="#18">18. 数值稳定性实现</a></li>
<li><a href="#19">19. 实验设计与消融研究</a></li>
<li><a href="#20">20. 理论开放问题</a></li>
<li><a href="#21">21. 扩展：多分类到多标签</a></li>
<li><a href="#22-stop_gradient-">22. 连接：从stop_gradient到γ-交叉熵</a></li>
<li><a href="#23">23. 实践建议总结</a></li>
</ul>
</li>
<li><a href="#_7">参考文献</a></li>
<li><a href="#_8">总结</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>