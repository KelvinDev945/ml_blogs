<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>矩阵的有效秩（Effective Rank） | ML & Math Blog Posts</title>
    <meta name="description" content="矩阵的有效秩（Effective Rank）&para;
原文链接: https://spaces.ac.cn/archives/10847
发布日期: 

秩（Rank）是线性代数中的重要概念，它代表了矩阵的内在维度。然而，数学上对秩的严格定义，很多时候并不完全适用于数值计算场景，因为秩等于非零奇异值的个数，而数学上对“等于零”这件事的理解跟数值计算有所不同，数学上的“等于零”是绝对地、严格地等...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #171 矩阵的有效秩（Effective Rank）
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#171</span>
                矩阵的有效秩（Effective Rank）
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-04-10</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=矩阵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 矩阵</span>
                </a>
                
                <a href="../index.html?tags=熵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 熵</span>
                </a>
                
                <a href="../index.html?tags=稀疏" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 稀疏</span>
                </a>
                
                <a href="../index.html?tags=低秩" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 低秩</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="effective-rank">矩阵的有效秩（Effective Rank）<a class="toc-link" href="#effective-rank" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10847">https://spaces.ac.cn/archives/10847</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>秩（Rank）是线性代数中的重要概念，它代表了矩阵的内在维度。然而，数学上对秩的严格定义，很多时候并不完全适用于数值计算场景，因为秩等于非零奇异值的个数，而数学上对“等于零”这件事的理解跟数值计算有所不同，数学上的“等于零”是绝对地、严格地等于零，哪怕是$10^{-100}$也是不等于零，但数值计算不一样，很多时候$10^{-10}$就可以当零看待。</p>
<p>因此，我们希望将秩的概念推广到更符合数值计算特性的形式，这便是有效秩（Effective Rank）概念的由来。</p>
<h2 id="_1">误差截断<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>需要指出的是，目前学术界对有效秩并没有统一的定义，接下来我们介绍的是一些从不同角度切入来定义有效秩的思路。对于实际问题，读者可以自行选择适合的定义来使用。</p>
<p>接下来我们主要从奇异值的角度来考虑秩。对于矩阵$\boldsymbol{M}\in\mathbb{R}^{n\times m}$，不失一般性，设$n\leq m$，那么它的标准秩为<br />
\begin{equation}\mathop{\text{rank}}(\boldsymbol{M}) \triangleq \max\{i\,|\,\sigma_i &gt; 0\}\leq n\end{equation}<br />
其中$\sigma_1\geq \sigma_2\geq \cdots\geq\sigma_n \geq 0$是$\boldsymbol{M}$的奇异值。直观上，有效秩的概念就是为了将接近于零的奇异值当成零处理，所以有效秩的一个基本属性是不超过标准秩，并且特殊情况下能退化成标准秩。满足该属性的一个简单的定义是<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M},\epsilon) \triangleq \max\{i\,|\,\sigma_i &gt; \epsilon\}\end{equation}<br />
然而，“大”与“小”的概念应该是相对的，1相对于0.01大，但相对于100就小了，所以看起来除以$\sigma_1$来标准化一下更科学：<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M},\epsilon) \triangleq \max\big\{i\,\big|\,\sigma_i/\sigma_1 &gt; \epsilon\big\}\end{equation}</p>
<p>除了直接对接近于零的奇异值进行截断外，我们还可以从低秩近似的角度来考虑这个问题。在<a href="/archives/10407">《低秩近似之路（二）：SVD》</a>我们证明了，一个矩阵的最优$r$秩近似就是只保留最大的$r$个奇异值的SVD结果。反过来，我们可以指定一个相对误差$\epsilon$，将有效秩定义为可以达到这个相对误差的最小秩：<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M},\epsilon) \triangleq \min\left\{ i\,\,\left|\,\,\sqrt{\left(\sum_{i=1}^r \sigma_i^2\right)\left/\left(\sum_{i=1}^n \sigma_i^2\right.\right)} \geq 1-\epsilon\right.\right\}\end{equation}<br />
这个定义的数值意义更清晰，但它只考虑了总体误差，所以有些例子反而不大优雅。比如$n\times n$的单位阵，我们期望它的有效秩总是$n$，因为每个奇异值都是等同的1，不应有任何截断行为。但采用上述定义的话，当$n$足够大（$1/n &lt; \epsilon$）时，有效秩就会小于$n$。</p>
<h2 id="_2">范数之比<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>上一节定义的有效秩虽然直观，但都依赖于一个超参数$\epsilon$，终究是不够简洁。现在我们的基本认知是有效秩的概念只能依赖于相对大小，所以问题等价为从$1\geq \sigma_2/\sigma_1\geq\cdots\geq\sigma_n/\sigma_1\geq 0$中构建有效秩。由于都在$[0,1]$内的特点，一个巧妙的想法是直接将它们求和<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq \sum_{i=1}^n\frac{\sigma_i}{\sigma_1}\end{equation}<br />
从<a href="/archives/10407">《低秩近似之路（二）：SVD》</a>我们知道，最大的奇异值$\sigma_1$是矩阵的<a href="https://en.wikipedia.org/wiki/Matrix_norm#Spectral_norm_(p_=_2)">谱范数（Spectral Norm）</a>，记为$\Vert\boldsymbol{M}\Vert_2$，而所有奇异值之和实际上也是一个矩阵范数，称为“<a href="https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms">核范数（Nuclear Norm）</a>”，通常记为$\Vert\boldsymbol{M}\Vert_<em>$，于是上式也可以简记为<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq \frac{\Vert\boldsymbol{M}\Vert_</em>}{\Vert\boldsymbol{M}\Vert_2}\label{eq:n-2}\end{equation}<br />
这最早的出处可能是<a href="https://papers.cool/arxiv/1501.01571">《An Introduction to Matrix Concentration Inequalities》</a>，在里边被称为“Intrinsic Dimension”，但相关性质在<a href="https://papers.cool/arxiv/0706.4138">《Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization》</a>已经被探讨过了。</p>
<p>类似地，我们也可以通过平方求和来构建有效秩：<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq \sum_{i=1}^n\frac{\sigma_i^2}{\sigma_1^2} = \frac{\Vert\boldsymbol{M}\Vert_F^2}{\Vert\boldsymbol{M}\Vert_2^2}\label{eq:f-2}\end{equation}<br />
这里的$\Vert\cdot\Vert_F$是$F$范数。该定义的出自应该是<a href="https://arxiv.org/abs/math/0503442">《Sampling from large matrices: an approach through geometric functional analysis》</a>，当时被称为“Numerical Rank”，如今更多称为“Stable Rank”，是比较流行的有效秩概念之一。</p>
<p>从计算复杂度来看，式$\eqref{eq:f-2}$比式$\eqref{eq:n-2}$更低。因为计算核范数需要全体奇异值，这意味着需要完整的SVD；而$\Vert\boldsymbol{M}\Vert_F^2$又等于矩阵所有元素的平方和，所以式$\eqref{eq:f-2}$主要的计算量是最大奇异值，这比计算全体奇异值成本更低。如果愿意，我们还可以将式$\eqref{eq:n-2}$、式$\eqref{eq:f-2}$推广到$k$次方求和，结果实际上是更一般的<a href="https://en.wikipedia.org/wiki/Schatten_norm">Schatten范数</a>与谱范数之比。</p>
<h2 id="_3">分布与熵<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>读者如果直接搜索“Effective Rank”，大概率会搜到论文<a href="https://www.eurasip.org/Proceedings/Eusipco/Eusipco2007/Papers/a5p-h05.pdf">《The Effective Rank: a Measure of Effective Dimensionality》</a>，这也是比较早探讨有效秩的文献，里边提出基于熵来定义有效秩。</p>
<p>首先，由于奇异值总是非负的，我们可以将它归一化得到一个概率分布：<br />
\begin{equation}p_i = \frac{\sigma_i^{\gamma}}{\sum_{j=1}^n \sigma_j^{\gamma}}\end{equation}<br />
其中$\gamma &gt; 0$，从文献看$\gamma=1$和$\gamma=2$都经常有人用（我们在<a href="/archives/10739">Moonlight</a>中用了$\gamma=2$），下面我们都以$\gamma=1$为例。有了概率分布，那么就可以计算信息熵（香农熵）：<br />
\begin{equation}H = -\sum_{i=1}^n p_i \log p_i\end{equation}<br />
回忆一下，熵的值域是$[0, \log n]$，取指数后则有$e^H \in [1, n]$，当分布是One Hot时$e^H=1$（只有一个非零奇异值），当分布均匀时$e^H=n$（全体奇异值相等），这正好是标准秩的两个特殊例子，这启发我们可以将有效秩定义为<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq e^H = \exp\left(-\sum_{i=1}^n p_i \log p_i\right)\label{eq:h-erank}\end{equation}<br />
代入$p_i$的定义，我们发现它可以进一步变换成<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) = \exp\left(\log\sum_{i=1}^n \sigma_i -\frac{\sum_{i=1}^n \sigma_i\log\sigma_i}{\sum_{i=1}^n \sigma_i}\right)\end{equation}<br />
很明显括号内的第一项$\exp$后就是$\Vert\boldsymbol{M}\Vert_*$；第二项是$\log\sigma_i$的加权平均，权重是$\sigma_i$，此时$\log\sigma_i$会约等于最大的$\log\sigma_1$，取指数后即$\sigma_1=\Vert\boldsymbol{M}\Vert_2$。所以，上式总的结果将会近似于式$\eqref{eq:n-2}$，这表明了基于熵来定义有效秩看似是一条截然不同的路径，但实际上跟上一节的范数之比有异曲同工之妙。</p>
<p>我们知道，标准秩满足三角不等式$\mathop{\text{rank}}(\boldsymbol{A}+\boldsymbol{B})\leq \mathop{\text{rank}}(\boldsymbol{A}) + \mathop{\text{rank}}(\boldsymbol{B})$，而原论文证明了，对于（半）正定对称矩阵$\boldsymbol{A},\boldsymbol{B}$，式$\eqref{eq:h-erank}$所定义的有效秩满足$\mathop{\text{erank}}(\boldsymbol{A}+\boldsymbol{B})\leq \mathop{\text{erank}}(\boldsymbol{A}) + \mathop{\text{erank}}(\boldsymbol{B})$，尚不清楚这个不等式是否可以推广到一般矩阵。目前看来，证明有效秩能否保持标准秩的一些不等式，是一件并不容易的事情。</p>
<h2 id="_4">稀疏指标<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>从上述一系列有效秩的定义中，尤其是从奇异值到分布再到熵的转变，相信已经有读者隐约意识到，有效秩跟稀疏性有明显的共性，实际上有效秩可以理解为奇异值向量的一个稀疏性度量，跟一般的稀疏性指标不同的是，我们会将值域对齐到$1\leq \mathop{\text{erank}}(\boldsymbol{M}) \leq \mathop{\text{rank}}(\boldsymbol{M}) \leq n$，使其跟秩的概念对齐，从而更直观地感知稀疏程度。</p>
<p>关于稀疏性度量，我们曾在<a href="/archives/9595">《如何度量数据的稀疏程度？》</a>有过比较系统的讨论，理论上那里边的结果都可以用来构造有效秩。事实上我们也是这样做的，“范数之比”一节中我们基于Schatten范数与谱范数之比来构建有效值，其实只相当于那篇文章的公式$(1)$，我们还可以其他公式如$(16)$，它相当于将有效秩定义为核范数和$F$范数之比的平方：<br />
\begin{equation}\mathop{\text{erank}}(\boldsymbol{M}) \triangleq \frac{\Vert\boldsymbol{M}\Vert_*^2}{\Vert\boldsymbol{M}\Vert_F^2} = \frac{(\sum_{i=1}^n\sigma_i)^2}{\sum_{i=1}^n\sigma_i^2}\end{equation}<br />
这同样满足$1\leq \mathop{\text{erank}}(\boldsymbol{M}) \leq \mathop{\text{rank}}(\boldsymbol{M})$，是一个可用的有效秩定义。</p>
<p>非常神奇，我们关于有效秩和稀疏性的认知，在无形之中达成了闭环。不得不说这是一种很美妙的体验：笔者开始学习稀疏性度量时对有效秩是一无所知的，而这几天在学习有效秩时，慢慢意识到它跟稀疏性本质是相通的，冥冥之中似乎有一种神秘力量，将我们在不同领域、不同学科中积累的知识悄然串联起来，最终汇聚在同一个正确的方向上。</p>
<h2 id="_5">文章小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文探讨了矩阵的有效秩（Effective Rank）概念，它是线性代数中矩阵的秩（Rank）概念在数值计算方面的延伸，能够更有效地度量矩阵的本质维度。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10847">https://spaces.ac.cn/archives/10847</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 10, 2025). 《矩阵的有效秩（Effective Rank） 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10847">https://spaces.ac.cn/archives/10847</a></p>
<p>@online{kexuefm-10847,<br />
title={矩阵的有效秩（Effective Rank）},<br />
author={苏剑林},<br />
year={2025},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/10847}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>在本节中，我们将对有效秩的各种定义进行深入的数学推导，探讨它们之间的内在联系，以及在信息论、矩阵理论和机器学习中的应用。</p>
<h3 id="_7">一、有效秩的基础定义与性质<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 奇异值分解的回顾<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>对于矩阵 $\boldsymbol{M}\in\mathbb{R}^{n\times m}$，设 $n\leq m$，其奇异值分解（SVD）为：
$$\boldsymbol{M} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T = \sum_{i=1}^{r} \sigma_i \boldsymbol{u}_i \boldsymbol{v}_i^T$$</p>
<p>其中：
- $r = \mathop{\text{rank}}(\boldsymbol{M})$ 是矩阵的代数秩
- $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r &gt; 0$ 是非零奇异值
- $\boldsymbol{u}_i \in \mathbb{R}^n$ 和 $\boldsymbol{v}_i \in \mathbb{R}^m$ 分别是左、右奇异向量</p>
<p>代数秩的定义为：
$$\mathop{\text{rank}}(\boldsymbol{M}) = |{i : \sigma_i &gt; 0}| = r$$</p>
<p>然而，在数值计算中，判断 $\sigma_i = 0$ 是不稳定的。例如，考虑奇异值序列 ${\sigma_i} = {1, 10^{-8}, 10^{-16}}$，虽然代数秩为3，但后两个奇异值在数值上可忽略。</p>
<h4 id="12">1.2 范数的定义与关系<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p>在探讨有效秩之前，我们先回顾几个重要的矩阵范数：</p>
<p><strong>谱范数（Spectral Norm）</strong>：
$$|\boldsymbol{M}|<em _124_boldsymbol_x="|\boldsymbol{x">2 = \sigma_1 = \max</em>|_2$$}|=1} |\boldsymbol{M}\boldsymbol{x</p>
<p><strong>Frobenius范数</strong>：
$$|\boldsymbol{M}|<em i="1">F = \sqrt{\sum</em>$$}^{r} \sigma_i^2} = \sqrt{\mathop{\text{tr}}(\boldsymbol{M}^T\boldsymbol{M})</p>
<p><strong>核范数（Nuclear Norm）</strong>：
$$|\boldsymbol{M}|<em i="1">* = \sum</em>)$$}^{r} \sigma_i = \mathop{\text{tr}}(\sqrt{\boldsymbol{M}^T\boldsymbol{M}</p>
<p><strong>Schatten $p$-范数</strong>（一般形式）：
$$|\boldsymbol{M}|<em i="1">p = \left(\sum</em>, \quad p \geq 1$$}^{r} \sigma_i^p\right)^{1/p</p>
<p>显然，谱范数、Frobenius范数和核范数分别是 Schatten $\infty$-范数、2-范数和1-范数。</p>
<p>这些范数满足如下不等式链：
$$|\boldsymbol{M}|<em>2 \leq |\boldsymbol{M}|_F \leq |\boldsymbol{M}|</em>* \leq \sqrt{r} |\boldsymbol{M}|_F \leq r |\boldsymbol{M}|_2$$</p>
<p><strong>推导</strong>：
- $|\boldsymbol{M}|<em i="1">2 = \sigma_1 \leq \sqrt{\sum</em>|}^r \sigma_i^2} = |\boldsymbol{M<em i="1">F$（显然）
- $|\boldsymbol{M}|_F^2 = \sum</em>|}^r \sigma_i^2 \leq \left(\sum_{i=1}^r \sigma_i\right)^2 = |\boldsymbol{M<em i="1"><em>^2$（当且仅当 $r=1$ 时等号成立）
- $|\boldsymbol{M}|_</em> = \sum</em>|}^r \sigma_i \leq \sqrt{r} \sqrt{\sum_{i=1}^r \sigma_i^2} = \sqrt{r} |\boldsymbol{M<em>F$（Cauchy-Schwarz不等式）
- $|\boldsymbol{M}|</em>* = \sum_{i=1}^r \sigma_i \leq r \sigma_1 = r |\boldsymbol{M}|_2$</p>
<h3 id="_8">二、基于范数比的有效秩<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<h4 id="21-stable-rank">2.1 稳定秩（Stable Rank）<a class="toc-link" href="#21-stable-rank" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：
$$\mathop{\text{srank}}(\boldsymbol{M}) = \frac{|\boldsymbol{M}|<em i="1">F^2}{|\boldsymbol{M}|_2^2} = \frac{\sum</em>$$}^r \sigma_i^2}{\sigma_1^2</p>
<p><strong>性质分析</strong>：</p>
<p>(1) <strong>值域</strong>：$1 \leq \mathop{\text{srank}}(\boldsymbol{M}) \leq r$</p>
<p><strong>证明</strong>：
- 下界：$\mathop{\text{srank}}(\boldsymbol{M}) = \frac{\sum_{i=1}^r \sigma_i^2}{\sigma_1^2} \geq \frac{\sigma_1^2}{\sigma_1^2} = 1$
- 上界：$\mathop{\text{srank}}(\boldsymbol{M}) = \frac{\sum_{i=1}^r \sigma_i^2}{\sigma_1^2} \leq \frac{r\sigma_1^2}{\sigma_1^2} = r$（当所有奇异值相等时取等号）</p>
<p>(2) <strong>极端情况</strong>：
- 当 $\boldsymbol{M}$ 秩1时（$\sigma_2 = \cdots = \sigma_r = 0$）：$\mathop{\text{srank}}(\boldsymbol{M}) = 1$
- 当所有非零奇异值相等时（$\sigma_1 = \sigma_2 = \cdots = \sigma_r$）：$\mathop{\text{srank}}(\boldsymbol{M}) = r$</p>
<p>(3) <strong>归一化不变性</strong>：对于任意 $c &gt; 0$，
$$\mathop{\text{srank}}(c\boldsymbol{M}) = \frac{|c\boldsymbol{M}|_F^2}{|c\boldsymbol{M}|_2^2} = \frac{c^2|\boldsymbol{M}|_F^2}{c^2|\boldsymbol{M}|_2^2} = \mathop{\text{srank}}(\boldsymbol{M})$$</p>
<h4 id="22-intrinsic-dimension">2.2 本征维度（Intrinsic Dimension）<a class="toc-link" href="#22-intrinsic-dimension" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：
$$\mathop{\text{irank}}(\boldsymbol{M}) = \frac{|\boldsymbol{M}|<em i="1">*}{|\boldsymbol{M}|_2} = \frac{\sum</em>$$}^r \sigma_i}{\sigma_1</p>
<p><strong>性质分析</strong>：</p>
<p>(1) <strong>值域</strong>：$1 \leq \mathop{\text{irank}}(\boldsymbol{M}) \leq r$</p>
<p><strong>证明</strong>：类似稳定秩，利用：
$$\sum_{i=1}^r \sigma_i \geq \sigma_1 \quad \text{且} \quad \sum_{i=1}^r \sigma_i \leq r\sigma_1$$</p>
<p>(2) <strong>与稳定秩的关系</strong>：</p>
<p>由Cauchy-Schwarz不等式：
$$\left(\sum_{i=1}^r \sigma_i\right)^2 \leq r \sum_{i=1}^r \sigma_i^2$$</p>
<p>因此：
$$\mathop{\text{irank}}(\boldsymbol{M})^2 = \frac{(\sum_{i=1}^r \sigma_i)^2}{\sigma_1^2} \leq r \cdot \frac{\sum_{i=1}^r \sigma_i^2}{\sigma_1^2} = r \cdot \mathop{\text{srank}}(\boldsymbol{M})$$</p>
<p>这说明本征维度通常小于或等于稳定秩（归一化后）。</p>
<h4 id="23-schatten">2.3 一般Schatten比<a class="toc-link" href="#23-schatten" title="Permanent link">&para;</a></h4>
<p>更一般地，我们可以定义：
$$\mathop{\text{erank}}<em i="1">p(\boldsymbol{M}) = \left(\frac{|\boldsymbol{M}|_p}{|\boldsymbol{M}|_2}\right)^p = \frac{\sum</em>$$}^r \sigma_i^p}{\sigma_1^p</p>
<p>特例：
- $p=1$：本征维度 $\mathop{\text{irank}}(\boldsymbol{M})$
- $p=2$：稳定秩 $\mathop{\text{srank}}(\boldsymbol{M})$
- $p\to\infty$：趋向于1</p>
<h3 id="_9">三、基于信息熵的有效秩<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 归一化奇异值分布<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>为了应用信息论的工具，我们首先将奇异值归一化为概率分布。考虑参数 $\gamma &gt; 0$，定义：
$$p_i = \frac{\sigma_i^\gamma}{\sum_{j=1}^r \sigma_j^\gamma} = \frac{\sigma_i^\gamma}{Z_\gamma}$$</p>
<p>其中配分函数：
$$Z_\gamma = \sum_{j=1}^r \sigma_j^\gamma = |\boldsymbol{M}|_\gamma^\gamma$$</p>
<p>显然 $p_i \geq 0$ 且 $\sum_{i=1}^r p_i = 1$，构成了一个离散概率分布。</p>
<p><strong>参数 $\gamma$ 的选择</strong>：
- $\gamma = 1$：线性归一化，对应核范数
- $\gamma = 2$：二次归一化，对应Frobenius范数
- $\gamma &gt; 1$：增强大奇异值的权重
- $\gamma &lt; 1$：增强小奇异值的权重</p>
<h4 id="32-shannon">3.2 Shannon熵及其有效秩<a class="toc-link" href="#32-shannon" title="Permanent link">&para;</a></h4>
<p><strong>Shannon熵</strong>定义为：
$$H(\boldsymbol{M}) = -\sum_{i=1}^r p_i \log p_i$$</p>
<p>其中对数底通常取 $e$ 或 2（本文统一使用自然对数）。</p>
<p><strong>熵的值域</strong>：$0 \leq H(\boldsymbol{M}) \leq \log r$</p>
<ul>
<li>最小值0：当某个 $p_i = 1$，其余为0（秩1矩阵）</li>
<li>最大值 $\log r$：当 $p_1 = p_2 = \cdots = p_r = 1/r$（均匀分布）</li>
</ul>
<p><strong>基于熵的有效秩</strong>定义为：
$$\mathop{\text{erank}}<em i="1">H(\boldsymbol{M}) = e^{H(\boldsymbol{M})} = \exp\left(-\sum</em>^r p_i \log p_i\right)$$</p>
<p>这样定义的有效秩满足 $1 \leq \mathop{\text{erank}}_H(\boldsymbol{M}) \leq r$。</p>
<h4 id="33">3.3 熵与范数的联系<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>现在我们证明熵定义的有效秩与范数比定义的有效秩之间的深刻联系。取 $\gamma = 1$，则：
$$p_i = \frac{\sigma_i}{\sum_{j=1}^r \sigma_j}$$</p>
<p>代入Shannon熵：
$$H = -\sum_{i=1}^r \frac{\sigma_i}{\sum_{j=1}^r \sigma_j} \log \frac{\sigma_i}{\sum_{j=1}^r \sigma_j}$$</p>
<p>展开对数：
$$H = -\sum_{i=1}^r \frac{\sigma_i}{\sum_{j=1}^r \sigma_j} \left(\log \sigma_i - \log \sum_{j=1}^r \sigma_j\right)$$</p>
<p>分离两项：
$$H = -\sum_{i=1}^r \frac{\sigma_i}{\sum_{j=1}^r \sigma_j} \log \sigma_i + \log \sum_{j=1}^r \sigma_j$$</p>
<p>记 $S = \sum_{j=1}^r \sigma_j = |\boldsymbol{M}|<em i="1">*$，则：
$$H = \log S - \frac{1}{S}\sum</em>^r \sigma_i \log \sigma_i$$</p>
<p>因此：
$$e^H = S \cdot \exp\left(-\frac{1}{S}\sum_{i=1}^r \sigma_i \log \sigma_i\right) = |\boldsymbol{M}|<em i="1"><em> \cdot \exp\left(-\frac{1}{|\boldsymbol{M}|_</em>}\sum</em>^r \sigma_i \log \sigma_i\right)$$</p>
<p><strong>关键观察</strong>：第二项是 $\log \sigma_i$ 的加权几何平均的指数。设：
$$\bar{\sigma}<em i="1">{\text{geom}} = \exp\left(\frac{\sum</em>\right)$$}^r \sigma_i \log \sigma_i}{\sum_{i=1}^r \sigma_i</p>
<p>这是以 $\sigma_i$ 为权重的几何平均。那么：
$$\mathop{\text{erank}}<em>H(\boldsymbol{M}) = \frac{|\boldsymbol{M}|</em>*}{\bar{\sigma}_{\text{geom}}}$$</p>
<p><strong>与本征维度的近似</strong>：</p>
<p>当奇异值衰减较快时，主导奇异值 $\sigma_1$ 远大于其他奇异值，此时：
$$\bar{\sigma}_{\text{geom}} \approx \sigma_1 = |\boldsymbol{M}|_2$$</p>
<p>因此：
$$\mathop{\text{erank}}<em>H(\boldsymbol{M}) \approx \frac{|\boldsymbol{M}|</em>*}{|\boldsymbol{M}|_2} = \mathop{\text{irank}}(\boldsymbol{M})$$</p>
<p>这揭示了<strong>熵定义和范数比定义的内在统一性</strong>。</p>
<h4 id="34">3.4 详细的近似分析<a class="toc-link" href="#34" title="Permanent link">&para;</a></h4>
<p>为了更精确地理解这个近似，我们进行Taylor展开分析。设：
$$\sigma_i = \sigma_1 \cdot t_i, \quad 1 = t_1 \geq t_2 \geq \cdots \geq t_r \geq 0$$</p>
<p>则：
$$\log \sigma_i = \log \sigma_1 + \log t_i$$</p>
<p>代入加权几何平均：
$$\bar{\sigma}<em i="1">{\text{geom}} = \exp\left(\frac{\sum</em>\right)$$}^r \sigma_i (\log \sigma_1 + \log t_i)}{\sum_{i=1}^r \sigma_i}\right) = \sigma_1 \exp\left(\frac{\sum_{i=1}^r \sigma_i \log t_i}{\sum_{i=1}^r \sigma_i</p>
<p>记修正因子：
$$\delta = \exp\left(\frac{\sum_{i=1}^r \sigma_i \log t_i}{\sum_{i=1}^r \sigma_i}\right) = \exp\left(\frac{\sigma_1 \sum_{i=1}^r t_i \log t_i}{\sigma_1 \sum_{i=1}^r t_i}\right) = \exp\left(\frac{\sum_{i=1}^r t_i \log t_i}{\sum_{i=1}^r t_i}\right)$$</p>
<p>则 $\bar{\sigma}<em>{\text{geom}} = \delta \cdot \sigma_1$，所以：
$$\mathop{\text{erank}}_H(\boldsymbol{M}) = \frac{|\boldsymbol{M}|</em>*}{\delta \cdot |\boldsymbol{M}|_2} = \frac{1}{\delta} \mathop{\text{irank}}(\boldsymbol{M})$$</p>
<p><strong>修正因子的性质</strong>：
- 当所有 $t_i = 1$ 时（均匀分布），$\delta = 1$
- 当 $t_2, \ldots, t_r \ll 1$ 时（快速衰减），$\delta &lt; 1$
- 总有 $0 &lt; \delta \leq 1$（因为 $\log t_i \leq 0$）</p>
<p>因此 $\mathop{\text{erank}}_H(\boldsymbol{M}) \geq \mathop{\text{irank}}(\boldsymbol{M})$，熵定义的有效秩通常略大于本征维度。</p>
<h3 id="renyitsallis">四、Rényi熵与Tsallis熵的推广<a class="toc-link" href="#renyitsallis" title="Permanent link">&para;</a></h3>
<h4 id="41-renyi">4.1 Rényi熵<a class="toc-link" href="#41-renyi" title="Permanent link">&para;</a></h4>
<p>Rényi熵是Shannon熵的参数化推广，定义为：
$$H_\alpha(\boldsymbol{M}) = \frac{1}{1-\alpha} \log \sum_{i=1}^r p_i^\alpha, \quad \alpha &gt; 0, \alpha \neq 1$$</p>
<p>当 $\alpha \to 1$ 时，Rényi熵收敛到Shannon熵：
$$\lim_{\alpha \to 1} H_\alpha(\boldsymbol{M}) = H(\boldsymbol{M})$$</p>
<p><strong>证明</strong>（L'Hôpital法则）：
$$\lim_{\alpha \to 1} H_\alpha = \lim_{\alpha \to 1} \frac{\log \sum_{i=1}^r p_i^\alpha}{1-\alpha}$$</p>
<p>分子分母求导：
$$= \lim_{\alpha \to 1} \frac{\frac{d}{d\alpha}\log \sum_{i=1}^r p_i^\alpha}{\frac{d}{d\alpha}(1-\alpha)} = \lim_{\alpha \to 1} \frac{\sum_{i=1}^r p_i^\alpha \log p_i}{\sum_{i=1}^r p_i^\alpha} \cdot (-1)$$</p>
<p>当 $\alpha = 1$：
$$= -\sum_{i=1}^r p_i \log p_i = H(\boldsymbol{M})$$</p>
<p><strong>基于Rényi熵的有效秩</strong>：
$$\mathop{\text{erank}}<em>{\alpha}(\boldsymbol{M}) = e^{H</em>\alpha(\boldsymbol{M})} = \left(\sum_{i=1}^r p_i^\alpha\right)^{\frac{1}{1-\alpha}}$$</p>
<p>取 $\gamma = 1$，$p_i = \sigma_i / |\boldsymbol{M}|<em><em>$：
$$\mathop{\text{erank}}<em i="1">{\alpha}(\boldsymbol{M}) = \left(\sum</em>|_}^r \left(\frac{\sigma_i}{|\boldsymbol{M</em>}\right)^\alpha\right)^{\frac{1}{1-\alpha}} = \frac{|\boldsymbol{M}|</em><em>}{\left(\sum_{i=1}^r \sigma_i^\alpha\right)^{\frac{1}{1-\alpha}}} \cdot \left(|\boldsymbol{M}|_</em>\right)^{\frac{\alpha-1}{1-\alpha}}$$</p>
<p>简化：
$$\mathop{\text{erank}}<em>{\alpha}(\boldsymbol{M}) = \frac{|\boldsymbol{M}|</em>*^{1/(\alpha)}}{(\sum_{i=1}^r \sigma_i^\alpha)^{1/\alpha}}$$</p>
<p>等等，让我重新计算。设 $p_i = \sigma_i / S$，其中 $S = |\boldsymbol{M}|<em i="1">*$，则：
$$\sum</em>^r \sigma_i^\alpha$$}^r p_i^\alpha = \sum_{i=1}^r \frac{\sigma_i^\alpha}{S^\alpha} = \frac{1}{S^\alpha} \sum_{i=1</p>
<p>因此：
$$\mathop{\text{erank}}<em i="1">{\alpha}(\boldsymbol{M}) = \left(\frac{1}{S^\alpha} \sum</em>$$}^r \sigma_i^\alpha\right)^{\frac{1}{1-\alpha}} = S^{\frac{\alpha}{1-\alpha}} \left(\sum_{i=1}^r \sigma_i^\alpha\right)^{\frac{1}{1-\alpha}</p>
<p><strong>特殊情况</strong>：</p>
<ul>
<li>$\alpha = 0$：$H_0 = \log r$，$\mathop{\text{erank}}_0 = r$（代数秩）</li>
<li>$\alpha = 2$：
$$\mathop{\text{erank}}<em i="1">2(\boldsymbol{M}) = \left(\sum</em>|}^r p_i^2\right)^{-1} = \frac{1}{\sum_{i=1}^r \left(\frac{\sigma_i}{|\boldsymbol{M<em i="1"><em>}\right)^2} = \frac{|\boldsymbol{M}|_</em>^2}{\sum</em>$$}^r \sigma_i^2} = \frac{|\boldsymbol{M}|_*^2}{|\boldsymbol{M}|_F^2</li>
</ul>
<p>这正是我们在"稀疏指标"一节中提到的另一种有效秩定义！</p>
<ul>
<li>$\alpha \to \infty$：$H_\infty = -\log \max_i p_i = -\log \frac{\sigma_1}{|\boldsymbol{M}|_<em>}$，
$$\mathop{\text{erank}}<em>\infty(\boldsymbol{M}) = \frac{|\boldsymbol{M}|</em></em>}{\sigma_1} = \mathop{\text{irank}}(\boldsymbol{M})$$</li>
</ul>
<h4 id="42-tsallis">4.2 Tsallis熵<a class="toc-link" href="#42-tsallis" title="Permanent link">&para;</a></h4>
<p>Tsallis熵是另一种熵的推广，定义为：
$$S_q(\boldsymbol{M}) = \frac{1}{q-1} \left(1 - \sum_{i=1}^r p_i^q\right), \quad q &gt; 0, q \neq 1$$</p>
<p>当 $q \to 1$ 时，Tsallis熵也收敛到Shannon熵。</p>
<p><strong>基于Tsallis熵的有效秩</strong>：类似地可以定义为：
$$\mathop{\text{erank}}<em i="1">q(\boldsymbol{M}) = \left(\sum</em>$$}^r p_i^q\right)^{\frac{1}{1-q}</p>
<p>注意到这与Rényi熵的有效秩形式完全相同（只是参数记号不同）。</p>
<h3 id="_10">五、信息论解释<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 有效秩作为等效均匀分布的大小<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>Shannon熵的一个重要性质是，它度量了分布的"等效均匀程度"。具体地，若 $H(p) = \log k$，则分布 $p$ 携带的信息量等价于一个大小为 $k$ 的均匀分布。</p>
<p>对于有效秩 $\mathop{\text{erank}}_H(\boldsymbol{M}) = e^H$，我们可以理解为：</p>
<blockquote>
<p>矩阵 $\boldsymbol{M}$ 的奇异值分布所携带的信息，等价于一个大小为 $\mathop{\text{erank}}_H(\boldsymbol{M})$ 的均匀分布。</p>
</blockquote>
<p>换句话说，虽然矩阵的代数秩可能是 $r$，但由于奇异值分布的不均匀性，其"有效维度"只有 $e^H \leq r$。</p>
<h4 id="52-pca">5.2 与主成分分析（PCA）的联系<a class="toc-link" href="#52-pca" title="Permanent link">&para;</a></h4>
<p>在PCA中，我们通常保留前 $k$ 个主成分使得：
$$\frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^r \lambda_i} \geq 1 - \epsilon$$</p>
<p>其中 $\lambda_i = \sigma_i^2$ 是特征值。这等价于：
$$\frac{\sum_{i=1}^k \sigma_i^2}{|\boldsymbol{M}|_F^2} \geq 1 - \epsilon$$</p>
<p>而稳定秩 $\mathop{\text{srank}}(\boldsymbol{M}) = |\boldsymbol{M}|_F^2 / \sigma_1^2$ 给出了一个"自动"的截断指标。</p>
<p><strong>解释</strong>：如果稳定秩为 $k$，则意味着矩阵的能量主要集中在前 $k$ 个奇异值上。</p>
<h4 id="53-mdl">5.3 最小描述长度（MDL）原理<a class="toc-link" href="#53-mdl" title="Permanent link">&para;</a></h4>
<p>从信息编码的角度，Shannon熵 $H$ 给出了对奇异值分布进行最优编码所需的平均比特数（以 $\log_2$ 为底）。有效秩 $e^H$ 则对应于"等效编码空间大小"。</p>
<p>若我们需要传输矩阵 $\boldsymbol{M}$ 的低秩近似，选择秩 $k = \lceil \mathop{\text{erank}}_H(\boldsymbol{M}) \rceil$ 可以在信息损失和压缩率之间取得平衡。</p>
<h3 id="_11">六、矩阵稀疏性的度量<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<h4 id="61-gini">6.1 奇异值的Gini系数<a class="toc-link" href="#61-gini" title="Permanent link">&para;</a></h4>
<p>Gini系数是经济学中度量不平等性的指标，也可用于度量奇异值的集中程度。定义：
$$G(\boldsymbol{M}) = \frac{\sum_{i=1}^r \sum_{j=1}^r |\sigma_i - \sigma_j|}{2r \sum_{i=1}^r \sigma_i} = \frac{\sum_{i=1}^r (2i - r - 1) \sigma_i}{r \sum_{i=1}^r \sigma_i}$$</p>
<p>其中第二个等式利用了奇异值的降序排列。</p>
<p><strong>性质</strong>：
- $0 \leq G(\boldsymbol{M}) \leq 1 - 1/r$
- $G = 0$：所有奇异值相等（最"均匀"）
- $G \to 1$：奇异值高度集中（最"稀疏"）</p>
<p><strong>与有效秩的关系</strong>：</p>
<p>高Gini系数对应低有效秩（奇异值集中在少数几个上），低Gini系数对应高有效秩（奇异值分布均匀）。可以证明：
$$\mathop{\text{erank}}(\boldsymbol{M}) \approx r(1 - G(\boldsymbol{M}))$$</p>
<h4 id="62">6.2 稀疏性度量的统一框架<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p>一般地，奇异值的稀疏性可以通过以下形式度量：
$$\mathop{\text{sparsity}}(\boldsymbol{M}) = 1 - \frac{\mathop{\text{erank}}(\boldsymbol{M})}{r}$$</p>
<p>这给出了一个归一化的稀疏度指标，值域为 $[0, 1-1/r]$。</p>
<h3 id="_12">七、与条件数的关系<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 条件数的定义<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p>矩阵的条件数定义为：
$$\kappa(\boldsymbol{M}) = \frac{\sigma_1}{\sigma_r} = \frac{|\boldsymbol{M}|_2}{|\boldsymbol{M}^{-1}|_2^{-1}}$$</p>
<p>（对于非满秩矩阵，$\sigma_r$ 取最小的非零奇异值）</p>
<p>条件数度量了矩阵的"病态程度"：
- $\kappa = 1$：正交矩阵（或其倍数），最"健康"
- $\kappa \gg 1$：病态矩阵，数值不稳定</p>
<h4 id="72">7.2 条件数与有效秩的权衡<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p>考虑一个简化模型：设矩阵有 $k$ 个奇异值等于 $\sigma$，其余 $r-k$ 个等于 $\epsilon \sigma$（$\epsilon \ll 1$）。则：
$$\mathop{\text{srank}}(\boldsymbol{M}) = \frac{k\sigma^2 + (r-k)\epsilon^2\sigma^2}{\sigma^2} = k + (r-k)\epsilon^2 \approx k$$</p>
<p>$$\kappa(\boldsymbol{M}) = \frac{\sigma}{\epsilon\sigma} = \frac{1}{\epsilon}$$</p>
<p>可见：
- 高有效秩（$k \approx r$）+ 低条件数（$\epsilon \approx 1$）：理想情况
- 低有效秩（$k \ll r$）+ 高条件数（$\epsilon \ll 1$）：典型的低秩+病态矩阵</p>
<p><strong>一般关系</strong>：</p>
<p>对于稳定秩，我们有：
$$\mathop{\text{srank}}(\boldsymbol{M}) \cdot \kappa(\boldsymbol{M})^{-2} = \frac{\sum_{i=1}^r \sigma_i^2}{\sigma_1^2} \cdot \frac{\sigma_r^2}{\sigma_1^2} = \frac{\sigma_r^2 \sum_{i=1}^r \sigma_i^2}{\sigma_1^4}$$</p>
<p>这表明稳定秩和条件数的倒数平方成正比关系（在固定其他奇异值时）。</p>
<h4 id="73">7.3 数值稳定性分析<a class="toc-link" href="#73" title="Permanent link">&para;</a></h4>
<p>在求解线性系统 $\boldsymbol{M}\boldsymbol{x} = \boldsymbol{b}$ 时，相对误差界为：
$$\frac{|\Delta \boldsymbol{x}|}{|\boldsymbol{x}|} \leq \kappa(\boldsymbol{M}) \frac{|\Delta \boldsymbol{b}|}{|\boldsymbol{b}|}$$</p>
<p>如果 $\mathop{\text{srank}}(\boldsymbol{M}) \ll r$，我们可以用稳定秩对应的低秩近似 $\tilde{\boldsymbol{M}}$ 来替代 $\boldsymbol{M}$，从而：
- 减少计算复杂度：从 $O(r^3)$ 到 $O(k^3)$，其中 $k \approx \mathop{\text{srank}}(\boldsymbol{M})$
- 改善数值稳定性：$\kappa(\tilde{\boldsymbol{M}}) &lt; \kappa(\boldsymbol{M})$（因为去除了小奇异值）</p>
<h3 id="_13">八、在低秩近似中的应用<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 最优低秩近似的误差界<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p>Eckart-Young-Mirsky定理指出，矩阵 $\boldsymbol{M}$ 的最优秩 $k$ 近似（在Frobenius范数下）为：
$$\boldsymbol{M}<em i="1">k = \sum</em>_i^T$$}^k \sigma_i \boldsymbol{u}_i \boldsymbol{v</p>
<p>其误差为：
$$|\boldsymbol{M} - \boldsymbol{M}<em i="k+1">k|_F^2 = \sum</em>^r \sigma_i^2$$</p>
<p><strong>相对误差</strong>：
$$\frac{|\boldsymbol{M} - \boldsymbol{M}<em i="k+1">k|_F^2}{|\boldsymbol{M}|_F^2} = \frac{\sum</em>$$}^r \sigma_i^2}{\sum_{i=1}^r \sigma_i^2</p>
<p>若取 $k = \lfloor \mathop{\text{srank}}(\boldsymbol{M}) \rfloor$，则：
$$\sum_{i=1}^k \sigma_i^2 \approx \mathop{\text{srank}}(\boldsymbol{M}) \cdot \sigma_1^2$$</p>
<p><strong>推导</strong>：设 $k = \mathop{\text{srank}}(\boldsymbol{M})$，即 $\sum_{i=1}^r \sigma_i^2 = k \sigma_1^2$。假设前 $k$ 个奇异值相对均匀，则：
$$\sum_{i=1}^k \sigma_i^2 \approx k \cdot \bar{\sigma}^2, \quad \sum_{i=k+1}^r \sigma_i^2 \approx (k - k) \sigma_1^2 = 0$$</p>
<p>因此相对误差较小。</p>
<h4 id="82">8.2 随机化低秩近似<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>在大规模矩阵计算中，完整的SVD代价高昂。随机化算法利用有效秩来指导采样：</p>
<p><strong>算法框架</strong>：
1. 估计有效秩 $k \approx \mathop{\text{erank}}(\boldsymbol{M})$（通过快速估计 $|\boldsymbol{M}|_F$ 和 $|\boldsymbol{M}|_2$）
2. 生成随机矩阵 $\boldsymbol{\Omega} \in \mathbb{R}^{m \times (k+p)}$（$p$ 是过采样参数）
3. 计算 $\boldsymbol{Y} = \boldsymbol{M}\boldsymbol{\Omega}$
4. 正交化得到 $\boldsymbol{Q}$，使得 $\boldsymbol{M} \approx \boldsymbol{Q}\boldsymbol{Q}^T\boldsymbol{M}$</p>
<p><strong>误差分析</strong>：</p>
<p>期望误差界为：
$$\mathbb{E}\left[|\boldsymbol{M} - \boldsymbol{Q}\boldsymbol{Q}^T\boldsymbol{M}|<em k_1="k+1">F\right] \leq \left(1 + \frac{k}{p-1}\right)^{1/2} \sigma</em>$$</p>
<p>若 $k = \mathop{\text{srank}}(\boldsymbol{M})$，则 $\sigma_{k+1}$ 通常很小，保证了好的近似质量。</p>
<h3 id="_14">九、在神经网络中的解释性<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 权重矩阵的有效秩<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p>在深度神经网络中，权重矩阵 $\boldsymbol{W} \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$ 的有效秩反映了其"表达容量"。</p>
<p><strong>观察</strong>：
- 训练初期：$\mathop{\text{erank}}(\boldsymbol{W}) \approx \min(d_{\text{out}}, d_{\text{in}})$（接近满秩）
- 训练收敛：$\mathop{\text{erank}}(\boldsymbol{W}) \ll \min(d_{\text{out}}, d_{\text{in}})$（显著降秩）</p>
<p>这种"隐式正则化"现象表明，梯度下降倾向于找到低有效秩的解。</p>
<h4 id="92">9.2 注意力矩阵的有效秩<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p>在Transformer中，注意力矩阵 $\boldsymbol{A} = \text{softmax}(\boldsymbol{Q}\boldsymbol{K}^T/\sqrt{d})$ 的有效秩度量了"注意力多样性"：</p>
<ul>
<li>高有效秩：注意力分散，模型考虑了更多的上下文信息</li>
<li>低有效秩：注意力集中，模型只关注少数关键位置</li>
</ul>
<p><strong>计算示例</strong>：</p>
<p>设序列长度为 $n$，若注意力均匀分布（$A_{ij} = 1/n$），则：
$$\mathop{\text{srank}}(\boldsymbol{A}) = \frac{\sum_{i,j} A_{ij}^2}{\max_{i,j} A_{ij}^2} = \frac{n \cdot n \cdot (1/n)^2}{(1/n)^2} = n$$</p>
<p>若注意力只集中在一个位置（$A_{i,j_0} = 1$，其余为0），则：
$$\mathop{\text{srank}}(\boldsymbol{A}) = 1$$</p>
<p><strong>实践意义</strong>：监控注意力矩阵的有效秩可以帮助诊断模型是否过拟合或欠拟合。</p>
<h4 id="93">9.3 梯度协方差矩阵<a class="toc-link" href="#93" title="Permanent link">&para;</a></h4>
<p>在优化过程中，梯度的协方差矩阵 $\boldsymbol{C} = \mathbb{E}[\nabla L \nabla L^T]$ 的有效秩反映了"有效梯度维度"：</p>
<p>$$\mathop{\text{erank}}(\boldsymbol{C}) \ll d \implies \text{梯度位于低维子空间}$$</p>
<p>这启发了诸如低秩自适应（LoRA）等参数高效微调方法。</p>
<h3 id="_15">十、有效秩的梯度与优化<a class="toc-link" href="#_15" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 稳定秩的梯度<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p>考虑将稳定秩作为正则化项：
$$\mathcal{R}(\boldsymbol{M}) = \mathop{\text{srank}}(\boldsymbol{M}) = \frac{|\boldsymbol{M}|_F^2}{|\boldsymbol{M}|_2^2}$$</p>
<p>要计算其梯度，我们需要分别对分子和分母求导。</p>
<p><strong>Frobenius范数的梯度</strong>：
$$\frac{\partial |\boldsymbol{M}|_F^2}{\partial \boldsymbol{M}} = 2\boldsymbol{M}$$</p>
<p><strong>谱范数的梯度</strong>：</p>
<p>设 $\sigma_1$ 对应的左右奇异向量为 $\boldsymbol{u}_1, \boldsymbol{v}_1$，则：
$$\frac{\partial |\boldsymbol{M}|_2}{\partial \boldsymbol{M}} = \boldsymbol{u}_1 \boldsymbol{v}_1^T$$</p>
<p>因此：
$$\frac{\partial |\boldsymbol{M}|_2^2}{\partial \boldsymbol{M}} = 2\sigma_1 \boldsymbol{u}_1 \boldsymbol{v}_1^T$$</p>
<p><strong>稳定秩的梯度</strong>（商法则）：
$$\frac{\partial \mathcal{R}}{\partial \boldsymbol{M}} = \frac{2\boldsymbol{M} \cdot |\boldsymbol{M}|_2^2 - |\boldsymbol{M}|_F^2 \cdot 2\sigma_1 \boldsymbol{u}_1 \boldsymbol{v}_1^T}{|\boldsymbol{M}|_2^4}$$</p>
<p>简化：
$$\frac{\partial \mathcal{R}}{\partial \boldsymbol{M}} = \frac{2}{|\boldsymbol{M}|_2^2} \left(\boldsymbol{M} - \mathop{\text{srank}}(\boldsymbol{M}) \cdot \sigma_1 \boldsymbol{u}_1 \boldsymbol{v}_1^T\right)$$</p>
<p><strong>解释</strong>：梯度包含两项：
1. $\boldsymbol{M}$：增加所有奇异值
2. $-\mathop{\text{srank}}(\boldsymbol{M}) \cdot \sigma_1 \boldsymbol{u}_1 \boldsymbol{v}_1^T$：减少最大奇异值</p>
<p>这促使奇异值分布更均匀，从而增大有效秩。</p>
<h4 id="102">10.2 核范数的梯度（参考）<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p>核范数常用于低秩正则化，其梯度为：
$$\frac{\partial |\boldsymbol{M}|_*}{\partial \boldsymbol{M}} = \boldsymbol{U}\boldsymbol{V}^T$$</p>
<p>其中 $\boldsymbol{M} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T$ 是SVD分解。</p>
<p><strong>证明</strong>：利用 $|\boldsymbol{M}|_* = \mathop{\text{tr}}(\sqrt{\boldsymbol{M}^T\boldsymbol{M}})$，通过矩阵微分计算得到。</p>
<h4 id="103">10.3 最大化有效秩的优化问题<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p>在某些应用中（如正则化、多样性增强），我们希望最大化有效秩：
$$\max_{\boldsymbol{M}} \mathop{\text{srank}}(\boldsymbol{M}) \quad \text{s.t.} \quad |\boldsymbol{M}|_F^2 = C$$</p>
<p>其中 $C$ 是常数约束。</p>
<p><strong>解</strong>：由于 $\mathop{\text{srank}}(\boldsymbol{M}) = |\boldsymbol{M}|<em _boldsymbol_M="\boldsymbol{M">F^2 / \sigma_1^2$，在 $|\boldsymbol{M}|_F^2$ 固定时，最大化有效秩等价于最小化 $\sigma_1^2$，即：
$$\min</em>^r \sigma_i^2 = C$$}} \sigma_1^2 \quad \text{s.t.} \quad \sum_{i=1</p>
<p><strong>最优解</strong>：所有奇异值相等，$\sigma_1 = \sigma_2 = \cdots = \sigma_r = \sqrt{C/r}$，此时 $\mathop{\text{srank}}(\boldsymbol{M}) = r$（最大值）。</p>
<h4 id="104">10.4 有效秩作为正则化项<a class="toc-link" href="#104" title="Permanent link">&para;</a></h4>
<p>在机器学习中，我们可以在损失函数中添加有效秩的惩罚：
$$\mathcal{L}<em _text_task="\text{task">{\text{total}} = \mathcal{L}</em>))$$}} + \lambda \cdot f(\mathop{\text{erank}}(\boldsymbol{W</p>
<p>其中 $f$ 是单调函数：
- $f(x) = -x$：鼓励高有效秩（多样性）
- $f(x) = x$：鼓励低有效秩（稀疏性）</p>
<p><strong>梯度下降更新</strong>：
$$\boldsymbol{W}^{(t+1)} = \boldsymbol{W}^{(t)} - \eta \left(\frac{\partial \mathcal{L}_{\text{task}}}{\partial \boldsymbol{W}} + \lambda f'(\mathop{\text{erank}}(\boldsymbol{W}^{(t)})) \frac{\partial \mathop{\text{erank}}}{\partial \boldsymbol{W}}\right)$$</p>
<h4 id="105-trace-norm">10.5 Trace-Norm正则化与有效秩<a class="toc-link" href="#105-trace-norm" title="Permanent link">&para;</a></h4>
<p>核范数（Trace Norm）正则化：
$$\min_{\boldsymbol{M}} \mathcal{L}(\boldsymbol{M}) + \mu |\boldsymbol{M}|_*$$</p>
<p>等价于对所有奇异值施加 $L_1$ 惩罚，促使矩阵低秩（小有效秩）。</p>
<p><strong>与本征维度的关系</strong>：</p>
<p>核范数正则化趋于减小 $|\boldsymbol{M}|_<em>$，同时在许多情况下 $|\boldsymbol{M}|<em>2$ 相对稳定，因此：
$$\mathop{\text{irank}}(\boldsymbol{M}) = \frac{|\boldsymbol{M}|</em></em>}{|\boldsymbol{M}|_2} \downarrow$$</p>
<p>这解释了为何核范数正则化能有效降低模型复杂度。</p>
<h3 id="_16">十一、总结与进一步的推广<a class="toc-link" href="#_16" title="Permanent link">&para;</a></h3>
<h4 id="111">11.1 各种有效秩定义的比较<a class="toc-link" href="#111" title="Permanent link">&para;</a></h4>
<p>我们总结几种主要的有效秩定义：</p>
<table>
<thead>
<tr>
<th>定义</th>
<th>公式</th>
<th>计算复杂度</th>
<th>性质</th>
</tr>
</thead>
<tbody>
<tr>
<td>稳定秩</td>
<td>$|\boldsymbol{M}|_F^2 / |\boldsymbol{M}|_2^2$</td>
<td>低（只需最大奇异值）</td>
<td>常用，可导</td>
</tr>
<tr>
<td>本征维度</td>
<td>$|\boldsymbol{M}|_* / |\boldsymbol{M}|_2$</td>
<td>高（需全部奇异值）</td>
<td>理论性质好</td>
</tr>
<tr>
<td>Shannon熵</td>
<td>$\exp(-\sum p_i \log p_i)$</td>
<td>高（需全部奇异值）</td>
<td>信息论意义</td>
</tr>
<tr>
<td>Rényi熵</td>
<td>$(\sum p_i^\alpha)^{1/(1-\alpha)}$</td>
<td>高</td>
<td>可调参数</td>
</tr>
<tr>
<td>核/F范数比</td>
<td>$|\boldsymbol{M}|_*^2 / |\boldsymbol{M}|_F^2$</td>
<td>高</td>
<td>稀疏性度量</td>
</tr>
</tbody>
</table>
<h4 id="112">11.2 开放问题<a class="toc-link" href="#112" title="Permanent link">&para;</a></h4>
<p>尽管有效秩已有广泛应用，仍有许多理论问题待解决：</p>
<ol>
<li>
<p><strong>三角不等式的推广</strong>：哪些有效秩定义满足 $\mathop{\text{erank}}(\boldsymbol{A}+\boldsymbol{B}) \leq \mathop{\text{erank}}(\boldsymbol{A}) + \mathop{\text{erank}}(\boldsymbol{B})$？</p>
</li>
<li>
<p><strong>乘积的有效秩</strong>：$\mathop{\text{erank}}(\boldsymbol{AB})$ 与 $\mathop{\text{erank}}(\boldsymbol{A}), \mathop{\text{erank}}(\boldsymbol{B})$ 的关系？</p>
</li>
<li>
<p><strong>张量的有效秩</strong>：如何推广到高阶张量？</p>
</li>
<li>
<p><strong>计算优化</strong>：能否不计算全部奇异值而准确估计熵定义的有效秩？</p>
</li>
</ol>
<h4 id="113">11.3 应用展望<a class="toc-link" href="#113" title="Permanent link">&para;</a></h4>
<p>有效秩在现代机器学习中有诸多潜在应用：</p>
<ul>
<li><strong>模型压缩</strong>：基于有效秩的自适应剪枝</li>
<li><strong>联邦学习</strong>：通讯效率与有效秩的权衡</li>
<li><strong>生成模型</strong>：扩散模型中的秩坍缩问题</li>
<li><strong>可解释性</strong>：通过有效秩理解模型的"内在维度"</li>
</ul>
<p>有效秩作为连接线性代数、信息论和机器学习的桥梁，将在理论和实践中继续发挥重要作用。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="通过梯度近似寻找normalization的替代品.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#170 通过梯度近似寻找Normalization的替代品</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="svd的导数.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#172 SVD的导数</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#effective-rank">矩阵的有效秩（Effective Rank）</a><ul>
<li><a href="#_1">误差截断</a></li>
<li><a href="#_2">范数之比</a></li>
<li><a href="#_3">分布与熵</a></li>
<li><a href="#_4">稀疏指标</a></li>
<li><a href="#_5">文章小结</a></li>
<li><a href="#_6">公式推导与注释</a><ul>
<li><a href="#_7">一、有效秩的基础定义与性质</a></li>
<li><a href="#_8">二、基于范数比的有效秩</a></li>
<li><a href="#_9">三、基于信息熵的有效秩</a></li>
<li><a href="#renyitsallis">四、Rényi熵与Tsallis熵的推广</a></li>
<li><a href="#_10">五、信息论解释</a></li>
<li><a href="#_11">六、矩阵稀疏性的度量</a></li>
<li><a href="#_12">七、与条件数的关系</a></li>
<li><a href="#_13">八、在低秩近似中的应用</a></li>
<li><a href="#_14">九、在神经网络中的解释性</a></li>
<li><a href="#_15">十、有效秩的梯度与优化</a></li>
<li><a href="#_16">十一、总结与进一步的推广</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>