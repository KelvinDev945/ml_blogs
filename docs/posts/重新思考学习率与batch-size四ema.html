<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>重新思考学习率与Batch Size（四）：EMA | ML & Math Blog Posts</title>
    <meta name="description" content="重新思考学习率与Batch Size（四）：EMA
原文链接: https://spaces.ac.cn/archives/11301
发布日期: 2025-09-22

我们在《重新思考学习率与Batch Size（二）：平均场》中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在《配置不同的学习率，LoRA...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">重新思考学习率与Batch Size（四）：EMA</h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-09-22</span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/11301" target="_blank">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                
            </div>
            
        </header>

        <!-- Post Body -->
        <div class="post-content">
            <h1 id="batch-sizeema">重新思考学习率与Batch Size（四）：EMA</h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11301">https://spaces.ac.cn/archives/11301</a></p>
<p><strong>发布日期</strong>: 2025-09-22</p>
<hr />
<p>我们在<a href="https://kexue.fm/archives/11280">《重新思考学习率与Batch Size（二）：平均场》</a>中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在<a href="https://kexue.fm/archives/10001">《配置不同的学习率，LoRA还能再涨一点？》</a>、<a href="https://kexue.fm/archives/10770">《初探MuP：超参数的跨模型尺度迁移规律》</a>等地方我们也用了这个简化。</p>
<p>然而，SignSGD真是Adam的良好近似吗？一个明显差异是SignSGD的Update RMS总是1，而Adam并非如此。笔者发现，导致这一差异的核心原因是动量，它普遍存在于Adam、Lion、Muon等优化器中。所以，本文我们来考察动量——更广义地说是EMA——的影响。</p>
<h2 id="_1">问题分析</h2>
<p>从Adam的视角看，SignSGD对应$\beta_1=\beta_2=0$这个特例，或者对应于Adam的第一步更新量（不管$\beta_1,\beta_2$如何）。因此，我们认为它跟Adam肯定有一些共性，能够捕捉到一些通用的规律。</p>
<p><a href="https://spaces.ac.cn/archives/11301" title="重新思考学习率与Batch Size（四）：EMA">[...]</a></p>
<hr />
<h2 id="_2">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>

        <!-- Back to Home -->
        <div class="text-center mt-5 mb-4">
            <a href="../index.html" class="btn btn-outline-primary">
                <i class="fas fa-arrow-left"></i> 返回首页
            </a>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>
</body>
</html>
