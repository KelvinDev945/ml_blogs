<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>生成扩散模型漫谈（二十七）：将步长作为条件输入 | ML & Math Blog Posts</title>
    <meta name="description" content="生成扩散模型漫谈（二十七）：将步长作为条件输入&para;
原文链接: https://spaces.ac.cn/archives/10617
发布日期: 

这篇文章我们再次聚焦于扩散模型的采样加速。众所周知，扩散模型的采样加速主要有两种思路，一是开发更高效的求解器，二是事后蒸馏。然而，据笔者观察，除了上两篇文章介绍过的SiD外，这两种方案都鲜有能将生成步数降低到一步的结果。虽然SiD能做到单步...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=微分方程">微分方程</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #301 生成扩散模型漫谈（二十七）：将步长作为条件输入
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#301</span>
                生成扩散模型漫谈（二十七）：将步长作为条件输入
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-12-15</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=微分方程" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 微分方程</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=采样" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 采样</span>
                </a>
                
                <a href="../index.html?tags=扩散" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 扩散</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">生成扩散模型漫谈（二十七）：将步长作为条件输入<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10617">https://spaces.ac.cn/archives/10617</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>这篇文章我们再次聚焦于扩散模型的采样加速。众所周知，扩散模型的采样加速主要有两种思路，一是开发更高效的求解器，二是事后蒸馏。然而，据笔者观察，除了上两篇文章介绍过的<a href="/archives/10085">SiD</a>外，这两种方案都鲜有能将生成步数降低到一步的结果。虽然SiD能做到单步生成，但它需要额外的蒸馏成本，并且蒸馏过程中用到了类似GAN的交替训练过程，总让人感觉差点意思。</p>
<p>本文要介绍的是<a href="https://papers.cool/arxiv/2410.12557">《One Step Diffusion via Shortcut Models》</a>，其突破性思想是将生成步长也作为扩散模型的条件输入，然后往训练目标中加入了一个直观的正则项，这样就能直接稳定训练出可以单步生成模型，可谓简单有效的经典之作。</p>
<h2 id="ode">ODE扩散<a class="toc-link" href="#ode" title="Permanent link">&para;</a></h2>
<p>原论文的结论是基于ODE式扩散模型的，而对于ODE式扩散的理论基础，我们在本系列的<a href="/archives/9228">（六）</a>、<a href="/archives/9280">（十二）</a>、<a href="/archives/9370">（十四）</a>、<a href="/archives/9379">（十五）</a>、<a href="/archives/9497">（十七）</a>等博客中已经多次介绍，其中最简单的一种理解方式大概是<a href="/archives/9497">（十七）</a>中的ReFlow视角，下面我们简单重复一下。</p>
<p>假设$\boldsymbol{x}<em>0\sim p_0(\boldsymbol{x}_0)$是先验分布采样的 _随机噪声</em> ，$\boldsymbol{x}<em>1\sim p_1(\boldsymbol{x}_1)$是目标分布采样的 _真实样本</em> （注：前面的文章中，普通都是$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">T$是噪声、$\boldsymbol{x}_0$是目标样本，这里方便起见反过来了），ReFlow允许我们指定任意从$\boldsymbol{x}_0$到$\boldsymbol{x}_1$的运动轨迹，最简单的轨迹自然是直线：<br />
\begin{equation}\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t \boldsymbol{x}_1\label{eq:line}\end{equation}<br />
两边求导，就可以得到它满足的ODE（常微分方程）：<br />
\begin{equation}\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{x}_1 - \boldsymbol{x}_0\end{equation}<br />
这个ODE很简单，但实际上没用，因为我们想要的是通过ODE由$\boldsymbol{x}_0$生成$\boldsymbol{x}_1$，而上述ODE却显式地依赖$\boldsymbol{x}_1$。为了解决这个问题，一个很简单的想法是“学一个$\boldsymbol{x}_t$的函数去逼近$\boldsymbol{x}_1 - \boldsymbol{x}_0$”，学完之后就用它来取代$\boldsymbol{x}_1 - \boldsymbol{x}_0$，即<br />
\begin{equation}\boldsymbol{\theta}^<em> = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\left[\Vert\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\Vert^2\right]\label{eq:loss}\end{equation}<br />
以及<br />
\begin{equation}\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{x}_1 - \boldsymbol{x}_0\quad\Rightarrow\quad\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{v}</em>^</em>}(\boldsymbol{x}_t, t)\label{eq:ode-core}\end{equation}<br />
这就是ReFlow。当然这里边还欠缺了一个理论证明，就是通过平方误差来拟合$\boldsymbol{v}</em>_t, t)$所得到的ODE确实能生成我们期望的分布，这部分大家自行看}}(\boldsymbol{x<a href="/archives/9497">《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》</a>就好。</p>
<h2 id="_2">步长自洽<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>假设我们已经有了$\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)$，那么通过求解微分方程$\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{v}</em>}}(\boldsymbol{x<em _="+" _epsilon="\epsilon" t="t">t, t)$就可以实现从$\boldsymbol{x}_0$到$\boldsymbol{x}_1$的变换。划重点，是“微分方程”，但实际上我们没法真的去数值计算微分方程，而是只能算“差分方程”：<br />
\begin{equation}\boldsymbol{x}</em>} - \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t = \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) \epsilon\label{eq:de}\end{equation}<br />
这个差分方程是原始ODE的“欧拉近似”，近似程度取决于步长$\epsilon$的大小，当$\epsilon\to 0$时就精确等于原始ODE，换言之步长越小越精确。然而，生成步数等于$1/\epsilon$，我们希望生成步数越少越好，这意味着不能用太大的步长，最好$\epsilon$可以等于1，这样$\boldsymbol{x}_1 = \boldsymbol{x}_0 + \boldsymbol{v}</em>_0, 0)$，一步就可以完成生成。}}(\boldsymbol{x</p>
<p>问题是，如果直接用大步长代入上式，最终所算得的$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1$必然会严重偏离精确解。这时候原论文（下称“Shortcut模型”）的巧妙构思就登场了：它认为模型$\boldsymbol{v}</em>}}(\boldsymbol{x<em _="+" _epsilon="\epsilon" t="t">t, t)$不应该只是$\boldsymbol{x}_t$和$t$的函数，还应该是步长$\epsilon$的函数，这样差分方程$\eqref{eq:de}$就可以自行适应步长：<br />
\begin{equation}\boldsymbol{x}</em>} - \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t = \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t, t, \epsilon) \epsilon\end{equation}<br />
目标$\eqref{eq:loss}$训练的是精确的ODE模型，所以它训练的是$\epsilon=0$的模型：<br />
\begin{equation}\mathcal{L}_1 = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\left[\frac{1}{2}\Vert\boldsymbol{v}</em>}}(\boldsymbol{x<em>t, t, 0) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\Vert^2\right]\end{equation}<br />
那$\epsilon &gt; 0$的部分又怎么训练呢？我们的目标是生成步数越少越好，这等价于说希望“ _两倍的步长走1步</em> 等于 <em>单倍的步长走2步</em> ”：<br />
\begin{equation}\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t + \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, 2\epsilon) 2\epsilon = \color{green}{\underbrace{\boldsymbol{x}_t + \boldsymbol{v}</em>}}(\boldsymbol{x<em _tilde_boldsymbol_x="\tilde{\boldsymbol{x">t, t, \epsilon) \epsilon}</em>}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon}}} + \boldsymbol{v}</em>}}\big(\color{green}{\underbrace{\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t + \boldsymbol{v}</em>}}(\boldsymbol{x<em _tilde_boldsymbol_x="\tilde{\boldsymbol{x">t, t, \epsilon) \epsilon}</em>}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon}}}, t+\epsilon, \epsilon\big) \epsilon\label{eq:cond}\end{equation}<br />
即$\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, 2\epsilon) = [\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, \epsilon) + \boldsymbol{v}</em>}}(\color{green}{\tilde{\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">{t+\epsilon}}, t+\epsilon, \epsilon)] /2$。为了达到这个目标，我们补充一项自洽性损失函数<br />
\begin{equation}\mathcal{L}_2 = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\left[\Vert\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, 2\epsilon) - [\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, \epsilon)+ \boldsymbol{v}</em>}}(\color{green}{\tilde{\boldsymbol{x}}_{t+\epsilon}}, t+\epsilon, \epsilon) ]/2\Vert^2\right]\end{equation
$\mathcal{L}_1$与$\mathcal{L}_2$相加，就构成了Shortcut模型的损失函数。</p>
<p>（注：有读者指出，更早的<a href="https://papers.cool/arxiv/2310.02279">《Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion》</a>提出过以离散化时间的起点和终点作为条件输入的做法，指定起点和终点后步长其实也就确定了，所以Shortcut以步长为输入的做法并不算完全创新。）</p>
<h2 id="_3">模型细节<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>以上基本就是Shortcut模型的全部理论内容，非常精巧且简明，但从理论到实验，还需要一些细节，比如步长$\epsilon$如何融入到模型中去。</p>
<p>首先，在训练$\mathcal{L}_2$时，Shortcut并没有均匀地从$[0,1]$采样$\epsilon$，而是设置了一个最小步长$2^{-7}$，然后将它们倍增至1，即所有的非零步长只有$\{2^{-7},2^{-6},2^{-5},2^{-4},2^{-3},2^{-2},2^{-1},1\}$这8个值，从前7个中均匀采样来训练$\mathcal{L}_2$。这样一来，$\epsilon$的取值就是有限的，算上$0$一共就只有9个，所以Shortcut模型直接以Embedding的方式来输入$\epsilon$，将它跟$t$的Embedding加在一起。</p>
<p>其次，注意到$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">2$的计算量是比$\mathcal{L}_1$大的，因为$\boldsymbol{v}</em>_1$更好训练，所以它的训练样本可以适当少些。}}(\tilde{\boldsymbol{x}}_{t+\epsilon}, t, \epsilon)$这一项需要两次前向传播，所以论文的做法是每个batch中$3/4$的样本都用来计算$\mathcal{L}_1$，剩下的$1/4$样本才用来算$\mathcal{L}_2$。该操作不仅是为了节省计算量，实际上还调节了$\mathcal{L}_1,\mathcal{L}_2$的权重，因为$\mathcal{L}_2$比$\mathcal{L</p>
<p>除此之外，论文在实践的时候还对$\mathcal{L}<em _boldsymbol_x="\boldsymbol{x">2$做了微调，多加了个stop gradient算子：<br />
\begin{equation}\mathcal{L}_2 = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\left[\Vert\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, 2\epsilon) - \color{skyblue}{\text{sg}[}\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, \epsilon)+ \boldsymbol{v}</em>}}(\color{green}{\tilde{\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon}}, t+\epsilon, \epsilon) \color{skyblue}{]}/2\Vert^2\right]\end{equation}<br />
为什么要这样做呢？按照作者的<a href="https://openreview.net/forum?id=OlzB6LnXcS¬eId=k4If3csXST">回复</a>，这是自引导学习的常见做法，被stop gradient的部分属于目标，不应该有梯度，跟<a href="https://papers.cool/arxiv/2006.07733">BYOL</a>、<a href="https://papers.cool/arxiv/2011.10566">SimSiam</a>等无监督学习方案类似。不过照笔者看来，这个操作最大的价值还是节省训练成本，因为$\boldsymbol{v}</em>, t, \epsilon)$这一项做了两次前向传播，如果要对它反向传播，计算量也要翻倍。}}(\tilde{\boldsymbol{x}}_{t+\epsilon</p>
<h2 id="_4">实验效果<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>现在我们来看Shortcut模型的实验效果，看起来它是目前单步生成效果最好的、单阶段训练的扩散模型：  </p>
<p><a href="/usr/uploads/2024/12/2449555374.png" title="点击查看原图"><img alt="各种扩散模型的生成质量评估" src="/usr/uploads/2024/12/2449555374.png" /></a></p>
<p>各种扩散模型的生成质量评估</p>
<p>这是它的实际采样效果图：  </p>
<p><a href="/usr/uploads/2024/12/3428964799.jpg" title="点击查看原图"><img alt="Flow Matching与Shortcut Model的实际采样效果对比" src="/usr/uploads/2024/12/3428964799.jpg" /></a></p>
<p>Flow Matching与Shortcut Model的实际采样效果对比</p>
<p>不过仔细观察单步生成的样本就会发现，其实还有明显的瑕疵，所以说虽然Shortcut模型相比于之前的单阶段训练方案来说已经取得了较大的进步，但还有明显的提升空间。</p>
<p>作者已经将Shortcut模型的代码开源，Github链接是：</p>
<blockquote>
<p><strong><a href="https://github.com/kvfrans/shortcut-models">https://github.com/kvfrans/shortcut-models</a></strong></p>
</blockquote>
<p>顺便说，Shortcut模型投到了ICLR 2025上，获得了reviewer的一致好评（全8分）。</p>
<h2 id="_5">延伸思考<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>看到Shortcut模型，不知道大家想到了哪些相关工作？笔者想到了一个可能大家都意想不到的，那就是我们在<a href="/archives/9881">《生成扩散模型漫谈（二十一）：中值定理加速ODE采样》</a>介绍过的AMED。</p>
<p>Shortcut模型与AMED的底层思想是相通的，它们都已经发现，单靠研究复杂的高阶求解器，将生成的NFE（模型的运行次数）降低到个位数就已经很简单了，更不用说做单步生成了。所以它们一致认为，真正要变的并不是求解器，而是模型。该怎么变呢？AMED想到的是“中值定理”：对ODE两端积分，我们有精确的<br />
\begin{equation}\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t + \epsilon} - \boldsymbol{x}_t = \int_t^{t + \epsilon}\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{\tau}, \tau) d\tau\end{equation}<br />
类比“<a href="https://en.wikipedia.org/wiki/Mean_value_theorem#Mean_value_theorems_for_definite_integrals">积分中值定理</a>”，我们能找到一个$s\in[t, t + \epsilon]$，成立<br />
\begin{equation}\frac{1}{\epsilon}\int_t^{t + \epsilon}\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{\tau}, \tau) d\tau = \boldsymbol{v}</em>}}(\boldsymbol{x<em _="+" _epsilon="\epsilon" t="t">s, s)\end{equation}<br />
于是我们得到<br />
\begin{equation}\boldsymbol{x}</em>} - \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t = \boldsymbol{v}</em>}}(\boldsymbol{x}_s, s) \epsilon\end{equation
当然，积分中值定理实际上只对标量函数成立，对向量函数是不保证成立的，所以说是“类比”。现在的问题是并不知道$s$的值，所以AMED的后续做法是用一个非常小的（计算量几乎可以忽略的）模型去预测$s$。</p>
<p>AMED是基于现成扩散模型的事后修正方法，因此它的效果取决于中值定理对$\boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)$模型的成立程度，这显得有些“运气成分”，并且AMED需要先用欧拉格式预估一下$\boldsymbol{x}_s$，所以它的NFE最少是2，不能做到单步生成。相比之下，Shortcut模型更“激进”，它直接把步长作为条件输入，将加速生成的条件$\eqref{eq:cond}$作为损失函数，这样一来不仅避免了“中值定理”近似的可行性讨论，还使得最少NFE可以降低到1。</p>
<p>更巧妙的是，细思之下我们会发现两者的做法其实也有些共性，前面我们说了Shortcut是直接将$\epsilon$转成Embedding加到$t$的Embeddding上的，这不相当于跟AMED一样都是修改$t$嘛！只不过AMED是直接修改$t$的数值，而Shortcut修改的是$t$的Embedding。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文介绍了一个单阶段训练就可以实现单步生成的扩散模型新工作，它的突破思想是将步长也当成条件输入到扩散模型中，并配以一个直观的正则项，这样只通过单阶段训练就可以得到单步生成的扩散模型。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10617">https://spaces.ac.cn/archives/10617</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Dec. 15, 2024). 《生成扩散模型漫谈（二十七）：将步长作为条件输入 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10617">https://spaces.ac.cn/archives/10617</a></p>
<p>@online{kexuefm-10617,<br />
title={生成扩散模型漫谈（二十七）：将步长作为条件输入},<br />
author={苏剑林},<br />
year={2024},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/10617}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<h3 id="1">1. 核心概念与数学框架<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11-reflow">1.1 ReFlow框架的基础<a class="toc-link" href="#11-reflow" title="Permanent link">&para;</a></h4>
<p>ReFlow框架允许我们在噪声分布 $\boldsymbol{x}_0 \sim p_0(\boldsymbol{x}_0)$ 和目标分布 $\boldsymbol{x}_1 \sim p_1(\boldsymbol{x}_1)$ 之间构建任意的运动轨迹。最简单的选择是直线轨迹：</p>
<p>\begin{equation}
\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t \boldsymbol{x}_1, \quad t \in [0,1] \tag{1}
\end{equation}</p>
<p><strong>数学直觉</strong>：这个线性插值公式确保了：
- 当 $t=0$ 时，$\boldsymbol{x}_0$ 是纯噪声
- 当 $t=1$ 时，$\boldsymbol{x}_1$ 是真实样本
- 中间时刻 $t \in (0,1)$ 是从噪声到真实样本的平滑过渡</p>
<p>对式 (1) 关于时间 $t$ 求导，得到速度场：</p>
<p>\begin{equation}
\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{x}_1 - \boldsymbol{x}_0 \tag{2}
\end{equation}</p>
<p><strong>证明</strong>：
\begin{equation}
\frac{d\boldsymbol{x}_t}{dt} = \frac{d}{dt}[(1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1] = -\boldsymbol{x}_0 + \boldsymbol{x}_1 = \boldsymbol{x}_1 - \boldsymbol{x}_0 \tag{3}
\end{equation}</p>
<h4 id="12">1.2 速度场学习问题<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p>式 (2) 中的ODE显式依赖于目标 $\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1$，这在生成阶段是不可知的。为此，我们引入参数化的速度场 $\boldsymbol{v}</em>_0$。}}(\boldsymbol{x}_t, t)$ 来近似 $\boldsymbol{x}_1 - \boldsymbol{x</p>
<p><strong>训练目标</strong>：
\begin{equation}
\boldsymbol{\theta}^* = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0,\boldsymbol{x}_1\sim p_1, t\sim U(0,1)}\left[\Vert\boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\Vert^2\right] \tag{4</p>
<p>其中 $\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$。</p>
<p><strong>损失函数的数学意义</strong>：
- 这是一个最小二乘问题，寻找最优的速度场预测
- 期望是对三个随机变量取的：起点 $\boldsymbol{x}_0$，终点 $\boldsymbol{x}_1$，以及时间 $t$
- $L^2$ 范数保证了速度场在各个方向上的均衡拟合</p>
<p>训练完成后，生成过程变为求解ODE：
\begin{equation}
\frac{d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t}{dt} = \boldsymbol{v}</em>
\end{equation}}^*}(\boldsymbol{x}_t, t) \tag{5</p>
<h3 id="2">2. 欧拉方法与步长问题<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 欧拉离散化<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p>在实际计算中，我们无法精确求解连续时间的ODE (5)，而是使用<strong>欧拉方法</strong>进行离散化：</p>
<p>\begin{equation}
\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon} = \boldsymbol{x}_t + \epsilon \cdot \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t) \tag{6</p>
<p><strong>欧拉方法的误差分析</strong>：</p>
<p>记精确解为 $\boldsymbol{x}^*(t)$，欧拉方法的局部截断误差为：</p>
<p>\begin{equation}
\boldsymbol{e}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{local}} = \boldsymbol{x}^<em>(t+\epsilon) - [\boldsymbol{x}^</em>(t) + \epsilon \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}^*(t), t)] \tag{7</p>
<p>通过泰勒展开：
\begin{equation}
\boldsymbol{x}^<em>(t+\epsilon) = \boldsymbol{x}^</em>(t) + \epsilon \frac{d\boldsymbol{x}^<em>}{dt}\bigg|_t + \frac{\epsilon^2}{2}\frac{d^2\boldsymbol{x}^</em>}{dt^2}\bigg|_t + O(\epsilon^3) \tag{8}
\end{equation}</p>
<p>因为 $\frac{d\boldsymbol{x}^<em>}{dt} = \boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}^</em>, t)$，所以：
\begin{equation}
\boldsymbol{e}_{\text{local}} = \frac{\epsilon^2}{2}\frac{d^2\boldsymbol{x}^*}{dt^2}\bigg|_t + O(\epsilon^3) = O(\epsilon^2) \tag{9}
\end{equation}</p>
<p><strong>全局误差</strong>：假设需要 $N = 1/\epsilon$ 步从 $t=0$ 到 $t=1$，全局误差为：
\begin{equation}
\boldsymbol{e}_{\text{global}} = N \cdot O(\epsilon^2) = \frac{1}{\epsilon} \cdot O(\epsilon^2) = O(\epsilon) \tag{10}
\end{equation}</p>
<p><strong>结论</strong>：步长 $\epsilon$ 越小，误差越小，但生成步数 $N = 1/\epsilon$ 越大，计算成本越高。</p>
<h4 id="22-">2.2 步长-精度权衡<a class="toc-link" href="#22-" title="Permanent link">&para;</a></h4>
<p>生成质量和计算成本之间存在根本矛盾：</p>
<table>
<thead>
<tr>
<th>步长 $\epsilon$</th>
<th>生成步数 $N$</th>
<th>误差量级</th>
<th>生成质量</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\epsilon = 1$</td>
<td>$N = 1$</td>
<td>$O(1)$</td>
<td>很差</td>
</tr>
<tr>
<td>$\epsilon = 0.1$</td>
<td>$N = 10$</td>
<td>$O(0.1)$</td>
<td>较好</td>
</tr>
<tr>
<td>$\epsilon = 0.01$</td>
<td>$N = 100$</td>
<td>$O(0.01)$</td>
<td>很好</td>
</tr>
<tr>
<td>$\epsilon = 0.001$</td>
<td>$N = 1000$</td>
<td>$O(0.001)$</td>
<td>极好</td>
</tr>
</tbody>
</table>
<p><strong>Shortcut模型的核心思想</strong>：与其被动接受欧拉方法的误差，不如主动让模型适应不同的步长。</p>
<h3 id="3-shortcut">3. Shortcut模型：步长条件化<a class="toc-link" href="#3-shortcut" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 步长条件速度场<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>传统速度场 $\boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)$ 只依赖于状态 $\boldsymbol{x}_t$ 和时间 $t$。Shortcut模型将步长 $\epsilon$ 也作为输入：</p>
<p>\begin{equation}
\boldsymbol{v}<em>{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \epsilon): \mathbb{R}^d \times [0,1] \times \mathbb{R}</em>+ \to \mathbb{R}^d \tag{11}
\end{equation}</p>
<p><strong>更新规则</strong>：
\begin{equation}
\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon} = \boldsymbol{x}_t + \epsilon \cdot \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t, \epsilon) \tag{12</p>
<p><strong>数学直觉</strong>：
- 当 $\epsilon \to 0$，$\boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \epsilon)$ 应该收敛到精确的ODE速度场
- 当 $\epsilon$ 较大时，模型可以"预见"更远的未来，输出修正后的速度场
- 这类似于高阶ODE求解器，但通过学习而非手工设计</p>
<h4 id="32-ode">3.2 第一个损失函数：精确ODE损失<a class="toc-link" href="#32-ode" title="Permanent link">&para;</a></h4>
<p>为了保证 $\epsilon \to 0$ 时的精确性，我们需要训练精确的ODE模型：</p>
<p>\begin{equation}
\mathcal{L}<em _boldsymbol_x="\boldsymbol{x">1(\boldsymbol{\theta}) = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0,\boldsymbol{x}_1\sim p_1, t\sim U(0,1)}\left[\Vert\boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t, 0) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\Vert^2\right] \tag{13</p>
<p><strong>注意事项</strong>：
- 这里步长设为 $\epsilon = 0$，表示无限小步长
- 这确保了模型在极限情况下的正确性
- 单独这个损失还不够，因为它没有告诉模型如何处理 $\epsilon &gt; 0$ 的情况</p>
<h3 id="4">4. 自洽性约束：核心创新<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 自洽性条件的推导<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p>Shortcut模型的关键洞察是：<strong>大步长应该等于多个小步长的组合</strong>。</p>
<p>具体地，用步长 $2\epsilon$ 走1步应该等于用步长 $\epsilon$ 走2步：</p>
<p>\begin{equation}
\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t + 2\epsilon \cdot \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t, 2\epsilon) = \text{(用步长 } \epsilon \text{ 走2步的结果)} \tag{14</p>
<p><strong>第一步</strong>（从 $t$ 到 $t+\epsilon$）：
\begin{equation}
\tilde{\boldsymbol{x}}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon} = \boldsymbol{x}_t + \epsilon \cdot \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t, \epsilon) \tag{15</p>
<p><strong>第二步</strong>（从 $t+\epsilon$ 到 $t+2\epsilon$）：
\begin{equation}
\tilde{\boldsymbol{x}}<em t_epsilon="t+\epsilon">{t+2\epsilon} = \tilde{\boldsymbol{x}}</em>} + \epsilon \cdot \boldsymbol{v<em t_epsilon="t+\epsilon">{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}</em>
\end{equation}}, t+\epsilon, \epsilon) \tag{16</p>
<p>将式 (15) 代入式 (16)：
\begin{equation}
\begin{aligned}
\tilde{\boldsymbol{x}}<em _boldsymbol_theta="\boldsymbol{\theta">{t+2\epsilon} &amp;= \boldsymbol{x}_t + \epsilon \cdot \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, \epsilon) \
&amp;\quad + \epsilon \cdot \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t + \epsilon \cdot \boldsymbol{v}</em>_t, t, \epsilon), t+\epsilon, \epsilon)
\end{aligned} \tag{17}
\end{equation}}}(\boldsymbol{x</p>
<p><strong>自洽性条件</strong>：要求
\begin{equation}
\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t + 2\epsilon \cdot \boldsymbol{v}</em>}}(\boldsymbol{x<em t_2_epsilon="t+2\epsilon">t, t, 2\epsilon) = \tilde{\boldsymbol{x}}</em>
\end{equation}} \tag{18</p>
<p>两边减去 $\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t$ 并除以 $2\epsilon$：
\begin{equation}
\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, 2\epsilon) = \frac{1}{2}\left[\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, \epsilon) + \boldsymbol{v}</em>
\end{equation}}}(\tilde{\boldsymbol{x}}_{t+\epsilon}, t+\epsilon, \epsilon)\right] \tag{19</p>
<p><strong>数学意义</strong>：大步长的速度场应该是两个小步长速度场的平均。这是一种<strong>时间一致性约束</strong>。</p>
<h4 id="42">4.2 第二个损失函数：自洽性损失<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p>基于式 (19)，我们构造自洽性损失：</p>
<p>\begin{equation}
\mathcal{L}<em _boldsymbol_x="\boldsymbol{x">2(\boldsymbol{\theta}) = \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0,\boldsymbol{x}_1,t,\epsilon}\left[\left\Vert\boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t, 2\epsilon) - \frac{1}{2}\left[\boldsymbol{v}_1 + \boldsymbol{v}_2\right]\right\Vert^2\right] \tag{20</p>
<p>其中：
\begin{equation}
\begin{aligned}
\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">1 &amp;= \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, \epsilon) \
\boldsymbol{v}_2 &amp;= \boldsymbol{v}</em>}}(\boldsymbol{x<em t_epsilon="t+\epsilon">t + \epsilon \boldsymbol{v}_1, t+\epsilon, \epsilon) \
\tilde{\boldsymbol{x}}</em>_1
\end{aligned} \tag{21}
\end{equation}} &amp;= \boldsymbol{x}_t + \epsilon \boldsymbol{v</p>
<p><strong>展开损失函数</strong>：
\begin{equation}
\begin{aligned}
\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">2 = \mathbb{E}\bigg[&amp;\Vert\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, 2\epsilon)\Vert^2 \
&amp;- \boldsymbol{v}</em>_2] \
&amp;+ \frac{1}{4}\Vert\boldsymbol{v}_1 + \boldsymbol{v}_2\Vert^2\bigg]
\end{aligned} \tag{22}
\end{equation}}}(\boldsymbol{x}_t, t, 2\epsilon) \cdot [\boldsymbol{v}_1 + \boldsymbol{v</p>
<h3 id="5">5. 多尺度自洽性<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 离散步长集合<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>Shortcut模型不对所有 $\epsilon \in (0,1]$ 进行训练，而是选择离散的步长集合：</p>
<p>\begin{equation}
\mathcal{E} = {2^{-7}, 2^{-6}, 2^{-5}, 2^{-4}, 2^{-3}, 2^{-2}, 2^{-1}, 2^0} \tag{23}
\end{equation}</p>
<p><strong>选择理由</strong>：
1. <strong>指数间隔</strong>：$\epsilon_k = 2^{-k}$ 覆盖了从小步长到大步长的广泛范围
2. <strong>二进制关系</strong>：$2\epsilon_k = \epsilon_{k-1}$，自洽性条件自然成立
3. <strong>计算效率</strong>：有限的步长集合可以用Embedding表示</p>
<h4 id="52">5.2 递归自洽性<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>对于任意的 $k$，我们有：
\begin{equation}
\epsilon_{k-1} = 2\epsilon_k \tag{24}
\end{equation}</p>
<p>因此自洽性条件可以递归应用：
\begin{equation}
\boldsymbol{v}<em k-1="k-1">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \epsilon</em>}) = \frac{1}{2}\left[\boldsymbol{v<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \epsilon_k) + \boldsymbol{v}</em>
\end{equation}}}(\tilde{\boldsymbol{x}}_{t+\epsilon_k}, t+\epsilon_k, \epsilon_k)\right] \tag{25</p>
<p><strong>训练策略</strong>：
- 对每个 $\epsilon_k \in {2^{-7}, ..., 2^{-1}}$，随机选择时间 $t \sim U(0, 1-\epsilon_k)$
- 计算自洽性损失 $\mathcal{L}_2$ for 步长对 $(\epsilon_k, 2\epsilon_k)$
- 这确保了所有尺度上的一致性</p>
<h3 id="6">6. 总损失函数与训练算法<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 组合损失<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>最终的训练目标是：
\begin{equation}
\mathcal{L}(\boldsymbol{\theta}) = \lambda_1 \mathcal{L}_1(\boldsymbol{\theta}) + \lambda_2 \mathcal{L}_2(\boldsymbol{\theta}) \tag{26}
\end{equation}</p>
<p><strong>权重选择</strong>：
- 理论上 $\lambda_1 = \lambda_2 = 1$ 即可
- 但实际中 $\mathcal{L}_2$ 涉及两次前向传播，计算量是 $\mathcal{L}_1$ 的两倍
- 论文采用策略：75% 样本用于 $\mathcal{L}_1$，25% 样本用于 $\mathcal{L}_2$
- 这相当于隐式地设置 $\lambda_1 : \lambda_2 = 3:1$</p>
<h4 id="62-stop-gradient">6.2 Stop Gradient技巧<a class="toc-link" href="#62-stop-gradient" title="Permanent link">&para;</a></h4>
<p>实际实现中，$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">2$ 被修改为：
\begin{equation}
\mathcal{L}_2 = \mathbb{E}\left[\left\Vert\boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t, 2\epsilon) - \text{sg}\left[\frac{\boldsymbol{v}_1 + \boldsymbol{v}_2}{2}\right]\right\Vert^2\right] \tag{27</p>
<p>其中 $\text{sg}[\cdot]$ 是stop gradient算子。</p>
<p><strong>原因分析</strong>：
1. <strong>目标视角</strong>：将 $\frac{\boldsymbol{v}_1 + \boldsymbol{v}_2}{2}$ 视为固定的"目标"
2. <strong>计算效率</strong>：避免对 $\boldsymbol{v}_2$ 的二次反向传播，节省内存和时间
3. <strong>训练稳定性</strong>：类似BYOL等自监督学习方法，防止梯度爆炸</p>
<p><strong>梯度流分析</strong>：</p>
<p>不使用stop gradient时：
\begin{equation}
\frac{\partial \mathcal{L}_2}{\partial \boldsymbol{\theta}} = \frac{\partial}{\partial \boldsymbol{\theta}}\left[\text{模型预测}\right] - \frac{\partial}{\partial \boldsymbol{\theta}}\left[\frac{\boldsymbol{v}_1 + \boldsymbol{v}_2}{2}\right] \tag{28}
\end{equation}</p>
<p>使用stop gradient后：
\begin{equation}
\frac{\partial \mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">2}{\partial \boldsymbol{\theta}} = \frac{\partial}{\partial \boldsymbol{\theta}}\left[\boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t, 2\epsilon)\right] \tag{29</p>
<h3 id="7-embedding">7. 步长Embedding的实现<a class="toc-link" href="#7-embedding" title="Permanent link">&para;</a></h3>
<h4 id="71-embedding">7.1 离散Embedding方案<a class="toc-link" href="#71-embedding" title="Permanent link">&para;</a></h4>
<p>由于步长集合 $\mathcal{E}$ 是有限的（9个值，包括0），可以用Embedding表示：</p>
<p>\begin{equation}
\epsilon \to \mathbf{e}<em _text_emb="\text{emb">{\epsilon} \in \mathbb{R}^{d</em>
\end{equation}}}} \tag{30</p>
<p><strong>实现细节</strong>：
1. 建立映射 ${0, 2^{-7}, ..., 2^0} \to {0, 1, ..., 8}$
2. 使用Embedding层：$\mathbf{E} \in \mathbb{R}^{9 \times d_{\text{emb}}}$
3. 组合时间和步长的Embedding：
   \begin{equation}
   \mathbf{c}<em _epsilon="\epsilon">{t,\epsilon} = \mathbf{e}_t + \mathbf{e}</em>
   \end{equation}
4. 将 $\mathbf{c}_{t,\epsilon}$ 注入U-Net的每一层} \tag{31</p>
<h4 id="72-embedding">7.2 与时间Embedding的融合<a class="toc-link" href="#72-embedding" title="Permanent link">&para;</a></h4>
<p><strong>标准时间Embedding</strong>（Sinusoidal位置编码）：
\begin{equation}
\begin{aligned}
[\mathbf{e}<em 2i="2i">t]</em>\right) \
[\mathbf{e}} &amp;= \sin\left(\frac{t}{10000^{2i/d}<em 2i_1="2i+1">t]</em>\right)
\end{aligned} \tag{32}
\end{equation}} &amp;= \cos\left(\frac{t}{10000^{2i/d}</p>
<p><strong>组合方案</strong>：
\begin{equation}
\mathbf{h} = \text{MLP}(\mathbf{e}<em _epsilon="\epsilon">t + \mathbf{e}</em>
\end{equation}}) \tag{33</p>
<h3 id="8">8. 推理过程分析<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 单步生成<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p>设步长 $\epsilon = 1$，从纯噪声 $\boldsymbol{x}_0 \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$ 出发：</p>
<p>\begin{equation}
\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1 = \boldsymbol{x}_0 + 1 \cdot \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_0, 0, 1) \tag{34</p>
<p><strong>为什么可行</strong>：
- 训练时的自洽性约束确保了 $\boldsymbol{v}_{\boldsymbol{\theta}}(\cdot, \cdot, 1)$ 能够"预见"整个轨迹
- 它隐式地整合了多个小步长的信息</p>
<h4 id="82">8.2 多步生成<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>也可以使用更小的步长 $\epsilon &lt; 1$ 进行多步生成：</p>
<p>\begin{equation}
\boldsymbol{x}<em k_1="k+1">{t</em>}} = \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_k} + \epsilon \cdot \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_{t_k}, t_k, \epsilon), \quad t_k = k\epsilon \tag{35</p>
<p><strong>质量-速度权衡</strong>：</p>
<table>
<thead>
<tr>
<th>生成步数</th>
<th>步长 $\epsilon$</th>
<th>FID</th>
<th>NFE</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>~10</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>0.5</td>
<td>~8</td>
<td>2</td>
</tr>
<tr>
<td>4</td>
<td>0.25</td>
<td>~6</td>
<td>4</td>
</tr>
<tr>
<td>8</td>
<td>0.125</td>
<td>~5</td>
<td>8</td>
</tr>
</tbody>
</table>
<h3 id="9-ode">9. 与传统ODE求解器的对比<a class="toc-link" href="#9-ode" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 欧拉方法（一阶）<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p><strong>传统欧拉</strong>：
\begin{equation}
\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon} = \boldsymbol{x}_t + \epsilon \cdot \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t) \tag{36</p>
<p>局部误差：$O(\epsilon^2)$，全局误差：$O(\epsilon)$</p>
<p><strong>Shortcut</strong>：
\begin{equation}
\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon} = \boldsymbol{x}_t + \epsilon \cdot \boldsymbol{v}</em>
\end{equation}}}(\boldsymbol{x}_t, t, \epsilon) \tag{37</p>
<p>通过学习适应步长，减小了固定步长带来的系统误差。</p>
<h4 id="92-heun">9.2 Heun方法（二阶）<a class="toc-link" href="#92-heun" title="Permanent link">&para;</a></h4>
<p><strong>Heun方法</strong>（预测-校正）：
\begin{equation}
\begin{aligned}
\tilde{\boldsymbol{x}}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon} &amp;= \boldsymbol{x}_t + \epsilon \cdot \boldsymbol{v}</em>}}(\boldsymbol{x<em t_epsilon="t+\epsilon">t, t) \
\boldsymbol{x}</em>} &amp;= \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t + \frac{\epsilon}{2}[\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) + \boldsymbol{v}</em>, t+\epsilon)]
\end{aligned} \tag{38}
\end{equation}}}(\tilde{\boldsymbol{x}}_{t+\epsilon</p>
<p>局部误差：$O(\epsilon^3)$，全局误差：$O(\epsilon^2)$</p>
<p><strong>Shortcut的隐式Heun</strong>：
\begin{equation}
\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, 2\epsilon) \approx \frac{1}{2}[\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t, \epsilon) + \boldsymbol{v}</em>
\end{equation}}}(\tilde{\boldsymbol{x}}_{t+\epsilon}, t+\epsilon, \epsilon)] \tag{39</p>
<p>这与Heun方法的思想类似，但是通过学习而非手工设计。</p>
<h3 id="10">10. 理论保证与收敛性<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 极限情况<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p><strong>定理1</strong>：当 $\epsilon \to 0$ 时，Shortcut模型收敛到精确ODE。</p>
<p><strong>证明草图</strong>：
- 损失 $\mathcal{L}<em 0="0" _epsilon="\epsilon" _to="\to">1$ 确保了 $\lim</em>_0$
- 这是精确ODE的速度场
- 因此在极限下，模型是正确的}\boldsymbol{v}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \epsilon) = \boldsymbol{x}_1 - \boldsymbol{x</p>
<h4 id="102">10.2 有限步长的逼近质量<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p>记 $\boldsymbol{x}_1^*$ 为精确解，$\boldsymbol{x}_1^{(\epsilon)}$ 为步长 $\epsilon$ 的近似解。</p>
<p><strong>定理2</strong>（非正式）：在适当的正则性条件下，
\begin{equation}
\Vert\boldsymbol{x}_1^{(\epsilon)} - \boldsymbol{x}_1^*\Vert = O(\epsilon^p) \tag{40}
\end{equation}
其中 $p \geq 1$，且可能 $p &gt; 1$（通过自洽性约束提升阶数）。</p>
<p><strong>直觉</strong>：自洽性约束 $\mathcal{L}_2$ 强制模型在不同尺度上保持一致，这降低了离散化误差。</p>
<h3 id="11">11. 实践建议与超参数<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<h4 id="111">11.1 训练超参数<a class="toc-link" href="#111" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>批量大小</strong>：512（其中75%用于 $\mathcal{L}_1$，25%用于 $\mathcal{L}_2$）</li>
<li><strong>学习率</strong>：$2 \times 10^{-4}$，使用cosine衰减</li>
<li><strong>训练步数</strong>：300K-500K</li>
<li><strong>时间采样</strong>：$t \sim U(0, 1)$ for $\mathcal{L}_1$；$t \sim U(0, 1-\epsilon)$ for $\mathcal{L}_2$</li>
<li><strong>步长采样</strong>：$\epsilon \sim \text{Uniform}(\mathcal{E} \setminus {0, 1})$ for $\mathcal{L}_2$</li>
</ul>
<h4 id="112">11.2 推理超参数<a class="toc-link" href="#112" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>单步生成</strong>：$\epsilon = 1$，NFE=1</li>
<li><strong>高质量生成</strong>：$\epsilon = 2^{-3}$ 或 $2^{-4}$，NFE=8或16</li>
<li><strong>可以动态调整</strong>：前期用大步长，后期用小步长</li>
</ul>
<h3 id="12_1">12. 与相关工作的联系<a class="toc-link" href="#12_1" title="Permanent link">&para;</a></h3>
<h4 id="121-amed">12.1 AMED（中值定理方法）<a class="toc-link" href="#121-amed" title="Permanent link">&para;</a></h4>
<p>AMED基于积分中值定理：
\begin{equation}
\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon} - \boldsymbol{x}_t = \epsilon \cdot \boldsymbol{v}</em>
\end{equation}
其中 $s \in [t, t+\epsilon]$ 需要预测。}}(\boldsymbol{x}_s, s) \tag{41</p>
<p><strong>对比</strong>：
- AMED：事后修正，需要额外小模型预测 $s$
- Shortcut：事先学习，将 $\epsilon$ 直接作为输入</p>
<h4 id="122-consistency-models">12.2 Consistency Models<a class="toc-link" href="#122-consistency-models" title="Permanent link">&para;</a></h4>
<p>Consistency Models直接学习映射 $f: (\boldsymbol{x}_t, t) \to \boldsymbol{x}_0$。</p>
<p><strong>对比</strong>：
- Consistency：直接预测终点
- Shortcut：预测速度场，更灵活（可多步可单步）</p>
<h3 id="13">13. 局限性与未来方向<a class="toc-link" href="#13" title="Permanent link">&para;</a></h3>
<h4 id="131">13.1 当前局限<a class="toc-link" href="#131" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>离散步长集合</strong>：只训练了有限的步长，中间值需要插值</li>
<li><strong>步长范围</strong>：$[2^{-7}, 1]$ 可能不够覆盖所有需求</li>
<li><strong>训练成本</strong>：$\mathcal{L}_2$ 需要两次前向传播，增加了训练时间</li>
</ol>
<h4 id="132">13.2 可能的改进<a class="toc-link" href="#132" title="Permanent link">&para;</a></h4>
<p><strong>连续步长Embedding</strong>：
\begin{equation}
\mathbf{e}_{\epsilon} = \text{MLP}(\log \epsilon) \tag{42}
\end{equation}
允许任意连续的 $\epsilon \in (0, 1]$。</p>
<p><strong>自适应步长</strong>：
\begin{equation}
\epsilon_{k+1} = f_{\boldsymbol{\phi}}(\boldsymbol{x}_{t_k}, t_k) \tag{43}
\end{equation}
根据当前状态动态选择步长。</p>
<p><strong>高阶自洽性</strong>：
\begin{equation}
\boldsymbol{v}<em _epsilon="\epsilon">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, 4\epsilon) = g(\boldsymbol{v}</em>
\end{equation}
建立更复杂的多尺度关系。}, \boldsymbol{v}_{2\epsilon}) \tag{44</p>
<h3 id="14">14. 数学总结<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<h4 id="141">14.1 核心公式回顾<a class="toc-link" href="#141" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>直线轨迹</strong>：$\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$</li>
<li><strong>ODE速度场</strong>：$\frac{d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t}{dt} = \boldsymbol{v}</em>_t, t, \epsilon)$}}(\boldsymbol{x</li>
<li><strong>精确ODE损失</strong>：$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">1 = \mathbb{E}[\Vert\boldsymbol{v}</em>_0)\Vert^2]$}}(\boldsymbol{x}_t, t, 0) - (\boldsymbol{x}_1 - \boldsymbol{x</li>
<li><strong>自洽性损失</strong>：$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">2 = \mathbb{E}[\Vert\boldsymbol{v}</em>\Vert^2]$}}(\boldsymbol{x}_t, t, 2\epsilon) - \frac{\boldsymbol{v}_1 + \boldsymbol{v}_2}{2</li>
<li><strong>更新规则</strong>：$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t+\epsilon} = \boldsymbol{x}_t + \epsilon \cdot \boldsymbol{v}</em>_t, t, \epsilon)$}}(\boldsymbol{x</li>
</ol>
<h4 id="142">14.2 理论意义<a class="toc-link" href="#142" title="Permanent link">&para;</a></h4>
<p>Shortcut模型展示了：
1. <strong>学习vs设计</strong>：与其设计复杂的ODE求解器，不如让模型学习适应不同步长
2. <strong>多尺度一致性</strong>：自洽性约束是一种强大的归纳偏置
3. <strong>单阶段训练</strong>：无需蒸馏即可实现单步生成</p>
<p>这为扩散模型加速开辟了新的范式。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="muon优化器赏析从向量到矩阵的本质跨越.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#300 Muon优化器赏析：从向量到矩阵的本质跨越</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈二十八分步理解一致性模型.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#302 生成扩散模型漫谈（二十八）：分步理解一致性模型</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">生成扩散模型漫谈（二十七）：将步长作为条件输入</a><ul>
<li><a href="#ode">ODE扩散</a></li>
<li><a href="#_2">步长自洽</a></li>
<li><a href="#_3">模型细节</a></li>
<li><a href="#_4">实验效果</a></li>
<li><a href="#_5">延伸思考</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a><ul>
<li><a href="#1">1. 核心概念与数学框架</a></li>
<li><a href="#2">2. 欧拉方法与步长问题</a></li>
<li><a href="#3-shortcut">3. Shortcut模型：步长条件化</a></li>
<li><a href="#4">4. 自洽性约束：核心创新</a></li>
<li><a href="#5">5. 多尺度自洽性</a></li>
<li><a href="#6">6. 总损失函数与训练算法</a></li>
<li><a href="#7-embedding">7. 步长Embedding的实现</a></li>
<li><a href="#8">8. 推理过程分析</a></li>
<li><a href="#9-ode">9. 与传统ODE求解器的对比</a></li>
<li><a href="#10">10. 理论保证与收敛性</a></li>
<li><a href="#11">11. 实践建议与超参数</a></li>
<li><a href="#12_1">12. 与相关工作的联系</a></li>
<li><a href="#13">13. 局限性与未来方向</a></li>
<li><a href="#14">14. 数学总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>