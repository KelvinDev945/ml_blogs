<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>滑动平均视角下的权重衰减和学习率 | ML & Math Blog Posts</title>
    <meta name="description" content="滑动平均视角下的权重衰减和学习率&para;
原文链接: https://spaces.ac.cn/archives/11459
发布日期: 

权重衰减（Weight Decay）和学习率（Learning Rate）是LLM预训练的重要组成部分，它们的设置是否妥当，是模型最终成败的关键因素之一。自AdamW以来，单独分离出Weight Decay来取代传统的L2正则，基本上已经成为了共识，但在...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=最优">最优</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #117 滑动平均视角下的权重衰减和学习率
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#117</span>
                滑动平均视角下的权重衰减和学习率
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/11459" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=最优" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 最优</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=学习率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 学习率</span>
                </a>
                
                <a href="../index.html?tags=平均场" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 平均场</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">滑动平均视角下的权重衰减和学习率<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11459">https://spaces.ac.cn/archives/11459</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>权重衰减（Weight Decay）和学习率（Learning Rate）是LLM预训练的重要组成部分，它们的设置是否妥当，是模型最终成败的关键因素之一。自<a href="https://papers.cool/arxiv/1711.05101">AdamW</a>以来，单独分离出Weight Decay来取代传统的L2正则，基本上已经成为了共识，但在此基础上，如何合理地设置Weight Decay和Learning Rate，并没有显著的理论进展。</p>
<p>本文将抛砖引玉，分享笔者关于这个问题的一些新理解：把训练过程看作对训练数据的滑动平均记忆，探讨如何设置Weight Decay和Learning Rate才能让这个记忆更为科学。</p>
<h2 id="_2">滑动平均<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>Weight Decay的一般形式是<br />
\begin{equation}\boldsymbol{\theta}<em t-1="t-1">t = \boldsymbol{\theta}</em>} - \eta_t (\boldsymbol{u<em t-1="t-1">t + \lambda_t \boldsymbol{\theta}</em>})\end{equation
其中$\boldsymbol{\theta}$是参数，$\boldsymbol{u}$是优化器给出的更新量，$\lambda_t,\eta_t$亦即我们说的Weight Decay和Learning Rate，而整个序列$\{\lambda_t\}$和$\{\eta_t\}$，我们分别称为“WD Schedule”和“LR Schedule”。引入记号<br />
\begin{equation}\begin{aligned}
\boldsymbol{m}<em t-1="t-1">t =&amp;\, \beta_1 \boldsymbol{m}</em>} + \left(1 - \beta_1\right) \boldsymbol{g<em t-1="t-1">t, &amp; \hat{\boldsymbol{m}}_t =&amp;\, \boldsymbol{m}_t\left/\left(1 - \beta_1^t\right)\right. &amp;\\[5pt]
\boldsymbol{v}_t =&amp;\, \beta_2 \boldsymbol{v}</em>_t\left/\left(1 - \beta_2^t\right)\right. &amp;} + \left(1 - \beta_2\right) \boldsymbol{g}_t^2,&amp; \hat{\boldsymbol{v}}_t =&amp;\, \boldsymbol{v
\end{aligned}\end{equation}<br />
那么对于SGDM来说有$\boldsymbol{u}_t=\boldsymbol{m}_t$，对于RMSProp来说$\boldsymbol{u}_t= \boldsymbol{g}_t/(\sqrt{\boldsymbol{v}_t} + \epsilon)$，对于Adam来说则是$\boldsymbol{u}_t=\hat{\boldsymbol{m}}_t\left/\left(\sqrt{\hat{\boldsymbol{v}}_t} + \epsilon\right)\right.$，对于SignSGDM来说则$\newcommand{sign}{\mathop{\text{sign}}}\boldsymbol{u}_t=\sign(\boldsymbol{m}_t)$，而Muon则是$\newcommand{msign}{\mathop{\text{msign}}}\boldsymbol{u}_t=\msign(\boldsymbol{m}_t)$。这里列举的例子，除了SGDM外，其他都算是某种自适应学习率优化器。</p>
<p>我们的出发点是滑动平均（Exponential Moving Average，EMA）视角，即将Weight Decay写成<br />
\begin{equation}\boldsymbol{\theta}<em t-1="t-1">t = (1 - \lambda_t \eta_t)\boldsymbol{\theta}</em>} - \eta_t \boldsymbol{u<em t-1="t-1">t = (1 - \lambda_t \eta_t)\boldsymbol{\theta}</em>} + \lambda_t \eta_t ( -\boldsymbol{u}_t / \lambda_t)\label{eq:wd-ema}\end{equation
此时Weight Decay表现为模型参数与$-\boldsymbol{u}_t / \lambda_t$的加权平均形式。滑动平均视角不是新的，<a href="https://papers.cool/arxiv/2405.13698">《How to set AdamW's weight decay as you scale model and dataset size》</a>、<a href="https://papers.cool/arxiv/2505.13738">《Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training》</a>等文章已有过讨论，本文则是在这个视角下将各方面算得更仔细一些。</p>
<p>接下来的篇幅我们主要以Adam为例，最后再讨论其他优化器的可用性。计算过程跟<a href="/archives/11307">《AdamW的Weight RMS的渐近估计（上）》</a>和<a href="/archives/11404">《AdamW的Weight RMS的渐近估计（下）》</a>会有相当一部份重叠之处，读者可以相互参考着阅读。</p>
<h2 id="_3">迭代展开<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>简单起见，我们先考虑常数$\lambda,\eta$，记$\beta_3 = 1 - \lambda\eta$，那么$\boldsymbol{\theta}<em t-1="t-1">t = \beta_3 \boldsymbol{\theta}</em>} + (1 - \beta_3)( -\boldsymbol{u<em i="1">t / \lambda)$，这样形式上就跟$\boldsymbol{m}_t,\boldsymbol{v}_t$一致了。直接迭代展开得<br />
\begin{equation}\boldsymbol{\theta}_t = \beta_3^t \boldsymbol{\theta}_0 + (1 - \beta_3)\sum</em>}^t \beta_3^{t-i} (-\boldsymbol{u<em i="1">i / \lambda) \end{equation}<br />
对于Adam有$\boldsymbol{u}_t=\hat{\boldsymbol{m}}_t\left/\left(\sqrt{\hat{\boldsymbol{v}}_t} + \epsilon\right)\right.$，一般在训练结束时，$t$都能足够大以至于$\beta_1^t,\beta_2^t$足够接近于零，所以可以不去区分$\boldsymbol{m}_t$和$\hat{\boldsymbol{m}}_t$、$\boldsymbol{v}_t$和$\hat{\boldsymbol{v}}_t$，进一步地，简单设$\epsilon=0$，那么可以简化成$\boldsymbol{u}_t=\boldsymbol{m}_t / \sqrt{\boldsymbol{v}_t}$，然后来个经典的平均场近似<br />
\begin{equation}\underbrace{\frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i} \boldsymbol{u<em _text_记为="\text{记为">i}</em>}\bar{\boldsymbol{u}<em i="1">t} = \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i} \frac{\boldsymbol{m<em i="1">i}{\sqrt{\boldsymbol{v}_i}}\approx \frac{\bar{\boldsymbol{m}}_t \,\,\triangleq\,\, \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i}\boldsymbol{m<em i="1">i}{\sqrt{\bar{\boldsymbol{v}}_t \,\,\triangleq\,\, \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i}\boldsymbol{v<em i="1">i}}\label{eq:u-bar}\end{equation}<br />
展开$\boldsymbol{m}_t,\boldsymbol{v}_t$得$\boldsymbol{m}_t = (1 - \beta_1)\sum</em>}^t \beta_1^{t-i}\boldsymbol{g<em i="1">i$和$\boldsymbol{v}_t = (1 - \beta_2)\sum</em>}^t \beta_2^{t-i}\boldsymbol{g<em i="1">i^2$，代入上式<br />
\begin{gather}
\bar{\boldsymbol{m}}_t = \frac{(1-\beta_3)(1 - \beta_1)}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i} \sum_{j=1}^i \beta_1^{i-j}\boldsymbol{g<em j="1">j = \frac{(1-\beta_3)(1 - \beta_1)}{(1-\beta_3^t)(\beta_3 - \beta_1)}\sum</em>}^t (\beta_3^{t-j+1} - \beta_1^{t-j+1})\boldsymbol{g<em i="1">j\\[6pt]
\bar{\boldsymbol{v}}_t = \frac{(1-\beta_3)(1 - \beta_2)}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i} \sum_{j=1}^i \beta_2^{i-j}\boldsymbol{g<em j="1">j^2 = \frac{(1-\beta_3)(1 - \beta_2)}{(1-\beta_3^t)(\beta_3 - \beta_2)}\sum</em>}^t (\beta_3^{t-j+1} - \beta_2^{t-j+1})\boldsymbol{g<em i="1">j^2
\end{gather}<br />
交换求和符号基于恒等式$\sum</em>^t a_i b_j$。综上，我们有}^t \sum_{j=1}^i a_i b_j = \sum_{j=1}^t \sum_{i=j<br />
\begin{equation}\boldsymbol{\theta}_t = \beta_3^t \boldsymbol{\theta}_0 + (1 - \beta_3^t)(-\bar{\boldsymbol{u}}_t / \lambda) \label{eq:theta-0-bar-u}\end{equation}<br />
权重$\boldsymbol{\theta}_t$是我们想要的训练结果，它被表示为$\boldsymbol{\theta}_0$和$-\bar{\boldsymbol{u}}_t / \lambda$的加权平均。其中，$\boldsymbol{\theta}_0$是初始权重，$\bar{\boldsymbol{u}}_t$则是数据相关的，在平均场近似下它约等于$\bar{\boldsymbol{m}}_t/\sqrt{\bar{\boldsymbol{v}}_t}$，而$\bar{\boldsymbol{m}}_t$和$\bar{\boldsymbol{v}}_t$可以表示成每一步梯度的加权求和，以$\bar{\boldsymbol{m}}_t$为例，第$j$步的梯度权重正比于$\beta_3^{t-j+1} - \beta_1^{t-j+1}$。</p>
<h2 id="_4">记忆周期<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>我们主要关心的是预训练，特点是Single-Epoch，大部分数据都只过一遍，所以训出好效果的关键之一是不要忘掉早期的数据。假设训练数据已经经过全局打乱，那么可以合理地认为每一个Batch的数据都同等重要。</p>
<p>数据是以梯度形式线性叠加到$\bar{\boldsymbol{m}}_t$中的，假设每一步梯度只包含当前Batch的信息，那么某个Batch想要不被遗忘，系数$\beta_3^{t-j+1} - \beta_1^{t-j+1}$就不能太小。考察函数$f(s) = \beta_3^s - \beta_1^s$，它是一个先增后减的函数，但因为$\beta_3$会比$\beta_1$更接近于1，所以增的步数不多，远处更多是接近指数下降，如下图：  </p>
<p><a href="/usr/uploads/2025/12/2468435175.png" title="点击查看原图"><img alt="梯度权重示意图" src="/usr/uploads/2025/12/2468435175.png" /></a></p>
<p>梯度权重示意图</p>
<p>总之趋势是距离越远系数越小。那么要想模型不遗忘每一个Batch，最远处的系数就不能太小。假设系数不小于$c \in (0, 1)$才能被记住，当$s$足够大时$\beta_1^s$先趋于0，所以$\beta_3^s - \beta_1^s\approx \beta_3^s$，由$\beta_3^s\geq c$可以解得$s \leq \frac{\log c}{\log \beta_3} \approx \frac{-\log c}{\lambda\eta}$。这表明，模型顶多能记住$\mathcal{O}(1/\lambda\eta)$步的数据，这是它的记忆周期。</p>
<p>那直接无脑$\lambda=0$，让记忆周期无限大，是否就可以不担心遗忘问题了？理论上是的，然而这并不是一个好选择。Weight Decay还有一个作用是帮助模型忘掉初始化。由式$\eqref{eq:theta-0-bar-u}$可知初始化$\boldsymbol{\theta}_0$的权重是$\beta_3^t$，如果$\beta_3$太大或者训练步数$t$太小，那么初始化的占比还很高，模型可能还处于欠拟合阶段。</p>
<p>此外，Weight Decay还有利于稳定模型“内科”。在<a href="/archives/11307">《AdamW的Weight RMS的渐近估计（上）》</a>我们已经推导过，AdamW的Weight RMS的渐近结果是$\sqrt{\eta/2\lambda}$，如果$\lambda=0$，那么Weight RMS会以$\eta\sqrt{t}$的速率膨胀。这意味着直接设置$\lambda=0$，还可能带来权重爆炸等模型内科异常。</p>
<p>因此，$\beta_3$不能太小，以免忘记早期数据，同时也不能太大，以免欠拟合或者权重爆炸。比较适合的设置是让$1/\lambda\eta$正比于训练步数，如果是Multi-Epoch的训练场景，则考虑让$1/\lambda\eta$正比于单个Epoch的训练步数。</p>
<h2 id="_5">动态版本<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>在实际训练中，我们更多是适用动态变化的LR Schedule，比如Consine Decay、Linear Decay、WSD（Warmup-Stable-Decay）等，所以上述静态Weight Decay和Learning Rate的结果并不完全符合实践，我们需要将它们推广到动态版。</p>
<p>从式$\eqref{eq:wd-ema}$出发，利用近似$1 - \lambda_t \eta_t\approx e^{-\lambda_t \eta_t}$，并迭代展开，可得<br />
\begin{equation}\boldsymbol{\theta}<em t-1="t-1">t = (1 - \lambda_t \eta_t)\boldsymbol{\theta}</em>} - \eta_t \boldsymbol{u<em t-1="t-1">t \approx e^{-\lambda_t \eta_t}\boldsymbol{\theta}</em>} - \eta_t \boldsymbol{u<em i="1">t = e^{-\kappa_t}\left(\boldsymbol{\theta}_0 - \sum</em>}^t e^{\kappa_i}\eta_i\boldsymbol{u<em i="1">i\right)\end{equation}<br />
其中$\kappa_t = \sum</em>\eta_i$，那么可以得到同样的平均场近似}^t \eta_i\lambda_i$。继续设$z_t = \sum_{i=1}^t e^{\kappa_i<br />
\begin{equation}\bar{\boldsymbol{u}}<em i="1">t\triangleq\frac{1}{z_t}\sum</em>}^t e^{\kappa_i}\eta_i \boldsymbol{u<em i="1">i = \frac{1}{z_t}\sum</em>}^t e^{\kappa_i}\eta_i \frac{\boldsymbol{m<em i="1">i}{\sqrt{\boldsymbol{v}_i}}\approx \frac{\bar{\boldsymbol{m}}_t \,\,\triangleq\,\, \frac{1}{z_t}\sum</em>}^t e^{\kappa_i}\eta_i\boldsymbol{m<em i="1">i}{\sqrt{\bar{\boldsymbol{v}}_t \,\,\triangleq\,\, \frac{1}{z_t}\sum</em>}^t e^{\kappa_i}\eta_i\boldsymbol{v<em i="1">i}}\end{equation}<br />
代入$\boldsymbol{m}_t,\boldsymbol{v}_t$的展开式得<br />
\begin{gather}
\bar{\boldsymbol{m}}_t = \frac{1}{z_t}\sum</em>}^t e^{\kappa_i}\eta_i\boldsymbol{m<em i="1">i = \frac{1 - \beta_1}{z_t}\sum</em>}^t e^{\kappa_i}\eta_i\sum_{j=1}^i \beta_1^{i-j}\boldsymbol{g<em j="1">j = \sum</em>}^t\boldsymbol{g<em i="j">j\underbrace{\frac{1 - \beta_1}{z_t}\sum</em>}^t e^{\kappa_i}\beta_1^{i-j}\eta_i<em i="1">{\text{记为}\bar{\beta}_1(j,t)} \\
\bar{\boldsymbol{v}}_t = \frac{1}{z_t}\sum</em>}^t e^{\kappa_i}\eta_i\boldsymbol{v<em i="1">i = \frac{1 - \beta_2}{z_t}\sum</em>}^t e^{\kappa_i}\eta_i\sum_{j=1}^i \beta_2^{i-j}\boldsymbol{g<em j="1">j^2 = \sum</em>}^t\boldsymbol{g<em i="j">j^2\underbrace{\frac{1 - \beta_2}{z_t}\sum</em> \\}^t e^{\kappa_i}\beta_2^{i-j}\eta_i}_{\text{记为}\bar{\beta}_2(j,t)
\end{gather}<br />
可以看到，跟静态Weight Decay和Learning Rate相比，动态版形式上并没有太大变化，只不过梯度的加权系数变成了稍微复杂一点的$\bar{\beta}_1(j,t)$和$\bar{\beta}_2(j,t)$。特别地，当$\beta_1,\beta_2\to 0$时，$\bar{\beta}_1(j,t)$和$\bar{\beta}_2(j,t)$简化为<br />
\begin{equation}\bar{\beta}_1(j,t) = \bar{\beta}_2(j,t) = \frac{e^{\kappa_j}\eta_j}{z_t}\label{eq:bb1-bb2-0}\end{equation}</p>
<h2 id="_6">最优调度<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>接下来可以做的事情有很多，最基本的就是根据具体的WD Schedule和LR Schedule去算$\bar{\beta}_1(j,t)$和$\bar{\beta}_2(j,t)$、估计记忆周期等。不过，这里我们选择做一件更极致的事情——直接去反推一个最优的WD Schedule和LR Schedule。</p>
<p>具体来说，前面我们假设了数据已经全局打乱，那么每个Batch的数据都同等重要，但静态版得到的系数$\bar{\beta}_1(j,t)\propto\beta_3^{t-j+1} - \beta_1^{t-j+1}$并非常数，而是随距离变化，这跟“每个Batch的数据都同等重要”不完全吻合。如果条件允许，那么我们期望它应该恒等于某个常数。根据这个期望，我们就可以反解出对应的$\lambda_j,\eta_j$。</p>
<p>简单起见，我们从$\beta_1,\beta_2\to 0$入手，此时期望条件可以写为$\forall 0\leq i,j \leq t, e^{\kappa_i}\eta_i/z_t = e^{\kappa_j}\eta_j/z_t$，整理得$\eta_i / \eta_j = e^{\kappa_j - \kappa_i}$，代入$i=j-1$得$\eta_{j-1}/\eta_j = e^{\kappa_j - \kappa_{j-1}} = e^{\lambda_j\eta_j}$，或者写成<br />
\begin{equation}e^{\lambda_j\eta_j}\eta_j = \eta_{j-1}\end{equation}<br />
这样就得到了一种求解$\lambda_j,\eta_j$的数值方法：每一步得到$\eta_{j-1}$后，通过求解该非线性方程就能继续得到$\lambda_j,\eta_j$，于是从$\eta_1$出发就可以递归地得到整个序列。如果想要更解析的结果，可以用导数近似差分：两端取对数得$\lambda_j\eta_j + \log \eta_j - \log \eta_{j-1} = 0$，将$\lambda_j,\eta_j$视为连续函数$\lambda_s,\eta_s$，$\log \eta_j - \log \eta_{j-1}$则视为$\log \eta_s$的导数近似，于是有<br />
\begin{equation}\lambda_s \eta_s + \frac{\dot{\eta}<em _max="\max">s}{\eta_s} \approx 0 \label{eq:lr-wd-ode}\end{equation}<br />
如果$\lambda_s$取常数$\lambda$，那么可以解得<br />
\begin{equation}\eta_s \approx \frac{\eta</em>}}{\lambda\eta_{\max} s + 1}\label{eq:opt-lrt-wd}\end{equation
这便是常数Weight Decay下的最佳LR Schedule，它不需要预设终点$t$和最小学习率$\eta_{\min}$，这意味着它可以无限训练下去，类似于WSD的Stable阶段，但它能自动平衡每一步梯度的系数。不过它也有个缺点：$s\to\infty$时它会趋于0。从<a href="/archives/11404">《AdamW的Weight RMS的渐近估计（下）》</a>可知Weight RMS会趋于$\lim\limits_{s\to\infty} \frac{\eta_s}{2\lambda_s}$，所以该缺点可能带来权重坍缩的风险。</p>
<p>为了解决这个问题，我们可以考虑让$\lambda_s = \alpha\eta_s$，$\alpha=\lambda_{\max}/\eta_{\max}$是一个常数，此时可以解得<br />
\begin{equation}\eta_s \approx \frac{\eta_{\max}}{\sqrt{2\lambda_{\max}\eta_{\max} s + 1}},\qquad \lambda_s \approx \frac{\lambda_{\max}}{\sqrt{2\lambda_{\max}\eta_{\max} s + 1}} \label{eq:opt-lrt-wdt}\end{equation}<br />
相应的$e^{\kappa_s} \approx \sqrt{2\lambda_{\max}\eta_{\max} s + 1}, e^{\kappa_s}\eta_s \approx \eta_{\max}, z_t\approx \eta_{\max} t, \bar{\beta}_1(j,t) = \bar{\beta}_2(j,t) \approx 1/t$。</p>
<h2 id="_7">一般结果<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>目前的结果，比如式$\eqref{eq:opt-lrt-wd}$和式$\eqref{eq:opt-lrt-wdt}$，都是基于$\beta_1,\beta_2=0$的，当它们不等于0时，结果需要变化吗？更一般地，上述结果都是基于Adam优化器的，它们多大程度上可以推广到其他优化器呢？</p>
<p>首先来看$\beta_1,\beta_2\neq 0$时的问题，答案是当$t$足够大时，结论并不用大改。以$\bar{\beta}<em i="j">1(j,t)$为例，在上述最优调度下$e^{\kappa_i}\eta_i$等于（跟$t$有关的）常数，那么根据定义<br />
\begin{equation}\bar{\beta}_1(j,t) = \frac{1 - \beta_1}{z_t}\sum</em>}^t e^{\kappa_i}\beta_1^{i-j}\eta_i \propto \sum_{i=j}^t \beta_1^{i-j} = \frac{1 - \beta_1^{t-j+1}}{1 - \beta_1}\end{equation
当$t$足够大时$\beta_1^{t-j+1}\to 0$，所以这也可以看成是一个跟$j$无关的常数。前面也说了，对于$\beta_1,\beta_2$来说，“$t$足够大”这件事情几乎时肯定的，所以直接用$\beta_1,\beta_2=0$的结果就行。</p>
<p>至于优化器，前面我们提到的优化器有SGDM、RMSProp、Adam、SignSGDM、Muon，它们可以分为两类。其中，SGDM是一类，它的$\bar{\boldsymbol{u}}_t$直接就是$\bar{\boldsymbol{m}}_t$，连平均场近似都不需要用，所以直到式$\eqref{eq:lr-wd-ode}$的结果都是适用的。不过，式$\eqref{eq:opt-lrt-wd}$和式$\eqref{eq:opt-lrt-wdt}$大概不是最适合的了，因为SGDM的渐近Weight RMS还依赖于梯度模长[<a href="https://papers.cool/arxiv/2305.17212">参考</a>]，所以要把梯度模长考虑进去才行，相对复杂一些。</p>
<p>剩下的RMSProp、Adam、SignSGDM、Muon我们将它归为另一类，都属于自适应学习率优化器，它们的更新规则都具有$\frac{\text{梯度}}{\sqrt{\text{梯度}{}^2}}$的其次形式，这种情况下，如果我们依旧相信平均场近似，那么就能得到同样的$\bar{\boldsymbol{m}}_t$、同样的$\beta_1(j,t)$，所以到式$\eqref{eq:lr-wd-ode}$的结果也是适用的；并且对于这类齐次型优化器，可以证明Weight RMS同样渐近正比于$\sqrt{\eta/\lambda}$，所以连同式$\eqref{eq:opt-lrt-wd}$和式$\eqref{eq:opt-lrt-wdt}$也可以复用。</p>
<h2 id="_8">假设讨论<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>我们的推导暂告一段落，这一节我们来讨论一下推导所依赖的假设。</p>
<p>纵观全文，推导过程中所用到的值得拿出来讨论的大假设主要有两个。第一个假设是平均场近似，首次介绍于<a href="/archives/11280">《重新思考学习率与Batch Size（二）：平均场》</a>。平均场本身肯定不是新的，它是物理学中的经典近似，但将其用来分析优化器的相关动态，应该是笔者首次引入的，目前已经用它估算过优化器的<a href="/archives/11280">Batch Size</a>、<a href="/archives/11267">Update RMS</a>、<a href="/archives/11307">Weight RMS</a>等，结果看起来是合理的。</p>
<p>对于平均场近似的有效性，其实我们没法评述太多，它更多体现了一种信仰。一方面，我们根据已有的估算结果的合理性，相信它会继续合理下去，至少能对一些标量指标给出有效的渐近估计。另一方面，对于自适应学习率优化器，由于其更新规则的非线性，分析难度大大增加，除了平均场近似外，我们其实也没什么计算工具能用了。</p>
<p>这其中最典型的例子就是Muon，因为它是非Element-wise的运算，以往像SignSGD那样逐分量的计算手段便失去了作用，而平均场近似依然奏效（参考<a href="/archives/11285">《重新思考学习率与Batch Size（三）：Muon》</a>）。所以，平均场近似实际上为一大类自适应学习率优化器的分析和估计提供了统一、有效、简洁的计算手段，目前看来似乎没有别的方法有同样的效果，所以我们只能继续相信它。</p>
<p>第二个大的假设是“每一步梯度只包含当前Batch的信息”，这个假设本质上是错误的，因为梯度不仅依赖于当前Batch的数据，还依赖于上一步的参数，而上一步的参数自然是包含了历史信息。不过，我们可以尝试补救一下，因为理论上来说，每个Batch都会带来新的信息，否则这个Batch就没有存在的意义了，所以补救的方法是改为“每一步梯度包含大致相同的增量信息”。</p>
<p>当然，仔细思考之下这个说法也是有争议的，因为学得越多，覆盖面越广，后来Batch的独特信息就越少。不过，还可以挣扎一下，那就是将知识分为“规律”和“事实”两大类，事实型知识——比如某个定理是某个数学家发现的——只能靠记忆，那么可以考虑改为“每一步梯度包含大致相同的事实型知识”。总之，从实践来看，“平等对待每一步梯度”所得的LR Schedule似乎真的是有好处的，所以总可以尝试为它构造一个解释。</p>
<p>最近的论文<a href="https://papers.cool/arxiv/2511.18903">《How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining》</a>提供了一个间接证据。它考虑了数据质量从低到高的课程学习，发现激进的LR Decay会让课程学习优势全无。而我们的结果是每一Batch的权重是式$\eqref{eq:bb1-bb2-0}$，正比于Learning Rate，如果LR Decay过于激进，那么后面的高质量数据权重反而过小，因而效果欠佳。能够合理解释这个现象，反过来显示了我们假设的合理性。</p>
<h2 id="_9">文章小结<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>本文从滑动平均的视角来理解权重衰减（WD）和学习率（LR），并探讨了在该视角下最优的WD Schedule和LR Schedule。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11459">https://spaces.ac.cn/archives/11459</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Dec. 05, 2025). 《滑动平均视角下的权重衰减和学习率 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11459">https://spaces.ac.cn/archives/11459</a></p>
<p>@online{kexuefm-11459,<br />
title={滑动平均视角下的权重衰减和学习率},<br />
author={苏剑林},<br />
year={2025},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/11459}},<br />
} </p>
<hr />
<h2 id="_10">公式推导与注释<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="利用熄火保护-通断器实现燃气灶智能关火.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#116 利用“熄火保护 + 通断器”实现燃气灶智能关火</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="adafactor优化器浅析附开源实现.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#118 AdaFactor优化器浅析（附开源实现）</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">滑动平均视角下的权重衰减和学习率</a><ul>
<li><a href="#_2">滑动平均</a></li>
<li><a href="#_3">迭代展开</a></li>
<li><a href="#_4">记忆周期</a></li>
<li><a href="#_5">动态版本</a></li>
<li><a href="#_6">最优调度</a></li>
<li><a href="#_7">一般结果</a></li>
<li><a href="#_8">假设讨论</a></li>
<li><a href="#_9">文章小结</a></li>
<li><a href="#_10">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>