<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>为什么线性注意力要加Short Conv？ | ML & Math Blog Posts</title>
    <meta name="description" content="为什么线性注意力要加Short Conv？
原文链接: https://spaces.ac.cn/archives/11320
发布日期: 2025-10-05

如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考《线性注意力简史：从模仿、创新到反哺》）模型都给$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}$加上了Sho...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">为什么线性注意力要加Short Conv？</h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-10-05</span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/11320" target="_blank">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <span class="tag"><i class="fas fa-tag"></i> 机器学习</span>
                
            </div>
            
        </header>

        <!-- Post Body -->
        <div class="post-content">
            <h1 id="short-conv">为什么线性注意力要加Short Conv？</h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11320">https://spaces.ac.cn/archives/11320</a></p>
<p><strong>发布日期</strong>: 2025-10-05</p>
<hr />
<p>如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考<a href="https://kexue.fm/archives/11033">《线性注意力简史：从模仿、创新到反哺》</a>）模型都给$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}$加上了Short Conv，比如下图所示的<a href="https://arxiv.org/abs/2406.06484">DeltaNet</a>：<br />
<a href="https://kexue.fm/usr/uploads/2025/10/175536171.png" title="点击查看原图"><img alt="DeltaNet中的Short Conv.png" src="https://kexue.fm/usr/uploads/2025/10/175536171.png" /></a></p>
<p>为什么要加这个Short Conv呢？直观理解可能是增加模型深度、增强模型的Token-Mixing能力等，说白了就是补偿线性化导致的表达能力下降。这个说法当然是大差不差，但它属于“万能模版”式的回答，我们更想对它的生效机制有更准确的认知。</p>
<p>接下来，笔者将给出自己的一个理解（更准确说应该是猜测）。</p>
<p><a href="https://spaces.ac.cn/archives/11320" title="为什么线性注意力要加Short Conv？">[...]</a></p>
<hr />
<h2 id="_1">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>

        <!-- Back to Home -->
        <div class="text-center mt-5 mb-4">
            <a href="../index.html" class="btn btn-outline-primary">
                <i class="fas fa-arrow-left"></i> 返回首页
            </a>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>
</body>
</html>
