<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA | ML & Math Blog Posts</title>
    <meta name="description" content="缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA&para;
原文链接: https://spaces.ac.cn/archives/10091
发布日期: 

前几天，幻方发布的DeepSeek-V2引起了大家的热烈讨论。首先，最让人哗然的是1块钱100万token的价格，普遍比现有的各种竞品API便宜了两个数量级，以至于有人调侃“这个价格哪怕它输出乱码，我也会认为这个乱码是一种艺术”；其...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=优化">优化</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #276 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#276</span>
                缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-05-13</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="mhamqagqamla">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA<a class="toc-link" href="#mhamqagqamla" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10091">https://spaces.ac.cn/archives/10091</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>前几天，幻方发布的<a href="https://papers.cool/arxiv/2405.04434">DeepSeek-V2</a>引起了大家的热烈讨论。首先，最让人哗然的是1块钱100万token的价格，普遍比现有的各种竞品API便宜了两个数量级，以至于有人调侃“这个价格哪怕它输出乱码，我也会认为这个乱码是一种艺术”；其次，从模型的技术报告看，如此便宜的价格背后的关键技术之一是它新提出的MLA（<strong>M</strong> ulti-head <strong>L</strong> atent <strong>A</strong> ttention），这是对GQA的改进，据说能比GQA更省更好，也引起了读者的广泛关注。</p>
<p>接下来，本文将跟大家一起梳理一下从MHA、MQA、GQA到MLA的演变历程，并着重介绍一下MLA的设计思路。</p>
<h2 id="mha">MHA<a class="toc-link" href="#mha" title="Permanent link">&para;</a></h2>
<p>MHA（<strong>M</strong> ulti-<strong>H</strong> ead <strong>A</strong> ttention），也就是多头注意力，是开山之作<a href="/archives/4765">《Attention is all you need》</a>所提出的一种Attention形式，可以说它是当前主流LLM的基础工作。在数学上，多头注意力MHA等价于多个独立的单头注意力的拼接，假设输入的（行）向量序列为$\boldsymbol{x}<em _leq="\leq" t="t">1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_l$，其中$\boldsymbol{x}_i\in\mathbb{R}^d$，那么MHA可以形式地记为<br />
\begin{equation}
\begin{gathered}
\boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>}^{(s)} ,\boldsymbol{v<em i_leq="i\leq" t="t">{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em> \\[15pt]}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\
\boldsymbol{k}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d\times d_k} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d\times d_v}
\end{gathered}<br />
\end{equation}<br />
简单起见，这里省略了Attention矩阵的缩放因子。实践上，常见的设置是$d_k = d_v = d / h$，对于LLAMA2-7b有$d=4096, h=32, d_k = d_v = 128$，LLAMA2-70b则是$d=8192,h=64, d_k = d_v = 128$</p>
<p>由于这里只考虑了主流的自回归LLM所用的Causal Attention，因此在token by token递归生成时，新预测出来的第$t+1$个token，并不会影响到已经算好的$\boldsymbol{k}<em _leq="\leq" t="t">{\leq t}^{(s)} ,\boldsymbol{v}</em>$，因此这部分结果我们可以缓存下来供后续生成调用，避免不必要的重复计算，这就是所谓的KV Cache。}^{(s)</p>
<p>而后面的MQA、GQA、MLA，都是围绕“如何减少KV Cache同时尽可能地保证效果”这个主题发展而来的产物。</p>
<h2 id="_1">瓶颈<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>一个自然的问题是：为什么降低KV Cache的大小如此重要？</p>
<p>众所周知，一般情况下LLM的推理都是在GPU上进行，单张GPU的显存是有限的，一部分我们要用来存放模型的参数和前向计算的激活值，这部分依赖于模型的体量，选定模型后它就是个常数；另外一部分我们要用来存放模型的KV Cache，这部分不仅依赖于模型的体量，还依赖于模型的输入长度，也就是在推理过程中是动态增长的，当Context长度足够长时，它的大小就会占主导地位，可能超出一张卡甚至一台机（8张卡）的总显存量。</p>
<p>在GPU上部署模型的原则是：能一张卡部署的，就不要跨多张卡；能一台机部署的，就不要跨多台机。这是因为“卡内通信带宽 &gt; 卡间通信带宽 &gt; 机间通信带宽”，由于“木桶效应”，模型部署时跨的设备越多，受设备间通信带宽的的“拖累”就越大，事实上即便是单卡H100内SRAM与HBM的带宽已经达到了3TB/s，但对于Short Context来说这个速度依然还是推理的瓶颈，更不用说更慢的卡间、机间通信了。</p>
<p>所以，减少KV Cache的目的就是要实现在更少的设备上推理更长的Context，或者在相同的Context长度下让推理的batch size更大，从而实现更快的推理速度或者更大的吞吐总量。当然，最终目的都是为了实现更低的推理成本。</p>
<p>要想更详细地了解这个问题，读者可以进一步阅读<a href="https://papers.cool/arxiv/2205.14135">《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》</a>、<a href="https://www.baseten.co/blog/llm-transformer-inference-guide/">《A guide to LLM inference and performance》</a>、<a href="https://zeux.io/2024/03/15/llm-inference-sol/">《LLM inference speed of light》</a>等文章，这里就不继续展开了（主要是笔者水平也有限，唯恐说多错多）。</p>
<h2 id="mqa">MQA<a class="toc-link" href="#mqa" title="Permanent link">&para;</a></h2>
<p>MQA，即“<strong>M</strong> ulti-<strong>Q</strong> uery <strong>A</strong> ttention”，是减少KV Cache的一次非常朴素的尝试，首次提出自<a href="https://papers.cool/arxiv/1911.02150">《Fast Transformer Decoding: One Write-Head is All You Need》</a>，这已经是2019年的论文了，这也意味着早在LLM火热之前，减少KV Cache就已经是研究人员非常关注的一个课题了。</p>
<p>MQA的思路很简单，直接让所有Attention Head共享同一个K、V，用公式来说，就是取消MHA所有的$\boldsymbol{k},\boldsymbol{v}$的上标${}^{(s)}$：<br />
\begin{equation}\require{cancel}
\begin{gathered}
\boldsymbol{o}<em _leq="\leq" t="t">t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>}^{\color{#ccc}{\smash{\bcancel{(s)}}}} ,\boldsymbol{v<em i_leq="i\leq" t="t">{\leq t}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\right)\triangleq\frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)\boldsymbol{v}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}}{\sum</em> \\[15pt]}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\
\boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \boldsymbol{x}_i\boldsymbol{W}_k^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_k} \\
\boldsymbol{v}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \boldsymbol{x}_i\boldsymbol{W}_v^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_v}
\end{gathered}<br />
\end{equation}<br />
使用MQA的模型包括<a href="https://arxiv.org/pdf/2204.02311">PaLM</a>、<a href="https://papers.cool/arxiv/2305.06161">StarCoder</a>、<a href="https://papers.cool/arxiv/2312.11805">Gemini</a>等。很明显，MQA直接将KV Cache减少到了原来的$1/h$，这是非常可观的，单从节省显存角度看已经是天花板了。</p>
<p>效果方面，目前看来大部分任务的损失都比较有限，且MQA的支持者相信这部分损失可以通过进一步训练来弥补回。此外，注意到MQA由于共享了K、V，将会导致Attention的参数量减少了将近一半，而为了模型总参数量的不变，通常会相应地增大FFN/GLU的规模，这也能弥补一部分效果损失。</p>
<h2 id="gqa">GQA<a class="toc-link" href="#gqa" title="Permanent link">&para;</a></h2>
<p>然而，也有人担心MQA对KV Cache的压缩太严重，以至于会影响模型的学习效率以及最终效果。为此，一个MHA与MQA之间的过渡版本GQA（<strong>G</strong> rouped-<strong>Q</strong> uery <strong>A</strong> ttention）应运而生，出自论文<a href="https://papers.cool/arxiv/2305.13245">《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》</a>，是去年的工作。</p>
<p>事后看来，GQA的思想也很朴素，它就是将所有Head分为$g$个组（$g$可以整除$h$），每组共享同一对K、V，用数学公式表示为<br />
\begin{equation}
\begin{gathered}
\boldsymbol{o}<em _leq="\leq" t="t">t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>}^{\color{red}{(\lceil sg/h\rceil)}} ,\boldsymbol{v<em i_leq="i\leq" t="t">{\leq t}^{\color{red}{(\lceil sg/h\rceil)}}\right)\triangleq\frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}}{}^{\top}\right)\boldsymbol{v}_i^{\color{red}{(\lceil sg/h\rceil)}}}{\sum</em> \\[15pt]}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}}{}^{\top}\right)
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\
\boldsymbol{k}_i^{\color{red}{(\lceil sg/h\rceil)}} = \boldsymbol{x}_i\boldsymbol{W}_k^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d\times d_k} \\
\boldsymbol{v}_i^{\color{red}{(\lceil sg/h\rceil)}} = \boldsymbol{x}_i\boldsymbol{W}_v^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{\color{red}{(\lceil sg/h\rceil)}}\in\mathbb{R}^{d\times d_v}
\end{gathered}<br />
\end{equation}<br />
这里的$\lceil\cdot\rceil$是上取整符号。GQA提供了MHA到MQA的自然过渡，当$g=h$时就是MHA，$g=1$时就是MQA，当$1 &lt; g &lt; h$时，它只将KV Cache压缩到$g/h$，压缩率不如MQA，但同时也提供了更大的自由度，效果上更有保证。GQA最知名的使用者，大概是Meta开源的<a href="https://llama.meta.com/llama2/">LLAMA2-70B</a>，以及<a href="https://llama.meta.com/llama3/">LLAMA3</a>全系列，此外使用GQA的模型还有<a href="https://papers.cool/arxiv/2312.08688">TigerBot</a>、<a href="https://papers.cool/arxiv/2401.02954">DeepSeek-V1</a>、<a href="https://papers.cool/arxiv/2402.19173">StarCoder2</a>、<a href="https://papers.cool/arxiv/2403.04652">Yi</a>、<a href="https://github.com/THUDM/ChatGLM2-6B">ChatGLM2</a>、<a href="https://github.com/THUDM/ChatGLM3">ChatGLM3</a>等，相比使用MQA的模型更多（ChatGLM虽然在它的介绍中说自己是MQA，但实际是$g=2$的GQA）。</p>
<p>在llama2/3-70B中，GQA的$g=8$，其他用了GQA的同体量模型基本上也保持了这个设置，这并非偶然，而是同样出于推理效率的考虑。我们知道，70B这个体量的模型，如果不进行极端的量化，那么不可能部署到单卡（A100/H100 80G）上。单卡不行，那么就能单机了，一般情况下一台机可以装8张卡，刚才我们说了，Attention的每个Head实际上是独立运算然后拼接起来的，当$g=8$时，正好可以每张卡负责计算一组K、V对应的Attention Head，这样可以在尽可能保证K、V多样性的同时最大程度上减少卡间通信。</p>
<h2 id="mla">MLA<a class="toc-link" href="#mla" title="Permanent link">&para;</a></h2>
<p>有了MHA、MQA、GQA的铺垫，我们理解MLA（<strong>M</strong> ulti-head <strong>L</strong> atent <strong>A</strong> ttention）就相对容易一些了。DeepSeek-V2的技术报告里是从低秩投影的角度引入MLA的，以至于有部分读者提出“为什么LoRA提出这么久了，直到MLA才提出对KV Cache低秩分解的做法”之类的疑问。</p>
<p>然而，笔者认为低秩投影这个角度并不贴近本质，因为要说低秩投影的话，事实上只要我们将GQA的所有K、V叠在一起，就会发现GQA也相当于在做低秩投影：<br />
\begin{equation}\underbrace{\left[\boldsymbol{k}<em _boldsymbol_c="\boldsymbol{c">i^{(1)},\cdots,\boldsymbol{k}_i^{(g)},\boldsymbol{v}_i^{(1)},\cdots,\boldsymbol{v}_i^{(g)}\right]}</em><em _boldsymbol_W="\boldsymbol{W">i\in\mathbb{R}^{g(d_k+d_v)}} = \boldsymbol{x}_i \underbrace{\left[\boldsymbol{W}_k^{(1)},\cdots,\boldsymbol{W}_k^{(g)},\boldsymbol{W}_v^{(1)},\cdots,\boldsymbol{W}_v^{(g)}\right]}</em>}_c\in\mathbb{R}^{d\times g(d_k+d_v)}}\end{equation
这里我们将所有$\boldsymbol{k}_i^{(s)},\boldsymbol{v}_i^{(s)}$拼在一起记为$\boldsymbol{c}_i$，相应的投影矩阵也拼在一起记为$\boldsymbol{W}_c$，注意到一般都有$d_c = g(d_k+d_v) &lt; d$，所以$\boldsymbol{x}_i$到$\boldsymbol{c}_i$的变换就是一个低秩投影。所以，MLA的本质改进不是低秩投影，而是低秩投影之后的工作。</p>
<h3 id="part-1">Part 1<a class="toc-link" href="#part-1" title="Permanent link">&para;</a></h3>
<p>GQA在投影之后做了什么呢？首先它将向量对半分为两份分别作为K、V，然后每一份又均分为$g$份，每一份复制$h/g$次，以此来“凑”够$h$个Attention Head所需要的K、V。我们知道分割、复制都是简单的线性变换，所以MLA的第一个想法是将这些简单的线性变换换成一般的线性变换，以增强模型的能力：<br />
\begin{equation}
\begin{gathered}
\boldsymbol{o}<em _leq="\leq" t="t">t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>}^{(s)} ,\boldsymbol{v<em i_leq="i\leq" t="t">{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em> \\[15pt]}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\
\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_c\times d_k} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \\[10pt]
\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c}
\end{gathered}<br />
\end{equation}<br />
然而，理论上这样是能增加模型能力，但别忘了GQA的主要目的是减少KV Cache，出于节省计算和通信成本的考虑，我们一般会缓存的是投影后的$\boldsymbol{k}_i, \boldsymbol{v}_i$而不是投影前的$\boldsymbol{c}_i$或$\boldsymbol{x}_i$，而MLA的这个做法，通过不同的投影矩阵再次让所有的K、V Head都变得各不相同，那么KV Cache的大小就恢复成跟MHA一样大了，违背了GQA的初衷。</p>
<p>对此，MLA发现，我们可以结合Dot-Attention的具体形式，通过一个简单但不失巧妙的恒等变换来规避这个问题。首先，在训练阶段还是照常进行，此时优化空间不大；然后，在推理阶段，我们利用<br />
\begin{equation}\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top} = \left(\boldsymbol{x}_t\boldsymbol{W}_q^{(s)}\right) \left(\boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\right){}^{\top} = \boldsymbol{x}_t\left(\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}\right)\boldsymbol{c}_i^{\top} \end{equation}<br />
这意味着推理阶段，我们可以将$\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$合并起来作为Q的投影矩阵，那么$\boldsymbol{c}_i$则取代了原本的$\boldsymbol{k}_i$，同理，在$\boldsymbol{o}_t$后面我们还有一个投影矩阵，于是$\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}$的$\boldsymbol{W}_v^{(s)}$也可以吸收到后面的投影矩阵中去，于是等效地$\boldsymbol{v}_i$也可以用$\boldsymbol{c}_i$代替，也就是说此时KV Cache只需要存下所有的$\boldsymbol{c}_i$就行，而不至于存下所有的$\boldsymbol{k}_i^{(s)}$、$\boldsymbol{v}_i^{(s)}$。注意到$\boldsymbol{c}_i$跟${}^{(s)}$无关，也就是说是所有头共享的，即MLA在推理阶段它可以恒等变换为一个MQA。</p>
<p>再次强调，本文的主题是一直都是减少KV Cache，那到目前为止，MLA做到了什么呢？答案是通过不同的投影矩阵来增强了GQA的能力，并且推理时可以保持同样大小的KV Cache。那么反过来，如果我们只需要跟GQA相近的能力，那么是不是就可以再次减少KV Cache了？换言之，$d_c$没必要取$g(d_k+d_v)$，而是取更小的值（DeepSeek-V2取了512），从而进一步压缩KV Cache，这就是MLA的核心思想。</p>
<blockquote>
<p><strong>补充说明：</strong></p>
<p>1、$\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$合并成一个矩阵的恒等变换，理论上只有在无限精度下才成立，实际上如果我们使用单精度尤其是BF16的话，经过变换后的精度损失往往还是挺明显的，经过多层累积后可能放大到比较可观的程度；</p>
<p>2、实际上我们一般不按照$\boldsymbol{x}_t\left(\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}\right)$来计算Q，而是按照$\left(\boldsymbol{x}_t\boldsymbol{W}_q^{(s)}\right)\boldsymbol{W}_k^{(s)}{}^{\top}$来计算，这样虽然是串行的，但在低秩假设下计算量更少，并且理论精度的损失也更少，不过在文章中，我们仍按照$\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$合并成一个矩阵来介绍。</p>
</blockquote>
<h3 id="part-2">Part 2<a class="toc-link" href="#part-2" title="Permanent link">&para;</a></h3>
<p>一切似乎都很完美，看上去一个又好又省的理想设计就要出炉了。不过别急，当我们再深入思考一下就会发现，到目前为止的MLA有一个难以绕开的缺陷——不兼容<a href="/archives/8265">RoPE（旋转位置编码）</a>。</p>
<p>刚才我们说了，MLA之所以能保持跟GQA一样大小的KV Cache，其关键一步是“将$\boldsymbol{W}<em m-n="m-n">q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$合并成一个（跟位置无关的）矩阵作为Q的投影矩阵”，但如果加了RoPE的话，这一步就无法实现了。这是因为RoPE是一个跟位置相关的、$d_k\times d_k$的分块对角矩阵$\boldsymbol{\mathcal{R}}_m$，满足$\boldsymbol{\mathcal{R}}_m\boldsymbol{\mathcal{R}}_n^{\top}=\boldsymbol{\mathcal{R}}</em>}$，MLA加入RoPE之后会让$\boldsymbol{W<em t-i="t-i">q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}$之间多插入了一项$\boldsymbol{\mathcal{R}}</em>$：<br />
\begin{equation}
\boldsymbol{q}<em t-i="t-i">i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\quad,\quad\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i} \\
\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top} = \left(\boldsymbol{x}_t\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_t}\right) \left(\boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\right){}^{\top} = \boldsymbol{x}_t\left(\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}</em>}}\boldsymbol{W<em t-i="t-i">k^{(s)}{}^{\top}\right)\boldsymbol{c}_i^{\top} \end{equation}<br />
这里的$\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}</em>$就无法合并为一个固定的投影矩阵了（跟位置差$t-i$相关），从而MLA的想法无法结合RoPE实现。}}\boldsymbol{W}_k^{(s)}{}^{\top</p>
<p>前段时间，笔者也很荣幸跟DeepSeek团队讨论过这个问题，但这个问题可以说非常本质，所以当时笔者实际上也没能提出什么有效的建议。最简单的方式是放弃RoPE，换用其他基于Attention Bias的位置编码，如<a href="/archives/9431#ALIBI">ALIBI</a>，但DeepSeek的实验显示它明显不如RoPE（注意，MLA不是不能加RoPE，而是加了RoPE之后无法用恒等变换技巧来减少KV Cache），笔者也提议过换<a href="/archives/9431#Sandwich">Sandwich</a>，它不像ALIBI单调衰减到负无穷，估计效果会好些，但感觉是治标不治本。还有一个折中的办法是将$\boldsymbol{q}<em m-n="m-n">i$的输入也改为$\boldsymbol{c}_i$，然后RoPE加在$\boldsymbol{c}_i$之后，即<br />
\begin{equation}\boldsymbol{q}_i^{(s)} = \boldsymbol{c}_i\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\boldsymbol{W}_q^{(s)},\quad\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\boldsymbol{W}_k^{(s)}\end{equation}<br />
这样$\boldsymbol{\mathcal{R}}_i$就可以吸收到$\boldsymbol{c}_i$中去，但这样就没有$\boldsymbol{\mathcal{R}}_m\boldsymbol{\mathcal{R}}_n^{\top}=\boldsymbol{\mathcal{R}}</em>$的运算了，此时的RoPE不再是通过绝对位置实现相对位置，而单纯是在Q、K上加绝对位置，让模型自己想办法提炼相对位置信息。</p>
<p>最后发布的MLA，采取了一种混合的方法——每个Attention Head的Q、K新增$d_r$个维度用来添加RoPE，其中K新增的维度每个Head共享：<br />
\begin{equation}
\begin{gathered}
\boldsymbol{o}<em _leq="\leq" t="t">t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>}^{(s)} ,\boldsymbol{v<em i_leq="i\leq" t="t">{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em>}\exp\left(\boldsymbol{q<em qc="qc">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\[15pt]
\boldsymbol{q}_i^{(s)} = \left[\boldsymbol{x}_i\boldsymbol{W}</em>}^{(s)}, \boldsymbol{x<em qr="qr">i\boldsymbol{W}</em>}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}<em qc="qc">i}\right]\in\mathbb{R}^{d_k + d_r},\quad \boldsymbol{W}</em>}^{(s)}\in\mathbb{R}^{d\times d_k},\boldsymbol{W<em kc="kc">{qr}^{(s)}\in\mathbb{R}^{d\times d_r}\\
\boldsymbol{k}_i^{(s)} = \left[\boldsymbol{c}_i\boldsymbol{W}</em>}^{(s)}, \boldsymbol{x<em kr="kr">i\boldsymbol{W}</em>}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}<em kc="kc">i}\right]\in\mathbb{R}^{d_k+d_r},\quad \boldsymbol{W}</em> \\}^{(s)}\in\mathbb{R}^{d_c\times d_k}, \boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r
\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \\[10pt]
\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c}
\end{gathered}<br />
\end{equation}<br />
这样一来，没有RoPE的维度就可以重复“Part 1”的操作，在推理时KV Cache只需要存$\boldsymbol{c}_i$，新增的带RoPE的维度就可以用来补充位置信息，并且由于所有Head共享，所以也就只有在K Cache这里增加了$d_r$个维度，原论文取了$d_r = d_k / 2 = 64$，相比原本的$d_c=512$，增加的幅度不大。</p>
<h3 id="part-3">Part 3<a class="toc-link" href="#part-3" title="Permanent link">&para;</a></h3>
<p>最后有一个细节，就是MLA的最终版本，还将Q的输入也改为了低秩投影形式，这与减少KV Cache无关，主要是为了减少训练期间参数量和相应的梯度（原论文说的是激活值，个人表示不大理解）所占的显存：<br />
\begin{equation}
\begin{gathered}
\boldsymbol{o}<em _leq="\leq" t="t">t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>}^{(s)} ,\boldsymbol{v<em i_leq="i\leq" t="t">{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em>}\exp\left(\boldsymbol{q<em qc="qc">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\[15pt]
\boldsymbol{q}_i^{(s)} = \left[\boldsymbol{c}_i'\boldsymbol{W}</em>}^{(s)}, \boldsymbol{c<em qr="qr">i'\boldsymbol{W}</em>}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}<em qc="qc">i}\right]\in\mathbb{R}^{d_k + d_r},\quad \boldsymbol{W}</em>}^{(s)}\in\mathbb{R}^{d_c'\times d_k},\boldsymbol{W<em kc="kc">{qr}^{(s)}\in\mathbb{R}^{d_c'\times d_r}\\
\boldsymbol{k}_i^{(s)} = \left[\boldsymbol{c}_i\boldsymbol{W}</em>}^{(s)}, \boldsymbol{x<em kr="kr">i\boldsymbol{W}</em>}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}<em kc="kc">i}\right]\in\mathbb{R}^{d_k+d_r},\quad \boldsymbol{W}</em>}^{(s)}\in\mathbb{R}^{d_c\times d_k}, \boldsymbol{W<em _leq="\leq" t="t">{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \\[10pt]
\boldsymbol{c}_i' = \boldsymbol{x}_i \boldsymbol{W}_c'\in\mathbb{R}^{d_c'},\quad \boldsymbol{W}_c'\in\mathbb{R}^{d\times d_c'} \\
\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \\
\end{gathered}<br />
\label{eq:mla-mha}\end{equation}<br />
注意$\boldsymbol{k}_i^{(s)}$中的第二项，带RoPE的部分，其输入还是$\boldsymbol{x}_i$而不是$\boldsymbol{c}_i$，这里保持了原论文的设置，不是笔误，$d_c'$原论文的取值是1536，跟$d_c=512$不同。同时，我们把带RoPE的MHA放在下面，方便大家对比：<br />
\begin{equation}
\begin{gathered}
\boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>}^{(s)} ,\boldsymbol{v<em i_leq="i\leq" t="t">{\leq t}^{(s)}\right)\triangleq\frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em> \\[15pt]}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\
\boldsymbol{k}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}}_i}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d\times d_k} \\
\boldsymbol{v}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d\times d_v}
\end{gathered}<br />
\end{equation}<br />
可以发现，其实在训练阶段，除了多了一步低秩投影以及只在部分维度加RoPE外，MLA与Q、K的Head Size由$d_k$换成$d_k + d_r$的MHA基本无异。</p>
<p>解码阶段的MLA则改为MQA形式<br />
\begin{equation}
\begin{gathered}
\boldsymbol{o}<em _leq="\leq" t="t">t = \left[\boldsymbol{o}_t^{(1)}\boldsymbol{W}_v^{(1)}, \boldsymbol{o}_t^{(2)}\boldsymbol{W}_v^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\boldsymbol{W}_v^{(h)}\right] \\[10pt]
\boldsymbol{o}_t^{(s)} = Attention\left(\boldsymbol{q}_t^{(s)}, \boldsymbol{k}</em>}^{\color{#ccc}{\smash{\bcancel{(s)}}}} ,\boldsymbol{c<em i_leq="i\leq" t="t">{\leq t}\right)\triangleq\frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)\boldsymbol{c}_i}{\sum</em>}\exp\left(\boldsymbol{q<em qc="qc">t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)} \\[15pt]
\boldsymbol{q}_i^{(s)} = \left[\boldsymbol{c}_i'\boldsymbol{W}</em>}^{(s)}\boldsymbol{W<em qr="qr">{kc}^{(s)}{}^{\top}, \boldsymbol{c}_i'\boldsymbol{W}</em>}^{(s)}\color{#3ce2f7}{\boldsymbol{\mathcal{R}<em kr="kr">i}\right]\in\mathbb{R}^{d_c + d_r}\\
\boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \left[\boldsymbol{c}_i, \boldsymbol{x}_i\boldsymbol{W}</em>}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\color{#3ce2f7}{\boldsymbol{\mathcal{R}<em qc="qc">i}\right]\in\mathbb{R}^{d_c+d_r}\\
\boldsymbol{W}</em>}^{(s)}\in\mathbb{R}^{d_c'\times d_k},\boldsymbol{W<em qr="qr">{kc}^{(s)}\in\mathbb{R}^{d_c\times d_k},\boldsymbol{W}</em> \\[10pt]}^{(s)}\in\mathbb{R}^{d_c'\times d_r},\boldsymbol{W}_{kr}^{\color{#ccc}{\smash{\bcancel{(s)}}}}\in\mathbb{R}^{d\times d_r
\boldsymbol{c}_i' = \boldsymbol{x}_i \boldsymbol{W}_c'\in\mathbb{R}^{d_c'},\quad \boldsymbol{W}_c'\in\mathbb{R}^{d\times d_c'} \\
\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c} \\
\end{gathered}<br />
\label{eq:mla-mqa}\end{equation}<br />
此时Q、K的Head Size变成了$d_c + d_r$，V的Head Size 则变成了$d_c$，按照原论文的设置，这是$d_k$、$d_v$的4倍。所以实际上MLA在解码阶段做的这个转换，虽然能有效减少KV Cache，但其解码的计算量是增加的。</p>
<p>那为什么还能提高推理效率呢？这又回到“瓶颈”一节所讨论的问题了，我们可以将LLM的推理分两部分：第一个Token的生成（Prefill）和后续每个Token的生成（Generation），Prefill阶段涉及到对输入所有Token的并行计算，然后把对应的KV Cache存下来，这部分对于计算、带宽和显存都是瓶颈，我们可以用MLA的MHA形式$\eqref{eq:mla-mha}$来算；但是Generation阶段由于每步只计算一个Token，实际上它更多的是带宽瓶颈和显存瓶颈，此时我们可以用MLA的MQA形式$\eqref{eq:mla-mqa}$来算，从而明显提高Generation的速度。</p>
<p>还有一个细节充分体现了这个特性。一般的LLM架构参数满足$h \times d_k = d$，即num_heads * head_size = hidden_size，但DeepSeek-V2不一样，它$d_k=128,d=5120$，但$h=128$，是一般设置的3倍！这是因为MLA的KV Cache大小跟$h$无关，增大$h$只会增加计算量和提升模型能力，但不会增加KV Cache，所以不会带来速度瓶颈。</p>
<h2 id="_2">小结<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>本文简单概述了多头注意力的演变历程，特别是从MHA向MQA、GQA，最终到MLA的变化理念，最后详细展开了对MLA的介绍。在本文中，MLA被视为GQA的一般化，它用投影矩阵的方式替代了GQA的分割、重复，并引入了一个恒等变换技巧来可以进一步压缩KV Cache，同时采用了一种混合方法来兼容RoPE。总的来说，MLA称得上是一种非常实用的注意力变体。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10091">https://spaces.ac.cn/archives/10091</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (May. 13, 2024). 《缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10091">https://spaces.ac.cn/archives/10091</a></p>
<p>@online{kexuefm-10091,<br />
title={缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA},<br />
author={苏剑林},<br />
year={2024},<br />
month={May},<br />
url={\url{https://spaces.ac.cn/archives/10091}},<br />
} </p>
<hr />
<h2 id="_3">公式推导与注释<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<h3 id="mha_1">一、多头注意力(MHA)的完整数学推导<a class="toc-link" href="#mha_1" title="Permanent link">&para;</a></h3>
<h4 id="11-mha">1.1 标准MHA的数学定义<a class="toc-link" href="#11-mha" title="Permanent link">&para;</a></h4>
<p>对于输入序列 $\boldsymbol{X} = [\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_l] \in \mathbb{R}^{l \times d}$，多头注意力机制可以形式化为：</p>
<p>\begin{equation}
\text{MHA}(\boldsymbol{X}) = \text{Concat}(\boldsymbol{O}^{(1)}, \boldsymbol{O}^{(2)}, \ldots, \boldsymbol{O}^{(h)}) \boldsymbol{W}_O
\tag{1}
\end{equation}</p>
<p>其中 $\boldsymbol{O}^{(s)} \in \mathbb{R}^{l \times d_v}$ 是第 $s$ 个头的输出，$\boldsymbol{W}_O \in \mathbb{R}^{hd_v \times d}$ 是输出投影矩阵。</p>
<p><strong>数学直觉</strong>: MHA通过多个独立的注意力头并行处理信息，每个头关注不同的表示子空间，最后拼接融合。这类似于卷积神经网络中的多通道机制。</p>
<h4 id="12">1.2 单头注意力的详细推导<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p>对于第 $s$ 个注意力头，计算过程为：</p>
<p>\begin{equation}
\boldsymbol{O}^{(s)} = \text{Attention}(\boldsymbol{Q}^{(s)}, \boldsymbol{K}^{(s)}, \boldsymbol{V}^{(s)})
\tag{2}
\end{equation}</p>
<p>展开注意力计算：</p>
<p>\begin{equation}
\text{Attention}(\boldsymbol{Q}^{(s)}, \boldsymbol{K}^{(s)}, \boldsymbol{V}^{(s)}) = \text{softmax}\left(\frac{\boldsymbol{Q}^{(s)} {\boldsymbol{K}^{(s)}}^\top}{\sqrt{d_k}}\right) \boldsymbol{V}^{(s)}
\tag{3}
\end{equation}</p>
<p>其中：
- $\boldsymbol{Q}^{(s)} = \boldsymbol{X}\boldsymbol{W}_q^{(s)} \in \mathbb{R}^{l \times d_k}$ (查询矩阵)
- $\boldsymbol{K}^{(s)} = \boldsymbol{X}\boldsymbol{W}_k^{(s)} \in \mathbb{R}^{l \times d_k}$ (键矩阵)
- $\boldsymbol{V}^{(s)} = \boldsymbol{X}\boldsymbol{W}_v^{(s)} \in \mathbb{R}^{l \times d_v}$ (值矩阵)</p>
<p><strong>缩放因子的数学原理</strong>: $1/\sqrt{d_k}$ 的缩放确保了点积的方差为1。假设 $\boldsymbol{q}$ 和 $\boldsymbol{k}$ 的每个元素独立同分布，均值为0，方差为1，则点积 $\boldsymbol{q}^\top \boldsymbol{k} = \sum_{i=1}^{d_k} q_i k_i$ 的方差为：</p>
<p>\begin{equation}
\text{Var}(\boldsymbol{q}^\top \boldsymbol{k}) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = d_k
\tag{4}
\end{equation}</p>
<p>因此除以 $\sqrt{d_k}$ 可以将方差归一化为1，防止softmax饱和。</p>
<h4 id="13">1.3 自回归生成中的因果注意力<a class="toc-link" href="#13" title="Permanent link">&para;</a></h4>
<p>对于自回归语言模型，第 $t$ 个token只能关注到位置 $\leq t$ 的token：</p>
<p>\begin{equation}
\boldsymbol{o}<em i="1">t^{(s)} = \sum</em>
\tag{5}
\end{equation}}^{t} \alpha_{t,i}^{(s)} \boldsymbol{v}_i^{(s)</p>
<p>其中注意力权重为：</p>
<p>\begin{equation}
\alpha_{t,i}^{(s)} = \frac{\exp\left(\boldsymbol{q}<em j="1">t^{(s)} {\boldsymbol{k}_i^{(s)}}^\top / \sqrt{d_k}\right)}{\sum</em>
\tag{6}
\end{equation}}^{t} \exp\left(\boldsymbol{q}_t^{(s)} {\boldsymbol{k}_j^{(s)}}^\top / \sqrt{d_k}\right)</p>
<p><strong>数学性质</strong>: 注意力权重满足 $\sum_{i=1}^{t} \alpha_{t,i}^{(s)} = 1$ 且 $\alpha_{t,i}^{(s)} \geq 0$，因此这是一个凸组合。</p>
<h4 id="14-mha">1.4 MHA的计算复杂度分析<a class="toc-link" href="#14-mha" title="Permanent link">&para;</a></h4>
<p><strong>时间复杂度</strong>:
1. QKV投影: $\mathcal{O}(3hld \cdot d_k) = \mathcal{O}(hld^2)$ (因为通常 $hd_k = d$)
2. 注意力矩阵计算: $\mathcal{O}(hl^2d_k)$
3. 注意力加权求和: $\mathcal{O}(hl^2d_v)$
4. 输出投影: $\mathcal{O}(ld \cdot hd_v) = \mathcal{O}(ld^2)$</p>
<p>总时间复杂度：</p>
<p>\begin{equation}
T_{\text{MHA}} = \mathcal{O}(ld^2 + hl^2d) = \mathcal{O}(l^2d)
\tag{7}
\end{equation}</p>
<p>(当 $l \gg d$ 时，$l^2d$ 项占主导)</p>
<p><strong>空间复杂度</strong>:
1. QKV矩阵: $\mathcal{O}(3hld_k) = \mathcal{O}(ld)$
2. 注意力矩阵: $\mathcal{O}(hl^2)$
3. 输出矩阵: $\mathcal{O}(ld)$</p>
<p>总空间复杂度：</p>
<p>\begin{equation}
S_{\text{MHA}} = \mathcal{O}(ld + hl^2)
\tag{8}
\end{equation}</p>
<h3 id="kv-cache">二、KV Cache的数学原理与内存分析<a class="toc-link" href="#kv-cache" title="Permanent link">&para;</a></h3>
<h4 id="21-kv-cache">2.1 KV Cache的必要性推导<a class="toc-link" href="#21-kv-cache" title="Permanent link">&para;</a></h4>
<p>在自回归生成中，生成第 $t+1$ 个token时，需要计算：</p>
<p>\begin{equation}
\boldsymbol{o}<em i="1">{t+1}^{(s)} = \sum</em>
\tag{9}
\end{equation}}^{t+1} \alpha_{t+1,i}^{(s)} \boldsymbol{v}_i^{(s)</p>
<p>注意到对于 $i \leq t$，$\boldsymbol{k}_i^{(s)}$ 和 $\boldsymbol{v}_i^{(s)}$ 在计算 $\boldsymbol{o}_t^{(s)}$ 时已经算过，无需重新计算。</p>
<p><strong>重复计算量分析</strong>: 如果不使用KV Cache，生成长度为 $L$ 的序列，总计算量为：</p>
<p>\begin{equation}
\text{Total FLOPs} = \sum_{t=1}^{L} \mathcal{O}(t \cdot hd_k) = \mathcal{O}(L^2hd_k)
\tag{10}
\end{equation}</p>
<p>使用KV Cache后，每步只需计算新token的KV：</p>
<p>\begin{equation}
\text{Total FLOPs (cached)} = \sum_{t=1}^{L} \mathcal{O}(hd_k) = \mathcal{O}(Lhd_k)
\tag{11}
\end{equation}</p>
<p><strong>加速比</strong>:</p>
<p>\begin{equation}
\text{Speedup} = \frac{\mathcal{O}(L^2hd_k)}{\mathcal{O}(Lhd_k)} = \mathcal{O}(L)
\tag{12}
\end{equation}</p>
<h4 id="22-mhakv-cache">2.2 MHA的KV Cache内存占用<a class="toc-link" href="#22-mhakv-cache" title="Permanent link">&para;</a></h4>
<p>对于MHA，每层需要缓存 $h$ 个头的K和V矩阵。对于长度为 $L$ 的序列：</p>
<p>单层KV Cache大小：</p>
<p>\begin{equation}
\text{Cache}_{\text{MHA}} = 2 \times h \times L \times d_k \times \text{sizeof(dtype)}
\tag{13}
\end{equation}</p>
<p>对于LLAMA2-7B ($d=4096, h=32, d_k=128$)，使用FP16 (2字节)：</p>
<p>\begin{equation}
\text{Cache}_{\text{MHA}}^{\text{per layer}} = 2 \times 32 \times L \times 128 \times 2 = 16384L \text{ bytes} = 16L \text{ KB}
\tag{14}
\end{equation}</p>
<p>LLAMA2-7B有32层，总KV Cache：</p>
<p>\begin{equation}
\text{Total Cache} = 32 \times 16L = 512L \text{ KB} = 0.5L \text{ MB}
\tag{15}
\end{equation}</p>
<p><strong>示例</strong>: 对于 $L=4096$ 的上下文：
- 单层: $16 \times 4096 = 65536$ KB = 64 MB
- 32层总计: $32 \times 64 = 2048$ MB = 2 GB</p>
<p>对于 $L=32768$ (32K上下文):
- 总KV Cache: $0.5 \times 32768 = 16384$ MB = 16 GB</p>
<h4 id="23-kv-cache">2.3 KV Cache的瓶颈分析<a class="toc-link" href="#23-kv-cache" title="Permanent link">&para;</a></h4>
<p><strong>内存带宽瓶颈</strong>: 假设GPU有带宽 $B$ (例如A100的HBM带宽为1.5 TB/s)，每次生成需要读取的KV Cache大小为 $S_{\text{cache}}$，则生成一个token的最小时间为：</p>
<p>\begin{equation}
t_{\text{min}} = \frac{S_{\text{cache}}}{B}
\tag{16}
\end{equation}</p>
<p>对于LLAMA2-7B，$L=4096$时 $S_{\text{cache}} = 2$ GB：</p>
<p>\begin{equation}
t_{\text{min}} = \frac{2 \text{ GB}}{1.5 \text{ TB/s}} = \frac{2}{1500} \text{ s} \approx 1.33 \text{ ms}
\tag{17}
\end{equation}</p>
<p><strong>吞吐量限制</strong>: 假设batch size为 $B_s$，每个请求的平均序列长度为 $\bar{L}$，GPU显存为 $M$，则最大batch size受限于：</p>
<p>\begin{equation}
B_s \leq \frac{M - M_{\text{model}}}{N_{\text{layer}} \times \text{Cache}_{\text{MHA}}(\bar{L})}
\tag{18}
\end{equation}</p>
<p>其中 $M_{\text{model}}$ 是模型参数占用的显存。</p>
<p><strong>数值示例</strong>: A100 80GB，LLAMA2-7B (FP16约14GB)，$\bar{L}=2048$：</p>
<p>\begin{equation}
B_s \leq \frac{80 - 14}{32 \times 0.5 \times 2048 / 1024} = \frac{66}{32} \approx 2
\tag{19}
\end{equation}</p>
<h3 id="mqa-multi-query-attention">三、MQA (Multi-Query Attention) 的数学推导<a class="toc-link" href="#mqa-multi-query-attention" title="Permanent link">&para;</a></h3>
<h4 id="31-mqa">3.1 MQA的动机与数学定义<a class="toc-link" href="#31-mqa" title="Permanent link">&para;</a></h4>
<p>MQA的核心思想是让所有头共享同一组K和V：</p>
<p>\begin{equation}
\boldsymbol{o}<em i="1">t^{(s)} = \sum</em>}^{t} \frac{\exp\left(\boldsymbol{q<em j="1">t^{(s)} \boldsymbol{k}_i^\top / \sqrt{d_k}\right)}{\sum</em>_i
\tag{20}
\end{equation}}^{t} \exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_j^\top / \sqrt{d_k}\right)} \boldsymbol{v</p>
<p>注意这里 $\boldsymbol{k}_i$ 和 $\boldsymbol{v}_i$ 没有上标 $(s)$，即所有头共享。</p>
<p><strong>参数对比</strong>:
- MHA: $h$ 个 $\boldsymbol{W}_q^{(s)}$，$h$ 个 $\boldsymbol{W}_k^{(s)}$，$h$ 个 $\boldsymbol{W}_v^{(s)}$
- MQA: $h$ 个 $\boldsymbol{W}_q^{(s)}$，1 个 $\boldsymbol{W}_k$，1 个 $\boldsymbol{W}_v$</p>
<h4 id="32-mqakv-cache">3.2 MQA的KV Cache压缩比<a class="toc-link" href="#32-mqakv-cache" title="Permanent link">&para;</a></h4>
<p>单层KV Cache：</p>
<p>\begin{equation}
\text{Cache}_{\text{MQA}} = 2 \times 1 \times L \times d_k \times \text{sizeof(dtype)}
\tag{21}
\end{equation}</p>
<p>压缩比：</p>
<p>\begin{equation}
\text{Compression Ratio} = \frac{\text{Cache}<em _text_MQA="\text{MQA">{\text{MHA}}}{\text{Cache}</em> = h
\tag{22}
\end{equation}}}} = \frac{2hLd_k}{2Ld_k</p>
<p><strong>数值示例</strong>: 对于LLAMA2-7B ($h=32$)，KV Cache减少到原来的 $1/32$：
- $L=4096$: MHA 2GB → MQA 64MB
- $L=32768$: MHA 16GB → MQA 512MB</p>
<h4 id="33-mqa">3.3 MQA的表达能力分析<a class="toc-link" href="#33-mqa" title="Permanent link">&para;</a></h4>
<p><strong>秩的角度</strong>: MHA的输出可以写成：</p>
<p>\begin{equation}
\boldsymbol{O}<em s="1">{\text{MHA}} = \sum</em>
\tag{23}
\end{equation}}^{h} \boldsymbol{A}^{(s)} \boldsymbol{V}^{(s)</p>
<p>其中 $\boldsymbol{A}^{(s)} \in \mathbb{R}^{l \times l}$ 是注意力矩阵。理论秩最高为 $\min(hd_v, l)$。</p>
<p>MQA的输出：</p>
<p>\begin{equation}
\boldsymbol{O}<em s="1">{\text{MQA}} = \left(\sum</em>
\tag{24}
\end{equation}}^{h} \boldsymbol{A}^{(s)}\right) \boldsymbol{V</p>
<p>理论秩最高为 $\min(d_v, l)$，降低了 $h$ 倍。</p>
<p><strong>容量损失</strong>: MQA相当于强制了一个低秩约束，可能降低模型的表达能力。实验表明对于大多数任务损失有限。</p>
<h4 id="34-mqa">3.4 MQA的参数量变化<a class="toc-link" href="#34-mqa" title="Permanent link">&para;</a></h4>
<p>MHA参数量：</p>
<p>\begin{equation}
P_{\text{MHA}} = h(d \times d_k + d \times d_k + d \times d_v) + hd_v \times d
\tag{25}
\end{equation}</p>
<p>当 $d_k = d_v = d/h$ 时：</p>
<p>\begin{equation}
P_{\text{MHA}} = 3d^2 + d^2 = 4d^2
\tag{26}
\end{equation}</p>
<p>MQA参数量：</p>
<p>\begin{equation}
P_{\text{MQA}} = hd \times d_k + d \times d_k + d \times d_v + hd_v \times d
\tag{27}
\end{equation}</p>
<p>\begin{equation}
P_{\text{MQA}} = d^2 + 2d \times \frac{d}{h} + d^2 = 2d^2 + \frac{2d^2}{h}
\tag{28}
\end{equation}</p>
<p>参数减少量：</p>
<p>\begin{equation}
\Delta P = 4d^2 - (2d^2 + \frac{2d^2}{h}) = 2d^2(1 - \frac{1}{h}) \approx 2d^2
\tag{29}
\end{equation}</p>
<p><strong>补偿策略</strong>: 为保持总参数量不变，通常增大FFN的隐藏层维度。</p>
<h3 id="gqa-grouped-query-attention">四、GQA (Grouped-Query Attention) 的数学推导<a class="toc-link" href="#gqa-grouped-query-attention" title="Permanent link">&para;</a></h3>
<h4 id="41-gqa">4.1 GQA的分组机制<a class="toc-link" href="#41-gqa" title="Permanent link">&para;</a></h4>
<p>GQA将 $h$ 个头分成 $g$ 组，每组共享一组K和V。设每组有 $h/g$ 个头：</p>
<p>\begin{equation}
\boldsymbol{o}<em i="1">t^{(s)} = \sum</em>, \quad s = 1, 2, \ldots, h
\tag{30}
\end{equation}}^{t} \alpha_{t,i}^{(s)} \boldsymbol{v}_i^{(\lceil sg/h \rceil)</p>
<p>其中 $\lceil sg/h \rceil$ 是第 $s$ 个头所属的组索引。</p>
<p><strong>示例</strong>: $h=8, g=2$:
- 组1: 头1,2,3,4 共享 $\boldsymbol{k}^{(1)}, \boldsymbol{v}^{(1)}$
- 组2: 头5,6,7,8 共享 $\boldsymbol{k}^{(2)}, \boldsymbol{v}^{(2)}$</p>
<h4 id="42-gqakv-cache">4.2 GQA的KV Cache分析<a class="toc-link" href="#42-gqakv-cache" title="Permanent link">&para;</a></h4>
<p>单层KV Cache：</p>
<p>\begin{equation}
\text{Cache}_{\text{GQA}} = 2 \times g \times L \times d_k \times \text{sizeof(dtype)}
\tag{31}
\end{equation}</p>
<p>压缩比：</p>
<p>\begin{equation}
\text{Compression Ratio} = \frac{\text{Cache}<em _text_GQA="\text{GQA">{\text{MHA}}}{\text{Cache}</em>
\tag{32}
\end{equation}}}} = \frac{h}{g</p>
<p><strong>参数化</strong>:
- $g=h$: GQA退化为MHA (无压缩)
- $g=1$: GQA退化为MQA (最大压缩)
- $1 &lt; g &lt; h$: 中间状态，平衡效果与效率</p>
<h4 id="43-llama2-70bgqa">4.3 LLAMA2-70B的GQA配置分析<a class="toc-link" href="#43-llama2-70bgqa" title="Permanent link">&para;</a></h4>
<p>LLAMA2-70B使用 $g=8$ 的GQA配置：
- $h=64$ (64个头)
- $g=8$ (8组)
- 每组 $64/8 = 8$ 个头</p>
<p><strong>内存节省</strong>:</p>
<p>\begin{equation}
\text{Saving} = 1 - \frac{g}{h} = 1 - \frac{8}{64} = 87.5\%
\tag{33}
\end{equation}</p>
<p><strong>分布式推理优势</strong>: 8组KV正好对应8张GPU卡，每张卡负责一组：</p>
<p>\begin{equation}
\text{Per-GPU Cache} = \frac{\text{Cache}_{\text{GQA}}}{8} = \frac{2gLd_k}{8} = \frac{Ld_k}{4} \times \text{sizeof(dtype)}
\tag{34}
\end{equation}</p>
<p>这避免了跨卡通信KV Cache的开销。</p>
<h4 id="44-gqa">4.4 GQA的低秩分解视角<a class="toc-link" href="#44-gqa" title="Permanent link">&para;</a></h4>
<p>将GQA的所有K和V拼接：</p>
<p>\begin{equation}
\boldsymbol{C} = [\boldsymbol{k}^{(1)}, \ldots, \boldsymbol{k}^{(g)}, \boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(g)}] \in \mathbb{R}^{l \times g(d_k + d_v)}
\tag{35}
\end{equation}</p>
<p>可以写成低秩投影：</p>
<p>\begin{equation}
\boldsymbol{C} = \boldsymbol{X} \boldsymbol{W}_c
\tag{36}
\end{equation}</p>
<p>其中 $\boldsymbol{W}_c \in \mathbb{R}^{d \times g(d_k + d_v)}$。</p>
<p><strong>秩约束</strong>: 由于 $g(d_k + d_v) &lt; h(d_k + d_v) = d$ (通常情况)，这是一个秩为 $g(d_k + d_v)$ 的低秩矩阵。</p>
<h3 id="mla-multi-head-latent-attention">五、MLA (Multi-head Latent Attention) 的深入推导<a class="toc-link" href="#mla-multi-head-latent-attention" title="Permanent link">&para;</a></h3>
<h4 id="51-mla">5.1 MLA的核心思想：低秩投影<a class="toc-link" href="#51-mla" title="Permanent link">&para;</a></h4>
<p>MLA的关键洞察是：GQA的分割和复制操作可以用更一般的线性变换替代。</p>
<p><strong>GQA的操作</strong>:
1. 投影到低维: $\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c \in \mathbb{R}^{d_c}$
2. 分割: 将 $\boldsymbol{c}_i$ 分成 $g$ 份
3. 复制: 每份复制 $h/g$ 次
4. 线性变换得到K和V</p>
<p><strong>MLA的改进</strong>: 用可学习的线性变换替代固定的分割和复制：</p>
<p>\begin{equation}
\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i \boldsymbol{W}_k^{(s)}, \quad \boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i \boldsymbol{W}_v^{(s)}
\tag{37}
\end{equation}</p>
<p>其中 $\boldsymbol{W}_k^{(s)} \in \mathbb{R}^{d_c \times d_k}$, $\boldsymbol{W}_v^{(s)} \in \mathbb{R}^{d_c \times d_v}$。</p>
<h4 id="52-mla">5.2 MLA的恒等变换技巧<a class="toc-link" href="#52-mla" title="Permanent link">&para;</a></h4>
<p><strong>训练阶段</strong>: 正常计算注意力</p>
<p>\begin{equation}
\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)\top} = (\boldsymbol{x}_t \boldsymbol{W}_q^{(s)}) (\boldsymbol{c}_i \boldsymbol{W}_k^{(s)})^\top
\tag{38}
\end{equation}</p>
<p><strong>推理阶段的恒等变换</strong>:</p>
<p>\begin{equation}
\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{(s)\top} = \boldsymbol{x}_t (\boldsymbol{W}_q^{(s)} \boldsymbol{W}_k^{(s)\top}) \boldsymbol{c}_i^\top
\tag{39}
\end{equation}</p>
<p>定义合并矩阵：</p>
<p>\begin{equation}
\boldsymbol{W}_{qk}^{(s)} = \boldsymbol{W}_q^{(s)} \boldsymbol{W}_k^{(s)\top} \in \mathbb{R}^{d \times d_c}
\tag{40}
\end{equation}</p>
<p>则：</p>
<p>\begin{equation}
\boldsymbol{q}<em qk="qk">t^{(s)} \boldsymbol{k}_i^{(s)\top} = (\boldsymbol{x}_t \boldsymbol{W}</em>_i^\top
\tag{41}
\end{equation}}^{(s)}) \boldsymbol{c</p>
<p><strong>关键洞察</strong>: KV Cache只需要存 $\boldsymbol{c}_i \in \mathbb{R}^{d_c}$，而不是 $h$ 个 $\boldsymbol{k}_i^{(s)} \in \mathbb{R}^{d_k}$ 和 $\boldsymbol{v}_i^{(s)} \in \mathbb{R}^{d_v}$！</p>
<h4 id="53-mlakv-cache">5.3 MLA的KV Cache大小<a class="toc-link" href="#53-mlakv-cache" title="Permanent link">&para;</a></h4>
<p>单层KV Cache (不含RoPE部分):</p>
<p>\begin{equation}
\text{Cache}_{\text{MLA}}^{\text{base}} = L \times d_c \times \text{sizeof(dtype)}
\tag{42}
\end{equation}</p>
<p>DeepSeek-V2设置 $d_c = 512$，相比MHA的 $h \times d_k = 32 \times 128 = 4096$，压缩比为：</p>
<p>\begin{equation}
\text{Compression} = \frac{4096}{512} = 8\times
\tag{43}
\end{equation}</p>
<h4 id="54-mlarope">5.4 MLA与RoPE的兼容性问题<a class="toc-link" href="#54-mlarope" title="Permanent link">&para;</a></h4>
<p><strong>矛盾</strong>: RoPE需要在K和Q上应用位置相关的旋转矩阵：</p>
<p>\begin{equation}
\boldsymbol{q}_t^{(s)} = \boldsymbol{x}_t \boldsymbol{W}_q^{(s)} \boldsymbol{\mathcal{R}}_t, \quad \boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i \boldsymbol{W}_k^{(s)} \boldsymbol{\mathcal{R}}_i
\tag{44}
\end{equation}</p>
<p>此时：</p>
<p>\begin{equation}
\boldsymbol{q}<em t-i="t-i">t^{(s)} \boldsymbol{k}_i^{(s)\top} = \boldsymbol{x}_t \boldsymbol{W}_q^{(s)} \boldsymbol{\mathcal{R}}_t \boldsymbol{\mathcal{R}}_i^\top \boldsymbol{W}_k^{(s)\top} \boldsymbol{c}_i^\top = \boldsymbol{x}_t \boldsymbol{W}_q^{(s)} \boldsymbol{\mathcal{R}}</em>_i^\top
\tag{45}
\end{equation}} \boldsymbol{W}_k^{(s)\top} \boldsymbol{c</p>
<p>由于 $\boldsymbol{\mathcal{R}}_{t-i}$ 依赖于位置差，无法预先合并到投影矩阵中！</p>
<h4 id="55-mla">5.5 MLA的混合解决方案<a class="toc-link" href="#55-mla" title="Permanent link">&para;</a></h4>
<p><strong>方案</strong>: 将Q和K分为两部分：</p>
<ol>
<li>
<p><strong>低秩部分</strong> (不加RoPE): 维度 $d_k$
\begin{equation}
\boldsymbol{q}<em qc="qc">{t,c}^{(s)} = \boldsymbol{x}_t \boldsymbol{W}</em>}^{(s)}, \quad \boldsymbol{k<em kc="kc">{i,c}^{(s)} = \boldsymbol{c}_i \boldsymbol{W}</em>
\tag{46}
\end{equation}}^{(s)</p>
</li>
<li>
<p><strong>RoPE部分</strong> (共享): 维度 $d_r$
\begin{equation}
\boldsymbol{q}<em qr="qr">{t,r}^{(s)} = \boldsymbol{x}_t \boldsymbol{W}</em>}^{(s)} \boldsymbol{\mathcal{R}<em i_r="i,r">t, \quad \boldsymbol{k}</em>} = \boldsymbol{x<em kr="kr">i \boldsymbol{W}</em>_i
\tag{47}
\end{equation}} \boldsymbol{\mathcal{R}</p>
</li>
</ol>
<p>注意 $\boldsymbol{k}_{i,r}$ 所有头共享 (无上标$(s)$)。</p>
<p><strong>最终的Q和K</strong>:</p>
<p>\begin{equation}
\boldsymbol{q}<em t_c="t,c">t^{(s)} = [\boldsymbol{q}</em>
\tag{48}
\end{equation}}^{(s)}, \boldsymbol{q}_{t,r}^{(s)}] \in \mathbb{R}^{d_k + d_r</p>
<p>\begin{equation}
\boldsymbol{k}<em i_c="i,c">i^{(s)} = [\boldsymbol{k}</em>
\tag{49}
\end{equation}}^{(s)}, \boldsymbol{k}_{i,r}] \in \mathbb{R}^{d_k + d_r</p>
<h4 id="56-mlakv-cache">5.6 MLA的最终KV Cache<a class="toc-link" href="#56-mlakv-cache" title="Permanent link">&para;</a></h4>
<p>需要缓存：
1. $\boldsymbol{c}<em i_r="i,r">i \in \mathbb{R}^{d_c}$ (低秩部分)
2. $\boldsymbol{k}</em>$ (RoPE部分，所有头共享)} \in \mathbb{R}^{d_r</p>
<p>单层总KV Cache：</p>
<p>\begin{equation}
\text{Cache}_{\text{MLA}} = L \times (d_c + d_r) \times \text{sizeof(dtype)}
\tag{50}
\end{equation}</p>
<p>DeepSeek-V2: $d_c = 512, d_r = 64$:</p>
<p>\begin{equation}
\text{Cache}_{\text{MLA}} = L \times 576 \times 2 = 1152L \text{ bytes}
\tag{51}
\end{equation}</p>
<p>相比MHA的 $2 \times 32 \times 128 \times L \times 2 = 16384L$ 字节:</p>
<p>\begin{equation}
\text{Compression} = \frac{16384}{1152} \approx 14.2\times
\tag{52}
\end{equation}</p>
<h4 id="57-mlaq">5.7 MLA的低秩投影Q<a class="toc-link" href="#57-mlaq" title="Permanent link">&para;</a></h4>
<p>为减少训练时的激活内存，MLA也对Q使用低秩投影：</p>
<p>\begin{equation}
\boldsymbol{c}_i' = \boldsymbol{x}_i \boldsymbol{W}_c' \in \mathbb{R}^{d_c'}
\tag{53}
\end{equation}</p>
<p>\begin{equation}
\boldsymbol{q}<em qc="qc">i^{(s)} = [\boldsymbol{c}_i' \boldsymbol{W}</em>}^{(s)}, \boldsymbol{c<em qr="qr">i' \boldsymbol{W}</em>_i]
\tag{54}
\end{equation}}^{(s)} \boldsymbol{\mathcal{R}</p>
<p>DeepSeek-V2: $d_c' = 1536$，相比 $d = 5120$ 节省约 70% 的Q投影参数。</p>
<h4 id="58-mla">5.8 MLA推理时的计算量分析<a class="toc-link" href="#58-mla" title="Permanent link">&para;</a></h4>
<p><strong>Prefill阶段</strong> (训练模式):
- QKV投影: $\mathcal{O}(Ld \cdot d_c') + \mathcal{O}(Lhd_c' \cdot d_k) + \mathcal{O}(Lhd_c \cdot d_v)$
- 注意力计算: $\mathcal{O}(L^2h(d_k + d_r))$</p>
<p><strong>Generation阶段</strong> (推理模式):
- Q计算: $\mathcal{O}(h \cdot d_c \cdot d_k)$ (通过 $\boldsymbol{W}<em qc="qc">{qk}^{(s)} = \boldsymbol{W}</em>$)
- 注意力计算: $\mathcal{O}(Lh(d_c + d_r))$}^{(s)} \boldsymbol{W}_{kc}^{(s)\top</p>
<p><strong>相比MHA的变化</strong>:
- 计算量增加: head size从 $d_k$ 变为 $d_c + d_r = 512 + 64 = 576$ (4.5倍)
- 但内存带宽减少: KV Cache减少14倍</p>
<p>由于Generation是内存带宽瓶颈，MLA仍然获得显著加速。</p>
<h3 id="-">六、性能-效率权衡的理论分析<a class="toc-link" href="#-" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 注意力机制的表达能力<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p><strong>定理</strong>: 对于序列长度为 $l$ 的输入，MHA的输出空间维度最多为 $\min(l, hd_v)$。</p>
<p><strong>证明</strong>: MHA的输出为：</p>
<p>\begin{equation}
\boldsymbol{O} = \text{Concat}\left(\sum_{i=1}^{l} \alpha_i^{(1)} \boldsymbol{v}<em i="1">i^{(1)}, \ldots, \sum</em>\right)
\tag{55}
\end{equation}}^{l} \alpha_i^{(h)} \boldsymbol{v}_i^{(h)</p>
<p>每个头的输出是 $l$ 个向量的凸组合，因此最多有 $l$ 个线性独立的输出。总共 $h$ 个头，每个头维度 $d_v$，因此最多 $\min(l, hd_v)$ 维。□</p>
<p><strong>推论</strong>: MQA的输出空间维度最多为 $\min(l, d_v)$，降低了 $h$ 倍。</p>
<h4 id="62-kv-cachetrade-off">6.2 KV Cache压缩与容量的trade-off<a class="toc-link" href="#62-kv-cachetrade-off" title="Permanent link">&para;</a></h4>
<p>定义<strong>有效容量</strong> $C$ 为模型能够建模的独立模式数量：</p>
<p>\begin{equation}
C_{\text{MHA}} \propto hd_v, \quad C_{\text{MQA}} \propto d_v, \quad C_{\text{GQA}} \propto gd_v
\tag{56}
\end{equation}</p>
<p>定义<strong>效率</strong> $E$ 为单位内存的吞吐量：</p>
<p>\begin{equation}
E \propto \frac{1}{\text{Cache Size}}
\tag{57}
\end{equation}</p>
<p><strong>Pareto前沿</strong>:</p>
<p>\begin{equation}
C \times E = \text{constant}
\tag{58}
\end{equation}</p>
<p>GQA通过参数 $g$ 在Pareto前沿上滑动，选择最优的容量-效率平衡点。</p>
<h4 id="63">6.3 数值示例：不同方案的对比<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p>假设 $d=4096, h=32, d_k=d_v=128, L=4096$:</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>KV Cache (MB)</th>
<th>有效容量</th>
<th>推理吞吐 (相对)</th>
</tr>
</thead>
<tbody>
<tr>
<td>MHA</td>
<td>2048</td>
<td>$32 \times 128 = 4096$</td>
<td>1.0×</td>
</tr>
<tr>
<td>GQA-8</td>
<td>256</td>
<td>$8 \times 128 = 1024$</td>
<td>8.0×</td>
</tr>
<tr>
<td>MQA</td>
<td>64</td>
<td>$128$</td>
<td>32×</td>
</tr>
<tr>
<td>MLA</td>
<td>144</td>
<td>$\approx 1536$ (有效)</td>
<td>14.2×</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>: MLA在几乎不损失容量的情况下，获得了接近MQA的效率提升。</p>
<h3 id="_4">七、实践建议与数值验证<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 选择合适的注意力机制<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p><strong>决策树</strong>:</p>
<ol>
<li><strong>内存充足</strong> ($L &lt; 2048$): 使用MHA，最大化模型容量</li>
<li><strong>中等序列</strong> ($2048 \leq L \leq 8192$): 使用GQA ($g=8$)，平衡效果与效率</li>
<li><strong>长序列</strong> ($L &gt; 8192$): 使用MLA或MQA，优先节省内存</li>
<li><strong>资源受限</strong>: 使用MQA，最大化吞吐</li>
</ol>
<h4 id="72-gqa">7.2 GQA的组数选择<a class="toc-link" href="#72-gqa" title="Permanent link">&para;</a></h4>
<p>经验公式：</p>
<p>\begin{equation}
g = \min\left(\frac{M_{\text{avail}}}{M_{\text{target}}}, h\right)
\tag{59}
\end{equation}</p>
<p>其中 $M_{\text{avail}}$ 是可用内存，$M_{\text{target}}$ 是目标KV Cache大小。</p>
<p><strong>示例</strong>: 希望KV Cache不超过1GB，$L=8192$:</p>
<p>\begin{equation}
2gLd_k \times 2 \leq 1024 \text{ MB} \Rightarrow g \leq \frac{1024 \times 10^6}{2 \times 8192 \times 128 \times 2} \approx 12
\tag{60}
\end{equation}</p>
<p>选择 $g=8$ (最接近的2的幂次)。</p>
<h4 id="73-mla">7.3 MLA的参数配置<a class="toc-link" href="#73-mla" title="Permanent link">&para;</a></h4>
<p><strong>低秩维度选择</strong>: DeepSeek-V2的经验是 $d_c \approx d/10$:</p>
<p>\begin{equation}
d_c = \max\left(\frac{d}{10}, 512\right)
\tag{61}
\end{equation}</p>
<p><strong>RoPE维度</strong>: 通常取 $d_r = d_k / 2$:</p>
<p>\begin{equation}
d_r = \frac{d_k}{2} = \frac{d/h}{2}
\tag{62}
\end{equation}</p>
<p><strong>Q的低秩维度</strong>: 约为K的3倍:</p>
<p>\begin{equation}
d_c' \approx 3d_c
\tag{63}
\end{equation}</p>
<h4 id="74">7.4 推理优化实践<a class="toc-link" href="#74" title="Permanent link">&para;</a></h4>
<p><strong>量化</strong>: FP16 → INT8可进一步减少2倍KV Cache:</p>
<p>\begin{equation}
\text{Cache}<em _text_FP16="\text{FP16">{\text{INT8}} = \frac{\text{Cache}</em>
\tag{64}
\end{equation}}}}{2</p>
<p><strong>分页注意力</strong>: 将KV Cache分块存储，减少内存碎片：</p>
<p>\begin{equation}
\text{Efficiency} = \frac{L \times \text{Block Size}}{\lceil L / \text{Block Size} \rceil \times \text{Block Size}} \geq 90\%
\tag{65}
\end{equation}</p>
<p>推荐Block Size = 64 或 128。</p>
<h3 id="_5">八、总结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h3>
<p>本文详细推导了从MHA到MLA的演变过程，关键公式总结：</p>
<p><strong>KV Cache大小</strong>:
\begin{equation}
\begin{cases}
\text{MHA}: &amp; 2hLd_k \
\text{MQA}: &amp; 2Ld_k \
\text{GQA}: &amp; 2gLd_k \
\text{MLA}: &amp; L(d_c + d_r)
\end{cases}
\tag{66}
\end{equation}</p>
<p><strong>压缩比</strong>:
\begin{equation}
\text{Ratio}_{\text{MLA}} = \frac{2hd_k}{d_c + d_r} = \frac{2 \times 32 \times 128}{512 + 64} \approx 14.2\times
\tag{67}
\end{equation}</p>
<p><strong>推理加速</strong>:
\begin{equation}
\text{Speedup} \approx \text{Compression Ratio} \times \text{Bandwidth Factor}
\tag{68}
\end{equation}</p>
<p>MLA通过巧妙的数学变换，在几乎不损失模型能力的前提下，实现了显著的内存节省和推理加速，是Transformer推理优化的重要里程碑。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈二十五基于恒等式的蒸馏上.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#275 生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="重温ssm一线性系统和hippo矩阵.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#277 重温SSM（一）：线性系统和HiPPO矩阵</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#mhamqagqamla">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA</a><ul>
<li><a href="#mha">MHA</a></li>
<li><a href="#_1">瓶颈</a></li>
<li><a href="#mqa">MQA</a></li>
<li><a href="#gqa">GQA</a></li>
<li><a href="#mla">MLA</a><ul>
<li><a href="#part-1">Part 1</a></li>
<li><a href="#part-2">Part 2</a></li>
<li><a href="#part-3">Part 3</a></li>
</ul>
</li>
<li><a href="#_2">小结</a></li>
<li><a href="#_3">公式推导与注释</a><ul>
<li><a href="#mha_1">一、多头注意力(MHA)的完整数学推导</a></li>
<li><a href="#kv-cache">二、KV Cache的数学原理与内存分析</a></li>
<li><a href="#mqa-multi-query-attention">三、MQA (Multi-Query Attention) 的数学推导</a></li>
<li><a href="#gqa-grouped-query-attention">四、GQA (Grouped-Query Attention) 的数学推导</a></li>
<li><a href="#mla-multi-head-latent-attention">五、MLA (Multi-head Latent Attention) 的深入推导</a></li>
<li><a href="#-">六、性能-效率权衡的理论分析</a></li>
<li><a href="#_4">七、实践建议与数值验证</a></li>
<li><a href="#_5">八、总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>