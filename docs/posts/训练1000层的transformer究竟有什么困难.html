<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>训练1000层的Transformer究竟有什么困难？ | ML & Math Blog Posts</title>
    <meta name="description" content="训练1000层的Transformer究竟有什么困难？&para;
原文链接: https://spaces.ac.cn/archives/8978
发布日期: 

众所周知，现在的Transformer越做越大，但这个“大”通常是“宽”而不是“深”，像GPT-3虽然参数有上千亿，但也只是一个96层的Transformer模型，与我们能想象的深度相差甚远。是什么限制了Transformer往“深”...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=优化">优化</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #171 训练1000层的Transformer究竟有什么困难？
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#171</span>
                训练1000层的Transformer究竟有什么困难？
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-03-09</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="1000transformer">训练1000层的Transformer究竟有什么困难？<a class="toc-link" href="#1000transformer" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8978">https://spaces.ac.cn/archives/8978</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，现在的Transformer越做越大，但这个“大”通常是“宽”而不是“深”，像GPT-3虽然参数有上千亿，但也只是一个96层的Transformer模型，与我们能想象的深度相差甚远。是什么限制了Transformer往“深”发展呢？可能有的读者认为是算力，但“宽而浅”的模型所需的算力不会比“窄而深”的模型少多少，所以算力并非主要限制，归根结底还是Transformer固有的训练困难。一般的观点是，深模型的训练困难源于梯度消失或者梯度爆炸，然而实践显示，哪怕通过各种手段改良了梯度，深模型依然不容易训练。</p>
<p>近来的一些工作（如<a href="https://papers.cool/arxiv/2004.08249">Admin</a>）指出，深模型训练的根本困难在于“增量爆炸”，即模型越深对输出的扰动就越大。上周的论文<a href="https://papers.cool/arxiv/2203.00555">《DeepNet: Scaling Transformers to 1,000 Layers》</a>则沿着这个思路进行尺度分析，根据分析结果调整了模型的归一化和初始化方案，最终成功训练出了1000层的Transformer模型。整个分析过程颇有参考价值，我们不妨来学习一下。</p>
<h2 id="_1">增量爆炸<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>原论文的完整分析比较长，而且有些假设或者描述细酌之下是不够合理的。所以在本文的分享中，笔者会尽量修正这些问题，试图以一个更合理的方式来得到类似结果。</p>
<p>假设损失函数为$\mathcal{L}(\boldsymbol{\theta})$，$\boldsymbol{\theta}$是它的参数，考虑参数由$\boldsymbol{\theta}$变为$\boldsymbol{\theta}+\Delta\boldsymbol{\theta}$时损失函数的增量：<br />
\begin{equation}\Delta\mathcal{L} = \mathcal{L}(\boldsymbol{\theta}+\Delta\boldsymbol{\theta}) - \mathcal{L}(\boldsymbol{\theta}) \approx \langle\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}),\Delta\boldsymbol{\theta}\rangle\end{equation}<br />
对于SGD有$\Delta\boldsymbol{\theta}=-\eta \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})$，那么$\Delta\mathcal{L} \approx -\eta\Vert\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\Vert^2$。设模型有$N$层，每层有$K$个参数矩阵（$K$接近常数），配合Xavier初始化以及各种Normalization手段，我们可以使得每个参数矩阵的梯度模长是$\mathcal{O}(1)$量级，所以有$\Delta\mathcal{L}=\mathcal{O}(\eta NK)$。因此，模型每一步的更新量是正比于模型深度$N$的，如果模型越深，那么更新量就越大，这意味着初始阶段模型越容易进入不大好的局部最优点，然后训练停滞甚至崩溃，这就是“增量爆炸”问题。</p>
<p>这时候解决方法有两个，一是初始阶段用更小的学习率进行训练（不超过$\eta/N$量级），然后慢慢增大学习率，这就是Warmup技巧；二就是调整初始化方案，使得参数的梯度是$\mathcal{O}(1/\sqrt{N})$量级，这样就自动抵消掉模型深度的影响。</p>
<h2 id="_2">量级分析<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>怎么做到第二种方案呢？我们可以尝试分析Transformer的梯度。然而，精确的梯度求起来比较繁琐，并且事实上我们也不需要精确的梯度，而只是要对梯度做一个量级分析，所以我们可以用如下的“量级分解”技巧转化为标量的导数问题。</p>
<p>对于一个矩阵$\boldsymbol{W}$，我们将其分解为$\boldsymbol{W}=\lambda \boldsymbol{U}$的形式，其中<br />
\begin{equation}\lambda = \mathop{\text{argmin}}_{\kappa &gt; 0} \Vert \boldsymbol{W}\boldsymbol{W}^{\top}/\kappa^2 - \boldsymbol{I}\Vert,\quad \end{equation}<br />
说白了，我们就是要将一个矩阵分解为一个标量$\lambda$与一个尽可能正交的矩阵$\boldsymbol{U}$之积。由于$\boldsymbol{U}$接近正交矩阵，它起到了一个标准参考系的作用，而对应的$\lambda$则代表了矩阵$\boldsymbol{W}$的量级。如果$\boldsymbol{W}$使用Xavier初始化，那么$\lambda$相当于其中的gain参数，即在Xavier初始化的基础上还要再乘一个$\lambda$。这是因为Xavier初始化的结果就接近一个正交矩阵，这一点可以参考<a href="/archives/7180">《从几何视角来理解模型参数的初始化策略》</a>。</p>
<p>在此分解之下，我们有<br />
\begin{equation}\frac{\partial \mathcal{L}(\lambda \boldsymbol{U})}{\partial \lambda} = \left\langle\frac{\partial \mathcal{L}(\lambda \boldsymbol{U})}{\partial (\lambda \boldsymbol{U})}, \boldsymbol{U}\right\rangle = \left\langle\frac{\partial \mathcal{L}(\boldsymbol{W})}{\partial \boldsymbol{W}}, \boldsymbol{U}\right\rangle\end{equation}<br />
这意味着$\frac{\partial \mathcal{L}}{\partial \lambda}$跟$\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}}$在量级上是成正比的，所以对$\frac{\partial \mathcal{L}}{\partial \lambda}$做量级分析就相当于对$\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}}$做量级分析。这样$\frac{\partial \mathcal{L}}{\partial \lambda}$就相当于$\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}}$量级的一个简单的“探针”，原来的矩阵求导就可以转化为标量求导，降低了分析难度。</p>
<h2 id="_3">前馈梯度<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>很多实验结果都显示虽然Pre Norm比Post Norm更容易训练，但Post Norm的最终效果往往更好些，所以原论文保留了Post Norm结构，并考虑了更一般的形式（DeepNorm）：<br />
\begin{equation}\boldsymbol{x}<em l_1="l+1">{l+1} = \text{LN}(\alpha\boldsymbol{x}_l + F(\boldsymbol{x}_l)) = \text{LN}(\boldsymbol{x}_l + F(\boldsymbol{x}_l)/\alpha)\end{equation}<br />
其中$\alpha &gt; 0$是一个常数。简单起见，我们先考虑FFN层，此时<br />
\begin{equation}\boldsymbol{x}</em>} = \text{LN}(\boldsymbol{x<em l_1="l+1">l + \phi(\boldsymbol{x}_l \boldsymbol{W}_1)\boldsymbol{W}_2/\alpha)\end{equation}<br />
这里的$\phi$是激活函数，一般为ReLU或其变体（Swish、GeLU等），它们（近似）满足$\phi(\lambda x) = \lambda \phi(x),\forall \lambda &gt; 0$。使用前一节的量级分解探针，我们得到<br />
\begin{equation}\boldsymbol{x}</em>} = \text{LN}(\underbrace{\boldsymbol{x<em _text_记为="\text{记为">l + \lambda_1 \lambda_2 \phi(\boldsymbol{x}_l \boldsymbol{U}_1)\boldsymbol{U}_2/\alpha}</em>}\boldsymbol{z<em l_1="l+1">{l+1}})\label{eq:ffn}\end{equation}<br />
求$\lambda$的梯度：<br />
\begin{equation}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \lambda_1} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>}}\frac{\partial \boldsymbol{z<em l_1="l+1">{l+1}}{\partial \lambda_1} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>}}\frac{\lambda_2 \phi(\boldsymbol{x<em l_1="l+1">l \boldsymbol{U}_1)\boldsymbol{U}_2}{\alpha} \\
\frac{\partial \mathcal{L}}{\partial \lambda_2} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>}}\frac{\partial \boldsymbol{z<em l_1="l+1">{l+1}}{\partial \lambda_2} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>}}\frac{\lambda_1 \phi(\boldsymbol{x<em l_1="l+1">l \boldsymbol{U}_1)\boldsymbol{U}_2}{\alpha} \end{aligned}\end{equation}<br />
我们断言$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}$、$\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>(1)$的，因此最终有}}$都是$\mathcal{O}(1)$的，并且由于$\boldsymbol{U}_1$、$\boldsymbol{U}_2$都接近正交矩阵，所以$\phi(\boldsymbol{x}_l \boldsymbol{U}_1)\boldsymbol{U}_2$也是$\mathcal{O<br />
\begin{equation}\frac{\partial \mathcal{L}}{\partial \lambda_1} = \mathcal{O}\left(\frac{\lambda_2}{\alpha}\right),\quad \frac{\partial \mathcal{L}}{\partial \lambda_2} = \mathcal{O}\left(\frac{\lambda_1}{\alpha}\right)\end{equation}</p>
<h2 id="_4">自注意力<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>现在考虑自Self Attention，作为量级分析，我们考虑单头注意力即可，其形式为<br />
\begin{equation}\boldsymbol{x}<em l_1="l+1">{l+1} = \text{LN}(\boldsymbol{x}_l + \sigma(\boldsymbol{x}_l \boldsymbol{W}_q\boldsymbol{W}_k^{\top}\boldsymbol{x}_l^{\top})\boldsymbol{x}_l\boldsymbol{W}_v\boldsymbol{W}_o/\alpha)\end{equation}<br />
其中$\sigma(\cdot)$是softmax操作的简写，这里省略了Attention的scale操作。对上式进行量级分解后的形式为<br />
\begin{equation}\boldsymbol{x}</em>} = \text{LN}(\underbrace{\boldsymbol{x<em _text_记为="\text{记为">l + \lambda_v\lambda_o \sigma (\lambda_q\lambda_k\boldsymbol{x}_l \boldsymbol{U}_q\boldsymbol{U}_k^{\top}\boldsymbol{x}_l^{\top})\boldsymbol{x}_l\boldsymbol{U}_v\boldsymbol{U}_o/\alpha}</em>}\boldsymbol{z<em l_1="l+1">{l+1}})\label{eq:sa}\end{equation}<br />
现在我们可以对各个$\lambda$分别求梯度，而由于softmax的存在，事实上$\lambda_q,\lambda_k$的梯度本身会很小，不会明显影响最终的更新量，所以其实我们考虑$\lambda_v,\lambda_o$的更新量足矣：<br />
\begin{equation}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \lambda_v} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>}}\frac{\partial \boldsymbol{z<em l_1="l+1">{l+1}}{\partial \lambda_v} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>}}\frac{\lambda_o \sigma (\lambda_q\lambda_k\boldsymbol{x<em l_1="l+1">l \boldsymbol{U}_q\boldsymbol{U}_k^{\top}\boldsymbol{x}_l^{\top})\boldsymbol{x}_l\boldsymbol{U}_v\boldsymbol{U}_o}{\alpha} \\
\frac{\partial \mathcal{L}}{\partial \lambda_o} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>}}\frac{\partial \boldsymbol{z<em l_1="l+1">{l+1}}{\partial \lambda_o} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>}}\frac{\lambda_v \sigma (\lambda_q\lambda_k\boldsymbol{x<em l_1="l+1">l \boldsymbol{U}_q\boldsymbol{U}_k^{\top}\boldsymbol{x}_l^{\top})\boldsymbol{x}_l\boldsymbol{U}_v\boldsymbol{U}_o}{\alpha} \end{aligned}\end{equation}<br />
同样断言$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}}$、$\frac{\partial \boldsymbol{x<em l_1="l+1">{l+1}}{\partial \boldsymbol{z}</em>(1)$的，因此结果跟FFN层的类似：}}$都是$\mathcal{O}(1)$的，并且注意softmax出来是一个概率分布，然后对$\boldsymbol{x}_l$的各个token做加权平均，通常而言，平均前后的向量会在同一数量级，所以我们认为$\sigma (\lambda_q\lambda_k\boldsymbol{x}_l \boldsymbol{U}_q\boldsymbol{U}_k^{\top}\boldsymbol{x}_l^{\top})\boldsymbol{x}_l\boldsymbol{U}_v\boldsymbol{U}_o$也是$\mathcal{O<br />
\begin{equation}\frac{\partial \mathcal{L}}{\partial \lambda_v} = \mathcal{O}\left(\frac{\lambda_o}{\alpha}\right),\quad \frac{\partial \mathcal{L}}{\partial \lambda_o} = \mathcal{O}\left(\frac{\lambda_v}{\alpha}\right)\end{equation}</p>
<h2 id="_5">初步结论<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>现在不管是FFN还是Self Attention，我们都得到了相似的结论，现在简单起见，假设每个参数的量级（至少在初始化阶段）是一致的，即所有的$\lambda$取同一个值，那么总的结论是<br />
\begin{equation}\frac{\partial \mathcal{L}}{\partial \lambda} = \mathcal{O}\left(\frac{\lambda}{\alpha}\right)\end{equation}<br />
即梯度的量级是$\mathcal{O}(\lambda/\alpha)$。另一方面，我们说$N$层的Transformer模型，一般是$N$层的Self Attention加$N$层的FFN，所以严格来说层数是$2N$。因此，按照“增量爆炸”一节的分析，我们需要将梯度调整到$\mathcal{O}(1/\sqrt{2N})$，上式告诉我们可以通过让$\lambda/\alpha=1/\sqrt{2N}$来实现。原论文的放缩更为宽松一些，得到的结果是$\lambda/\alpha = 1/\sqrt{4N}$，量级上是等价的。</p>
<p>现在我们得到了$\lambda$与$\alpha$的一个比例关系，但无法直接得到$\lambda$和$\alpha$的具体值。按照论文的说法，是从对称角度出发，让$\lambda=1/\alpha$，从而可以解得<br />
\begin{equation}\alpha = (2N)^{1/4},\quad \lambda = (2N)^{-1/4}\label{eq:result}\end{equation}<br />
然而，单纯对称的解释显然是不够说服力的，我们需要搞清楚不同的选择究竟有什么不同的结果。为此，我们可以比较另外两组解：</p>
<blockquote>
<p><strong>另解一：</strong> $\alpha=1,\lambda=(2N)^{-1/2}$，此时参数的初始化缩小到原来的$(2N)^{-1/2}$倍，梯度也被缩小到原来的$(2N)^{-1/2}$倍，根据SGD的$\Delta\boldsymbol{\theta}=-\eta \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})$得出每步的更新量也是原来的$(2N)^{-1/2}$倍，也就是说，调整前后的相对学习幅度是没有变化的，因此有可能刚开始$\lambda=\mathcal{O}((2N)^{-1/2})$级别，但训练集几步后就脱离了这个量级了。</p>
<p><strong>另解二：</strong> $\alpha=(2N)^{1/2},\lambda=1$，此时参数的初始化没有缩小，但梯度也被缩小到原来的$(2N)^{-1/2}$倍，根据SGD的$\Delta\boldsymbol{\theta}=-\eta \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})$得出每步的更新量也是原来的$(2N)^{-1/2}$倍，调整前后的相对学习幅度是明显缩小了，因此有可能出现学习得非常慢的情况。</p>
</blockquote>
<p>这两种情况看上去都各有缺点，因此介乎两者之间的式$\eqref{eq:result}$似乎就能解释得通了，它就是保持梯度缩放到原来的$(2N)^{-1/2}$倍的同时，让初始学习步伐稍微慢一些，但又不至于太慢，隐式地起到了Warmup的作用。</p>
<h2 id="_6">多种优化<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>上面的分析都是基于SGD进行的，但事实上我们很少直接用SGD去训练NLP模型，我们更多是自适应学习率优化器，主要有两大类：一是用二阶矩来校正学习率，Adam、AdamW等都属此类；另一类是通过参数模长进一步校正学习率，比如<a href="/archives/7094">LAMB</a>、<a href="/archives/7302">AdaFactor</a>。原论文的说法是“我们在SGD上进行推导，然后在Adam上验证发现也还可以”，但从理论上来讲，它们并不完全通用，这一节我们就来针对性地做一下分析。</p>
<p>对于Adam类优化器来说，每一步的更新量大约为$\Delta\boldsymbol{\theta}=-\eta\,\text{sign}(\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}))$，所以$\Delta\mathcal{L} \approx -\eta\Vert\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\Vert_1$，它是正比于梯度的1次方而不是2次方，因此要过要让更新量跟层数无关，那么梯度应该缩小到原来的$1/(2N)$倍才对，即应该有$\lambda/\alpha=1/(2N)$，如果同样让$\lambda=1/\alpha$，那么有<br />
\begin{equation}\alpha = (2N)^{1/2},\quad \lambda = (2N)^{-1/2}\end{equation}</p>
<p>对于LAMB类优化器来说，每一步更新量大约为$\Delta\boldsymbol{\theta}=-\eta\Vert\theta\Vert\,\text{sign}(\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}))$，所以$\Delta\mathcal{L} \approx -\eta\Vert\theta\Vert\Vert\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\Vert_1$，注意到参数的缩放比例是$\lambda$、梯度的缩放比例是$\lambda/\alpha$，所以$\Delta\mathcal{L}=\mathcal{O}(2N\lambda^2/\alpha)$，从而是$\lambda^2/\alpha=1/(2N)$。注意这类优化器每步的相对更新量是一样的（等于学习率$\eta$），不管怎么调整$\alpha,\lambda$其相对更新大小都不会变化，所以我们可以直接取$\alpha=1,\lambda=(2N)^{-1/2}$。</p>
<p>结果汇总对比如下：<br />
\begin{array}{c|cc|cc}
\hline
\text{优化器} &amp; \Delta\boldsymbol{\theta} &amp; \Delta\mathcal{L} &amp; \alpha &amp; \lambda \\
\hline
\text{SGD} &amp; -\eta \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}) &amp; -\eta\Vert\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\Vert^2 &amp; (2N)^{1/4} &amp; (2N)^{-1/4}\\
\text{Adam} &amp; -\eta\,\text{sign}(\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})) &amp; -\eta\Vert\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\Vert_1 &amp; (2N)^{1/2}&amp; (2N)^{-1/2}\\
\text{LAMB} &amp; -\eta\Vert\theta\Vert\,\text{sign}(\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})) &amp; -\eta\Vert\theta\Vert\Vert\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\Vert_1 &amp; 1 &amp; (2N)^{-1/2}\\
\hline
\end{array}</p>
<h2 id="_7">事后分析<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>前面的两节推导过程都用到了断言“$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">{l+1}}$、$\frac{\partial \boldsymbol{x}</em>(1)$的”，那么它是否成立呢？这里我们事后分析一下。}}{\partial \boldsymbol{z}_{l+1}}$都是$\mathcal{O</p>
<p>其实也很简单，经过前述调整后，不管是FFN层$\eqref{eq:ffn}$还是Self Attention层$\eqref{eq:sa}$，初始阶段每个残差分支的权重被缩放到原来的$\lambda^2/\alpha$倍，不管是哪种优化器的结果，$\lambda^2/\alpha$都是一个比较小的数字，这意味着初始阶段整个模型其实接近一个恒等函数，因此$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">{l+1}}$、$\frac{\partial \boldsymbol{x}</em>(1)$的，所以结论和断言是自洽的。}}{\partial \boldsymbol{z}_{l+1}}$自然都是$\mathcal{O</p>
<p>另外，可能有读者想问同样的分析是否可以用到Pre Norm结构上呢？答案是可以的，并且结论是基本一致的，只是因为Norm放在了残差分支之前，所以就没必要设置$\alpha$参数了，所以结论就是上述关于Post Norm的结果中所有的$\alpha$都等于为1，然后重新计算相应的$\lambda$。</p>
<p>最后，读者可能有疑问的是花了那么多功夫讨论把模型做深，那么模型深度真有那么重要吗？有，原论文给出了一个漂亮的实验结果，用一个200层的“深而窄”的模型（32亿参数），战胜了之前48层“浅而宽”的SOTA模型（120亿参数）：  </p>
<p><a href="/usr/uploads/2022/03/2952207079.png" title="点击查看原图"><img alt="“深而窄”的模型胜于“浅而宽”的模型" src="/usr/uploads/2022/03/2952207079.png" /></a></p>
<p>“深而窄”的模型胜于“浅而宽”的模型</p>
<h2 id="_8">文章小结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>本文分析了将Transformer做“深”的瓶颈所在并给出了相应的解决方案，文章的主要思路源于微软新出的DeepNet，并对原论文的分析过程做了一定的简化和完善。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8978">https://spaces.ac.cn/archives/8978</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Mar. 09, 2022). 《训练1000层的Transformer究竟有什么困难？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8978">https://spaces.ac.cn/archives/8978</a></p>
<p>@online{kexuefm-8978,<br />
title={训练1000层的Transformer究竟有什么困难？},<br />
author={苏剑林},<br />
year={2022},<br />
month={Mar},<br />
url={\url{https://spaces.ac.cn/archives/8978}},<br />
} </p>
<hr />
<h2 id="_9">公式推导与注释<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<h3 id="_10">一、深层网络的梯度问题数学分析<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 梯度消失与梯度爆炸的基础理论<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>考虑一个 $N$ 层的深度神经网络，损失函数为 $\mathcal{L}$。第 $l$ 层的参数 $\boldsymbol{\theta}_l$ 的梯度可以通过链式法则表示：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\theta}<em i="l+1">l} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_N} \prod</em>}^{N} \frac{\partial \boldsymbol{x<em i-1="i-1">i}{\partial \boldsymbol{x}</em>
\tag{1}
\end{equation}}} \frac{\partial \boldsymbol{x}_l}{\partial \boldsymbol{\theta}_l</p>
<p>其中 $\boldsymbol{x}_i$ 是第 $i$ 层的输出。</p>
<p><strong>关键观察</strong>: 梯度包含一个连乘项 $\prod_{i=l+1}^{N} \frac{\partial \boldsymbol{x}<em i-1="i-1">i}{\partial \boldsymbol{x}</em>$，这是梯度传播的雅可比矩阵链。}</p>
<h4 id="12">1.2 雅可比矩阵的范数分析<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p>设每层的雅可比矩阵 $\boldsymbol{J}<em i-1="i-1">i = \frac{\partial \boldsymbol{x}_i}{\partial \boldsymbol{x}</em>_i|$。则：}}$，其范数为 $|\boldsymbol{J</p>
<p>\begin{equation}
\left|\prod_{i=l+1}^{N} \boldsymbol{J}<em i="l+1">i\right| \leq \prod</em>_i|
\tag{2}
\end{equation}}^{N} |\boldsymbol{J</p>
<p><strong>情况1: 梯度消失</strong> - 如果 $|\boldsymbol{J}_i| &lt; 1$，设 $|\boldsymbol{J}_i| = \gamma &lt; 1$:</p>
<p>\begin{equation}
\left|\prod_{i=l+1}^{N} \boldsymbol{J}_i\right| \leq \gamma^{N-l}
\tag{3}
\end{equation}</p>
<p>当 $N-l$ 很大时，$\gamma^{N-l} \to 0$，导致梯度消失。</p>
<p><strong>情况2: 梯度爆炸</strong> - 如果 $|\boldsymbol{J}_i| &gt; 1$，设 $|\boldsymbol{J}_i| = \beta &gt; 1$:</p>
<p>\begin{equation}
\left|\prod_{i=l+1}^{N} \boldsymbol{J}_i\right| \geq \beta^{N-l}
\tag{4}
\end{equation}</p>
<p>当 $N-l$ 很大时，$\beta^{N-l} \to \infty$，导致梯度爆炸。</p>
<p><strong>数值示例</strong>: 假设 $\gamma = 0.9$，$N = 100$，$l = 1$:</p>
<p>\begin{equation}
0.9^{99} \approx 2.65 \times 10^{-5}
\tag{5}
\end{equation}</p>
<p>梯度衰减了约 $10^5$ 倍！</p>
<h4 id="13-transformer">1.3 Transformer中的梯度传播<a class="toc-link" href="#13-transformer" title="Permanent link">&para;</a></h4>
<p>标准Transformer的一层包含两个子层：
1. 自注意力层: $\boldsymbol{y}<em l_1="l+1">l = \boldsymbol{x}_l + \text{Attention}(\boldsymbol{x}_l)$
2. FFN层: $\boldsymbol{x}</em>_l)$} = \boldsymbol{y}_l + \text{FFN}(\boldsymbol{y</p>
<p>不含残差时，雅可比矩阵为：</p>
<p>\begin{equation}
\boldsymbol{J}_l = \frac{\partial \text{FFN}(\text{Attention}(\boldsymbol{x}_l))}{\partial \boldsymbol{x}_l}
\tag{6}
\end{equation}</p>
<p>含残差时（Post-LN）：</p>
<p>\begin{equation}
\boldsymbol{J}_l = \boldsymbol{I} + \frac{\partial \text{FFN}(\boldsymbol{y}_l)}{\partial \boldsymbol{y}_l} \left(\boldsymbol{I} + \frac{\partial \text{Attention}(\boldsymbol{x}_l)}{\partial \boldsymbol{x}_l}\right)
\tag{7}
\end{equation}</p>
<p><strong>关键洞察</strong>: 残差连接引入的 $\boldsymbol{I}$ 项确保 $\boldsymbol{J}_l$ 的特征值至少有1，防止梯度消失。</p>
<h3 id="_11">二、增量爆炸问题的深入分析<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 增量爆炸的数学定义<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p>对于参数更新 $\Delta \boldsymbol{\theta} = -\eta \nabla_{\boldsymbol{\theta}} \mathcal{L}$，损失函数的一阶近似变化为：</p>
<p>\begin{equation}
\Delta \mathcal{L} = \mathcal{L}(\boldsymbol{\theta} + \Delta\boldsymbol{\theta}) - \mathcal{L}(\boldsymbol{\theta}) \approx \langle \nabla_{\boldsymbol{\theta}} \mathcal{L}, \Delta\boldsymbol{\theta} \rangle
\tag{8}
\end{equation}</p>
<p>对于SGD:</p>
<p>\begin{equation}
\Delta \mathcal{L} \approx -\eta |\nabla_{\boldsymbol{\theta}} \mathcal{L}|^2
\tag{9}
\end{equation}</p>
<p><strong>问题</strong>: 对于 $N$ 层Transformer，每层有 $K$ 个参数矩阵（Self-Attention有QKV和输出，FFN有两层，通常 $K \approx 6$）。</p>
<h4 id="22">2.2 深度模型的梯度范数<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>假设每个参数矩阵 $\boldsymbol{W}<em _boldsymbol_W="\boldsymbol{W">i$ 的梯度范数为 $|\nabla</em>| = g$ (通过归一化和初始化控制)。}_i} \mathcal{L</p>
<p>总梯度范数的平方：</p>
<p>\begin{equation}
|\nabla_{\boldsymbol{\theta}} \mathcal{L}|^2 = \sum_{i=1}^{NK} |\nabla_{\boldsymbol{W}_i} \mathcal{L}|^2 = NKg^2
\tag{10}
\end{equation}</p>
<p>因此：</p>
<p>\begin{equation}
\Delta \mathcal{L} \approx -\eta NK g^2 = \mathcal{O}(\eta N)
\tag{11}
\end{equation}</p>
<p><strong>关键结论</strong>: 更新量正比于层数 $N$！</p>
<h4 id="23">2.3 增量爆炸的后果<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p>对于深层网络（$N$ 很大）：
1. 初始阶段梯度步长过大
2. 容易跳过局部最优，进入次优区域
3. 训练不稳定，甚至发散</p>
<p><strong>数值示例</strong>: $N=100$ 层 vs $N=10$ 层，相同学习率下更新量差 10 倍！</p>
<p>\begin{equation}
\frac{\Delta \mathcal{L}<em N="10">{N=100}}{\Delta \mathcal{L}</em> = 10
\tag{12}
\end{equation}}} = \frac{100}{10</p>
<h3 id="layernorm">三、LayerNorm的数学推导<a class="toc-link" href="#layernorm" title="Permanent link">&para;</a></h3>
<h4 id="31-layernorm">3.1 LayerNorm的定义<a class="toc-link" href="#31-layernorm" title="Permanent link">&para;</a></h4>
<p>对于输入 $\boldsymbol{x} \in \mathbb{R}^d$，LayerNorm计算：</p>
<p>\begin{equation}
\text{LN}(\boldsymbol{x}) = \boldsymbol{\gamma} \odot \frac{\boldsymbol{x} - \boldsymbol{\mu}}{\sqrt{\boldsymbol{\sigma}^2 + \epsilon}} + \boldsymbol{\beta}
\tag{13}
\end{equation}</p>
<p>其中：
- $\boldsymbol{\mu} = \frac{1}{d} \sum_{i=1}^{d} x_i$ (均值)
- $\boldsymbol{\sigma}^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2$ (方差)
- $\boldsymbol{\gamma}, \boldsymbol{\beta} \in \mathbb{R}^d$ 是可学习参数
- $\epsilon$ 是数值稳定性常数（通常 $10^{-5}$）</p>
<h4 id="32-layernorm">3.2 LayerNorm的梯度推导<a class="toc-link" href="#32-layernorm" title="Permanent link">&para;</a></h4>
<p>设 $\boldsymbol{y} = \text{LN}(\boldsymbol{x})$，计算 $\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}}$：</p>
<p>首先计算归一化部分 $\hat{\boldsymbol{x}} = \frac{\boldsymbol{x} - \boldsymbol{\mu}}{\sqrt{\boldsymbol{\sigma}^2 + \epsilon}}$：</p>
<p>\begin{equation}
\frac{\partial \hat{x}<em ij="ij">i}{\partial x_j} = \frac{1}{\sqrt{\boldsymbol{\sigma}^2 + \epsilon}} \left(\delta</em>
\tag{14}
\end{equation}} - \frac{1}{d}\right) - \frac{\hat{x}_i \cdot (x_j - \mu)}{d(\boldsymbol{\sigma}^2 + \epsilon)</p>
<p>其中 $\delta_{ij}$ 是Kronecker delta。</p>
<p>完整的梯度为：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial x_i} = \frac{\gamma_i}{\sqrt{\boldsymbol{\sigma}^2 + \epsilon}} \left[\frac{\partial \mathcal{L}}{\partial y_i} - \frac{1}{d} \sum_{j=1}^{d} \frac{\partial \mathcal{L}}{\partial y_j} - \hat{x}<em j="1">i \frac{1}{d} \sum</em>\right]
\tag{15}
\end{equation}}^{d} \hat{x}_j \frac{\partial \mathcal{L}}{\partial y_j</p>
<p><strong>重要性质</strong>: LayerNorm的梯度自动归一化，$|\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}}| = \mathcal{O}(1)$，有助于稳定训练。</p>
<h4 id="33-rmsnorm">3.3 RMSNorm的简化推导<a class="toc-link" href="#33-rmsnorm" title="Permanent link">&para;</a></h4>
<p>RMSNorm简化了LayerNorm，去掉了均值计算和偏置项：</p>
<p>\begin{equation}
\text{RMSNorm}(\boldsymbol{x}) = \boldsymbol{\gamma} \odot \frac{\boldsymbol{x}}{\text{RMS}(\boldsymbol{x})}
\tag{16}
\end{equation}</p>
<p>其中 RMS (Root Mean Square):</p>
<p>\begin{equation}
\text{RMS}(\boldsymbol{x}) = \sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}
\tag{17}
\end{equation}</p>
<p><strong>优势</strong>:
1. 计算更快（少一次均值计算）
2. 梯度更简单
3. 实验效果与LayerNorm相当</p>
<p>RMSNorm的梯度：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial x_i} = \frac{\gamma_i}{\text{RMS}(\boldsymbol{x})} \left[\frac{\partial \mathcal{L}}{\partial y_i} - \frac{x_i}{d \cdot \text{RMS}(\boldsymbol{x})^2} \sum_{j=1}^{d} x_j \frac{\partial \mathcal{L}}{\partial y_j}\right]
\tag{18}
\end{equation}</p>
<h3 id="post-ln-vs-pre-ln">四、Post-LN vs Pre-LN的理论分析<a class="toc-link" href="#post-ln-vs-pre-ln" title="Permanent link">&para;</a></h3>
<h4 id="41-post-ln">4.1 Post-LN的结构<a class="toc-link" href="#41-post-ln" title="Permanent link">&para;</a></h4>
<p>\begin{equation}
\boldsymbol{x}_{l+1} = \text{LN}(\boldsymbol{x}_l + F(\boldsymbol{x}_l))
\tag{19}
\end{equation}</p>
<p>其中 $F$ 是Self-Attention或FFN。</p>
<p><strong>梯度流</strong>:</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">l} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>\right)
\tag{20}
\end{equation}}} \frac{\partial \text{LN}(\boldsymbol{x}_l + F(\boldsymbol{x}_l))}{\partial (\boldsymbol{x}_l + F(\boldsymbol{x}_l))} \left(\boldsymbol{I} + \frac{\partial F(\boldsymbol{x}_l)}{\partial \boldsymbol{x}_l</p>
<p><strong>问题</strong>: LayerNorm的雅可比矩阵会重新缩放梯度，可能破坏残差连接的恒等映射。</p>
<h4 id="42-pre-ln">4.2 Pre-LN的结构<a class="toc-link" href="#42-pre-ln" title="Permanent link">&para;</a></h4>
<p>\begin{equation}
\boldsymbol{x}_{l+1} = \boldsymbol{x}_l + F(\text{LN}(\boldsymbol{x}_l))
\tag{21}
\end{equation}</p>
<p><strong>梯度流</strong>:</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">l} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>\right)
\tag{22}
\end{equation}}} \left(\boldsymbol{I} + \frac{\partial F(\text{LN}(\boldsymbol{x}_l))}{\partial \text{LN}(\boldsymbol{x}_l)} \frac{\partial \text{LN}(\boldsymbol{x}_l)}{\partial \boldsymbol{x}_l</p>
<p><strong>优势</strong>: 恒等路径 $\boldsymbol{I}$ 不受LayerNorm影响，梯度传播更稳定。</p>
<h4 id="43-deepnorm">4.3 DeepNorm的改进<a class="toc-link" href="#43-deepnorm" title="Permanent link">&para;</a></h4>
<p>DeepNorm引入缩放因子 $\alpha$:</p>
<p>\begin{equation}
\boldsymbol{x}_{l+1} = \text{LN}(\alpha \boldsymbol{x}_l + F(\boldsymbol{x}_l))
\tag{23}
\end{equation}</p>
<p>等价形式：</p>
<p>\begin{equation}
\boldsymbol{x}_{l+1} = \text{LN}\left(\boldsymbol{x}_l + \frac{F(\boldsymbol{x}_l)}{\alpha}\right)
\tag{24}
\end{equation}</p>
<p><strong>设计原理</strong>: 通过调整 $\alpha$ 控制残差分支的贡献，使初始阶段模型接近恒等函数。</p>
<h3 id="_12">五、量级分解与初始化策略<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 矩阵的量级分解<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>将参数矩阵 $\boldsymbol{W}$ 分解为:</p>
<p>\begin{equation}
\boldsymbol{W} = \lambda \boldsymbol{U}
\tag{25}
\end{equation}</p>
<p>其中：
- $\lambda &gt; 0$ 是标量增益
- $\boldsymbol{U}$ 接近正交矩阵 ($\boldsymbol{U}\boldsymbol{U}^\top \approx \boldsymbol{I}$)</p>
<p><strong>最优化问题</strong>:</p>
<p>\begin{equation}
\lambda = \arg\min_{\kappa &gt; 0} |\boldsymbol{W}\boldsymbol{W}^\top / \kappa^2 - \boldsymbol{I}|_F
\tag{26}
\end{equation}</p>
<p>解为：</p>
<p>\begin{equation}
\lambda = \sqrt{\frac{\text{tr}(\boldsymbol{W}\boldsymbol{W}^\top)}{d}} = |\boldsymbol{W}|_F / \sqrt{d}
\tag{27}
\end{equation}</p>
<h4 id="52">5.2 梯度的量级探针<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>对于损失函数 $\mathcal{L}(\lambda \boldsymbol{U})$，标量梯度为：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \lambda} = \left\langle \frac{\partial \mathcal{L}}{\partial (\lambda \boldsymbol{U})}, \boldsymbol{U} \right\rangle = \left\langle \frac{\partial \mathcal{L}}{\partial \boldsymbol{W}}, \boldsymbol{U} \right\rangle
\tag{28}
\end{equation}</p>
<p><strong>关键性质</strong>: $\frac{\partial \mathcal{L}}{\partial \lambda}$ 与 $|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}}|$ 在量级上成正比。</p>
<h4 id="53-xavier">5.3 Xavier初始化的数学原理<a class="toc-link" href="#53-xavier" title="Permanent link">&para;</a></h4>
<p>Xavier初始化要求前向传播时方差保持不变。对于线性层 $\boldsymbol{y} = \boldsymbol{W}\boldsymbol{x}$:</p>
<p>\begin{equation}
\text{Var}(y_i) = \sum_{j=1}^{d_{\text{in}}} \text{Var}(W_{ij} x_j) = d_{\text{in}} \cdot \text{Var}(W_{ij}) \cdot \text{Var}(x_j)
\tag{29}
\end{equation}</p>
<p>要使 $\text{Var}(y_i) = \text{Var}(x_j)$，需要：</p>
<p>\begin{equation}
\text{Var}(W_{ij}) = \frac{1}{d_{\text{in}}}
\tag{30}
\end{equation}</p>
<p>考虑反向传播的对称性，最终：</p>
<p>\begin{equation}
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{d_{\text{in}} + d_{\text{out}}}\right)
\tag{31}
\end{equation}</p>
<p><strong>矩阵视角</strong>: Xavier初始化使 $\boldsymbol{W}$ 接近正交矩阵，即 $\lambda \approx 1$。</p>
<h3 id="ffn">六、FFN层的梯度量级分析<a class="toc-link" href="#ffn" title="Permanent link">&para;</a></h3>
<h4 id="61-ffn">6.1 FFN的数学形式<a class="toc-link" href="#61-ffn" title="Permanent link">&para;</a></h4>
<p>\begin{equation}
\boldsymbol{x}_{l+1} = \text{LN}(\boldsymbol{x}_l + \phi(\boldsymbol{x}_l \boldsymbol{W}_1) \boldsymbol{W}_2 / \alpha)
\tag{32}
\end{equation}</p>
<p>其中 $\phi$ 是激活函数（ReLU, GELU等）。</p>
<p><strong>齐次性</strong>: 对于ReLU及其变体，满足 $\phi(\lambda \boldsymbol{x}) \approx \lambda \phi(\boldsymbol{x})$ (当 $\lambda &gt; 0$)。</p>
<h4 id="62">6.2 量级分解后的形式<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p>使用 $\boldsymbol{W}_i = \lambda_i \boldsymbol{U}_i$:</p>
<p>\begin{equation}
\boldsymbol{z}_{l+1} = \boldsymbol{x}_l + \lambda_1 \lambda_2 \phi(\boldsymbol{x}_l \boldsymbol{U}_1) \boldsymbol{U}_2 / \alpha
\tag{33}
\end{equation}</p>
<h4 id="63">6.3 梯度计算<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p>对 $\lambda_1$ 的梯度：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \lambda_1} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">{l+1}} \frac{\partial \boldsymbol{x}</em>
\tag{34}
\end{equation}}}{\partial \boldsymbol{z}_{l+1}} \frac{\lambda_2 \phi(\boldsymbol{x}_l \boldsymbol{U}_1) \boldsymbol{U}_2}{\alpha</p>
<p><strong>量级估计</strong>:
- $\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">{l+1}} = \mathcal{O}(1)$ (归一化作用)
- $\frac{\partial \boldsymbol{x}</em>(1)$ (LayerNorm雅可比)
- $\phi(\boldsymbol{x}_l \boldsymbol{U}_1) \boldsymbol{U}_2 = \mathcal{O}(1)$ (正交性)}}{\partial \boldsymbol{z}_{l+1}} = \mathcal{O</p>
<p>因此：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \lambda_1} = \mathcal{O}\left(\frac{\lambda_2}{\alpha}\right)
\tag{35}
\end{equation}</p>
<p>同理：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \lambda_2} = \mathcal{O}\left(\frac{\lambda_1}{\alpha}\right)
\tag{36}
\end{equation}</p>
<h3 id="self-attention">七、Self-Attention的梯度量级分析<a class="toc-link" href="#self-attention" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 单头注意力的量级分解<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p>\begin{equation}
\boldsymbol{x}_{l+1} = \text{LN}(\boldsymbol{x}_l + \sigma(\boldsymbol{x}_l \boldsymbol{W}_q \boldsymbol{W}_k^\top \boldsymbol{x}_l^\top) \boldsymbol{x}_l \boldsymbol{W}_v \boldsymbol{W}_o / \alpha)
\tag{37}
\end{equation}</p>
<p>其中 $\sigma$ 表示softmax操作。</p>
<p>分解后：</p>
<p>\begin{equation}
\boldsymbol{z}_{l+1} = \boldsymbol{x}_l + \lambda_v \lambda_o \sigma(\lambda_q \lambda_k \boldsymbol{x}_l \boldsymbol{U}_q \boldsymbol{U}_k^\top \boldsymbol{x}_l^\top) \boldsymbol{x}_l \boldsymbol{U}_v \boldsymbol{U}_o / \alpha
\tag{38}
\end{equation}</p>
<h4 id="72-softmax">7.2 Softmax的量级不变性<a class="toc-link" href="#72-softmax" title="Permanent link">&para;</a></h4>
<p><strong>关键性质</strong>: Softmax对输入的缩放不敏感（到某种程度）。</p>
<p>\begin{equation}
\sigma(c\boldsymbol{x}) = \sigma(\boldsymbol{x}), \quad \text{当缩放被温度吸收}
\tag{39}
\end{equation}</p>
<p>在Attention中，$\lambda_q \lambda_k$ 的缩放效果类似于调整温度，但softmax后的加权平均仍保持 $\mathcal{O}(1)$ 量级。</p>
<p><strong>数学直觉</strong>:</p>
<p>\begin{equation}
\sum_{i} \text{softmax}(\boldsymbol{a})_i \boldsymbol{v}_i \approx \mathcal{O}(|\boldsymbol{v}|)
\tag{40}
\end{equation}</p>
<p>由于 $\boldsymbol{v}_i = \boldsymbol{x}_i \boldsymbol{U}_v$ 且 $\boldsymbol{U}_v$ 正交，有 $|\boldsymbol{v}| = \mathcal{O}(|\boldsymbol{x}|) = \mathcal{O}(1)$ (经过LayerNorm)。</p>
<h4 id="73-vo">7.3 V和O的梯度<a class="toc-link" href="#73-vo" title="Permanent link">&para;</a></h4>
<p>对 $\lambda_v$ 的梯度：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \lambda_v} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">{l+1}} \frac{\partial \boldsymbol{x}</em>\right)
\tag{41}
\end{equation}}}{\partial \boldsymbol{z}_{l+1}} \frac{\lambda_o \sigma(\cdots) \boldsymbol{x}_l \boldsymbol{U}_v \boldsymbol{U}_o}{\alpha} = \mathcal{O}\left(\frac{\lambda_o}{\alpha</p>
<p>对 $\lambda_o$ 的梯度：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \lambda_o} = \mathcal{O}\left(\frac{\lambda_v}{\alpha}\right)
\tag{42}
\end{equation}</p>
<p><strong>统一结论</strong>: 无论FFN还是Self-Attention：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \lambda} = \mathcal{O}\left(\frac{\lambda}{\alpha}\right)
\tag{43}
\end{equation}</p>
<h3 id="_13">八、深度归一化方案的推导<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 抑制增量爆炸的目标<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p>回顾增量爆炸：每个参数的梯度为 $\mathcal{O}(\lambda/\alpha)$，$N$ 层网络共 $2NK$ 个参数（每层K个Self-Attention参数，K个FFN参数）。</p>
<p>总梯度范数平方：</p>
<p>\begin{equation}
|\nabla_{\boldsymbol{\theta}} \mathcal{L}|^2 = 2NK \cdot \mathcal{O}\left(\frac{\lambda^2}{\alpha^2}\right)
\tag{44}
\end{equation}</p>
<p>SGD的更新量：</p>
<p>\begin{equation}
\Delta \mathcal{L} = -\eta |\nabla_{\boldsymbol{\theta}} \mathcal{L}|^2 = \mathcal{O}\left(\eta \cdot 2NK \cdot \frac{\lambda^2}{\alpha^2}\right)
\tag{45}
\end{equation}</p>
<p><strong>目标</strong>: 使更新量与 $N$ 无关，即：</p>
<p>\begin{equation}
\frac{\lambda^2}{\alpha^2} = \mathcal{O}\left(\frac{1}{N}\right)
\tag{46}
\end{equation}</p>
<h4 id="82">8.2 三种优化器的分析<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p><strong>SGD</strong>: $\Delta \boldsymbol{\theta} = -\eta \nabla_{\boldsymbol{\theta}} \mathcal{L}$</p>
<p>\begin{equation}
\Delta \mathcal{L} \propto -\eta |\nabla_{\boldsymbol{\theta}} \mathcal{L}|^2 = \mathcal{O}\left(\eta NK \frac{\lambda^2}{\alpha^2}\right)
\tag{47}
\end{equation}</p>
<p>要求 $\frac{\lambda^2}{\alpha^2} = \frac{1}{2NK}$，即：</p>
<p>\begin{equation}
\frac{\lambda}{\alpha} = \frac{1}{\sqrt{2NK}} = \frac{1}{\sqrt{2N}} \quad (\text{假设} K \approx 1)
\tag{48}
\end{equation}</p>
<p><strong>Adam</strong>: $\Delta \boldsymbol{\theta} \approx -\eta \cdot \text{sign}(\nabla_{\boldsymbol{\theta}} \mathcal{L})$</p>
<p>\begin{equation}
\Delta \mathcal{L} \propto -\eta |\nabla_{\boldsymbol{\theta}} \mathcal{L}|_1 = \mathcal{O}\left(\eta \sqrt{NK} \cdot \frac{\lambda}{\alpha}\right)
\tag{49}
\end{equation}</p>
<p>要求 $\frac{\lambda}{\alpha} = \frac{1}{\sqrt{NK}} = \frac{1}{\sqrt{N}}$。</p>
<p><strong>LAMB</strong>: $\Delta \boldsymbol{\theta} \approx -\eta |\boldsymbol{\theta}| \cdot \text{sign}(\nabla_{\boldsymbol{\theta}} \mathcal{L})$</p>
<p>\begin{equation}
\Delta \mathcal{L} \propto -\eta \lambda \sqrt{NK} \cdot \frac{\lambda}{\alpha} = \mathcal{O}\left(\eta \sqrt{NK} \cdot \frac{\lambda^2}{\alpha}\right)
\tag{50}
\end{equation}</p>
<p>要求 $\frac{\lambda^2}{\alpha} = \frac{1}{\sqrt{NK}}$，即 $\lambda = 1, \alpha = \sqrt{N}$ (选择一种简单解)。</p>
<h4 id="83-alpha-lambda">8.3 $\alpha$ 和 $\lambda$ 的最优选择<a class="toc-link" href="#83-alpha-lambda" title="Permanent link">&para;</a></h4>
<p><strong>对称性原则</strong>: 让 $\lambda = 1/\alpha$，使得残差分支和主路径在初始阶段平衡。</p>
<p>对于SGD:</p>
<p>\begin{equation}
\frac{\lambda}{\alpha} = \frac{1}{\sqrt{2N}} \quad \text{且} \quad \lambda = \frac{1}{\alpha}
\tag{51}
\end{equation}</p>
<p>解得：</p>
<p>\begin{equation}
\alpha = (2N)^{1/4}, \quad \lambda = (2N)^{-1/4}
\tag{52}
\end{equation}</p>
<p>对于Adam:</p>
<p>\begin{equation}
\alpha = N^{1/2}, \quad \lambda = N^{-1/2}
\tag{53}
\end{equation}</p>
<p>对于LAMB:</p>
<p>\begin{equation}
\alpha = 1, \quad \lambda = N^{-1/2}
\tag{54}
\end{equation}</p>
<h3 id="_14">九、初始化对训练的影响<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 残差分支的初始权重<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p>DeepNorm设置 $\lambda = (2N)^{-1/4}$ 意味着每个残差分支的初始贡献为：</p>
<p>\begin{equation}
\frac{\lambda^2}{\alpha} = \frac{(2N)^{-1/2}}{(2N)^{1/4}} = (2N)^{-3/4}
\tag{55}
\end{equation}</p>
<p><strong>数值示例</strong>: $N=1000$ 时：</p>
<p>\begin{equation}
\frac{\lambda^2}{\alpha} = (2000)^{-3/4} \approx 0.0042
\tag{56}
\end{equation}</p>
<p>初始阶段每个残差分支仅贡献 $0.4\%$ 的变化，模型接近恒等函数！</p>
<h4 id="92">9.2 恒等初始化的重要性<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p><strong>定理</strong>: 如果初始阶段 $\boldsymbol{x}_{l+1} \approx \boldsymbol{x}_l$（恒等函数），则：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_{l+1}} \approx \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_l}
\tag{57}
\end{equation}</p>
<p>梯度几乎无衰减地传播到底层。</p>
<p><strong>证明</strong>: 由链式法则：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">l} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>
\tag{58}
\end{equation}}} \frac{\partial \boldsymbol{x}_{l+1}}{\partial \boldsymbol{x}_l</p>
<p>当 $\boldsymbol{x}_{l+1} = \boldsymbol{x}_l + \epsilon F(\boldsymbol{x}_l)$ 且 $\epsilon \to 0$:</p>
<p>\begin{equation}
\frac{\partial \boldsymbol{x}_{l+1}}{\partial \boldsymbol{x}_l} \approx \boldsymbol{I}
\tag{59}
\end{equation}</p>
<p>因此 $\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">l} \approx \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>$。□}</p>
<h3 id="warmup">十、Warmup的数学必要性<a class="toc-link" href="#warmup" title="Permanent link">&para;</a></h3>
<h4 id="101-warmup">10.1 学习率warmup的目的<a class="toc-link" href="#101-warmup" title="Permanent link">&para;</a></h4>
<p>在训练初期，即使有DeepNorm，参数的量级仍需要时间来稳定。Warmup通过逐渐增大学习率来适应这个过程。</p>
<p><strong>线性warmup</strong>:</p>
<p>\begin{equation}
\eta(t) = \begin{cases}
\frac{t}{T_{\text{warmup}}} \eta_{\text{max}}, &amp; t \leq T_{\text{warmup}} \
\eta_{\text{max}}, &amp; t &gt; T_{\text{warmup}}
\end{cases}
\tag{60}
\end{equation}</p>
<h4 id="102-warmup">10.2 Warmup与增量爆炸的关系<a class="toc-link" href="#102-warmup" title="Permanent link">&para;</a></h4>
<p>即使梯度被归一化到 $\mathcal{O}(1/\sqrt{N})$，初始的大步长仍可能导致参数偏离最优初始化。</p>
<p><strong>自适应策略</strong>: Warmup相当于动态调整 $\eta$ 使得：</p>
<p>\begin{equation}
\eta(t) \cdot |\nabla_{\boldsymbol{\theta}} \mathcal{L}| \approx \text{constant}
\tag{61}
\end{equation}</p>
<p>在初期梯度较大时用较小的 $\eta$，避免过度更新。</p>
<h4 id="103">10.3 数值示例<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p>假设 $N=100$，使用Adam：
- 无warmup: 直接用 $\eta = 10^{-4}$ 可能导致初期震荡
- 有warmup: 前1000步从 $\eta = 10^{-7}$ 线性增加到 $10^{-4}$</p>
<p>初期更新量：</p>
<p>\begin{equation}
\Delta \boldsymbol{\theta}_{\text{warmup}} = \frac{t}{1000} \cdot 10^{-4} \cdot \text{sign}(\nabla) \approx 10^{-7} \text{sign}(\nabla) \quad (t \ll 1000)
\tag{62}
\end{equation}</p>
<p>相比无warmup减少了1000倍，让参数有时间"适应"。</p>
<h3 id="pre-ln-vs-post-ln">十一、Pre-LN vs Post-LN的收敛性分析<a class="toc-link" href="#pre-ln-vs-post-ln" title="Permanent link">&para;</a></h3>
<h4 id="111-pre-ln">11.1 Pre-LN的梯度传播<a class="toc-link" href="#111-pre-ln" title="Permanent link">&para;</a></h4>
<p>Pre-LN: $\boldsymbol{x}_{l+1} = \boldsymbol{x}_l + F(\text{LN}(\boldsymbol{x}_l))$</p>
<p>梯度：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">l} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>(\lambda)\right]
\tag{63}
\end{equation}}} \left[\boldsymbol{I} + \mathcal{O</p>
<p>跨 $N$ 层：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l="1">1} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_N} \prod</em>(\lambda)]
\tag{64}
\end{equation}}^{N-1} [\boldsymbol{I} + \mathcal{O</p>
<p><strong>范数估计</strong>:</p>
<p>\begin{equation}
\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_1}\right| \approx \left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_N}\right| (1 + \mathcal{O}(\lambda))^N
\tag{65}
\end{equation}</p>
<p>当 $\lambda$ 很小时，$(1 + \mathcal{O}(\lambda))^N \approx 1$，梯度传播稳定。</p>
<h4 id="112-post-ln">11.2 Post-LN的梯度传播<a class="toc-link" href="#112-post-ln" title="Permanent link">&para;</a></h4>
<p>Post-LN: $\boldsymbol{x}_{l+1} = \text{LN}(\boldsymbol{x}_l + F(\boldsymbol{x}_l))$</p>
<p>梯度包含LayerNorm的雅可比：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em l_1="l+1">l} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>(\lambda)]
\tag{66}
\end{equation}}} \cdot \boldsymbol{J}_{\text{LN}} \cdot [\boldsymbol{I} + \mathcal{O</p>
<p><strong>问题</strong>: $\boldsymbol{J}_{\text{LN}}$ 会重新归一化，可能破坏梯度的恒等传播。</p>
<h4 id="113-deepnorm">11.3 DeepNorm的优化<a class="toc-link" href="#113-deepnorm" title="Permanent link">&para;</a></h4>
<p>DeepNorm在Post-LN基础上添加 $\alpha$ 缩放：</p>
<p>\begin{equation}
\boldsymbol{x}_{l+1} = \text{LN}(\alpha \boldsymbol{x}_l + F(\boldsymbol{x}_l))
\tag{67}
\end{equation}</p>
<p>当 $\alpha &gt; 1$ 时，主路径被放大，减轻LayerNorm的破坏作用。</p>
<p><strong>最优 $\alpha$</strong>: 使得主路径在LayerNorm后仍占主导：</p>
<p>\begin{equation}
\alpha = (2N)^{1/4}
\tag{68}
\end{equation}</p>
<h3 id="_15">十二、实践建议与数值验证<a class="toc-link" href="#_15" title="Permanent link">&para;</a></h3>
<h4 id="121">12.1 不同深度的配置建议<a class="toc-link" href="#121" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>层数 $N$</th>
<th>优化器</th>
<th>$\alpha$</th>
<th>$\lambda$</th>
<th>Warmup步数</th>
</tr>
</thead>
<tbody>
<tr>
<td>12</td>
<td>Adam</td>
<td>2.5</td>
<td>0.4</td>
<td>1000</td>
</tr>
<tr>
<td>24</td>
<td>Adam</td>
<td>3.5</td>
<td>0.29</td>
<td>2000</td>
</tr>
<tr>
<td>100</td>
<td>Adam</td>
<td>7.1</td>
<td>0.14</td>
<td>4000</td>
</tr>
<tr>
<td>200</td>
<td>Adam</td>
<td>10</td>
<td>0.1</td>
<td>8000</td>
</tr>
<tr>
<td>1000</td>
<td>Adam</td>
<td>22.4</td>
<td>0.045</td>
<td>16000</td>
</tr>
</tbody>
</table>
<h4 id="122">12.2 训练稳定性指标<a class="toc-link" href="#122" title="Permanent link">&para;</a></h4>
<p><strong>梯度范数监控</strong>:</p>
<p>\begin{equation}
G(l) = \left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_l}\right|, \quad l = 1, 2, \ldots, N
\tag{69}
\end{equation}</p>
<p>健康训练应满足：</p>
<p>\begin{equation}
0.1 \leq \frac{G(l)}{G(N)} \leq 10, \quad \forall l
\tag{70}
\end{equation}</p>
<p><strong>参数更新比例</strong>:</p>
<p>\begin{equation}
R = \frac{|\Delta \boldsymbol{\theta}|}{|\boldsymbol{\theta}|} \approx 10^{-3} \sim 10^{-2}
\tag{71}
\end{equation}</p>
<h4 id="123-1000transformer">12.3 数值示例：1000层Transformer<a class="toc-link" href="#123-1000transformer" title="Permanent link">&para;</a></h4>
<p>配置：
- $d = 1024$, $h = 8$
- Adam优化器
- $\alpha = 22.4$, $\lambda = 0.045$
- 学习率 $10^{-4}$ (峰值)
- Warmup 16000步</p>
<p>初始梯度范数（第1层 vs 第1000层）：</p>
<p>\begin{equation}
\frac{G(1)}{G(1000)} = (1.045)^{1000} \approx 2.1
\tag{72}
\end{equation}</p>
<p>在可接受范围内！</p>
<h3 id="_16">十三、总结<a class="toc-link" href="#_16" title="Permanent link">&para;</a></h3>
<p>本节详细推导了深层Transformer训练的数学原理：</p>
<p><strong>核心问题</strong>:
1. 梯度消失/爆炸: $|\nabla| \propto \gamma^N$ 或 $\beta^N$
2. 增量爆炸: $\Delta \mathcal{L} \propto \eta N$</p>
<p><strong>解决方案</strong>:
1. 残差连接: 确保 $\boldsymbol{J}_l \succeq \boldsymbol{I}$
2. LayerNorm: 归一化梯度到 $\mathcal{O}(1)$
3. DeepNorm: 通过 $\alpha, \lambda$ 缩放抑制增量爆炸
4. Warmup: 初期小学习率适应参数</p>
<p><strong>关键公式</strong>:
\begin{equation}
\begin{cases}
\alpha = (2N)^{1/4}, \lambda = (2N)^{-1/4} &amp; \text{(SGD)} \
\alpha = N^{1/2}, \lambda = N^{-1/2} &amp; \text{(Adam)} \
\alpha = 1, \lambda = N^{-1/2} &amp; \text{(LAMB)}
\end{cases}
\tag{73}
\end{equation}</p>
<p>这些理论保证了1000层甚至更深的Transformer能够稳定训练，打破了"浅而宽"的局限。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="指数梯度下降-元学习-自适应学习率.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#170 指数梯度下降 + 元学习 = 自适应学习率</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="门控注意力单元gau还需要warmup吗.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#172 门控注意力单元（GAU）还需要Warmup吗？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#1000transformer">训练1000层的Transformer究竟有什么困难？</a><ul>
<li><a href="#_1">增量爆炸</a></li>
<li><a href="#_2">量级分析</a></li>
<li><a href="#_3">前馈梯度</a></li>
<li><a href="#_4">自注意力</a></li>
<li><a href="#_5">初步结论</a></li>
<li><a href="#_6">多种优化</a></li>
<li><a href="#_7">事后分析</a></li>
<li><a href="#_8">文章小结</a></li>
<li><a href="#_9">公式推导与注释</a><ul>
<li><a href="#_10">一、深层网络的梯度问题数学分析</a></li>
<li><a href="#_11">二、增量爆炸问题的深入分析</a></li>
<li><a href="#layernorm">三、LayerNorm的数学推导</a></li>
<li><a href="#post-ln-vs-pre-ln">四、Post-LN vs Pre-LN的理论分析</a></li>
<li><a href="#_12">五、量级分解与初始化策略</a></li>
<li><a href="#ffn">六、FFN层的梯度量级分析</a></li>
<li><a href="#self-attention">七、Self-Attention的梯度量级分析</a></li>
<li><a href="#_13">八、深度归一化方案的推导</a></li>
<li><a href="#_14">九、初始化对训练的影响</a></li>
<li><a href="#warmup">十、Warmup的数学必要性</a></li>
<li><a href="#pre-ln-vs-post-ln">十一、Pre-LN vs Post-LN的收敛性分析</a></li>
<li><a href="#_15">十二、实践建议与数值验证</a></li>
<li><a href="#_16">十三、总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>