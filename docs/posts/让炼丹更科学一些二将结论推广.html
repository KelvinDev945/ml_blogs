<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>让炼丹更科学一些（二）：将结论推广到无界域 | ML & Math Blog Posts</title>
    <meta name="description" content="让炼丹更科学一些（二）：将结论推广到无界域&para;
原文链接: https://spaces.ac.cn/archives/11469
发布日期: 

两年前，笔者打算开一个“科学炼丹”专题，本想着系统整理一下优化器的经典理论结果，但写了第一篇《让炼丹更科学一些（一）：SGD的平均损失收敛》后，就一直搁置至今。主要原因在于，笔者总觉得这些经典优化结论所依赖的条件过于苛刻，跟实际应用相去甚远，尤...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=不等式">不等式</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #6 让炼丹更科学一些（二）：将结论推广到无界域
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#6</span>
                让炼丹更科学一些（二）：将结论推广到无界域
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/11469" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=不等式" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 不等式</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=sgd" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> sgd</span>
                </a>
                
                <a href="../index.html?tags=炼丹" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 炼丹</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">让炼丹更科学一些（二）：将结论推广到无界域<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11469">https://spaces.ac.cn/archives/11469</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>两年前，笔者打算开一个“科学炼丹”专题，本想着系统整理一下优化器的经典理论结果，但写了第一篇<a href="/archives/9902">《让炼丹更科学一些（一）：SGD的平均损失收敛》</a>后，就一直搁置至今。主要原因在于，笔者总觉得这些经典优化结论所依赖的条件过于苛刻，跟实际应用相去甚远，尤其是进入LLM时代后，这些结论的参考价值似乎更加有限，所以就没什么动力继续写下去。</p>
<p>然而，近期在思考Scaling Law的相关问题时，笔者发现这些结论结果并非想象中那么“没用”，它可以为一些经验结果提供有益的理论洞见。因此，本文将重启该系列，继续推进这个专题文章的撰写，“偿还”之前欠下的“债务”。</p>
<h2 id="_2">结论回顾<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>记号方面我们沿用第一篇文章的，所以不再重复记号的介绍。第一篇文章的主要结论是：在适当的假设之下，SGD成立<br />
\begin{equation}\frac{1}{T}\sum_{t=1}^T L(\boldsymbol{x}<em t="1">t,\boldsymbol{\theta}_t) - \frac{1}{T}\sum</em>}^T L(\boldsymbol{x<em t="1">t,\boldsymbol{\theta}^*)\leq \frac{R^2}{2T\eta_T} + \frac{G^2}{2T}\sum</em>}^T\eta_t\label{leq:avg-1}\end{equation
其中$R, G$是优化轨迹无关的常数，而“适当的假设”包括：</p>
<blockquote>
<p>1、$\boldsymbol{\Theta}$是一个有界凸集，$R=\max\limits_{\boldsymbol{\theta}_1,\boldsymbol{\theta}_2\in \boldsymbol{\Theta}}\Vert\boldsymbol{\theta}_1-\boldsymbol{\theta}_2\Vert &lt; \infty$；</p>
<p>2、对于任意$\boldsymbol{\theta}\in \boldsymbol{\Theta}$以及任意$\boldsymbol{x}$，$L(\boldsymbol{x},\boldsymbol{\theta})$都是关于$\boldsymbol{\theta}$的凸函数；</p>
<p>3、对于任意$\boldsymbol{\theta}\in \boldsymbol{\Theta}$以及任意$\boldsymbol{x}$，都有$\Vert\nabla_{\boldsymbol{\theta}}L(\boldsymbol{x},\boldsymbol{\theta})\Vert\leq G &lt; \infty$；</p>
<p>4、学习率$\eta_t$是关于$t$的单调递减函数（即$\eta_t\geq \eta_{t+1}$）；</p>
</blockquote>
<p>这里比较“别扭”的地方可能是第一点“有界凸集”中的“有界”，因为SGD的优化轨迹原则上是没法保证有界的，所以为了保证有界性我们要将SGD修改为投影SGD：<br />
\begin{equation}\boldsymbol{\theta}<em _boldsymbol_Theta="\boldsymbol{\Theta">{t+1} = \Pi</em>}}\big(\boldsymbol{\theta}_t - \eta_t \boldsymbol{g}(\boldsymbol{x}_t,\boldsymbol{\theta}_t)\big)\in \boldsymbol{\Theta}\label{eq:sgd-p}\end{equation
当然，有界性在实践中完全没有问题，适当的有界性甚至还能增强优化算法的稳定性。但是从“理论洁癖”的角度看，多加一个约束总是不舒服的。所以，本文我们先把有界性这个条件去掉，以得到更简捷、更具启发性的证明</p>
<h2 id="_3">新的思路<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>顺便说，这一节以及本文接下来章节的证明，主要参考了博客<a href="https://parameterfree.com/">Parameter-free</a>的文章<a href="https://parameterfree.com/2020/08/07/last-iterate-of-sgd-converges-even-in-unbounded-domains/">《Last Iterate of SGD Converges (Even in Unbounded Domains)》</a>，这是一个关于优化理论的经典博客，除了这篇外上面还有很多优化器方面的理论介绍，有不少还是作者原创的，值得一读。</p>
<p>跟上一篇文章的证明一样，我们出发点都是恒等式
\begin{equation}\begin{aligned}
\Vert\boldsymbol{\theta}<em t="1">{t+1} - \boldsymbol{\varphi}\Vert^2=&amp;\, \Vert\boldsymbol{\theta}_t - \eta_t \boldsymbol{g}(\boldsymbol{x}_t,\boldsymbol{\theta}_t)- \boldsymbol{\varphi}\Vert^2 \\
=&amp;\, \Vert\boldsymbol{\theta}_t - \boldsymbol{\varphi}\Vert^2 - 2\eta_t (\boldsymbol{\theta}_t- \boldsymbol{\varphi})\cdot\boldsymbol{g}(\boldsymbol{x}_t,\boldsymbol{\theta}_t) + \eta_t^2\Vert\boldsymbol{g}(\boldsymbol{x}_t,\boldsymbol{\theta}_t)\Vert^2
\end{aligned}\end{equation}<br />
这里$\boldsymbol{\varphi}$是任意向量，上一篇文章直接取了$\boldsymbol{\varphi}=\boldsymbol{\theta}^*$，但这里我们保留了它取任意值的可能性。这一恒等式之后，上文采用了两边除以$2\eta_t$然后对$t$求和的方案，导致在后面的放缩中需要有界性条件才容易操作；本文则不打算除以$2\eta_t$，而是直接两端对$t$求和，这样中间的$\Vert\boldsymbol{\theta}_t - \boldsymbol{\varphi}\Vert^2$就自然地消去了：<br />
\begin{equation}\begin{aligned}
2\sum</em>}^T \eta_t (\boldsymbol{\theta<em T_1="T+1">t- \boldsymbol{\varphi})\cdot\boldsymbol{g}(\boldsymbol{x}_t,\boldsymbol{\theta}_t) =&amp;\, \Vert\boldsymbol{\theta}_1 - \boldsymbol{\varphi}\Vert^2 - \Vert\boldsymbol{\theta}</em>} - \boldsymbol{\varphi}\Vert^2 + \sum_{t=1}^T \eta_t^2 \Vert\boldsymbol{g}(\boldsymbol{x<em t="1">t,\boldsymbol{\theta}_t)\Vert^2 \\
\leq&amp;\, \Vert\boldsymbol{\theta}_1 - \boldsymbol{\varphi}\Vert^2 + G^2\sum</em>^T \eta_t^2
\end{aligned}\end{equation}<br />
利用$L$的凸性可知$(\boldsymbol{\theta}<em t="1">t- \boldsymbol{\varphi})\cdot\boldsymbol{g}(\boldsymbol{x}_t,\boldsymbol{\theta}_t) \geq L(\boldsymbol{x}_t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\varphi})$，代入上式左端整理得<br />
\begin{equation}\sum</em>}^T \eta_t [L(\boldsymbol{x<em t="1">t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\varphi})]\leq \frac{\Vert\boldsymbol{\theta}_1 - \boldsymbol{\varphi}\Vert^2}{2} + \frac{G^2}{2}\sum</em>}^T \eta_t^2\label{leq:avg-2-mid1}\end{equation
这已经非常接近我们想要的结果了，并且在放缩过程中没有用到每一点的有界性，所以只需要对初始化$\boldsymbol{\theta}_1$与终点$\boldsymbol{\varphi}$的距离作出假设。</p>
<h2 id="_4">加个期望<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>接下来，如果我们能进一步假设$L(\boldsymbol{x}<em t="1">t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\varphi})$的非负性，那么就可以结合$\eta_t$的单调递减性得到$\eta_t [L(\boldsymbol{x}_t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\varphi})] \geq \eta_T [L(\boldsymbol{x}_t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\varphi})]$，继而代入上式得<br />
\begin{equation}\frac{1}{T}\sum</em>}^T L(\boldsymbol{x<em t="1">t,\boldsymbol{\theta}_t) - \frac{1}{T}\sum</em>}^T L(\boldsymbol{x<em t="1">t,\boldsymbol{\varphi})\leq \frac{\Vert\boldsymbol{\theta}_1 - \boldsymbol{\varphi}\Vert^2}{2 T \eta_T} + \frac{G^2}{2T}\sum</em>}^T \frac{\eta_t^2}{\eta_T}\end{equation
问题是，即便我们设$\boldsymbol{\varphi}$是全局最优点$\boldsymbol{\theta}^*$，也没法保证$L(\boldsymbol{x}<em t="1">t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\varphi})$非负，因为最优点对全体样本来说是最优点，但对部分样本来说不一定是最优点。为了避开这个麻烦，我们留意到式$\eqref{leq:avg-2-mid1}$右端是$\boldsymbol{x}_t$无关的，而左端是$\boldsymbol{x}_t$相关的，所以对两边求期望，不等式依然成立：<br />
\begin{equation}\sum</em>}^T \eta_t \mathbb{E}[L(\boldsymbol{x<em t="1">t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\varphi})]\leq \frac{\Vert\boldsymbol{\theta}_1 - \boldsymbol{\varphi}\Vert^2}{2} + \frac{G^2}{2}\sum</em>}^T \eta_t^2\label{leq:avg-2-mid2}\end{equation
这里$\mathbb{E}$是$\mathbb{E}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_T}$，即“以不同的采样顺序、无限次重复SGD，对每一条优化轨迹求平均值”。接着，由于$\boldsymbol{\varphi}$是数据无关的任意向量，所以有$\mathbb{E}[L(\boldsymbol{x}_t,\boldsymbol{\varphi})] = \mathbb{E}</em><em t-1="t-1">t}[L(\boldsymbol{x}_t,\boldsymbol{\varphi})] = L(\boldsymbol{\varphi})$，正好是我们的优化目标；至于$L(\boldsymbol{x}_t,\boldsymbol{\theta}_t)$，注意$\boldsymbol{\theta}_t$是依赖$\boldsymbol{x}_1,\cdots,\boldsymbol{x}</em>$的，所以我们顶多有<br />
\begin{equation}\mathbb{E}[L(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t,\boldsymbol{\theta}_t)] = \mathbb{E}</em><em _boldsymbol_x="\boldsymbol{x">1,\cdots,\boldsymbol{x}_t}[L(\boldsymbol{x}_t,\boldsymbol{\theta}_t)] = \mathbb{E}</em><em t-1="t-1">1,\cdots,\boldsymbol{x}</em>}}[\mathbb{E<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{x}_t}[L(\boldsymbol{x}_t,\boldsymbol{\theta}_t)]] = \mathbb{E}</em><em t-1="t-1">1,\cdots,\boldsymbol{x}</em>}}[L(\boldsymbol{\theta<em t="1">t)] = \mathbb{E}[L(\boldsymbol{\theta}_t)]\end{equation}<br />
由于$\boldsymbol{\varphi}$的数据无关性，$L(\boldsymbol{\varphi})$也可以写成$\mathbb{E}[L(\boldsymbol{\varphi})]$，将它们代入到式$\eqref{leq:avg-2-mid2}$得<br />
\begin{equation}\sum</em>}^T \eta_t \mathbb{E}[L(\boldsymbol{\theta<em t="1">t) - L(\boldsymbol{\varphi})]\leq \frac{\Vert\boldsymbol{\theta}_1 - \boldsymbol{\varphi}\Vert^2}{2} + \frac{G^2}{2}\sum</em>}^T \eta_t^2\label{leq:avg-2-mid3}\end{equation</p>
<h2 id="_5">思想讨论<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>在进一步推导之前，笔者打算多讨论几句：我们 <em>为什么要</em> 以及 <em>为什么能</em> 引入这种期望形式？</p>
<p>“为什么要”的答案很简单——为了让推导能够进行下去。比如，我们想要$L(\boldsymbol{x}_t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\varphi})$的非负性，但这是没法保证的，而加了期望后，我们就可以得到$L(\boldsymbol{\theta}_t) - L(\boldsymbol{\varphi})$的非负性，只要$\boldsymbol{\varphi}$取理论最优点就行。总的来说，通过求期望可以消去对数据变量$\boldsymbol{x}_t$的依赖，简化后续推导，是这一步骤的主要目的。</p>
<p>那“为什么能”呢？可能会有部分读者认为，我们又不会真的去重复训练很多次然后求平均，为何要关心这么一个期望结果呢？要回答这个问题，需要先思考一下：我们本质上想要什么？事实上，我们只想要一个SGD在某种程度上的收敛结论，由于目标函数限定了凸函数，我们希望这个结论在数学上是严格的。</p>
<p>对于一个复杂的随机过程而言，数学期望往往是我们能算的最基本结果，甚至可能是唯一能算的。因此，对两端加期望，虽然让结论与实践有所偏离，但它依然不失为一个具有参考价值的收敛结论。众所周知，实践通常更为复杂，所以只要能跟实践有一定联系、能够描述一些现象、产生一些启发，都是有价值的理论结果。</p>
<h2 id="_6">单调放缩<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>言归正传。现在有了式$\eqref{leq:avg-2-mid3}$，我们设$\boldsymbol{\varphi}$为理论最优点$\boldsymbol{\theta}^<em>$，就可以得到$L(\boldsymbol{x}_t,\boldsymbol{\theta}_t) - L(\boldsymbol{x}_t,\boldsymbol{\theta}^</em>)$的非负性，然后继续假设$\eta_t$的单调递减性，那么就可以将$\eta_t$换成$\eta_T$，最后两边除以$T\eta_T$得到<br />
\begin{equation}\frac{1}{T}\sum_{t=1}^T \mathbb{E}[L(\boldsymbol{\theta}<em t="1">t) - L(\boldsymbol{\theta}^<em>)] \leq \frac{\Vert\boldsymbol{\theta}_1 - \boldsymbol{\theta}^</em>\Vert^2}{2T\eta_T} + \frac{G^2}{2T}\sum</em>}^T \frac{\eta_t^2}{\eta_T}\label{leq:avg-2}\end{equation
对比式$\eqref{leq:avg-1}$，两者在关于$T$和$\eta_t$的依赖关系是相似的：右端第一项$R^2$换成了$\Vert\boldsymbol{\theta}<em t="1">1 - \boldsymbol{\theta}^<em>\Vert^2$，这相对来说是缩小的；而右端第二项求和的$\eta_t$换成了$\eta_t^2/\eta_T$，这通常是放大的。一小一大之下，区别大吗？我们观察两个例子。第一例是常数学习率$\frac{\alpha}{\sqrt{T}}$，此时结论$\eqref{leq:avg-1}$和$\eqref{leq:avg-2}$本质是一样的，以$\eqref{leq:avg-2}$为例，代入得到<br />
\begin{equation}\frac{1}{T}\sum_{t=1}^T \mathbb{E}[L(\boldsymbol{\theta}_t) - L(\boldsymbol{\theta}^</em>)] \leq \frac{1}{2\sqrt{T}}\left(\frac{\Vert\boldsymbol{\theta}_1 - \boldsymbol{\theta}^<em>\Vert^2}{\alpha} + G^2\alpha\right)\end{equation}<br />
也就是都能达到$\mathcal{O}(1/\sqrt{T})$的收敛速率。第二例是动态学习率$\eta_t = \frac{\alpha}{\sqrt{t}}$，上一篇文章我们已经证明了，代入到结论$\eqref{leq:avg-1}$，也能得到$\mathcal{O}(1/\sqrt{T})$的收敛速率，但如果代入到$\eqref{leq:avg-2}$，结果是<br />
\begin{equation}\frac{1}{T}\sum_{t=1}^T \mathbb{E}[L(\boldsymbol{\theta}_t) - L(\boldsymbol{\theta}^</em>)] \leq \frac{\Vert\boldsymbol{\theta}_1 - \boldsymbol{\theta}^*\Vert^2}{2 \alpha \sqrt{T}} + \frac{G^2\alpha}{2\sqrt{T}}\sum</em>}^T \frac{1}{t} \sim \mathcal{O}\left(\frac{\ln T}{\sqrt{T}}\right)\end{equation
因此，推广到无界域的代价是收敛速率从$\mathcal{O}(1/\sqrt{T})$略微放大成了$\mathcal{O}(\ln T/\sqrt{T})$，这种程度的放大对于实践来说并无区别。此外，这个结果仅仅是对于$\eta_t = \frac{\alpha}{\sqrt{t}}$而言的，通过设$\eta_t = \frac{\alpha}{\sqrt{t\ln t}}$，还可以将它逼近$\mathcal{O}(\sqrt{\ln T/T})$，但这实际上并无本质区别，更多是一种推导游戏。</p>
<h2 id="_7">加权平均<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>事实上，对于式$\eqref{leq:avg-2-mid3}$，还有一种更简单的处理方式：直接两边除以$\sum_{t=1}^T \eta_t$，然后代入$\boldsymbol{\varphi}=\boldsymbol{\theta}^<em>$得<br />
\begin{equation}\frac{\sum_{t=1}^T \eta_t \mathbb{E}[L(\boldsymbol{\theta}_t) - L(\boldsymbol{\theta}^</em>)]}{\sum_{t=1}^T \eta_t}\leq \frac{\Vert\boldsymbol{\theta}<em t="1">1 - \boldsymbol{\theta}^*\Vert^2}{2\sum</em>}^T \eta_t} + \frac{G^2}{2}\frac{\sum_{t=1}^T \eta_t^2}{\sum_{t=1}^T \eta_t}\label{leq:avg-3}\end{equation
这样不需要对学习率做单调递减假设，可以说是相当宽松的一个收敛结论了，它描述的是优化轨迹上的加权平均的收敛性质。可能有读者质疑为什么要加权呢？还是刚才的答案——“我们只想要一个SGD在某种程度上的收敛结论”——所以从逼近最优损失值来看，加不加权又有什么所谓呢？</p>
<p>某种程度上，式$\eqref{leq:avg-3}$给我们带来的启发要多于式$\eqref{leq:avg-2}$。比如，要想右端收敛得尽可能快，那么学习率之和$\sum_{t=1}^T \eta_t$要尽可能大，但平方和$\sum_{t=1}^T \eta_t^2$要尽可能小，如果想要$T\to\infty$时收敛到最优点，那么学习率应当满足<br />
\begin{equation}\sum_{t=1}^{\infty} \eta_t = \infty \qquad\text{和}\qquad \sum_{t=1}^{\infty} \eta_t^2 \bigg/ \sum_{t=1}^{\infty} \eta_t = 0\end{equation}<br />
这就得到了关于学习率策略的两个经典条件。此外，式$\eqref{leq:avg-3}$不显式依赖于终点学习率$\eta_T$，从而也允许$\eta_T\to 0$，因此我们可以更灵活调度学习率。总而言之，式$\eqref{leq:avg-3}$会给我们一种更“从容”的感觉，审美上也更为舒服。</p>
<h2 id="_8">文章小结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>这篇文章我们重启了“科学炼丹”专题，将上一篇文章的SGD在有界域收敛的结论推广到了无界域，得到了更丰富的结果。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11469">https://spaces.ac.cn/archives/11469</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Dec. 12, 2025). 《让炼丹更科学一些（二）：将结论推广到无界域 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11469">https://spaces.ac.cn/archives/11469</a></p>
<p>@online{kexuefm-11469,<br />
title={让炼丹更科学一些（二）：将结论推广到无界域},<br />
author={苏剑林},<br />
year={2025},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/11469}},<br />
} </p>
<hr />
<h2 id="_9">公式推导与注释<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="让炼丹更科学一些三sgd的终.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#5 让炼丹更科学一些（三）：SGD的终点损失收敛</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="一个二值化词向量模型是怎么跟果蝇搭上关系的.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#7 一个二值化词向量模型，是怎么跟果蝇搭上关系的？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">让炼丹更科学一些（二）：将结论推广到无界域</a><ul>
<li><a href="#_2">结论回顾</a></li>
<li><a href="#_3">新的思路</a></li>
<li><a href="#_4">加个期望</a></li>
<li><a href="#_5">思想讨论</a></li>
<li><a href="#_6">单调放缩</a></li>
<li><a href="#_7">加权平均</a></li>
<li><a href="#_8">文章小结</a></li>
<li><a href="#_9">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>