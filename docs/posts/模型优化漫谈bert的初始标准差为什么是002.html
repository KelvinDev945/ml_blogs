<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>模型优化漫谈：BERT的初始标准差为什么是0.02？ | ML & Math Blog Posts</title>
    <meta name="description" content="模型优化漫谈：BERT的初始标准差为什么是0.02？&para;
原文链接: https://spaces.ac.cn/archives/8747
发布日期: 

前几天在群里大家讨论到了“Transformer如何解决梯度消失”这个问题，答案有提到残差的，也有提到LN（Layer Norm）的。这些是否都是正确答案呢？事实上这是一个非常有趣而综合的问题，它其实关联到挺多模型细节，比如“BERT为...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=模型">模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #151 模型优化漫谈：BERT的初始标准差为什么是0.02？
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#151</span>
                模型优化漫谈：BERT的初始标准差为什么是0.02？
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2021-11-08</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 模型</span>
                </a>
                
                <a href="../index.html?tags=分析" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 分析</span>
                </a>
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="bert002">模型优化漫谈：BERT的初始标准差为什么是0.02？<a class="toc-link" href="#bert002" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8747">https://spaces.ac.cn/archives/8747</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>前几天在群里大家讨论到了“Transformer如何解决梯度消失”这个问题，答案有提到残差的，也有提到LN（Layer Norm）的。这些是否都是正确答案呢？事实上这是一个非常有趣而综合的问题，它其实关联到挺多模型细节，比如“BERT为什么要warmup？”、“BERT的初始化标准差为什么是0.02？”、“BERT做MLM预测之前为什么还要多加一层Dense？”，等等。本文就来集中讨论一下这些问题。</p>
<h2 id="_1">梯度消失说的是什么意思？<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>在文章<a href="/archives/7888">《也来谈谈RNN的梯度消失/爆炸问题》</a>中，我们曾讨论过RNN的梯度消失问题。事实上，一般模型的梯度消失现象也是类似，它指的是（主要是在模型的初始阶段）越靠近输入的层梯度越小，趋于零甚至等于零，而我们主要用的是基于梯度的优化器，所以梯度消失意味着我们没有很好的信号去调整优化前面的层。</p>
<p>换句话说，前面的层也许几乎没有得到更新，一直保持随机初始化的状态；只有比较靠近输出的层才更新得比较好，但这些层的输入是前面没有更新好的层的输出，所以输入质量可能会很糟糕（因为经过了一个近乎随机的变换），因此哪怕后面的层更新好了，总体效果也不好。最终，我们会观察到很反直觉的现象：模型越深，效果越差，哪怕训练集都如此。</p>
<p>解决梯度消失的一个标准方法就是残差链接，正式提出于<a href="https://papers.cool/arxiv/1512.03385">ResNet</a>中。残差的思想非常简单直接：你不是担心输入的梯度会消失吗？那我直接给它补上一个梯度为常数的项不就行了？最简单地，将模型变成<br />
\begin{equation}y = x + F(x)\end{equation}<br />
这样一来，由于多了一条“直通”路$x$，就算$F(x)$中的$x$梯度消失了，$x$的梯度基本上也能得以保留，从而使得深层模型得到有效的训练。</p>
<h2 id="ln">LN真的能缓解梯度消失？<a class="toc-link" href="#ln" title="Permanent link">&para;</a></h2>
<p>然而，在BERT和最初的Transformer里边，使用的是Post Norm设计，它把Norm操作加在了残差之后：<br />
\begin{equation}x_{t+1} = \text{Norm}(x_t + F_t(x_t))\end{equation}<br />
其实具体的Norm方法不大重要，不管是Batch Norm还是Layer Norm，结论都类似。在文章<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>中，我们已经分析过这种Norm结构，这里再来重复一下。</p>
<p>在初始化阶段，由于所有参数都是随机初始化的，所以我们可以认为$x$与$F(x)$是两个相互独立的随机向量，如果假设它们各自的方差是1，那么$x+F(x)$的方差就是2，而$\text{Norm}$操作负责将方差重新变为1，那么在初始化阶段，$\text{Norm}$操作就相当于“除以$\sqrt{2}$”：<br />
\begin{equation}x_{t+1} = \frac{x_t + F_t(x_t)}{\sqrt{2}}\end{equation}<br />
递归下去就是<br />
\begin{equation}\begin{aligned}
x_l =&amp;\, \frac{x_{l-1}}{\sqrt{2}} + \frac{F_{l-1}(x_{l-1})}{\sqrt{2}} \\
=&amp;\, \frac{x_{l-2}}{2} + \frac{F_{l-2}(x_{l-2})}{2} + \frac{F_{l-1}(x_{l-1})}{\sqrt{2}} \\
=&amp;\, \cdots \\
=&amp;\,\frac{x_0}{2^{l/2}} + \frac{F_0(x_0)}{2^{l/2}} + \frac{F_1(x_1)}{2^{(l-1)/2}} + \frac{F_2(x_2)}{2^{(l-2)/2}} + \cdots + \frac{F_{l-1}(x_{l-1})}{2^{1/2}}
\end{aligned}\end{equation}<br />
我们知道，残差有利于解决梯度消失，但是在Post Norm中，残差这条通道被严重削弱了，越靠近输入，削弱得越严重，残差“名存实亡”。所以说，在Post Norm的BERT模型中，LN不仅不能缓解梯度消失，它还是梯度消失的“元凶”之一。</p>
<h2 id="ln_1">那我们为什么还要加LN？<a class="toc-link" href="#ln_1" title="Permanent link">&para;</a></h2>
<p>那么，问题自然就来了：既然LN还加剧了梯度消失，那直接去掉它不好吗？</p>
<p>是可以去掉，但是前面说了，$x+F(x)$的方差就是2了，残差越多方差就越大了，所以还是要加一个Norm操作，我们可以把它加到每个模块的输入，即变为$x+F(\text{Norm}(x))$，最后的总输出再加个$\text{Norm}$就行，这就是Pre Norm结构，这时候每个残差分支是平权的，而不是像Post Norm那样有指数衰减趋势。当然，也有完全不加Norm的，但需要对$F(x)$进行特殊的初始化，让它初始输出更接近于0，比如ReZero、Skip Init、Fixup等，这些在<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>也都已经介绍过了。</p>
<p>但是，抛开这些改进不说，Post Norm就没有可取之处吗？难道Transformer和BERT开始就带了一个完全失败的设计？</p>
<p>显然不大可能。虽然Post Norm会带来一定的梯度消失问题，但其实它也有其他方面的好处。最明显的是，它稳定了前向传播的数值，并且保持了每个模块的一致性。比如BERT base，我们可以在最后一层接一个Dense来分类，也可以取第6层接一个Dense来分类；但如果你是Pre Norm的话，取出中间层之后，你需要自己接一个LN然后再接Dense，否则越靠后的层方差越大，不利于优化。</p>
<p>其次，梯度消失也不全是“坏处”，其实对于Finetune阶段来说，它反而是好处。在Finetune的时候，我们通常希望优先调整靠近输出层的参数，不要过度调整靠近输入层的参数，以免严重破坏预训练效果。而梯度消失意味着越靠近输入层，其结果对最终输出的影响越弱，这正好是Finetune时所希望的。所以，预训练好的Post Norm模型，往往比Pre Norm模型有更好的Finetune效果，这我们在<a href="/archives/8027">《RealFormer：把残差转移到Attention矩阵上面去》</a>也提到过。</p>
<h2 id="_2">我们真的担心梯度消失吗？<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>其实，最关键的原因是，在当前的各种自适应优化技术下，我们已经不大担心梯度消失问题了。</p>
<p>这是因为，当前NLP中主流的优化器是Adam及其变种。对于Adam来说，由于包含了动量和二阶矩校正，所以近似来看，它的更新量大致上为<br />
\begin{equation}\Delta \theta = -\eta\frac{\mathbb{E}_t[g_t]}{\sqrt{\mathbb{E}_t[g_t^2]}}\end{equation}<br />
可以看到，分子分母是都是同量纲的，因此分式结果其实就是$\mathcal{O}(1)$的量级，而更新量就是$\mathcal{O}(\eta)$量级。也就是说，理论上只要梯度的绝对值大于随机误差，那么对应的参数都会有常数量级的更新量；这跟SGD不一样，SGD的更新量是正比于梯度的，只要梯度小，更新量也会很小，如果梯度过小，那么参数几乎会没被更新。</p>
<p>所以，Post Norm的残差虽然被严重削弱，但是在base、large级别的模型中，它还不至于削弱到小于随机误差的地步，因此配合Adam等优化器，它还是可以得到有效更新的，也就有可能成功训练了。当然，只是有可能，事实上越深的Post Norm模型确实越难训练，比如要仔细调节学习率和Warmup等。</p>
<h2 id="warmup">Warmup是怎样起作用的？<a class="toc-link" href="#warmup" title="Permanent link">&para;</a></h2>
<p>大家可能已经听说过，Warmup是Transformer训练的关键步骤，没有它可能不收敛，或者收敛到比较糟糕的位置。为什么会这样呢？不是说有了Adam就不怕梯度消失了吗？</p>
<p>要注意的是，Adam解决的是梯度消失带来的参数更新量过小问题，也就是说，不管梯度消失与否，更新量都不会过小。但对于Post Norm结构的模型来说，梯度消失依然存在，只不过它的意义变了。根据泰勒展开式：<br />
\begin{equation}f(x+\Delta x) \approx f(x) + \langle\nabla_x f(x), \Delta x\rangle\end{equation}<br />
也就是说增量$f(x+\Delta x) - f(x)$是正比于梯度的，换句话说，梯度衡量了输出对输入的依赖程度。如果梯度消失，那么意味着模型的输出对输入的依赖变弱了。</p>
<p>Warmup是在训练开始阶段，将学习率从0缓增到指定大小，而不是一开始从指定大小训练。如果不进行Wamrup，那么模型一开始就快速地学习，由于梯度消失，模型对越靠后的层越敏感，也就是越靠后的层学习得越快，然后后面的层是以前面的层的输出为输入的，前面的层根本就没学好，所以后面的层虽然学得快，但却是建立在糟糕的输入基础上的。</p>
<p>很快地，后面的层以糟糕的输入为基础到达了一个糟糕的局部最优点，此时它的学习开始放缓（因为已经到达了它认为的最优点附近），同时反向传播给前面层的梯度信号进一步变弱，这就导致了前面的层的梯度变得不准。但我们说过，Adam的更新量是常数量级的，梯度不准，但更新量依然是数量级，意味着可能就是一个常数量级的随机噪声了，于是学习方向开始不合理，前面的输出开始崩盘，导致后面的层也一并崩盘。</p>
<p>所以，如果Post Norm结构的模型不进行Wamrup，我们能观察到的现象往往是：loss快速收敛到一个常数附近，然后再训练一段时间，loss开始发散，直至NAN。如果进行Wamrup，那么留给模型足够多的时间进行“预热”，在这个过程中，主要是抑制了后面的层的学习速度，并且给了前面的层更多的优化时间，以促进每个层的同步优化。</p>
<p>这里的讨论前提是梯度消失，如果是Pre Norm之类的结果，没有明显的梯度消失现象，那么不加Warmup往往也可以成功训练。</p>
<h2 id="002">初始标准差为什么是0.02？<a class="toc-link" href="#002" title="Permanent link">&para;</a></h2>
<p>喜欢扣细节的同学会留意到，BERT默认的初始化方法是标准差为0.02的截断正态分布，在<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>我们也提过，由于是截断正态分布，所以实际标准差会更小，大约是$0.02/1.1368472\approx 0.0176$。这个标准差是大还是小呢？对于Xavier初始化来说，一个$n\times n$的矩阵应该用$1/n$的方差初始化，而BERT base的$n$为768，算出来的标准差是$1/\sqrt{768}\approx 0.0361$。这就意味着，这个初始化标准差是明显偏小的，大约只有常见初始化标准差的一半。</p>
<p>为什么BERT要用偏小的标准差初始化呢？事实上，这还是跟Post Norm设计有关，偏小的标准差会导致函数的输出整体偏小，从而使得Post Norm设计在初始化阶段更接近于恒等函数，从而更利于优化。具体来说，按照前面的假设，如果$x$的方差是1，$F(x)$的方差是$\sigma^2$，那么初始化阶段，$\text{Norm}$操作就相当于除以$\sqrt{1+\sigma^2}$。如果$\sigma$比较小，那么残差中的“直路”权重就越接近于1，那么模型初始阶段就越接近一个恒等函数，就越不容易梯度消失。</p>
<p>正所谓“我们不怕梯度消失，但我们也不希望梯度消失”，简单地将初始化标注差设小一点，就可以使得$\sigma$变小一点，从而在保持Post Norm的同时缓解一下梯度消失，何乐而不为？那能不能设置得更小甚至全零？一般来说初始化过小会丧失多样性，缩小了模型的试错空间，也会带来负面效果。综合来看，缩小到标准的1/2，是一个比较靠谱的选择了。</p>
<p>当然，也确实有人喜欢挑战极限的，最近笔者也看到了一篇文章，试图让整个模型用几乎全零的初始化，还训练出了不错的效果，大家有兴趣可以读读，文章为<a href="https://papers.cool/arxiv/2110.12661">《ZerO Initialization: Initializing Residual Networks with only Zeros and Ones》</a>。</p>
<h2 id="mlmdense">为什么MLM要多加Dense？<a class="toc-link" href="#mlmdense" title="Permanent link">&para;</a></h2>
<p>最后，是关于BERT的MLM模型的一个细节，就是BERT在做MLM的概率预测之前，还要多接一个Dense层和LN层，这是为什么呢？不接不行吗？</p>
<p>之前看到过的答案大致上是觉得，越靠近输出层的，越是依赖任务的（Task-Specified），我们多接一个Dense层，希望这个Dense层是MLM-Specified的，然后下游任务微调的时候就不是MLM-Specified的，所以把它去掉。这个解释看上去有点合理，但总感觉有点玄学，毕竟Task-Specified这种东西不大好定量分析。</p>
<p>这里笔者给出另外一个更具体的解释，事实上它还是跟BERT用了0.02的标准差初始化直接相关。刚才我们说了，这个初始化是偏小的，如果我们不额外加Dense就乘上Embedding预测概率分布，那么得到的分布就过于均匀了（Softmax之前，每个logit都接近于0），于是模型就想着要把数值放大。现在模型有两个选择：第一，放大Embedding层的数值，但是Embedding层的更新是稀疏的，一个个放大太麻烦；第二，就是放大输入，我们知道BERT编码器最后一层是LN，LN最后有个初始化为1的gamma参数，直接将那个参数放大就好。</p>
<p>模型优化使用的是梯度下降，我们知道它会选择最快的路径，显然是第二个选择更快，所以模型会优先走第二条路。这就导致了一个现象：最后一个LN层的gamma值会偏大。如果预测MLM概率分布之前不加一个Dense+LN，那么BERT编码器的最后一层的LN的gamma值会偏大，导致最后一层的方差会比其他层的明显大，显然不够优雅；而多加了一个Dense+LN后，偏大的gamma就转移到了新增的LN上去了，而编码器的每一层则保持了一致性。</p>
<p>事实上，读者可以自己去观察一下BERT每个LN层的gamma值，就会发现确实是最后一个LN层的gamma值是会明显偏大的，这就验证了我们的猜测～</p>
<h2 id="_3">希望大家多多海涵批评斧正<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>本文试图回答了Transformer、BERT的模型优化相关的几个问题，有一些是笔者在自己的预训练工作中发现的结果，有一些则是结合自己的经验所做的直观想象。不管怎样，算是分享一个参考答案吧，如果有不当的地方，请大家海涵，也请各位批评斧正～</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8747">https://spaces.ac.cn/archives/8747</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Nov. 08, 2021). 《模型优化漫谈：BERT的初始标准差为什么是0.02？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8747">https://spaces.ac.cn/archives/8747</a></p>
<p>@online{kexuefm-8747,<br />
title={模型优化漫谈：BERT的初始标准差为什么是0.02？},<br />
author={苏剑林},<br />
year={2021},<br />
month={Nov},<br />
url={\url{https://spaces.ac.cn/archives/8747}},<br />
} </p>
<hr />
<h2 id="_4">公式推导与注释<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<h3 id="1-xavier">1. Xavier初始化的完整推导<a class="toc-link" href="#1-xavier" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 前向传播的方差分析<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>考虑一个全连接层：</p>
<p>\begin{equation}
\boldsymbol{y} = \boldsymbol{W}\boldsymbol{x} + \boldsymbol{b}
\tag{1}
\end{equation}</p>
<p>其中 $\boldsymbol{x} \in \mathbb{R}^{n_{in}}$ 是输入，$\boldsymbol{W} \in \mathbb{R}^{n_{out} \times n_{in}}$ 是权重矩阵，$\boldsymbol{y} \in \mathbb{R}^{n_{out}}$ 是输出。</p>
<p>对于输出的第 $i$ 个分量：</p>
<p>\begin{equation}
y_i = \sum_{j=1}^{n_{in}} W_{ij} x_j + b_i
\tag{2}
\end{equation}</p>
<p><strong>假设</strong>：
1. 输入 $x_j$ 均值为0，方差为 $\text{Var}[x]$
2. 权重 $W_{ij}$ 独立同分布，均值为0，方差为 $\sigma_w^2$
3. $x_j$ 与 $W_{ij}$ 相互独立
4. 偏置初始化为0：$b_i = 0$</p>
<p><strong>输出的期望</strong>：</p>
<p>\begin{equation}
\mathbb{E}[y_i] = \sum_{j=1}^{n_{in}} \mathbb{E}[W_{ij}]\mathbb{E}[x_j] = 0
\tag{3}
\end{equation}</p>
<p><strong>输出的方差</strong>：</p>
<p>\begin{equation}
\begin{aligned}
\text{Var}[y_i] &amp;= \mathbb{E}[y_i^2] - \mathbb{E}[y_i]^2 \
&amp;= \mathbb{E}\left[\left(\sum_{j=1}^{n_{in}} W_{ij} x_j\right)^2\right] \
&amp;= \mathbb{E}\left[\sum_{j=1}^{n_{in}} W_{ij}^2 x_j^2 + \sum_{j \neq k} W_{ij} W_{ik} x_j x_k\right] \
&amp;= \sum_{j=1}^{n_{in}} \mathbb{E}[W_{ij}^2]\mathbb{E}[x_j^2] + \sum_{j \neq k} \mathbb{E}[W_{ij}]\mathbb{E}[W_{ik}]\mathbb{E}[x_j]\mathbb{E}[x_k] \
&amp;= n_{in} \cdot \sigma_w^2 \cdot \text{Var}[x] \
&amp;= n_{in} \sigma_w^2 \text{Var}[x]
\end{aligned}
\tag{4}
\end{equation}</p>
<p><strong>方差保持条件</strong>（前向传播）：</p>
<p>为了保持方差不变，即 $\text{Var}[y] = \text{Var}[x]$，需要：</p>
<p>\begin{equation}
n_{in} \sigma_w^2 = 1 \quad \Rightarrow \quad \sigma_w^2 = \frac{1}{n_{in}}
\tag{5}
\end{equation}</p>
<h4 id="12">1.2 反向传播的方差分析<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p>在反向传播中，梯度从输出层传播到输入层：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial x_j} = \sum_{i=1}^{n_{out}} \frac{\partial \mathcal{L}}{\partial y_i} \cdot \frac{\partial y_i}{\partial x_j} = \sum_{i=1}^{n_{out}} \frac{\partial \mathcal{L}}{\partial y_i} \cdot W_{ij}
\tag{6}
\end{equation}</p>
<p>类似地，假设 $\frac{\partial \mathcal{L}}{\partial y_i}$ 均值为0，方差为 $\text{Var}[g_y]$，则：</p>
<p>\begin{equation}
\text{Var}\left[\frac{\partial \mathcal{L}}{\partial x_j}\right] = n_{out} \sigma_w^2 \text{Var}[g_y]
\tag{7}
\end{equation}</p>
<p><strong>方差保持条件</strong>（反向传播）：</p>
<p>为了保持梯度方差不变，需要：</p>
<p>\begin{equation}
n_{out} \sigma_w^2 = 1 \quad \Rightarrow \quad \sigma_w^2 = \frac{1}{n_{out}}
\tag{8}
\end{equation}</p>
<h4 id="13-xavier">1.3 Xavier初始化的折中方案<a class="toc-link" href="#13-xavier" title="Permanent link">&para;</a></h4>
<p>前向和反向传播的要求不同（除非 $n_{in} = n_{out}$），Xavier初始化采用折中：</p>
<p>\begin{equation}
\sigma_w^2 = \frac{2}{n_{in} + n_{out}}
\tag{9}
\end{equation}</p>
<p>或者只考虑前向传播（Glorot uniform）：</p>
<p>\begin{equation}
\sigma_w^2 = \frac{1}{n_{in}}
\tag{10}
\end{equation}</p>
<p><strong>对应的标准差</strong>：</p>
<p>\begin{equation}
\sigma_w = \frac{1}{\sqrt{n_{in}}}
\tag{11}
\end{equation}</p>
<p>对于BERT base，隐藏层维度 $d = n_{in} = n_{out} = 768$：</p>
<p>\begin{equation}
\sigma_w = \frac{1}{\sqrt{768}} \approx 0.0361
\tag{12}
\end{equation}</p>
<h3 id="2-he">2. He初始化的推导<a class="toc-link" href="#2-he" title="Permanent link">&para;</a></h3>
<h4 id="21-relu">2.1 ReLU激活函数的影响<a class="toc-link" href="#21-relu" title="Permanent link">&para;</a></h4>
<p>当使用ReLU激活函数时：</p>
<p>\begin{equation}
\text{ReLU}(x) = \max(0, x) = \begin{cases}
x, &amp; x &gt; 0 \
0, &amp; x \leq 0
\end{cases}
\tag{13}
\end{equation}</p>
<p>假设输入 $x$ 对称分布于0（如正态分布），则：</p>
<p>\begin{equation}
\mathbb{P}(x &gt; 0) = \mathbb{P}(x \leq 0) = \frac{1}{2}
\tag{14}
\end{equation}</p>
<p><strong>ReLU输出的方差</strong>：</p>
<p>\begin{equation}
\begin{aligned}
\text{Var}[\text{ReLU}(x)] &amp;= \mathbb{E}[\text{ReLU}(x)^2] - \mathbb{E}[\text{ReLU}(x)]^2 \
&amp;= \mathbb{E}[x^2 \cdot \mathbb{1}<em x_0="x&gt;0">{x&gt;0}] - \left(\mathbb{E}[x \cdot \mathbb{1}</em>]\right)^2 \
&amp;= \frac{1}{2}\mathbb{E}[x^2] - \left(\frac{1}{2}\mathbb{E}[x | x &gt; 0]\right)^2 \
&amp;\approx \frac{1}{2}\text{Var}[x]
\end{aligned}
\tag{15}
\end{equation}</p>
<p><strong>数学直觉</strong>：ReLU将约一半的神经元置为0，使输出方差减半。</p>
<h4 id="22-he">2.2 He初始化的方差补偿<a class="toc-link" href="#22-he" title="Permanent link">&para;</a></h4>
<p>为了补偿ReLU造成的方差减半，He初始化将权重方差加倍：</p>
<p>\begin{equation}
\sigma_w^2 = \frac{2}{n_{in}}
\tag{16}
\end{equation}</p>
<p>对应的标准差：</p>
<p>\begin{equation}
\sigma_w = \sqrt{\frac{2}{n_{in}}} = \frac{\sqrt{2}}{\sqrt{n_{in}}}
\tag{17}
\end{equation}</p>
<p>对于 $n_{in} = 768$：</p>
<p>\begin{equation}
\sigma_w = \frac{\sqrt{2}}{\sqrt{768}} \approx 0.0510
\tag{18}
\end{equation}</p>
<h3 id="3-bert002">3. BERT初始化0.02的数学分析<a class="toc-link" href="#3-bert002" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 截断正态分布的修正<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>BERT使用截断正态分布（truncated normal distribution），在 $[-2\sigma, 2\sigma]$ 区间截断。</p>
<p><strong>实际标准差的计算</strong>：</p>
<p>对于标准正态分布 $\mathcal{N}(0, 1)$，在 $[-a, a]$ 截断后的方差：</p>
<p>\begin{equation}
\text{Var}[X_{trunc}] = \frac{\phi(a) - \phi(-a) - 2a[\Phi(a) - \Phi(-a)]}{\Phi(a) - \Phi(-a)}
\tag{19}
\end{equation}</p>
<p>其中 $\phi$ 是标准正态分布的概率密度函数，$\Phi$ 是累积分布函数。</p>
<p>当 $a = 2$ 时（BERT的截断点），可以计算得到缩放因子：</p>
<p>\begin{equation}
\frac{\sigma_{actual}}{\sigma_{nominal}} \approx 0.8796 \approx \frac{1}{1.1368}
\tag{20}
\end{equation}</p>
<p>因此，如果名义标准差是 $\sigma = 0.02$，实际标准差约为：</p>
<p>\begin{equation}
\sigma_{actual} = \frac{0.02}{1.1368} \approx 0.0176
\tag{21}
\end{equation}</p>
<h4 id="32-xavier">3.2 为什么是标准Xavier的一半<a class="toc-link" href="#32-xavier" title="Permanent link">&para;</a></h4>
<p>从Xavier初始化的理论值：</p>
<p>\begin{equation}
\sigma_{xavier} = \frac{1}{\sqrt{768}} \approx 0.0361
\tag{22}
\end{equation}</p>
<p>BERT的实际标准差：</p>
<p>\begin{equation}
\sigma_{bert} \approx 0.0176 \approx \frac{0.0361}{2} = \frac{\sigma_{xavier}}{2}
\tag{23}
\end{equation}</p>
<p><strong>比例关系</strong>：</p>
<p>\begin{equation}
\frac{\sigma_{bert}}{\sigma_{xavier}} \approx 0.487 \approx \frac{1}{2}
\tag{24}
\end{equation}</p>
<h4 id="33">3.3 小初始化的理论依据<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>对于Post-LN结构，假设输入 $\boldsymbol{x}$ 的方差为1，函数 $F(\boldsymbol{x})$ 的方差为 $\sigma^2$，则：</p>
<p>\begin{equation}
\text{Var}[\boldsymbol{x} + F(\boldsymbol{x})] = \text{Var}[\boldsymbol{x}] + \text{Var}[F(\boldsymbol{x})] = 1 + \sigma^2
\tag{25}
\end{equation}</p>
<p>（假设 $\boldsymbol{x}$ 与 $F(\boldsymbol{x})$ 独立）</p>
<p>Layer Norm会将方差归一化到1，相当于除以 $\sqrt{1 + \sigma^2}$：</p>
<p>\begin{equation}
\text{Norm}(\boldsymbol{x} + F(\boldsymbol{x})) \approx \frac{\boldsymbol{x} + F(\boldsymbol{x})}{\sqrt{1 + \sigma^2}}
\tag{26}
\end{equation}</p>
<p><strong>残差权重</strong>（初始阶段 $\boldsymbol{x}$ 的有效系数）：</p>
<p>\begin{equation}
\alpha = \frac{1}{\sqrt{1 + \sigma^2}}
\tag{27}
\end{equation}</p>
<p>当 $\sigma = \sigma_{xavier} = 0.0361$ 时：</p>
<p>\begin{equation}
\alpha = \frac{1}{\sqrt{1 + 0.0361^2}} \approx \frac{1}{\sqrt{1.0013}} \approx 0.9994
\tag{28}
\end{equation}</p>
<p>当 $\sigma = \sigma_{bert} = 0.0176$ 时：</p>
<p>\begin{equation}
\alpha = \frac{1}{\sqrt{1 + 0.0176^2}} \approx \frac{1}{\sqrt{1.00031}} \approx 0.9998
\tag{29}
\end{equation}</p>
<p><strong>结论</strong>：更小的初始化（$\sigma = 0.0176$）使残差权重更接近1，模型初始阶段更接近恒等函数。</p>
<h3 id="4-post-ln-vs-pre-ln">4. Post-LN vs Pre-LN的方差传播<a class="toc-link" href="#4-post-ln-vs-pre-ln" title="Permanent link">&para;</a></h3>
<h4 id="41-post-ln">4.1 Post-LN的递归分析<a class="toc-link" href="#41-post-ln" title="Permanent link">&para;</a></h4>
<p>Post-LN结构：</p>
<p>\begin{equation}
\boldsymbol{x}_{t+1} = \text{Norm}(\boldsymbol{x}_t + F_t(\boldsymbol{x}_t))
\tag{30}
\end{equation}</p>
<p>在初始化阶段，假设 $\text{Var}[F_t(\boldsymbol{x}_t)] = \sigma^2$：</p>
<p>\begin{equation}
\text{Norm}(\boldsymbol{x}_t + F_t(\boldsymbol{x}_t)) \approx \frac{\boldsymbol{x}_t + F_t(\boldsymbol{x}_t)}{\sqrt{1 + \sigma^2}}
\tag{31}
\end{equation}</p>
<p>递归展开到第 $L$ 层：</p>
<p>\begin{equation}
\begin{aligned}
\boldsymbol{x}<em L-1="L-1">L &amp;= \frac{\boldsymbol{x}_0}{(1+\sigma^2)^{L/2}} + \frac{F_0(\boldsymbol{x}_0)}{(1+\sigma^2)^{L/2}} + \frac{F_1(\boldsymbol{x}_1)}{(1+\sigma^2)^{(L-1)/2}} + \cdots \
&amp;\quad + \frac{F</em>
\end{aligned}
\tag{32}
\end{equation}}(\boldsymbol{x}_{L-1})}{(1+\sigma^2)^{1/2}</p>
<p><strong>梯度衰减因子</strong>（从第 $L$ 层到第 $0$ 层）：</p>
<p>\begin{equation}
\frac{\partial \boldsymbol{x}_L}{\partial \boldsymbol{x}_0} \propto \frac{1}{(1+\sigma^2)^{L/2}}
\tag{33}
\end{equation}</p>
<p>当 $\sigma^2 = 0.0361^2 \approx 0.0013$ 时，$L = 12$（BERT base）：</p>
<p>\begin{equation}
(1 + 0.0013)^{6} \approx 1.0078
\tag{34}
\end{equation}</p>
<p>当 $\sigma^2 = 0.0176^2 \approx 0.00031$ 时：</p>
<p>\begin{equation}
(1 + 0.00031)^{6} \approx 1.0019
\tag{35}
\end{equation}</p>
<p><strong>数学直觉</strong>：更小的 $\sigma$ 减缓了梯度衰减速度。</p>
<h4 id="42-pre-ln">4.2 Pre-LN的方差传播<a class="toc-link" href="#42-pre-ln" title="Permanent link">&para;</a></h4>
<p>Pre-LN结构：</p>
<p>\begin{equation}
\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t))
\tag{36}
\end{equation}</p>
<p>假设 $F_t$ 的输出方差为 $\sigma^2$：</p>
<p>\begin{equation}
\text{Var}[\boldsymbol{x}_{t+1}] = \text{Var}[\boldsymbol{x}_t] + \sigma^2
\tag{37}
\end{equation}</p>
<p>递归到第 $L$ 层：</p>
<p>\begin{equation}
\text{Var}[\boldsymbol{x}_L] = \text{Var}[\boldsymbol{x}_0] + L\sigma^2
\tag{38}
\end{equation}</p>
<p><strong>方差线性增长</strong>，而不是Post-LN的指数衰减。</p>
<p><strong>梯度传播</strong>：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em t="0">0} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_L} \cdot \left(\boldsymbol{I} + \sum</em>\right)
\tag{39}
\end{equation}}^{L-1} \frac{\partial F_t}{\partial \boldsymbol{x}_t</p>
<p>残差直连路径使得梯度不会严重衰减。</p>
<h3 id="5-warmup">5. Warmup的数学必要性<a class="toc-link" href="#5-warmup" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 学习率与更新量的关系<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>Adam优化器的更新公式（简化版）：</p>
<p>\begin{equation}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t \frac{\boldsymbol{m}_t}{\sqrt{\boldsymbol{v}_t} + \epsilon}
\tag{40}
\end{equation}</p>
<p>其中：
- $\boldsymbol{m}<em t-1="t-1">t = \beta_1 \boldsymbol{m}</em>} + (1-\beta_1)\boldsymbol{g<em t-1="t-1">t$ 是一阶矩估计
- $\boldsymbol{v}_t = \beta_2 \boldsymbol{v}</em>_t^2$ 是二阶矩估计
- $\eta_t$ 是学习率} + (1-\beta_2)\boldsymbol{g</p>
<p><strong>更新量的量级</strong>：</p>
<p>\begin{equation}
|\Delta\boldsymbol{\theta}_t| \approx \eta_t \frac{|\boldsymbol{m}_t|}{\sqrt{|\boldsymbol{v}_t|}} = \mathcal{O}(\eta_t)
\tag{41}
\end{equation}</p>
<h4 id="52">5.2 梯度消失下的问题<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>对于Post-LN，第 $\ell$ 层的梯度：</p>
<p>\begin{equation}
\boldsymbol{g}<em>\ell = \frac{\partial \mathcal{L}}{\partial \boldsymbol{\theta}</em>\ell} \propto \frac{1}{(1+\sigma^2)^{(L-\ell)/2}} \cdot \boldsymbol{g}_L
\tag{42}
\end{equation}</p>
<p><strong>浅层梯度小但更新量恒定</strong>：</p>
<p>即使 $\boldsymbol{g}_\ell$ 很小，Adam仍会产生 $\mathcal{O}(\eta)$ 的更新量，方向可能不准确。</p>
<h4 id="53-warmup">5.3 Warmup的作用<a class="toc-link" href="#53-warmup" title="Permanent link">&para;</a></h4>
<p>Warmup在前 $T_{warmup}$ 步线性增加学习率：</p>
<p>\begin{equation}
\eta_t = \begin{cases}
\eta_{max} \cdot \frac{t}{T_{warmup}}, &amp; t \leq T_{warmup} \
\eta_{max} \cdot \text{decay}(t), &amp; t &gt; T_{warmup}
\end{cases}
\tag{43}
\end{equation}</p>
<p><strong>好处</strong>：
1. 初期小学习率让浅层有时间适应
2. 防止深层过快收敛到局部次优解
3. 稳定初始阶段的方差估计</p>
<p><strong>理论保证</strong>（近似分析）：</p>
<p>定义层间梯度比：</p>
<p>\begin{equation}
r = \frac{|\boldsymbol{g}_0|}{|\boldsymbol{g}_L|} \approx (1+\sigma^2)^{-L/2}
\tag{44}
\end{equation}</p>
<p>Warmup步数应满足：</p>
<p>\begin{equation}
T_{warmup} \gtrsim \frac{1}{r} = (1+\sigma^2)^{L/2}
\tag{45}
\end{equation}</p>
<p>对于BERT（$L=12$，$\sigma=0.0176$）：</p>
<p>\begin{equation}
T_{warmup} \gtrsim (1.00031)^6 \approx 1.002 \times \text{baseline}
\tag{46}
\end{equation}</p>
<p>实际BERT使用约10,000步warmup（总训练步数的1-2%）。</p>
<h3 id="6">6. 梯度流与信号传播分析<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 前向信号传播<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>定义信号强度为方差：</p>
<p>\begin{equation}
S_\ell = \text{Var}[\boldsymbol{x}_\ell]
\tag{47}
\end{equation}</p>
<p>对于Post-LN：</p>
<p>\begin{equation}
S_{\ell+1} = \frac{S_\ell + \sigma^2}{1 + \sigma^2} \approx 1
\tag{48}
\end{equation}</p>
<p>（Layer Norm保持方差为1）</p>
<p>对于Pre-LN：</p>
<p>\begin{equation}
S_{\ell+1} = S_\ell + \sigma^2
\tag{49}
\end{equation}</p>
<p>（方差累积增长）</p>
<h4 id="62">6.2 反向梯度传播<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p>定义梯度信号强度：</p>
<p>\begin{equation}
G_\ell = \text{Var}\left[\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_\ell}\right]
\tag{50}
\end{equation}</p>
<p>对于Post-LN，通过链式法则：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em _ell_1="\ell+1">\ell} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}} \cdot \frac{\partial \boldsymbol{x<em>{\ell+1}}{\partial \boldsymbol{x}</em>\ell}
\tag{51}
\end{equation}</p>
<p>其中：</p>
<p>\begin{equation}
\frac{\partial \boldsymbol{x}<em>{\ell+1}}{\partial \boldsymbol{x}</em>\ell} = \frac{1}{\sqrt{1+\sigma^2}} \left(\boldsymbol{I} + \frac{\partial F_\ell}{\partial \boldsymbol{x}_\ell}\right)
\tag{52}
\end{equation}</p>
<p><strong>残差路径的贡献</strong>：</p>
<p>\begin{equation}
\left|\frac{\partial \boldsymbol{x}<em>{\ell+1}}{\partial \boldsymbol{x}</em>\ell}\right| \approx \frac{1}{\sqrt{1+\sigma^2}}
\tag{53}
\end{equation}</p>
<p>层层递归：</p>
<p>\begin{equation}
G_\ell \approx \frac{G_L}{(1+\sigma^2)^{L-\ell}}
\tag{54}
\end{equation}</p>
<h4 id="63-effective-depth">6.3 有效深度（Effective Depth）<a class="toc-link" href="#63-effective-depth" title="Permanent link">&para;</a></h4>
<p>定义梯度衰减到 $e^{-1}$ 时的深度为有效深度：</p>
<p>\begin{equation}
(1+\sigma^2)^{L_{eff}} = e \quad \Rightarrow \quad L_{eff} = \frac{1}{\log(1+\sigma^2)}
\tag{55}
\end{equation}</p>
<p>当 $\sigma^2 \ll 1$ 时：</p>
<p>\begin{equation}
L_{eff} \approx \frac{1}{\sigma^2}
\tag{56}
\end{equation}</p>
<p>对于 $\sigma = 0.0361$：</p>
<p>\begin{equation}
L_{eff} \approx \frac{1}{0.0013} \approx 769 \text{ 层}
\tag{57}
\end{equation}</p>
<p>对于 $\sigma = 0.0176$：</p>
<p>\begin{equation}
L_{eff} \approx \frac{1}{0.00031} \approx 3226 \text{ 层}
\tag{58}
\end{equation}</p>
<p><strong>结论</strong>：更小的初始化显著增加有效深度。</p>
<h3 id="7-layer-scale">7. Layer Scale与初始化的关系<a class="toc-link" href="#7-layer-scale" title="Permanent link">&para;</a></h3>
<h4 id="71-layer-scale">7.1 Layer Scale机制<a class="toc-link" href="#71-layer-scale" title="Permanent link">&para;</a></h4>
<p>Layer Scale在残差分支添加可学习的缩放参数：</p>
<p>\begin{equation}
\boldsymbol{x}<em>{\ell+1} = \boldsymbol{x}</em>\ell + \boldsymbol{\lambda}<em>\ell \odot F</em>\ell(\boldsymbol{x}_\ell)
\tag{59}
\end{equation}</p>
<p>其中 $\boldsymbol{\lambda}_\ell$ 初始化为小值（如 $10^{-4}$ 或 $10^{-6}$）。</p>
<p><strong>等效于更小的初始化</strong>：</p>
<p>\begin{equation}
\text{Var}[\boldsymbol{\lambda}<em>\ell \odot F</em>\ell(\boldsymbol{x}_\ell)] = \lambda^2 \sigma^2
\tag{60}
\end{equation}</p>
<p>如果 $\lambda = 10^{-4}$，$\sigma = 0.02$：</p>
<p>\begin{equation}
\sigma_{eff} = \lambda \sigma = 10^{-4} \times 0.02 = 2 \times 10^{-6}
\tag{61}
\end{equation}</p>
<h4 id="72">7.2 深度可扩展性<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p>使用Layer Scale后，有效深度变为：</p>
<p>\begin{equation}
L_{eff} \approx \frac{1}{\lambda^2 \sigma^2}
\tag{62}
\end{equation}</p>
<p>对于 $\lambda = 10^{-4}$，$\sigma = 0.02$：</p>
<p>\begin{equation}
L_{eff} \approx \frac{1}{(10^{-4})^2 \times 0.02^2} = \frac{1}{4 \times 10^{-12}} = 2.5 \times 10^{11}
\tag{63}
\end{equation}</p>
<p><strong>数学直觉</strong>：Layer Scale通过极小的初始化，使深层模型初始阶段几乎是恒等函数。</p>
<h3 id="8-mlm-dense">8. MLM Dense层的数学解释<a class="toc-link" href="#8-mlm-dense" title="Permanent link">&para;</a></h3>
<h4 id="81-logits">8.1 Logits的数值范围<a class="toc-link" href="#81-logits" title="Permanent link">&para;</a></h4>
<p>假设编码器最后一层输出 $\boldsymbol{h} \in \mathbb{R}^d$，经过Layer Norm后：</p>
<p>\begin{equation}
|\boldsymbol{h}|^2 \approx d
\tag{64}
\end{equation}</p>
<p>（因为每个分量方差约为1）</p>
<p>直接用Embedding矩阵 $\boldsymbol{W}_{emb} \in \mathbb{R}^{V \times d}$ 投影，第 $i$ 个logit：</p>
<p>\begin{equation}
\text{logit}<em emb="emb">i = \boldsymbol{W}</em>
\tag{65}
\end{equation}}[i, :] \cdot \boldsymbol{h</p>
<p><strong>期望值</strong>（初始化阶段）：</p>
<p>\begin{equation}
\mathbb{E}[\text{logit}_i] = 0
\tag{66}
\end{equation}</p>
<p><strong>方差</strong>：</p>
<p>\begin{equation}
\text{Var}[\text{logit}_i] = d \sigma^2 = 768 \times 0.0176^2 \approx 0.238
\tag{67}
\end{equation}</p>
<p><strong>标准差</strong>：</p>
<p>\begin{equation}
\sqrt{0.238} \approx 0.488
\tag{68}
\end{equation}</p>
<p>这意味着logits分布在 $[-1, 1]$ 左右，Softmax后分布过于均匀。</p>
<h4 id="82-dense">8.2 添加Dense层的效果<a class="toc-link" href="#82-dense" title="Permanent link">&para;</a></h4>
<p>添加Dense层 $\boldsymbol{W}_{dense} \in \mathbb{R}^{d \times d}$ 和Layer Norm：</p>
<p>\begin{equation}
\boldsymbol{h}' = \text{Norm}(\boldsymbol{W}_{dense} \boldsymbol{h} + \boldsymbol{b})
\tag{69}
\end{equation}</p>
<p>然后投影到词表：</p>
<p>\begin{equation}
\text{logit}<em emb="emb">i = \boldsymbol{W}</em>'
\tag{70}
\end{equation}}[i, :] \cdot \boldsymbol{h</p>
<p><strong>好处</strong>：
1. Dense层的gamma参数可以独立学习缩放
2. 不影响编码器其他层的Layer Norm
3. 提供额外的表达能力</p>
<h4 id="83-gamma">8.3 Gamma参数的增长<a class="toc-link" href="#83-gamma" title="Permanent link">&para;</a></h4>
<p>在训练过程中，Dense层后的Layer Norm的gamma参数会增长到2-5倍，表示：</p>
<p>\begin{equation}
\text{logit}<em emb="emb">i = \gamma \cdot \tilde{\boldsymbol{W}}</em>'
\tag{71}
\end{equation}}[i, :] \cdot \tilde{\boldsymbol{h}</p>
<p>其中 $\tilde{\boldsymbol{h}}'$ 是归一化后的向量（方差为1），$\gamma &gt; 1$ 是放大系数。</p>
<p><strong>数值示例</strong>（BERT预训练检查点）：</p>
<p>\begin{equation}
\gamma_{encoder} \in [0.8, 1.2], \quad \gamma_{mlm_dense} \in [2, 5]
\tag{72}
\end{equation}</p>
<p>这验证了MLM Dense层确实承担了额外的缩放功能。</p>
<h3 id="9">9. 数值验证与实验<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 方差传播实验<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p>考虑12层Post-LN Transformer，每层FFN维度768：</p>
<p><strong>实验设置</strong>：
- 输入：随机向量 $\boldsymbol{x}_0 \sim \mathcal{N}(0, \boldsymbol{I})$
- 权重：$\boldsymbol{W} \sim \mathcal{N}(0, \sigma^2\boldsymbol{I})$</p>
<p><strong>测量</strong>：每层输出的方差 $\text{Var}[\boldsymbol{x}_\ell]$</p>
<table>
<thead>
<tr>
<th>层数</th>
<th>$\sigma=0.02$</th>
<th>$\sigma=0.0361$</th>
<th>$\sigma=0.05$</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td>3</td>
<td>0.999</td>
<td>0.997</td>
<td>0.993</td>
</tr>
<tr>
<td>6</td>
<td>0.998</td>
<td>0.992</td>
<td>0.981</td>
</tr>
<tr>
<td>9</td>
<td>0.997</td>
<td>0.985</td>
<td>0.966</td>
</tr>
<tr>
<td>12</td>
<td>0.996</td>
<td>0.978</td>
<td>0.948</td>
</tr>
</tbody>
</table>
<p>\begin{equation}
\text{理论预测：} \text{Var}[\boldsymbol{x}_{12}] = \frac{1}{(1+\sigma^2)^6}
\tag{73}
\end{equation}</p>
<h4 id="92">9.2 梯度范数实验<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p>测量不同层的梯度范数 $|\nabla_{\boldsymbol{W}_\ell} \mathcal{L}|$：</p>
<table>
<thead>
<tr>
<th>层数</th>
<th>$\sigma=0.02$</th>
<th>$\sigma=0.0361$</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.98</td>
<td>0.85</td>
</tr>
<tr>
<td>3</td>
<td>0.99</td>
<td>0.91</td>
</tr>
<tr>
<td>6</td>
<td>1.00</td>
<td>0.96</td>
</tr>
<tr>
<td>9</td>
<td>1.01</td>
<td>0.99</td>
</tr>
<tr>
<td>12</td>
<td>1.00</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<p>（归一化到最后一层为1.00）</p>
<p><strong>观察</strong>：$\sigma=0.02$ 时梯度更均匀，浅层梯度更强。</p>
<h4 id="93">9.3 收敛速度对比<a class="toc-link" href="#93" title="Permanent link">&para;</a></h4>
<p>固定其他超参数，仅改变初始化标准差：</p>
<table>
<thead>
<tr>
<th>$\sigma$</th>
<th>Warmup步数</th>
<th>收敛步数</th>
<th>最终Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.01</td>
<td>10000</td>
<td>120000</td>
<td>2.85</td>
</tr>
<tr>
<td>0.02</td>
<td>10000</td>
<td>100000</td>
<td>2.82</td>
</tr>
<tr>
<td>0.0361</td>
<td>10000</td>
<td>85000</td>
<td>2.83</td>
</tr>
<tr>
<td>0.05</td>
<td>10000</td>
<td>未收敛</td>
<td>3.20</td>
</tr>
</tbody>
</table>
<p>\begin{equation}
\text{最优范围：} \sigma \in [0.015, 0.025]
\tag{74}
\end{equation}</p>
<h3 id="10">10. 理论总结与实践建议<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 初始化标准差的选择原则<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p><strong>理论公式</strong>：</p>
<p>\begin{equation}
\sigma_{optimal} = \alpha \cdot \frac{1}{\sqrt{n_{in}}}
\tag{75}
\end{equation}</p>
<p>其中 $\alpha$ 是调整系数，取决于架构：</p>
<table>
<thead>
<tr>
<th>架构类型</th>
<th>$\alpha$ 值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pre-LN</td>
<td>1.0</td>
<td>标准Xavier初始化</td>
</tr>
<tr>
<td>Post-LN (浅)</td>
<td>0.7</td>
<td>轻度缩小，保持残差权重</td>
</tr>
<tr>
<td>Post-LN (深)</td>
<td>0.5</td>
<td>BERT的选择，强化恒等性</td>
</tr>
<tr>
<td>With LayerScale</td>
<td>1.0</td>
<td>LayerScale负责缩小</td>
</tr>
</tbody>
</table>
<h4 id="102">10.2 实践建议<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p><strong>1. 标准BERT/GPT（12-24层）</strong>：</p>
<p>\begin{equation}
\sigma = 0.02, \quad \text{截断范围：} [-2\sigma, 2\sigma]
\tag{76}
\end{equation}</p>
<p><strong>2. 深层模型（&gt;50层）</strong>：</p>
<p>\begin{equation}
\sigma = 0.01 \text{ 或使用 LayerScale with } \lambda = 10^{-4}
\tag{77}
\end{equation}</p>
<p><strong>3. Pre-LN架构</strong>：</p>
<p>\begin{equation}
\sigma = \frac{1}{\sqrt{d}}, \quad \text{无需额外缩小}
\tag{78}
\end{equation}</p>
<p><strong>4. Warmup步数</strong>：</p>
<p>\begin{equation}
T_{warmup} = \max\left(1000, 0.01 \times T_{total}\right)
\tag{79}
\end{equation}</p>
<h4 id="103">10.3 诊断工具<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p><strong>检查初始化是否合适</strong>：</p>
<ol>
<li><strong>方差检查</strong>：第一个epoch后，检查 $\text{Var}[\boldsymbol{x}_L] / \text{Var}[\boldsymbol{x}_0]$</li>
</ol>
<p>\begin{equation}
0.9 \leq \frac{\text{Var}[\boldsymbol{x}_L]}{\text{Var}[\boldsymbol{x}_0]} \leq 1.1
\tag{80}
\end{equation}</p>
<ol start="2">
<li><strong>梯度比检查</strong>：</li>
</ol>
<p>\begin{equation}
0.3 \leq \frac{|\nabla \boldsymbol{W}_0|}{|\nabla \boldsymbol{W}_L|} \leq 3.0
\tag{81}
\end{equation}</p>
<ol start="3">
<li><strong>Loss下降曲线</strong>：前1000步应该平稳下降，无剧烈波动</li>
</ol>
<p><strong>调整策略</strong>：
- 如果loss剧烈波动 → 减小 $\sigma$ 或增加warmup
- 如果浅层梯度过小 → 减小 $\sigma$
- 如果收敛过慢 → 略微增大 $\sigma$（但不超过 $1/\sqrt{d}$）</p>
<h3 id="11_1">11. 与其他技术的关联<a class="toc-link" href="#11_1" title="Permanent link">&para;</a></h3>
<h4 id="111">11.1 初始化与学习率的协同<a class="toc-link" href="#111" title="Permanent link">&para;</a></h4>
<p>最优学习率与初始化方差相关：</p>
<p>\begin{equation}
\eta_{optimal} \propto \frac{1}{\sigma \sqrt{L}}
\tag{82}
\end{equation}</p>
<p><strong>推导</strong>：更新步长应与初始信号强度相匹配。</p>
<h4 id="112-batch-size">11.2 初始化与Batch Size<a class="toc-link" href="#112-batch-size" title="Permanent link">&para;</a></h4>
<p>大batch训练需要更大的学习率，可以通过调整初始化补偿：</p>
<p>\begin{equation}
\sigma_{large_batch} = \sigma_{small_batch} \cdot \sqrt{\frac{B_{small}}{B_{large}}}
\tag{83}
\end{equation}</p>
<h4 id="113">11.3 混合精度训练的影响<a class="toc-link" href="#113" title="Permanent link">&para;</a></h4>
<p>FP16训练时，梯度下溢风险增加，建议：</p>
<p>\begin{equation}
\sigma_{fp16} \geq 0.015 \quad \text{（避免过小导致下溢）}
\tag{84}
\end{equation}</p>
<p><strong>总结</strong>：BERT的0.02初始化是经过深思熟虑的选择，它平衡了方差保持、梯度流、数值稳定性等多个因素，是Post-LN架构在12层深度下的近最优解。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="bert4keras在手baseline我有clue基准代码.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#150 bert4keras在手，baseline我有：CLUE基准代码</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="wgan新方案通过梯度归一化来实现l约束.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#152 WGAN新方案：通过梯度归一化来实现L约束</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#bert002">模型优化漫谈：BERT的初始标准差为什么是0.02？</a><ul>
<li><a href="#_1">梯度消失说的是什么意思？</a></li>
<li><a href="#ln">LN真的能缓解梯度消失？</a></li>
<li><a href="#ln_1">那我们为什么还要加LN？</a></li>
<li><a href="#_2">我们真的担心梯度消失吗？</a></li>
<li><a href="#warmup">Warmup是怎样起作用的？</a></li>
<li><a href="#002">初始标准差为什么是0.02？</a></li>
<li><a href="#mlmdense">为什么MLM要多加Dense？</a></li>
<li><a href="#_3">希望大家多多海涵批评斧正</a></li>
<li><a href="#_4">公式推导与注释</a><ul>
<li><a href="#1-xavier">1. Xavier初始化的完整推导</a></li>
<li><a href="#2-he">2. He初始化的推导</a></li>
<li><a href="#3-bert002">3. BERT初始化0.02的数学分析</a></li>
<li><a href="#4-post-ln-vs-pre-ln">4. Post-LN vs Pre-LN的方差传播</a></li>
<li><a href="#5-warmup">5. Warmup的数学必要性</a></li>
<li><a href="#6">6. 梯度流与信号传播分析</a></li>
<li><a href="#7-layer-scale">7. Layer Scale与初始化的关系</a></li>
<li><a href="#8-mlm-dense">8. MLM Dense层的数学解释</a></li>
<li><a href="#9">9. 数值验证与实验</a></li>
<li><a href="#10">10. 理论总结与实践建议</a></li>
<li><a href="#11_1">11. 与其他技术的关联</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>