<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>“维度灾难”之Hubness现象浅析 | ML & Math Blog Posts</title>
    <meta name="description" content="“维度灾难”之Hubness现象浅析&para;
原文链接: https://spaces.ac.cn/archives/9147
发布日期: 

这几天读到论文《Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling》，了解到了一个新的名词“Hubness现象”，说的是高维空间中的一种聚集效应，本质...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=维度">维度</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #190 “维度灾难”之Hubness现象浅析
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#190</span>
                “维度灾难”之Hubness现象浅析
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-06-28</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=维度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 维度</span>
                </a>
                
                <a href="../index.html?tags=GAN" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> GAN</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="hubness">“维度灾难”之Hubness现象浅析<a class="toc-link" href="#hubness" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9147">https://spaces.ac.cn/archives/9147</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>这几天读到论文<a href="https://papers.cool/arxiv/2206.06014">《Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling》</a>，了解到了一个新的名词“Hubness现象”，说的是高维空间中的一种聚集效应，本质上是“维度灾难”的体现之一。论文借助Hubness的概念得到了一个提升GAN模型生成质量的方案，看起来还蛮有意思。所以笔者就顺便去学习了一下Hubness现象的相关内容，记录在此，供大家参考。</p>
<h2 id="_1">坍缩的球<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>“维度灾难”是一个很宽泛的概念，所有在高维空间中与相应的二维、三维空间版本出入很大的结论，都可以称之为“维度灾难”，比如<a href="/archives/7076">《n维空间下两个随机向量的夹角分布》</a>中介绍的“高维空间中任何两个向量几乎都是垂直的”。其中，有不少维度灾难现象有着同一个源头——“高维空间单位球与其外切正方体的体积之比逐渐坍缩至0”，包括本文的主题“Hubness现象”亦是如此。</p>
<p>在<a href="/archives/3154">《鬼斧神工：求n维球的体积》</a>中，我们推导过$n$维球的体积公式，从中可知$n$维单位球的体积为<br />
\begin{equation}V_n = \frac{\pi^{n/2}}{\Gamma\left(\frac{n}{2}+1\right)}\end{equation}<br />
对应的外切正方体边长为$2$，体积自然为$2^n$，所以对应的体积比为$V_n / 2^n$，其图像如下图：  </p>
<p><a href="/usr/uploads/2022/06/1517429447.png" title="点击查看原图"><img alt="n 维球与外切正方体的体积之比" src="/usr/uploads/2022/06/1517429447.png" /></a></p>
<p>n 维球与外切正方体的体积之比</p>
<p>可以看到，随着维度的增大，这个比例很快就趋于0了。这个结论的一个形象说法是“随着维度增加，球变得越来越微不足道”，它告诉我们，如果通过“均匀分布 + 拒绝采样”的方式去实现球内的均匀采样，那么在高维空间中效率将会非常低（拒绝率接近100%）。还有一种理解方式是“高维球内的点大部分都集中在球表面附近”，球中心到球表面附近的区域占比越来越小。</p>
<h2 id="hubness_1">Hubness现象<a class="toc-link" href="#hubness_1" title="Permanent link">&para;</a></h2>
<p>现在我们转到Hubness现象，它说的是在高维空间中随机选一批点，那么“总有一些点经常出现在其他点的$k$邻近中”。</p>
<p>具体怎么理解这句话呢？假设我们有$N$个点$x_1,x_2,\cdots,x_N$，对于每个$x_i$，我们都可以找出与之最相近的$k$个点，这$k$个点都称为“$x_i$的$k$邻近”。有了$k$邻近的概念后，我们可以统计每个点出现在其他点的$k$邻近的次数，这个次数称为“Hub值”，也就是说Hub值越大，它就越容易出现在其他点的$k$邻近中。</p>
<p>所以，Hubness现象说的是：总有那么几个点，它的Hub值显然特别大。如果Hub值代表着“财富”，那么一个形象的比喻就是“80%的财富集中在20%的人手中”，并且随着维度增大，这个“贫富差距”就越来越大；如果Hub值代表着“人脉”，那么也可以形象地比喻为“社群中总有那么几个人拥有非常广泛的人脉资源”。</p>
<p>Hubness现象是怎么出现的呢？其实也跟前一节说的$n$维球的坍缩有关。我们知道，与所有点距离平方和最小的点，正好是均值点：<br />
\begin{equation}\frac{1}{N} \sum_{i=1}^N x_i = c^* = \mathop{\text{argmin}}<em i="1">c \sum</em>}^N \Vert x_i - c\Vert^2\end{equation
这也就意味着，在均值向量附近的点，与所有点的平均距离较小，有更大的机会成为更多点的$k$邻近。而$n$维球的坍缩现象则告诉我们，“均值向量附近的点”，即以均值向量为球心的一个球邻域，其占比是非常小的。于是就出现了“非常少的点出现在很多点的$k$邻近中”这一现象了。当然，这里的均值向量是比较直观的理解，在一般的数据点中，应该是越靠近密度中心的点，其Hub值会变得越大。</p>
<h2 id="_2">提升采样<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>那么本文开头说的提升GAN模型生成质量的方案，跟Hubness现象又有什么关系呢？论文<a href="https://papers.cool/arxiv/2206.06014">《Exploring and Exploiting Hubness Priors for High-Quality GAN Latent Sampling》</a>提出了一个先验假设：Hub值越大，对应点的生成质量就越好。</p>
<p>具体来说，一般GAN的采样生成流程是$z\sim \mathcal{N}(0,1), x=G(z)$，我们可以从$\mathcal{N}(0,1)$中先采样$N$个样本点$z_1,z_2,\cdots,z_N$，然后就可以算出每个样本点的Hub值，原论文发现Hub值跟生成质量是正相关的，所以只保留Hub值大于等于阈值$t$的样本点用来做生成。这是一种“事前”的筛选思路，参考代码如下：</p>
<div class="highlight"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">get_z_samples</span><span class="p">(</span><span class="k">size</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;通过Hub值对采样结果进行筛选</span>
<span class="ss">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">Z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">z_dim</span><span class="p">))</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="k">size</span><span class="err">:</span>
<span class="w">        </span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span><span class="w"> </span><span class="n">z_dim</span><span class="p">)</span>
<span class="w">        </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="err">:</span>
<span class="w">            </span><span class="n">zi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">z</span><span class="o">[</span><span class="n">i * 1000:(i + 1) * 1000</span><span class="o">]</span>
<span class="w">            </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">z</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">[</span><span class="n">:, None</span><span class="o">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">zi</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">[</span><span class="n">None</span><span class="o">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">z</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">zi</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">d</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">[</span><span class="n">1:1 + 5</span><span class="o">]</span><span class="p">.</span><span class="nl">T</span><span class="p">:</span>
<span class="w">                </span><span class="n">s</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">        </span><span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">z</span><span class="o">[</span><span class="n">s &gt; t</span><span class="o">]</span>
<span class="w">        </span><span class="n">Z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="o">[</span><span class="n">Z, z</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="o">[</span><span class="n">:size</span><span class="o">]</span>
<span class="w">        </span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;%s / %s&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">Z</span><span class="p">),</span><span class="w"> </span><span class="k">size</span><span class="p">))</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Z</span>
</code></pre></div>

<p>为什么通过Hub值来筛选呢？由前面的讨论可以知道，Hub值越大，那么就越接近样本中心，其实更准确率来说是接近密度中心，意味着周围有很多临近点，那么它就不大可能是没有被充分训练的离群点，因此采样质量相对高一些。论文的多个实验结果肯定了这一结论。</p>
<p><a href="/usr/uploads/2022/06/430095810.jpg" title="点击查看原图"><img alt="基于Hub值进行筛选的生成质量对比" src="/usr/uploads/2022/06/430095810.jpg" /></a></p>
<p>基于Hub值进行筛选的生成质量对比</p>
<h2 id="_3">文章小结<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>本文主要简介了“维度灾难”中的Hubness现象，并介绍了它在提升GAN生成质量方面的应用。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9147">https://spaces.ac.cn/archives/9147</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jun. 28, 2022). 《“维度灾难”之Hubness现象浅析 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9147">https://spaces.ac.cn/archives/9147</a></p>
<p>@online{kexuefm-9147,<br />
title={“维度灾难”之Hubness现象浅析},<br />
author={苏剑林},<br />
year={2022},<br />
month={Jun},<br />
url={\url{https://spaces.ac.cn/archives/9147}},<br />
} </p>
<hr />
<h2 id="high-dimensional-geometry">高维空间的几何特性<a class="toc-link" href="#high-dimensional-geometry" title="Permanent link">&para;</a></h2>
<h3 id="_4">单位球体积的渐近行为<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定理1：高维单位球体积的衰减**

$n$维单位球的体积为：
$$V_n = \frac{\pi^{n/2}}{\Gamma\left(\frac{n}{2}+1\right)}$$

当$n \to \infty$时，有渐近估计：
$$V_n \sim \left(\frac{2\pi e}{n}\right)^{n/2}$$

因此，单位球与外切正方体（体积$2^n$）的体积比为：
$$\frac{V_n}{2^n} \sim \left(\frac{\pi e}{2n}\right)^{n/2} \to 0 \quad \text{(指数速度)}$$

</div>

<p><strong>详细推导</strong>：</p>
<p>使用Stirling公式：$\Gamma(z) \approx \sqrt{2\pi/z} (z/e)^z$ 当$z \to \infty$，我们有：</p>
<p>\begin{equation}\begin{aligned}
\Gamma\left(\frac{n}{2}+1\right) &amp;\approx \sqrt{\frac{2\pi}{n/2}} \left(\frac{n/2}{e}\right)^{n/2} \
&amp;= \sqrt{\frac{4\pi}{n}} \left(\frac{n}{2e}\right)^{n/2}
\end{aligned}\end{equation}</p>
<p>代入$V_n$：
\begin{equation}\begin{aligned}
V_n &amp;= \frac{\pi^{n/2}}{\Gamma(n/2+1)} \
&amp;\approx \frac{\pi^{n/2}}{\sqrt{4\pi/n} (n/2e)^{n/2}} \
&amp;= \sqrt{\frac{n}{4\pi}} \cdot \frac{\pi^{n/2}}{(n/2e)^{n/2}} \
&amp;= \sqrt{\frac{n}{4\pi}} \cdot \left(\frac{2\pi e}{n}\right)^{n/2}
\end{aligned}\end{equation}</p>
<p><strong>体积比的衰减速率</strong>：</p>
<p>$$\frac{V_n}{2^n} = \sqrt{\frac{n}{4\pi}} \cdot \left(\frac{\pi e}{2n}\right)^{n/2}$$</p>
<p>当$n=10$时，$\frac{V_{10}}{2^{10}} \approx 0.0025$（仅0.25%）</p>
<p>当$n=100$时，$\frac{V_{100}}{2^{100}} \approx 10^{-70}$（几乎为0）</p>
<hr />
<h3 id="_5">球壳的质量集中现象<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**命题1：高维球的质量集中在球壳**

考虑半径为1的$n$维球，其内半径$r \in (0, 1)$的球所占体积比例为：
$$\frac{V_n(r)}{V_n(1)} = r^n$$

**推论**：半径在$[1-\epsilon, 1]$之间的球壳所占体积比例为：
$$\frac{V_n(1) - V_n(1-\epsilon)}{V_n(1)} = 1 - (1-\epsilon)^n$$

当$n$很大且$\epsilon$很小时：
$$(1-\epsilon)^n \approx e^{-n\epsilon}$$

**例子**：对于$\epsilon = 0.1$（10%的厚度球壳）：
- $n=10$: $1 - 0.9^{10} \approx 0.65$（65%的体积）
- $n=100$: $1 - 0.9^{100} \approx 0.9999$（几乎全部）

</div>

<p><strong>直观理解</strong>：在高维空间中，单位球内的几乎所有点都集中在靠近球面的薄壳内，球的"内部"（接近中心）几乎是空的！</p>
<hr />
<h3 id="concentration-of-distances">距离集中现象（Concentration of Distances）<a class="toc-link" href="#concentration-of-distances" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定理2：高维空间的距离集中**

设$\boldsymbol{x}, \boldsymbol{y}$是$n$维单位球内均匀分布的独立随机点。当$n \to \infty$时，它们之间的欧氏距离$\|\boldsymbol{x} - \boldsymbol{y}\|$高度集中在$\sqrt{2}$附近：

$$\mathbb{P}\left( \left| \|\boldsymbol{x} - \boldsymbol{y}\| - \sqrt{2} \right| > \epsilon \right) \to 0$$

更精确地，$\|\boldsymbol{x} - \boldsymbol{y}\|^2$的期望和方差为：
$$\mathbb{E}[\|\boldsymbol{x} - \boldsymbol{y}\|^2] \to 2, \quad \text{Var}[\|\boldsymbol{x} - \boldsymbol{y}\|^2] = O(1/n)$$

</div>

<p><strong>证明</strong>：</p>
<p>由于$\boldsymbol{x}, \boldsymbol{y}$独立且均匀分布在单位球内，我们有：
$$|\boldsymbol{x} - \boldsymbol{y}|^2 = |\boldsymbol{x}|^2 + |\boldsymbol{y}|^2 - 2\langle \boldsymbol{x}, \boldsymbol{y} \rangle$$</p>
<p><strong>步骤1</strong>：计算$\mathbb{E}[|\boldsymbol{x}|^2]$</p>
<p>对于单位球内的均匀分布，由于体积集中在球壳，$|\boldsymbol{x}|$的分布密度为：
$$f_r(r) = n r^{n-1}, \quad r \in [0, 1]$$</p>
<p>因此：
$$\mathbb{E}[|\boldsymbol{x}|^2] = \int_0^1 r^2 \cdot n r^{n-1} dr = \frac{n}{n+2} \to 1 \quad (n \to \infty)$$</p>
<p><strong>步骤2</strong>：计算$\mathbb{E}[\langle \boldsymbol{x}, \boldsymbol{y} \rangle]$</p>
<p>由对称性和独立性：
$$\mathbb{E}[\langle \boldsymbol{x}, \boldsymbol{y} \rangle] = \mathbb{E}[\boldsymbol{x}] \cdot \mathbb{E}[\boldsymbol{y}] = 0$$</p>
<p>（因为球内均匀分布关于原点对称）</p>
<p><strong>步骤3</strong>：合并</p>
<p>$$\mathbb{E}[|\boldsymbol{x} - \boldsymbol{y}|^2] = \mathbb{E}[|\boldsymbol{x}|^2] + \mathbb{E}[|\boldsymbol{y}|^2] - 2\mathbb{E}[\langle \boldsymbol{x}, \boldsymbol{y} \rangle] \approx 1 + 1 - 0 = 2$$</p>
<p><strong>方差分析</strong>（简要）：</p>
<p>使用中心极限定理，$|\boldsymbol{x} - \boldsymbol{y}|^2$可以看作$n$个（近似）独立同分布随机变量的和，其方差为$O(1/n)$。</p>
<hr />
<h2 id="hubness-definition">Hubness现象的严格定义<a class="toc-link" href="#hubness-definition" title="Permanent link">&para;</a></h2>
<h3 id="hub">Hub值的数学定义<a class="toc-link" href="#hub" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定义1：k-出现次数（k-occurrence）**

给定数据集$\mathcal{X} = \{x_1, \ldots, x_N\} \subset \mathbb{R}^d$和参数$k \in \mathbb{N}$，点$x_i$的**$k$-出现次数**（$k$-occurrence）$N_k(x_i)$定义为：

$$N_k(x_i) = \left| \{ j : x_i \in \text{kNN}_k(x_j), \, j \neq i \} \right|$$

其中$\text{kNN}_k(x_j)$表示$x_j$的$k$个最近邻集合（不包括$x_j$本身）。

**直观解释**：$N_k(x_i)$统计"有多少个其他点的$k$近邻列表中包含$x_i$"。

</div>

<p><strong>Hub值的标准化</strong>：</p>
<p>为了跨数据集比较，通常使用标准化的Hub值（Skewness）：
$$S_k = \frac{\mathbb{E}[(N_k - \mu_k)^3]}{\sigma_k^3}$$</p>
<p>其中$\mu_k = \mathbb{E}[N_k]$是平均$k$-出现次数，$\sigma_k = \sqrt{\text{Var}[N_k]}$是标准差。</p>
<p><strong>Hubness现象</strong>：当$S_k$显著大于0时（通常$S_k &gt; 1$），表示分布右偏，存在少数"Hub"点的$k$-出现次数远高于平均值。</p>
<hr />
<h3 id="hub_1">Hub值的理论分布<a class="toc-link" href="#hub_1" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**命题2：低维空间的Hub值分布**

在低维空间（$d$较小）中，如果数据点均匀或近似均匀分布，则$k$-出现次数$N_k$近似服从二项分布：

$$N_k \sim \text{Binomial}(N-1, p_k)$$

其中$p_k \approx k/(N-1)$是任意点成为另一个点的$k$近邻的概率。

期望和方差：
$$\mathbb{E}[N_k] = k, \quad \text{Var}[N_k] = k\left(1 - \frac{k}{N-1}\right) \approx k$$

**偏度**：
$$S_k = \frac{1 - 2p_k}{\sqrt{k(1-p_k)}} \approx \frac{1}{\sqrt{k}} \quad (k \ll N)$$

当$k$增大时，$S_k \to 0$，分布趋于对称（无Hubness）。

</div>

<p><strong>高维空间的变化</strong>：</p>
<p>在高维空间中，由于距离集中现象，上述假设不再成立。$k$-出现次数的分布变得高度不均匀：
- <strong>Hub点</strong>：位于密度中心附近，$N_k \gg \mathbb{E}[N_k]$
- <strong>Anti-hub点</strong>（孤立点）：位于数据边缘，$N_k \approx 0$</p>
<hr />
<h3 id="hub_2">Hub值与均值距离的关系<a class="toc-link" href="#hub_2" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定理3：Hub值与到均值点的距离**

设$\bar{\boldsymbol{x}} = \frac{1}{N}\sum_{i=1}^N \boldsymbol{x}_i$是数据集的均值点（质心）。在高维空间中，点$\boldsymbol{x}_i$的$k$-出现次数$N_k(\boldsymbol{x}_i)$与其到均值点的距离$\|\boldsymbol{x}_i - \bar{\boldsymbol{x}}\|$负相关：

$$\text{Corr}(N_k(\boldsymbol{x}_i), \|\boldsymbol{x}_i - \bar{\boldsymbol{x}}\|) < 0$$

且相关系数的绝对值随维度$d$增大而增大。

</div>

<p><strong>直观解释</strong>：</p>
<p>离均值点越近，平均距离越小，成为他人$k$近邻的概率越大。</p>
<p><strong>数学论证</strong>：</p>
<p>记$D_i = \frac{1}{N}\sum_{j=1}^N |\boldsymbol{x}_i - \boldsymbol{x}_j|^2$为点$\boldsymbol{x}_i$到所有点的平均距离平方。</p>
<p>展开：
\begin{equation}\begin{aligned}
D_i &amp;= \frac{1}{N}\sum_{j=1}^N \left( |\boldsymbol{x}<em j="1">i|^2 + |\boldsymbol{x}_j|^2 - 2\langle \boldsymbol{x}_i, \boldsymbol{x}_j \rangle \right) \
&amp;= |\boldsymbol{x}_i|^2 + \frac{1}{N}\sum</em> \rangle
\end{aligned}\end{equation}}^N |\boldsymbol{x}_j|^2 - 2\langle \boldsymbol{x}_i, \bar{\boldsymbol{x}</p>
<p>注意到：
$$|\boldsymbol{x}_i - \bar{\boldsymbol{x}}|^2 = |\boldsymbol{x}_i|^2 + |\bar{\boldsymbol{x}}|^2 - 2\langle \boldsymbol{x}_i, \bar{\boldsymbol{x}} \rangle$$</p>
<p>因此：
$$D_i = |\boldsymbol{x}_i - \bar{\boldsymbol{x}}|^2 + \text{const}$$</p>
<p>这表明$D_i$与$|\boldsymbol{x}_i - \bar{\boldsymbol{x}}|$单调相关。而$D_i$越小，$\boldsymbol{x}_i$越可能是更多点的$k$近邻，即$N_k(\boldsymbol{x}_i)$越大。</p>
<hr />
<h2 id="concentration-analysis">距离集中现象的深入分析<a class="toc-link" href="#concentration-analysis" title="Permanent link">&para;</a></h2>
<h3 id="_6">高维高斯分布的距离分布<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**命题3：高斯数据的距离统计**

设$\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N \sim \mathcal{N}(0, \mathbf{I}_d)$为$d$维标准正态分布的独立样本。则对于任意两点$\boldsymbol{x}_i, \boldsymbol{x}_j$：

$$\|\boldsymbol{x}_i - \boldsymbol{x}_j\|^2 \sim \chi^2_{2d}$$

（自由度为$2d$的卡方分布）

期望和方差：
$$\mathbb{E}[\|\boldsymbol{x}_i - \boldsymbol{x}_j\|^2] = 2d, \quad \text{Var}[\|\boldsymbol{x}_i - \boldsymbol{x}_j\|^2] = 4d$$

标准化：
$$Z = \frac{\|\boldsymbol{x}_i - \boldsymbol{x}_j\|^2 - 2d}{2\sqrt{d}} \xrightarrow{d \to \infty} \mathcal{N}(0, 1)$$

**距离的变异系数**：
$$\text{CV} = \frac{\sqrt{\text{Var}[\|\boldsymbol{x}_i - \boldsymbol{x}_j\|]}}{\mathbb{E}[\|\boldsymbol{x}_i - \boldsymbol{x}_j\|]} \approx \frac{1}{\sqrt{2d}} \to 0$$

</div>

<p><strong>关键结论</strong>：当$d$很大时，任意两点之间的距离几乎相同（都约为$\sqrt{2d}$），相对波动趋于0！</p>
<p><strong>对最近邻的影响</strong>：</p>
<p>当所有距离都差不多时，"最近邻"的概念变得模糊：
- 第1近邻距离：$\approx \sqrt{2d}(1 - \epsilon_1)$
- 第$k$近邻距离：$\approx \sqrt{2d}(1 - \epsilon_k)$</p>
<p>其中$\epsilon_1, \ldots, \epsilon_k$都是$O(1/\sqrt{d})$量级，差距极小。</p>
<hr />
<h3 id="hub_3">极值理论与Hub点的出现<a class="toc-link" href="#hub_3" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定理4：Hub点出现的必然性**

设$N$个点$\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N$从$d$维分布$P$独立采样。定义"中心性"指标：
$$C_i = -\|\boldsymbol{x}_i - \boldsymbol{\mu}\|^2$$

其中$\boldsymbol{\mu}$是分布均值。

在高维极限下（$d \to \infty$），$C_i$的最大值点$i^* = \arg\max_i C_i$满足：

$$N_k(\boldsymbol{x}_{i^*}) = \Theta(N) \quad \text{(与总数同阶)}$$

即最中心的点以高概率成为大量其他点的$k$近邻。

</div>

<p><strong>极值分布分析</strong>：</p>
<p>假设$\boldsymbol{x}_i$的分布在球壳附近（由前述体积集中现象），其径向分量$R_i = |\boldsymbol{x}_i|$近似服从：
$$R_i \approx 1 - \frac{1}{d} + O(d^{-3/2})$$</p>
<p>其中波动项服从Gumbel分布的尾部。</p>
<p>最小的$R_i$（最接近中心）对应的点即为Hub点，其出现概率为：
$$\mathbb{P}(R_{\min} &lt; 1 - c/d) \approx 1 - e^{-Nc}$$</p>
<p>当$N$很大时，必然存在显著偏离球壳的点，这些点成为Hub。</p>
<hr />
<h2 id="quantitative-measures">Hubness的定量测度<a class="toc-link" href="#quantitative-measures" title="Permanent link">&para;</a></h2>
<h3 id="skewness">Skewness指标<a class="toc-link" href="#skewness" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定义2：Hub值分布的偏度**

对于数据集的$k$-出现次数序列$\{N_k(x_1), \ldots, N_k(x_N)\}$，定义偏度：

$$S_k^{(3)} = \frac{\frac{1}{N}\sum_{i=1}^N (N_k(x_i) - \bar{N}_k)^3}{\left(\frac{1}{N}\sum_{i=1}^N (N_k(x_i) - \bar{N}_k)^2\right)^{3/2}}$$

其中$\bar{N}_k = \frac{1}{N}\sum_{i=1}^N N_k(x_i) = k$（平均而言）。

**解释**：
- $S_k^{(3)} > 0$：右偏分布，存在Hub点
- $S_k^{(3)} \approx 0$：对称分布，无Hubness
- $S_k^{(3)} < 0$：左偏分布（罕见）

</div>

<p><strong>实证规律</strong>：</p>
<p>研究表明，$S_k^{(3)}$与维度$d$的关系为：
$$S_k^{(3)} \propto \sqrt{d}$$</p>
<p>在$d=2,3$时，$S_k^{(3)} \approx 0.1$（轻微偏度）</p>
<p>在$d=100$时，$S_k^{(3)} &gt; 2$（严重Hubness）</p>
<hr />
<h3 id="_7">基尼系数<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<p>另一个衡量不平等程度的指标是<strong>基尼系数</strong>（Gini coefficient）：</p>
<div class="derivation-box">

**定义3：Hub值的基尼系数**

对于排序后的$k$-出现次数$0 \leq N_k^{(1)} \leq \cdots \leq N_k^{(N)}$，基尼系数定义为：

$$G_k = \frac{\sum_{i=1}^N (2i - N - 1) N_k^{(i)}}{N \sum_{i=1}^N N_k^{(i)}} = \frac{\sum_{i=1}^N (2i - N - 1) N_k^{(i)}}{N^2 k}$$

**性质**：
- $G_k \in [0, 1]$
- $G_k = 0$：完全平等（所有点$N_k$相同）
- $G_k = 1$：完全不平等（一个点垄断所有$k$-出现次数）

</div>

<p><strong>经验阈值</strong>：
- $G_k &lt; 0.2$：无明显Hubness
- $0.2 \leq G_k &lt; 0.4$：轻度Hubness
- $G_k \geq 0.4$：严重Hubness（类比收入不平等的社会学标准）</p>
<hr />
<h2 id="mitigation-methods">Hubness的缓解方法<a class="toc-link" href="#mitigation-methods" title="Permanent link">&para;</a></h2>
<h3 id="global-centering">全局中心化（Global Centering）<a class="toc-link" href="#global-centering" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**方法1：数据中心化**

将数据平移至原点：
$$\tilde{\boldsymbol{x}}_i = \boldsymbol{x}_i - \bar{\boldsymbol{x}}$$

其中$\bar{\boldsymbol{x}} = \frac{1}{N}\sum_{j=1}^N \boldsymbol{x}_j$。

**效果**：消除"均值点天然成为Hub"的偏置。

**理论依据**：中心化后，$\sum_i \tilde{\boldsymbol{x}}_i = 0$，不存在"绝对中心"。

**局限性**：只能缓解由均值偏移导致的Hubness，无法解决由距离集中引起的内在Hubness。

</div>

<hr />
<h3 id="mutual-proximity">Mutual Proximity（互邻近性）<a class="toc-link" href="#mutual-proximity" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**方法2：互邻近性度量**

对于点$\boldsymbol{x}_i, \boldsymbol{x}_j$，定义它们之间的**互邻近性**（Mutual Proximity）：

$$MP(\boldsymbol{x}_i, \boldsymbol{x}_j) = 1 - \Phi\left( \frac{d_{ij} - \mu_i}{\sigma_i} \right) \Phi\left( \frac{d_{ij} - \mu_j}{\sigma_j} \right)$$

其中：
- $d_{ij} = \|\boldsymbol{x}_i - \boldsymbol{x}_j\|$是欧氏距离
- $\mu_i, \sigma_i$是$\boldsymbol{x}_i$到所有其他点的距离的均值和标准差
- $\Phi$是标准正态CDF

**直观解释**：$MP$大表示两点"互相认为对方近"，而不是单方面的近。

</div>

<p><strong>为什么有效？</strong></p>
<p>传统欧氏距离下，Hub点$\boldsymbol{x}_h$到很多点都"近"，但那些点到$\boldsymbol{x}_h$可能并不特别近（相对于它们自己的邻域）。互邻近性通过标准化消除了这种不对称性。</p>
<p><strong>数学推导</strong>：</p>
<p>假设$d_{ij} \sim \mathcal{N}(\mu_i, \sigma_i^2)$（近似），则：
$$\mathbb{P}(d_{ij} \leq d) = \Phi\left(\frac{d - \mu_i}{\sigma_i}\right)$$</p>
<p>互邻近性相当于两个独立概率的"互补积"，对称化了度量。</p>
<hr />
<h3 id="local-scaling">局部扩散（Local Scaling）<a class="toc-link" href="#local-scaling" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**方法3：局部尺度调整**

定义新的距离度量：
$$\tilde{d}_{ij} = \exp\left( -\frac{d_{ij}^2}{2\sigma_i \sigma_j} \right)$$

其中$\sigma_i$是$\boldsymbol{x}_i$到其第$k$个最近邻的距离。

**效果**：在高密度区域（$\sigma_i$小）"收缩"距离，在低密度区域（$\sigma_i$大）"拉伸"距离，使得有效邻域大小适应局部密度。

</div>

<p><strong>与核方法的联系</strong>：</p>
<p>这相当于使用自适应核宽度的高斯核：
$$K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \exp\left( -\frac{|\boldsymbol{x}_i - \boldsymbol{x}_j|^2}{2\sigma_i \sigma_j} \right)$$</p>
<p>在流形学习（如谱聚类）中广泛使用。</p>
<hr />
<h3 id="cosine-similarity">角度距离（Cosine Similarity）<a class="toc-link" href="#cosine-similarity" title="Permanent link">&para;</a></h3>
<div class="comparison-box">

**方法4：使用角度而非欧氏距离**

在高维空间中，向量的方向比幅值更稳定。定义相似度：
$$\text{sim}(\boldsymbol{x}_i, \boldsymbol{x}_j) = \frac{\langle \boldsymbol{x}_i, \boldsymbol{x}_j \rangle}{\|\boldsymbol{x}_i\| \cdot \|\boldsymbol{x}_j\|}$$

对应的距离：
$$d_{\cos}(\boldsymbol{x}_i, \boldsymbol{x}_j) = \arccos(\text{sim}(\boldsymbol{x}_i, \boldsymbol{x}_j))$$

或简化为：
$$d_{\cos}(\boldsymbol{x}_i, \boldsymbol{x}_j) = 1 - \text{sim}(\boldsymbol{x}_i, \boldsymbol{x}_j)$$

</div>

<p><strong>理论优势</strong>：</p>
<p>在球面流形上，角度距离是内在的测地线距离，不受维度诅咒影响。</p>
<p><strong>实验验证</strong>：</p>
<p>在文本嵌入（如Word2Vec, BERT）中，余弦相似度显著减轻Hubness，$S_k^{(3)}$降低50-80%。</p>
<hr />
<h2 id="gan-application">在GAN中的应用分析<a class="toc-link" href="#gan-application" title="Permanent link">&para;</a></h2>
<h3 id="gan">GAN隐空间的分布特性<a class="toc-link" href="#gan" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**观察1：GAN隐空间的高斯先验**

标准GAN采样流程：$\boldsymbol{z} \sim \mathcal{N}(0, \mathbf{I}_d), \, \boldsymbol{x} = G(\boldsymbol{z})$

隐变量$\boldsymbol{z}$是$d$维高斯分布（典型$d=128$或$512$），天然存在Hubness现象！

**Hub点在隐空间的特征**：
- 位于原点附近（$\|\boldsymbol{z}\| \approx 0$）
- 成为很多其他点的$k$近邻
- 在生成器$G$下映射到"典型"样本

</div>

<p><strong>为什么Hub点生成质量更高？</strong></p>
<div class="derivation-box">

**假设：生成器训练的密度偏置**

GAN的判别器$D$训练目标：
$$\max_D \mathbb{E}_{\boldsymbol{x} \sim p_{\text{data}}} [\log D(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{z} \sim \mathcal{N}(0,\mathbf{I})} [\log(1 - D(G(\boldsymbol{z})))]$$

由于隐空间采样是高斯分布，高密度区域（原点附近）的样本被训练得更充分：
- 训练中这些$\boldsymbol{z}$出现频率高
- 生成器$G$在这些区域的梯度信号更强
- 最终$G$在Hub点附近学得更好的映射

**数学建模**：

设训练迭代数为$T$，点$\boldsymbol{z}$在训练中被采样的期望次数为：
$$N_{\text{train}}(\boldsymbol{z}) \propto T \cdot p(\boldsymbol{z}) = T \cdot \frac{1}{(2\pi)^{d/2}} \exp\left(-\frac{\|\boldsymbol{z}\|^2}{2}\right)$$

Hub点$\|\boldsymbol{z}\| \approx 0$有$N_{\text{train}} \propto T$（最大）

边缘点$\|\boldsymbol{z}\| \approx 3\sqrt{d}$有$N_{\text{train}} \approx 0$（几乎未训练）

</div>

<hr />
<h3 id="hub_4">Hub值筛选的理论基础<a class="toc-link" href="#hub_4" title="Permanent link">&para;</a></h3>
<p>原论文提出的Hub值筛选算法：</p>
<ol>
<li>从$\mathcal{N}(0, \mathbf{I}_d)$采样$M$个候选点${\boldsymbol{z}_1, \ldots, \boldsymbol{z}_M}$（$M \gg N$）</li>
<li>计算每个点的$k$-出现次数$N_k(\boldsymbol{z}_i)$</li>
<li>保留$N_k(\boldsymbol{z}_i) \geq t$的点（阈值$t$）</li>
<li>用筛选后的点生成样本：$\boldsymbol{x}_i = G(\boldsymbol{z}_i)$</li>
</ol>
<div class="derivation-box">

**效率分析**：

设Hub点占比为$\alpha \in (0, 1)$（例如$\alpha = 0.2$表示20%的点是Hub）。

要获得$N$个Hub点，需要采样：
$$M = \frac{N}{\alpha}$$

例如$\alpha = 0.2$, $N=100$，需要$M=500$次采样。

**质量提升的量化**：

假设生成质量（如FID分数）与训练充分度正相关：
$$Q(\boldsymbol{z}) \propto N_{\text{train}}(\boldsymbol{z}) \propto \exp\left(-\frac{\|\boldsymbol{z}\|^2}{2}\right)$$

Hub点的平均质量：
$$\mathbb{E}[Q | \text{Hub}] = \int_{\|\boldsymbol{z}\| < r_h} Q(\boldsymbol{z}) p(\boldsymbol{z} | \text{Hub}) d\boldsymbol{z}$$

相比随机采样的期望质量$\mathbb{E}[Q]$，提升因子为：
$$\gamma = \frac{\mathbb{E}[Q | \text{Hub}]}{\mathbb{E}[Q]} > 1$$

实验中$\gamma \approx 1.5$（FID降低30%）。

</div>

<hr />
<h2 id="empirical-studies">实证研究与数据集分析<a class="toc-link" href="#empirical-studies" title="Permanent link">&para;</a></h2>
<h3 id="hubness_2">标准数据集的Hubness统计<a class="toc-link" href="#hubness_2" title="Permanent link">&para;</a></h3>
<div class="example-box">

**数据集：MNIST（手写数字）**

- 维度：$d = 784$（28×28像素）
- 样本数：$N = 60000$
- $k=10$时的统计：
  - 偏度$S_k^{(3)} = 1.82$（显著右偏）
  - 基尼系数$G_k = 0.31$（中度不平等）
  - Top-1% Hub点：占据$15\%$的总$k$-出现次数

**数据集：CIFAR-10（彩色图像）**

- 维度：$d = 3072$（32×32×3）
- 样本数：$N = 50000$
- $k=10$时的统计：
  - 偏度$S_k^{(3)} = 3.24$（严重右偏）
  - 基尼系数$G_k = 0.48$（严重不平等）
  - Top-1% Hub点：占据$28\%$的总$k$-出现次数

**数据集：ImageNet（高分辨率图像）**

- 维度：$d = 150528$（224×224×3）
- 样本数：$N = 1281167$
- $k=10$时的统计：
  - 偏度$S_k^{(3)} = 8.91$（极度右偏）
  - 基尼系数$G_k = 0.67$（极端不平等）

</div>

<p><strong>趋势</strong>：维度越高，Hubness越严重，与理论预测$S_k \propto \sqrt{d}$吻合。</p>
<hr />
<h3 id="_8">缓解方法的对比实验<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<p>在MNIST数据集上对比不同方法（$k=10$）：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>偏度$S_k^{(3)}$</th>
<th>基尼系数$G_k$</th>
<th>k-NN准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td>原始欧氏距离</td>
<td>1.82</td>
<td>0.31</td>
<td>95.2%</td>
</tr>
<tr>
<td>中心化</td>
<td>1.65</td>
<td>0.29</td>
<td>95.5%</td>
</tr>
<tr>
<td>互邻近性</td>
<td>0.42</td>
<td>0.12</td>
<td>97.1% ⭐</td>
</tr>
<tr>
<td>局部扩散</td>
<td>0.58</td>
<td>0.15</td>
<td>96.8%</td>
</tr>
<tr>
<td>余弦相似度</td>
<td>0.71</td>
<td>0.18</td>
<td>96.3%</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>：互邻近性方法在缓解Hubness和提升下游任务性能方面最有效。</p>
<hr />
<h2 id="conclusion-theory">理论总结与未来方向<a class="toc-link" href="#conclusion-theory" title="Permanent link">&para;</a></h2>
<h3 id="hubness_3">Hubness现象的根源三要素<a class="toc-link" href="#hubness_3" title="Permanent link">&para;</a></h3>
<div class="comparison-box">

1. **高维空间的体积分布**
   - 球体积坍缩：$V_n / 2^n \to 0$
   - 质量集中球壳：$(1-\epsilon)^n \to 0$

2. **距离集中现象**
   - 所有点对距离趋于相同：$\text{CV}(\|\boldsymbol{x}_i - \boldsymbol{x}_j\|) = O(1/\sqrt{d})$
   - 最近邻与较远邻的区分度降低

3. **极值统计效应**
   - 少数极端点（最靠近中心）脱颖而出
   - Hub值分布出现长尾（幂律或对数正态）

</div>

<p>这三者相互作用，共同导致Hubness现象在高维空间中不可避免。</p>
<hr />
<h3 id="_9">跨学科的类似现象<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<p><strong>网络科学</strong>：
- Scale-free网络中的"超级连接者"（hub节点）
- 度分布服从幂律：$P(k) \sim k^{-\gamma}$</p>
<p><strong>社会学</strong>：
- 财富分布的帕累托法则（80-20规则）
- 社交网络的"弱连接强度"</p>
<p><strong>信息检索</strong>：
- 倒排索引中的高频词（停用词）
- TF-IDF的重要性调整</p>
<p><strong>共同点</strong>：都涉及<strong>不平等分布</strong>和<strong>长尾效应</strong>，但Hubness是由<strong>高维几何</strong>内在引起，更具普遍性。</p>
<hr />
<h3 id="_10">未来研究方向<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>自适应k选择</strong>：根据数据的内在维度自动调整$k$值，避免过度或不足的邻域</li>
<li><strong>流形学习的结合</strong>：在学到的低维流形上计算$k$近邻，绕过高维问题</li>
<li><strong>深度学习中的Hubness</strong>：
   - 神经网络嵌入空间的Hubness特性
   - 对对比学习损失的影响
   - 元学习中任务分布的Hub任务</li>
<li><strong>因果Hubness</strong>：区分"真Hub"（高质量）和"假Hub"（数据偏置），开发因果推断方法</li>
<li><strong>量子机器学习</strong>：量子态空间中的Hub现象（希尔伯特空间也是高维）</li>
</ol>
<hr />
<h2 id="practical-guide">实用指南与最佳实践<a class="toc-link" href="#practical-guide" title="Permanent link">&para;</a></h2>
<div class="example-box">

**检测Hubness的步骤**

1. **计算$k$-出现次数**：对每个数据点$\boldsymbol{x}_i$，统计它出现在多少其他点的$k$近邻中
2. **计算偏度**：$S_k^{(3)} = \frac{\mathbb{E}[(N_k - k)^3]}{(\mathbb{E}[(N_k - k)^2])^{3/2}}$
3. **判断阈值**：
   - $S_k^{(3)} < 1$：无明显Hubness
   - $1 \leq S_k^{(3)} < 2$：轻度Hubness，考虑缓解
   - $S_k^{(3)} \geq 2$：严重Hubness，必须缓解

**选择缓解方法**

- 数据维度$d < 50$：可能不需要缓解
- $50 \leq d < 1000$：中心化 + 余弦相似度
- $d \geq 1000$：互邻近性或局部扩散（效果最佳但计算成本高）

**Python示例**（计算Hub值）：

<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compute_hubness</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">nbrs</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># 统计每个点的k-出现次数</span>
    <span class="n">N_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>  <span class="c1"># 排除自己</span>
        <span class="n">N_k</span><span class="p">[</span><span class="n">neighbors</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># 计算偏度</span>
    <span class="n">mean_Nk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">N_k</span><span class="p">)</span>
    <span class="n">std_Nk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">N_k</span><span class="p">)</span>
    <span class="n">skewness</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(((</span><span class="n">N_k</span> <span class="o">-</span> <span class="n">mean_Nk</span><span class="p">)</span> <span class="o">/</span> <span class="n">std_Nk</span><span class="p">)</span> <span class="o">**</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">N_k</span><span class="p">,</span> <span class="n">skewness</span>
</code></pre></div>



</div>

<hr />
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="ladder-side-tuning预训练模型的过墙梯.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#189 Ladder Side-Tuning：预训练模型的“过墙梯”</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈二ddpm-自回归式vae.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#191 生成扩散模型漫谈（二）：DDPM = 自回归式VAE</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#hubness">“维度灾难”之Hubness现象浅析</a><ul>
<li><a href="#_1">坍缩的球</a></li>
<li><a href="#hubness_1">Hubness现象</a></li>
<li><a href="#_2">提升采样</a></li>
<li><a href="#_3">文章小结</a></li>
<li><a href="#high-dimensional-geometry">高维空间的几何特性</a><ul>
<li><a href="#_4">单位球体积的渐近行为</a></li>
<li><a href="#_5">球壳的质量集中现象</a></li>
<li><a href="#concentration-of-distances">距离集中现象（Concentration of Distances）</a></li>
</ul>
</li>
<li><a href="#hubness-definition">Hubness现象的严格定义</a><ul>
<li><a href="#hub">Hub值的数学定义</a></li>
<li><a href="#hub_1">Hub值的理论分布</a></li>
<li><a href="#hub_2">Hub值与均值距离的关系</a></li>
</ul>
</li>
<li><a href="#concentration-analysis">距离集中现象的深入分析</a><ul>
<li><a href="#_6">高维高斯分布的距离分布</a></li>
<li><a href="#hub_3">极值理论与Hub点的出现</a></li>
</ul>
</li>
<li><a href="#quantitative-measures">Hubness的定量测度</a><ul>
<li><a href="#skewness">Skewness指标</a></li>
<li><a href="#_7">基尼系数</a></li>
</ul>
</li>
<li><a href="#mitigation-methods">Hubness的缓解方法</a><ul>
<li><a href="#global-centering">全局中心化（Global Centering）</a></li>
<li><a href="#mutual-proximity">Mutual Proximity（互邻近性）</a></li>
<li><a href="#local-scaling">局部扩散（Local Scaling）</a></li>
<li><a href="#cosine-similarity">角度距离（Cosine Similarity）</a></li>
</ul>
</li>
<li><a href="#gan-application">在GAN中的应用分析</a><ul>
<li><a href="#gan">GAN隐空间的分布特性</a></li>
<li><a href="#hub_4">Hub值筛选的理论基础</a></li>
</ul>
</li>
<li><a href="#empirical-studies">实证研究与数据集分析</a><ul>
<li><a href="#hubness_2">标准数据集的Hubness统计</a></li>
<li><a href="#_8">缓解方法的对比实验</a></li>
</ul>
</li>
<li><a href="#conclusion-theory">理论总结与未来方向</a><ul>
<li><a href="#hubness_3">Hubness现象的根源三要素</a></li>
<li><a href="#_9">跨学科的类似现象</a></li>
<li><a href="#_10">未来研究方向</a></li>
</ul>
</li>
<li><a href="#practical-guide">实用指南与最佳实践</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>