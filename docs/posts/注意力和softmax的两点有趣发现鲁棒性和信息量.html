<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>注意力和Softmax的两点有趣发现：鲁棒性和信息量 | ML & Math Blog Posts</title>
    <meta name="description" content="注意力和Softmax的两点有趣发现：鲁棒性和信息量&para;
原文链接: https://spaces.ac.cn/archives/9593
发布日期: 

最近几周笔者一直都在思考注意力机制的相关性质，在这个过程中对注意力及Softmax有了更深刻的理解。在这篇文章中，笔者简单分享其中的两点：

1、Softmax注意力天然能够抵御一定的噪声扰动；
2、从信息熵角度也可以对初始化问题形成直...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=信息">信息</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #229 注意力和Softmax的两点有趣发现：鲁棒性和信息量
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#229</span>
                注意力和Softmax的两点有趣发现：鲁棒性和信息量
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2023-04-25</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=信息" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 信息</span>
                </a>
                
                <a href="../index.html?tags=熵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 熵</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="softmax">注意力和Softmax的两点有趣发现：鲁棒性和信息量<a class="toc-link" href="#softmax" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9593">https://spaces.ac.cn/archives/9593</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>最近几周笔者一直都在思考注意力机制的相关性质，在这个过程中对注意力及Softmax有了更深刻的理解。在这篇文章中，笔者简单分享其中的两点：</p>
<blockquote>
<p>1、Softmax注意力天然能够抵御一定的噪声扰动；</p>
<p>2、从信息熵角度也可以对初始化问题形成直观理解。</p>
</blockquote>
<h2 id="_1">鲁棒性<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>基于Softmax归一化的注意力机制，可以写为<br />
\begin{equation}o = \frac{\sum\limits_{i=1}^n e^{s_i} v_i}{\sum\limits_{i=1}^n e^{s_i}}\end{equation}<br />
有一天笔者突然想到一个问题：如果往$s_i$中加入独立同分布的噪声会怎样？为此，我们考虑<br />
\begin{equation}\tilde{o} = \frac{\sum\limits_{i=1}^n e^{s_i+\varepsilon_i} v_i}{\sum\limits_{i=1}^n e^{s_i+\varepsilon_i}}\end{equation}<br />
其中$\varepsilon_i$是独立同分布的噪声。然而，简单分析后笔者发现结论是“不怎么样”，注意力机制天然能抵御这类噪声，即$\tilde{o}\approx o$。</p>
<p>为了理解这一点，只需要意识到：<br />
\begin{equation}\tilde{o} = \frac{\frac{1}{n}\sum\limits_{i=1}^n e^{s_i+\varepsilon_i} v_i}{\frac{1}{n}\sum\limits_{i=1}^n e^{s_i+\varepsilon_i}}=\frac{\mathbb{E}_i[e^{s_i+\varepsilon_i} v_i]}{\mathbb{E}_i[e^{s_i+\varepsilon_i}]}\approx \frac{\mathbb{E}_i[e^{s_i}v_i]\mathbb{E}[e^{\varepsilon}]}{\mathbb{E}_i[e^{s_i}]\mathbb{E}[e^{\varepsilon}]}=\frac{\mathbb{E}_i[e^{s_i}v_i]}{\mathbb{E}_i[e^{s_i}]}=o\end{equation}<br />
约等号是利用了$\varepsilon_i$跟$s_i,v_i$相互独立，所以积的期望等于期望的积。</p>
<h2 id="_2">信息量<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>如果我们记$p_i = e^{s_i}\left/\sum\limits_{i=1}^n e^{s_i}\right.$，那么$p_i$描述了一个离散型概率分布，我们可以算信息熵<br />
\begin{equation}H = -\sum_{i=1}^n p_i\log p_i\quad\in[0,\log n]\end{equation}<br />
在<a href="/archives/3534">《“熵”不起：从熵、最大熵原理到最大熵模型（一）》</a>中我们讨论过，熵是不确定性的度量，也是信息量的度量。怎么理解两者的联系呢？熵本质上是均匀度的度量，越均匀越不确定，所以熵是不确定性的度量，熵的下界是0，所以不确定性也意味着它是我们从“不确定”到“完全确定”所能获得的最大信息量。</p>
<p>我们知道，如果将$s_i$初始化得非常大，那么$p_i$就会接近一个one hot分布，此时就会由于梯度消失而无法训练（参考<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》）</a>。笔者发现从信息量的角度也可以很直观理解这一点：模型训练本身就是从不确定（随机模型）到确定（训练模型）的过程，优化器负责从随机模型中“榨取”信息，而one hot分布的信息量为0，优化器“无利可图”，说不准还要“倒贴”，自然也就没法优化好了。所以我们要将模型初始化得尽量均匀，以保证可以“榨取”的信息量最大。</p>
<p>当然，除了要保证信息量的上界足够大外，还要保证信息量的下界足够小，才能保证可以“榨取”的信息量尽量大。之前在介绍对比学习中，有读者不理解温度参数的意义，其实也可以从信息量来理解。记<br />
\begin{equation}p_i = \frac{e^{(\cos\theta_i) / \tau}}{\sum\limits_{i=1}^n e^{(\cos\theta_i)/\tau}}\end{equation}<br />
如果$\tau=1$，那么信息熵的上界为$\log n$，但是下界约为$\log n - 0.4745$（参考<a href="/archives/9593/comment-page-1#comment-28363">评论区</a>），能获得的信息量太少，所以我们要缩小$\tau$，使得信息熵的下界接近0，从而增加能够获得的信息量。</p>
<h2 id="_3">简言之<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>简单水了一篇博客。可以看出，最终的结论还是——<a href="/archives/9019">《听说Attention与Softmax更配哦～》</a>。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9593">https://spaces.ac.cn/archives/9593</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 25, 2023). 《注意力和Softmax的两点有趣发现：鲁棒性和信息量 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9593">https://spaces.ac.cn/archives/9593</a></p>
<p>@online{kexuefm-9593,<br />
title={注意力和Softmax的两点有趣发现：鲁棒性和信息量},<br />
author={苏剑林},<br />
year={2023},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/9593}},<br />
} </p>
<hr />
<h2 id="_4">详细数学推导与理论分析<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<h3 id="1-softmax">1. Softmax注意力机制回顾<a class="toc-link" href="#1-softmax" title="Permanent link">&para;</a></h3>
<p><strong>标准注意力公式</strong>：</p>
<p>对于查询向量$\boldsymbol{q} \in \mathbb{R}^d$和键值对${(\boldsymbol{k}<em i="1">i, \boldsymbol{v}_i)}</em>^n$，注意力输出为：
\begin{equation}
\boldsymbol{o} = \sum_{i=1}^n \alpha_i \boldsymbol{v}<em i="1">i = \frac{\sum</em>}^n e^{s_i} \boldsymbol{v<em i="1">i}{\sum</em>
\end{equation}}^n e^{s_i}} \tag{1</p>
<p>其中：
- $s_i = \langle \boldsymbol{q}, \boldsymbol{k}_i \rangle$ 是注意力分数（score）
- $\alpha_i = \frac{e^{s_i}}{\sum_j e^{s_j}}$ 是注意力权重</p>
<p><strong>紧凑形式</strong>：
\begin{equation}
\boldsymbol{o} = \frac{\sum_{i=1}^n e^{s_i} \boldsymbol{v}<em i="1">i}{\sum</em>
\end{equation}}^n e^{s_i}} = \frac{\mathbb{E}_i[e^{s_i} \boldsymbol{v}_i]}{\mathbb{E}_i[e^{s_i}]} \cdot n \tag{2</p>
<p>其中$\mathbb{E}<em i="1">i[\cdot] = \frac{1}{n}\sum</em>^n (\cdot)$表示经验期望。</p>
<h3 id="2">2. 噪声鲁棒性的数学证明<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<p><strong>问题设定</strong>：假设注意力分数受到噪声扰动：
\begin{equation}
\tilde{s}_i = s_i + \varepsilon_i \tag{3}
\end{equation}</p>
<p>其中${\varepsilon_i}_{i=1}^n$是独立同分布的噪声，满足：
- $\varepsilon_i \perp!!!\perp (s_i, \boldsymbol{v}_i)$（独立性）
- $\mathbb{E}[\varepsilon_i] = 0$（零均值，可选）
- $\mathbb{E}[e^{\varepsilon_i}] = c &lt; \infty$（指数矩有限）</p>
<p><strong>加噪后的注意力输出</strong>：
\begin{equation}
\tilde{\boldsymbol{o}} = \frac{\sum_{i=1}^n e^{s_i + \varepsilon_i} \boldsymbol{v}<em i="1">i}{\sum</em>}^n e^{s_i + \varepsilon_i}} = \frac{\sum_{i=1}^n e^{s_i} e^{\varepsilon_i} \boldsymbol{v<em i="1">i}{\sum</em>
\end{equation}}^n e^{s_i} e^{\varepsilon_i}} \tag{4</p>
<p><strong>关键变换</strong>（改写为期望形式）：
\begin{equation}
\tilde{\boldsymbol{o}} = \frac{n \cdot \mathbb{E}_i[e^{s_i} e^{\varepsilon_i} \boldsymbol{v}_i]}{n \cdot \mathbb{E}_i[e^{s_i} e^{\varepsilon_i}]} = \frac{\mathbb{E}_i[e^{s_i} e^{\varepsilon_i} \boldsymbol{v}_i]}{\mathbb{E}_i[e^{s_i} e^{\varepsilon_i}]} \tag{5}
\end{equation}</p>
<p><strong>利用独立性</strong>：</p>
<p>由于$\varepsilon_i \perp!!!\perp (s_i, \boldsymbol{v}_i)$，我们有：
\begin{equation}
\mathbb{E}_i[e^{s_i} e^{\varepsilon_i} \boldsymbol{v}_i] \approx \mathbb{E}_i[e^{s_i} \boldsymbol{v}_i] \cdot \mathbb{E}[e^{\varepsilon}] \tag{6}
\end{equation}</p>
<p>\begin{equation}
\mathbb{E}_i[e^{s_i} e^{\varepsilon_i}] \approx \mathbb{E}_i[e^{s_i}] \cdot \mathbb{E}[e^{\varepsilon}] \tag{7}
\end{equation}</p>
<p>其中$\mathbb{E}[e^{\varepsilon}]$是噪声的指数矩，与$i$无关。</p>
<p><strong>约等号说明</strong>：这里用了<strong>样本平均近似总体期望</strong>的假设。严格来说：
\begin{equation}
\frac{1}{n}\sum_{i=1}^n e^{s_i} e^{\varepsilon_i} \boldsymbol{v}_i \xrightarrow{n\to\infty} \mathbb{E}[e^s e^\varepsilon \boldsymbol{v}] = \mathbb{E}[e^s \boldsymbol{v}] \mathbb{E}[e^\varepsilon] \tag{8}
\end{equation}</p>
<p>（根据大数定律和独立性）</p>
<p><strong>噪声消除</strong>：</p>
<p>代入式(5)：
\begin{equation}
\tilde{\boldsymbol{o}} \approx \frac{\mathbb{E}_i[e^{s_i} \boldsymbol{v}_i] \cdot \mathbb{E}[e^{\varepsilon}]}{\mathbb{E}_i[e^{s_i}] \cdot \mathbb{E}[e^{\varepsilon}]} = \frac{\mathbb{E}_i[e^{s_i} \boldsymbol{v}_i]}{\mathbb{E}_i[e^{s_i}]} = \boldsymbol{o} \tag{9}
\end{equation}</p>
<p><strong>结论</strong>：噪声的影响被<strong>约掉</strong>了！</p>
<h3 id="3">3. 鲁棒性的严格分析<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<p>上述推导使用了近似，现在我们进行更严格的分析。</p>
<p><strong>定理1</strong>（噪声鲁棒性）：</p>
<p>假设$\varepsilon_i$独立同分布，$\mathbb{E}[e^\varepsilon] = c$，$\text{Var}(e^\varepsilon) = \sigma^2$。则：
\begin{equation}
\mathbb{E}[\tilde{\boldsymbol{o}}] = \boldsymbol{o} + O\left(\frac{1}{\sqrt{n}}\right) \tag{10}
\end{equation}</p>
<p><strong>证明</strong>：</p>
<p>记$X_i = e^{s_i} \boldsymbol{v}_i$，$Y_i = e^{s_i}$，$Z_i = e^{\varepsilon_i}$。则：
\begin{equation}
\tilde{\boldsymbol{o}} = \frac{\sum_i X_i Z_i}{\sum_i Y_i Z_i} \tag{11}
\end{equation}</p>
<p>对分子：
\begin{equation}
\mathbb{E}\left[\sum_i X_i Z_i\right] = \sum_i \mathbb{E}[X_i] \mathbb{E}[Z_i] = c \sum_i e^{s_i} \boldsymbol{v}_i \tag{12}
\end{equation}</p>
<p>对分母：
\begin{equation}
\mathbb{E}\left[\sum_i Y_i Z_i\right] = c \sum_i e^{s_i} \tag{13}
\end{equation}</p>
<p>利用<strong>Delta方法</strong>（一阶Taylor展开）：</p>
<p>对于$f(x, y) = \frac{x}{y}$，在$(\bar{x}, \bar{y})$附近：
\begin{equation}
f(X, Y) \approx f(\bar{x}, \bar{y}) + \frac{\partial f}{\partial x}(X - \bar{x}) + \frac{\partial f}{\partial y}(Y - \bar{y}) \tag{14}
\end{equation}</p>
<p>其中：
\begin{equation}
\frac{\partial f}{\partial x} = \frac{1}{y}, \quad \frac{\partial f}{\partial y} = -\frac{x}{y^2} \tag{15}
\end{equation}</p>
<p>因此：
\begin{equation}
\mathbb{E}\left[\frac{X}{Y}\right] \approx \frac{\mathbb{E}[X]}{\mathbb{E}[Y]} + \frac{\text{Cov}(X, Y) \mathbb{E}[X] - \text{Var}(Y) \mathbb{E}[X]}{\mathbb{E}[Y]^3} \tag{16}
\end{equation}</p>
<p>对于我们的情况，由于$Z_i$独立：
\begin{equation}
\text{Var}\left(\sum_i Y_i Z_i\right) = \sum_i \text{Var}(Y_i Z_i) = \sigma^2 \sum_i e^{2s_i} = O(n) \tag{17}
\end{equation}</p>
<p>因此误差为$O(1/\sqrt{n})$。</p>
<p><strong>更直观的理解</strong>：</p>
<p>当$n$很大时，根据中心极限定理：
\begin{equation}
\frac{1}{n}\sum_{i=1}^n e^{s_i} e^{\varepsilon_i} \xrightarrow{d} \mathbb{E}[e^s e^\varepsilon] = \mathbb{E}[e^s] \mathbb{E}[e^\varepsilon] \tag{18}
\end{equation}</p>
<p>所以噪声的影响在大$n$下被平均掉了！</p>
<h3 id="4-lipschitz">4. Lipschitz连续性分析<a class="toc-link" href="#4-lipschitz" title="Permanent link">&para;</a></h3>
<p><strong>定义</strong>：函数$f: \mathbb{R}^n \to \mathbb{R}^d$是$L$-Lipschitz的，如果：
\begin{equation}
|f(\boldsymbol{x}) - f(\boldsymbol{y})| \leq L |\boldsymbol{x} - \boldsymbol{y}| \tag{19}
\end{equation}</p>
<p><strong>定理2</strong>（Softmax注意力的Lipschitz常数）：</p>
<p>将注意力视为函数$f: \mathbb{R}^n \to \mathbb{R}^d$，$f(\boldsymbol{s}) = \boldsymbol{o}$。则：
\begin{equation}
|f(\boldsymbol{s} + \boldsymbol{\varepsilon}) - f(\boldsymbol{s})| \leq L |\boldsymbol{\varepsilon}| \tag{20}
\end{equation}</p>
<p>其中Lipschitz常数$L$依赖于$\boldsymbol{s}$和${\boldsymbol{v}_i}$。</p>
<p><strong>证明</strong>：</p>
<p>记$\alpha_i = \frac{e^{s_i}}{\sum_j e^{s_j}}$，$\tilde{\alpha}_i = \frac{e^{s_i + \varepsilon_i}}{\sum_j e^{s_j + \varepsilon_j}}$。</p>
<p>则：
\begin{equation}
\begin{aligned}
|\boldsymbol{o}' - \boldsymbol{o}| &amp;= \left|\sum_i (\tilde{\alpha}_i - \alpha_i) \boldsymbol{v}_i\right| \
&amp;\leq \sum_i |\tilde{\alpha}_i - \alpha_i| \cdot |\boldsymbol{v}_i| \
&amp;\leq \max_i |\boldsymbol{v}_i| \cdot \sum_i |\tilde{\alpha}_i - \alpha_i|
\end{aligned} \tag{21}
\end{equation}</p>
<p><strong>Softmax的Lipschitz常数</strong>：</p>
<p>对于softmax函数$\sigma: \mathbb{R}^n \to \Delta^{n-1}$（$\Delta^{n-1}$是单纯形）：
\begin{equation}
|\sigma(\boldsymbol{s} + \boldsymbol{\varepsilon}) - \sigma(\boldsymbol{s})|<em>1 \leq |\boldsymbol{\varepsilon}|</em>\infty \tag{22}
\end{equation}</p>
<p>（这是softmax的经典性质）</p>
<p>因此：
\begin{equation}
|\boldsymbol{o}' - \boldsymbol{o}| \leq \max_i |\boldsymbol{v}<em>i| \cdot |\boldsymbol{\varepsilon}|</em>\infty \tag{23}
\end{equation}</p>
<p><strong>结论</strong>：注意力对score的小扰动是<strong>Lipschitz连续</strong>的，这保证了鲁棒性。</p>
<h3 id="5">5. 不同噪声分布的影响<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<p><strong>高斯噪声</strong>：$\varepsilon \sim \mathcal{N}(0, \sigma^2)$</p>
<p>矩生成函数：
\begin{equation}
\mathbb{E}[e^{\varepsilon}] = e^{\sigma^2/2} \tag{24}
\end{equation}</p>
<p>代入式(9)：
\begin{equation}
\tilde{\boldsymbol{o}} \approx \boldsymbol{o} \quad \text{（噪声约掉）} \tag{25}
\end{equation}</p>
<p><strong>均匀噪声</strong>：$\varepsilon \sim U(-a, a)$</p>
<p>\begin{equation}
\mathbb{E}[e^{\varepsilon}] = \frac{1}{2a}\int_{-a}^a e^t dt = \frac{e^a - e^{-a}}{2a} = \frac{\sinh(a)}{a} \tag{26}
\end{equation}</p>
<p>同样会约掉。</p>
<p><strong>Laplace噪声</strong>：$\varepsilon \sim \text{Laplace}(0, b)$</p>
<p>\begin{equation}
\mathbb{E}[e^{\varepsilon}] = \int_{-\infty}^{\infty} \frac{1}{2b} e^{-|t|/b} e^t dt \tag{27}
\end{equation}</p>
<p>当$b &gt; 1$时发散，但在实际应用中$b$通常很小。</p>
<p><strong>关键观察</strong>：只要$\mathbb{E}[e^{\varepsilon}]$存在且有限，噪声就能被约掉！</p>
<h3 id="6">6. 信息熵的定义与性质<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<p><strong>离散概率分布的熵</strong>：</p>
<p>对于注意力权重${\alpha_i}<em i="1">{i=1}^n$（满足$\alpha_i \geq 0$，$\sum_i \alpha_i = 1$），Shannon熵为：
\begin{equation}
H(\boldsymbol{\alpha}) = -\sum</em>
\end{equation}}^n \alpha_i \log \alpha_i \tag{28</p>
<p><strong>熵的性质</strong>：</p>
<ol>
<li><strong>非负性</strong>：$H(\boldsymbol{\alpha}) \geq 0$</li>
<li><strong>有界性</strong>：$0 \leq H(\boldsymbol{\alpha}) \leq \log n$</li>
<li><strong>最大值</strong>：当$\alpha_i = \frac{1}{n}$（均匀分布）时，$H = \log n$</li>
<li><strong>最小值</strong>：当$\alpha_i = \delta_{ij}$（one-hot分布）时，$H = 0$</li>
</ol>
<p><strong>从注意力分数到熵</strong>：</p>
<p>\begin{equation}
H(s_1, \ldots, s_n) = -\sum_{i=1}^n \frac{e^{s_i}}{\sum_j e^{s_j}} \log \frac{e^{s_i}}{\sum_j e^{s_j}} \tag{29}
\end{equation}</p>
<p>展开：
\begin{equation}
\begin{aligned}
H &amp;= -\sum_i \frac{e^{s_i}}{\sum_j e^{s_j}} \left(s_i - \log\sum_j e^{s_j}\right) \
&amp;= \log\sum_j e^{s_j} - \frac{\sum_i s_i e^{s_i}}{\sum_j e^{s_j}}
\end{aligned} \tag{30}
\end{equation}</p>
<p>记$Z = \sum_j e^{s_j}$（配分函数），则：
\begin{equation}
H = \log Z - \frac{\sum_i s_i e^{s_i}}{Z} \tag{31}
\end{equation}</p>
<h3 id="7">7. 熵与初始化的关系<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p><strong>问题</strong>：如果将注意力分数$s_i$初始化得很大，会怎样？</p>
<p><strong>分析</strong>：假设$s_i \sim \mathcal{N}(0, \sigma^2)$，当$\sigma \to \infty$时：</p>
<p>\begin{equation}
\alpha_i \approx \delta_{i, i^<em>}, \quad i^</em> = \arg\max_i s_i \tag{32}
\end{equation}</p>
<p>即：注意力权重退化为<strong>one-hot分布</strong>。</p>
<p><strong>熵的计算</strong>：</p>
<p>当$\sigma$很大时，几乎所有权重都集中在最大的$s_i$上：
\begin{equation}
H \approx 0 \tag{33}
\end{equation}</p>
<p><strong>信息量的解释</strong>：</p>
<p>熵$H$度量了<strong>不确定性</strong>，也是从"不确定"到"确定"能获得的<strong>最大信息量</strong>。</p>
<ul>
<li>$H = 0$：完全确定，无信息可学习</li>
<li>$H = \log n$：完全不确定，最大信息可学习</li>
</ul>
<p><strong>训练的视角</strong>：</p>
<p>机器学习的目标是从随机模型（高熵）学习到确定模型（低熵）。如果初始化时熵已经很低，那么<strong>可学习的信息量很少</strong>，导致训练困难。</p>
<p><strong>梯度消失</strong>：</p>
<p>当$\alpha_i \approx \delta_{ij}$时，对于$i \neq j$：
\begin{equation}
\frac{\partial \alpha_i}{\partial s_i} = \alpha_i(1 - \alpha_i) \approx 0 \tag{34}
\end{equation}</p>
<p>因此梯度消失，无法有效训练。</p>
<h3 id="8">8. 熵的下界与初始化<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<p><strong>熵与分数方差的关系</strong>：</p>
<p>假设$s_i \sim \mathcal{N}(\mu, \sigma^2)$，则可以证明（见[评论区]）：</p>
<p>\begin{equation}
H \approx \log n - C \tag{35}
\end{equation}</p>
<p>其中$C$是一个与$\sigma$相关的常数，满足$C \approx 0.4745$（当$\sigma = 1$时）。</p>
<p><strong>关键观察</strong>：即使$s_i$是标准正态分布，熵的下界也大约是：
\begin{equation}
H_{\min} \approx \log n - 0.4745 \tag{36}
\end{equation}</p>
<p>这意味着<strong>信息熵的范围</strong>大约是$[0.4745, \log n]$，而不是$[0, \log n]$！</p>
<p><strong>可学习的信息量</strong>：
\begin{equation}
\Delta H = H_{\max} - H_{\min} \approx \log n - 0.4745 \tag{37}
\end{equation}</p>
<p>当$n$很大时，$\Delta H \approx \log n$，信息量充足。</p>
<p>但如果初始化使得$H \approx H_{\min}$，那么可学习的信息量太少！</p>
<h3 id="9">9. 温度参数的作用<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p><strong>温度缩放</strong>：</p>
<p>在对比学习等场景中，常用温度参数$\tau$：
\begin{equation}
\alpha_i = \frac{e^{s_i / \tau}}{\sum_j e^{s_j / \tau}} \tag{38}
\end{equation}</p>
<p><strong>温度对熵的影响</strong>：</p>
<p>当$\tau \to 0$时：
\begin{equation}
\alpha_i \to \delta_{i, \arg\max_j s_j}, \quad H \to 0 \tag{39}
\end{equation}</p>
<p>当$\tau \to \infty$时：
\begin{equation}
\alpha_i \to \frac{1}{n}, \quad H \to \log n \tag{40}
\end{equation}</p>
<p><strong>中间温度</strong>：</p>
<p>合适的$\tau$能够<strong>调节熵的范围</strong>：
\begin{equation}
H(\tau) = -\sum_i \frac{e^{s_i/\tau}}{\sum_j e^{s_j/\tau}} \log \frac{e^{s_i/\tau}}{\sum_j e^{s_j/\tau}} \tag{41}
\end{equation}</p>
<p>改写：
\begin{equation}
H(\tau) = \log\sum_j e^{s_j/\tau} - \frac{1}{\tau} \frac{\sum_i s_i e^{s_i/\tau}}{\sum_j e^{s_j/\tau}} \tag{42}
\end{equation}</p>
<p><strong>对比学习中的温度</strong>：</p>
<p>在对比学习中，$s_i = \cos\theta_i$（余弦相似度），范围是$[-1, 1]$。</p>
<p>如果$\tau = 1$，则：
\begin{equation}
H \in [0, \log n - 0.4745] \tag{43}
\end{equation}</p>
<p>信息量不足！</p>
<p><strong>解决方案</strong>：缩小$\tau$（如$\tau = 0.07$），使得：
\begin{equation}
s_i' = \frac{s_i}{\tau} \in \left[-\frac{1}{\tau}, \frac{1}{\tau}\right] = [-14.3, 14.3] \tag{44}
\end{equation}</p>
<p>这样熵的下界接近0，信息量充足：
\begin{equation}
H \in [0, \log n] \tag{45}
\end{equation}</p>
<h3 id="10">10. 信息论的几何视角<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<p><strong>概率单纯形</strong>：</p>
<p>$n$个类别的概率分布构成$(n-1)$维单纯形：
\begin{equation}
\Delta^{n-1} = \left{\boldsymbol{\alpha} \in \mathbb{R}^n : \alpha_i \geq 0, \sum_i \alpha_i = 1\right} \tag{46}
\end{equation}</p>
<p><strong>熵作为"距离"</strong>：</p>
<p>熵可以理解为分布到<strong>中心点</strong>（均匀分布）的"距离"：
\begin{equation}
H(\boldsymbol{\alpha}) = \log n - D_{\text{KL}}(\boldsymbol{\alpha} | \boldsymbol{u}) \tag{47}
\end{equation}</p>
<p>其中$\boldsymbol{u} = [\frac{1}{n}, \ldots, \frac{1}{n}]$。</p>
<p><strong>Fisher信息矩阵</strong>：</p>
<p>在概率流形上，Fisher信息度量定义了Riemannian度量：
\begin{equation}
g_{ij} = \mathbb{E}_{\alpha}\left[\frac{\partial \log p(x|\alpha)}{\partial \alpha_i} \frac{\partial \log p(x|\alpha)}{\partial \alpha_j}\right] \tag{48}
\end{equation}</p>
<p>对于categorical分布，这简化为：
\begin{equation}
g_{ij} = \frac{\delta_{ij}}{\alpha_i} - 1 \tag{49}
\end{equation}</p>
<p><strong>熵的梯度</strong>：</p>
<p>在这个度量下，熵的梯度为：
\begin{equation}
\nabla_{\alpha} H = -\log \boldsymbol{\alpha} - \mathbf{1} \tag{50}
\end{equation}</p>
<p>这指向了均匀分布！</p>
<h3 id="11">11. 熵与注意力机制的训练动态<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>训练初期</strong>（随机初始化）：</p>
<p>$s_i$接近0，$\alpha_i \approx \frac{1}{n}$，$H \approx \log n$（高熵）。</p>
<p><strong>训练中期</strong>：</p>
<p>模型开始区分重要和不重要的tokens，某些$\alpha_i$变大，$H$下降。</p>
<p><strong>训练后期</strong>：</p>
<p>注意力权重趋向peaked分布，$H$接近下界（但不应该为0，否则梯度消失）。</p>
<p><strong>理想的训练轨迹</strong>：</p>
<p>\begin{equation}
H(t): \log n \to \text{中等值} \to H_{\text{final}} \tag{51}
\end{equation}</p>
<p>其中$H_{\text{final}}$应该足够低（表示学到了pattern），但不能太低（保持梯度）。</p>
<h3 id="12-softmax">12. Softmax的平滑近似<a class="toc-link" href="#12-softmax" title="Permanent link">&para;</a></h3>
<p><strong>LogSumExp作为max的平滑近似</strong>：</p>
<p>Softmax与max函数密切相关：
\begin{equation}
\max_i s_i \leq \log\sum_i e^{s_i} \leq \max_i s_i + \log n \tag{52}
\end{equation}</p>
<p><strong>证明</strong>：</p>
<p>下界：$\log\sum_i e^{s_i} \geq \log e^{\max_i s_i} = \max_i s_i$</p>
<p>上界：$\log\sum_i e^{s_i} \leq \log(n \cdot e^{\max_i s_i}) = \max_i s_i + \log n$</p>
<p><strong>紧致性</strong>：</p>
<p>当某个$s_j \gg s_i$ ($i \neq j$)时：
\begin{equation}
\log\sum_i e^{s_i} \approx \max_i s_i = s_j \tag{53}
\end{equation}</p>
<p><strong>注意力权重的近似</strong>：</p>
<p>\begin{equation}
\alpha_i \approx \begin{cases}
1, &amp; i = \arg\max_j s_j \
0, &amp; \text{otherwise}
\end{cases} \tag{54}
\end{equation}</p>
<p><strong>与硬注意力的联系</strong>：</p>
<p>硬注意力（hard attention）直接选择最大的：
\begin{equation}
\boldsymbol{o}<em>{\text{hard}} = \boldsymbol{v}</em>{i^<em>}, \quad i^</em> = \arg\max_i s_i \tag{55}
\end{equation}</p>
<p>Softmax注意力是其平滑版本：
\begin{equation}
\boldsymbol{o}<em i_="i^*">{\text{soft}} = \sum_i \alpha_i \boldsymbol{v}_i \approx \boldsymbol{v}</em>
\end{equation}} \tag{56</p>
<h3 id="13">13. 对抗样本的影响分析<a class="toc-link" href="#13" title="Permanent link">&para;</a></h3>
<p><strong>对抗扰动</strong>：</p>
<p>假设对分数进行精心设计的扰动：
\begin{equation}
\tilde{s}_i = s_i + \delta_i, \quad |\boldsymbol{\delta}| \leq \epsilon \tag{57}
\end{equation}</p>
<p>目标是最大化输出变化$|\tilde{\boldsymbol{o}} - \boldsymbol{o}|$。</p>
<p><strong>一阶近似</strong>：</p>
<p>对注意力输出进行Taylor展开：
\begin{equation}
\tilde{\boldsymbol{o}} \approx \boldsymbol{o} + \sum_i \frac{\partial \boldsymbol{o}}{\partial s_i} \delta_i \tag{58}
\end{equation}</p>
<p><strong>梯度计算</strong>：</p>
<p>\begin{equation}
\frac{\partial \boldsymbol{o}}{\partial s_i} = \frac{\partial}{\partial s_i}\left(\sum_j \alpha_j \boldsymbol{v}_j\right) = \sum_j \frac{\partial \alpha_j}{\partial s_i} \boldsymbol{v}_j \tag{59}
\end{equation}</p>
<p>利用softmax的性质：
\begin{equation}
\frac{\partial \alpha_j}{\partial s_i} = \begin{cases}
\alpha_i(1 - \alpha_i), &amp; i = j \
-\alpha_i \alpha_j, &amp; i \neq j
\end{cases} \tag{60}
\end{equation}</p>
<p>代入：
\begin{equation}
\frac{\partial \boldsymbol{o}}{\partial s_i} = \alpha_i(1-\alpha_i)\boldsymbol{v}<em i="i" j_neq="j\neq">i - \alpha_i\sum</em>
\end{equation}}\alpha_j\boldsymbol{v}_j = \alpha_i(\boldsymbol{v}_i - \boldsymbol{o}) \tag{61</p>
<p><strong>对抗方向</strong>：</p>
<p>最大化$|\tilde{\boldsymbol{o}} - \boldsymbol{o}|$的扰动为：
\begin{equation}
\delta_i^* = \epsilon \cdot \text{sign}\left(\left\langle \frac{\partial \boldsymbol{o}}{\partial s_i}, \boldsymbol{v} \right\rangle\right) \tag{62}
\end{equation}</p>
<p>其中$\boldsymbol{v}$是某个目标方向。</p>
<p><strong>鲁棒性分析</strong>：</p>
<p>注意到式(61)中的因子$\alpha_i(1-\alpha_i)$：
- 当$\alpha_i \approx 0$或$\alpha_i \approx 1$时，梯度很小（鲁棒）
- 当$\alpha_i \approx 0.5$时，梯度最大（易受攻击）</p>
<p>这与熵有关：均匀分布（$\alpha_i = \frac{1}{n}$）时熵最大，对抗扰动的影响也最大。</p>
<h3 id="14">14. 缩放因子的理论分析<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<p><strong>标准Scaled Dot-Product Attention</strong>：</p>
<p>\begin{equation}
\boldsymbol{o} = \text{softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d}}\right) \boldsymbol{V} \tag{63}
\end{equation}</p>
<p>其中$d$是维度，$\sqrt{d}$是缩放因子。</p>
<p><strong>为什么需要缩放？</strong></p>
<p>假设$\boldsymbol{q}, \boldsymbol{k} \in \mathbb{R}^d$的元素独立同分布，$\mathbb{E}[q_i] = 0$，$\text{Var}(q_i) = \sigma^2$。</p>
<p>则：
\begin{equation}
s = \langle \boldsymbol{q}, \boldsymbol{k} \rangle = \sum_{i=1}^d q_i k_i \tag{64}
\end{equation}</p>
<p>期望：$\mathbb{E}[s] = 0$</p>
<p>方差：
\begin{equation}
\text{Var}(s) = \sum_{i=1}^d \text{Var}(q_i k_i) = d \cdot \mathbb{E}[q_i^2] \mathbb{E}[k_i^2] = d\sigma^4 \tag{65}
\end{equation}</p>
<p><strong>问题</strong>：当$d$很大时，$s$的方差变得很大，导致$e^s$数值不稳定。</p>
<p><strong>解决方案</strong>：除以$\sqrt{d}$：
\begin{equation}
s' = \frac{s}{\sqrt{d}} \Rightarrow \text{Var}(s') = \sigma^4 \tag{66}
\end{equation}</p>
<p>这样方差不随$d$变化！</p>
<p><strong>与熵的联系</strong>：</p>
<p>如果不缩放，当$d \to \infty$时，$s_i$的方差趋向无穷，导致：
\begin{equation}
\alpha_{i^*} \to 1, \quad H \to 0 \tag{67}
\end{equation}</p>
<p>缩放后，熵保持在合理范围。</p>
<h3 id="15">15. 信息瓶颈理论<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>信息瓶颈（Information Bottleneck）</strong>：</p>
<p>在深度学习中，隐藏层$Z$应该：
1. 最大化与标签$Y$的互信息：$I(Z; Y)$
2. 最小化与输入$X$的互信息：$I(Z; X)$</p>
<p>平衡两者：
\begin{equation}
\mathcal{L}_{\text{IB}} = I(Z; X) - \beta I(Z; Y) \tag{68}
\end{equation}</p>
<p><strong>注意力的视角</strong>：</p>
<p>注意力机制相当于从${\boldsymbol{v}_i}$中选择信息压缩到$\boldsymbol{o}$。</p>
<ul>
<li>高熵$H$：保留更多信息（$I(Z; X)$大）</li>
<li>低熵$H$：压缩信息（$I(Z; X)$小）</li>
</ul>
<p><strong>最优熵</strong>：</p>
<p>存在一个最优熵$H^<em>$，平衡信息保留和压缩。初始化时应该接近$H^</em>$。</p>
<h3 id="16">16. 多头注意力与熵<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<p><strong>多头注意力</strong>：</p>
<p>\begin{equation}
\text{MultiHead}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \boldsymbol{W}^O \tag{69}
\end{equation}</p>
<p>其中：
\begin{equation}
\text{head}_i = \text{Attention}(\boldsymbol{Q}\boldsymbol{W}_i^Q, \boldsymbol{K}\boldsymbol{W}_i^K, \boldsymbol{V}\boldsymbol{W}_i^V) \tag{70}
\end{equation}</p>
<p><strong>每个头的熵</strong>：</p>
<p>第$i$个头的熵为$H_i$。总的信息量大约是：
\begin{equation}
H_{\text{total}} \approx \sum_{i=1}^h H_i \tag{71}
\end{equation}</p>
<p>（近似，假设各头独立）</p>
<p><strong>多样性</strong>：</p>
<p>理想情况下，不同的头应该关注不同的方面，即熵分布${H_i}$应该有多样性。</p>
<h3 id="17">17. 自注意力的谱分析<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<p><strong>自注意力矩阵</strong>：</p>
<p>对于自注意力，$\boldsymbol{A} = \text{softmax}(\boldsymbol{Q}\boldsymbol{K}^\top / \sqrt{d})$是一个$n \times n$随机矩阵（每行和为1）。</p>
<p><strong>谱性质</strong>：</p>
<ol>
<li>最大特征值$\lambda_1 = 1$（对应特征向量$\mathbf{1}$）</li>
<li>其他特征值$|\lambda_i| &lt; 1$</li>
</ol>
<p><strong>与熵的关系</strong>：</p>
<p>可以证明，熵与特征值分布有关：
\begin{equation}
H \approx -\sum_i \lambda_i \log \lambda_i \tag{72}
\end{equation}</p>
<p>（这是von Neumann熵的类比）</p>
<p><strong>Rank与信息</strong>：</p>
<p>注意力矩阵的有效秩（effective rank）：
\begin{equation}
\text{rank}_{\text{eff}}(\boldsymbol{A}) = \exp(H) \tag{73}
\end{equation}</p>
<p>这度量了注意力分布的"有效宽度"。</p>
<h3 id="18">18. 位置编码的影响<a class="toc-link" href="#18" title="Permanent link">&para;</a></h3>
<p><strong>绝对位置编码</strong>：</p>
<p>添加位置编码$\boldsymbol{p}<em ij="ij">i$后：
\begin{equation}
s</em>
\end{equation}} = \langle \boldsymbol{q}_i + \boldsymbol{p}_i, \boldsymbol{k}_j + \boldsymbol{p}_j \rangle \tag{74</p>
<p><strong>对熵的影响</strong>：</p>
<p>位置编码增加了$s_{ij}$的多样性，通常会：
- 增加熵$H$（如果位置编码是随机的）
- 减少熵$H$（如果位置编码诱导了结构，如局部性）</p>
<p><strong>相对位置编码</strong>：</p>
<p>\begin{equation}
s_{ij} = \langle \boldsymbol{q}<em i-j="i-j">i, \boldsymbol{k}_j \rangle + b</em>
\end{equation}} \tag{75</p>
<p>其中$b_k$是相对位置偏置。</p>
<p>这倾向于使注意力集中在附近的tokens，<strong>降低熵</strong>。</p>
<h3 id="19">19. 注意力熵的实验观察<a class="toc-link" href="#19" title="Permanent link">&para;</a></h3>
<p><strong>Transformer训练中的熵变化</strong>（经验观察）：</p>
<ol>
<li><strong>初始阶段</strong>：$H \approx \log n$（接近均匀）</li>
<li><strong>快速下降</strong>：$H$迅速下降，模型学习到基本patterns</li>
<li><strong>震荡稳定</strong>：$H$在某个值附近震荡</li>
<li><strong>过拟合</strong>：如果继续训练，$H$可能继续下降（过拟合迹象）</li>
</ol>
<p><strong>层间差异</strong>：</p>
<ul>
<li><strong>浅层</strong>：$H$较高，保留更多全局信息</li>
<li><strong>深层</strong>：$H$较低，关注特定的重要tokens</li>
</ul>
<p><strong>任务依赖</strong>：</p>
<ul>
<li><strong>语言建模</strong>：$H$逐层下降</li>
<li><strong>机器翻译</strong>：encoder的$H$较高，decoder的$H$较低</li>
</ul>
<h3 id="20">20. 正则化与熵约束<a class="toc-link" href="#20" title="Permanent link">&para;</a></h3>
<p><strong>熵正则化</strong>：</p>
<p>在损失函数中添加熵项：
\begin{equation}
\mathcal{L}<em _text_task="\text{task">{\text{total}} = \mathcal{L}</em>
\end{equation}}} + \lambda H(\boldsymbol{\alpha}) \tag{76</p>
<ul>
<li>$\lambda &gt; 0$：鼓励高熵（平滑注意力）</li>
<li>$\lambda &lt; 0$：鼓励低熵（peaked注意力）</li>
</ul>
<p><strong>效果</strong>：</p>
<ul>
<li>高熵正则：防止过拟合，增强鲁棒性</li>
<li>低熵正则：加速收敛，但可能过拟合</li>
</ul>
<p><strong>与Label Smoothing的类比</strong>：</p>
<p>Label smoothing也是一种熵正则化，鼓励输出分布有更高的熵。</p>
<h3 id="21">21. 稀疏注意力与熵<a class="toc-link" href="#21" title="Permanent link">&para;</a></h3>
<p><strong>稀疏注意力</strong>：</p>
<p>只计算部分attention scores：
\begin{equation}
\alpha_i = \begin{cases}
\frac{e^{s_i}}{\sum_{j \in \mathcal{N}(i)} e^{s_j}}, &amp; j \in \mathcal{N}(i) \
0, &amp; \text{otherwise}
\end{cases} \tag{77}
\end{equation}</p>
<p>其中$\mathcal{N}(i)$是$i$的邻域。</p>
<p><strong>熵的变化</strong>：</p>
<p>稀疏化通常降低有效序列长度$n$，因此最大熵变为：
\begin{equation}
H_{\max} = \log |\mathcal{N}(i)| &lt; \log n \tag{78}
\end{equation}</p>
<p><strong>信息损失</strong>：</p>
<p>稀疏化会损失一些信息，但换来了计算效率。</p>
<h3 id="22-vs">22. 连续vs.离散注意力<a class="toc-link" href="#22-vs" title="Permanent link">&para;</a></h3>
<p><strong>离散注意力</strong>（如Pointer Network）：</p>
<p>选择单个index：
\begin{equation}
i^* = \arg\max_i s_i \tag{79}
\end{equation}</p>
<p>熵为0（确定性）。</p>
<p><strong>Gumbel-Softmax</strong>：</p>
<p>从softmax分布中采样，同时保持可微：
\begin{equation}
\alpha_i = \frac{e^{(s_i + g_i) / \tau}}{\sum_j e^{(s_j + g_j) / \tau}} \tag{80}
\end{equation}</p>
<p>其中$g_i \sim \text{Gumbel}(0, 1)$。</p>
<p>温度$\tau$控制熵：
- $\tau \to 0$：接近离散（低熵）
- $\tau$大：接近连续（高熵）</p>
<h3 id="23">23. 注意力可视化与熵<a class="toc-link" href="#23" title="Permanent link">&para;</a></h3>
<p><strong>热图可视化</strong>：</p>
<p>绘制注意力矩阵$\boldsymbol{A} = [\alpha_{ij}]$，颜色深浅表示权重大小。</p>
<p><strong>熵的视觉含义</strong>：</p>
<ul>
<li><strong>高熵</strong>：热图较均匀，颜色浅</li>
<li><strong>低熵</strong>：热图有明显的亮点，颜色深</li>
</ul>
<p><strong>通过可视化诊断</strong>：</p>
<ol>
<li><strong>过于均匀</strong>（$H \approx \log n$）：模型未学到有用信息</li>
<li><strong>过于集中</strong>（$H \approx 0$）：可能过拟合或梯度消失</li>
<li><strong>适中熵</strong>：理想状态</li>
</ol>
<h3 id="24-gap">24. 理论与实践的Gap<a class="toc-link" href="#24-gap" title="Permanent link">&para;</a></h3>
<p><strong>理论假设</strong>：</p>
<ul>
<li>噪声独立同分布</li>
<li>大$n$极限</li>
<li>线性近似</li>
</ul>
<p><strong>实际情况</strong>：</p>
<ul>
<li>噪声可能相关（如系统性偏差）</li>
<li>$n$有限（通常$n &lt; 1000$）</li>
<li>非线性效应</li>
</ul>
<p><strong>Gap的影响</strong>：</p>
<p>理论预测的完美鲁棒性在实践中可能不成立，但<strong>定性结论</strong>仍然有用：
- Softmax注意力对独立噪声有一定鲁棒性
- 熵是训练动态的重要指标</p>
<h3 id="25">25. 开放问题与未来方向<a class="toc-link" href="#25" title="Permanent link">&para;</a></h3>
<p><strong>问题1</strong>：能否设计更鲁棒的注意力机制？</p>
<p>例如，使用其他归一化方法（如L1归一化）？</p>
<p><strong>问题2</strong>：最优熵的理论表征</p>
<p>对于给定任务和数据，最优的注意力熵是多少？</p>
<p><strong>问题3</strong>：熵的动态调整</p>
<p>能否自适应地调整温度参数，使熵在训练过程中自动达到最优值？</p>
<p><strong>问题4</strong>：多模态注意力的熵</p>
<p>在视觉-语言模型中，cross-modal attention的熵有何特点？</p>
<p><strong>未来方向</strong>：
1. 自适应温度学习
2. 熵感知的正则化
3. 鲁棒性的理论保证
4. 高效的熵计算方法</p>
<hr />
<h2 id="_5">参考文献<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<ol>
<li>Vaswani et al., "Attention Is All You Need", NeurIPS 2017</li>
<li>Pereyra et al., "Regularizing Neural Networks by Penalizing Confident Output Distributions", ICLR 2017</li>
<li>Jang et al., "Categorical Reparameterization with Gumbel-Softmax", ICLR 2017</li>
<li>Cordonnier et al., "On the Relationship between Self-Attention and Convolutional Layers", ICLR 2020</li>
<li>Kobayashi et al., "Attention is Not Only a Weight: Analyzing Transformers with Vector Norms", EMNLP 2020</li>
</ol>
<h2 id="_6">总结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文深入分析了Softmax注意力的两个关键性质：</p>
<ol>
<li>
<p><strong>鲁棒性</strong>：
   - 对独立同分布噪声有天然鲁棒性
   - 噪声的指数矩被约掉
   - Lipschitz连续性保证稳定性</p>
</li>
<li>
<p><strong>信息量</strong>：
   - 熵是不确定性和信息量的度量
   - 初始化应保证足够的熵（可学习信息）
   - 温度参数调节熵的范围</p>
</li>
</ol>
<p><strong>关键洞察</strong>：
- Softmax结构隐含了平均化效应，天然具有降噪能力
- 熵提供了理解训练动态的信息论视角
- 合适的初始化和温度选择对训练至关重要</p>
<p>这些理论分析为Transformer等attention-based模型的设计和调试提供了坚实的理论基础。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="梯度视角下的lora简介分析猜测及推广.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#228 梯度视角下的LoRA：简介、分析、猜测及推广</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="如何度量数据的稀疏程度.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#230 如何度量数据的稀疏程度？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#softmax">注意力和Softmax的两点有趣发现：鲁棒性和信息量</a><ul>
<li><a href="#_1">鲁棒性</a></li>
<li><a href="#_2">信息量</a></li>
<li><a href="#_3">简言之</a></li>
<li><a href="#_4">详细数学推导与理论分析</a><ul>
<li><a href="#1-softmax">1. Softmax注意力机制回顾</a></li>
<li><a href="#2">2. 噪声鲁棒性的数学证明</a></li>
<li><a href="#3">3. 鲁棒性的严格分析</a></li>
<li><a href="#4-lipschitz">4. Lipschitz连续性分析</a></li>
<li><a href="#5">5. 不同噪声分布的影响</a></li>
<li><a href="#6">6. 信息熵的定义与性质</a></li>
<li><a href="#7">7. 熵与初始化的关系</a></li>
<li><a href="#8">8. 熵的下界与初始化</a></li>
<li><a href="#9">9. 温度参数的作用</a></li>
<li><a href="#10">10. 信息论的几何视角</a></li>
<li><a href="#11">11. 熵与注意力机制的训练动态</a></li>
<li><a href="#12-softmax">12. Softmax的平滑近似</a></li>
<li><a href="#13">13. 对抗样本的影响分析</a></li>
<li><a href="#14">14. 缩放因子的理论分析</a></li>
<li><a href="#15">15. 信息瓶颈理论</a></li>
<li><a href="#16">16. 多头注意力与熵</a></li>
<li><a href="#17">17. 自注意力的谱分析</a></li>
<li><a href="#18">18. 位置编码的影响</a></li>
<li><a href="#19">19. 注意力熵的实验观察</a></li>
<li><a href="#20">20. 正则化与熵约束</a></li>
<li><a href="#21">21. 稀疏注意力与熵</a></li>
<li><a href="#22-vs">22. 连续vs.离散注意力</a></li>
<li><a href="#23">23. 注意力可视化与熵</a></li>
<li><a href="#24-gap">24. 理论与实践的Gap</a></li>
<li><a href="#25">25. 开放问题与未来方向</a></li>
</ul>
</li>
<li><a href="#_5">参考文献</a></li>
<li><a href="#_6">总结</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>