<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>生成扩散模型漫谈（二十八）：分步理解一致性模型 | ML & Math Blog Posts</title>
    <meta name="description" content="生成扩散模型漫谈（二十八）：分步理解一致性模型&para;
原文链接: https://spaces.ac.cn/archives/10633
发布日期: 

书接上文，在《生成扩散模型漫谈（二十七）：将步长作为条件输入》中，我们介绍了加速采样的Shortcut模型，其对比的模型之一就是“一致性模型（Consistency Models）”。事实上，早在《生成扩散模型漫谈（十七）：构建ODE的一般...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=微分方程">微分方程</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #302 生成扩散模型漫谈（二十八）：分步理解一致性模型
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#302</span>
                生成扩散模型漫谈（二十八）：分步理解一致性模型
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-12-18</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=微分方程" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 微分方程</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=采样" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 采样</span>
                </a>
                
                <a href="../index.html?tags=扩散" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 扩散</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">生成扩散模型漫谈（二十八）：分步理解一致性模型<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10633">https://spaces.ac.cn/archives/10633</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>书接上文，在<a href="/archives/10617">《生成扩散模型漫谈（二十七）：将步长作为条件输入》</a>中，我们介绍了加速采样的Shortcut模型，其对比的模型之一就是“<a href="https://papers.cool/arxiv/2303.01469">一致性模型（Consistency Models）</a>”。事实上，早在<a href="/archives/9497">《生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）》</a>介绍ReFlow时，就有读者提到了一致性模型，但笔者总感觉它更像是实践上的Trick，理论方面略显单薄，所以兴趣寥寥。</p>
<p>不过，既然我们开始关注扩散模型加速采样方面的进展，那么一致性模型就是一个绕不开的工作。因此，趁着这个机会，笔者在这里分享一下自己对一致性模型的理解。</p>
<h2 id="_2">熟悉配方<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>还是熟悉的配方，我们的出发点依旧是<a href="/archives/9497">ReFlow</a>，因为它大概是ODE式扩散最简单的理解方式。设$\boldsymbol{x}<em>0\sim p_0(\boldsymbol{x}_0)$是目标分布的 _真实样本</em> ，$\boldsymbol{x}<em>1\sim p_1(\boldsymbol{x}_1)$是先验分布的 _随机噪声</em> ，$\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$是加噪样本，那么ReFlow的训练目标是：<br />
\begin{equation}\boldsymbol{\theta}^<em> = \mathop{\text{argmin}}<em U_0_1_boldsymbol_x="U[0,1],\boldsymbol{x" t_sim="t\sim">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\left[w(t)\Vert\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) - (\boldsymbol{x}_1 - \boldsymbol{x}_0)\Vert^2\right]\label{eq:loss}\end{equation}<br />
其中$w(t)$是可调的权重。训练完成后可以通过求解$d\boldsymbol{x}_t/dt = \boldsymbol{v}</em>^</em>}(\boldsymbol{x}_t, t)$来实现$\boldsymbol{x}_1$到$\boldsymbol{x}_0$的变换，从而完成采样。</p>
<p>需要指出的是，一致性模型的Noise Schedule是$\boldsymbol{x}_t = \boldsymbol{x}_0 + t\boldsymbol{x}_1$（当$t$足够大时$\boldsymbol{x}_t$同样接近于纯噪声），跟ReFlow略有不同。不过本文的主要目的，是尝试一步步引导出跟一致性模型相同的训练思想和训练目标，笔者认为ReFlow的更好理解一些，所以还是按照ReFlow的来介绍，至于具体的训练细节大家按需自行调整就好。</p>
<p>利用$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$，我们可以消去目标$\eqref{eq:loss}$中的$\boldsymbol{x}_1$：<br />
\begin{equation}\boldsymbol{\theta}^* = \mathop{\text{argmin}}</em>}} \mathbb{E<em _boldsymbol_theta="\boldsymbol{\theta">{t\sim U[0,1],\boldsymbol{x}_0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t)\Vert \underbrace{\boldsymbol{x}_t - t\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_f="\boldsymbol{f">t, t)}</em><em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)} - \boldsymbol{x}_0\Vert^2\big]\label{eq:loss-2}\end{equation}<br />
其中$\tilde{w}(t) = w(t)/t^2$。注意$\boldsymbol{x}_0$是真实样本，$\boldsymbol{x}_t$是加噪样本，所以ReFlow的训练目标实际上也是在去噪。预测干净样本的模型为$\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)=\boldsymbol{x}_t - t\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)$，这个函数有一个重要特性是恒成立$\boldsymbol{f}</em>_0$，这正是一致性模型的关键约束之一。}}(\boldsymbol{x}_0, 0)=\boldsymbol{x</p>
<h2 id="_3">分步理解<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>接下来让我们一步步解构ReFlow的训练过程，试图从中找到更好的训练目标。首先我们将$[0,1]$等分为$n$份，每份大小为$1/n$，记$t_k = k/n$，那么$t$就只需从有限集合$\{0,t_1,t_2,\cdots,t_n\}$均匀采样。当然我们也可以选择非均匀的离散化方式，这些都是非关键的细节问题。</p>
<p>由于$t_0=0$是平凡的，我们从$t_1$开始，第一步的训练目标是<br />
\begin{equation}\boldsymbol{\theta}<em _boldsymbol_theta="\boldsymbol{\theta">1^<em> = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t_1)\Vert \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_1}, t_1) - \boldsymbol{x}_0\Vert^2\big]\end{equation}<br />
接着，考虑第二步的训练目标，还是按照$\eqref{eq:loss-2}$的话，那么应该是$\Vert \boldsymbol{f}</em>_0\Vert^2$的期望，但现在我们评估一个新目标：}}(\boldsymbol{x}_{t_2}, t_2) - \boldsymbol{x<br />
\begin{equation}\boldsymbol{\theta}_2^</em> = \mathop{\text{argmin}}</em>}} \mathbb{E<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{x}_0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t_2)\Vert \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_2}, t_2) - \boldsymbol{f}</em><em t_1="t_1">1^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t_1}, t_1)\Vert^2\big]\end{equation}<br />
也就是说预测对象改为$\boldsymbol{f}</em>_1^</em>}(\boldsymbol{x}</em>}, t_1)$而不是$\boldsymbol{x<em t_2="t_2">0$。为什么要这样改呢？我们分可行性和必要性两方面来讨论。可行性方面，$\boldsymbol{x}</em>}$相比$\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_1}$加了更多噪声，所以它去噪会更困难，换言之$\boldsymbol{f}</em><em t_1="t_1">2^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t_2}, t_2)$的复原程度是不如$\boldsymbol{f}</em>_1^</em>}(\boldsymbol{x}</em>}, t_1)$的，所以用$\boldsymbol{f<em t_1="t_1">{\boldsymbol{\theta}_1^*}(\boldsymbol{x}</em>_0$作为第二步的训练目标完全是可行的。}, t_1)$替换掉$\boldsymbol{x</p>
<p>可即便如此，那又有什么换的必要呢？答案是减少“轨迹交叉”。由于$\boldsymbol{x}<em t_k="t_k">{t_k} = (1-t_k)\boldsymbol{x}_0 + t_k\boldsymbol{x}_1$，因此随着$k$的增大，$\boldsymbol{x}</em>}$对$\boldsymbol{x<em t_k="t_k">0$的依赖会越来越弱，以至于两个不同的$\boldsymbol{x}_0$，它们对应的$\boldsymbol{x}</em>_0$为预测目标的话，就会出现“一个输入，多个目标”的困境，这就是“轨迹交叉”。}$会很接近，这时候还是以$\boldsymbol{x</p>
<p>面对这个困境，ReFlow的策略是事后蒸馏，因为预训练完后求解$d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t/dt = \boldsymbol{v}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, t)$就可以得到很多$(\boldsymbol{x}_0,\boldsymbol{x}_1)$对，用这些配对的$\boldsymbol{x}_0,\boldsymbol{x}_1$去构建$\boldsymbol{x}_t$就能避免交叉。一致性模型的想法是把预测目标换成$\boldsymbol{f}</em>^}_{k-1</em>}(\boldsymbol{x}<em k-1="k-1">{t</em>}}, t_{k-1})$，因为对于“同一$\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">1$、不同$\boldsymbol{x}_0$”，$\boldsymbol{f}</em><em t__k-1="t_{k-1">{k-1}^*}(\boldsymbol{x}</em>_0$间的差异要小，所以也能减少交叉风险。}}, t_{k-1})$间的差异会比$\boldsymbol{x</p>
<p>简单来说，就是$\boldsymbol{f}<em t_1="t_1">{\boldsymbol{\theta}_2^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t_2}, t_2)$预测$\boldsymbol{f}</em>_1^</em>}(\boldsymbol{x}</em>}, t_1)$比预测$\boldsymbol{x<em t_2="t_2">0$更容易，并且该达到的效果也能达到，所以调整了预测目标。类似地，我们可以写出<br />
\begin{equation}\begin{gathered}
\boldsymbol{\theta}_3^<em> = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t_3)\Vert \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_3}, t_3) - \boldsymbol{f}</em>_2^</em>}(\boldsymbol{x}</em>, t_2)\Vert^2\big] \\
\boldsymbol{\theta}<em t_3="t_3">4^<em> = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t_4)\Vert \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_4}, t_4) - \boldsymbol{f}</em>_3^</em>}(\boldsymbol{x}</em>, t_3)\Vert^2\big] \\
\vdots \\[5pt]
\boldsymbol{\theta}<em t__n-1="t_{n-1">n^<em> = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t_n)\Vert \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_n}, t_n) - \boldsymbol{f}</em>^}_{n-1</em>}(\boldsymbol{x}</em>)\Vert^2\big]}}, t_{n-1
\end{gathered}\end{equation}</p>
<h2 id="_4">一致训练<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>现在我们已经完成了ReFlow模型的解构，并且得到了一个新的自认为更合理的训练目标，但代价是得到了$n$套参数$\boldsymbol{\theta}<em t__k-1="t_{k-1">1^<em>,\boldsymbol{\theta}_2^</em>,\cdots,\boldsymbol{\theta}_n^<em>$，这当然不是我们想要的，我们只想要一个模型。于是我们认为所有的$\boldsymbol{\theta}_i^</em>$可以共用同一套参数，于是我们可以写出训练目标<br />
\begin{equation}\boldsymbol{\theta}^<em> = \mathop{\text{argmin}}<em k_sim_n_boldsymbol_x="k\sim[n],\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t_k)\Vert \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_k}, t_k) - \boldsymbol{f}</em>^</em>}(\boldsymbol{x}</em>}}, t_{k-1})\Vert^2\big]\label{eq:loss-3}\end{equation
这里$k\sim[n]$是指$k$从$\{1,2,\cdots,n\}$中均匀采样。上式的问题是，$\boldsymbol{\theta}^<em>$是我们要求的参数，但它又出现在目标函数中，这显然是不科学的（知道$\boldsymbol{\theta}^</em>$了我还训练干嘛），因此必须修改上述目标使得它更为可行。</p>
<p>$\boldsymbol{\theta}^<em>$的意义是理论最优解，考虑到随着训练的推进，$\boldsymbol{\theta}$会慢慢逼近$\boldsymbol{\theta}^</em>$，所以在目标函数中我们可以将这个条件放宽为“超前解”，即它只要比当前的$\boldsymbol{\theta}$更好就行了。怎么构建“超前解”呢？一致性模型的做法是对历史权重进行<a href="/archives/6575#%E6%9D%83%E9%87%8D%E6%BB%91%E5%8A%A8%E5%B9%B3%E5%9D%87">EMA（Exponential Moving Average，指数滑动平均</a>），这往往能得到一个更优秀的解，早些年我们在打比赛时就经常用到这个技巧。</p>
<p>因此，一致性模型最终的训练目标是：<br />
\begin{equation}\boldsymbol{\theta}^* = \mathop{\text{argmin}}<em k_sim_n_boldsymbol_x="k\sim[n],\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t_k)\Vert \boldsymbol{f}</em>}}(\boldsymbol{x<em _bar_boldsymbol_theta="\bar{\boldsymbol{\theta">{t_k}, t_k) - \boldsymbol{f}</em>}}}(\boldsymbol{x<em k-1="k-1">{t</em>}}, t_{k-1})\Vert^2\big]\label{eq:loss-4}\end{equation
其中$\bar{\boldsymbol{\theta}}$是$\boldsymbol{\theta}$的EMA。这就是原论文中的“一致性训练（Consistency Training，CT）”。从实践上来看，我们也可以将$\Vert\cdot - \cdot\Vert^2$换成更一般的度量$d(\cdot, \cdot)$，以更贴合数据特性。</p>
<h2 id="_5">采样分析<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>由于我们是从ReFlow出发一步步“等价变换”过来的，所以训练完成后一种基本的采样方式就是跟ReFlow一样求解ODE<br />
\begin{equation}d\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t/dt = \boldsymbol{v}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, t) = \frac{\boldsymbol{x}_t - \boldsymbol{f}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, t)}{t}\label{eq:ode}\end{equation}<br />
当然，如果费那么大劲得到的是跟ReFlow一样的结果，那么就纯粹是瞎折腾了。幸运的是，一致性训练所得的模型，有一个重要的优势是可以使用更大的采样步长——甚至等于1的步长，这就可以实现单步生成：<br />
\begin{equation}\boldsymbol{x}_0 = \boldsymbol{x}_1 - \boldsymbol{v}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1, 1)\times 1 = \boldsymbol{f}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1, 1)\end{equation}<br />
理由是<br />
\begin{equation}\begin{aligned}
\Vert\boldsymbol{f}</em>^<em>}(\boldsymbol{x}<em k="1">1, 1) - \boldsymbol{x}_0\Vert =&amp;\, \left\Vert\sum</em>^}^n \Big[\boldsymbol{f}_{\boldsymbol{\theta</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t_k}, t_k) - \boldsymbol{f}</em>^<em>}(\boldsymbol{x}<em k-1="k-1">{t</em>)\Big]\right\Vert \\[5pt]}}, t_{k-1
\leq&amp;\, \sum_{k=1}^n \Vert\boldsymbol{f}_{\boldsymbol{\theta}^</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">{t_k}, t_k) - \boldsymbol{f}</em>^<em>}(\boldsymbol{x}<em k-1="k-1">{t</em>)\Vert \\}}, t_{k-1
\end{aligned}\label{eq:f-x1-x0}\end{equation}<br />
可以看到，一致性训练相当于在优化$\Vert\boldsymbol{f}_{\boldsymbol{\theta}^</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1, 1) - \boldsymbol{x}_0\Vert$的上界，当损失足够小时，意味着$\Vert\boldsymbol{f}</em>_0\Vert$也足够小，因此可以一步生成。}^*}(\boldsymbol{x}_1, 1) - \boldsymbol{x</p>
<p>可$\Vert\boldsymbol{f}<em t_k="t_k">{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1, 1) - \boldsymbol{x}_0\Vert$是原本ReFlow的训练目标，为什么直接优化它会不如优化它的上界呢？这又回到了“轨迹交叉”的问题了，直接训练的话，$\boldsymbol{x}_0,\boldsymbol{x}_1$都是随机采样的，没有一一配对关系，所以无法直接训练出一步生成模型。但训练上界的话，通过多个$\boldsymbol{f}</em>^</em>}(\boldsymbol{x}</em>}, t_k),\boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}^*}(\boldsymbol{x}</em>_1$的配对。}}, t_{k-1})$的传递性，隐含地实现了$\boldsymbol{x}_0,\boldsymbol{x</p>
<p>如果单步生成的效果不能让我们满意，我们也可以增加采样步数来提高生成质量，这里边又有两种思路：1、用更小的步长来数值求解$\eqref{eq:ode}$；2、转化为类似SDE的随机迭代。前者比较常规，我们主要讨论后者。</p>
<p>首先注意到式$\eqref{eq:f-x1-x0}$中的$\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1, 1)$换成任意$\boldsymbol{f}</em>^</em>}(\boldsymbol{x}_t, t)$，也可以得到类似的不等关系，这意味着任意的$\boldsymbol{f}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t, t)$预测的都是$\boldsymbol{x}_0$，这样一来，我们从$\boldsymbol{x}_1$出发，通过$\boldsymbol{f}</em>^</em>}(\boldsymbol{x}<em t__n-1="t_{n-1">1, 1)$就得到一个初步的$\boldsymbol{x}_0$，但可能不够完美，于是我们通过加噪来“掩饰”这种不完美，得到一个$\boldsymbol{x}</em>}}$，代入$\boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}^<em>}(\boldsymbol{x}<em n-1="n-1">{t</em>)$得到一个更好一点的结果，依此类推：}}, t_{n-1<br />
\begin{equation}\begin{aligned}
&amp;\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}) \\
&amp;\boldsymbol{x}_0\leftarrow \boldsymbol{f}</em>^</em>}(\boldsymbol{x}_1, 1) \\
&amp;\text{for }k=n-1,n-2,\cdots,1: \\
&amp;\qquad \boldsymbol{z} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I}) \\
&amp;\qquad \boldsymbol{x}</em>} \leftarrow (1 - t_k)\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">0 + t_k\boldsymbol{z} \\
&amp;\qquad \boldsymbol{x}_0\leftarrow \boldsymbol{f}</em>, t_k)}^*}(\boldsymbol{x}_{t_k
\end{aligned}\end{equation}</p>
<h2 id="_6">用于蒸馏<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>一致性模型的训练思想同样可以用于现成扩散模型的蒸馏，结果称为“一致性蒸馏（Consistency Distillation，CD）”，方法是把式$\eqref{eq:loss-4}$中$\boldsymbol{f}<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k)$的学习目标由$\boldsymbol{f<em t__k-1="t_{k-1">{\bar{\boldsymbol{\theta}}}(\boldsymbol{x}</em>}}, t_{k-1})$换成$\boldsymbol{f<em t__k-1="t_{k-1">{\bar{\boldsymbol{\theta}}}(\hat{\boldsymbol{x}}</em>^}}^{\boldsymbol{\varphi<em>}, t_{k-1})$：<br />
\begin{equation}\boldsymbol{\theta}^</em> = \mathop{\text{argmin}}<em k_sim_n_boldsymbol_x="k\sim[n],\boldsymbol{x">{\boldsymbol{\theta}} \mathbb{E}</em><em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_1\sim p_1(\boldsymbol{x}_1)}\big[\tilde{w}(t_k)\Vert \boldsymbol{f}</em>}}(\boldsymbol{x<em _bar_boldsymbol_theta="\bar{\boldsymbol{\theta">{t_k}, t_k) - \boldsymbol{f}</em>}}}(\hat{\boldsymbol{x}<em k-1="k-1">{t</em>^}}^{\boldsymbol{\varphi<em>}, t_{k-1})\Vert^2\big]\label{eq:loss-5}\end{equation}<br />
其中$\hat{\boldsymbol{x}}<em k-1="k-1">{t</em>^}}^{\boldsymbol{\varphi</em>}$是由教师扩散模型以$\boldsymbol{x}<em t__k-1="t_{k-1">{t_k}$为初值所预测的$\boldsymbol{x}</em>$，比如最简单的欧拉求解器，我们有}<br />
\begin{equation}\hat{\boldsymbol{x}}<em k-1="k-1">{t</em>^}}^{\boldsymbol{\varphi<em>} \approx \boldsymbol{x}<em k-1="k-1">{t_k} - (t_k - t</em>^})\boldsymbol{v}_{\boldsymbol{\varphi</em>}(\boldsymbol{x}_{t_k}, t_k)\end{equation}<br />
这样做的理由也很简单，如果有了预训练好的扩散模型，那么我们就没必要在直线$\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$上找学习目标了，因为这是人为定义的，终究有交叉的风险，而是改为由预训练好扩散模型来预测轨迹，这样找出来的学习目标可能并不一定是“最直”的，但肯定不会有交叉。</p>
<p>如果不计成本，我们也可以从随机采样的$\boldsymbol{x}_1$出发，加上预训练扩散模型解出的$\boldsymbol{x}_0$，通过成对的$(\boldsymbol{x}_0,\boldsymbol{x}_1)$来构建学习目标，这差不多就是ReFlow的蒸馏思路，缺点是必须对教师模型运行完整的采样过程，费时费力。相比之下，一致性蒸馏只需要运行单步教师模型，计算成本更低。</p>
<p>不过，一致性蒸馏在蒸馏过程中还需要真实样本，这在某些场景下也是一个缺点。如果蒸馏过程既不想运行完整的教师模型采样，又不想提供真实数据，那么有一个选择就是我们之前介绍过的<a href="/archives/10085">SiD</a>，代价是模型的推导更加复杂了。</p>
<h2 id="_7">文章小结<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>本文通过逐步解构和优化ReFLow训练流程的方式，提供了一个从ReFlow逐渐过渡到一致性模型（Consistency Models）的直观理解路径。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10633">https://spaces.ac.cn/archives/10633</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Dec. 18, 2024). 《生成扩散模型漫谈（二十八）：分步理解一致性模型 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10633">https://spaces.ac.cn/archives/10633</a></p>
<p>@online{kexuefm-10633,<br />
title={生成扩散模型漫谈（二十八）：分步理解一致性模型},<br />
author={苏剑林},<br />
year={2024},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/10633}},<br />
} </p>
<hr />
<h2 id="_8">推导<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<h3 id="_9">一致性模型的数学定义<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<p>一致性模型的核心思想是构建一个函数$\boldsymbol{f}_{\boldsymbol{\theta}}$，它能够将任意时刻$t$的噪声样本$\boldsymbol{x}_t$映射回干净数据$\boldsymbol{x}_0$。我们首先给出一致性模型的严格数学定义。</p>
<p><strong>定义1（一致性函数）</strong>：给定一个概率流ODE
\begin{equation}
\frac{d\boldsymbol{x}<em t_in_0_T_="t\in[0,T]">t}{dt} = \boldsymbol{v}(\boldsymbol{x}_t, t), \quad t \in [0, T]
\end{equation}
其轨迹为${\boldsymbol{x}_t}</em>^d$满足以下性质：}$，一致性函数$\boldsymbol{f}: (\mathbb{R}^d, \mathbb{R}_+) \to \mathbb{R</p>
<ol>
<li>
<p><strong>自洽性（Self-Consistency）</strong>：对于ODE轨迹上的任意两个点$\boldsymbol{x}<em t_="t'">t$和$\boldsymbol{x}</em>$（$t, t' &gt; 0$），有
\begin{equation}
\boldsymbol{f}(\boldsymbol{x}<em t_="t'">t, t) = \boldsymbol{f}(\boldsymbol{x}</em>, t')
\end{equation}</p>
</li>
<li>
<p><strong>边界条件（Boundary Condition）</strong>：当$t \to 0$时，有
\begin{equation}
\boldsymbol{f}(\boldsymbol{x}_0, 0) = \boldsymbol{x}_0
\end{equation}</p>
</li>
</ol>
<p>这两个性质确保了一致性函数能够将ODE轨迹上的任意点映射到同一个终点$\boldsymbol{x}_0$。</p>
<p><strong>命题1（一致性函数的存在性）</strong>：对于任意满足Lipschitz连续条件的概率流ODE，一致性函数存在且唯一。</p>
<p><strong>证明</strong>：对于给定的初值$\boldsymbol{x}<em s_in_0_t_="s\in[0,t]">t$，根据ODE理论，存在唯一的轨迹${\boldsymbol{x}_s}</em>$满足
\begin{equation}
\boldsymbol{x}<em>s = \boldsymbol{x}_t + \int_t^s \boldsymbol{v}(\boldsymbol{x}</em>\tau, \tau) d\tau
\end{equation}
定义$\boldsymbol{f}(\boldsymbol{x}_t, t) = \boldsymbol{x}_0$，其中$\boldsymbol{x}_0$是该轨迹在$s=0$时的取值。由ODE解的唯一性，这个定义是良定义的。自洽性由ODE轨迹的连续性保证，边界条件显然成立。$\square$</p>
<h3 id="ode">从概率流ODE到一致性模型<a class="toc-link" href="#ode" title="Permanent link">&para;</a></h3>
<p>我们从扩散过程的概率流ODE出发。考虑标准的扩散过程，其前向过程为
\begin{equation}
d\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t = \boldsymbol{f}(t)\boldsymbol{x}_t dt + g(t) d\boldsymbol{w}_t
\end{equation}
对应的概率流ODE为
\begin{equation}
\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{f}(t)\boldsymbol{x}_t - \frac{1}{2}g(t)^2 \nabla</em>_t)
\end{equation}}_t} \log p_t(\boldsymbol{x</p>
<p>对于ReFlow的特殊情况，我们有$\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$，因此
\begin{equation}
\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{x}_1 - \boldsymbol{x}_0
\end{equation}</p>
<p><strong>命题2（ReFlow的一致性函数）</strong>：对于ReFlow的噪声调度$\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$，理想的一致性函数为
\begin{equation}
\boldsymbol{f}(\boldsymbol{x}_t, t) = \boldsymbol{x}_t - t(\boldsymbol{x}_1 - \boldsymbol{x}_0) = \boldsymbol{x}_0
\end{equation}</p>
<p><strong>证明</strong>：从$\boldsymbol{x}_t$到$\boldsymbol{x}_0$的轨迹满足
\begin{align}
\boldsymbol{x}_s &amp;= \boldsymbol{x}_t + \int_t^s (\boldsymbol{x}_1 - \boldsymbol{x}_0) d\tau \
&amp;= \boldsymbol{x}_t + (s - t)(\boldsymbol{x}_1 - \boldsymbol{x}_0) \
&amp;= (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1 + (s-t)(\boldsymbol{x}_1 - \boldsymbol{x}_0) \
&amp;= (1-s)\boldsymbol{x}_0 + s\boldsymbol{x}_1
\end{align}
当$s=0$时，$\boldsymbol{x}_0 = \boldsymbol{x}_t - t(\boldsymbol{x}_1 - \boldsymbol{x}_0)$。$\square$</p>
<p>由于$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">1 - \boldsymbol{x}_0 = \boldsymbol{v}(\boldsymbol{x}_t, t)$是我们通过神经网络学习的速度场，因此
\begin{equation}
\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) = \boldsymbol{x}_t - t\boldsymbol{v}</em>_t, t)
\end{equation}}}(\boldsymbol{x</p>
<p>这个形式自然满足边界条件$\boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{x}_0, 0) = \boldsymbol{x}_0$。</p>
<h3 id="_10">一致性函数的关键性质<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<p><strong>性质1（Lipschitz连续性）</strong>：如果速度场$\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}, t)$关于$\boldsymbol{x}$是$L_v$-Lipschitz连续的，则一致性函数$\boldsymbol{f}</em>$是$(1 + tL_v)$-Lipschitz连续的。}}(\boldsymbol{x}, t)$关于$\boldsymbol{x</p>
<p><strong>证明</strong>：对于任意$\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^d$，
\begin{align}
|\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}, t) - \boldsymbol{f}</em>}}(\boldsymbol{y}, t)| &amp;= |\boldsymbol{x} - t\boldsymbol{v<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}, t) - \boldsymbol{y} + t\boldsymbol{v}</em>, t)| \
&amp;\leq |\boldsymbol{x} - \boldsymbol{y}| + t|\boldsymbol{v}}}(\boldsymbol{y<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}, t) - \boldsymbol{v}</em>, t)| \
&amp;\leq |\boldsymbol{x} - \boldsymbol{y}| + tL_v|\boldsymbol{x} - \boldsymbol{y}| \
&amp;= (1 + tL_v)|\boldsymbol{x} - \boldsymbol{y}|
\end{align}
$\square$}}(\boldsymbol{y</p>
<p><strong>性质2（时间可微性）</strong>：如果$\boldsymbol{v}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}$关于$t$可微，则
\begin{equation}
\frac{\partial \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)}{\partial t} = -\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) - t\frac{\partial \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t, t)}{\partial t} + t\nabla</em>}} \boldsymbol{v<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \boldsymbol{v}</em>_t, t)
\end{equation}}}(\boldsymbol{x</p>
<p><strong>证明</strong>：使用链式法则，
\begin{align}
\frac{\partial \boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)}{\partial t} &amp;= \frac{\partial}{\partial t}\left[\boldsymbol{x}_t - t\boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)\right] \
&amp;= \frac{\partial \boldsymbol{x}_t}{\partial t} - \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) - t\frac{\partial \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)}{\partial \boldsymbol{x}_t}\frac{\partial \boldsymbol{x}_t}{\partial t} - t\frac{\partial \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)}{\partial t} \
&amp;= \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) - \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t, t) - t\nabla</em>}} \boldsymbol{v<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t) - t\frac{\partial \boldsymbol{v}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)}{\partial t}
\end{align}
其中使用了$\frac{\partial \boldsymbol{x}_t}{\partial t} = \boldsymbol{v}</em>_t, t)$。$\square$}}(\boldsymbol{x</p>
<h3 id="cd">一致性蒸馏（CD）的完整推导<a class="toc-link" href="#cd" title="Permanent link">&para;</a></h3>
<p>一致性蒸馏的目标是从预训练的扩散模型$\boldsymbol{v}_{\boldsymbol{\varphi}^*}$中蒸馏出一致性模型。我们给出完整的理论推导。</p>
<p><strong>定理1（一致性蒸馏的损失函数）</strong>：给定预训练的速度场$\boldsymbol{v}<em t="t" t-_Delta="t-\Delta">{\boldsymbol{\varphi}^<em>}$，一致性蒸馏的最优目标为
\begin{equation}
\mathcal{L}<em t_sim_mathcal_U="t\sim\mathcal{U">{CD}(\boldsymbol{\theta}) = \mathbb{E}</em>}[0,1], \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0, \boldsymbol{x}_1\sim p_1}\left[w(t)d\left(\boldsymbol{f}</em>}}(\boldsymbol{x<em _text_sg="\text{sg">t, t), \boldsymbol{f}</em>^}(\boldsymbol{\theta})}(\hat{\boldsymbol{x}}_{t-\Delta t}^{\boldsymbol{\varphi</em>}, t-\Delta t)\right)\right]
\end{equation}
其中$\text{sg}$表示stop-gradient操作，$\hat{\boldsymbol{x}}</em>}^{\boldsymbol{\varphi}^*}$是从$\boldsymbol{x<em t="t" t-_Delta="t-\Delta">t$出发用教师模型预测的$\boldsymbol{x}</em>$。</p>
<p><strong>证明</strong>：我们的目标是使$\boldsymbol{f}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{\theta}}$在ODE轨迹上保持一致。对于ODE
\begin{equation}
\frac{d\boldsymbol{x}_t}{dt} = \boldsymbol{v}</em>^<em>}(\boldsymbol{x}<em t="t" t-_Delta="t-\Delta">t, t)
\end{equation}
从$\boldsymbol{x}_t$到$\boldsymbol{x}</em>$的单步Euler更新为
\begin{equation}
\hat{\boldsymbol{x}}_{t-\Delta t}^{\boldsymbol{\varphi}^</em>} = \boldsymbol{x}<em _boldsymbol_varphi="\boldsymbol{\varphi">t - \Delta t \cdot \boldsymbol{v}</em>_t, t) + O(\Delta t^2)
\end{equation}}^*}(\boldsymbol{x</p>
<p>由一致性条件，我们要求
\begin{equation}
\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \approx \boldsymbol{f}</em>, t-\Delta t)
\end{equation}}}(\hat{\boldsymbol{x}}_{t-\Delta t}^{\boldsymbol{\varphi}^*</p>
<p>为了稳定训练，我们对目标使用EMA或stop-gradient：
\begin{equation}
\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{CD}(\boldsymbol{\theta}) = \mathbb{E}\left[w(t)d\left(\boldsymbol{f}</em>}}(\boldsymbol{x<em _bar_boldsymbol_theta="\bar{\boldsymbol{\theta">t, t), \boldsymbol{f}</em>, t-\Delta t)\right)\right]
\end{equation}
其中$\bar{\boldsymbol{\theta}}$是$\boldsymbol{\theta}$的EMA。$\square$}}}(\hat{\boldsymbol{x}}_{t-\Delta t}^{\boldsymbol{\varphi}^*</p>
<p><strong>推论1（离散化误差）</strong>：当使用$n$步离散化时，一致性蒸馏的累积误差为
\begin{equation}
|\boldsymbol{f}<em k="1">{\boldsymbol{\theta}}(\boldsymbol{x}_1, 1) - \boldsymbol{x}_0| \leq \sum</em>}^n |\boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>)
\end{equation}}}, t_{k-1})| + O(n^{-1</p>
<p><strong>证明</strong>：使用三角不等式，
\begin{align}
|\boldsymbol{f}<em k="1">{\boldsymbol{\theta}}(\boldsymbol{x}_1, 1) - \boldsymbol{x}_0| &amp;= \left|\sum</em>}^n [\boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>)]\right| \
&amp;\leq \sum_{k=1}^n |\boldsymbol{f}}}, t_{k-1<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>)|
\end{align}}}, t_{k-1</p>
<p>每个单步误差包含：
1. 模型预测误差：$|\boldsymbol{f}<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\hat{\boldsymbol{x}}</em>)|$
2. 数值离散化误差：$O(\Delta t^2) = O(n^{-2})$}}, t_{k-1</p>
<p>总离散化误差为$n \cdot O(n^{-2}) = O(n^{-1})$。$\square$</p>
<h3 id="ct">一致性训练（CT）的数学原理<a class="toc-link" href="#ct" title="Permanent link">&para;</a></h3>
<p>与一致性蒸馏不同，一致性训练不需要预训练的教师模型，而是直接从数据学习。</p>
<p><strong>定理2（一致性训练的损失函数）</strong>：一致性训练的目标为
\begin{equation}
\mathcal{L}<em k_sim_text_Unif="k\sim\text{Unif">{CT}(\boldsymbol{\theta}) = \mathbb{E}</em>}[1,n], \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">0\sim p_0, \boldsymbol{x}_1\sim p_1}\left[w(t_k)d\left(\boldsymbol{f}</em>}}(\boldsymbol{x<em _bar_boldsymbol_theta="\bar{\boldsymbol{\theta">{t_k}, t_k), \boldsymbol{f}</em>}}}(\boldsymbol{x<em k-1="k-1">{t</em>)\right)\right]
\end{equation}
其中$\boldsymbol{x}_{t_k} = (1-t_k)\boldsymbol{x}_0 + t_k\boldsymbol{x}_1$，$\bar{\boldsymbol{\theta}}$是$\boldsymbol{\theta}$的EMA。}}, t_{k-1</p>
<p><strong>关键观察</strong>：这个损失函数的巧妙之处在于：
1. $\boldsymbol{x}<em t__k-1="t_{k-1">{t_k}$和$\boldsymbol{x}</em>_1)$，因此它们在同一条ODE轨迹上
2. 通过自监督的方式，强制模型在这条轨迹上保持一致}}$共享同一对$(\boldsymbol{x}_0, \boldsymbol{x</p>
<p><strong>命题3（CT损失的梯度）</strong>：一致性训练损失的梯度为
\begin{equation}
\nabla_{\boldsymbol{\theta}} \mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{CT} = \mathbb{E}\left[w(t_k) \nabla</em>}} \boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k)^T \nabla d\left(\boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k), \boldsymbol{f<em t__k-1="t_{k-1">{\bar{\boldsymbol{\theta}}}(\boldsymbol{x}</em>)\right)\right]
\end{equation}}}, t_{k-1</p>
<p>对于$L^2$距离度量$d(\boldsymbol{a}, \boldsymbol{b}) = |\boldsymbol{a} - \boldsymbol{b}|^2$，梯度简化为
\begin{equation}
\nabla_{\boldsymbol{\theta}} \mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{CT} = 2\mathbb{E}\left[w(t_k) \nabla</em>}} \boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k)^T \left(\boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\bar{\boldsymbol{\theta}}}(\boldsymbol{x}</em>)\right)\right]
\end{equation}}}, t_{k-1</p>
<p><strong>定理3（CT的收敛性）</strong>：假设$\boldsymbol{f}<em _min="\min">{\boldsymbol{\theta}}$是$L_f$-Lipschitz连续的，权重满足$w(t) \geq w</em>)$的速率收敛到局部最优。} &gt; 0$，且学习率适当选择，则一致性训练算法以$O(1/\sqrt{T</p>
<p><strong>证明（梗概）</strong>：关键是证明损失函数$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{CT}$满足：
1. <strong>梯度有界性</strong>：$|\nabla</em>}} \mathcal{L<em CT="CT">{CT}| \leq G$对某个常数$G$成立
2. <strong>光滑性</strong>：$\mathcal{L}</em>}$是$L$-光滑的，即$|\nabla_{\boldsymbol{\theta}} \mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">{CT}(\boldsymbol{\theta}) - \nabla</em>'|$}} \mathcal{L}_{CT}(\boldsymbol{\theta}')| \leq L|\boldsymbol{\theta} - \boldsymbol{\theta</p>
<p>梯度有界性由Lipschitz连续性保证：
\begin{align}
|\nabla_{\boldsymbol{\theta}} \mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{CT}| &amp;\leq 2\mathbb{E}\left[w(t_k) |\nabla</em>}} \boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k)| \cdot |\boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\bar{\boldsymbol{\theta}}}(\boldsymbol{x}</em>)|\right] \
&amp;\leq 2w_{\max} L_f \mathbb{E}\left[|\boldsymbol{f}}}, t_{k-1<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\bar{\boldsymbol{\theta}}}(\boldsymbol{x}</em>)|\right]
\end{align}}}, t_{k-1</p>
<p>应用标准的随机梯度下降收敛理论，得到$O(1/\sqrt{T})$的收敛速率。$\square$</p>
<h3 id="_11">轨迹交叉问题的数学分析<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<p>轨迹交叉是影响一致性模型性能的关键问题。我们给出定量分析。</p>
<p><strong>定义2（轨迹交叉度）</strong>：对于两对不同的初始值$(\boldsymbol{x}_0^{(1)}, \boldsymbol{x}_1^{(1)})$和$(\boldsymbol{x}_0^{(2)}, \boldsymbol{x}_1^{(2)})$，定义在时刻$t$的轨迹交叉度为
\begin{equation}
\rho(t) = \frac{|\boldsymbol{x}_t^{(1)} - \boldsymbol{x}_t^{(2)}|}{|\boldsymbol{x}_0^{(1)} - \boldsymbol{x}_0^{(2)}|}
\end{equation}</p>
<p><strong>命题4（ReFlow的轨迹交叉）</strong>：对于ReFlow的噪声调度$\boldsymbol{x}_t = (1-t)\boldsymbol{x}_0 + t\boldsymbol{x}_1$，轨迹交叉度为
\begin{equation}
\rho(t) = \frac{|(1-t)(\boldsymbol{x}_0^{(1)} - \boldsymbol{x}_0^{(2)}) + t(\boldsymbol{x}_1^{(1)} - \boldsymbol{x}_1^{(2)})|}{|\boldsymbol{x}_0^{(1)} - \boldsymbol{x}_0^{(2)}|}
\end{equation}</p>
<p>当$\boldsymbol{x}_1$独立同分布时，期望交叉度为
\begin{equation}
\mathbb{E}[\rho(t)] \approx 1 - t + O(t^2)
\end{equation}
这表明当$t$增大时，轨迹间的相对距离会减小，导致交叉风险增加。</p>
<p><strong>定理4（一致性训练减少轨迹交叉）</strong>：使用一致性训练的分步目标，有效交叉度降低为
\begin{equation}
\tilde{\rho}<em _boldsymbol_theta="\boldsymbol{\theta">k = \frac{|\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_k}^{(1)}, t_k) - \boldsymbol{f}</em>
\end{equation}
当$k$较小时，$\tilde{\rho}_k \approx 1$，避免了轨迹交叉问题。}}(\boldsymbol{x}_{t_k}^{(2)}, t_k)|}{|\boldsymbol{x}_0^{(1)} - \boldsymbol{x}_0^{(2)}|</p>
<p><strong>证明</strong>：对于相邻时刻$t_{k-1}$和$t_k$，由一致性训练目标
\begin{equation}
\boldsymbol{f}<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) \approx \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>)
\end{equation}}}, t_{k-1</p>
<p>归纳地，从$k=1$开始：
\begin{align}
\boldsymbol{f}<em t_1="t_1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_1) &amp;\approx \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">0 \
\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_2}, t_2) &amp;\approx \boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_1}, t_1) \approx \boldsymbol{x}_0 \
&amp;\vdots \
\boldsymbol{f}</em>_0
\end{align}}}(\boldsymbol{x}_{t_k}, t_k) &amp;\approx \boldsymbol{x</p>
<p>因此
\begin{align}
\tilde{\rho}<em _boldsymbol_theta="\boldsymbol{\theta">k &amp;= \frac{|\boldsymbol{f}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t_k}^{(1)}, t_k) - \boldsymbol{f}</em> \
&amp;\approx \frac{|\boldsymbol{x}_0^{(1)} - \boldsymbol{x}_0^{(2)}|}{|\boldsymbol{x}_0^{(1)} - \boldsymbol{x}_0^{(2)}|} = 1
\end{align}
$\square$}}(\boldsymbol{x}_{t_k}^{(2)}, t_k)|}{|\boldsymbol{x}_0^{(1)} - \boldsymbol{x}_0^{(2)}|</p>
<h3 id="_12">自洽性损失函数的深入分析<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<p>自洽性是一致性模型的核心约束。我们分析不同形式的自洽性损失。</p>
<p><strong>距离度量的选择</strong>：常用的距离度量包括：</p>
<ol>
<li>
<p><strong>$L^2$距离</strong>：
\begin{equation}
d_{L^2}(\boldsymbol{a}, \boldsymbol{b}) = |\boldsymbol{a} - \boldsymbol{b}|<em i="1">2^2 = \sum</em>^d (a_i - b_i)^2
\end{equation}</p>
</li>
<li>
<p><strong>$L^1$距离</strong>：
\begin{equation}
d_{L^1}(\boldsymbol{a}, \boldsymbol{b}) = |\boldsymbol{a} - \boldsymbol{b}|<em i="1">1 = \sum</em>^d |a_i - b_i|
\end{equation}</p>
</li>
<li>
<p><strong>感知损失（Perceptual Loss）</strong>：对于图像数据，使用预训练网络$\Phi$的特征距离
\begin{equation}
d_{\text{perc}}(\boldsymbol{a}, \boldsymbol{b}) = \sum_{\ell} w_\ell |\Phi_\ell(\boldsymbol{a}) - \Phi_\ell(\boldsymbol{b})|<em>2^2
\end{equation}
其中$\Phi</em>\ell$表示第$\ell$层的特征提取器。</p>
</li>
</ol>
<p><strong>命题5（距离度量的影响）</strong>：不同的距离度量对一致性训练的影响体现在梯度的尺度和方向上。对于$L^2$距离，梯度为
\begin{equation}
\nabla_{\boldsymbol{f}} d_{L^2}(\boldsymbol{f}, \boldsymbol{g}) = 2(\boldsymbol{f} - \boldsymbol{g})
\end{equation}
对于$L^1$距离，梯度为
\begin{equation}
\nabla_{\boldsymbol{f}} d_{L^1}(\boldsymbol{f}, \boldsymbol{g}) = \text{sign}(\boldsymbol{f} - \boldsymbol{g})
\end{equation}</p>
<p>$L^1$距离的梯度更稳定，对异常值更鲁棒，但在原点处不可微；$L^2$距离在优化上更平滑。</p>
<p><strong>权重函数$w(t)$的设计</strong>：权重函数$w(t)$控制不同时刻的重要性。常见选择包括：</p>
<ol>
<li><strong>均匀权重</strong>：$w(t) = 1$</li>
<li><strong>时间相关权重</strong>：$w(t) = t^2$或$w(t) = 1/t^2$</li>
<li><strong>信噪比相关权重</strong>：$w(t) = \text{SNR}(t) = \frac{(1-t)^2}{t^2}$</li>
</ol>
<p><strong>定理5（最优权重函数）</strong>：在最小化KL散度的意义下，最优权重函数为
\begin{equation}
w^*(t) = \frac{p_t(\boldsymbol{x}<em KL="KL">t)}{p_0(\boldsymbol{x}_0)} \cdot \frac{\partial^2}{\partial t^2} D</em>(p_t | p_0)
\end{equation}
但这在实践中难以计算。</p>
<h3 id="_13">单步生成的理论保证<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<p>单步生成是一致性模型最吸引人的特性。我们给出理论保证。</p>
<p><strong>定理6（单步生成误差界）</strong>：假设一致性训练达到$\epsilon$-最优，即
\begin{equation}
\mathbb{E}\left[|\boldsymbol{f}<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>)|^2\right] \leq \epsilon^2
\end{equation}
对所有$k=1,\ldots,n$成立，则单步生成的误差满足
\begin{equation}
\mathbb{E}\left[|\boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{x}_1, 1) - \boldsymbol{x}_0|^2\right] \leq n\epsilon^2
\end{equation}}}, t_{k-1</p>
<p><strong>证明</strong>：利用三角不等式和Jensen不等式，
\begin{align}
\mathbb{E}\left[|\boldsymbol{f}<em k="1">{\boldsymbol{\theta}}(\boldsymbol{x}_1, 1) - \boldsymbol{x}_0|^2\right] &amp;= \mathbb{E}\left[\left|\sum</em>}^n [\boldsymbol{f<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>)]\right|^2\right] \
&amp;\leq \mathbb{E}\left[\left(\sum_{k=1}^n |\boldsymbol{f}}}, t_{k-1<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>)|\right)^2\right] \
&amp;\leq n \sum_{k=1}^n \mathbb{E}\left[|\boldsymbol{f}}}, t_{k-1<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k) - \boldsymbol{f<em t__k-1="t_{k-1">{\boldsymbol{\theta}}(\boldsymbol{x}</em>)|^2\right] \
&amp;\leq n \cdot n\epsilon^2 = n^2\epsilon^2
\end{align}}}, t_{k-1</p>
<p>更紧的界可以通过假设相邻步间的误差独立得到：
\begin{equation}
\mathbb{E}\left[|\boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{x}_1, 1) - \boldsymbol{x}_0|^2\right] \leq n\epsilon^2
\end{equation}
$\square$</p>
<p><strong>推论2（采样质量与离散化步数）</strong>：为了达到目标误差$\delta$，需要的离散化步数为
\begin{equation}
n = \Theta\left(\frac{\epsilon^2}{\delta^2}\right)
\end{equation}
这表明，训练时使用更多的离散化步数（更小的$\epsilon$）可以在推理时实现更好的单步生成质量。</p>
<h3 id="_14">数值稳定性分析<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h3>
<p>数值稳定性是实际训练中的关键问题。</p>
<p><strong>问题1（梯度爆炸）</strong>：当$t$接近0时，$\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) = \boldsymbol{x}_t - t\boldsymbol{v}</em>_t, t)$中的$1/t$项可能导致数值不稳定。}}(\boldsymbol{x</p>
<p><strong>解决方案1（跳过边界）</strong>：在训练时，避免采样$t$过小的值，通常设置$t_{\min} &gt; 0$（例如$t_{\min} = 0.001$）。</p>
<p><strong>问题2（EMA参数的选择）</strong>：EMA更新为
\begin{equation}
\bar{\boldsymbol{\theta}}<em t_1="t+1">{t+1} = \mu \bar{\boldsymbol{\theta}}_t + (1-\mu)\boldsymbol{\theta}</em>
\end{equation}
其中$\mu \in (0, 1)$是动量系数。</p>
<p><strong>命题6（EMA的有效时间窗口）</strong>：EMA参数的有效时间窗口为
\begin{equation}
T_{\text{eff}} = \frac{1}{1-\mu}
\end{equation}
这意味着$\bar{\boldsymbol{\theta}}<em _text_eff="\text{eff">t$主要由过去$T</em>$步的参数决定。}</p>
<p><strong>证明</strong>：考虑EMA展开式
\begin{equation}
\bar{\boldsymbol{\theta}}<em k="0">t = (1-\mu)\sum</em>
\end{equation}
权重$\mu^k$在$k \approx 1/(1-\mu)$时衰减到$1/e$，定义为有效时间窗口。$\square$}^{t-1} \mu^k \boldsymbol{\theta}_{t-k</p>
<p>实践中，通常使用自适应的$\mu$：
\begin{equation}
\mu(t) = \min\left{0.999, \frac{1 + t/1000}{1 + t/900}\right}
\end{equation}
在训练初期使用较小的$\mu$，随后逐渐增大。</p>
<p><strong>问题3（离散化步数的选择）</strong>：训练时使用$n$步离散化，推理时可以使用$m$步（$m \leq n$）。</p>
<p><strong>定理7（多步推理的误差界）</strong>：如果训练时使用$n$步离散化，推理时使用$m$步（$m &lt; n$），则误差界为
\begin{equation}
\mathbb{E}\left[|\boldsymbol{f}_{\boldsymbol{\theta}}(\boldsymbol{x}_1, 1) - \boldsymbol{x}_0|^2\right] \leq m\epsilon^2 + C\left(\frac{m}{n}\right)
\end{equation}
其中第一项是一致性误差，第二项是离散化误差。</p>
<h3 id="_15">与其他扩散模型的关系<a class="toc-link" href="#_15" title="Permanent link">&para;</a></h3>
<p><strong>命题7（一致性模型与DDIM的关系）</strong>：DDIM的确定性采样可以写为
\begin{equation}
\boldsymbol{x}<em k-1="k-1">{t</em>}} = \sqrt{\alpha_{t_{k-1}}}\underbrace{\frac{\boldsymbol{x<em t_k="t_k">{t_k} - \sqrt{1-\alpha</em>}}\boldsymbol{\epsilon<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>}, t_k)}{\sqrt{\alpha_{t_k}}}<em t__k-1="t_{k-1">{\hat{\boldsymbol{x}}_0} + \sqrt{1-\alpha</em>}}}\boldsymbol{\epsilon<em t_k="t_k">{\boldsymbol{\theta}}(\boldsymbol{x}</em>, t_k)
\end{equation}</p>
<p>定义$\boldsymbol{f}<em t_k="t_k">{\text{DDIM}}(\boldsymbol{x}</em>}, t_k) = \hat{\boldsymbol{x}<em _text_DDIM="\text{DDIM">0$，则DDIM隐式地构造了一个一致性函数。但与一致性模型不同的是，DDIM的$\boldsymbol{f}</em>$没有显式地通过自洽性损失训练。}</p>
<p><strong>命题8（一致性模型与Score-based模型的关系）</strong>：Score-based模型学习得分函数$\boldsymbol{s}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) = \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t} \log p_t(\boldsymbol{x}_t)$，通过Tweedie公式，可以得到
\begin{equation}
\mathbb{E}[\boldsymbol{x}_0 | \boldsymbol{x}_t] = \boldsymbol{x}_t + \sigma_t^2 \boldsymbol{s}</em>_t, t)
\end{equation}}}(\boldsymbol{x</p>
<p>对于ReFlow的特殊情况，得分函数和一致性函数的关系为
\begin{equation}
\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \approx \mathbb{E}[\boldsymbol{x}_0 | \boldsymbol{x}_t] = \boldsymbol{x}_t + t^2 \boldsymbol{s}</em>_t, t)
\end{equation}}}(\boldsymbol{x</p>
<h3 id="_16">实际训练算法<a class="toc-link" href="#_16" title="Permanent link">&para;</a></h3>
<p>我们给出一致性训练和一致性蒸馏的完整伪代码。</p>
<p><strong>算法1：一致性训练（CT）</strong></p>
<div class="highlight"><pre><span></span><code>输入：数据分布 p_0，先验分布 p_1，离散化步数 n，EMA动量 μ，学习率 η
初始化：随机初始化 θ，设置 θ̄ = θ
for epoch = 1, 2, ... do
    采样 x_0 ~ p_0, x_1 ~ p_1
    均匀采样 k ~ Uniform{1, 2, ..., n}
    计算 t_k = k/n, t_{k-1} = (k-1)/n
    构造 x_{t_k} = (1 - t_k)x_0 + t_k x_1
    构造 x_{t_{k-1}} = (1 - t_{k-1})x_0 + t_{k-1} x_1

    计算一致性损失：
    L = w(t_k) || f_θ(x_{t_k}, t_k) - f_{θ̄}(x_{t_{k-1}}, t_{k-1}) ||²

    梯度更新：
    θ ← θ - η ∇_θ L

    EMA更新：
    θ̄ ← μθ̄ + (1-μ)θ
end for
输出：θ
</code></pre></div>

<p><strong>算法2：一致性蒸馏（CD）</strong></p>
<div class="highlight"><pre><span></span><code>输入：预训练模型 v_φ*，数据分布 p_0，先验分布 p_1，
      离散化步数 n，EMA动量 μ，学习率 η
初始化：随机初始化 θ（或从 v_φ* 初始化），设置 θ̄ = θ
for epoch = 1, 2, ... do
    采样 x_0 ~ p_0, x_1 ~ p_1
    均匀采样 k ~ Uniform{1, 2, ..., n}
    计算 t_k = k/n, t_{k-1} = (k-1)/n
    构造 x_{t_k} = (1 - t_k)x_0 + t_k x_1

    使用教师模型预测：
    x̂_{t_{k-1}} = x_{t_k} - (t_k - t_{k-1}) v_φ*(x_{t_k}, t_k)

    计算一致性损失：
    L = w(t_k) || f_θ(x_{t_k}, t_k) - f_{θ̄}(x̂_{t_{k-1}}, t_{k-1}) ||²

    梯度更新：
    θ ← θ - η ∇_θ L

    EMA更新：
    θ̄ ← μθ̄ + (1-μ)θ
end for
输出：θ
</code></pre></div>

<p><strong>算法3：多步采样</strong></p>
<div class="highlight"><pre><span></span><code>输入：一致性模型 f_θ*，采样步数 m，时间调度 {t̃_1, ..., t̃_m}
初始化：采样 x_1 ~ N(0, I)
x_0 ← f_θ*(x_1, 1)
for k = m-1, m-2, ..., 1 do
    采样 z ~ N(0, I)
    添加噪声：x_{t̃_k} ← (1 - t̃_k)x_0 + t̃_k z
    去噪：x_0 ← f_θ*(x_{t̃_k}, t̃_k)
end for
输出：x_0
</code></pre></div>

<p><strong>关键实现细节</strong>：</p>
<ol>
<li>
<p><strong>参数化技巧</strong>：为了确保边界条件$\boldsymbol{f}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{x}_0, 0) = \boldsymbol{x}_0$，常用的参数化方式为
\begin{equation}
\boldsymbol{f}</em>}}(\boldsymbol{x<em _text_skip="\text{skip">t, t) = c</em>}}(t)\boldsymbol{x<em _text_out="\text{out">t + c</em>}}(t)\boldsymbol{F<em _text_skip="\text{skip">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)
\end{equation}
其中$c</em>(0) = 0$。一个常用的选择是
\begin{equation}
c_{\text{skip}}(t) = \frac{\sigma_{\text{data}}^2}{(t-\epsilon)^2 + \sigma_{\text{data}}^2}, \quad c_{\text{out}}(t) = \frac{\sigma_{\text{data}}(t-\epsilon)}{\sqrt{\sigma_{\text{data}}^2 + t^2}}
\end{equation}
其中$\sigma_{\text{data}}$是数据的标准差，$\epsilon$是小的正常数。}}(0) = 1$，$c_{\text{out}</p>
</li>
<li>
<p><strong>时间采样策略</strong>：为了更有效地训练，可以使用非均匀的时间采样。一个常用的策略是对数均匀采样：
\begin{equation}
t \sim \mathcal{U}[\log t_{\min}, \log t_{\max}], \quad t = e^{\tilde{t}}
\end{equation}
这样可以在小$t$值处有更密集的采样，因为那里的函数变化更剧烈。</p>
</li>
<li>
<p><strong>学习率调度</strong>：推荐使用warmup + cosine decay的学习率调度：
\begin{equation}
\eta(t) = \begin{cases}
\eta_{\max} \frac{t}{T_{\text{warmup}}} &amp; t \leq T_{\text{warmup}} \
\eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\frac{\pi(t-T_{\text{warmup}})}{T_{\max}-T_{\text{warmup}}}\right) &amp; t &gt; T_{\text{warmup}}
\end{cases}
\end{equation}</p>
</li>
<li>
<p><strong>梯度裁剪</strong>：为了稳定训练，建议使用梯度裁剪：
\begin{equation}
\nabla_{\boldsymbol{\theta}} L \leftarrow \min\left{1, \frac{\tau}{|\nabla_{\boldsymbol{\theta}} L|}\right} \nabla_{\boldsymbol{\theta}} L
\end{equation}
其中$\tau$是裁剪阈值，通常设为1.0。</p>
</li>
</ol>
<h3 id="_17">理论与实践的差距<a class="toc-link" href="#_17" title="Permanent link">&para;</a></h3>
<p>虽然一致性模型在理论上很优雅，但实践中仍存在一些挑战：</p>
<p><strong>挑战1（训练稳定性）</strong>：由于使用EMA目标，训练初期可能不稳定。解决方案包括：
- 从预训练的扩散模型初始化
- 使用较大的初始EMA动量$\mu$
- 逐步增加离散化步数$n$（curriculum learning）</p>
<p><strong>挑战2（单步生成质量）</strong>：虽然理论上可以单步生成，但实践中单步生成的质量往往不如多步采样。这是因为：
- 训练时的$\epsilon$可能不够小
- 轨迹交叉问题没有完全解决
- 模型容量不足</p>
<p><strong>挑战3（模式覆盖）</strong>：一致性训练可能会导致模式覆盖问题，即生成的样本多样性不足。这可以通过以下方式缓解：
- 使用更大的模型
- 增加训练数据
- 使用diversity-promoting的损失函数</p>
<p><strong>总结</strong>：一致性模型提供了一种优雅的单步生成方法，通过自洽性约束强制模型在ODE轨迹上保持一致。其核心思想是用分步的、更容易学习的目标替代直接的端到端目标，从而避免轨迹交叉问题。数学上，一致性模型本质上是在学习ODE轨迹的首次命中时间函数，即从任意点$\boldsymbol{x}_t$找到其对应的起点$\boldsymbol{x}_0$。这种统一的视角将扩散模型、流匹配和一致性模型连接起来，为生成模型的理论和实践提供了新的洞察。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈二十七将步长作为条件输入.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#301 生成扩散模型漫谈（二十七）：将步长作为条件输入</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="从谱范数梯度到新式权重衰减的思考.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#303 从谱范数梯度到新式权重衰减的思考</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">生成扩散模型漫谈（二十八）：分步理解一致性模型</a><ul>
<li><a href="#_2">熟悉配方</a></li>
<li><a href="#_3">分步理解</a></li>
<li><a href="#_4">一致训练</a></li>
<li><a href="#_5">采样分析</a></li>
<li><a href="#_6">用于蒸馏</a></li>
<li><a href="#_7">文章小结</a></li>
<li><a href="#_8">推导</a><ul>
<li><a href="#_9">一致性模型的数学定义</a></li>
<li><a href="#ode">从概率流ODE到一致性模型</a></li>
<li><a href="#_10">一致性函数的关键性质</a></li>
<li><a href="#cd">一致性蒸馏（CD）的完整推导</a></li>
<li><a href="#ct">一致性训练（CT）的数学原理</a></li>
<li><a href="#_11">轨迹交叉问题的数学分析</a></li>
<li><a href="#_12">自洽性损失函数的深入分析</a></li>
<li><a href="#_13">单步生成的理论保证</a></li>
<li><a href="#_14">数值稳定性分析</a></li>
<li><a href="#_15">与其他扩散模型的关系</a></li>
<li><a href="#_16">实际训练算法</a></li>
<li><a href="#_17">理论与实践的差距</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>