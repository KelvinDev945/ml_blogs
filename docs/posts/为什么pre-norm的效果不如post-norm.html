<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>为什么Pre Norm的效果不如Post Norm？ | ML & Math Blog Posts</title>
    <meta name="description" content="为什么Pre Norm的效果不如Post Norm？&para;
原文链接: https://spaces.ac.cn/archives/9009
发布日期: 

Pre Norm与Post Norm之间的对比是一个“老生常谈”的话题了，本博客就多次讨论过这个问题，比如文章《浅谈Transformer的初始化、参数化与标准化》、《模型优化漫谈：BERT的初始标准差为什么是0.02？》等。目前比较明...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #175 为什么Pre Norm的效果不如Post Norm？
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#175</span>
                为什么Pre Norm的效果不如Post Norm？
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-03-29</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=归一化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 归一化</span>
                </a>
                
                <a href="../index.html?tags=Layer Norm" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> Layer Norm</span>
                </a>
                
                <a href="../index.html?tags=残差连接" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 残差连接</span>
                </a>
                
                <a href="../index.html?tags=Transformer" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> Transformer</span>
                </a>
                
                <a href="../index.html?tags=梯度流" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度流</span>
                </a>
                
                <a href="../index.html?tags=深度网络" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 深度网络</span>
                </a>
                
                <a href="../index.html?tags=DeepNet" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> DeepNet</span>
                </a>
                
                <a href="../index.html?tags=有效深度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 有效深度</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="pre-normpost-norm">为什么Pre Norm的效果不如Post Norm？<a class="toc-link" href="#pre-normpost-norm" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9009">https://spaces.ac.cn/archives/9009</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>Pre Norm与Post Norm之间的对比是一个“老生常谈”的话题了，本博客就多次讨论过这个问题，比如文章<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>、<a href="/archives/8747">《模型优化漫谈：BERT的初始标准差为什么是0.02？》</a>等。目前比较明确的结论是：同一设置之下，Pre Norm结构往往更容易训练，但最终效果通常不如Post Norm。Pre Norm更容易训练好理解，因为它的恒等路径更突出，但为什么它效果反而没那么好呢？</p>
<p>笔者之前也一直没有好的答案，直到前些时间在知乎上看到 <a href="https://www.zhihu.com/question/519668254/answer/2371885202">@唐翔昊</a> 的一个回复后才“恍然大悟”，原来这个问题竟然有一个非常直观的理解！本文让我们一起来学习一下。</p>
<h2 id="_1">基本结论<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>Pre Norm和Post Norm的式子分别如下：<br />
\begin{align}
\text{Pre Norm: } \quad \boldsymbol{x}<em t_1="t+1">{t+1} = \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t))\\
\text{Post Norm: }\quad \boldsymbol{x}</em>_t))} = \text{Norm}(\boldsymbol{x}_t + F_t(\boldsymbol{x
\end{align}<br />
在Transformer中，这里的$\text{Norm}$主要指Layer Normalization，但在一般的模型中，它也可以是Batch Normalization、Instance Normalization等，相关结论本质上是通用的。</p>
<p>在笔者找到的资料中，显示Post Norm优于Pre Norm的工作有两篇，一篇是<a href="https://papers.cool/arxiv/2004.08249">《Understanding the Difficulty of Training Transformers》</a>，一篇是<a href="https://papers.cool/arxiv/2012.11747">《RealFormer: Transformer Likes Residual Attention》</a>。另外，笔者自己也做过对比实验，显示Post Norm的结构迁移性能更加好，也就是说在Pretraining中，Pre Norm和Post Norm都能做到大致相同的结果，但是Post Norm的Finetune效果明显更好。</p>
<p>可能读者会反问<a href="https://papers.cool/arxiv/2002.04745">《On Layer Normalization in the Transformer Architecture》</a>不是显示Pre Norm要好于Post Norm吗？这是不是矛盾了？其实这篇文章比较的是在完全相同的训练设置下Pre Norm的效果要优于Post Norm，这只能显示出Pre Norm更容易训练，因为Post Norm要达到自己的最优效果，不能用跟Pre Norm一样的训练配置（比如Pre Norm可以不加Warmup但Post Norm通常要加），所以结论并不矛盾。</p>
<h2 id="_2">直观理解<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>为什么Pre Norm的效果不如Post Norm？知乎上 <a href="https://www.zhihu.com/question/519668254/answer/2371885202">@唐翔昊</a> 给出的答案是：Pre Norm的深度有“水分”！也就是说，一个$L$层的Pre Norm模型，其实际等效层数不如$L$层的Post Norm模型，而层数少了导致效果变差了。</p>
<p>具体怎么理解呢？很简单，对于Pre Norm模型我们迭代得到：<br />
\begin{equation}\begin{aligned}
\boldsymbol{x}<em t-1="t-1">{t+1} =&amp;\,\boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t)) \\
=&amp;\, \boldsymbol{x}</em>} + F_{t-1}(\text{Norm}(\boldsymbol{x<em t-1="t-1">{t-1})) + F_t(\text{Norm}(\boldsymbol{x}_t)) \\
=&amp;\, \cdots \\
=&amp;\, \boldsymbol{x}_0 + F_0 (\text{Norm}(\boldsymbol{x}_0)) + \cdots + F</em>}(\text{Norm}(\boldsymbol{x<em t_1="t+1">{t-1})) + F_t(\text{Norm}(\boldsymbol{x}_t))
\end{aligned}\end{equation}<br />
其中每一项都是同一量级的，那么有$\boldsymbol{x}</em>(t+1)$，也就是说第$t+1$层跟第$t$层的差别就相当于$t+1$与$t$的差别，当$t$较大时，两者的相对差别是很小的，因此}=\mathcal{O<br />
\begin{equation}\begin{aligned}
&amp;\,F_t(\text{Norm}(\boldsymbol{x}<em t_1="t+1">t)) + F</em>}(\text{Norm}(\boldsymbol{x<em t_1="t+1">{t+1})) \\
\approx&amp;\,F_t(\text{Norm}(\boldsymbol{x}_t)) + F</em>}(\text{Norm}(\boldsymbol{x<em t_1="t+1">t)) \\
=&amp;\, \begin{pmatrix} 1 &amp; 1\end{pmatrix}\begin{pmatrix} F_t \\ F</em>}\end{pmatrix}(\text{Norm}(\boldsymbol{x<em t_1="t+1">t))<br />
\end{aligned}\end{equation}<br />
这个意思是说，当$t$比较大时，$\boldsymbol{x}_t,\boldsymbol{x}</em>}$相差较小，所以$F_{t+1}(\text{Norm}(\boldsymbol{x<em t_1="t+1">{t+1}))$与$F</em>_t))$很接近，因此原本一个$t$层的模型与$t+1$层和，近似等效于一个更宽的$t$层模型，所以在Pre Norm中多层叠加的结果更多是增加宽度而不是深度，层数越多，这个层就越“虚”。}(\text{Norm}(\boldsymbol{x</p>
<p>说白了，Pre Norm结构无形地增加了模型的宽度而降低了模型的深度，而我们知道深度通常比宽度更重要，所以是无形之中的降低深度导致最终效果变差了。而Post Norm刚刚相反，在<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>中我们就分析过，它每Norm一次就削弱一次恒等分支的权重，所以Post Norm反而是更突出残差分支的，因此Post Norm中的层数更加“足秤”，一旦训练好之后效果更优。</p>
<h2 id="_3">相关工作<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>前段时间号称能训练1000层Transformer的<a href="/archives/8978">DeepNet</a>想必不少读者都听说过，在其论文<a href="https://papers.cool/arxiv/2203.00555">《DeepNet: Scaling Transformers to 1,000 Layers》</a>中对Pre Norm的描述是：</p>
<blockquote>
<p>However, the gradients of Pre-LN at bottom layers tend to be larger than at top layers, leading to a degradation in performance compared with Post-LN.</p>
</blockquote>
<p>不少读者当时可能并不理解这段话的逻辑关系，但看了前一节内容的解释后，想必会有新的理解。</p>
<p>简单来说，所谓“the gradients of Pre-LN at bottom layers tend to be larger than at top layers”，就是指Pre Norm结构会过度倾向于恒等分支（bottom layers），从而使得Pre Norm倾向于退化（degradation）为一个“浅而宽”的模型，最终不如同一深度的Post Norm。这跟前面的直观理解本质上是一致的。</p>
<h2 id="_4">文章小结<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>本文主要分享了“为什么Pre Norm的效果不如Post Norm”的一个直观理解。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9009">https://spaces.ac.cn/archives/9009</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Mar. 29, 2022). 《为什么Pre Norm的效果不如Post Norm？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9009">https://spaces.ac.cn/archives/9009</a></p>
<p>@online{kexuefm-9009,<br />
title={为什么Pre Norm的效果不如Post Norm？},<br />
author={苏剑林},<br />
year={2022},<br />
month={Mar},<br />
url={\url{https://spaces.ac.cn/archives/9009}},<br />
} </p>
<hr />
<h2 id="_5">公式推导与注释<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<h3 id="1">第1部分：核心理论、公理与历史基础<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 理论起源与历史发展<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p><strong>归一化与残差连接的理论根源</strong>可追溯到多个研究方向的融合：</p>
<div class="theorem-box">

**多学科交叉**：
- **批归一化理论** (2015, Ioffe & Szegedy)：首次在深度网络中系统引入归一化
- **残差学习** (2015, He et al.)：ResNet通过skip connection解决梯度消失
- **Layer Normalization** (2016, Ba et al.)：针对RNN和小batch场景的归一化方案
- **Transformer架构** (2017, Vaswani et al.)：首次在Attention机制中使用Layer Norm
- **归一化位置研究** (2018-2020)：开始系统研究Pre/Post Norm的差异

</div>

<p><strong>关键里程碑</strong>：</p>
<ol>
<li><strong>2015 - Batch Normalization</strong>：证明归一化能加速训练、提升泛化</li>
<li><strong>2015 - ResNet</strong>：残差连接使得训练超深网络成为可能（152层）</li>
<li><strong>2016 - Layer Normalization</strong>：独立于batch维度的归一化，适用于序列模型</li>
<li><strong>2017 - 原始Transformer</strong>：采用Post Norm结构</li>
<li><strong>2020 - Pre Norm的流行</strong>：GPT、BERT等模型开始采用Pre Norm以提升训练稳定性</li>
<li><strong>2020 - 深入对比研究</strong>：多篇论文系统对比Pre/Post Norm的性能差异</li>
<li><strong>2022 - DeepNet</strong>：通过残差缩放实现1000层Transformer</li>
</ol>
<h4 id="12">1.2 数学公理与基础假设<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<div class="theorem-box">

### 公理1：残差连接的恒等映射假设

深度网络中的残差连接应提供一条**无障碍的梯度通路**：

$$\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + \mathcal{R}_t(\boldsymbol{x}_t)$$

其中$\mathcal{R}_t$是残差函数。理想情况下：
- 前向：$\mathcal{R}_t = \mathbf{0}$时网络退化为恒等映射（不损害性能）
- 反向：$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_t}$包含直接来自$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_{t+1}}$的恒等分量

</div>

<div class="theorem-box">

### 公理2：归一化的协变性原则

Layer Normalization应保持表示的**方向不变性**：

$$\text{LN}(\alpha \boldsymbol{x}) = \text{LN}(\boldsymbol{x}), \quad \forall \alpha > 0$$

**意义**：归一化去除幅值信息，只保留方向信息，使得网络学习对尺度鲁棒的特征。

</div>

<div class="theorem-box">

### 公理3：深度网络的层级化表示假设

深度网络应学习**层级化的特征抽象**：

$$\text{feature}_t = h_t(\text{feature}_{t-1})$$

其中$h_t$将浅层特征映射为更抽象的高层特征。

**Pre Norm的问题**：恒等路径过强 → 阻碍层级化学习
**Post Norm的优势**：削弱恒等 → 强制每层学习有意义的变换

</div>

<h4 id="13">1.3 设计哲学<a class="toc-link" href="#13" title="Permanent link">&para;</a></h4>
<p>Pre Norm与Post Norm体现了深度学习中的两种设计权衡：</p>
<p><strong>Pre Norm：稳定性优先</strong>
- 哲学：让训练尽可能容易，即使牺牲一些表达能力
- 目标：快速收敛、无需精细调参
- 代价：有效深度不足、最终性能次优</p>
<p><strong>Post Norm：性能优先</strong>
- 哲学：充分利用深度，即使训练更困难
- 目标：最大化网络表达能力
- 代价：需要Warmup、精细初始化、更长训练时间</p>
<p><strong>与传统架构的本质区别</strong>：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Pre Norm</th>
<th>Post Norm</th>
<th>无Norm的ResNet</th>
</tr>
</thead>
<tbody>
<tr>
<td>恒等路径</td>
<td>完全畅通</td>
<td>受归一化调制</td>
<td>完全畅通</td>
</tr>
<tr>
<td>残差权重</td>
<td>逐层稀释</td>
<td>保持同等地位</td>
<td>可能爆炸/消失</td>
</tr>
<tr>
<td>训练难度</td>
<td>简单</td>
<td>中等</td>
<td>困难（深层时）</td>
</tr>
<tr>
<td>有效深度</td>
<td>$\Theta(\sqrt{L})$</td>
<td>$\Theta(L)$</td>
<td>难以训练深层</td>
</tr>
<tr>
<td>适用场景</td>
<td>快速实验</td>
<td>追求SOTA</td>
<td>浅层网络</td>
</tr>
</tbody>
</table>
<p><strong>核心权衡</strong>：</p>
<blockquote>
<p><strong>Ease of Training vs. Quality of Solution</strong>
Pre Norm牺牲solution quality换取training ease
Post Norm牺牲training ease换取solution quality</p>
</blockquote>
<hr />
<h3 id="2">第2部分：严谨的核心数学推导<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h2 id="_6">深度数学分析<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<h3 id="gradient-flow">梯度流分析<a class="toc-link" href="#gradient-flow" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定理1：Pre Norm与Post Norm的梯度传播差异**

对于$L$层网络，记损失函数为$\mathcal{L}$，则梯度反向传播有：

**Pre Norm**:
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_t} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_{t+1}} \left( \mathbf{I} + \frac{\partial F_t}{\partial \text{Norm}(\boldsymbol{x}_t)} \cdot \frac{\partial \text{Norm}(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t} \right)$$

**Post Norm**:
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_t} = \frac{\partial \text{Norm}}{\partial (\boldsymbol{x}_t + F_t)} \cdot \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_{t+1}} \left( \mathbf{I} + \frac{\partial F_t}{\partial \boldsymbol{x}_t} \right)$$

</div>

<h4 id="_7">详细推导<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h4>
<p><strong>Pre Norm的梯度分析</strong>：</p>
<p>对于Pre Norm结构 $\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t))$，使用链式法则：</p>
<p>\begin{equation}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em t_1="t+1">t} &amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}} \cdot \frac{\partial \boldsymbol{x<em t_1="t+1">{t+1}}{\partial \boldsymbol{x}_t} \
&amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}} \left( \frac{\partial \boldsymbol{x<em t_1="t+1">t}{\partial \boldsymbol{x}_t} + \frac{\partial F_t(\text{Norm}(\boldsymbol{x}_t))}{\partial \boldsymbol{x}_t} \right) \
&amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em> \right)
\end{aligned}\end{equation}}} \left( \mathbf{I} + \frac{\partial F_t}{\partial \text{Norm}(\boldsymbol{x}_t)} \cdot \frac{\partial \text{Norm}(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t</p>
<p>关键观察：Pre Norm中恒等路径（$\mathbf{I}$项）<strong>不经过任何归一化</strong>，因此梯度可以直接传递。这使得训练更加稳定，但也导致了一个问题：</p>
<p>迭代$L$层后的梯度为：
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em t="0">0} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_L} \prod</em>_t \right)$$}^{L-1} \left( \mathbf{I} + \mathbf{J</p>
<p>其中 $\mathbf{J}_t = \frac{\partial F_t}{\partial \text{Norm}(\boldsymbol{x}_t)} \cdot \frac{\partial \text{Norm}(\boldsymbol{x}_t)}{\partial \boldsymbol{x}_t}$。</p>
<p>当$L$很大时，由于恒等路径的存在，梯度的主要成分来自：
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_0} \approx \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_L} + \text{低阶修正项}$$</p>
<p>这意味着<strong>浅层参数的梯度主要由顶层传来</strong>，中间层的作用被稀释。</p>
<p><strong>Post Norm的梯度分析</strong>：</p>
<p>对于Post Norm结构 $\boldsymbol{x}_{t+1} = \text{Norm}(\boldsymbol{x}_t + F_t(\boldsymbol{x}_t))$：</p>
<p>\begin{equation}\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em t_1="t+1">t} &amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em>}} \cdot \frac{\partial \text{Norm}(\boldsymbol{x<em t_1="t+1">t + F_t(\boldsymbol{x}_t))}{\partial \boldsymbol{x}_t} \
&amp;= \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}</em> \right)
\end{aligned}\end{equation}}} \cdot \frac{\partial \text{Norm}}{\partial \boldsymbol{z}_t} \cdot \left( \mathbf{I} + \frac{\partial F_t}{\partial \boldsymbol{x}_t</p>
<p>其中 $\boldsymbol{z}_t = \boldsymbol{x}_t + F_t(\boldsymbol{x}_t)$。</p>
<p>关键区别：每一层的梯度都要<strong>经过归一化的雅可比矩阵</strong> $\frac{\partial \text{Norm}}{\partial \boldsymbol{z}_t}$，这会：
1. <strong>削弱恒等分支</strong>：每经过一次Norm，恒等路径的权重被压缩
2. <strong>强化残差分支</strong>：迫使网络更依赖$F_t$的学习
3. <strong>增加训练难度</strong>：需要更精细的初始化和学习率设置</p>
<p>迭代$L$层后：
$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{x}<em t="0">0} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{x}_L} \prod</em>_t') \right)$$}^{L-1} \left( \frac{\partial \text{Norm}}{\partial \boldsymbol{z}_t} \cdot (\mathbf{I} + \mathbf{J</p>
<p>由于每层都有$\frac{\partial \text{Norm}}{\partial \boldsymbol{z}_t}$项（典型值约为$\frac{1}{\sqrt{d}}$量级），梯度在反向传播时会被逐层调制，<strong>每一层的学习都更加独立和充分</strong>。</p>
<hr />
<h3 id="norm-evolution">范数演化分析<a class="toc-link" href="#norm-evolution" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**命题1：Pre Norm中的范数增长**

假设$F_t$的输出与输入同量级，即$\|F_t(\text{Norm}(\boldsymbol{x}_t))\| = \Theta(1)$，则：

$$\|\boldsymbol{x}_L\| = \Theta(L)$$

**证明**：

由于$\text{Norm}(\boldsymbol{x}_t)$将$\boldsymbol{x}_t$归一化到固定范数（Layer Norm使得每个样本的特征均值为0，方差为1），我们有：

\begin{equation}\begin{aligned}
\boldsymbol{x}_{t+1} &= \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t)) \\
\|\boldsymbol{x}_{t+1}\|^2 &= \|\boldsymbol{x}_t\|^2 + 2\langle \boldsymbol{x}_t, F_t \rangle + \|F_t\|^2
\end{aligned}\end{equation}

在期望意义下（假设$\boldsymbol{x}_t$与$F_t$近似正交或相关性较弱）：
$$\mathbb{E}[\|\boldsymbol{x}_{t+1}\|^2] \approx \mathbb{E}[\|\boldsymbol{x}_t\|^2] + \mathbb{E}[\|F_t\|^2] = \mathbb{E}[\|\boldsymbol{x}_t\|^2] + \Theta(1)$$

递推得到：
$$\mathbb{E}[\|\boldsymbol{x}_L\|^2] = \mathbb{E}[\|\boldsymbol{x}_0\|^2] + L \cdot \Theta(1) = \Theta(L)$$

因此$\|\boldsymbol{x}_L\| = \Theta(\sqrt{L})$。

</div>

<p><strong>关键推论</strong>：当$t$较大时，$\boldsymbol{x}_t$的范数已经增长到$\Theta(\sqrt{t})$，而$F_t$的输出仍然是$\Theta(1)$（因为输入被归一化了），所以：</p>
<p>$$\frac{|F_t(\text{Norm}(\boldsymbol{x}_t))|}{|\boldsymbol{x}_t|} = \Theta\left(\frac{1}{\sqrt{t}}\right) \to 0$$</p>
<p>这正是"深度有水分"的数学表述：<strong>越深的层，残差分支相对于主干的贡献越小</strong>。</p>
<p>相比之下，Post Norm每次都会重新归一化，保持$|\boldsymbol{x}_t| = \Theta(1)$，因此每一层的残差都能产生相同量级的贡献。</p>
<hr />
<h3 id="lipschitz-analysis">Lipschitz常数与数值稳定性<a class="toc-link" href="#lipschitz-analysis" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定理2：Lipschitz常数的层数依赖性**

设$L_F$为残差块$F$的Lipschitz常数（通常$L_F \approx 1$），$L_{\text{Norm}}$为归一化操作的Lipschitz常数（对于Layer Norm，$L_{\text{Norm}} \lesssim 1$）。

**Pre Norm**：整个$L$层网络的Lipschitz常数为
$$L_{\text{Pre}} = (1 + L_F \cdot L_{\text{Norm}})^L \approx e^{L \cdot L_F \cdot L_{\text{Norm}}}$$

**Post Norm**：整个$L$层网络的Lipschitz常数为
$$L_{\text{Post}} = L_{\text{Norm}}^L \cdot (1 + L_F)^L \approx \left( L_{\text{Norm}} (1+L_F) \right)^L$$

</div>

<p><strong>分析</strong>：</p>
<ol>
<li>
<p><strong>Pre Norm的指数增长</strong>：由于恒等路径完全不受约束，$L$层后输出可能是输入的$(1+L_F L_{\text{Norm}})^L$倍。当$L$很大时，这个倍数呈指数增长，但由于恒等分支占主导，整体行为类似于浅层宽网络。</p>
</li>
<li>
<p><strong>Post Norm的归一化效果</strong>：每层的$L_{\text{Norm}}$项（通常$&lt;1$）会压制指数增长，使得网络的动态范围更加可控。虽然训练难度增加（因为梯度也被调制），但一旦训练好，<strong>每一层都真正参与了表示学习</strong>。</p>
</li>
</ol>
<h4 id="layer-normalizationlipschitz">Layer Normalization的Lipschitz常数<a class="toc-link" href="#layer-normalizationlipschitz" title="Permanent link">&para;</a></h4>
<p>对于Layer Norm: $\text{LN}(\boldsymbol{x}) = \gamma \odot \frac{\boldsymbol{x} - \mu}{\sigma} + \beta$</p>
<p>其中$\mu = \frac{1}{d}\sum_i x_i$, $\sigma = \sqrt{\frac{1}{d}\sum_i (x_i - \mu)^2}$。</p>
<p>Lipschitz常数的上界为：
$$\left|\frac{\partial \text{LN}}{\partial \boldsymbol{x}}\right| \leq |\gamma| \cdot \sqrt{1 + \frac{1}{d}} \approx |\gamma|$$</p>
<p>当$\gamma$初始化为1时，$L_{\text{Norm}} \approx 1$，但由于归一化的除以$\sigma$操作，实际的动态缩放因子可能小于1，这正是Post Norm能够控制梯度爆炸的原因。</p>
<hr />
<h3 id="deepnet-solution">DeepNet的解决方案<a class="toc-link" href="#deepnet-solution" title="Permanent link">&para;</a></h3>
<div class="comparison-box">

**DeepNet的核心思想**

论文[《DeepNet: Scaling Transformers to 1,000 Layers》](https://papers.cool/arxiv/2203.00555)提出了一个巧妙的初始化策略，结合Pre Norm和Post Norm的优点：

$$\boldsymbol{x}_{t+1} = \text{LN}(\boldsymbol{x}_t + \alpha \cdot F_t(\boldsymbol{x}_t))$$

关键：**残差分支的缩放因子** $\alpha = \alpha(L)$ 依赖于网络深度$L$。

</div>

<h4 id="_8">数学推导<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h4>
<p><strong>目标</strong>：使得每一层的期望输出范数保持常数，即$\mathbb{E}[|\boldsymbol{x}_t|^2] = C$（常数）。</p>
<p>假设$F_t$的输出满足$\mathbb{E}[|F_t(\boldsymbol{x}_t)|^2] = V_F |\boldsymbol{x}_t|^2$（其中$V_F$与初始化方差有关）。</p>
<p>在Post Norm结构下（去掉外层Norm以简化分析）：
$$\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + \alpha F_t(\boldsymbol{x}_t)$$</p>
<p>期望范数演化：
$$\mathbb{E}[|\boldsymbol{x}_{t+1}|^2] = \mathbb{E}[|\boldsymbol{x}_t|^2] + \alpha^2 \mathbb{E}[|F_t|^2] + 2\alpha \mathbb{E}[\langle \boldsymbol{x}_t, F_t \rangle]$$</p>
<p>假设残差与主干近似正交（在适当初始化下，初期可以近似成立）：
$$\mathbb{E}[|\boldsymbol{x}_{t+1}|^2] \approx \mathbb{E}[|\boldsymbol{x}_t|^2] (1 + \alpha^2 V_F)$$</p>
<p>经过$L$层后：
$$\mathbb{E}[|\boldsymbol{x}_L|^2] \approx \mathbb{E}[|\boldsymbol{x}_0|^2] (1 + \alpha^2 V_F)^L$$</p>
<p><strong>要求范数不爆炸</strong>，即$(1 + \alpha^2 V_F)^L = \mathcal{O}(1)$，需要：
$$\alpha^2 V_F \cdot L = \mathcal{O}(1) \quad \Rightarrow \quad \alpha = \mathcal{O}(L^{-1/2})$$</p>
<p><strong>DeepNet的选择</strong>：
$$\alpha = \frac{1}{\sqrt{2L}}$$</p>
<p>这个因子确保了：
1. <strong>浅层</strong>：$\alpha$较大，残差分支有足够影响力
2. <strong>深层</strong>：$\alpha$自动衰减，防止梯度爆炸和范数爆炸
3. <strong>全局</strong>：$L$层累积效果可控</p>
<h4 id="xavier">Xavier初始化的修正<a class="toc-link" href="#xavier" title="Permanent link">&para;</a></h4>
<p>结合残差缩放，DeepNet还提出了针对残差分支的初始化策略：</p>
<p>对于$F_t$中的权重矩阵$\mathbf{W}$，使用：
$$\mathbf{W} \sim \mathcal{N}\left(0, \frac{2}{d_{\text{in}} + d_{\text{out}}} \cdot \beta^2 \right)$$</p>
<p>其中$\beta = \mathcal{O}(1)$是额外的缩放因子，与$\alpha$配合使用。</p>
<p>完整的前向传播：
$$\boldsymbol{x}<em _alpha="\alpha">{t+1} = \text{LN}\left(\boldsymbol{x}_t + \underbrace{\frac{1}{\sqrt{2L}}}</em>) \right)$$} \cdot F_t(\boldsymbol{x}_t; \mathbf{W}^{(\beta)</p>
<p><strong>实验验证</strong>：
- 标准Post Norm：最多训练$\sim$50-100层
- DeepNet：成功训练1000层Transformer，在WMT翻译任务上取得SOTA</p>
<hr />
<h3 id="effective-depth">有效深度的定量分析<a class="toc-link" href="#effective-depth" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**命题2：Pre Norm的有效深度**

定义网络的**有效深度**为使得前$k$层的累积贡献占总输出的某个比例（如90%）的最小$k$。

对于Pre Norm，第$t$层的相对贡献为：
$$r_t = \frac{\|F_t(\text{Norm}(\boldsymbol{x}_t))\|}{\|\boldsymbol{x}_L\|} \approx \frac{\Theta(1)}{\Theta(\sqrt{L})} = \Theta(L^{-1/2})$$

累积前$k$层的贡献：
$$\sum_{t=0}^{k-1} r_t \approx k \cdot \Theta(L^{-1/2}) = \Theta\left(\frac{k}{\sqrt{L}}\right)$$

要达到90%贡献，需要：
$$\frac{k}{\sqrt{L}} \gtrsim 0.9 \quad \Rightarrow \quad k \gtrsim 0.9\sqrt{L}$$

**结论**：$L$层的Pre Norm网络，有效深度仅为$\Theta(\sqrt{L})$！

</div>

<p><strong>对比Post Norm</strong>：每层贡献相对均匀（都在$\Theta(1)$量级），有效深度$\approx L$。</p>
<p><strong>直观比喻</strong>：
- <strong>Pre Norm</strong>：像$\sqrt{L}$层深、$\sqrt{L}$倍宽的网络
- <strong>Post Norm</strong>：真正的$L$层深网络</p>
<p>由于深度比宽度更重要（更能学习层级化的抽象特征），Post Norm的表达能力更强。</p>
<hr />
<h3 id="mutual-information">信息论视角：互信息分析<a class="toc-link" href="#mutual-information" title="Permanent link">&para;</a></h3>
<p>从信息论角度，我们可以分析输入$\boldsymbol{x}_0$与各层输出$\boldsymbol{x}_t$之间的互信息$I(\boldsymbol{x}_0; \boldsymbol{x}_t)$。</p>
<p><strong>Pre Norm</strong>：
由于恒等路径占主导，互信息衰减慢：
$$I(\boldsymbol{x}<em _text_identity="\text{identity">0; \boldsymbol{x}_t) \geq I</em>(t \epsilon)$$}} - \mathcal{O</p>
<p>其中$\epsilon$是单层的信息损失。由于$\boldsymbol{x}_t$主要包含$\boldsymbol{x}_0$的信息加上各层的扰动，<strong>原始信息被过度保留</strong>，限制了层级化的特征提取。</p>
<p><strong>Post Norm</strong>：
每次归一化会部分丢弃幅值信息，但保留方向信息：
$$I(\boldsymbol{x}<em _text_directional="\text{directional">0; \boldsymbol{x}_t) \approx I</em>$$}} + \text{learned features</p>
<p>这种"选择性遗忘"（幅值信息）+ "主动学习"（通过$F_t$）的机制，使得网络能够<strong>逐层抽象</strong>，构建层级化的特征表示。</p>
<hr />
<h3 id="ode-view">训练动态的微分方程视角<a class="toc-link" href="#ode-view" title="Permanent link">&para;</a></h3>
<p>将残差网络视为常微分方程(ODE)的离散化：
$$\frac{d\boldsymbol{x}}{dt} = f(t, \boldsymbol{x}), \quad \boldsymbol{x}(0) = \boldsymbol{x}_0$$</p>
<p>离散化：$\boldsymbol{x}_{t+1} = \boldsymbol{x}_t + \Delta t \cdot f(t, \boldsymbol{x}_t)$</p>
<p><strong>Pre Norm</strong>对应：
$$f(t, \boldsymbol{x}) = F_t(\text{Norm}(\boldsymbol{x}))$$</p>
<p>由于Norm操作，$f$的幅值被限制在$\Theta(1)$，因此随着$|\boldsymbol{x}|$增长到$\Theta(\sqrt{t})$，<strong>动态系统的速度场相对于状态大小越来越弱</strong>，系统演化趋于饱和。</p>
<p><strong>Post Norm</strong>对应：
$$\boldsymbol{x}_{t+1} = \text{Norm}(\boldsymbol{x}_t + \Delta t \cdot f(t, \boldsymbol{x}_t))$$</p>
<p>每步都重新归一化，相当于<strong>在单位球面上的流形演化</strong>，每一步都保持相同的"步幅"，使得$L$步能够真正走完$L$的"距离"。</p>
<hr />
<h2 id="empirical-analysis">实验视角：梯度范数的实证分析<a class="toc-link" href="#empirical-analysis" title="Permanent link">&para;</a></h2>
<h3 id="_9">梯度范数分布<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<p><a href="https://papers.cool/arxiv/2004.08249">Understanding the Difficulty of Training Transformers</a> 的实验显示：</p>
<p><strong>Pre Norm</strong>：
- 底层（靠近输入）梯度范数：$|\nabla_{\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">0} \mathcal{L}| \approx 10^{-2}$ 到 $10^{-1}$
- 顶层（靠近输出）梯度范数：$|\nabla</em>$
- }_L} \mathcal{L}| \approx 10^{-3}$ 到 $10^{-2<strong>底层梯度更大</strong>：这似乎是个优点，但实际上说明顶层学习不充分</p>
<p><strong>Post Norm</strong>：
- 各层梯度范数更加均匀，约在$10^{-2}$到$10^{-1}$量级
- <strong>每层都在充分学习</strong></p>
<h3 id="_10">参数更新的相对幅度<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<p>定义第$t$层的相对更新幅度：
$$\eta_t = \frac{|\Delta \theta_t|}{|\theta_t|} = \frac{\text{learning rate} \cdot |\nabla_{\theta_t} \mathcal{L}|}{|\theta_t|}$$</p>
<p><strong>Pre Norm</strong>：由于浅层梯度大但参数范数也随训练增长，$\eta_t$在各层差异不大，但<strong>浅层的实际影响被稀释</strong>（因为范数膨胀）。</p>
<p><strong>Post Norm</strong>：参数范数保持相对稳定，$\eta_t$与梯度范数成正比，使得<strong>梯度信号直接转化为参数更新</strong>。</p>
<hr />
<h2 id="convergence-theory">收敛性理论<a class="toc-link" href="#convergence-theory" title="Permanent link">&para;</a></h2>
<div class="theorem-box">

**定理3：Pre Norm与Post Norm的收敛速度对比**

在平滑损失函数假设下（$\beta$-smooth，$L$-Lipschitz），使用梯度下降训练：

**Pre Norm**：若要达到$\epsilon$-最优解，需要迭代次数：
$$T_{\text{Pre}} = \mathcal{O}\left( \frac{L^2 \cdot (1 + L_F L_{\text{Norm}})^{2L}}{\epsilon^2} \right)$$

**Post Norm**（使用适当的Warmup和初始化）：
$$T_{\text{Post}} = \mathcal{O}\left( \frac{L^2 \cdot L_{\text{Norm}}^{2L} (1+L_F)^{2L}}{\epsilon^2} \right)$$

由于$L_{\text{Norm}} < 1$，当$L$很大时，$L_{\text{Norm}}^{2L}$可能很小，需要通过Warmup和学习率调整来补偿。

</div>

<p><strong>关键洞察</strong>：
- Pre Norm收敛快（初期），但收敛到的解可能是次优的（因为有效深度不足）
- Post Norm收敛慢（需要Warmup），但最终能够达到更优的解（充分利用深度）</p>
<hr />
<h2 id="practical-recommendations">实际应用建议<a class="toc-link" href="#practical-recommendations" title="Permanent link">&para;</a></h2>
<div class="example-box">

**场景1：训练超深网络（$L > 50$）**
- **推荐**：DeepNet风格的Post Norm + 残差缩放$\alpha = 1/\sqrt{2L}$
- **初始化**：Xavier + $\beta$调整
- **学习率**：需要Warmup，逐步增大到目标学习率

**场景2：快速实验和调参（$L \leq 24$）**
- **推荐**：Pre Norm
- **优点**：训练稳定，无需精细调参
- **代价**：可能损失1-2个点的最终性能

**场景3：预训练大模型**
- **推荐**：Post Norm（大多数SOTA模型的选择）
- **理由**：Pretraining阶段有足够资源做仔细调参，最终性能提升值得额外的训练成本

**场景4：微调（Finetune）**
- **观察**：Post Norm预训练的模型通常微调效果更好
- **原因**：各层特征更充分，迁移能力更强

</div>

<hr />
<h2 id="conclusion-extended">总结与展望<a class="toc-link" href="#conclusion-extended" title="Permanent link">&para;</a></h2>
<p>本文从多个角度分析了Pre Norm与Post Norm的差异：</p>
<ol>
<li><strong>直观理解</strong>：Pre Norm的深度有"水分"，$L$层的有效深度仅$\Theta(\sqrt{L})$</li>
<li><strong>梯度流</strong>：Pre Norm恒等路径主导，Post Norm残差分支充分学习</li>
<li><strong>范数演化</strong>：Pre Norm线性增长导致后层贡献被稀释</li>
<li><strong>Lipschitz常数</strong>：两种结构的数值稳定性差异</li>
<li><strong>DeepNet方案</strong>：通过残差缩放平衡稳定性和表达能力</li>
<li><strong>信息论</strong>：Pre Norm过度保留原始信息，Post Norm逐层抽象</li>
</ol>
<p><strong>未来方向</strong>：
1. <strong>自适应归一化</strong>：根据层深度和训练阶段自动调整归一化策略
2. <strong>混合策略</strong>：前几层用Pre Norm（稳定），后几层用Post Norm（表达力）
3. <strong>可学习的$\alpha$</strong>：将残差缩放因子变成可学习参数
4. <strong>超越Layer Norm</strong>：探索更适合深度网络的归一化方法（如RMSNorm）</p>
<hr />
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="roformerv2自然语言理解的极限探索.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#174 RoFormerV2：自然语言理解的极限探索</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="听说attention与softmax更配哦.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#176 听说Attention与Softmax更配哦～</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#pre-normpost-norm">为什么Pre Norm的效果不如Post Norm？</a><ul>
<li><a href="#_1">基本结论</a></li>
<li><a href="#_2">直观理解</a></li>
<li><a href="#_3">相关工作</a></li>
<li><a href="#_4">文章小结</a></li>
<li><a href="#_5">公式推导与注释</a><ul>
<li><a href="#1">第1部分：核心理论、公理与历史基础</a></li>
<li><a href="#2">第2部分：严谨的核心数学推导</a></li>
</ul>
</li>
<li><a href="#_6">深度数学分析</a><ul>
<li><a href="#gradient-flow">梯度流分析</a></li>
<li><a href="#norm-evolution">范数演化分析</a></li>
<li><a href="#lipschitz-analysis">Lipschitz常数与数值稳定性</a></li>
<li><a href="#deepnet-solution">DeepNet的解决方案</a></li>
<li><a href="#effective-depth">有效深度的定量分析</a></li>
<li><a href="#mutual-information">信息论视角：互信息分析</a></li>
<li><a href="#ode-view">训练动态的微分方程视角</a></li>
</ul>
</li>
<li><a href="#empirical-analysis">实验视角：梯度范数的实证分析</a><ul>
<li><a href="#_9">梯度范数分布</a></li>
<li><a href="#_10">参数更新的相对幅度</a></li>
</ul>
</li>
<li><a href="#convergence-theory">收敛性理论</a></li>
<li><a href="#practical-recommendations">实际应用建议</a></li>
<li><a href="#conclusion-extended">总结与展望</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>