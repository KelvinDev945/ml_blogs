<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AdamW的Weight RMS的... | ML & Math Blog Posts</title>
    <meta name="description" content="AdamW的Weight RMS的...&para;
原文链接: https://spaces.ac.cn/archives/11307
发布日期: 

在《为什么Adam的Update RMS是0.2？》中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 @EIFY 指出相同的结果已经出现在论文《Rotational Equilibrium: How Weight Decay...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=估计">估计</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #133 AdamW的Weight RMS的...
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#133</span>
                AdamW的Weight RMS的...
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/11307" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=估计" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 估计</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=平均场" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 平均场</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="adamwweight-rms">AdamW的Weight RMS的...<a class="toc-link" href="#adamwweight-rms" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11307">https://spaces.ac.cn/archives/11307</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在<a href="/archives/11267">《为什么Adam的Update RMS是0.2？》</a>中，我们用平均场近似估计了Adam的Update RMS。不久后，读者 <a href="https://x.com/EIFY/status/1965888629814988984">@EIFY</a> 指出相同的结果已经出现在论文<a href="https://papers.cool/arxiv/2305.17212">《Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks》</a>中。阅读后，笔者发现其中不仅包含了Update RMS的估计，还包含了Weight RMS的估计。</p>
<p>也就是说，AdamW训出来的模型，其权重的RMS是可以事先估计出来一个渐近结果的。大家会不会觉得这个结论有点意外？反正笔者第一次看到它是颇为意外的，直觉上权重模长是模型根据训练集自己学出来的，结果它告诉我这已经隐藏在优化器的超参中，可谓很反直觉了。</p>
<p>这篇文章我们还是用平均场近似方法，来复现对Weight RMS的渐近估计。</p>
<h2 id="_1">滑动视角<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>首先还是来回顾AdamW的更新规则：<br />
\begin{equation}\text{Adam}\color{skyblue}{\text{W}}:=\left\{\begin{aligned}<br />
&amp;\boldsymbol{m}<em t-1="t-1">t = \beta_1 \boldsymbol{m}</em>} + \left(1 - \beta_1\right) \boldsymbol{g<em t-1="t-1">t\\<br />
&amp;\boldsymbol{v}_t = \beta_2 \boldsymbol{v}</em>} + \left(1 - \beta_2\right) \boldsymbol{g<em t-1="t-1">t^2\\<br />
&amp;\hat{\boldsymbol{m}}_t = \boldsymbol{m}_t\left/\left(1 - \beta_1^t\right)\right.\\<br />
&amp;\hat{\boldsymbol{v}}_t = \boldsymbol{v}_t\left/\left(1 - \beta_2^t\right)\right.\\<br />
&amp;\boldsymbol{u}_t =\hat{\boldsymbol{m}}_t\left/\left(\sqrt{\hat{\boldsymbol{v}}_t} + \epsilon\right)\right.\\<br />
&amp;\boldsymbol{\theta}_t = \boldsymbol{\theta}</em>} - \eta_t (\boldsymbol{u<em t-1="t-1">t \color{skyblue}{ + \lambda_t \boldsymbol{\theta}</em>)}<br />
\end{aligned}\right.\end{equation}<br />
再次说明，这里加粗符号默认都是$\mathbb{R}^d$的向量，向量的乘除（包括平方、开根号）默认都是Element-wise的Hadamard积/商。</p>
<p>跟<a href="/archives/11267">《为什么Adam的Update RMS是0.2？》</a>一样，我们考虑$t\to\infty$（对于$\beta_1,\beta_2$来说）和$\epsilon\to 0$，所以$\boldsymbol{u}<em t-1="t-1">t=\boldsymbol{m}_t/\sqrt{\boldsymbol{v}_t}$。我们暂时先考虑$\eta_t,\lambda_t$都是常数的例子，所以它们的下标可以省略掉，并且记$\beta_3 = 1-\eta\lambda$，我们有<br />
\begin{equation}\boldsymbol{\theta}_t = \beta_3\boldsymbol{\theta}</em>} + (1-\beta_3)(-\boldsymbol{u}_t/\lambda)\label{eq:ema-wd}\end{equation<br />
这个式子表明，我们可以从更新量的滑动平均（Exponential Moving Average，EMA）角度来理解Weight Decay。这是一个很有意义的视角转换，是<a href="https://papers.cool/arxiv/2405.13698">《How to set AdamW’s weight decay as you scale model and dataset size》</a>、<a href="https://papers.cool/arxiv/2505.13738">《Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training》</a>等工作的基础。</p>
<h2 id="_2">加权平均<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>根据式$\eqref{eq:ema-wd}$，我们可以将$\boldsymbol{\theta}<em i="1">t$展开为加权平均形式<br />
\begin{equation}\boldsymbol{\theta}_t = \beta_3^t\boldsymbol{\theta}_0 + (1-\beta_3)\sum</em>}^t \beta_3^{t-i} (-\boldsymbol{u<em i="1">i/\lambda)\label{eq:theta-t}\end{equation}<br />
同理，$\boldsymbol{m}_t$和$\boldsymbol{v}_t$也可以展开为<br />
\begin{equation}\boldsymbol{m}_t = (1 - \beta_1)\sum</em>}^t \beta_1^{t-i}\boldsymbol{g<em i="1">i,\qquad \boldsymbol{v}_t = (1 - \beta_2)\sum</em>}^t \beta_2^{t-i}\boldsymbol{g}_i^2\label{eq:mv-roll}\end{equation<br />
这里有个小细节，$\boldsymbol{\theta}_t$的表达式我们保留了$\boldsymbol{\theta}_0$，但$\boldsymbol{m}_t$和$\boldsymbol{v}_t$的表达式我们没有保留$\boldsymbol{m}_0$和$\boldsymbol{v}_0$，原因有两个：1、$\boldsymbol{m}$和$\boldsymbol{v}$的初始化一般是零；2、即便它们初始化不是零，但对应的$\beta_1^t$和$\beta_2^t$也会足够接近于零，因此初始化的影响可以忽略。</p>
<p>然而，$\boldsymbol{\theta}$是模型权重，它的初始化通常不是零，并且$\beta_3$往往非常接近于1，对于整个训练周期而言，$\beta_3^t$不一定能充分接近于零，因此我们显式保留$\beta_3^t$和$\boldsymbol{\theta}_0$，按需取舍。</p>
<h2 id="_3">快速估计<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>我们的任务是估计Weight RMS，即$\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>$，顾名思义，它是各个分量的Root Mean Square：<br />
\begin{equation}\Vert\boldsymbol{\theta}\Vert_{RMS} = \sqrt{\frac{1}{d}\sum_{i=1}^d \theta_i^2},\qquad\qquad \text{其中 }\boldsymbol{\theta} = (\theta_1,\theta_2,\cdots,\theta_d)\end{equation}<br />
它跟模长的区别就是多除了个$\sqrt{d}$，所以模长的大部分性质对RMS同样成立。对于$\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>^2$，可以得到}$，我们有一个快速但不是那么准确的推导方式：直接对式$\eqref{eq:ema-wd}$两边求$\Vert\cdot\Vert_{RMS<br />
\begin{equation}\begin{aligned}<br />
\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>}^2 =&amp;\, \Vert\beta_3\boldsymbol{\theta<em RMS="RMS">{t-1} + (1-\beta_3)(-\boldsymbol{u}_t/\lambda)\Vert</em>^2 \\[5pt]<br />
=&amp;\, \beta_3^2\Vert\boldsymbol{\theta}<em RMS="RMS">{t-1}\Vert</em>}^2 + (1-\beta_3)^2\Vert\boldsymbol{u<em RMS="RMS">t\Vert</em>}^2/\lambda^2 - 2\beta_3(1-\beta_3)\boldsymbol{\theta<em t-1="t-1">{t-1}\cdot\boldsymbol{u}_t/(\lambda d)<br />
\end{aligned}\end{equation}<br />
假设$\boldsymbol{\theta}</em>},\boldsymbol{u<em t-1="t-1">t$近乎正交，那么$\boldsymbol{\theta}</em>}\cdot\boldsymbol{u<em RMS="RMS">t\approx 0$，这在高维空间中通常是一个不错的近似（参考<a href="/archives/7076">《n维空间下两个随机向量的夹角分布》</a>），然后$\Vert\boldsymbol{u}_t\Vert</em>}$我们已经算过了，答案是约等于$\sqrt{\frac{1-\beta_1}{1+\beta_1}}$，最后我们考虑的是趋于稳态的结果，所以$\Vert\boldsymbol{\theta<em RMS="RMS">t\Vert</em>}^2=\Vert\boldsymbol{\theta<em RMS="RMS">{t-1}\Vert</em>^2$，于是有<br />
\begin{equation}(1-\beta_3^2)\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>}^2 \approx (1-\beta_3)^2 \frac{1-\beta_1}{1+\beta_1} /\lambda^2\qquad\Rightarrow\qquad \Vert\boldsymbol{\theta<em RMS="RMS">t\Vert</em>} \approx \sqrt{\frac{1-\beta_1}{1+\beta_1}\frac{\eta}{2\lambda}}\end{equation<br />
从左式到右式还用到了$\beta_3\approx 1$的近似。最后的结果会有些误差，因为$\boldsymbol{\theta}<em RMS="RMS">t\cdot\boldsymbol{u}_t\approx 0$实际上并不那么成立，但$\Vert\boldsymbol{\theta}_t\Vert</em>$的结论是正确的。类似的推导还出现在}\propto \sqrt{\eta/\lambda<a href="https://papers.cool/arxiv/2506.02285">《Why Gradients Rapidly Increase Near the End of Training》</a>。</p>
<h2 id="_4">更好近似<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>很多情况下我们只需要知道$\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>$就行了，这是一个比较通用的结论。而对于追求更准确结论的读者来说，我们可以用平均场方法得到一个更好的近似，代价是计算过程会复杂不少，但好处是我们可以获得更多更清晰的认知。}\propto \sqrt{\eta/\lambda</p>
<h3 id="_5">步骤之一<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h3>
<p>我们从式$\eqref{eq:theta-t}$出发，求和这一项，本身就具有加权平均的形式，所以我们先用第一次平均场：<br />
\begin{equation}\underbrace{\frac{1-\beta_3}{1-\beta_3^t}\sum_{i=1}^t \beta_3^{t-i} \boldsymbol{u}<em _text_记为="\text{记为">i}</em>}\bar{\boldsymbol{u}<em i="1">t} = \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i} \frac{\boldsymbol{m<em i="1">i}{\sqrt{\boldsymbol{v}_i}}\approx \frac{\bar{\boldsymbol{m}}_t \,\,\triangleq\,\, \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i}\boldsymbol{m<em i="1">i}{\sqrt{\bar{\boldsymbol{v}}_t \,\,\triangleq\,\, \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i}\boldsymbol{v<em RMS="RMS">i}}\label{eq:u-bar}\end{equation}<br />
现在再次回到式$\eqref{eq:theta-t}$，由于$\boldsymbol{\theta}_0$是随机的初始化向量，因此可以假设$\boldsymbol{\theta}_0$与$\bar{\boldsymbol{u}}_t$正交，于是我们有<br />
\begin{equation}\Vert\boldsymbol{\theta}_t\Vert</em>}^2 \approx \beta_3^{2t}\Vert\boldsymbol{\theta<em RMS="RMS">0\Vert</em>}^2 + (1-\beta_3^t)^2 \lambda^{-2}\Vert \bar{\boldsymbol{u}<em RMS="RMS">t\Vert</em>}^2\end{equation<br />
现在我们要求$\Vert \bar{\boldsymbol{u}}<em RMS="RMS">t\Vert</em>}^2$，根据之前的经验，我们需要假设$\boldsymbol{g<em RMS="RMS">j$独立同分布地服从$\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\sigma}^2)$，然后求<br />
\begin{equation}\mathbb{E}[\bar{\boldsymbol{u}}_t^2] \approx \mathbb{E}\left[\frac{\bar{\boldsymbol{m}}_t^2}{\bar{\boldsymbol{v}}_t}\right] \approx \frac{\mathbb{E}[\bar{\boldsymbol{m}}_t^2]}{\mathbb{E}[\bar{\boldsymbol{v}}_t]}\end{equation}<br />
最后再对$\mathbb{E}[\bar{\boldsymbol{u}}_t^2]$的各个分量求平均，那么就可以作为$\Vert \bar{\boldsymbol{u}}_t\Vert</em>^2$的近似。</p>
<h3 id="_6">步骤之二<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h3>
<p>结合式$\eqref{eq:mv-roll}$，我们得到<br />
\begin{gather}<br />
\sum_{i=1}^t \beta_3^{t-i}\boldsymbol{m}<em i="1">i = (1 - \beta_1)\sum</em>}^t \beta_3^{t-i} \sum_{j=1}^i \beta_1^{i-j}\boldsymbol{g<em j="1">j = (1 - \beta_1)\sum</em>}^t \frac{\beta_3^{t-j+1} - \beta_1^{t-j+1}}{\beta_3 - \beta_1}\boldsymbol{g<em i="1">j\\<br />
\sum</em>}^t \beta_3^{t-i}\boldsymbol{v<em i="1">i = (1 - \beta_2)\sum</em>}^t \beta_3^{t-i} \sum_{j=1}^i \beta_2^{i-j}\boldsymbol{g<em j="1">j^2 = (1 - \beta_2)\sum</em>}^t \frac{\beta_3^{t-j+1} - \beta_2^{t-j+1}}{\beta_3 - \beta_2}\boldsymbol{g<em RMS="RMS">j^2\\<br />
\end{gather}<br />
最后一个双重求和化简，如果大家没有思路，可以交给Kimi完成（参考<a href="https://www.kimi.com/share/d3d35hpsfuv6jqe78c20">链接</a>）。由上式可知$\bar{\boldsymbol{m}}_t,\bar{\boldsymbol{v}}_t$分别是梯度和梯度平方的加权平均，所以求$\Vert \bar{\boldsymbol{u}}_t\Vert</em>^2$跟<a href="/archives/11267">《为什么Adam的Update RMS是0.2？》</a>求$\Vert \boldsymbol{u}<em RMS="RMS">t\Vert</em>^2$本质上是一样的，只不过加权系数不同。</p>
<h3 id="_7">步骤之三<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<p>我们先求分母<br />
\begin{equation}\begin{aligned}<br />
\mathbb{E}[\bar{\boldsymbol{v}}<em j="1">t] =&amp;\, \frac{(1 - \beta_3)(1 - \beta_2)}{1 - \beta_3^t}\sum</em>}^t \frac{\beta_3^{t-j+1} - \beta_2^{t-j+1}}{\beta_3 - \beta_2}\mathbb{E}[\boldsymbol{g<em j="1">j^2] \\<br />
=&amp;\, \frac{(1 - \beta_3)(1 - \beta_2)}{1 - \beta_3^t}\sum</em>^2) \\}^t \frac{\beta_3^{t-j+1} - \beta_2^{t-j+1}}{\beta_3 - \beta_2}(\boldsymbol{\mu}^2 + \boldsymbol{\sigma<br />
=&amp;\, \frac{(1 - \beta_3)(1 - \beta_2)}{(1 - \beta_3^t)(\beta_3 - \beta_2)}\left(\frac{\beta_3 - \beta_3^{t+1}}{1 - \beta_3} - \frac{\beta_2 - \beta_2^{t+1}}{1 - \beta_2}\right)(\boldsymbol{\mu}^2 + \boldsymbol{\sigma}^2) \\[5pt]<br />
\approx &amp;\, \boldsymbol{\mu}^2 + \boldsymbol{\sigma}^2<br />
\end{aligned}\end{equation}<br />
最后一步的约等号，是因为实际训练中，$\beta_3$会足够接近于1，而$\beta_2^{t+1}$会足够接近于0，但$\beta_3^{t+1}$不一定，所以我们将$\beta_2^{t+1}$替换成零，并在化简之后将独立的$\beta_3$替换成$1$，最后再加上近似$\beta_3^{t+1}\approx \beta_3^t$。</p>
<h3 id="_8">步骤之四<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<p>然后是$\mathbb{E}[\bar{\boldsymbol{m}}<em j="1">t^2] = \mathbb{E}[\bar{\boldsymbol{m}}_t]^2 + \mathbb{V}ar[\bar{\boldsymbol{m}}_t]$，$\mathbb{E}[\bar{\boldsymbol{m}}_t]$的计算跟$\mathbb{E}[\bar{\boldsymbol{v}}_t]$类似，结果是$\boldsymbol{\mu}$，$\mathbb{V}ar[\bar{\boldsymbol{m}}_t]$的计算我们则利用方差的平方可加性：<br />
\begin{equation}\begin{aligned}<br />
\mathbb{V}ar[\bar{\boldsymbol{m}}_t] =&amp;\, \frac{(1 - \beta_3)^2(1 - \beta_1)^2}{(1-\beta_3^t)^2}\sum</em>}^t \left(\frac{\beta_3^{t-j+1} - \beta_1^{t-j+1}}{\beta_3 - \beta_1}\right)^2\mathbb{V}ar[\boldsymbol{g<em j="1">j] \\<br />
=&amp;\, \frac{(1 - \beta_3)^2(1 - \beta_1)^2}{(1-\beta_3^t)^2}\sum</em>^2 \\}^t \left(\frac{\beta_3^{t-j+1} - \beta_1^{t-j+1}}{\beta_3 - \beta_1}\right)^2 \boldsymbol{\sigma<br />
=&amp;\, \frac{(1 - \beta_3)^2(1 - \beta_1)^2}{(1-\beta_3^t)^2(\beta_3 - \beta_1)^2}\left(\frac{\beta_3^2 - \beta_3^{2(t+1)}}{1 - \beta_3^2} + \frac{\beta_1^2 - \beta_1^{2(t+1)}}{1 - \beta_1^2} - 2\frac{\beta_1\beta_3 - \beta_1^{t+1}\beta_3^{t+1}}{1 - \beta_1\beta_3}\right) \boldsymbol{\sigma}^2 \\[5pt]<br />
\approx &amp;\, (1 - \beta_3)(1 + \beta_3^t)\boldsymbol{\sigma}^2/2(1 - \beta_3^t)<br />
\end{aligned}\end{equation}<br />
取约等号的理由同上。</p>
<h3 id="_9">步骤之五<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<p>代入上两节的计算结果，我们有<br />
\begin{equation}\mathbb{E}[\bar{\boldsymbol{u}}<em RMS="RMS">t^2] \approx \frac{\boldsymbol{\mu}^2 + (1 - \beta_3)(1 + \beta_3^t)\boldsymbol{\sigma}^2/2(1 - \beta_3^t)}{\boldsymbol{\mu}^2 + \boldsymbol{\sigma}^2}\end{equation}<br />
那么<br />
\begin{equation}\Vert\bar{\boldsymbol{u}}_t\Vert</em>}^2 \approx \frac{\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2 + (1 - \beta_3)(1 + \beta_3^t)/2(1 - \beta_3^t)}{\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2 + 1} \end{equation<br />
最终有<br />
\begin{equation}\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>}^2 \approx \beta_3^{2t}\Vert\boldsymbol{\theta<em RMS="RMS">0\Vert</em>}^2 + (1-\beta_3^t)^2 \frac{\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2 + (1 - \beta_3)(1 + \beta_3^t)/2(1 - \beta_3^t)}{\lambda^2(\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2 + 1)}\label{eq:theta-rms}\end{equation</p>
<h2 id="_10">结果浅析<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>式$\eqref{eq:theta-rms}$看起来比较复杂，我们观察几个特例。首先考虑$\boldsymbol{\mu}=\boldsymbol{0}$这个例子，此时<br />
\begin{equation}\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>}^2 \approx \beta_3^{2t}\Vert\boldsymbol{\theta<em RMS="RMS">0\Vert</em>}^2 + (1-\beta_3^{2t}) (1 - \beta_3)/2\lambda^2 = \beta_3^{2t}\Vert\boldsymbol{\theta<em RMS="RMS">0\Vert</em>}^2 + (1-\beta_3^{2t}) \eta/2\lambda\label{eq:theta-rms-mu0}\end{equation<br />
特别地，如果考虑$t\to\infty$，或者$\Vert\boldsymbol{\theta}<em RMS="RMS">0\Vert</em>^2$就初始化为$\eta/2\lambda$，那么就有<br />
\begin{equation}\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>} \approx \sqrt{\frac{\eta}{2\lambda}}\label{eq:theta-rms-simple}\end{equation<br />
这便是论文<a href="https://papers.cool/arxiv/2305.17212">《Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks》</a>给出的结果，跟原论文的假设一致，它是零均值下的随机游走的稳态结果。如果不考虑$t\to\infty$，而是考虑$\lambda\to 0$的极限，那么由式$\eqref{eq:theta-rms-mu0}$我们将得到<br />
\begin{equation}\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>}^2 \approx \Vert\boldsymbol{\theta<em RMS="RMS">0\Vert</em>}^2 + \eta^2 t\end{equation<br />
这表明在没有Weight Decay的时候，$\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>$得}$大致按照$\eta\sqrt{t}$的速度增长，这也表明在没有Weight Decay时，我们可以通过设置特定的学习率Schedule来实现Weight RMS的稳定性。另一方面，如果Batch Size足够大，导致信噪比项$\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2$占主导，那么由式$\eqref{eq:theta-rms<br />
\begin{equation}\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>}^2 \approx \beta_3^{2t}\Vert\boldsymbol{\theta<em RMS="RMS">0\Vert</em>}^2 + (1-\beta_3^t)^2 \frac{\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2}{\lambda^2(\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2 + 1)}\end{equation<br />
这个可能适用于模型需要主动增加Weight RMS的特殊情形。不过从经验来看，这种情况发生的概率一般比较小。</p>
<h2 id="_11">模拟实验<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h2>
<p>我们可以用如下模拟脚本，来简单验证上述的准确性：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">N</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">100000</span>
<span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.95</span>
<span class="n">m</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="p">(</span><span class="n">m</span> <span class="o">/</span> <span class="n">v</span><span class="o">**</span><span class="mf">0.5</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span>

<span class="n">weight_rms</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="kp">mean</span><span class="p">()</span><span class="o">**</span><span class="mf">0.5</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weight_rms</span><span class="p">)</span>
</code></pre></div>

<p>大家可以自行改变权重的初始化或梯度的均值方差等，看最终结果跟式$\eqref{eq:theta-rms}$的吻合程度，笔者自行试了一波，整体来说还是比较靠谱的。</p>
<h2 id="_12">符号版本<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h2>
<p>只需要将前述证明调整一下，就可以适用于“SignSGDM + Weight Decay”的组合：<br />
\begin{equation}\text{SignSGDM}\color{skyblue}{\text{W}}:=\left\{\begin{aligned}<br />
&amp;\boldsymbol{m}<em t-1="t-1">t = \beta_1 \boldsymbol{m}</em>} + \left(1 - \beta_1\right) \boldsymbol{g<em t-1="t-1">t\\<br />
&amp;\boldsymbol{u}_t = \newcommand{sign}{\mathop{\text{sign}}}\sign(\boldsymbol{m}_t)\\<br />
&amp;\boldsymbol{\theta}_t = \boldsymbol{\theta}</em>} - \eta_t (\boldsymbol{u<em t-1="t-1">t \color{skyblue}{ + \lambda_t \boldsymbol{\theta}</em>)}<br />
\end{aligned}\right.\end{equation}<br />
修改的地方是由于$\sign(\boldsymbol{m}<em i="1">t)=\boldsymbol{m}_t/\sqrt{\boldsymbol{m}_t^2}$，所以要将$\bar{\boldsymbol{v}}_t$的定义改为<br />
\begin{equation}\bar{\boldsymbol{v}}_t \triangleq \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i}\boldsymbol{m<em i="1">i^2\end{equation}<br />
那么<br />
\begin{equation}\mathbb{E}[\bar{\boldsymbol{v}}_t] = \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i}\mathbb{E}[\boldsymbol{m<em i="1">i^2] \approx \frac{1-\beta_3}{1-\beta_3^t}\sum</em>}^t \beta_3^{t-i}\mathbb{E}\left(\boldsymbol{\mu}^2 + \frac{1-\beta_1}{1 + \beta_1}\boldsymbol{\sigma}^2\right) = \boldsymbol{\mu}^2 + \frac{1-\beta_1}{1 + \beta_1}\boldsymbol{\sigma}^2\end{equation<br />
其中$\mathbb{E}[\boldsymbol{m}<em RMS="RMS">i^2]$的计算我们参考<a href="/archives/11267">《为什么Adam的Update RMS是0.2？》</a>或<a href="/archives/11301">《重新思考学习率与Batch Size（四）：EMA》</a>都行。利用上述结果，我们得到<br />
\begin{equation}\Vert\boldsymbol{\theta}_t\Vert</em>}^2 \approx \beta_3^{2t}\Vert\boldsymbol{\theta<em RMS="RMS">0\Vert</em>}^2 + (1-\beta_3^t)^2 \frac{\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2 + (1 - \beta_3)(1 + \beta_3^t)/2(1 - \beta_3^t)}{\lambda^2\left(\Vert\boldsymbol{\mu}\Vert^2/\Vert\boldsymbol{\sigma}\Vert^2 + \frac{1-\beta_1}{1 + \beta_1}\right)}\end{equation<br />
特别地，考虑$\boldsymbol{\mu}=0,t\to\infty$的极限，我们有<br />
\begin{equation}\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>}^2 \approx \sqrt{\frac{\eta}{2\lambda}\frac{1+\beta_1}{1 - \beta_1}}\end{equation<br />
这个结果也很合理，因为SignSGDMW的Update RMS是AdamW的$\sqrt{\frac{1+\beta_1}{1 - \beta_1}}$倍，所以同样$\eta,\lambda$下它的Weight RMS也是$\sqrt{\frac{1+\beta_1}{1 - \beta_1}}$倍。</p>
<h2 id="_13">相关分析<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h2>
<p>前面说了，结果$\eqref{eq:theta-rms-simple}$跟论文<a href="https://papers.cool/arxiv/2305.17212">《Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks》</a>是一致的，但我们的推导方法是完全不同的，并且能得到更一般的$\eqref{eq:theta-rms}$。不过，原论文也有一些很有意思的地方，比如它所提的 <strong>Total Update Contribution (TUC)</strong> 概念，就值得赏析一番。</p>
<p>TUC的思想是这样的：由于动量机制的存在，当前的梯度$\boldsymbol{g}<em t_1="t+1">t$不止停留在当前步骤，它还会影响到未来的步骤（但会打个“折扣”），所以假设训练步数趋于无穷，我们可以考虑当前梯度$\boldsymbol{g}_t$对整个训练过程的<strong>总贡献</strong> 。具体来说，对于Adam我们有$\boldsymbol{u}_t=\boldsymbol{m}_t/\sqrt{\boldsymbol{v}_t}$，当前$\boldsymbol{g}_t$对$\boldsymbol{u}_t$的贡献是$(1-\beta_1)\boldsymbol{g}_t/\sqrt{\boldsymbol{v}_t}$，下一步$\boldsymbol{g}_t$将会打个折扣（乘以$\beta_1$），而且分母改为$\boldsymbol{v}</em>$，依此类推，所以可以定义总贡献为<br />
\begin{equation}\tilde{\boldsymbol{u}}<em k="t">t = \sum</em>}^{\infty} (1-\beta_1)\beta_1^{k-t}\frac{\boldsymbol{g<em RMS="RMS">t}{\sqrt{\boldsymbol{v}_k}}\end{equation}<br />
这样我们就将更新$\boldsymbol{u}_1,\boldsymbol{u}_2,\boldsymbol{u}_3,\cdots$分解为更新$\tilde{\boldsymbol{u}}_1,\tilde{\boldsymbol{u}}_2,\tilde{\boldsymbol{u}}_3,\cdots$，这样的好处是每个$\tilde{\boldsymbol{u}}$只有单步梯度，那么我们就可以重复快速估计一节的推导：<br />
\begin{equation}\Vert\boldsymbol{\theta}_t\Vert</em>}^2 = \Vert\beta_3\boldsymbol{\theta<em RMS="RMS">{t-1} + (1-\beta_3)(-\tilde{\boldsymbol{u}}_t/\lambda)\Vert</em>}^2 \approx \beta_3^2\Vert\boldsymbol{\theta<em RMS="RMS">{t-1}\Vert</em>}^2 + (1-\beta_3)^2\Vert\tilde{\boldsymbol{u}<em RMS="RMS">t\Vert</em>}^2/\lambda^2 \label{eq:tilde-u-rms}\end{equation<br />
最后的近似依赖于$\boldsymbol{\theta}<em t-1="t-1">{t-1}\cdot\tilde{\boldsymbol{u}}_t\approx 0$，我们断言$\boldsymbol{\theta}</em>}\cdot\tilde{\boldsymbol{u}<em t-1="t-1">t$比$\boldsymbol{\theta}</em>}\cdot\boldsymbol{u<em t-1="t-1">t$更接近于零，因为$\tilde{\boldsymbol{u}}_t$只依赖于当前梯度$\boldsymbol{g}_t$，而$\boldsymbol{\theta}</em>}$还没接触到$\boldsymbol{g<em t-1="t-1">t$，所以它们是独立的变量，假设$\boldsymbol{g}_t$具有零均值时，$\boldsymbol{\theta}</em>}\cdot\tilde{\boldsymbol{u}<em RMS="RMS">t\approx 0$往往就容易成立了。而为了估计$\Vert\tilde{\boldsymbol{u}}_t\Vert</em>}^2$，原论文直接假设$\boldsymbol{g<em RMS="RMS">t/\sqrt{\boldsymbol{v}_k}$具有相同方向并且单位RMS，于是<br />
\begin{equation}\Vert\tilde{\boldsymbol{u}}_t\Vert</em>} = \sum_{k=t}^{\infty} (1-\beta_1)\beta_1^{k-t}\left\Vert\frac{\boldsymbol{g<em RMS="RMS">t}{\sqrt{\boldsymbol{v}_k}}\right\Vert</em>} = \sum_{k=t}^{\infty} (1-\beta_1)\beta_1^{k-t} = 1\end{equation<br />
代入式$\eqref{eq:tilde-u-rms}$，结合快速估计一节同样的近似处理，解得<br />
\begin{equation}\Vert\boldsymbol{\theta}<em RMS="RMS">t\Vert</em>} \approx \sqrt{\frac{\eta}{2\lambda}}\end{equation<br />
然而，如果局限在原论文看，我们会发现有很多近似是莫名其妙的，比如$\boldsymbol{v}<em RMS="RMS">t$中也有$\boldsymbol{g}_t$，所以说$\tilde{\boldsymbol{u}}_t$只包含当前$\boldsymbol{g}_t$的影响是不大准确的，还有$\Vert\boldsymbol{g}_t/\sqrt{\boldsymbol{v}_k}\Vert</em>=1$的断言也显得比较生硬。但如果放到本文来看，我们会发现在平均场近似下，原论文的各种操作会显得很合理，所以原论文其实已经隐含地用到了平均场方法。</p>
<h2 id="_14">文章小结<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h2>
<p>这篇文章我们用平均场近似推导了一个有趣且可能让人意外的结论：AdamW训出来的模型，其权重的RMS也是可以渐近估计出来的，一般情况下，它只依赖于学习率和Weight Decay。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11307">https://spaces.ac.cn/archives/11307</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Oct. 01, 2025). 《AdamW的Weight RMS的渐近估计 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11307">https://spaces.ac.cn/archives/11307</a></p>
<p>@online{kexuefm-11307,<br />
title={AdamW的Weight RMS的渐近估计},<br />
author={苏剑林},<br />
year={2025},<br />
month={Oct},<br />
url={\url{https://spaces.ac.cn/archives/11307}},<br />
} </p>
<hr />
<h2 id="_15">公式推导与注释<a class="toc-link" href="#_15" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="观测iss.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#132 观测ISS</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="msign的导数.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#134 msign的导数</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#adamwweight-rms">AdamW的Weight RMS的...</a><ul>
<li><a href="#_1">滑动视角</a></li>
<li><a href="#_2">加权平均</a></li>
<li><a href="#_3">快速估计</a></li>
<li><a href="#_4">更好近似</a><ul>
<li><a href="#_5">步骤之一</a></li>
<li><a href="#_6">步骤之二</a></li>
<li><a href="#_7">步骤之三</a></li>
<li><a href="#_8">步骤之四</a></li>
<li><a href="#_9">步骤之五</a></li>
</ul>
</li>
<li><a href="#_10">结果浅析</a></li>
<li><a href="#_11">模拟实验</a></li>
<li><a href="#_12">符号版本</a></li>
<li><a href="#_13">相关分析</a></li>
<li><a href="#_14">文章小结</a></li>
<li><a href="#_15">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>