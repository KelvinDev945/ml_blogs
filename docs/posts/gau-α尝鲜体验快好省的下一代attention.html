<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GAU-α：尝鲜体验快好省的下一代Attention | ML & Math Blog Posts</title>
    <meta name="description" content="GAU-α：尝鲜体验快好省的下一代Attention&para;
原文链接: https://spaces.ac.cn/archives/9052
发布日期: 

在《FLASH：可能是近来最有意思的高效Transformer设计》中，我们介绍了GAU（Gated Attention Unit，门控线性单元），在这里笔者愿意称之为“目前最有潜力的下一代Attention设计”，因为它真正达到了“更...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #180 GAU-α：尝鲜体验快好省的下一代Attention
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#180</span>
                GAU-α：尝鲜体验快好省的下一代Attention
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-04-22</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=预训练" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 预训练</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="gau-attention">GAU-α：尝鲜体验快好省的下一代Attention<a class="toc-link" href="#gau-attention" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9052">https://spaces.ac.cn/archives/9052</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在<a href="/archives/8934">《FLASH：可能是近来最有意思的高效Transformer设计》</a>中，我们介绍了GAU（Gated Attention Unit，门控线性单元），在这里笔者愿意称之为“目前最有潜力的下一代Attention设计”，因为它真正达到了“更快（速度）、更好（效果）、更省（显存）”的特点。</p>
<p>然而，有些读者在自己的测试中得到了相反的结果，比如收敛更慢、效果更差等，这与笔者的测试结果大相径庭。本文就来分享一下笔者自己的训练经验，并且放出一个尝鲜版“GAU-α”供大家测试。</p>
<blockquote>
<p><strong>开源地址：<a href="https://github.com/ZhuiyiTechnology/GAU-alpha">https://github.com/ZhuiyiTechnology/GAU-alpha</a></strong></p>
</blockquote>
<h2 id="gau-">GAU-α<a class="toc-link" href="#gau-" title="Permanent link">&para;</a></h2>
<p>首先介绍一下开源出来的“GAU-α”在CLUE任务上的成绩单：<br />
$$\small{\begin{array}{c|ccccccccccc}
\hline
&amp; \text{iflytek} &amp; \text{tnews} &amp; \text{afqmc} &amp; \text{cmnli} &amp; \text{ocnli} &amp; \text{wsc} &amp; \text{csl} &amp; \text{cmrc2018} &amp; \text{c3} &amp; \text{chid} &amp; \text{cluener}\\
\hline
\text{BERT} &amp; 60.06 &amp; 56.80 &amp; 72.41 &amp; 79.56 &amp; 73.93 &amp; 78.62 &amp; 83.93 &amp; 56.17 &amp; 60.54 &amp; 85.69 &amp; 79.45 \\
\text{RoBERTa} &amp; 60.64 &amp; \textbf{58.06} &amp; 74.05 &amp; 81.24 &amp; 76.00 &amp; \textbf{87.50} &amp; 84.50 &amp; 56.54 &amp; 67.66 &amp; 86.71 &amp; 79.47\\
\text{RoFormer} &amp; 60.91 &amp; 57.54 &amp; 73.52 &amp; 80.92 &amp; \textbf{76.07} &amp; 86.84 &amp; 84.63 &amp; 56.26 &amp; 67.24 &amp; 86.57 &amp; 79.72\\
\text{RoFormerV2}^* &amp; 60.87 &amp; 56.54 &amp; 72.75 &amp; 80.34 &amp; 75.36 &amp; 80.92 &amp; 84.67 &amp; 57.91 &amp; 64.62 &amp; 85.09 &amp; \textbf{81.08}\\
\hline
\text{GAU-}\alpha &amp; \textbf{61.41} &amp; 57.76 &amp; \textbf{74.17} &amp; \textbf{81.82} &amp; 75.86 &amp; 79.93 &amp; \textbf{85.67} &amp; \textbf{58.09} &amp; \textbf{68.24} &amp; \textbf{87.91} &amp; 80.01\\
\hline
\end{array}}$$</p>
<p>所有的模型都是Base版，上表显示的是CLUE任务上验证集上的结果，大家的运行方式和比较都是公平的，作为一个相对比较来说是合理的。另外，这里的RoFormerV2*并非<a href="/archives/8998">《RoFormerV2：自然语言理解的极限探索》</a>中的多任务版本，而是仅仅进行了MLM预训练的版本（该版本没开源），这样对比是因为GAU-α也仅仅进行了MLM预训练。</p>
<p>从表中可以看出，除了WSC这个数据量极少的“异类”外，GAU-α在多数任务上都有优势，并且除了WSC外的平均成绩是最好的。其中，RoFormerV2<em>与GAU-α的比较是最为公平的，因为它们的训练脚本、训练数据、整体结构都是一样的，唯一不同就是GAU-α是将RoFormerV2</em>中的Attention+FFN组合换成了两层GAU，两者对比充分显示出了GAU设计“更好”的特点。</p>
<p>此外，我们在<a href="/archives/8998">《RoFormerV2：自然语言理解的极限探索》</a>介绍过RoFormerV2对结构进行了简化，从而获得更快的速度，具有同样整体结构的GAU-α也是如此，所以GAU-α的速度是比表中的BERT、RoBERTa、RoFormer都要快的，但平均效果却更胜一筹。更进一步的测试显示，当序列长度超过512时，GAU-α的速度开始超过同样精简过的RoFormerV2，并且显存占用更低，越长则对GAU-α更有利。</p>
<h2 id="_1">训练<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>现在介绍一下模型的训练细节，完整的代码已经开源到Github中，如有疑惑可以对照着代码来读。</p>
<p><strong>模型架构</strong> ： GAU-α就是将RoFormerV2的Attention+FFN换成了两层GAU，在<a href="/archives/8934">之前的文章</a>中我们比较过两层GAU的计算量和参数量大致相当于Attention+FFN组合，所以这样的替换是合理的；RoFormerV2的特点是保留了Post Norm结构，去掉了所有的Bias项，并且Layer Norm换成了RMS Norm的最简单变体，在GAU-α中也是如此。</p>
<p><strong>归一化</strong> ： 在<a href="/archives/9019">《听说Attention与Softmax更配哦～》</a>中我们讨论过Attention的归一化问题，GAU-α的Attention归一化选取了其中笔者自行提出的具有较好外推能力的<a href="/archives/8823">熵不变性Softmax</a>（在bert4keras中暂称为softmax_plus）。</p>
<p><strong>训练方式</strong> ： 在初始化方面笔者按照<a href="/archives/8978">《训练1000层的Transformer究竟有什么困难？》</a>进行了调整，因此无须Wamrup就可以直接训练，优化器用的是LAMB，学习率分段线性衰减；预训练任务用的是全词MLM，分词工具用百度的LAC，这些跟RoFormerV2都是对齐的。</p>
<p>好像值得一提的也就这么多了，确实没进行多大的改变。除了在归一化方式上花了点时间进行测试，其他方面也没多费时间，直接训练就得到了不错的效果。</p>
<h2 id="_2">小结<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>GAU是笔者认为的“目前最有潜力的下一代Attention设计”，本文分享了GAU的一些训练经验，并开源了一个尝鲜版“GAU-α”。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9052">https://spaces.ac.cn/archives/9052</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 22, 2022). 《GAU-α：尝鲜体验快好省的下一代Attention 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9052">https://spaces.ac.cn/archives/9052</a></p>
<p>@online{kexuefm-9052,<br />
title={GAU-α：尝鲜体验快好省的下一代Attention},<br />
author={苏剑林},<br />
year={2022},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/9052}},<br />
} </p>
<hr />
<h2 id="_3">公式推导与注释<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<h3 id="1-gau">1. GAU架构基础<a class="toc-link" href="#1-gau" title="Permanent link">&para;</a></h3>
<h4 id="11-attention">1.1 标准Attention回顾<a class="toc-link" href="#11-attention" title="Permanent link">&para;</a></h4>
<p>标准的Scaled Dot-Product Attention定义为：
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_k}}\right)V \tag{1}
\end{equation}</p>
<p>其中 $Q, K, V \in \mathbb{R}^{n \times d}$ 分别是查询、键、值矩阵，$n$ 是序列长度，$d$ 是特征维度。</p>
<h4 id="12-gau">1.2 GAU的核心设计<a class="toc-link" href="#12-gau" title="Permanent link">&para;</a></h4>
<p>GAU（Gated Attention Unit）将注意力机制与门控机制结合，其核心形式为：
\begin{equation}
\text{GAU}(X) = (X \odot \text{Attention}(X)) W_O \tag{2}
\end{equation}</p>
<p>其中 $\odot$ 表示逐元素乘法（Hadamard积），这是GAU的关键创新点。</p>
<h3 id="2-gau">2. GAU完整数学推导<a class="toc-link" href="#2-gau" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 输入变换<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p>给定输入 $X \in \mathbb{R}^{n \times d}$，GAU首先通过三个线性变换：
\begin{equation}
U = XW_U + b_U \in \mathbb{R}^{n \times e} \tag{3}
\end{equation}
\begin{equation}
V = XW_V + b_V \in \mathbb{R}^{n \times e} \tag{4}
\end{equation}
\begin{equation}
\text{Base} = XW_{\text{base}} + b_{\text{base}} \in \mathbb{R}^{n \times e} \tag{5}
\end{equation}</p>
<p>其中 $e$ 是中间维度，通常 $e = 2d$ 以保持计算量平衡。</p>
<p><strong>推导注释</strong>：这三个变换的作用不同：
- $U$ 用于生成门控信号和注意力的查询/键
- $V$ 用于生成注意力的值
- $\text{Base}$ 作为基础表示，类似于残差连接的主路径</p>
<h4 id="22">2.2 门控机制<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>门控信号通过以下方式计算：
\begin{equation}
Z = \phi(U) \in \mathbb{R}^{n \times e} \tag{6}
\end{equation}</p>
<p>其中 $\phi$ 通常是 Swish 激活函数或 GELU：
\begin{equation}
\text{Swish}(x) = x \cdot \sigma(\beta x) = \frac{x}{1 + e^{-\beta x}} \tag{7}
\end{equation}</p>
<p><strong>推导分析</strong>：为什么使用 Swish？
计算 Swish 的梯度：
\begin{equation}
\frac{d\text{Swish}(x)}{dx} = \sigma(\beta x) + \beta x \sigma(\beta x)(1-\sigma(\beta x)) \tag{8}
\end{equation}</p>
<p>这个梯度形式在 $x&gt;0$ 时近似为1（类似ReLU），但在 $x&lt;0$ 时有小的非零值，避免了"神经元死亡"问题。</p>
<h4 id="23">2.3 注意力分数计算<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p>GAU使用单头注意力，查询和键都来自同一个矩阵：
\begin{equation}
Q = K = \gamma(U) \in \mathbb{R}^{n \times s} \tag{9}
\end{equation}</p>
<p>其中 $\gamma$ 是归一化函数（如RMSNorm），$s$ 是注意力维度。</p>
<p><strong>RMSNorm推导</strong>：
\begin{equation}
\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + \epsilon}} \tag{10}
\end{equation}</p>
<p>RMSNorm的梯度为：
\begin{equation}
\frac{\partial \text{RMSNorm}(x_i)}{\partial x_j} = \frac{1}{\text{RMS}(x)}\left(\delta_{ij} - \frac{x_i x_j}{\text{RMS}(x)^2}\right) \tag{11}
\end{equation}</p>
<p>注意力矩阵计算：
\begin{equation}
A_{raw} = QK^{\top} = \gamma(U)\gamma(U)^{\top} \in \mathbb{R}^{n \times n} \tag{12}
\end{equation}</p>
<h4 id="24">2.4 相对位置编码<a class="toc-link" href="#24" title="Permanent link">&para;</a></h4>
<p>GAU采用RoPE（Rotary Position Embedding）：
\begin{equation}
\text{RoPE}(x, m) = \begin{pmatrix} x_1 \ x_2 \ x_3 \ x_4 \ \vdots \end{pmatrix} \otimes \begin{pmatrix} \cos(m\theta_1) \ \cos(m\theta_1) \ \cos(m\theta_2) \ \cos(m\theta_2) \ \vdots \end{pmatrix} + \begin{pmatrix} -x_2 \ x_1 \ -x_4 \ x_3 \ \vdots \end{pmatrix} \otimes \begin{pmatrix} \sin(m\theta_1) \ \sin(m\theta_1) \ \sin(m\theta_2) \ \sin(m\theta_2) \ \vdots \end{pmatrix} \tag{13}
\end{equation}</p>
<p>其中频率定义为：
\begin{equation}
\theta_i = 10000^{-2i/d}, \quad i = 0, 1, \ldots, \frac{d}{2}-1 \tag{14}
\end{equation}</p>
<p><strong>RoPE性质推导</strong>：
对于位置 $m$ 和 $n$ 的两个向量，其内积满足：
\begin{equation}
\langle \text{RoPE}(q, m), \text{RoPE}(k, n) \rangle = \text{Re}\left(\sum_{i=1}^{d/2} (q_{2i-1} + iq_{2i})(k_{2i-1} - ik_{2i})e^{i(m-n)\theta_i}\right) \tag{15}
\end{equation}</p>
<p>这意味着内积只依赖于相对位置 $m-n$，具有平移不变性。</p>
<h4 id="25-softmax">2.5 熵不变性Softmax<a class="toc-link" href="#25-softmax" title="Permanent link">&para;</a></h4>
<p>这是GAU-α的关键创新，标准Softmax为：
\begin{equation}
a_{ij} = \frac{e^{s_{ij}}}{\sum_{k=1}^n e^{s_{ik}}} \tag{16}
\end{equation}</p>
<p>熵不变性Softmax引入对数缩放：
\begin{equation}
a_{ij} = \frac{e^{\lambda(n) s_{ij}}}{\sum_{k=1}^n e^{\lambda(n) s_{ik}}} \tag{17}
\end{equation}</p>
<p>其中缩放因子：
\begin{equation}
\lambda(n) = \frac{\log n}{\log 512} \tag{18}
\end{equation}</p>
<p><strong>熵不变性推导</strong>：
Shannon熵定义为：
\begin{equation}
H = -\sum_{j=1}^n a_{ij} \log a_{ij} \tag{19}
\end{equation}</p>
<p>代入式(17)：
\begin{equation}
H = -\sum_{j=1}^n a_{ij}\left(\lambda s_{ij} - \log\sum_k e^{\lambda s_{ik}}\right) = \log\sum_k e^{\lambda s_{ik}} - \lambda\sum_j a_{ij}s_{ij} \tag{20}
\end{equation}</p>
<p>假设 $s_{ij}$ 是独立同分布的随机变量，使用对数求和指数（LSE）的性质：
\begin{equation}
\log\sum_{k=1}^n e^{\lambda s_k} \approx \log n + \lambda \max_k s_k \quad \text{(当 } \lambda \text{ 足够大时)} \tag{21}
\end{equation}</p>
<p>更精确地，使用拉普拉斯近似：
\begin{equation}
\log\sum_{k=1}^n e^{\lambda s_k} \approx \log n + \mathbb{E}[\lambda s] + \frac{\lambda^2}{2}\text{Var}[s] \tag{22}
\end{equation}</p>
<p>为了使熵 $H$ 对 $n$ 不敏感，需要 $\log n$ 项被抵消，即：
\begin{equation}
\lambda \propto \log n \tag{23}
\end{equation}</p>
<h4 id="26">2.6 完整的注意力输出<a class="toc-link" href="#26" title="Permanent link">&para;</a></h4>
<p>结合位置编码和熵不变性Softmax：
\begin{equation}
A = \text{softmax}\left(\lambda(n) \frac{\text{RoPE}(Q)\text{RoPE}(K)^{\top}}{\sqrt{s}}\right) \in \mathbb{R}^{n \times n} \tag{24}
\end{equation}</p>
<p>注意力值：
\begin{equation}
O = AV \in \mathbb{R}^{n \times e} \tag{25}
\end{equation}</p>
<h4 id="27">2.7 门控融合<a class="toc-link" href="#27" title="Permanent link">&para;</a></h4>
<p>GAU的最终输出结合了门控和注意力：
\begin{equation}
Y = (Z \odot O + \text{Base})W_O \tag{26}
\end{equation}</p>
<p><strong>展开推导</strong>：
\begin{equation}
Y_i = \sum_{j=1}^e \left(\sum_{k=1}^n Z_{ik} \cdot A_{ik} \cdot V_{kj} + \text{Base}<em O_j_ell="O,j\ell">{ij}\right) W</em>
\end{equation}} \tag{27</p>
<p>其中 $Z \odot O$ 是逐元素门控，$\text{Base}$ 提供直接路径。</p>
<h3 id="3">3. 参数量和计算量分析<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 参数量计算<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>GAU单层的参数包括：
- $W_U, b_U$: $(d \times e) + e = de + e$
- $W_V, b_V$: $(d \times e) + e = de + e$
- $W_{\text{base}}, b_{\text{base}}$: $(d \times e) + e = de + e$
- $W_O$: $e \times d = ed$</p>
<p>总参数量：
\begin{equation}
P_{GAU} = 3(de + e) + ed = 4de + 3e \tag{28}
\end{equation}</p>
<p>取 $e = 2d$：
\begin{equation}
P_{GAU} = 4d \cdot 2d + 3 \cdot 2d = 8d^2 + 6d \approx 8d^2 \tag{29}
\end{equation}</p>
<p><strong>对比标准Transformer</strong>：
标准Attention+FFN的参数量：
- Attention (Q, K, V, O): $4(d \times d_k) + d_k \times d = 4dd_k + d_kd = 5dd_k$（取$d_k=d$）
- FFN: $d \times 4d + 4d \times d = 8d^2$</p>
<p>总计：
\begin{equation}
P_{\text{Transformer}} = 5d^2 + 8d^2 = 13d^2 \tag{30}
\end{equation}</p>
<p>两层GAU的参数量：
\begin{equation}
P_{2\times GAU} = 2 \times 8d^2 = 16d^2 \tag{31}
\end{equation}</p>
<p>相比标准Transformer，参数量比例：
\begin{equation}
\frac{P_{2\times GAU}}{P_{\text{Transformer}}} = \frac{16d^2}{13d^2} \approx 1.23 \tag{32}
\end{equation}</p>
<p><strong>结论</strong>：两层GAU参数量略多于Attention+FFN，但由于去除了多头机制和某些归一化层，实际可比。</p>
<h4 id="32-flops">3.2 计算量分析（FLOPs）<a class="toc-link" href="#32-flops" title="Permanent link">&para;</a></h4>
<p>单个GAU的前向传播FLOPs：</p>
<p><strong>步骤1</strong>：线性变换 $U, V, \text{Base}$
\begin{equation}
\text{FLOPs}_1 = 3 \times (2n \times d \times e) = 6nde \tag{33}
\end{equation}</p>
<p><strong>步骤2</strong>：激活函数（忽略，相对较小）</p>
<p><strong>步骤3</strong>：注意力矩阵 $QK^{\top}$
\begin{equation}
\text{FLOPs}_2 = 2n^2s \tag{34}
\end{equation}</p>
<p><strong>步骤4</strong>：Softmax（忽略，相对较小）</p>
<p><strong>步骤5</strong>：注意力值 $AV$
\begin{equation}
\text{FLOPs}_3 = 2n^2e \tag{35}
\end{equation}</p>
<p><strong>步骤6</strong>：门控 $Z \odot O$（逐元素，忽略）</p>
<p><strong>步骤7</strong>：输出投影 $W_O$
\begin{equation}
\text{FLOPs}_4 = 2ned \tag{36}
\end{equation}</p>
<p>总FLOPs（取 $e=2d, s=d/2$）：
\begin{equation}
\text{FLOPs}_{GAU} = 6n \cdot d \cdot 2d + 2n^2 \cdot \frac{d}{2} + 2n^2 \cdot 2d + 2n \cdot 2d \cdot d \tag{37}
\end{equation}
\begin{equation}
= 12nd^2 + n^2d + 4n^2d + 4nd^2 = 16nd^2 + 5n^2d \tag{38}
\end{equation}</p>
<p><strong>对比标准Attention</strong>：
\begin{equation}
\text{FLOPs}_{\text{Attention}} = 4nd^2 + 2n^2d + 2n^2d + 2nd^2 = 6nd^2 + 4n^2d \tag{39}
\end{equation}</p>
<p>FFN的FLOPs：
\begin{equation}
\text{FLOPs}_{\text{FFN}} = 2n \cdot d \cdot 4d + 2n \cdot 4d \cdot d = 16nd^2 \tag{40}
\end{equation}</p>
<p>标准Transformer总计：
\begin{equation}
\text{FLOPs}_{\text{Transformer}} = 22nd^2 + 4n^2d \tag{41}
\end{equation}</p>
<p>两层GAU：
\begin{equation}
\text{FLOPs}_{2\times GAU} = 2(16nd^2 + 5n^2d) = 32nd^2 + 10n^2d \tag{42}
\end{equation}</p>
<p><strong>复杂度对比</strong>：
- 当 $n \ll d$ 时，$\text{FLOPs}<em _text_Transformer="\text{Transformer">{2\times GAU} \approx 32nd^2$ vs $\text{FLOPs}</em> \approx 22nd^2$，GAU约慢1.45倍
- 当 $n \approx d$ 时，两者相当
- 当 $n \gg d$ 时，GAU由于 $10n^2d$ vs $4n^2d$ 的优势，在长序列上更优}</p>
<h4 id="33">3.3 显存占用分析<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p><strong>激活值显存</strong>（需要保存用于反向传播）：
- $U, V, \text{Base}$: $3ne$
- $Z$: $ne$
- $Q, K$: $2ns$
- $A$: $n^2$（注意力矩阵）
- $O$: $ne$</p>
<p>总激活显存：
\begin{equation}
M_{GAU} = 5ne + 2ns + n^2 \tag{43}
\end{equation}</p>
<p>取 $e=2d, s=d/2$：
\begin{equation}
M_{GAU} = 10nd + nd + n^2 = 11nd + n^2 \tag{44}
\end{equation}</p>
<p><strong>标准Attention</strong>：
\begin{equation}
M_{\text{Attention}} = 4nd + n^2 \quad (\text{Q, K, V, A}) \tag{45}
\end{equation}</p>
<p>FFN显存（假设 $d_{\text{ffn}}=4d$）：
\begin{equation}
M_{\text{FFN}} = 4nd \tag{46}
\end{equation}</p>
<p>总计：
\begin{equation}
M_{\text{Transformer}} = 8nd + n^2 \tag{47}
\end{equation}</p>
<p>两层GAU：
\begin{equation}
M_{2\times GAU} = 22nd + 2n^2 \tag{48}
\end{equation}</p>
<p>显存比例：
\begin{equation}
\frac{M_{2\times GAU}}{M_{\text{Transformer}}} = \frac{22nd + 2n^2}{8nd + n^2} \tag{49}
\end{equation}</p>
<p>当 $n$ 较大时，$\frac{M_{2\times GAU}}{M_{\text{Transformer}}} \to 2$，显存约2倍。</p>
<p><strong>优化</strong>：通过梯度检查点（Gradient Checkpointing）可以权衡计算和显存：
- 不保存中间激活 $U, V, Z$ 等
- 反向传播时重新计算
- 显存减少至：$M_{GAU}^{\text{opt}} \approx n^2 + 2nd$</p>
<h3 id="4">4. 训练技巧和稳定性<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 权重初始化<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p>GAU使用修改的Xavier初始化。对于权重矩阵 $W \in \mathbb{R}^{m \times n}$：
\begin{equation}
W_{ij} \sim \mathcal{N}\left(0, \frac{2}{m + n}\right) \tag{50}
\end{equation}</p>
<p><strong>推导依据</strong>：假设输入 $x \sim \mathcal{N}(0, \sigma_x^2)$，输出 $y = Wx$：
\begin{equation}
\text{Var}[y_i] = \text{Var}\left[\sum_{j=1}^m W_{ij}x_j\right] = \sum_{j=1}^m \text{Var}[W_{ij}]\text{Var}[x_j] = m \cdot \frac{2}{m+n} \cdot \sigma_x^2 \tag{51}
\end{equation}</p>
<p>当 $m=n$ 时，$\text{Var}[y_i] = \sigma_x^2$，保持方差稳定。</p>
<p><strong>深层调整</strong>：对于第 $\ell$ 层，缩放初始化：
\begin{equation}
W^{(\ell)} \sim \mathcal{N}\left(0, \frac{2}{(m+n)\sqrt{\ell}}\right) \tag{52}
\end{equation}</p>
<p>这基于以下观察：深层网络中，梯度会随层数累积，除以 $\sqrt{\ell}$ 可以稳定训练。</p>
<h4 id="42">4.2 学习率调度<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p>GAU-α使用分段线性衰减，定义为：
\begin{equation}
\eta(t) = \begin{cases}
\eta_{\max} &amp; t \leq t_0 \
\eta_{\max} \cdot \frac{T - t}{T - t_0} &amp; t_0 &lt; t \leq T \
0 &amp; t &gt; T
\end{cases} \tag{53}
\end{equation}</p>
<p>其中 $\eta_{\max}$ 是最大学习率，$t_0$ 是开始衰减的步数，$T$ 是总步数。</p>
<p><strong>与Warmup对比</strong>：标准Warmup为：
\begin{equation}
\eta_{\text{warmup}}(t) = \begin{cases}
\eta_{\max} \cdot \frac{t}{t_{\text{warmup}}} &amp; t \leq t_{\text{warmup}} \
\eta_{\max} &amp; t &gt; t_{\text{warmup}}
\end{cases} \tag{54}
\end{equation}</p>
<p>GAU-α由于更好的初始化，可以省略Warmup直接使用恒定学习率再衰减。</p>
<h4 id="43-lamb">4.3 优化器：LAMB<a class="toc-link" href="#43-lamb" title="Permanent link">&para;</a></h4>
<p>LAMB（Layer-wise Adaptive Moments optimizer for Batch training）是Adam的扩展：
\begin{equation}
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \tag{55}
\end{equation}
\begin{equation}
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \tag{56}
\end{equation}
\begin{equation}
\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \tag{57}
\end{equation}</p>
<p>Adam更新：
\begin{equation}
\theta_t^{\text{Adam}} = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} \tag{58}
\end{equation}</p>
<p>LAMB额外的层归一化：
\begin{equation}
r_1 = |\theta_{t-1}|<em t-1="t-1">2, \quad r_2 = \left|\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\right|_2 \tag{59}
\end{equation}
\begin{equation}
\theta_t^{\text{LAMB}} = \theta</em>
\end{equation}} - \eta \frac{r_1}{r_2} \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} \tag{60</p>
<p><strong>推导意义</strong>：比值 $\frac{r_1}{r_2}$ 使得更新步长相对于参数范数进行调整，防止大参数层更新过快。</p>
<p><strong>收敛性分析</strong>：在凸优化设置下，LAMB的收敛率为：
\begin{equation}
\mathbb{E}[f(\theta_T) - f(\theta^*)] \leq \mathcal{O}\left(\frac{1}{\sqrt{T}}\right) \tag{61}
\end{equation}</p>
<h4 id="44">4.4 梯度裁剪<a class="toc-link" href="#44" title="Permanent link">&para;</a></h4>
<p>全局梯度范数裁剪：
\begin{equation}
g_{\text{clip}} = \begin{cases}
g &amp; |g|_2 \leq \tau \
\frac{\tau}{|g|_2} g &amp; |g|_2 &gt; \tau
\end{cases} \tag{62}
\end{equation}</p>
<p>其中 $\tau$ 是裁剪阈值（通常取1.0）。</p>
<p><strong>梯度爆炸分析</strong>：在深度为 $L$ 的网络中，梯度传播满足：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \theta^{(1)}} = \frac{\partial \mathcal{L}}{\partial \theta^{(L)}} \prod_{\ell=1}^{L-1} \frac{\partial \theta^{(\ell+1)}}{\partial \theta^{(\ell)}} \tag{63}
\end{equation}</p>
<p>若 $\left|\frac{\partial \theta^{(\ell+1)}}{\partial \theta^{(\ell)}}\right| &gt; 1$，则梯度指数增长。裁剪确保：
\begin{equation}
|g_{\text{clip}}|_2 \leq \tau \tag{64}
\end{equation}</p>
<h4 id="45-post-norm-vs-pre-norm">4.5 Post Norm vs Pre Norm<a class="toc-link" href="#45-post-norm-vs-pre-norm" title="Permanent link">&para;</a></h4>
<p>GAU-α使用Post Norm结构：
\begin{equation}
X_{\ell+1} = \text{Norm}(X_{\ell} + \text{GAU}(X_{\ell})) \tag{65}
\end{equation}</p>
<p>对比Pre Norm：
\begin{equation}
X_{\ell+1}^{\text{Pre}} = X_{\ell} + \text{GAU}(\text{Norm}(X_{\ell})) \tag{66}
\end{equation}</p>
<p><strong>梯度流分析</strong>：Post Norm的梯度：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial X_{\ell}} = \frac{\partial \mathcal{L}}{\partial X_{\ell+1}} \frac{\partial \text{Norm}(X_{\ell} + \text{GAU}(X_{\ell}))}{\partial X_{\ell}} \tag{67}
\end{equation}</p>
<p>Pre Norm的梯度：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial X_{\ell}} = \frac{\partial \mathcal{L}}{\partial X_{\ell+1}} \left(I + \frac{\partial \text{GAU}(\text{Norm}(X_{\ell}))}{\partial X_{\ell}}\right) \tag{68}
\end{equation}</p>
<p>Pre Norm由于 $I$ 的存在，梯度更容易直接传播，但Post Norm在配合好的初始化后，训练更稳定且效果更好。</p>
<h4 id="46-rms-norm">4.6 RMS Norm细节<a class="toc-link" href="#46-rms-norm" title="Permanent link">&para;</a></h4>
<p>RMSNorm相比LayerNorm省略了减均值操作：
\begin{equation}
\text{LayerNorm}(x) = \frac{x - \mu}{\sigma} \cdot \gamma + \beta \tag{69}
\end{equation}
\begin{equation}
\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot \gamma \tag{70}
\end{equation}</p>
<p>其中 $\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2}$。</p>
<p><strong>计算优势</strong>：
- LayerNorm需要两遍扫描（一遍计算均值，一遍计算方差）
- RMSNorm只需一遍扫描计算平方和</p>
<p><strong>理论分析</strong>：对于零均值输入（通过前一层归一化保证），有：
\begin{equation}
\text{Var}[x] = \mathbb{E}[x^2] - (\mathbb{E}[x])^2 \approx \mathbb{E}[x^2] = \text{RMS}(x)^2 \tag{71}
\end{equation}</p>
<p>因此RMSNorm近似LayerNorm，但计算更快。</p>
<p><strong>参数量对比</strong>：
- LayerNorm: $2d$ 参数 ($\gamma, \beta$)
- RMSNorm: $d$ 参数 (仅 $\gamma$)</p>
<h3 id="5-softmax">5. 熵不变性Softmax详细推导<a class="toc-link" href="#5-softmax" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 问题设定<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>给定注意力分数 $s_{ij} = \frac{q_i \cdot k_j}{\sqrt{d}}$，标准Softmax：
\begin{equation}
a_{ij} = \frac{\exp(s_{ij})}{\sum_{k=1}^n \exp(s_{ik})} \tag{72}
\end{equation}</p>
<p>熵定义：
\begin{equation}
H_i = -\sum_{j=1}^n a_{ij} \log a_{ij} \tag{73}
\end{equation}</p>
<p><strong>问题</strong>：当序列长度 $n$ 增加时，$H_i$ 也会增加，导致注意力更分散。</p>
<h4 id="52">5.2 熵的期望值<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>展开熵的表达式：
\begin{equation}
H_i = -\sum_{j=1}^n a_{ij} \left(s_{ij} - \log \sum_k e^{s_{ik}}\right) = \log \sum_k e^{s_{ik}} - \sum_j a_{ij} s_{ij} \tag{74}
\end{equation}</p>
<p>定义配分函数：
\begin{equation}
Z_i = \sum_{k=1}^n e^{s_{ik}} \tag{75}
\end{equation}</p>
<p>则：
\begin{equation}
H_i = \log Z_i - \mathbb{E}<em ij="ij">{j \sim a_i}[s</em>
\end{equation}}] \tag{76</p>
<h4 id="53">5.3 独立同分布假设<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p>假设 $s_{ij}$ 独立同分布，服从某分布 $p(s)$。则：
\begin{equation}
\mathbb{E}[\log Z_i] = \mathbb{E}\left[\log \sum_{k=1}^n e^{s_k}\right] \tag{77}
\end{equation}</p>
<p>使用Jensen不等式：
\begin{equation}
\mathbb{E}[\log Z] = \mathbb{E}\left[\log \sum_{k=1}^n e^{s_k}\right] \geq \log \mathbb{E}\left[\sum_{k=1}^n e^{s_k}\right] = \log(n \mathbb{E}[e^s]) \tag{78}
\end{equation}</p>
<p>更精确的估计使用鞍点近似：
\begin{equation}
\log \sum_{k=1}^n e^{s_k} \approx \log n + \log \mathbb{E}[e^s] + \mathcal{O}(1/n) \tag{79}
\end{equation}</p>
<h4 id="54">5.4 高斯假设下的推导<a class="toc-link" href="#54" title="Permanent link">&para;</a></h4>
<p>假设 $s_{ij} \sim \mathcal{N}(\mu, \sigma^2)$，则：
\begin{equation}
\mathbb{E}[e^s] = e^{\mu + \sigma^2/2} \tag{80}
\end{equation}</p>
<p>因此：
\begin{equation}
\mathbb{E}[H_i] \approx \log n + \mu + \frac{\sigma^2}{2} - \mathbb{E}[\mathbb{E}<em ij="ij">{j \sim a_i}[s</em>
\end{equation}}]] \tag{81</p>
<p>假设注意力集中在top-k个位置，则第二项约为：
\begin{equation}
\mathbb{E}[\mathbb{E}<em ij="ij">{j \sim a_i}[s</em>
\end{equation}}]] \approx \mu + c\sigma \tag{82</p>
<p>其中 $c$ 是常数（约为1）。代入得：
\begin{equation}
\mathbb{E}[H_i] \approx \log n + \frac{\sigma^2}{2} - c\sigma \tag{83}
\end{equation}</p>
<p><strong>关键观察</strong>：熵的主要 $n$ 依赖项是 $\log n$。</p>
<h4 id="55">5.5 缩放因子设计<a class="toc-link" href="#55" title="Permanent link">&para;</a></h4>
<p>为了抵消 $\log n$ 的影响，引入缩放因子 $\lambda(n)$：
\begin{equation}
a_{ij}^{\text{new}} = \frac{\exp(\lambda(n) s_{ij})}{\sum_k \exp(\lambda(n) s_{ik})} \tag{84}
\end{equation}</p>
<p>新的熵：
\begin{equation}
H_i^{\text{new}} = \log \sum_k e^{\lambda s_k} - \lambda \mathbb{E}<em ij="ij">{j \sim a_i^{\text{new}}}[s</em>
\end{equation}}] \tag{85</p>
<p>使用前面的近似：
\begin{equation}
\log \sum_k e^{\lambda s_k} \approx \log n + \lambda\mu + \frac{\lambda^2\sigma^2}{2} \tag{86}
\end{equation}</p>
<p>第二项：
\begin{equation}
\lambda \mathbb{E}<em ij="ij">{j \sim a_i^{\text{new}}}[s</em>
\end{equation}}] \approx \lambda(\mu + c\sigma\sqrt{\lambda}) \tag{87</p>
<p>其中 $\sqrt{\lambda}$ 来自于softmax在缩放后的锐化效应。</p>
<p>代入得：
\begin{equation}
H_i^{\text{new}} \approx \log n + \lambda\mu + \frac{\lambda^2\sigma^2}{2} - \lambda\mu - c\sigma\lambda^{3/2} \tag{88}
\end{equation}
\begin{equation}
= \log n + \frac{\lambda^2\sigma^2}{2} - c\sigma\lambda^{3/2} \tag{89}
\end{equation}</p>
<p>为了使 $H_i^{\text{new}}$ 对 $n$ 不敏感，需要：
\begin{equation}
\frac{\partial H_i^{\text{new}}}{\partial n} \approx 0 \tag{90}
\end{equation}</p>
<p>即：
\begin{equation}
\frac{1}{n} + \left(\lambda\sigma^2 - \frac{3c\sigma}{2}\lambda^{1/2}\right)\frac{d\lambda}{dn} \approx 0 \tag{91}
\end{equation}</p>
<p>若 $\lambda$ 主导项与 $\log n$ 成正比，设 $\lambda = \alpha \log n$：
\begin{equation}
\frac{d\lambda}{dn} = \frac{\alpha}{n} \tag{92}
\end{equation}</p>
<p>代入：
\begin{equation}
\frac{1}{n} + \frac{\alpha}{n}\left(\alpha\sigma^2\log n - \frac{3c\sigma}{2}\sqrt{\alpha\log n}\right) \approx 0 \tag{93}
\end{equation}</p>
<p>当 $n$ 足够大时，第一项可忽略，得：
\begin{equation}
\alpha\sigma^2\log n \approx \frac{3c\sigma}{2}\sqrt{\alpha\log n} \tag{94}
\end{equation}</p>
<p>解得：
\begin{equation}
\alpha \approx \frac{9c^2}{4\sigma^2 \log n} \cdot \log n = \frac{9c^2}{4\sigma^2} \tag{95}
\end{equation}</p>
<p><strong>实践选择</strong>：归一化后 $\sigma \approx 1$，$c \approx 1$，因此：
\begin{equation}
\lambda(n) = \kappa \log n \tag{96}
\end{equation}</p>
<p>其中 $\kappa$ 是可调超参数，实验中取 $\kappa = \frac{1}{\log 512}$ 使得 $n=512$ 时退化为标准Softmax。</p>
<h4 id="56">5.6 信息论解释<a class="toc-link" href="#56" title="Permanent link">&para;</a></h4>
<p>从信息论角度，熵 $H$ 度量分布的"不确定性"或"信息量"。</p>
<p><strong>互信息</strong>：注意力机制可视为 $Q$ 和 $K$ 之间的信息传递，互信息：
\begin{equation}
I(Q; K) = H(K) - H(K|Q) \tag{97}
\end{equation}</p>
<p>其中：
\begin{equation}
H(K|Q=q_i) = H_i = -\sum_j a_{ij} \log a_{ij} \tag{98}
\end{equation}</p>
<p>当 $n$ 增加时，$H(K)$ 增加（更多选择），但我们希望 $H(K|Q)$ 保持不变（给定查询后，关键token的不确定性不变），从而：
\begin{equation}
I(Q; K) \propto \log n \tag{99}
\end{equation}</p>
<p>互信息随 $n$ 增长，这是合理的（更多token提供更多信息）。</p>
<h3 id="6-attention">6. 与标准Attention的对比<a class="toc-link" href="#6-attention" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 公式对比<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方面</th>
<th>标准Attention</th>
<th>GAU-α</th>
</tr>
</thead>
<tbody>
<tr>
<td>查询/键</td>
<td>$Q \neq K$</td>
<td>$Q = K$</td>
</tr>
<tr>
<td>多头</td>
<td>是</td>
<td>否（单头）</td>
</tr>
<tr>
<td>门控</td>
<td>无</td>
<td>有 ($Z \odot O$)</td>
</tr>
<tr>
<td>缩放</td>
<td>$1/\sqrt{d}$</td>
<td>$\frac{\log n}{\sqrt{d}\log 512}$</td>
</tr>
<tr>
<td>归一化</td>
<td>LayerNorm</td>
<td>RMSNorm</td>
</tr>
<tr>
<td>位置编码</td>
<td>可选</td>
<td>RoPE</td>
</tr>
</tbody>
</table>
<h4 id="62">6.2 性能对比推导<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p><strong>标准Attention的有效秩</strong>：
注意力矩阵 $A$ 的秩最多为 $\min(n, d)$，但实际有效秩由奇异值分布决定：
\begin{equation}
r_{\text{eff}} = \frac{(\sum_i \sigma_i)^2}{\sum_i \sigma_i^2} \tag{100}
\end{equation}</p>
<p><strong>GAU的有效秩</strong>：由于门控机制，GAU的输出可以表示为：
\begin{equation}
Y = Z \odot (AV) + \text{Base} \tag{101}
\end{equation}</p>
<p>有效秩增加到：
\begin{equation}
r_{\text{eff}}^{\text{GAU}} \geq r_{\text{eff}}^{\text{Attention}} \tag{102}
\end{equation}</p>
<p>因为 $Z \odot (AV)$ 允许逐位置的不同缩放，增加了表达能力。</p>
<h4 id="63">6.3 长度外推性对比<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p><strong>标准Attention</strong>：设训练长度 $n_{\text{train}} = 512$，测试长度 $n_{\text{test}} = 1024$。</p>
<p>熵变化：
\begin{equation}
\Delta H = H(n_{\text{test}}) - H(n_{\text{train}}) \approx \log \frac{n_{\text{test}}}{n_{\text{train}}} = \log 2 \approx 0.693 \tag{103}
\end{equation}</p>
<p>这意味着注意力更分散，性能下降。</p>
<p><strong>GAU-α</strong>：使用 $\lambda(n) = \frac{\log n}{\log 512}$：
\begin{equation}
H^{\text{GAU}}(n) \approx \log n + C - \lambda(n) \cdot f(n) \tag{104}
\end{equation}</p>
<p>其中 $f(n)$ 是关于 $n$ 缓慢变化的函数。代入 $\lambda(n)$：
\begin{equation}
H^{\text{GAU}}(n) \approx \log n + C - \frac{\log n}{\log 512} \cdot f(n) \approx \text{const} \tag{105}
\end{equation}</p>
<p>因此熵基本不变，长度外推性更好。</p>
<p><strong>实验验证</strong>（论文数据）：
\begin{equation}
\text{Accuracy}<em _text_Attention-E="\text{Attention-E">{\text{Attention-O}}(n=256) = 23.02\% \tag{106}
\end{equation}
\begin{equation}
\text{Accuracy}</em>
\end{equation}}}(n=256) = 34.04\% \tag{107</p>
<p>提升：
\begin{equation}
\Delta = \frac{34.04 - 23.02}{23.02} \approx 47.8\% \tag{108}
\end{equation}</p>
<h3 id="7">7. 高级话题<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71-gau">7.1 GAU的梯度流分析<a class="toc-link" href="#71-gau" title="Permanent link">&para;</a></h4>
<p>考虑损失 $\mathcal{L}$ 对输入 $X$ 的梯度：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial X} = \frac{\partial \mathcal{L}}{\partial Y} \frac{\partial Y}{\partial X} \tag{109}
\end{equation}</p>
<p>展开：
\begin{equation}
\frac{\partial Y}{\partial X} = W_O^{\top} \frac{\partial}{\partial X}(Z \odot O + \text{Base}) \tag{110}
\end{equation}</p>
<p>包含三条路径：
1. <strong>门控路径</strong>：$\frac{\partial Z}{\partial X} = \frac{\partial \phi(U)}{\partial U} \frac{\partial U}{\partial X}$
2. <strong>注意力路径</strong>：$\frac{\partial O}{\partial X} = \frac{\partial (AV)}{\partial X}$
3. <strong>残差路径</strong>：$\frac{\partial \text{Base}}{\partial X} = W_{\text{base}}$</p>
<p>总梯度：
\begin{equation}
\frac{\partial \mathcal{L}}{\partial X} = W_O^{\top}\left(O \odot \frac{\partial Z}{\partial X} + Z \odot \frac{\partial O}{\partial X} + W_{\text{base}}\right) \tag{111}
\end{equation}</p>
<p><strong>梯度范数估计</strong>：假设各项独立：
\begin{equation}
\mathbb{E}\left[\left|\frac{\partial \mathcal{L}}{\partial X}\right|^2\right] \approx |W_O|^2 \left(|O|^2|\nabla Z|^2 + |Z|^2|\nabla O|^2 + |W_{\text{base}}|^2\right) \tag{112}
\end{equation}</p>
<p>由于 $Z, O, \text{Base}$ 都有 $\mathcal{O}(ne)$ 的元素，梯度范数为 $\mathcal{O}(\sqrt{ne})$，与标准Attention的 $\mathcal{O}(\sqrt{nd})$ 相当（$e \approx 2d$）。</p>
<h4 id="72">7.2 理论收敛性<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p><strong>假设</strong>：损失函数 $\mathcal{L}$ 是 $L$-光滑的，即：
\begin{equation}
|\nabla \mathcal{L}(\theta_1) - \nabla \mathcal{L}(\theta_2)| \leq L|\theta_1 - \theta_2| \tag{113}
\end{equation}</p>
<p>使用LAMB优化器，学习率 $\eta &lt; \frac{1}{L}$，经过 $T$ 步后：
\begin{equation}
\min_{t \leq T} \mathbb{E}[|\nabla \mathcal{L}(\theta_t)|^2] \leq \frac{2(\mathcal{L}(\theta_0) - \mathcal{L}^*)}{\eta T} + \eta L \sigma^2 \tag{114}
\end{equation}</p>
<p>其中 $\sigma^2$ 是梯度方差。</p>
<p><strong>GAU的优势</strong>：由于门控机制提供了更平滑的损失面（实验观察），$L$ 更小，允许更大的学习率。</p>
<h4 id="73">7.3 表达能力分析<a class="toc-link" href="#73" title="Permanent link">&para;</a></h4>
<p><strong>定理（非正式）</strong>：两层GAU可以近似任意Attention+FFN组合。</p>
<p><strong>证明思路</strong>：
1. FFN可以表示为：$\text{FFN}(x) = W_2 \sigma(W_1 x)$
2. GAU的 $Z \odot O$ 可以模拟门控FFN：$Z$ 对应 $\sigma(W_1 x)$，$O$ 对应 $W_2$
3. Attention部分直接对应
4. 通过两层GAU，可以分别处理Attention和FFN功能</p>
<p><strong>通用逼近</strong>：GAU作为非线性变换，满足通用逼近定理的条件（包含非线性激活和足够宽度）。</p>
<h3 id="8">8. 实现细节和优化<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 融合算子<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p><strong>Softmax融合</strong>：将缩放、指数、求和、除法融合为单个CUDA kernel：
\begin{equation}
\text{FusedSoftmax}(s, \lambda, n) = \frac{\exp(\lambda s)}{\sum_k \exp(\lambda s_k)} \tag{115}
\end{equation}</p>
<p><strong>加速比</strong>：通过减少内存访问，融合算子可达到 $2\times$ 加速。</p>
<h4 id="82">8.2 混合精度训练<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>使用FP16存储权重和激活，但累积梯度用FP32：
\begin{equation}
\theta_t^{\text{FP32}} = \theta_{t-1}^{\text{FP32}} - \eta \cdot \text{FP32}(\nabla \mathcal{L}(\text{FP16}(\theta_{t-1}))) \tag{116}
\end{equation}</p>
<p><strong>动态缩放</strong>：为避免下溢，梯度乘以缩放因子 $s$：
\begin{equation}
g_{\text{scaled}} = s \cdot g \tag{117}
\end{equation}</p>
<p>更新时除以 $s$：
\begin{equation}
\theta_t = \theta_{t-1} - \eta \frac{g_{\text{scaled}}}{s} \tag{118}
\end{equation}</p>
<p>$s$ 动态调整：若梯度溢出（出现inf/nan），则 $s \leftarrow s/2$；若连续1000步无溢出，则 $s \leftarrow 2s$。</p>
<h4 id="83">8.3 分布式训练<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p><strong>数据并行</strong>：将batch分割到 $N$ 个GPU，每个GPU计算局部梯度 $g_i$，然后AllReduce求平均：
\begin{equation}
g = \frac{1}{N}\sum_{i=1}^N g_i \tag{119}
\end{equation}</p>
<p><strong>梯度累积</strong>：当GPU显存不足时，累积 $K$ 个mini-batch的梯度：
\begin{equation}
g_{\text{accum}} = \frac{1}{K}\sum_{k=1}^K g_k \tag{120}
\end{equation}</p>
<p>有效batch size：$B_{\text{eff}} = N \times K \times B_{\text{local}}$</p>
<h3 id="9">9. 总结<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p>GAU-α通过以下创新实现了"快好省"：
1. <strong>门控机制</strong>（式26）：增强表达能力
2. <strong>熵不变性Softmax</strong>（式17-18）：改善长度外推
3. <strong>单头设计</strong>：减少参数和计算
4. <strong>优化的归一化</strong>（式70）：提高效率
5. <strong>更好的初始化</strong>（式52）：稳定训练</p>
<p>这些设计共同作用，使GAU-α成为高效的Attention替代方案。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="你的语言模型有没有无法预测的词.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#179 你的语言模型有没有“无法预测的词”？</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="在bert4keras中使用混合精度和xla加速训练.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#181 在bert4keras中使用混合精度和XLA加速训练</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#gau-attention">GAU-α：尝鲜体验快好省的下一代Attention</a><ul>
<li><a href="#gau-">GAU-α</a></li>
<li><a href="#_1">训练</a></li>
<li><a href="#_2">小结</a></li>
<li><a href="#_3">公式推导与注释</a><ul>
<li><a href="#1-gau">1. GAU架构基础</a></li>
<li><a href="#2-gau">2. GAU完整数学推导</a></li>
<li><a href="#3">3. 参数量和计算量分析</a></li>
<li><a href="#4">4. 训练技巧和稳定性</a></li>
<li><a href="#5-softmax">5. 熵不变性Softmax详细推导</a></li>
<li><a href="#6-attention">6. 与标准Attention的对比</a></li>
<li><a href="#7">7. 高级话题</a></li>
<li><a href="#8">8. 实现细节和优化</a></li>
<li><a href="#9">9. 总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>