<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GAU-α：尝鲜体验快好省的下一代Attention | ML & Math Blog Posts</title>
    <meta name="description" content="GAU-α：尝鲜体验快好省的下一代Attention
原文链接: https://spaces.ac.cn/archives/9052
发布日期: 

在《FLASH：可能是近来最有意思的高效Transformer设计》中，我们介绍了GAU（Gated Attention Unit，门控线性单元），在这里笔者愿意称之为“目前最有潜力的下一代Attention设计”，因为它真正达到了“更快（速度）、...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">GAU-α：尝鲜体验快好省的下一代Attention</h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/9052" target="_blank">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                <span class="tag"><i class="fas fa-tag"></i> attention</span>
                <span class="tag"><i class="fas fa-tag"></i> 预训练</span>
                <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                <span class="tag"><i class="fas fa-tag"></i> attention</span>
                
            </div>
            
        </header>

        <!-- Post Body -->
        <div class="post-content">
            <h1 id="gau-attention">GAU-α：尝鲜体验快好省的下一代Attention</h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9052">https://spaces.ac.cn/archives/9052</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在<a href="/archives/8934">《FLASH：可能是近来最有意思的高效Transformer设计》</a>中，我们介绍了GAU（Gated Attention Unit，门控线性单元），在这里笔者愿意称之为“目前最有潜力的下一代Attention设计”，因为它真正达到了“更快（速度）、更好（效果）、更省（显存）”的特点。</p>
<p>然而，有些读者在自己的测试中得到了相反的结果，比如收敛更慢、效果更差等，这与笔者的测试结果大相径庭。本文就来分享一下笔者自己的训练经验，并且放出一个尝鲜版“GAU-α”供大家测试。</p>
<blockquote>
<p><strong>开源地址：<a href="https://github.com/ZhuiyiTechnology/GAU-alpha">https://github.com/ZhuiyiTechnology/GAU-alpha</a></strong></p>
</blockquote>
<h2 id="gau-">GAU-α</h2>
<p>首先介绍一下开源出来的“GAU-α”在CLUE任务上的成绩单：<br />
$$\small{\begin{array}{c|ccccccccccc}<br />
\hline<br />
&amp; \text{iflytek} &amp; \text{tnews} &amp; \text{afqmc} &amp; \text{cmnli} &amp; \text{ocnli} &amp; \text{wsc} &amp; \text{csl} &amp; \text{cmrc2018} &amp; \text{c3} &amp; \text{chid} &amp; \text{cluener}\\<br />
\hline<br />
\text{BERT} &amp; 60.06 &amp; 56.80 &amp; 72.41 &amp; 79.56 &amp; 73.93 &amp; 78.62 &amp; 83.93 &amp; 56.17 &amp; 60.54 &amp; 85.69 &amp; 79.45 \\<br />
\text{RoBERTa} &amp; 60.64 &amp; \textbf{58.06} &amp; 74.05 &amp; 81.24 &amp; 76.00 &amp; \textbf{87.50} &amp; 84.50 &amp; 56.54 &amp; 67.66 &amp; 86.71 &amp; 79.47\\<br />
\text{RoFormer} &amp; 60.91 &amp; 57.54 &amp; 73.52 &amp; 80.92 &amp; \textbf{76.07} &amp; 86.84 &amp; 84.63 &amp; 56.26 &amp; 67.24 &amp; 86.57 &amp; 79.72\\<br />
\text{RoFormerV2}^* &amp; 60.87 &amp; 56.54 &amp; 72.75 &amp; 80.34 &amp; 75.36 &amp; 80.92 &amp; 84.67 &amp; 57.91 &amp; 64.62 &amp; 85.09 &amp; \textbf{81.08}\\<br />
\hline<br />
\text{GAU-}\alpha &amp; \textbf{61.41} &amp; 57.76 &amp; \textbf{74.17} &amp; \textbf{81.82} &amp; 75.86 &amp; 79.93 &amp; \textbf{85.67} &amp; \textbf{58.09} &amp; \textbf{68.24} &amp; \textbf{87.91} &amp; 80.01\\<br />
\hline<br />
\end{array}}$$</p>
<p>所有的模型都是Base版，上表显示的是CLUE任务上验证集上的结果，大家的运行方式和比较都是公平的，作为一个相对比较来说是合理的。另外，这里的RoFormerV2*并非<a href="/archives/8998">《RoFormerV2：自然语言理解的极限探索》</a>中的多任务版本，而是仅仅进行了MLM预训练的版本（该版本没开源），这样对比是因为GAU-α也仅仅进行了MLM预训练。</p>
<p>从表中可以看出，除了WSC这个数据量极少的“异类”外，GAU-α在多数任务上都有优势，并且除了WSC外的平均成绩是最好的。其中，RoFormerV2<em>与GAU-α的比较是最为公平的，因为它们的训练脚本、训练数据、整体结构都是一样的，唯一不同就是GAU-α是将RoFormerV2</em>中的Attention+FFN组合换成了两层GAU，两者对比充分显示出了GAU设计“更好”的特点。</p>
<p>此外，我们在<a href="/archives/8998">《RoFormerV2：自然语言理解的极限探索》</a>介绍过RoFormerV2对结构进行了简化，从而获得更快的速度，具有同样整体结构的GAU-α也是如此，所以GAU-α的速度是比表中的BERT、RoBERTa、RoFormer都要快的，但平均效果却更胜一筹。更进一步的测试显示，当序列长度超过512时，GAU-α的速度开始超过同样精简过的RoFormerV2，并且显存占用更低，越长则对GAU-α更有利。</p>
<h2 id="_1">训练</h2>
<p>现在介绍一下模型的训练细节，完整的代码已经开源到Github中，如有疑惑可以对照着代码来读。</p>
<p><strong>模型架构</strong> ： GAU-α就是将RoFormerV2的Attention+FFN换成了两层GAU，在<a href="/archives/8934">之前的文章</a>中我们比较过两层GAU的计算量和参数量大致相当于Attention+FFN组合，所以这样的替换是合理的；RoFormerV2的特点是保留了Post Norm结构，去掉了所有的Bias项，并且Layer Norm换成了RMS Norm的最简单变体，在GAU-α中也是如此。</p>
<p><strong>归一化</strong> ： 在<a href="/archives/9019">《听说Attention与Softmax更配哦～》</a>中我们讨论过Attention的归一化问题，GAU-α的Attention归一化选取了其中笔者自行提出的具有较好外推能力的<a href="/archives/8823">熵不变性Softmax</a>（在bert4keras中暂称为softmax_plus）。</p>
<p><strong>训练方式</strong> ： 在初始化方面笔者按照<a href="/archives/8978">《训练1000层的Transformer究竟有什么困难？》</a>进行了调整，因此无须Wamrup就可以直接训练，优化器用的是LAMB，学习率分段线性衰减；预训练任务用的是全词MLM，分词工具用百度的LAC，这些跟RoFormerV2都是对齐的。</p>
<p>好像值得一提的也就这么多了，确实没进行多大的改变。除了在归一化方式上花了点时间进行测试，其他方面也没多费时间，直接训练就得到了不错的效果。</p>
<h2 id="_2">小结</h2>
<p>GAU是笔者认为的“目前最有潜力的下一代Attention设计”，本文分享了GAU的一些训练经验，并开源了一个尝鲜版“GAU-α”。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9052">https://spaces.ac.cn/archives/9052</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 22, 2022). 《GAU-α：尝鲜体验快好省的下一代Attention 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9052">https://spaces.ac.cn/archives/9052</a></p>
<p>@online{kexuefm-9052,<br />
title={GAU-α：尝鲜体验快好省的下一代Attention},<br />
author={苏剑林},<br />
year={2022},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/9052}},<br />
} </p>
<hr />
<h2 id="_3">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>

        <!-- Back to Home -->
        <div class="text-center mt-5 mb-4">
            <a href="../index.html" class="btn btn-outline-primary">
                <i class="fas fa-arrow-left"></i> 返回首页
            </a>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>
</body>
</html>
