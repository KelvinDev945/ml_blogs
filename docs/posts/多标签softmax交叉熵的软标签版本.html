<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>多标签“Softmax+交叉熵”的软标签版本 | ML & Math Blog Posts</title>
    <meta name="description" content="多标签“Softmax+交叉熵”的软标签版本
原文链接: https://spaces.ac.cn/archives/9064
发布日期: 

（注：本文的相关内容已整理成论文《ZLPR: A Novel Loss for Multi-label Classification》，如需引用可以直接引用英文论文，谢谢。）
在《将“Softmax+交叉熵”推广到多标签分类问题》中，我们提出了一个用于多标...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">多标签“Softmax+交叉熵”的软标签版本</h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/9064" target="_blank">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                <span class="tag"><i class="fas fa-tag"></i> 损失函数</span>
                <span class="tag"><i class="fas fa-tag"></i> 光滑</span>
                <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                <span class="tag"><i class="fas fa-tag"></i> attention</span>
                
            </div>
            
        </header>

        <!-- Post Body -->
        <div class="post-content">
            <h1 id="softmax">多标签“Softmax+交叉熵”的软标签版本</h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9064">https://spaces.ac.cn/archives/9064</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p><strong>（注：本文的相关内容已整理成论文<a href="https://papers.cool/arxiv/2208.02955">《ZLPR: A Novel Loss for Multi-label Classification》</a>，如需引用可以直接引用英文论文，谢谢。）</strong></p>
<p>在<a href="/archives/7359">《将“Softmax+交叉熵”推广到多标签分类问题》</a>中，我们提出了一个用于多标签分类的损失函数：<br />
\begin{equation}\log \left(1 + \sum\limits_{i\in\Omega_{neg}} e^{s_i}\right) + \log \left(1 + \sum\limits_{j\in\Omega_{pos}} e^{-s_j}\right)\label{eq:original}\end{equation}<br />
这个损失函数有着单标签分类中“Softmax+交叉熵”的优点，即便在正负类不平衡的依然能够有效工作。但从这个损失函数的形式我们可以看到，它只适用于“硬标签”，这就意味着label smoothing、<a href="/archives/5693">mixup</a>等技巧就没法用了。本文则尝试解决这个问题，提出上述损失函数的一个软标签版本。</p>
<h2 id="_1">巧妙联系</h2>
<p>多标签分类的经典方案就是转化为多个二分类问题，即每个类别用sigmoid函数$\sigma(x)=1/(1+e^{-x})$激活，然后各自用二分类交叉熵损失。当正负类别极其不平衡时，这种做法的表现通常会比较糟糕，而相比之下损失$\eqref{eq:original}$通常是一个更优的选择。</p>
<p>在之前文章的评论区中，读者 <a href="/archives/7359/comment-page-2#comment-14196">@wu.yan</a> 揭示了多个“sigmoid+二分类交叉熵”与式$\eqref{eq:original}$的一个巧妙的联系：多个“sigmoid+二分类交叉熵”可以适当地改写成<br />
\begin{equation}\begin{aligned}<br />
&amp;\,-\sum_{j\in\Omega_{pos}}\log\sigma(s_j)-\sum_{i\in\Omega_{neg}}\log(1-\sigma(s_i))\\<br />
=&amp;\,\log\prod_{j\in\Omega_{pos}}(1+e^{-s_j})+\log\prod_{i\in\Omega_{neg}}(1+e^{s_i})\\<br />
=&amp;\,\log\left(1+\sum_{j\in\Omega_{pos}}e^{-s_j}+\cdots\right)+\log\left(1+\sum_{i\in\Omega_{neg}}e^{s_i}+\cdots\right)<br />
\end{aligned}\label{eq:link}\end{equation}<br />
对比式$\eqref{eq:original}$，我们可以发现式$\eqref{eq:original}$正好是上述多个“sigmoid+二分类交叉熵”的损失去掉了$\cdots$所表示的高阶项！在正负类别不平衡时，这些高阶项占据了过高的权重，加剧了不平衡问题，从而效果不佳；相反，去掉这些高阶项后，并没有改变损失函数的作用（希望正类得分大于0、负类得分小于0），同时因为括号内的求和数跟类别数是线性关系，因此正负类各自的损失差距不会太大。</p>
<h2 id="_2">形式猜测</h2>
<p>这个巧妙联系告诉我们，要寻找式$\eqref{eq:original}$的软标签版本，可以尝试从多个“sigmoid+二分类交叉熵”的软标签版本出发，然后尝试去掉高阶项。所谓软标签，指的是标签不再是0或1，而是0～1之间的任意实数都有可能，表示属于该类的可能性。而对于二分类交叉熵，它的软标签版本很简单：<br />
\begin{equation}-t\log\sigma(s)-(1-t)\log(1-\sigma(s))\end{equation}<br />
这里$t$就是软标签，而$s$就是对应的打分。模仿过程$\eqref{eq:link}$，我们可以得到<br />
\begin{equation}\begin{aligned}<br />
&amp;\,-\sum_i t_i\log\sigma(s_i)-\sum_i (1-t_i)\log(1-\sigma(s_i))\\<br />
=&amp;\,\log\prod_i(1+e^{-s_i})^{t_i}+\log\prod_i (1+e^{s_i})^{1-t_i}\\<br />
=&amp;\,\log\prod_i(1+t_i e^{-s_i} + \cdots)+\log\prod_i (1+(1-t_i)e^{s_i}+\cdots)\\<br />
=&amp;\,\log\left(1+\sum_i t_i e^{-s_i}+\cdots\right)+\log\left(1+\sum_i(1-t_i)e^{s_i}+\cdots\right)<br />
\end{aligned}\end{equation}<br />
如果去掉高阶项，那么就得到<br />
\begin{equation}\log\left(1+\sum_i t_i e^{-s_i}\right)+\log\left(1+\sum_i(1-t_i)e^{s_i}\right)\label{eq:soft}\end{equation}<br />
它就是式$\eqref{eq:original}$的软标签版本的候选形式，可以发现当$t_i\in\{0,1\}$时，正好是退化为式$\eqref{eq:original}$的。</p>
<h2 id="_3">证明结果</h2>
<p>就目前来说，式$\eqref{eq:soft}$顶多是一个“候选”形式，要将它“转正”，我们需要证明在$t_i$为0～1浮点数时，式$\eqref{eq:soft}$能学出有意义的结果。所谓有意义，指的是理论上能够通过$s_i$来重构$t_i$的信息（$s_i$是模型预测结果，$t_i$是给定标签，所以$s_i$能重构$t_i$是机器学习的目标）。</p>
<p>为此，我们记式$\eqref{eq:soft}$为$l$，并求$s_i$的偏导数：<br />
\begin{equation}\frac{\partial l}{\partial s_i} = \frac{-t_i e^{-s_i}}{1+\sum\limits_i t_i e^{-s_i}}+\frac{(1-t_i)e^{s_i}}{1+\sum\limits_i(1-t_i)e^{s_i}}\end{equation}<br />
我们知道$l$的最小值出现在所有$\frac{\partial l}{\partial s_i}$都等于0时，直接去解方程组$\frac{\partial l}{\partial s_i}=0$并不容易，但笔者留意到一个神奇的“巧合”：当$t_i e^{-s_i}=(1-t_i)e^{s_i}$时，每个$\frac{\partial l}{\partial s_i}$自动地等于0！所以$t_i e^{-s_i}=(1-t_i)e^{s_i}$应该就是$l$的最优解了，解得<br />
\begin{equation}t_i = \frac{1}{1+e^{-2s_i}}=\sigma(2s_i)\end{equation}<br />
这是一个很漂亮的结果，它告诉我们几个信息：</p>
<blockquote>
<p>1、式$\eqref{eq:soft}$确实是式$\eqref{eq:original}$合理的软标签推广，它能通过$s_i$完全重建$t_i$的信息，其形式也刚好与sigmoid相关；</p>
<p>2、如果我们要将结果输出为0～1的概率值，那么正确的做法应该是$\sigma(2s_i)$而不是直觉中的$\sigma(s_i)$；</p>
<p>3、既然最后的概率公式也具有sigmoid的形式，那么反过来想，也可以理解为我们依旧还是在学习多个sigmoid激活的二分类问题，只不过损失函数换成了式$\eqref{eq:soft}$。</p>
</blockquote>
<h2 id="_4">实现技巧</h2>
<p>式$\eqref{eq:soft}$的实现可以参考bert4keras的代码<a href="https://github.com/bojone/bert4keras/blob/5f5d493fe7be9ff2bd0e303e78ed945d386ed8fd/bert4keras/backend.py#L331">multilabel_categorical_crossentropy</a>，其中有个小细节值得跟大家一起交流一下。</p>
<p>首先，我们将式$\eqref{eq:soft}$可以等价地改写成<br />
\begin{equation}\log\left(1+\sum_i e^{-s_i + \log t_i}\right)+\log\left(1+\sum_i e^{s_i + \log (1-t_i)}\right)\label{eq:soft-log}\end{equation}<br />
所以看上去，只需要将$\log t_i$加到$-s_i$、将$\log(1-t_i)$加到$s_i$上，补零后做常规的$\text{logsumexp}$即可。但实际上，$t_i$是有可能取到$0$或$1$的，对应的$\log t_i$或$\log(1-t_i)$就是负无穷，而框架无法直接处理负无穷，因此通常在$\log$之前需要clip一下，即选定$\epsilon &gt; 0$后定义<br />
\begin{equation}\text{clip}(t)=\left\{\begin{aligned}&amp;\epsilon, &amp;t &lt; \epsilon \\<br />
&amp;t, &amp;\epsilon\leq t\leq 1-\epsilon\\<br />
&amp;1-\epsilon, &amp;t &gt; 1-\epsilon\end{aligned}\right.\end{equation}</p>
<p>但这样一clip，问题就来了。由于$\epsilon$不是真的无穷小，比如$\epsilon=10^{-7}$，那么$\log\epsilon$大约是$-16$左右；而像GlobalPointer这样的场景中，我们会提前把不合理的$s_i$给mask掉，方式是将对应的$s_i$置为一个绝对值很大的负数，比如$-10^7$；然而我们再看式$\eqref{eq:soft-log}$，第一项的求和对象是$e^{-s_i + \log t_i}$，所以$-10^7$就会变成$10^7$，如果$t_i$没有clip，那么理论上$\log t_i$是$\log 0 = -\infty$，可以把$-s_i + \log t_i$重新变回负无穷，但刚才我们已经看到进行了clip之后的$\log t_i$顶多就是$-16$，远远比不上$-s_i$的$10^7$，所以$-s_i + \log t_i$依然是一个大正数。</p>
<p>为了解决这个问题，我们不止要对$t_i$进行clip，我们还要找出原本小于$\epsilon$的$t_i$，手动将对应的$-s_i$置为绝对值很大的负数，同样还要找出大于$1-\epsilon$的$t_i$，将对应的$s_i$置为绝对值很大的负数，这样做就是将小于$\epsilon$的按照绝对等于0额外处理，将大于$1-\epsilon$的按照绝对等于1处理。</p>
<h2 id="_5">文章小结</h2>
<p>本文主要将笔者之前提出的多标签“Softmax+交叉熵”推广到软标签场景，有了对应的软标签版本后，我们就可以将它与label smoothing、<a href="/archives/5693">mixup</a>等技巧结合起来了，像GlobalPointer等又可以多一个炼丹方向。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9064">https://spaces.ac.cn/archives/9064</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (May. 07, 2022). 《多标签“Softmax+交叉熵”的软标签版本 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9064">https://spaces.ac.cn/archives/9064</a></p>
<p>@online{kexuefm-9064,<br />
title={多标签“Softmax+交叉熵”的软标签版本},<br />
author={苏剑林},<br />
year={2022},<br />
month={May},<br />
url={\url{https://spaces.ac.cn/archives/9064}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>

        <!-- Back to Home -->
        <div class="text-center mt-5 mb-4">
            <a href="../index.html" class="btn btn-outline-primary">
                <i class="fas fa-arrow-left"></i> 返回首页
            </a>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>
</body>
</html>
