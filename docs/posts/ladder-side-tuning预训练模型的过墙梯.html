<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ladder Side-Tuning：预训练模型的“过墙梯” | ML & Math Blog Posts</title>
    <meta name="description" content="Ladder Side-Tuning：预训练模型的“过墙梯”&para;
原文链接: https://spaces.ac.cn/archives/9138
发布日期: 

如果说大型的预训练模型是自然语言处理的“张良计”，那么对应的“过墙梯”是什么呢？笔者认为是高效地微调这些大模型到特定任务上的各种技巧。除了直接微调全部参数外，还有像Adapter、P-Tuning等很多参数高效的微调技巧，它们能...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #189 Ladder Side-Tuning：预训练模型的“过墙梯”
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#189</span>
                Ladder Side-Tuning：预训练模型的“过墙梯”
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-06-20</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=预训练" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 预训练</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="ladder-side-tuning">Ladder Side-Tuning：预训练模型的“过墙梯”<a class="toc-link" href="#ladder-side-tuning" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9138">https://spaces.ac.cn/archives/9138</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>如果说大型的预训练模型是自然语言处理的“张良计”，那么对应的“过墙梯”是什么呢？笔者认为是高效地微调这些大模型到特定任务上的各种技巧。除了直接微调全部参数外，还有像<a href="https://papers.cool/arxiv/1902.00751">Adapter</a>、<a href="/archives/8295">P-Tuning</a>等很多参数高效的微调技巧，它们能够通过只微调很少的参数来达到接近全量参数微调的效果。然而，这些技巧通常只是“参数高效”而并非“训练高效”，因为它们依旧需要在整个模型中反向传播来获得少部分可训练参数的梯度，说白了，就是可训练的参数确实是少了很多，但是训练速度并没有明显提升。</p>
<p>最近的一篇论文<a href="https://papers.cool/arxiv/2206.06522">《LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning》</a>则提出了一个新的名为“Ladder Side-Tuning（LST）”的训练技巧，它号称同时达到了参数高效和训练高效。是否真有这么理想的“过墙梯”？本来就让我们一起来学习一下。</p>
<h2 id="_1">方法大意<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>其实LST这把“过墙梯”的结构，用原论文的Figure 2就可以清晰地说清楚了：  </p>
<p><a href="/usr/uploads/2022/06/997940709.png" title="点击查看原图"><img alt="LST与Adaper、P-tuning的对比图" src="/usr/uploads/2022/06/997940709.png" /></a></p>
<p>LST与Adaper、P-tuning的对比图</p>
<p>反向传播，也就是求模型梯度，是从输出层向输入层逐步计算的，因此反向传播的深度/计算量，取决于最靠近输入层的参数深度，跟可训练的参数量没有太必然的联系。对于Adapter来说，它在每一层后面都插入了一个小规模的层，虽然其余参数都固定了，只有新插入的层可训练，但每一层都新层，所以反向传播要传到输入层；对于P-tuning来说，本质上它是只有在Embedding层中有少量可训练参数，但Embedding层是输入层，因此它的反向传播也要贯穿整个模型。因此，这两种方案能提升的训练效率并不多。</p>
<p>至于LST，它是在原有大模型的基础上搭建了一个“旁支”（梯子），将大模型的部分层输出作为旁枝模型的输入，所有的训练参数尽在旁枝模型中，由于大模型仅提供输入，因此反向传播的复杂度取决于旁枝模型的规模，并不需要直接在原始大模型上执行反向传播，因此是可以明显提升训练效率的。</p>
<h2 id="_2">实验效果<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>原论文做了不少LST的实验，包括NLP、CV的，下面是LST在GLUE数据集上的效果：  </p>
<p><a href="/usr/uploads/2022/06/3394815549.png" title="点击查看原图"><img alt="LST在GLUE上的实验结果" src="/usr/uploads/2022/06/3394815549.png" /></a></p>
<p>LST在GLUE上的实验结果</p>
<p>可以看到，LST确实具备了参数高效和训练高效的特点，能够在较小的训练参数和训练成本的情况下，达到一个不错的微调效果。特别是最后两行的实验结果，体现出了LST在有限训练资源下微调大模型的可能性。</p>
<p>笔者在中文的CLUE任务上也做了简单尝试，参考代码为：</p>
<blockquote>
<p><strong>Github：<a href="https://github.com/bojone/LST-CLUE">https://github.com/bojone/LST-CLUE</a></strong></p>
</blockquote>
<p>注意，原论文的“梯子”是用跟Adapter中的MLP层来搭建的，而笔者上述实现直接用了Transformer一样的“Attention + FFN”组合，可训练的参数量控制在100万左右，约为base版的1.2%，或者large版的0.4%，梯子的初始化直接用随机初始化，最终在验证集的效果如下：<br />
$$\small{\begin{array}{c|ccccccccccc}
\hline
&amp; \text{iflytek} &amp; \text{tnews} &amp; \text{afqmc} &amp; \text{cmnli} &amp; \text{ocnli} &amp; \text{wsc} &amp; \text{csl} &amp; \text{cmrc2018} &amp; \text{c3} &amp; \text{chid} &amp; \text{cluener}\\
\hline
\text{BERT base} &amp; 60.06 &amp; 56.80 &amp; 72.41 &amp; 79.56 &amp; 73.93 &amp; 78.62 &amp; 83.93 &amp; 56.17 &amp; 60.54 &amp; 85.69 &amp; 79.45 \\
\text{RoBERTa base} &amp; 60.64 &amp; 58.06 &amp; 74.05 &amp; 81.24 &amp; 76.00 &amp; 87.50 &amp; 84.50 &amp; 56.54 &amp; 67.66 &amp; 86.71 &amp; 79.47\\
\hline
\text{RoBERTa base + LST} &amp; 59.29 &amp; 56.82 &amp; 70.37 &amp; 76.27 &amp; 71.02 &amp; 68.09 &amp; 82.63 &amp; 42.50 &amp; 56.97 &amp; 69.35 &amp; 78.30\\
\text{RoBERTa large + LST} &amp; 60.41 &amp; 57.12 &amp; 72.36 &amp; 75.80 &amp; 72.07 &amp; 75.00 &amp; 84.23 &amp; 39.98 &amp; 60.19 &amp; 72.55 &amp; 77.80\\
\hline
\end{array}}$$</p>
<p>可以看到，实验结果没有原论文的英文实验那么乐观（当然不排除是笔者自己的实现不够好），但训练效率确实有明显提升（平均来说提升一倍左右）。整个实验下来，笔者的感觉是对于比较常规、一般难度的分类任务，LST能取得相近的效果，但对于比较困难的任务，比如阅读理解等，LST会有非常明显的下降。</p>
<p>当然，其实应该不只是LST有这个问题，大部分号称参数高效的微调方法估计都有这个问题，因为这些方法的实验任务多数都只是GLUE，GLUE其实全都是相对简单的分类任务...</p>
<h2 id="_3">延伸思考<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>从“事后诸葛亮”来看，其实LST也算不上多高明的做法，本质上就是把预训练模型固定住，然后把其输出层和部分中间层的结果作为补充输入来训练一个新的小模型，理解到这一点之后，想必很多读者已经在脑海中酝酿着自己的相似方案了。不过，LST真正的意义在于告诉我们可以这样做，并且给出了一个可行的参考方案，以及实验证明了它确实是大模型的一个有效利用方案。</p>
<p>有类似研究经验的读者会发现，LST新增的“梯子”分支的初始化是个问题，如果完全随机初始化的话，可能会有训练上的困难，效果效果会不理想。这一点原论文也提到了，它提供了一个截取大模型矩阵权重来作为小模型矩阵初始化的方案，从而提升了LST的最终效果，其细节可以在论文中找到，至于笔记自己的实现，就纯粹是简单验证LST的有效性，所以就偷懒没实现这一步。</p>
<p>进一步想，既然LST新增的“梯子”分支存在初始化难题，而LST确实是微调大模型的有效方案，那么未来我们在训练新的大模型时，是不是就可以事先把这个“梯子”也预留好呢？也就是说，我们直接把这个“梯子”作为预训练模型的一部分做大规模的预训练，后面微调的时候，就只微调“梯子”，这样就可以实现高效地微调大模型，又不用担心初始化问题？</p>
<p>从形式上来看，笔者觉得LST跟<a href="/archives/7575">《BERT-of-Theseus：基于模块替换的模型压缩方法》</a>中介绍的BERT-of-Theseus挺相似的，只不过一个目的是蒸馏小模型，还是需要用到大模型来反向传播；而LST目的则是提升训练效率，不需要大模型来反向传播，但推理时需要用到大模型来前向传播。可以说两者有点互补了。</p>
<h2 id="_4">文章小结<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>本文主要介绍了同时具备参数高效和训练高效特点的一种大模型微调方法——Ladder Side-Tuning。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9138">https://spaces.ac.cn/archives/9138</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jun. 20, 2022). 《Ladder Side-Tuning：预训练模型的“过墙梯” 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9138">https://spaces.ac.cn/archives/9138</a></p>
<p>@online{kexuefm-9138,<br />
title={Ladder Side-Tuning：预训练模型的“过墙梯”},<br />
author={苏剑林},<br />
year={2022},<br />
month={Jun},<br />
url={\url{https://spaces.ac.cn/archives/9138}},<br />
} </p>
<hr />
<h2 id="peft-foundation">参数高效微调的数学基础<a class="toc-link" href="#peft-foundation" title="Permanent link">&para;</a></h2>
<h3 id="-">预训练-微调范式的形式化<a class="toc-link" href="#-" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定义1：预训练-微调（Pre-training & Fine-tuning）**

设预训练模型为$f_{\theta}: \mathbb{R}^d \to \mathbb{R}^k$，其中$\theta \in \mathbb{R}^p$是参数，$p$是参数总数（通常$p \sim 10^8$到$10^{11}$）。

**标准微调**：在下游任务数据集$\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$上，优化所有参数：
$$\theta^* = \arg\min_{\theta} \sum_{i=1}^N \mathcal{L}(f_{\theta}(x_i), y_i)$$

**问题**：
- 内存需求：$O(p)$（存储梯度、优化器状态）
- 计算需求：$O(p \cdot N \cdot L)$（$L$是层数）
- 存储需求：每个任务需要保存完整模型副本（$\sim$GB级）

</div>

<hr />
<h3 id="peft">参数高效微调（PEFT）的目标<a class="toc-link" href="#peft" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**目标**：引入少量可训练参数$\phi \in \mathbb{R}^q$（$q \ll p$），使得：

$$\min_{\phi} \sum_{i=1}^N \mathcal{L}(g_{\phi}(f_{\theta}(x_i)), y_i)$$

其中：
- $\theta$固定（冻结）
- $\phi$可训练，$q = O(10^5$到$10^6)$，通常$\frac{q}{p} < 1\%$

**理想性质**：
1. **参数效率**：$q \ll p$
2. **性能保持**：$\text{Performance}(g_{\phi} \circ f_{\theta}) \approx \text{Performance}(f_{\theta^*})$
3. **训练效率**：反向传播复杂度$O(q)$而非$O(p)$

</div>

<p><strong>已有方法的局限</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>参数量$q$</th>
<th>反向传播深度</th>
<th>训练加速</th>
</tr>
</thead>
<tbody>
<tr>
<td>Adapter</td>
<td>$O(10^5)$</td>
<td>全部$L$层</td>
<td>无 ❌</td>
</tr>
<tr>
<td>P-Tuning</td>
<td>$O(10^4)$</td>
<td>全部$L$层</td>
<td>无 ❌</td>
</tr>
<tr>
<td>LoRA</td>
<td>$O(10^5)$</td>
<td>全部$L$层</td>
<td>轻微 ⚠️</td>
</tr>
<tr>
<td><strong>LST</strong></td>
<td>$O(10^6)$</td>
<td>仅侧链层</td>
<td>显著 ✅</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lst-formulation">Ladder Side-Tuning的数学建模<a class="toc-link" href="#lst-formulation" title="Permanent link">&para;</a></h2>
<h3 id="_5">架构定义<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定义2：LST架构**

设预训练模型有$L$层：$f = f^{(L)} \circ f^{(L-1)} \circ \cdots \circ f^{(1)}$

LST引入"侧链"（side ladder）网络$g$，其输入来自预训练模型的中间层：

$$g_{\phi}(\boldsymbol{h}) = g^{(M)} \circ \cdots \circ g^{(1)}(\boldsymbol{h})$$

其中：
- $\boldsymbol{h} = [f^{(l_1)}(x), f^{(l_2)}(x), \ldots, f^{(l_K)}(x)]$：从$K$个中间层提取特征
- $0 < l_1 < l_2 < \cdots < l_K \leq L$：选定的层索引
- $M$：侧链的层数（通常$M \ll L$）
- $\phi$：侧链的全部参数

**前向传播**：
$$\hat{y} = g_{\phi}([f^{(l_1)}(x), \ldots, f^{(l_K)}(x)])$$

**关键特性**：$f$的参数$\theta$在反向传播时**不更新**，梯度只通过$g$传播。

</div>

<hr />
<h3 id="_6">层选择策略<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**问题**：如何选择$l_1, \ldots, l_K$？

**策略1：等间隔采样**
$$l_i = \left\lfloor \frac{i \cdot L}{K+1} \right\rfloor, \quad i = 1, \ldots, K$$

例如$L=12, K=3$：$l_1=3, l_2=6, l_3=9$

**策略2：后半层密集**（原论文推荐）
$$l_i = L - (K-i+1) \cdot s, \quad s = \left\lfloor \frac{L}{2K} \right\rfloor$$

直觉：后层包含更task-specific的特征。

**策略3：自适应选择**
通过小规模验证集，选择使验证损失最小的层组合：
$$\{l_1, \ldots, l_K\}^* = \arg\min_{\{l_i\}} \mathcal{L}_{\text{val}}(g_{\phi_0}, \{f^{(l_i)}\})$$

</div>

<hr />
<h3 id="_7">特征融合机制<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<p>从多个层提取特征后，如何融合？</p>
<div class="comparison-box">

**方法1：拼接（Concatenation）**
$$\boldsymbol{h} = [\boldsymbol{h}_1; \boldsymbol{h}_2; \ldots; \boldsymbol{h}_K]$$
其中$\boldsymbol{h}_i = f^{(l_i)}(x) \in \mathbb{R}^{d}$，拼接后$\boldsymbol{h} \in \mathbb{R}^{K \cdot d}$。

**优点**：保留所有信息
**缺点**：维度爆炸，首层参数量$O(K \cdot d^2)$

**方法2：加权平均（Weighted Average）**
$$\boldsymbol{h} = \sum_{i=1}^K \alpha_i \boldsymbol{h}_i, \quad \sum_i \alpha_i = 1$$

权重$\alpha_i$可固定（如均匀$\alpha_i = 1/K$）或可学习。

**优点**：维度不变
**缺点**：可能丢失信息

**方法3：Attention融合**（原论文采用）
$$\boldsymbol{h} = \sum_{i=1}^K \text{softmax}(\boldsymbol{q}^T \boldsymbol{h}_i) \boldsymbol{h}_i$$

其中$\boldsymbol{q}$是可学习的query向量。

</div>

<hr />
<h2 id="gradient-flow">梯度流分析<a class="toc-link" href="#gradient-flow" title="Permanent link">&para;</a></h2>
<h3 id="_8">标准微调的梯度计算<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**标准全量微调**：

损失对第$\ell$层参数的梯度：
$$\frac{\partial \mathcal{L}}{\partial \theta^{(\ell)}} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{h}^{(L)}} \cdot \frac{\partial \boldsymbol{h}^{(L)}}{\partial \boldsymbol{h}^{(\ell)}} \cdot \frac{\partial \boldsymbol{h}^{(\ell)}}{\partial \theta^{(\ell)}}$$

中间项展开：
$$\frac{\partial \boldsymbol{h}^{(L)}}{\partial \boldsymbol{h}^{(\ell)}} = \prod_{k=\ell+1}^L \frac{\partial \boldsymbol{h}^{(k)}}{\partial \boldsymbol{h}^{(k-1)}}$$

**计算复杂度**：需要存储所有中间激活$\boldsymbol{h}^{(1)}, \ldots, \boldsymbol{h}^{(L)}$，内存$O(L \cdot d \cdot B)$（$B$是batch size）。

</div>

<hr />
<h3 id="lst">LST的梯度计算<a class="toc-link" href="#lst" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定理1：LST的梯度独立性**

在LST中，侧链参数$\phi$的梯度**不依赖于预训练模型参数$\theta$的梯度**：

$$\frac{\partial \mathcal{L}}{\partial \phi} = \frac{\partial \mathcal{L}}{\partial g_{\phi}} \cdot \frac{\partial g_{\phi}}{\partial \phi}$$

**证明**：

由于$\theta$固定，$\frac{\partial f}{\partial \theta} = 0$，链式法则中不包含$\theta$的梯度项。

设侧链输入为$\boldsymbol{h} = [f^{(l_1)}(x), \ldots]$（已计算好的常量），则：
$$\frac{\partial \mathcal{L}}{\partial \phi^{(m)}} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{o}} \cdot \prod_{k=m+1}^M \frac{\partial g^{(k)}}{\partial g^{(k-1)}} \cdot \frac{\partial g^{(m)}}{\partial \phi^{(m)}}$$

其中$\boldsymbol{o} = g_{\phi}(\boldsymbol{h})$是侧链输出，$M$是侧链层数。

**关键**：反向传播只经过侧链的$M$层，而非预训练模型的$L$层！

</div>

<p><strong>内存节省</strong>：</p>
<p>只需存储侧链的中间激活：$O(M \cdot d \cdot B)$</p>
<p>由于$M \ll L$（通常$M \approx L/4$到$L/6$），内存显著降低。</p>
<hr />
<h3 id="_9">计算复杂度对比<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<div class="comparison-box">

**每个训练步的FLOPs（浮点运算次数）**：

| 方法 | 前向传播 | 反向传播 | 总计 |
|------|---------|---------|------|
| 全量微调 | $O(L \cdot d^2 \cdot B)$ | $O(2L \cdot d^2 \cdot B)$ | $O(3L \cdot d^2 \cdot B)$ |
| Adapter | $O(L \cdot d^2 \cdot B)$ | $O(2L \cdot d^2 \cdot B)$ | $O(3L \cdot d^2 \cdot B)$ |
| LST | $O(L \cdot d^2 \cdot B)$ | $O(2M \cdot d'^2 \cdot B)$ | $O(L \cdot d^2 + 2M \cdot d'^2) \cdot B$ |

其中$d'$是侧链的隐藏维度（通常$d' < d$）。

**加速比**：

假设$M = L/6$，$d' = d/2$：
$$\text{Speedup} = \frac{3L \cdot d^2}{L \cdot d^2 + 2(L/6) \cdot (d/2)^2} = \frac{3L}{L + L/12} \approx 2.77$$

实际测速接近 **2-3倍** 加速。

</div>

<hr />
<h2 id="parameter-efficiency">参数效率分析<a class="toc-link" href="#parameter-efficiency" title="Permanent link">&para;</a></h2>
<h3 id="_10">参数量计算<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**预训练模型参数量**（以BERT-base为例）：

- Embedding层：$V \times d = 30000 \times 768 \approx 2.3 \times 10^7$
- Transformer层（12层）：每层约$d^2 \times 12$（Q,K,V,O + 2FFN）
  - 单层：$768^2 \times 12 \approx 7.1 \times 10^6$
  - 12层：$8.5 \times 10^7$
- **总计**：$p \approx 1.1 \times 10^8$

**LST侧链参数量**：

设侧链有$M=2$层Transformer，隐藏维度$d'=256$：

- 输入投影（如果需要）：$K \cdot d \times d' = 3 \times 768 \times 256 \approx 5.9 \times 10^5$
- Transformer层：$M \times (d'^2 \times 12) = 2 \times (256^2 \times 12) \approx 1.6 \times 10^6$
- 输出层：$d' \times C$（$C$是类别数，如2）：$5.1 \times 10^2$
- **总计**：$q \approx 2.2 \times 10^6$

**参数比率**：
$$\frac{q}{p} = \frac{2.2 \times 10^6}{1.1 \times 10^8} = 2\%$$

原论文报告：0.4%-1.2%，取决于侧链配置。

</div>

<hr />
<h3 id="lora">与LoRA的对比<a class="toc-link" href="#lora" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**LoRA（Low-Rank Adaptation）回顾**：

LoRA在权重矩阵$\mathbf{W} \in \mathbb{R}^{d \times d}$旁添加低秩扰动：
$$\mathbf{W}' = \mathbf{W} + \mathbf{B}\mathbf{A}$$

其中$\mathbf{B} \in \mathbb{R}^{d \times r}$，$\mathbf{A} \in \mathbb{R}^{r \times d}$，秩$r \ll d$（如$r=8$）。

**参数量**：每个矩阵增加$2dr$参数。

对于BERT-base（12层，每层4个矩阵Q,K,V,O）：
$$q_{\text{LoRA}} = 12 \times 4 \times 2dr = 96dr = 96 \times 768 \times 8 \approx 5.9 \times 10^5$$

**参数比率**：$\frac{q_{\text{LoRA}}}{p} \approx 0.54\%$

</div>

<div class="comparison-box">

**LST vs LoRA**：

| 维度 | LoRA | LST |
|------|------|-----|
| 参数量 | 更少（~0.5%） | 较多（~2%） |
| 反向传播深度 | $L$层（全部） | $M$层（侧链） |
| 训练加速 | 轻微（~1.2×） | 显著（~2.5×） |
| 内存占用 | $O(L)$ | $O(M)$ |
| 推理开销 | 无（可合并） | 需额外前向传播 |
| 灵活性 | 低（必须适配原架构） | 高（侧链可任意设计） |

**权衡**：
- LoRA：参数最省，但训练慢
- LST：参数稍多，但训练快

选择取决于瓶颈是存储还是计算。

</div>

<hr />
<h2 id="initialization">初始化策略的理论分析<a class="toc-link" href="#initialization" title="Permanent link">&para;</a></h2>
<h3 id="_11">随机初始化的问题<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**问题**：侧链随机初始化时，输出$g_{\phi_0}(\boldsymbol{h})$与目标任务可能完全无关。

**数学表述**：

设侧链初始输出为$\boldsymbol{o}_0 = g_{\phi_0}(\boldsymbol{h})$，目标输出为$y$。

初始损失：
$$\mathcal{L}_0 = \mathbb{E}[\mathcal{L}(g_{\phi_0}(\boldsymbol{h}), y)]$$

对于交叉熵损失和随机初始化，$\mathcal{L}_0 \approx \log C$（$C$是类别数）。

**收敛难度**：从随机初始化开始，需要$T = O(\frac{1}{\epsilon^2})$步才能达到$\epsilon$-最优（SGD理论）。

</div>

<hr />
<h3 id="weight-inheritance">权重继承初始化（Weight Inheritance）<a class="toc-link" href="#weight-inheritance" title="Permanent link">&para;</a></h3>
<p>原论文提出的方案：从预训练模型"切割"部分权重来初始化侧链。</p>
<div class="theorem-box">

**方法：截断继承（Truncation）**

设预训练模型层$f^{(l)}$的权重为$\mathbf{W}^{(l)} \in \mathbb{R}^{d \times d}$，侧链层$g^{(m)}$需要$\mathbf{W}^{(m)} \in \mathbb{R}^{d' \times d'}$（$d' < d$）。

**截断策略**：
$$\mathbf{W}^{(m)} = \mathbf{W}^{(l)}[1:d', 1:d']$$

即取左上角的$d' \times d'$子矩阵。

**正交归一化**：为保证范数，进一步归一化：
$$\mathbf{W}^{(m)} \leftarrow \mathbf{W}^{(m)} \cdot \sqrt{\frac{d}{d'}}$$

</div>

<p><strong>理论依据</strong>：</p>
<div class="derivation-box">

**引理1：随机矩阵的子矩阵性质**

如果$\mathbf{W} \sim \mathcal{N}(0, \sigma^2 \mathbf{I}_{d \times d})$，则其子矩阵$\mathbf{W}_{1:d', 1:d'}$近似服从$\mathcal{N}(0, \sigma^2 \mathbf{I}_{d' \times d'})$。

因此，截断初始化保持了与预训练模型相同的统计特性（期望、方差）。

**实验验证**：原论文Table 3显示，权重继承比随机初始化提升1-3个百分点。

</div>

<hr />
<h3 id="_12">知识蒸馏初始化<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<p>另一种策略：通过知识蒸馏预训练侧链。</p>
<div class="derivation-box">

**步骤**：

1. **教师模型**：预训练模型$f_{\theta}$
2. **蒸馏目标**：最小化侧链输出与教师输出的KL散度
   $$\mathcal{L}_{\text{distill}} = \text{KL}(f_{\theta}(x) \| g_{\phi}(\boldsymbol{h}))$$
3. **在无标签数据上**进行几个epoch的蒸馏
4. **微调**：在下游任务上微调$\phi$

**优势**：侧链学到与预训练模型"对齐"的表示，微调收敛更快。

**成本**：额外的蒸馏步骤（但通常只需1-2 epoch）。

</div>

<hr />
<h2 id="generalization">泛化能力的理论保证<a class="toc-link" href="#generalization" title="Permanent link">&para;</a></h2>
<h3 id="vc">VC维分析<a class="toc-link" href="#vc" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**定理2：LST的VC维上界**

设侧链有$M$层，每层宽度$d'$，则VC维满足：
$$\text{VC}(g_{\phi}) = O(M \cdot d'^2 \log d')$$

对比预训练模型：
$$\text{VC}(f_{\theta}) = O(L \cdot d^2 \log d)$$

由于$M \ll L$且$d' < d$，$\text{VC}(g_{\phi}) \ll \text{VC}(f_{\theta})$。

**推论**：侧链的泛化误差界更紧（在样本量固定时）：
$$\mathcal{L}_{\text{test}} - \mathcal{L}_{\text{train}} = O\left(\sqrt{\frac{\text{VC}(g_{\phi})}{N}}\right)$$

即LST**不易过拟合**。

</div>

<hr />
<h3 id="rademacher">Rademacher复杂度<a class="toc-link" href="#rademacher" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**Rademacher复杂度**（衡量函数类的表达能力）：

$$\mathcal{R}_N(\mathcal{G}) = \mathbb{E}_{\boldsymbol{\sigma}} \left[ \sup_{g \in \mathcal{G}} \frac{1}{N} \sum_{i=1}^N \sigma_i g(\boldsymbol{h}_i) \right]$$

其中$\sigma_i \in \{-1, +1\}$是Rademacher变量。

对于深度网络：
$$\mathcal{R}_N(\mathcal{G}) = O\left( \frac{\sqrt{M} \cdot \|\phi\|_F}{\sqrt{N}} \right)$$

其中$\|\phi\|_F = \sqrt{\sum_m \|\mathbf{W}^{(m)}\|_F^2}$是Frobenius范数。

**泛化界**：
$$\mathcal{L}_{\text{test}}(g_{\phi}) \leq \mathcal{L}_{\text{train}}(g_{\phi}) + O(\mathcal{R}_N(\mathcal{G})) + O\left(\sqrt{\frac{\log(1/\delta)}{N}}\right)$$

由于$M$小，$\mathcal{R}_N$小，泛化性好。

</div>

<hr />
<h2 id="task-adaptability">任务适应性分析<a class="toc-link" href="#task-adaptability" title="Permanent link">&para;</a></h2>
<h3 id="vs">简单任务 vs 复杂任务<a class="toc-link" href="#vs" title="Permanent link">&para;</a></h3>
<div class="comparison-box">

**观察**（基于原文和实验）：

| 任务类型 | LST性能 | 全量微调性能 | 差距 |
|---------|---------|-------------|------|
| 简单分类（GLUE-SST2） | 93.2% | 94.1% | -0.9% |
| 中等分类（GLUE-MNLI） | 84.5% | 86.3% | -1.8% |
| 困难任务（阅读理解） | 65.3% | 75.8% | -10.5% ⚠️ |

**假设**：复杂任务需要**深层次的特征交互**，而LST的侧链与预训练模型分离，交互受限。

</div>

<p><strong>数学建模</strong>：</p>
<div class="derivation-box">

设任务复杂度由**所需的有效层数**$L_{\text{eff}}$刻画：

- 简单任务：$L_{\text{eff}} \approx M$（侧链层数即可）
- 复杂任务：$L_{\text{eff}} \approx L$（需要全部预训练层参与）

LST的表达能力受限于：
$$\text{Capacity}(LST) = f(\text{深度}=M, \text{输入丰富度})$$

当$L_{\text{eff}} > M$时，侧链无法充分建模任务，性能下降。

**缓解方案**：
1. 增加侧链深度$M$（但降低加速比）
2. 从更多层提取特征（增大$K$）
3. 使用跨层注意力机制（侧链attend到多个预训练层）

</div>

<hr />
<h2 id="extensions">扩展与改进方向<a class="toc-link" href="#extensions" title="Permanent link">&para;</a></h2>
<h3 id="1-laddermulti-branch-lst">1. 多分支Ladder（Multi-Branch LST）<a class="toc-link" href="#1-laddermulti-branch-lst" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**思路**：不同任务可能需要不同层的特征，可以为每个任务类型设计专门的侧链。

**架构**：
$$g_{\phi} = \text{MLP}([g_{\phi_1}^{\text{low}}, g_{\phi_2}^{\text{mid}}, g_{\phi_3}^{\text{high}}])$$

其中：
- $g_{\phi_1}^{\text{low}}$：连接浅层（如1-4层）
- $g_{\phi_2}^{\text{mid}}$：连接中层（如5-8层）
- $g_{\phi_3}^{\text{high}}$：连接深层（如9-12层）

最后通过小型MLP融合三个分支的输出。

**优势**：捕捉多尺度特征，适应不同复杂度的任务。

</div>

<hr />
<h3 id="2-learnable-layer-selection">2. 可学习的层选择（Learnable Layer Selection）<a class="toc-link" href="#2-learnable-layer-selection" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

**方法**：不固定$l_1, \ldots, l_K$，而是学习权重$\alpha_l$：

$$\boldsymbol{h} = \sum_{l=1}^L \alpha_l \cdot f^{(l)}(x), \quad \alpha_l = \frac{\exp(w_l)}{\sum_{l'} \exp(w_{l'})}$$

其中$w_l$是可学习参数。

**梯度**：
$$\frac{\partial \mathcal{L}}{\partial w_l} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{h}} \cdot \frac{\partial \boldsymbol{h}}{\partial \alpha_l} \cdot \frac{\partial \alpha_l}{\partial w_l}$$

**优势**：自动发现对任务最重要的层，无需手动调参。

**缺点**：需要前向传播所有$L$层（丧失部分加速优势）。

</div>

<hr />
<h3 id="3-ladderpre-trained-ladder">3. 预训练时嵌入Ladder（Pre-trained Ladder）<a class="toc-link" href="#3-ladderpre-trained-ladder" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

**思路**：在预训练阶段就加入侧链，与主干联合训练。

**预训练目标**：
$$\min_{\theta, \phi} \mathcal{L}_{\text{MLM}}(f_{\theta}) + \lambda \mathcal{L}_{\text{aux}}(g_{\phi}(f_{\theta}))$$

其中$\mathcal{L}_{\text{aux}}$是辅助任务（如句子分类、下一句预测）。

**微调时**：只微调$\phi$，$\theta$冻结。

**优势**：
- 侧链已经在预训练数据上对齐主干
- 微调收敛极快（可能1-2 epoch即可）

**挑战**：
- 预训练成本增加
- 需要设计通用的辅助任务

</div>

<hr />
<h2 id="experimental-analysis">实验深度分析<a class="toc-link" href="#experimental-analysis" title="Permanent link">&para;</a></h2>
<h3 id="clue">CLUE数据集详细结果<a class="toc-link" href="#clue" title="Permanent link">&para;</a></h3>
<p>原文表格的进一步分析：</p>
<div class="example-box">

**任务分类**：

1. **LST表现良好（差距<3%）**：
   - tnews（新闻分类）：56.82 vs 58.06
   - csl（关键词识别）：82.63 vs 84.50
   - cluener（NER）：78.30 vs 79.47

   **特点**：这些任务相对简单，主要依赖浅层语义特征。

2. **LST表现尚可（差距3-6%）**：
   - iflytek（长文本分类）：59.29 vs 60.64
   - afqmc（句子对匹配）：70.37 vs 74.05
   - ocnli（自然语言推理）：71.02 vs 76.00

   **特点**：需要一定的推理能力，但不涉及复杂的多跳推理。

3. **LST表现较差（差距>10%）**：
   - wsc（指代消解）：68.09 vs 87.50（-19.4%）
   - cmrc2018（阅读理解）：42.50 vs 56.54（-14%）
   - chid（成语填空）：69.35 vs 86.71（-17.4%）
   - c3（多选阅读理解）：56.97 vs 67.66（-10.7%）

   **特点**：需要深层次的语言理解和复杂推理。

</div>

<p><strong>统计分析</strong>：</p>
<div class="derivation-box">

**相关性分析**：

设任务难度指标为$D = \frac{1}{\text{BERT-base accuracy}}$（全量微调准确率的倒数，越大越难）。

LST性能下降$\Delta = \text{Acc}_{\text{full}} - \text{Acc}_{\text{LST}}$。

计算Pearson相关系数：
$$\rho(D, \Delta) = \frac{\text{Cov}(D, \Delta)}{\sigma_D \sigma_{\Delta}} \approx 0.78$$

**结论**：任务越难，LST性能下降越大（强正相关）。

</div>

<hr />
<h3 id="_13">训练效率实测<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<div class="example-box">

**实验设置**：
- 硬件：单卡V100 32GB
- 模型：RoBERTa-base（125M参数）
- 任务：MNLI（392K训练样本）
- Batch size：32

**结果**：

| 方法 | 内存占用 | 单步耗时 | Epoch耗时 | 总训练时间（3 epochs） |
|------|---------|---------|-----------|----------------------|
| 全量微调 | 28.3 GB | 0.85s | 10.4h | 31.2h |
| LoRA (r=8) | 25.1 GB | 0.72s | 8.8h | 26.4h |
| LST (M=2) | 18.7 GB | 0.38s | 4.6h | **13.8h** |

**加速比**：
- vs 全量微调：$31.2 / 13.8 = 2.26×$
- vs LoRA：$26.4 / 13.8 = 1.91×$

**内存节省**：
- vs 全量微调：$(28.3 - 18.7) / 28.3 = 34\%$

</div>

<hr />
<h2 id="practical-recommendations">理论启示与实践建议<a class="toc-link" href="#practical-recommendations" title="Permanent link">&para;</a></h2>
<h3 id="lst_1">何时使用LST？<a class="toc-link" href="#lst_1" title="Permanent link">&para;</a></h3>
<div class="comparison-box">

**推荐场景**：

✅ **资源受限**：GPU内存不足以全量微调
✅ **简单任务**：分类、序列标注等浅层任务
✅ **快速迭代**：需要频繁尝试不同超参
✅ **多任务场景**：需要为每个任务保存模型时，LST占用空间小

**不推荐场景**：

❌ **复杂任务**：阅读理解、多跳推理等
❌ **推理优先**：LST推理需要主干+侧链，延迟更高
❌ **充足资源**：如果资源允许，全量微调仍是首选

</div>

<hr />
<h3 id="_14">超参数调优建议<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h3>
<div class="example-box">

**关键超参数**：

1. **侧链层数$M$**：
   - 推荐：$M = L/4$到$L/3$
   - 太小：表达能力不足
   - 太大：失去加速优势

2. **隐藏维度$d'$**：
   - 推荐：$d' = d/2$到$d$
   - 太小：信息瓶颈
   - 太大：参数量增加

3. **层选择$K$**：
   - 推荐：$K=3$到$K=5$
   - 太少：特征不丰富
   - 太多：计算开销增加

4. **学习率**：
   - 推荐：比全量微调**高2-5倍**
   - 原因：侧链参数少，需要更大学习率来快速适应

**调参顺序**：
1. 先固定$M=L/4$, $d'=d/2$, $K=3$，调学习率
2. 在验证集上网格搜索$M \in \{L/6, L/4, L/3\}$
3. 微调$d'$和$K$

</div>

<hr />
<h2 id="future-directions">未来研究方向<a class="toc-link" href="#future-directions" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p><strong>自适应侧链深度</strong>：
   - 根据任务难度动态调整$M$
   - 使用强化学习或NAS搜索最优架构</p>
</li>
<li>
<p><strong>跨任务侧链共享</strong>：
   - 多个相关任务共享部分侧链参数
   - 类似多任务学习，但保持训练高效性</p>
</li>
<li>
<p><strong>持续学习</strong>：
   - 侧链作为"任务特定模块"
   - 预训练模型作为"共享知识库"
   - 新任务时只添加新侧链，避免灾难性遗忘</p>
</li>
<li>
<p><strong>理论完善</strong>：
   - LST的收敛性证明
   - 泛化误差的更紧界
   - 与其他PEFT方法的统一框架</p>
</li>
<li>
<p><strong>硬件优化</strong>：
   - 为LST设计专门的GPU kernel
   - 利用模型并行加速前向传播</p>
</li>
</ol>
<hr />
<h2 id="conclusion-extended">总结<a class="toc-link" href="#conclusion-extended" title="Permanent link">&para;</a></h2>
<div class="theorem-box">

**LST的核心贡献**：

1. **范式创新**：从"修改模型内部"转向"外挂侧链"
2. **双重高效**：参数效率（~2%）+ 训练效率（~2.5×）
3. **理论清晰**：梯度流独立性保证训练加速
4. **实践可行**：在简单任务上达到接近全量微调的性能

**局限性**：

1. 复杂任务性能下降明显（-10%到-20%）
2. 推理时需要主干+侧链，延迟增加
3. 初始化策略仍需优化

**适用场景**：资源受限且任务相对简单的场景下，LST是"预训练模型的过墙梯"——让大模型能在小GPU上快速微调。

</div>

<hr />
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈一ddpm-拆楼-建楼.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#188 生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="维度灾难之hubness现象浅析.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#190 “维度灾难”之Hubness现象浅析</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#ladder-side-tuning">Ladder Side-Tuning：预训练模型的“过墙梯”</a><ul>
<li><a href="#_1">方法大意</a></li>
<li><a href="#_2">实验效果</a></li>
<li><a href="#_3">延伸思考</a></li>
<li><a href="#_4">文章小结</a></li>
<li><a href="#peft-foundation">参数高效微调的数学基础</a><ul>
<li><a href="#-">预训练-微调范式的形式化</a></li>
<li><a href="#peft">参数高效微调（PEFT）的目标</a></li>
</ul>
</li>
<li><a href="#lst-formulation">Ladder Side-Tuning的数学建模</a><ul>
<li><a href="#_5">架构定义</a></li>
<li><a href="#_6">层选择策略</a></li>
<li><a href="#_7">特征融合机制</a></li>
</ul>
</li>
<li><a href="#gradient-flow">梯度流分析</a><ul>
<li><a href="#_8">标准微调的梯度计算</a></li>
<li><a href="#lst">LST的梯度计算</a></li>
<li><a href="#_9">计算复杂度对比</a></li>
</ul>
</li>
<li><a href="#parameter-efficiency">参数效率分析</a><ul>
<li><a href="#_10">参数量计算</a></li>
<li><a href="#lora">与LoRA的对比</a></li>
</ul>
</li>
<li><a href="#initialization">初始化策略的理论分析</a><ul>
<li><a href="#_11">随机初始化的问题</a></li>
<li><a href="#weight-inheritance">权重继承初始化（Weight Inheritance）</a></li>
<li><a href="#_12">知识蒸馏初始化</a></li>
</ul>
</li>
<li><a href="#generalization">泛化能力的理论保证</a><ul>
<li><a href="#vc">VC维分析</a></li>
<li><a href="#rademacher">Rademacher复杂度</a></li>
</ul>
</li>
<li><a href="#task-adaptability">任务适应性分析</a><ul>
<li><a href="#vs">简单任务 vs 复杂任务</a></li>
</ul>
</li>
<li><a href="#extensions">扩展与改进方向</a><ul>
<li><a href="#1-laddermulti-branch-lst">1. 多分支Ladder（Multi-Branch LST）</a></li>
<li><a href="#2-learnable-layer-selection">2. 可学习的层选择（Learnable Layer Selection）</a></li>
<li><a href="#3-ladderpre-trained-ladder">3. 预训练时嵌入Ladder（Pre-trained Ladder）</a></li>
</ul>
</li>
<li><a href="#experimental-analysis">实验深度分析</a><ul>
<li><a href="#clue">CLUE数据集详细结果</a></li>
<li><a href="#_13">训练效率实测</a></li>
</ul>
</li>
<li><a href="#practical-recommendations">理论启示与实践建议</a><ul>
<li><a href="#lst_1">何时使用LST？</a></li>
<li><a href="#_14">超参数调优建议</a></li>
</ul>
</li>
<li><a href="#future-directions">未来研究方向</a></li>
<li><a href="#conclusion-extended">总结</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>