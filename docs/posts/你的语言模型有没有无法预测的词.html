<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>你的语言模型有没有“无法预测的词”？ | ML & Math Blog Posts</title>
    <meta name="description" content="你的语言模型有没有“无法预测的词”？
原文链接: https://spaces.ac.cn/archives/9046
发布日期: 

众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不可能成为概率...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">你的语言模型有没有“无法预测的词”？</h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/9046" target="_blank">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                <span class="tag"><i class="fas fa-tag"></i> 多任务</span>
                <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                <span class="tag"><i class="fas fa-tag"></i> attention</span>
                <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                
            </div>
            
        </header>

        <!-- Post Body -->
        <div class="post-content">
            <h1 id="_1">你的语言模型有没有“无法预测的词”？</h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9046">https://spaces.ac.cn/archives/9046</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不可能成为概率最大的那个。</p>
<p>当然，这种情况一般只出现在类别数远远超过编码向量维度的场景，常规的分类问题很少这么极端的。然而，我们知道语言模型本质上也是一个分类模型，它的类别数也就是词表的总大小，往往是远超过向量维度的，那么我们的语言模型是否有“无法预测的词”？（只考虑Greedy解码）</p>
<h2 id="_2">是否存在</h2>
<p>ACL2022的论文<a href="https://papers.cool/arxiv/2203.06462">《Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice》</a>首先探究了这个问题，正如其标题所言，答案是“理论上存在但实际出现概率很小”。</p>
<p>首先我们来看“理论上存在”。为了证明其存在性，我们只需要具体地构建一个例子。设各个类别向量分为$\boldsymbol{w}_1,\boldsymbol{w}_2,\cdots,\boldsymbol{w}_n\in\mathbb{R}^d$，偏置项为$b_1,b_2,\cdots,b_n$，假设类别$k$是可预测的，那么就存在$\boldsymbol{z}\in\mathbb{R}^d$，同时满足<br />
\begin{equation}\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k &gt; \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i\quad (\forall i \neq k)\end{equation}<br />
反过来，如果类别$k$不可预测，那么对于任意$\boldsymbol{z}\in\mathbb{R}^d$，必须存在某个$i\neq k$，满足<br />
\begin{equation}\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k \leq \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i\end{equation}<br />
由于现在我们只需要举例子，所以简单起见我们先考虑无偏置项的情况，并设$k=n$，此时条件为$\langle \boldsymbol{w}_i - \boldsymbol{w}_n, \boldsymbol{z}\rangle \geq 0$，也就是说，任意向量$\boldsymbol{z}$必然能找到向量$\boldsymbol{w}_i - \boldsymbol{w}_n$与之夹角小于等于90度。不难想象，当向量数大于空间维度、向量均匀分布在空间中时，这是有可能出现的，比如二维平面上的任意向量，就必然与$(0,1),(1,0),(0,-1),(-1,0)$之一的夹角小于90度，从而我们可以构造出例子：<br />
\begin{equation}\left\{\begin{aligned}<br />
&amp;\boldsymbol{w}_5 = (1, 1) \quad(\boldsymbol{w}_5\text{可以随便选})\\<br />
&amp;\boldsymbol{w}_1 = (1, 1) + (0, 1) = (1, 2)\\<br />
&amp;\boldsymbol{w}_2 = (1, 1) + (1, 0) = (2, 1)\\<br />
&amp;\boldsymbol{w}_3 = (1, 1) + (0, -1) = (1, 0)\\<br />
&amp;\boldsymbol{w}_4 = (1, 1) + (-1, 0) = (0, 1)\\<br />
\end{aligned}\right.\end{equation}<br />
在这个例子中，类别5就是不可预测的了，不信大家可以代入一些$\boldsymbol{z}$试试。</p>
<h2 id="_3">怎么判断</h2>
<p>现在我们已经确认了“无法预测的类别”是可能存在的，那么一个很自然的问题就是，对于一个训练好的模型，也就是给定$\boldsymbol{w}_1,\boldsymbol{w}_2,\cdots,\boldsymbol{w}_n\in\mathbb{R}^d$和$b_1,b_2,\cdots,b_n$，怎么判断其中是否存在不可预测的类别呢？</p>
<p>根据前一节的描述，从解不等式的角度来看，如果类别$k$是可预测的，那么下述不等式组的解集就会非空<br />
\begin{equation}\langle\boldsymbol{w}_k - \boldsymbol{w}_i,\boldsymbol{z}\rangle + (b_k - b_i) &gt; 0\quad (\forall i \neq k)\end{equation}<br />
不失一般性，我们同样设$k=n$，并且记$\Delta\boldsymbol{w}_i = \boldsymbol{w}_n - \boldsymbol{w}_i, \Delta b_i = b_n - b_i$，留意到<br />
\begin{equation}\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i &gt; 0\,(i = 1,2,\cdots,n-1)\quad\Leftrightarrow\quad \min_i \langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i &gt; 0\end{equation}<br />
所以，只要我们尽量最大化$\min\limits_i \langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i$，如果最终结果是正的，那么类别$n$就是可预测的，否则就是不可预测的。如果之前读过<a href="/archives/8896">《多任务学习漫谈（二）：行梯度之事》</a>的读者，就会发现该问题“似曾相识”，特别是如果没有偏置项的情况下，它跟多任务学习中寻找“帕累托最优”的过程是几乎一致的。</p>
<p>现在问题变为<br />
\begin{equation}\max_{\boldsymbol{z}} \min_i \langle\Delta\boldsymbol{w}<em _Vert="\Vert" _boldsymbol_z="\boldsymbol{z">i,\boldsymbol{z}\rangle + \Delta b_i\end{equation}<br />
为了避免发散到无穷，我们可以加个约束$\Vert \boldsymbol{z}\Vert\leq r$：<br />
\begin{equation}\max</em>}\Vert\leq r} \min_i \langle\Delta\boldsymbol{w<em n-1="n-1">i,\boldsymbol{z}\rangle + \Delta b_i \end{equation}<br />
其中$r$是一个常数，只要$r$取得足够大，它就能跟实际情况足够吻合，因为神经网络的输出通常来说也是有界的。接下来的过程就跟<a href="/archives/8896">《多任务学习漫谈（二）：行梯度之事》</a>的几乎一样了，首先引入<br />
\begin{equation}\mathbb{P}^{n-1} = \left\{(\alpha_1,\alpha_2,\cdots,\alpha</em>})\left|\alpha_1,\alpha_2,\cdots,\alpha_{n-1}\geq 0, \sum_i \alpha_i = 1\right.\right\}\end{equation<br />
那么问题变成<br />
\begin{equation}\max_{\Vert \boldsymbol{z}\Vert\leq r} \min_{\alpha\in\mathbb{P}^{n-1}} \left\langle\sum_i \alpha_i \Delta\boldsymbol{w}<em _alpha_in_mathbb_P="\alpha\in\mathbb{P">i,\boldsymbol{z}\right\rangle + \sum_i \alpha_i \Delta b_i\end{equation}<br />
根据冯·诺依曼的<a href="https://en.wikipedia.org/wiki/Minimax_theorem">Minimax定理</a>，可以交换$\max$和$\min$的顺序<br />
\begin{equation}\min</em>}^{n-1}} \max_{\Vert \boldsymbol{z}\Vert\leq r}\left\langle\sum_i \alpha_i \Delta\boldsymbol{w<em _alpha_in_mathbb_P="\alpha\in\mathbb{P">i,\boldsymbol{z}\right\rangle + \sum_i \alpha_i \Delta b_i\end{equation}<br />
很显然，$\max$这一步在$\Vert\boldsymbol{z}\Vert=r$且$\boldsymbol{z}$跟$\sum\limits_i \alpha_i \Delta\boldsymbol{w}_i$同向时取到，结果为<br />
\begin{equation}\min</em>}^{n-1}} r\left\Vert\sum_i \alpha_i \Delta\boldsymbol{w<em _alpha_in_mathbb_P="\alpha\in\mathbb{P">i\right\Vert + \sum_i \alpha_i \Delta b_i\end{equation}<br />
当$r$足够大时，偏置项的影响就非常小了，所以这几乎就等价于没有偏置项的情形<br />
\begin{equation}\min</em>}^{n-1}} \left\Vert\sum_i \alpha_i \Delta\boldsymbol{w}_i\right\Vert\end{equation<br />
最后的$\min$的求解过程已经在<a href="/archives/8896">《多任务学习漫谈（二）：行梯度之事》</a>中讨论过了，主要用到了<a href="https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe算法</a>，不再重复。</p>
<p><strong>（注：以上判别过程是笔者自己给出的，跟论文<a href="https://papers.cool/arxiv/2203.06462">《Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice》</a>中的方法并不相同。）</strong></p>
<h2 id="_4">实践如何</h2>
<p>前面的讨论都是理论上的，那么实际的语言模型出现“无法预测的词”的概率大不大呢？原论文对一些训练好的语言模型和生成模型进行了检验，发现实际上出现的概率很小，比如下表中的机器翻译模型检验结果：  </p>
<p><a href="/usr/uploads/2022/04/3980694051.png" title="点击查看原图"><img alt="机器翻译模型的检验结果" src="/usr/uploads/2022/04/3980694051.png" /></a></p>
<p>机器翻译模型的检验结果</p>
<p>其实这不难理解，从前面的讨论中我们知道“无法预测的词”一般只出现在类别数远远大于向量维度的情况，也就是原论文标题中的“Low-Rank”。但由于“维度灾难”的原因，“远远大于”这个概念其实并非我们直观所想的那样，比如对于2维空间来说，类别数为4就可以称得上“远远大于”，但如果是200维空间，那么即便是类别数为40000也算不上“远远大于”。常见的语言模型向量维度基本上都有几百维，而词表顶多也就是数十万的级别，因此其实还是算不上“远远大于”，因此出现“无法预测的词”的概率就很小了。</p>
<p>另外，我们还可以证明，如果所有的$\boldsymbol{w}_i$互不相同但是模长都相等，那么是绝对不会出现“无法预测的词”，因此这种不可预测的情况只出现在$\boldsymbol{w}_i$模长差异较大的情况，而在当前主流的深度模型中，由于各种Normalization技术的应用，$\boldsymbol{w}_i$模长差异较大的情况很少出现了，这进一步降低了“无法预测的词”的出现概率了。</p>
<p>当然，还是文章开头说了，本文的“无法预测的词”指的是最大化预测，也就是Greedy Search，如果用Beam Search或者随机采样，那么即便存在“无法预测的词”，也依然是可能生成出来的。这个“无法预测的词”，更多是一个好玩但实用价值不大的理论概念了，</p>
<h2 id="_5">最后小结</h2>
<p>本文向大家介绍了一个没什么实用价值但是颇为有意思的现象：你的语言模型可能存在一些“无法预测的词”，它永远不可能成为概率最大者。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9046">https://spaces.ac.cn/archives/9046</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 20, 2022). 《你的语言模型有没有“无法预测的词”？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9046">https://spaces.ac.cn/archives/9046</a></p>
<p>@online{kexuefm-9046,<br />
title={你的语言模型有没有“无法预测的词”？},<br />
author={苏剑林},<br />
year={2022},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/9046}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>

        <!-- Back to Home -->
        <div class="text-center mt-5 mb-4">
            <a href="../index.html" class="btn btn-outline-primary">
                <i class="fas fa-arrow-left"></i> 返回首页
            </a>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>
</body>
</html>
