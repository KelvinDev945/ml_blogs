<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>你的语言模型有没有“无法预测的词”？ | ML & Math Blog Posts</title>
    <meta name="description" content="你的语言模型有没有“无法预测的词”？&para;
原文链接: https://spaces.ac.cn/archives/9046
发布日期: 

众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #179 你的语言模型有没有“无法预测的词”？
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#179</span>
                你的语言模型有没有“无法预测的词”？
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-04-20</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=多任务" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 多任务</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">你的语言模型有没有“无法预测的词”？<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9046">https://spaces.ac.cn/archives/9046</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不可能成为概率最大的那个。</p>
<p>当然，这种情况一般只出现在类别数远远超过编码向量维度的场景，常规的分类问题很少这么极端的。然而，我们知道语言模型本质上也是一个分类模型，它的类别数也就是词表的总大小，往往是远超过向量维度的，那么我们的语言模型是否有“无法预测的词”？（只考虑Greedy解码）</p>
<h2 id="_2">是否存在<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>ACL2022的论文<a href="https://papers.cool/arxiv/2203.06462">《Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice》</a>首先探究了这个问题，正如其标题所言，答案是“理论上存在但实际出现概率很小”。</p>
<p>首先我们来看“理论上存在”。为了证明其存在性，我们只需要具体地构建一个例子。设各个类别向量分为$\boldsymbol{w}_1,\boldsymbol{w}_2,\cdots,\boldsymbol{w}_n\in\mathbb{R}^d$，偏置项为$b_1,b_2,\cdots,b_n$，假设类别$k$是可预测的，那么就存在$\boldsymbol{z}\in\mathbb{R}^d$，同时满足<br />
\begin{equation}\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k &gt; \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i\quad (\forall i \neq k)\end{equation}<br />
反过来，如果类别$k$不可预测，那么对于任意$\boldsymbol{z}\in\mathbb{R}^d$，必须存在某个$i\neq k$，满足<br />
\begin{equation}\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k \leq \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i\end{equation}<br />
由于现在我们只需要举例子，所以简单起见我们先考虑无偏置项的情况，并设$k=n$，此时条件为$\langle \boldsymbol{w}_i - \boldsymbol{w}_n, \boldsymbol{z}\rangle \geq 0$，也就是说，任意向量$\boldsymbol{z}$必然能找到向量$\boldsymbol{w}_i - \boldsymbol{w}_n$与之夹角小于等于90度。不难想象，当向量数大于空间维度、向量均匀分布在空间中时，这是有可能出现的，比如二维平面上的任意向量，就必然与$(0,1),(1,0),(0,-1),(-1,0)$之一的夹角小于90度，从而我们可以构造出例子：<br />
\begin{equation}\left\{\begin{aligned}
&amp;\boldsymbol{w}_5 = (1, 1) \quad(\boldsymbol{w}_5\text{可以随便选})\\
&amp;\boldsymbol{w}_1 = (1, 1) + (0, 1) = (1, 2)\\
&amp;\boldsymbol{w}_2 = (1, 1) + (1, 0) = (2, 1)\\
&amp;\boldsymbol{w}_3 = (1, 1) + (0, -1) = (1, 0)\\
&amp;\boldsymbol{w}_4 = (1, 1) + (-1, 0) = (0, 1)\\
\end{aligned}\right.\end{equation}<br />
在这个例子中，类别5就是不可预测的了，不信大家可以代入一些$\boldsymbol{z}$试试。</p>
<h2 id="_3">怎么判断<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>现在我们已经确认了“无法预测的类别”是可能存在的，那么一个很自然的问题就是，对于一个训练好的模型，也就是给定$\boldsymbol{w}_1,\boldsymbol{w}_2,\cdots,\boldsymbol{w}_n\in\mathbb{R}^d$和$b_1,b_2,\cdots,b_n$，怎么判断其中是否存在不可预测的类别呢？</p>
<p>根据前一节的描述，从解不等式的角度来看，如果类别$k$是可预测的，那么下述不等式组的解集就会非空<br />
\begin{equation}\langle\boldsymbol{w}_k - \boldsymbol{w}_i,\boldsymbol{z}\rangle + (b_k - b_i) &gt; 0\quad (\forall i \neq k)\end{equation}<br />
不失一般性，我们同样设$k=n$，并且记$\Delta\boldsymbol{w}_i = \boldsymbol{w}_n - \boldsymbol{w}_i, \Delta b_i = b_n - b_i$，留意到<br />
\begin{equation}\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i &gt; 0\,(i = 1,2,\cdots,n-1)\quad\Leftrightarrow\quad \min_i \langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i &gt; 0\end{equation}<br />
所以，只要我们尽量最大化$\min\limits_i \langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i$，如果最终结果是正的，那么类别$n$就是可预测的，否则就是不可预测的。如果之前读过<a href="/archives/8896">《多任务学习漫谈（二）：行梯度之事》</a>的读者，就会发现该问题“似曾相识”，特别是如果没有偏置项的情况下，它跟多任务学习中寻找“帕累托最优”的过程是几乎一致的。</p>
<p>现在问题变为<br />
\begin{equation}\max_{\boldsymbol{z}} \min_i \langle\Delta\boldsymbol{w}<em _Vert="\Vert" _boldsymbol_z="\boldsymbol{z">i,\boldsymbol{z}\rangle + \Delta b_i\end{equation}<br />
为了避免发散到无穷，我们可以加个约束$\Vert \boldsymbol{z}\Vert\leq r$：<br />
\begin{equation}\max</em>}\Vert\leq r} \min_i \langle\Delta\boldsymbol{w<em n-1="n-1">i,\boldsymbol{z}\rangle + \Delta b_i \end{equation}<br />
其中$r$是一个常数，只要$r$取得足够大，它就能跟实际情况足够吻合，因为神经网络的输出通常来说也是有界的。接下来的过程就跟<a href="/archives/8896">《多任务学习漫谈（二）：行梯度之事》</a>的几乎一样了，首先引入<br />
\begin{equation}\mathbb{P}^{n-1} = \left\{(\alpha_1,\alpha_2,\cdots,\alpha</em>})\left|\alpha_1,\alpha_2,\cdots,\alpha_{n-1}\geq 0, \sum_i \alpha_i = 1\right.\right\}\end{equation
那么问题变成
\begin{equation}\max_{\Vert \boldsymbol{z}\Vert\leq r} \min_{\alpha\in\mathbb{P}^{n-1}} \left\langle\sum_i \alpha_i \Delta\boldsymbol{w}<em _alpha_in_mathbb_P="\alpha\in\mathbb{P">i,\boldsymbol{z}\right\rangle + \sum_i \alpha_i \Delta b_i\end{equation}<br />
根据冯·诺依曼的<a href="https://en.wikipedia.org/wiki/Minimax_theorem">Minimax定理</a>，可以交换$\max$和$\min$的顺序<br />
\begin{equation}\min</em>}^{n-1}} \max_{\Vert \boldsymbol{z}\Vert\leq r}\left\langle\sum_i \alpha_i \Delta\boldsymbol{w<em _alpha_in_mathbb_P="\alpha\in\mathbb{P">i,\boldsymbol{z}\right\rangle + \sum_i \alpha_i \Delta b_i\end{equation}<br />
很显然，$\max$这一步在$\Vert\boldsymbol{z}\Vert=r$且$\boldsymbol{z}$跟$\sum\limits_i \alpha_i \Delta\boldsymbol{w}_i$同向时取到，结果为<br />
\begin{equation}\min</em>}^{n-1}} r\left\Vert\sum_i \alpha_i \Delta\boldsymbol{w<em _alpha_in_mathbb_P="\alpha\in\mathbb{P">i\right\Vert + \sum_i \alpha_i \Delta b_i\end{equation}<br />
当$r$足够大时，偏置项的影响就非常小了，所以这几乎就等价于没有偏置项的情形<br />
\begin{equation}\min</em>}^{n-1}} \left\Vert\sum_i \alpha_i \Delta\boldsymbol{w}_i\right\Vert\end{equation
最后的$\min$的求解过程已经在<a href="/archives/8896">《多任务学习漫谈（二）：行梯度之事》</a>中讨论过了，主要用到了<a href="https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe算法</a>，不再重复。</p>
<p><strong>（注：以上判别过程是笔者自己给出的，跟论文<a href="https://papers.cool/arxiv/2203.06462">《Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice》</a>中的方法并不相同。）</strong></p>
<h2 id="_4">实践如何<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>前面的讨论都是理论上的，那么实际的语言模型出现“无法预测的词”的概率大不大呢？原论文对一些训练好的语言模型和生成模型进行了检验，发现实际上出现的概率很小，比如下表中的机器翻译模型检验结果：  </p>
<p><a href="/usr/uploads/2022/04/3980694051.png" title="点击查看原图"><img alt="机器翻译模型的检验结果" src="/usr/uploads/2022/04/3980694051.png" /></a></p>
<p>机器翻译模型的检验结果</p>
<p>其实这不难理解，从前面的讨论中我们知道“无法预测的词”一般只出现在类别数远远大于向量维度的情况，也就是原论文标题中的“Low-Rank”。但由于“维度灾难”的原因，“远远大于”这个概念其实并非我们直观所想的那样，比如对于2维空间来说，类别数为4就可以称得上“远远大于”，但如果是200维空间，那么即便是类别数为40000也算不上“远远大于”。常见的语言模型向量维度基本上都有几百维，而词表顶多也就是数十万的级别，因此其实还是算不上“远远大于”，因此出现“无法预测的词”的概率就很小了。</p>
<p>另外，我们还可以证明，如果所有的$\boldsymbol{w}_i$互不相同但是模长都相等，那么是绝对不会出现“无法预测的词”，因此这种不可预测的情况只出现在$\boldsymbol{w}_i$模长差异较大的情况，而在当前主流的深度模型中，由于各种Normalization技术的应用，$\boldsymbol{w}_i$模长差异较大的情况很少出现了，这进一步降低了“无法预测的词”的出现概率了。</p>
<p>当然，还是文章开头说了，本文的“无法预测的词”指的是最大化预测，也就是Greedy Search，如果用Beam Search或者随机采样，那么即便存在“无法预测的词”，也依然是可能生成出来的。这个“无法预测的词”，更多是一个好玩但实用价值不大的理论概念了，</p>
<h2 id="_5">最后小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文向大家介绍了一个没什么实用价值但是颇为有意思的现象：你的语言模型可能存在一些“无法预测的词”，它永远不可能成为概率最大者。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9046">https://spaces.ac.cn/archives/9046</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 20, 2022). 《你的语言模型有没有“无法预测的词”？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9046">https://spaces.ac.cn/archives/9046</a></p>
<p>@online{kexuefm-9046,<br />
title={你的语言模型有没有“无法预测的词”？},<br />
author={苏剑林},<br />
year={2022},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/9046}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<h3 id="_7">一、不可预测类别的数学定义<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 分类模型的标准形式<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>对于一个$n$类分类模型，设编码向量为$\boldsymbol{z}\in\mathbb{R}^d$，其中$d$是向量维度。输出层的标准形式为：</p>
<p>\begin{equation}
\text{logit}_k = \langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k,\quad k=1,2,\cdots,n
\tag{1}
\end{equation}</p>
<p><strong>注释</strong>：这里$\boldsymbol{w}_k\in\mathbb{R}^d$是类别$k$对应的权重向量，$b_k$是偏置项，$\langle\cdot,\cdot\rangle$表示内积。</p>
<p>预测概率通过Softmax计算：</p>
<p>\begin{equation}
p(k|\boldsymbol{z}) = \frac{\exp(\text{logit}<em j="1">k)}{\sum</em>}^n \exp(\text{logit<em j="1">j)} = \frac{\exp(\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k)}{\sum</em>
\tag{2}
\end{equation}}^n \exp(\langle\boldsymbol{w}_j,\boldsymbol{z}\rangle + b_j)</p>
<h4 id="12">1.2 可预测性的严格定义<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p><strong>定义1（可预测类别）</strong>：类别$k$称为可预测的（Argmaxable），当且仅当存在$\boldsymbol{z}\in\mathbb{R}^d$使得：</p>
<p>\begin{equation}
k = \arg\max_{i=1,\cdots,n} \text{logit}_i
\tag{3}
\end{equation}</p>
<p><strong>数学直觉</strong>：这意味着存在某个输入能使类别$k$成为概率最大的预测结果。</p>
<p>等价地，类别$k$可预测当且仅当存在$\boldsymbol{z}\in\mathbb{R}^d$满足：</p>
<p>\begin{equation}
\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k &gt; \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i,\quad \forall i \neq k
\tag{4}
\end{equation}</p>
<p><strong>定义2（不可预测类别）</strong>：类别$k$称为不可预测的（Unargmaxable），当且仅当对任意$\boldsymbol{z}\in\mathbb{R}^d$，都存在某个$i\neq k$使得：</p>
<p>\begin{equation}
\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k \leq \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i
\tag{5}
\end{equation}</p>
<h4 id="13">1.3 几何解释<a class="toc-link" href="#13" title="Permanent link">&para;</a></h4>
<p>将不等式(4)改写为：</p>
<p>\begin{equation}
\langle\boldsymbol{w}_k - \boldsymbol{w}_i,\boldsymbol{z}\rangle &gt; b_i - b_k,\quad \forall i \neq k
\tag{6}
\end{equation}</p>
<p><strong>几何意义</strong>：在$d$维空间中，每个不等式定义一个半空间。类别$k$可预测等价于这$n-1$个半空间的交集非空。</p>
<p>记$\Delta\boldsymbol{w}_i = \boldsymbol{w}_k - \boldsymbol{w}_i$，$\Delta b_i = b_k - b_i$，则条件变为：</p>
<p>\begin{equation}
\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle &gt; -\Delta b_i,\quad \forall i \neq k
\tag{7}
\end{equation}</p>
<h3 id="_8">二、不可预测类别的存在性证明<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 无偏置情形的构造性证明<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p><strong>定理1</strong>：当类别数$n &gt; d$且无偏置项时，存在配置使得某个类别不可预测。</p>
<p><strong>证明</strong>：不失一般性，设类别$n$为待检验的类别。类别$n$不可预测等价于：</p>
<p>\begin{equation}
\forall \boldsymbol{z}\in\mathbb{R}^d,\, \exists i\in{1,\cdots,n-1}:\, \langle\boldsymbol{w}_i - \boldsymbol{w}_n,\boldsymbol{z}\rangle \geq 0
\tag{8}
\end{equation}</p>
<p>定义差向量集合：</p>
<p>\begin{equation}
\mathcal{V} = {\boldsymbol{v}_i = \boldsymbol{w}_i - \boldsymbol{w}_n\,|\, i=1,\cdots,n-1}
\tag{9}
\end{equation}</p>
<p>条件(8)等价于：对任意单位向量$\hat{\boldsymbol{z}}$，至少存在一个$\boldsymbol{v}_i$与$\hat{\boldsymbol{z}}$的夹角$\theta_i$满足$\theta_i \leq \pi/2$。</p>
<p><strong>关键观察</strong>：在$d$维空间中，如果$n-1$个向量${\boldsymbol{v}<em n-1="n-1">1,\cdots,\boldsymbol{v}</em>}$"覆盖"了所有方向，则条件成立。</p>
<h4 id="22">2.2 二维情形的具体构造<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>对于$d=2$的情况，构造如下配置：</p>
<p>选择$\boldsymbol{w}_5 = (1, 1)$作为参考点，然后定义：</p>
<p>\begin{equation}
\begin{aligned}
\boldsymbol{v}_1 &amp;= (0, 1) \quad &amp;(\text{指向正}\,y\,\text{方向}) \
\boldsymbol{v}_2 &amp;= (1, 0) \quad &amp;(\text{指向正}\,x\,\text{方向}) \
\boldsymbol{v}_3 &amp;= (0, -1) \quad &amp;(\text{指向负}\,y\,\text{方向}) \
\boldsymbol{v}_4 &amp;= (-1, 0) \quad &amp;(\text{指向负}\,x\,\text{方向})
\end{aligned}
\tag{10}
\end{equation}</p>
<p>则其他类别的权重向量为：</p>
<p>\begin{equation}
\boldsymbol{w}_i = \boldsymbol{w}_5 + \boldsymbol{v}_i,\quad i=1,2,3,4
\tag{11}
\end{equation}</p>
<p>即：</p>
<p>\begin{equation}
\begin{aligned}
\boldsymbol{w}_1 &amp;= (1, 2) \
\boldsymbol{w}_2 &amp;= (2, 1) \
\boldsymbol{w}_3 &amp;= (1, 0) \
\boldsymbol{w}_4 &amp;= (0, 1)
\end{aligned}
\tag{12}
\end{equation}</p>
<p><strong>验证</strong>：对于任意向量$\boldsymbol{z} = (z_x, z_y)$，考察四个内积：</p>
<p>\begin{equation}
\begin{aligned}
\langle\boldsymbol{v}_1,\boldsymbol{z}\rangle &amp;= z_y \
\langle\boldsymbol{v}_2,\boldsymbol{z}\rangle &amp;= z_x \
\langle\boldsymbol{v}_3,\boldsymbol{z}\rangle &amp;= -z_y \
\langle\boldsymbol{v}_4,\boldsymbol{z}\rangle &amp;= -z_x
\end{aligned}
\tag{13}
\end{equation}</p>
<p>分四种情况：
- 若$z_y \geq 0$且$z_y \geq |z_x|$，则$\langle\boldsymbol{v}_1,\boldsymbol{z}\rangle \geq 0$
- 若$z_x \geq 0$且$z_x \geq |z_y|$，则$\langle\boldsymbol{v}_2,\boldsymbol{z}\rangle \geq 0$
- 若$z_y \leq 0$且$|z_y| \geq |z_x|$，则$\langle\boldsymbol{v}_3,\boldsymbol{z}\rangle \geq 0$
- 若$z_x \leq 0$且$|z_x| \geq |z_y|$，则$\langle\boldsymbol{v}_4,\boldsymbol{z}\rangle \geq 0$</p>
<p>这四种情况覆盖了所有可能，因此类别5确实不可预测。$\square$</p>
<h4 id="23">2.3 高维推广<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p><strong>引理1</strong>：在$d$维空间中，至多需要$2d$个向量即可覆盖所有方向。</p>
<p><strong>证明</strong>：选择$2d$个标准基向量及其负方向：</p>
<p>\begin{equation}
{\boldsymbol{e}_1, -\boldsymbol{e}_1, \boldsymbol{e}_2, -\boldsymbol{e}_2, \cdots, \boldsymbol{e}_d, -\boldsymbol{e}_d}
\tag{14}
\end{equation}</p>
<p>其中$\boldsymbol{e}_i$是第$i$个坐标为1、其余为0的单位向量。</p>
<p>对任意$\boldsymbol{z} = (z_1, z_2, \cdots, z_d)$，设$|z_k| = \max_i |z_i|$。则：
- 若$z_k \geq 0$，则$\langle\boldsymbol{e}_k,\boldsymbol{z}\rangle = z_k \geq 0$
- 若$z_k &lt; 0$，则$\langle-\boldsymbol{e}_k,\boldsymbol{z}\rangle = -z_k &gt; 0$</p>
<p>因此总存在某个向量与$\boldsymbol{z}$内积非负。$\square$</p>
<h3 id="_9">三、判别算法的凸优化理论<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<h4 id="31-minimax">3.1 问题转化为Minimax优化<a class="toc-link" href="#31-minimax" title="Permanent link">&para;</a></h4>
<p>给定${\boldsymbol{w}_1,\cdots,\boldsymbol{w}_n}$和${b_1,\cdots,b_n}$，判断类别$k$是否可预测。</p>
<p>不失一般性设$k=n$，定义：</p>
<p>\begin{equation}
\Delta\boldsymbol{w}_i = \boldsymbol{w}_n - \boldsymbol{w}_i,\quad \Delta b_i = b_n - b_i,\quad i=1,\cdots,n-1
\tag{15}
\end{equation}</p>
<p>类别$n$可预测等价于下述优化问题有正解：</p>
<p>\begin{equation}
\max_{\boldsymbol{z}\in\mathbb{R}^d} \min_{i=1,\cdots,n-1} \left(\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i\right)
\tag{16}
\end{equation}</p>
<p><strong>数学直觉</strong>：我们寻找一个$\boldsymbol{z}$，使得所有不等式的最小"松弛量"尽可能大。</p>
<h4 id="32">3.2 添加有界约束<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>为避免发散，添加球约束：</p>
<p>\begin{equation}
\max_{|\boldsymbol{z}| \leq r} \min_{i=1,\cdots,n-1} \left(\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i\right)
\tag{17}
\end{equation}</p>
<p>其中$r &gt; 0$是足够大的常数。</p>
<p><strong>性质</strong>：当$r \to \infty$时，若最优值趋于正数，则类别可预测；若趋于非正数，则不可预测。</p>
<h4 id="33">3.3 引入概率单纯形<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>定义$(n-1)$维概率单纯形：</p>
<p>\begin{equation}
\mathbb{P}^{n-1} = \left{(\alpha_1,\cdots,\alpha_{n-1})\,\Big|\,\alpha_i \geq 0,\, \sum_{i=1}^{n-1}\alpha_i = 1\right}
\tag{18}
\end{equation}</p>
<p><strong>关键观察</strong>：最小值等价于对所有凸组合的下界：</p>
<p>\begin{equation}
\min_{i=1,\cdots,n-1} f_i = \min_{\alpha\in\mathbb{P}^{n-1}} \sum_{i=1}^{n-1} \alpha_i f_i
\tag{19}
\end{equation}</p>
<p><strong>证明</strong>：设最小值在$j$处达到，即$f_j = \min_i f_i$。取$\alpha_j = 1$，其余$\alpha_i = 0$，则凸组合等于$f_j$。</p>
<p>反之，对任意$\alpha\in\mathbb{P}^{n-1}$：</p>
<p>\begin{equation}
\sum_{i=1}^{n-1} \alpha_i f_i \geq \sum_{i=1}^{n-1} \alpha_i \min_j f_j = \min_j f_j
\tag{20}
\end{equation}</p>
<p>因此两者相等。$\square$</p>
<h4 id="34-minimax">3.4 Minimax定理的应用<a class="toc-link" href="#34-minimax" title="Permanent link">&para;</a></h4>
<p>应用式(19)，问题(17)变为：</p>
<p>\begin{equation}
\max_{|\boldsymbol{z}| \leq r} \min_{\alpha\in\mathbb{P}^{n-1}} \left\langle\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}<em i="1">i,\boldsymbol{z}\right\rangle + \sum</em>\alpha_i\Delta b_i
\tag{21}
\end{equation}}^{n-1</p>
<p><strong>冯·诺依曼Minimax定理</strong>：对于凸-凹函数$f(\boldsymbol{x},\boldsymbol{y})$，若定义域紧致，则：</p>
<p>\begin{equation}
\max_{\boldsymbol{x}} \min_{\boldsymbol{y}} f(\boldsymbol{x},\boldsymbol{y}) = \min_{\boldsymbol{y}} \max_{\boldsymbol{x}} f(\boldsymbol{x},\boldsymbol{y})
\tag{22}
\end{equation}</p>
<p><strong>验证凸凹性</strong>：
- 函数关于$\boldsymbol{z}$是线性的（凸且凹）
- 函数关于$\alpha$是线性的（凸且凹）
- $|\boldsymbol{z}| \leq r$是凸集
- $\mathbb{P}^{n-1}$是凸集</p>
<p>因此可交换max和min：</p>
<p>\begin{equation}
\min_{\alpha\in\mathbb{P}^{n-1}} \max_{|\boldsymbol{z}| \leq r} \left\langle\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}<em i="1">i,\boldsymbol{z}\right\rangle + \sum</em>\alpha_i\Delta b_i
\tag{23}
\end{equation}}^{n-1</p>
<h4 id="35">3.5 内层最大化问题的解析解<a class="toc-link" href="#35" title="Permanent link">&para;</a></h4>
<p>固定$\alpha$，内层问题为：</p>
<p>\begin{equation}
\max_{|\boldsymbol{z}| \leq r} \langle\boldsymbol{g}_\alpha,\boldsymbol{z}\rangle
\tag{24}
\end{equation}</p>
<p>其中$\boldsymbol{g}<em i="1">\alpha = \sum</em>_i$。}^{n-1}\alpha_i\Delta\boldsymbol{w</p>
<p><strong>解析解</strong>：由Cauchy-Schwarz不等式：</p>
<p>\begin{equation}
\langle\boldsymbol{g}<em>\alpha,\boldsymbol{z}\rangle \leq |\boldsymbol{g}</em>\alpha| \cdot |\boldsymbol{z}| \leq r|\boldsymbol{g}_\alpha|
\tag{25}
\end{equation}</p>
<p>等号成立当且仅当$\boldsymbol{z} = r\frac{\boldsymbol{g}<em>\alpha}{|\boldsymbol{g}</em>\alpha|}$（假设$\boldsymbol{g}_\alpha \neq \boldsymbol{0}$）。</p>
<p>因此最优值为：</p>
<p>\begin{equation}
r|\boldsymbol{g}<em i="1">\alpha| + \sum</em>\alpha_i\Delta b_i
\tag{26}
\end{equation}}^{n-1</p>
<h4 id="36">3.6 外层最小化问题<a class="toc-link" href="#36" title="Permanent link">&para;</a></h4>
<p>问题简化为：</p>
<p>\begin{equation}
\min_{\alpha\in\mathbb{P}^{n-1}} \left(r\left|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}<em i="1">i\right| + \sum</em>\alpha_i\Delta b_i\right)
\tag{27}
\end{equation}}^{n-1</p>
<p><strong>渐近分析</strong>：当$r \to \infty$时，第一项占主导，问题渐近于：</p>
<p>\begin{equation}
\min_{\alpha\in\mathbb{P}^{n-1}} \left|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i\right|
\tag{28}
\end{equation}</p>
<p><strong>物理意义</strong>：寻找向量${\Delta\boldsymbol{w}_i}$的最短凸组合。</p>
<h3 id="frank-wolfe">四、Frank-Wolfe算法详解<a class="toc-link" href="#frank-wolfe" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 算法原理<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p>Frank-Wolfe算法（又称条件梯度法）求解约束优化问题：</p>
<p>\begin{equation}
\min_{\boldsymbol{x}\in\mathcal{C}} f(\boldsymbol{x})
\tag{29}
\end{equation}</p>
<p>其中$f$是凸函数，$\mathcal{C}$是凸集。</p>
<p><strong>算法思想</strong>：在当前点$\boldsymbol{x}_t$处，用线性近似代替原函数：</p>
<p>\begin{equation}
f(\boldsymbol{x}) \approx f(\boldsymbol{x}_t) + \langle\nabla f(\boldsymbol{x}_t), \boldsymbol{x} - \boldsymbol{x}_t\rangle
\tag{30}
\end{equation}</p>
<p>最小化线性部分等价于：</p>
<p>\begin{equation}
\boldsymbol{s}<em _boldsymbol_x="\boldsymbol{x">t = \arg\min</em>\rangle
\tag{31}
\end{equation}}\in\mathcal{C}} \langle\nabla f(\boldsymbol{x}_t), \boldsymbol{x</p>
<p>然后沿方向$\boldsymbol{s}_t - \boldsymbol{x}_t$进行线搜索：</p>
<p>\begin{equation}
\boldsymbol{x}_{t+1} = (1-\gamma_t)\boldsymbol{x}_t + \gamma_t\boldsymbol{s}_t
\tag{32}
\end{equation}</p>
<p>其中步长$\gamma_t \in [0,1]$。</p>
<h4 id="42-28">4.2 应用于问题(28)<a class="toc-link" href="#42-28" title="Permanent link">&para;</a></h4>
<p>对于$f(\alpha) = \left|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i\right|$，梯度为：</p>
<p>\begin{equation}
\frac{\partial f}{\partial \alpha_j} = \frac{\Delta\boldsymbol{w}<em i="1">j \cdot \sum</em>}^{n-1}\alpha_i\Delta\boldsymbol{w<em i="1">i}{\left|\sum</em>
\tag{33}
\end{equation}}^{n-1}\alpha_i\Delta\boldsymbol{w}_i\right|</p>
<p>记$\boldsymbol{g}<em i="1">t = \sum</em>_i$，则梯度向量为：}^{n-1}\alpha_i^{(t)}\Delta\boldsymbol{w</p>
<p>\begin{equation}
\nabla f(\alpha^{(t)}) = \frac{1}{|\boldsymbol{g}<em n-1="n-1">t|}(\Delta\boldsymbol{w}_1 \cdot \boldsymbol{g}_t, \cdots, \Delta\boldsymbol{w}</em>_t)
\tag{34}
\end{equation}} \cdot \boldsymbol{g</p>
<p><strong>线性子问题</strong>：</p>
<p>\begin{equation}
\boldsymbol{s}<em _alpha_in_mathbb_P="\alpha\in\mathbb{P">t = \arg\min</em>_t)
\tag{35}
\end{equation}}^{n-1}} \sum_{j=1}^{n-1} \alpha_j (\Delta\boldsymbol{w}_j \cdot \boldsymbol{g</p>
<p><strong>解析解</strong>：选择使内积最小的索引：</p>
<p>\begin{equation}
j^* = \arg\min_{j=1,\cdots,n-1} (\Delta\boldsymbol{w}_j \cdot \boldsymbol{g}_t)
\tag{36}
\end{equation}</p>
<p>令$\alpha_{j^*} = 1$，其余为0。</p>
<h4 id="43">4.3 步长选择<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>线搜索问题为：</p>
<p>\begin{equation}
\gamma_t = \arg\min_{\gamma\in[0,1]} |(1-\gamma)\boldsymbol{g}<em j_="j^*">t + \gamma\Delta\boldsymbol{w}</em>|
\tag{37}
\end{equation}</p>
<p>展开平方：</p>
<p>\begin{equation}
\begin{aligned}
&amp;|(1-\gamma)\boldsymbol{g}<em>t + \gamma\Delta\boldsymbol{w}</em>{j^<em>}|^2 \
=&amp;\, (1-\gamma)^2|\boldsymbol{g}<em>t|^2 + 2\gamma(1-\gamma)(\boldsymbol{g}_t \cdot \Delta\boldsymbol{w}</em>{j^</em>}) + \gamma^2|\Delta\boldsymbol{w}_{j^*}|^2
\end{aligned}
\tag{38}
\end{equation}</p>
<p>对$\gamma$求导并令其为0：</p>
<p>\begin{equation}
-2(1-\gamma)|\boldsymbol{g}<em>t|^2 + 2(1-2\gamma)(\boldsymbol{g}_t \cdot \Delta\boldsymbol{w}</em>{j^<em>}) + 2\gamma|\Delta\boldsymbol{w}_{j^</em>}|^2 = 0
\tag{39}
\end{equation}</p>
<p>解得：</p>
<p>\begin{equation}
\gamma_t = \frac{|\boldsymbol{g}<em>t|^2 - \boldsymbol{g}_t \cdot \Delta\boldsymbol{w}</em>{j^<em>}}{|\boldsymbol{g}<em>t|^2 - 2(\boldsymbol{g}_t \cdot \Delta\boldsymbol{w}</em>{j^</em>}) + |\Delta\boldsymbol{w}<em j_="j^*">{j^<em>}|^2} = \frac{|\boldsymbol{g}<em>t|^2 - \boldsymbol{g}_t \cdot \Delta\boldsymbol{w}</em>{j^</em>}}{|\boldsymbol{g}_t - \Delta\boldsymbol{w}</em>
\tag{40}
\end{equation}}|^2</p>
<p>若计算值超出$[0,1]$，则截断到边界。</p>
<h4 id="44">4.4 算法伪代码<a class="toc-link" href="#44" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>输入：向量集合{Δw_1, ..., Δw_{n-1}}
输出：最小范数凸组合及其范数

1. 初始化：α^(0) = (1/(n-1), ..., 1/(n-1))
2. For t = 0, 1, 2, ..., T:
   a. 计算：g_t = Σ_{i=1}^{n-1} α_i^(t) Δw_i
   b. 计算梯度方向：找到j* = argmin_j (Δw_j · g_t)
   c. 计算步长：γ_t 根据式(40)
   d. 更新：α^(t+1) = (1-γ_t)α^(t) + γ_t e_{j*}
      其中e_{j*}是第j*个标准基向量
   e. 若||g_{t+1} - g_t|| &lt; ε，停止
3. 返回：g_T 和 ||g_T||
</code></pre></div>

<h4 id="45">4.5 收敛性分析<a class="toc-link" href="#45" title="Permanent link">&para;</a></h4>
<p><strong>定理2</strong>：Frank-Wolfe算法对光滑凸函数有收敛率$O(1/T)$。</p>
<p><strong>证明思路</strong>：
1. 设$f^<em> = \min_{\alpha\in\mathbb{P}^{n-1}} f(\alpha)$
2. 利用凸性：$f(\alpha^{(t+1)}) - f^</em> \leq \langle\nabla f(\alpha^{(t)}), \alpha^{(t+1)} - \alpha^*\rangle$
3. 结合步长选择和光滑性条件，得到递推不等式
4. 累加得到$O(1/T)$收敛率</p>
<p>对于我们的问题，$f(\alpha) = |\sum_i \alpha_i \Delta\boldsymbol{w}_i|$虽然在某些点不可微（当和向量为零时），但在实践中几乎总是可微的。</p>
<h3 id="_10">五、实践中的考虑<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 数值稳定性<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：当$|\boldsymbol{g}_t| \approx 0$时，梯度计算式(34)可能数值不稳定。</p>
<p><strong>解决方案</strong>：添加小的正则化项：</p>
<p>\begin{equation}
\nabla f(\alpha) \approx \frac{1}{|\boldsymbol{g}| + \epsilon}(\Delta\boldsymbol{w}<em n-1="n-1">1 \cdot \boldsymbol{g}, \cdots, \Delta\boldsymbol{w}</em>)
\tag{41}
\end{equation}} \cdot \boldsymbol{g</p>
<p>其中$\epsilon = 10^{-8}$是小常数。</p>
<h4 id="52">5.2 维度诅咒的影响<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p><strong>观察</strong>：文中提到"远远大于"的相对性。在$d$维空间中，随机向量间的夹角分布集中在$\pi/2$附近。</p>
<p><strong>定量分析</strong>：两个随机单位向量的内积$X = \boldsymbol{u} \cdot \boldsymbol{v}$近似服从：</p>
<p>\begin{equation}
X \sim \mathcal{N}\left(0, \frac{1}{d}\right)
\tag{42}
\end{equation}</p>
<p>因此内积的标准差为$1/\sqrt{d}$，当$d$很大时迅速趋于0。</p>
<p><strong>推论</strong>：在高维空间中，向量趋于正交，因此需要指数级多的向量才能"覆盖"所有方向。</p>
<p>例如，要使任意方向与至少一个向量夹角小于$\theta$，需要的向量数量大约为：</p>
<p>\begin{equation}
N \sim \left(\frac{\pi}{\theta}\right)^{d-1}
\tag{43}
\end{equation}</p>
<p>这解释了为什么$n \gg d$在高维时的含义与低维不同。</p>
<h4 id="53">5.3 模长一致性的保护作用<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p><strong>定理3</strong>：若所有$\boldsymbol{w}_i$满足$|\boldsymbol{w}_i| = c$（常数），且互不相同，则所有类别都可预测。</p>
<p><strong>证明</strong>：对于类别$k$，选择$\boldsymbol{z} = \boldsymbol{w}_k$。则：</p>
<p>\begin{equation}
\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle = |\boldsymbol{w}_k|^2 = c^2
\tag{44}
\end{equation}</p>
<p>\begin{equation}
\langle\boldsymbol{w}_i,\boldsymbol{z}\rangle = \boldsymbol{w}_i \cdot \boldsymbol{w}_k &lt; |\boldsymbol{w}_i| \cdot |\boldsymbol{w}_k| = c^2
\tag{45}
\end{equation}</p>
<p>最后一个不等式因为$\boldsymbol{w}_i \neq \boldsymbol{w}_k$严格成立（Cauchy-Schwarz不等式等号成立当且仅当向量共线）。</p>
<p>因此$\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle &gt; \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle$对所有$i \neq k$成立，类别$k$可预测。$\square$</p>
<p><strong>实践意义</strong>：Layer Normalization等技术通过约束向量模长，降低了不可预测类别出现的概率。</p>
<h4 id="54-beam-search">5.4 Beam Search与随机采样<a class="toc-link" href="#54-beam-search" title="Permanent link">&para;</a></h4>
<p>Greedy解码选择：</p>
<p>\begin{equation}
\hat{k} = \arg\max_{k} p(k|\boldsymbol{z})
\tag{46}
\end{equation}</p>
<p>即使某类别$k_0$不可预测（即$k_0 \neq \hat{k}$对所有$\boldsymbol{z}$成立），在Beam Search中仍可能出现：</p>
<p><strong>Beam Search</strong>保留top-$K$候选：</p>
<p>\begin{equation}
\mathcal{B} = {k_1, k_2, \cdots, k_K}\,\text{ where }\, p(k_1|\boldsymbol{z}) \geq p(k_2|\boldsymbol{z}) \geq \cdots \geq p(k_K|\boldsymbol{z})
\tag{47}
\end{equation}</p>
<p>只要$k_0 \in \mathcal{B}$，它就有机会被选择。</p>
<p><strong>随机采样</strong>按概率分布采样：</p>
<p>\begin{equation}
k \sim p(\cdot|\boldsymbol{z})
\tag{48}
\end{equation}</p>
<p>即使$p(k_0|\boldsymbol{z})$很小但非零，$k_0$仍有非零概率被采样。</p>
<h3 id="_11">六、计算示例<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 二维实例的数值验证<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>使用式(12)的构造，验证类别5不可预测。</p>
<p>测试向量$\boldsymbol{z} = (1, 0.5)$：</p>
<p>\begin{equation}
\begin{aligned}
\langle\boldsymbol{w}_1 - \boldsymbol{w}_5, \boldsymbol{z}\rangle &amp;= \langle(0,1), (1,0.5)\rangle = 0.5 &gt; 0 \
\langle\boldsymbol{w}_2 - \boldsymbol{w}_5, \boldsymbol{z}\rangle &amp;= \langle(1,0), (1,0.5)\rangle = 1.0 &gt; 0 \
\langle\boldsymbol{w}_3 - \boldsymbol{w}_5, \boldsymbol{z}\rangle &amp;= \langle(0,-1), (1,0.5)\rangle = -0.5 &lt; 0 \
\langle\boldsymbol{w}_4 - \boldsymbol{w}_5, \boldsymbol{z}\rangle &amp;= \langle(-1,0), (1,0.5)\rangle = -1.0 &lt; 0
\end{aligned}
\tag{49}
\end{equation}</p>
<p>最大值为1.0，对应类别2，验证了类别5不是最优。</p>
<p>测试其他方向可以类似验证。</p>
<h4 id="62-frank-wolfe">6.2 Frank-Wolfe迭代示例<a class="toc-link" href="#62-frank-wolfe" title="Permanent link">&para;</a></h4>
<p>假设$n-1=3$，$\Delta\boldsymbol{w}_1 = (1,0)$，$\Delta\boldsymbol{w}_2 = (0,1)$，$\Delta\boldsymbol{w}_3 = (-0.5,-0.5)$。</p>
<p><strong>迭代0</strong>：
- $\alpha^{(0)} = (1/3, 1/3, 1/3)$
- $\boldsymbol{g}_0 = (1/3)(1,0) + (1/3)(0,1) + (1/3)(-0.5,-0.5) = (1/6, 1/6)$
- $|\boldsymbol{g}_0| \approx 0.236$</p>
<p>内积：
- $\Delta\boldsymbol{w}_1 \cdot \boldsymbol{g}_0 = 1/6 \approx 0.167$
- $\Delta\boldsymbol{w}_2 \cdot \boldsymbol{g}_0 = 1/6 \approx 0.167$
- $\Delta\boldsymbol{w}_3 \cdot \boldsymbol{g}_0 = -1/6 \approx -0.167$（最小）</p>
<p>选择$j^* = 3$，更新$\alpha$朝向$(0,0,1)$方向...</p>
<p>（迭代多步后收敛到最优解）</p>
<h3 id="_12">七、理论扩展与开放问题<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 有偏置情形<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p>当包含偏置项时，问题变为：</p>
<p>\begin{equation}
\min_{\alpha\in\mathbb{P}^{n-1}} \left(r\left|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}<em i="1">i\right| + \sum</em>\alpha_i\Delta b_i\right)
\tag{50}
\end{equation}}^{n-1</p>
<p><strong>挑战</strong>：偏置项打破了纯几何结构，需要权衡两项的影响。</p>
<p><strong>启发式方法</strong>：通过增广向量将偏置融入：</p>
<p>\begin{equation}
\tilde{\boldsymbol{w}}_i = (\boldsymbol{w}_i, b_i) \in \mathbb{R}^{d+1}
\tag{51}
\end{equation}</p>
<p>在$(d+1)$维空间中应用无偏置算法。</p>
<h4 id="72">7.2 近似算法的误差界<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：Frank-Wolfe算法给出的解有多接近真实最优解？</p>
<p><strong>误差界</strong>：经过$T$次迭代后：</p>
<p>\begin{equation}
f(\alpha^{(T)}) - f^* \leq \frac{2LC}T}
\tag{52}
\end{equation}</p>
<p>其中$L$是Lipschitz常数，$C$是初始对偶间隙。</p>
<p>对于$f(\alpha) = |\sum_i \alpha_i \Delta\boldsymbol{w}_i|$：
- Lipschitz常数：$L = \max_i |\Delta\boldsymbol{w}_i|$
- 初始间隙：$C = f(\alpha^{(0)}) - \langle\nabla f(\alpha^{(0)}), \alpha^{(0)} - \alpha^*\rangle$</p>
<h4 id="73">7.3 概率性结果<a class="toc-link" href="#73" title="Permanent link">&para;</a></h4>
<p><strong>定理4</strong>（启发式）：在随机模型下，若$\boldsymbol{w}_i \sim \mathcal{N}(\boldsymbol{0}, \frac{1}{d}I_d)$独立采样，则不可预测类别出现的概率随$n/e^d$指数衰减。</p>
<p><strong>直觉</strong>：需要覆盖$S^{d-1}$球面，单位球面"面积"增长为$O(1)$，而随机向量的覆盖半径为$O(1/\sqrt{d})$，需要的向量数为$O(e^d)$。</p>
<h3 id="_13">八、实践建议<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 监控指标<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p>在训练语言模型时，可以定期检查：</p>
<ol>
<li>
<p><strong>权重矩阵的条件数</strong>：
   \begin{equation}
   \kappa(\boldsymbol{W}) = \frac{\sigma_{\max}(\boldsymbol{W})}{\sigma_{\min}(\boldsymbol{W})}
   \tag{53}
   \end{equation}
   条件数过大可能导致不可预测类别。</p>
</li>
<li>
<p><strong>最小凸组合范数</strong>：
   对每个类别$k$，计算$\min_\alpha |\sum_{i \neq k} \alpha_i (\boldsymbol{w}_i - \boldsymbol{w}_k)|$。若某个类别的值接近0，则可能不可预测。</p>
</li>
</ol>
<h4 id="82">8.2 正则化策略<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p><strong>谱归一化</strong>：约束权重矩阵的最大奇异值：</p>
<p>\begin{equation}
\boldsymbol{W}<em _max="\max">{\text{SN}} = \frac{\boldsymbol{W}}{\sigma</em>
\tag{54}
\end{equation}}(\boldsymbol{W})</p>
<p><strong>权重衰减与模长平衡</strong>：添加损失项：</p>
<p>\begin{equation}
\mathcal{L}<em i="1">{\text{reg}} = \sum</em>)^2
\tag{55}
\end{equation}}^n (|\boldsymbol{w}_i| - \bar{c</p>
<p>其中$\bar{c}$是目标模长。</p>
<h4 id="83">8.3 架构选择<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p><strong>Layer Normalization</strong>自动平衡特征尺度：</p>
<p>\begin{equation}
\text{LN}(\boldsymbol{z}) = \frac{\boldsymbol{z} - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
\tag{56}
\end{equation}</p>
<p>确保输出向量的范数在合理范围内。</p>
<p><strong>总结</strong>：通过理论分析与数值方法的结合，我们能够有效判断和预防语言模型中不可预测类别的出现，这对于理解模型行为和改进训练策略都具有重要意义。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="globalpointer下的kl散度应该是怎样的.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#178 GlobalPointer下的“KL散度”应该是怎样的？</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="gau-α尝鲜体验快好省的下一代attention.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#180 GAU-α：尝鲜体验快好省的下一代Attention</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">你的语言模型有没有“无法预测的词”？</a><ul>
<li><a href="#_2">是否存在</a></li>
<li><a href="#_3">怎么判断</a></li>
<li><a href="#_4">实践如何</a></li>
<li><a href="#_5">最后小结</a></li>
<li><a href="#_6">公式推导与注释</a><ul>
<li><a href="#_7">一、不可预测类别的数学定义</a></li>
<li><a href="#_8">二、不可预测类别的存在性证明</a></li>
<li><a href="#_9">三、判别算法的凸优化理论</a></li>
<li><a href="#frank-wolfe">四、Frank-Wolfe算法详解</a></li>
<li><a href="#_10">五、实践中的考虑</a></li>
<li><a href="#_11">六、计算示例</a></li>
<li><a href="#_12">七、理论扩展与开放问题</a></li>
<li><a href="#_13">八、实践建议</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>