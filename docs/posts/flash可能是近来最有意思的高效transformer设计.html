<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FLASH：可能是近来最有意思的高效Transformer设计 | ML & Math Blog Posts</title>
    <meta name="description" content="FLASH：可能是近来最有意思的高效Transformer设计&para;
原文链接: https://spaces.ac.cn/archives/8934
发布日期: 

高效Transformer，泛指所有概率Transformer效率的工作，笔者算是关注得比较早了，最早的博客可以追溯到2019年的《为节约而生：从标准Attention到稀疏Attention》，当时做这块的工作很少。后来，这...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #169 FLASH：可能是近来最有意思的高效Transformer设计
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#169</span>
                FLASH：可能是近来最有意思的高效Transformer设计
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-02-25</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=高效Transformer" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 高效Transformer</span>
                </a>
                
                <a href="../index.html?tags=GAU" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> GAU</span>
                </a>
                
                <a href="../index.html?tags=门控注意力" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 门控注意力</span>
                </a>
                
                <a href="../index.html?tags=线性复杂度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 线性复杂度</span>
                </a>
                
                <a href="../index.html?tags=稀疏注意力" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 稀疏注意力</span>
                </a>
                
                <a href="../index.html?tags=FLASH" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> FLASH</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="flashtransformer">FLASH：可能是近来最有意思的高效Transformer设计<a class="toc-link" href="#flashtransformer" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8934">https://spaces.ac.cn/archives/8934</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>高效Transformer，泛指所有概率Transformer效率的工作，笔者算是关注得比较早了，最早的博客可以追溯到2019年的<a href="/archives/6853">《为节约而生：从标准Attention到稀疏Attention》</a>，当时做这块的工作很少。后来，这类工作逐渐多了，笔者也跟进了一些，比如<a href="/archives/7546">线性Attention</a>、<a href="/archives/7921">Performer</a>、<a href="/archives/8180">Nyströmformer</a>，甚至自己也做了一些探索，比如之前的“<a href="/search/Transformer%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF/">Transformer升级之路</a>”。再后来，相关工作越来越多，但大多都很无趣，所以笔者就没怎么关注了。</p>
<p><a href="/usr/uploads/2022/02/1135966367.png" title="点击查看原图"><img alt="本文模型脉络图" src="/usr/uploads/2022/02/1135966367.png" /></a></p>
<p>本文模型脉络图</p>
<p>大抵是“久旱逢甘霖”的感觉，最近终于出现了一个比较有意思的高效Transformer工作——来自Google的<a href="https://papers.cool/arxiv/2202.10447">《Transformer Quality in Linear Time》</a>，经过细读之后，笔者认为论文里边真算得上是“惊喜满满”了～</p>
<h2 id="_1">何喜之有<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>什么样的结果值得我们用“惊喜”来形容？有没有言过其实？我们不妨先来看看论文做到了什么：</p>
<blockquote>
<p>1、提出了一种新的Transformer变体，它依然具有二次的复杂度，但是相比标准的Transformer，它有着更快的速度、更低的显存占用以及更好的效果；</p>
<p>2、提出一种新的线性化Transformer方案，它不但提升了原有线性Attention的效果，还保持了做Decoder的可能性，并且做Decoder时还能保持高效的训练并行性。</p>
</blockquote>
<p>说实话，笔者觉得做到以上任意一点都是非常难得的，而这篇论文一下子做到了两点，所以我愿意用“惊喜满满”来形容它。更重要的是，论文的改进总的来说还是比较自然和优雅的，不像很多类似工作一样显得很生硬。此外，笔者自己也做了简单的复现实验，结果显示论文的可复现性应该是蛮好的，所以真的有种“Transformer危矣”的感觉了。</p>
<h2 id="_2">门控注意<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>闲话少说，进入主题。我们知道<a href="/archives/4765">标准的Transformer</a>其实是Attention层和FFN层交替构建的，而这篇论文的核心是提出了一个融合了两者的新设计GAU（Gated Attention Unit，门控注意力单元），它是新模型更快、更省、更好的关键，此外它使得整个模型只有一种层，也显得更为优雅。</p>
<h3 id="_3">威力初显<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h3>
<p>怎么做到Attention和FFN的融合呢？首先，标准的FFN是两层MLP模型：<br />
\begin{equation}\boldsymbol{O}=\phi(\boldsymbol{X}\boldsymbol{W}_u)\boldsymbol{W}_o\end{equation}<br />
这里$\boldsymbol{X}\in\mathbb{R}^{n\times d},\boldsymbol{W}_u\in\mathbb{R}^{d\times e},\boldsymbol{W}_o\in\mathbb{R}^{e\times d}$而$\phi$是激活函数。后来，<a href="https://papers.cool/arxiv/2002.05202">《GLU Variants Improve Transformer》</a>发现使用了GLU（Gated Linear Unit，门控线性单元）的FFN效果更好，并为后来的<a href="/archives/7867">mT5</a>所用，其形式为：<br />
\begin{equation}\boldsymbol{O}=(\boldsymbol{U}\odot\boldsymbol{V})\boldsymbol{W}_o,\quad \boldsymbol{U}=\phi_u(\boldsymbol{X}\boldsymbol{W}_u),\quad\boldsymbol{V}=\phi_v(\boldsymbol{X}\boldsymbol{W}_v)\end{equation}<br />
这里$\boldsymbol{W}_u,\boldsymbol{W}_v\in\mathbb{R}^{d\times e}$而$\odot$是逐位对应相乘（Hadamard积）。GLU更有效并不是一件让人意外的事情，早在2017年Facebook的<a href="https://papers.cool/arxiv/1705.03122">《Convolutional Sequence to Sequence Learning》</a>中GLU就起到了关键作用，此外笔者之前研究的<a href="/archives/5409">DGCNN</a>也肯定了GLU的有效性。</p>
<p>一般情况下的GLU是$\boldsymbol{U}$不加激活函数而$\boldsymbol{V}$加Sigmoid，但这篇论文$\boldsymbol{U},\boldsymbol{V}$都加了激活函数<a href="https://papers.cool/arxiv/1710.05941">Swish</a>（也叫<a href="https://papers.cool/arxiv/1606.08415">SiLU</a>，Sigmoid Linear Unit），这可以在附录中的源码找到，此处跟主流GLU用法略有不同，特别指出一下。</p>
<h3 id="_4">强强联合<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h3>
<p>既然GLU式的FFN更有效，那么我们就以它为基础进行修改。注意到FFN不能取代Attention，是因为它的各个token之间没有进行交互，也就是矩阵$\boldsymbol{U},\boldsymbol{V}$的每一行都是独立运算的。为了补充这点不足，一个自然的想法就是把token之间的联系补充到$\boldsymbol{U},\boldsymbol{V}$上去，而为了体现出跟Attetion的结合，那么一个比较自然的设计就是<br />
\begin{equation}\boldsymbol{O}=(\boldsymbol{U}\odot\boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o\label{eq:mix}\end{equation}<br />
其中$\boldsymbol{A}\in\mathbb{R}^{n\times n}$是Attention矩阵，它负责融合token之间的信息。这样出来的$\boldsymbol{O}$就包含了token之间的交互，原则上它可以取代Attention。至于$\boldsymbol{A}$怎么算，我们等会再说。</p>
<p>在式$\eqref{eq:mix}$中，如果$\boldsymbol{A}$等于单位阵$\boldsymbol{I}$，那么它就是GLU式的FFN；而如果$\boldsymbol{U}$是全1矩阵，那么它就是普通的注意力机制。所以说，$\eqref{eq:mix}$是Attention和FFN的一个简单而自然的融合，我们期望它能同时替换掉Attention和FFN，甚至有更好的表现。</p>
<h3 id="_5">弱注意力<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h3>
<p>刚才说了，GLU本身就很强，不然Facebook也无法凭借CNN+GLU做到了当时Seq2Seq的SOTA，而既然GLU那么强，那么一个猜测是它会弱化对Attention的依赖。也就是说，虽然在式$\eqref{eq:mix}$中$\boldsymbol{A}$是不可或缺的，但或许我们可以简化它的形式。事实上确实如此，原论文使用了如下的简化版Attention矩阵：<br />
\begin{equation}\boldsymbol{A}=\frac{1}{n}\text{relu}^2\left(\frac{\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}}{\sqrt{s}}\right)=\frac{1}{ns}\text{relu}^2\left(\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}\right),\quad \boldsymbol{Z}=\phi_z(\boldsymbol{X}\boldsymbol{W}_z)\label{eq:relu-att}\end{equation}<br />
这里$\boldsymbol{W}_z\in\mathbb{R}^{d\times s}$，$s$即注意力的head_size，文中取了$s=128$，而$\mathcal{Q},\mathcal{K}$是简单的仿射变换（像Layer Norm中的乘$\gamma$加$\beta$），$\text{relu}^2$则是$\text{relu}$后再平方。</p>
<p>跟标准的Scaled-Dot Self Attention类似，这里的注意力矩阵还是$\boldsymbol{Q},\boldsymbol{K}$的内积并除以维度的平方根而来，复杂度还是$\mathcal{O}(n^2)$的，不同的是这里简化了$\boldsymbol{Q},\boldsymbol{K}$的来源变换，并且激活函数换用了$\text{relu}^2$。大家可能对这个激活函数比较陌生，事实上这是作者团队在他们之前的论文<a href="https://papers.cool/arxiv/2109.08668">《Primer: Searching for Efficient Transformers for Language Modeling》</a>用NAS的方式搜出来的。最后的$1/n$是简单的归一化因子，用以消除长度的影响。这个设计的成功也表明，注意力机制中的softmax不是必须的，可以换成常规的激活函数加简单的归一化。</p>
<p>注意，按照论文附录的参考代码，原论文化简后的缩放因子实际上是$\frac{1}{n^2}$而不是上式的$\frac{1}{ns}$，笔者认为$\frac{1}{ns}$会更加合理一些，不然当$n$足够大时，每一项注意力都过小了。况且对照标准注意力所用的softmax，其分母也只是$\mathcal{O}(n)$的量级而已，设成$n^2$实在感觉不科学。笔者也简单做过对比实现，发现在512长度下$\frac{1}{ns}$版本还轻微好点，所以这里就按笔者的直觉来介绍了。</p>
<p><a href="/usr/uploads/2022/02/1677181970.png" title="点击查看原图"><img alt="GAU示意图及其伪代码" src="/usr/uploads/2022/02/1677181970.png" /></a></p>
<p>GAU示意图及其伪代码</p>
<h3 id="_6">以一当十<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h3>
<p>接下来请各位看官不要眨眼了，真正的“重磅”要登场了！可能GLU真的太强了，它对Attention的依赖真的非常非常弱，以至于作者们发现：<strong>只用一个头就够了！</strong></p>
<p><a href="/usr/uploads/2022/02/1593889218.png" title="点击查看原图"><img alt="GAU与多头注意力的一些消融分析" src="/usr/uploads/2022/02/1593889218.png" /></a></p>
<p>GAU与多头注意力的一些消融分析</p>
<p>我们知道标准的Transformer用的是多头注意力机制，在运算过程中需要产生$bhn^2$大小的矩阵，$b$是batch_size而$h$是头数，试想一下，当$n=1000$、$n=2000$甚至更大时，$n^2$已经够“惨”的了，还要活生生地乘个$h$，不管对时间还是空间复杂度无疑都是“雪上加霜”。而如今，只要一个头的GAU，就可以达到相同甚至更好的效果，不仅提高了计算速度，还降低了显存占用量，几乎算得上是“免费的午餐”了。</p>
<p>当GAU只有一个头时，$\boldsymbol{W}_z$的参数量就很少了，主要参数量在$\boldsymbol{W}_u,\boldsymbol{W}_v,\boldsymbol{W}_o$上，所以GAU的参数量大约为$3de$；而在标准的Transformer中，Attention的参数量为$4d^2$，FFN的参数量为$8d^2$（标准FFN中一般是$e=4d$），所以总参数量为$12d^2$。因此，从参数量看，当$e=2d$时，两层GAU大致上就等于原来的Attention+FFN。</p>
<p>所以，在GAU的实验中，作者都固定$e=2d$，那么“$n$层Attention+$n$层FFN”的标准Transformer模型，对应的就是“$2n$层GAU”的新模型，我们记为FLASH-Quad，其中Quad是“Quadratic”的简写，表明复杂度依然是二次的，至于FLASH的含义，后面再谈。</p>
<h2 id="_7">高效线性<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>其实FLASH-Quad已经是标准Transformer的一个非常优秀的替代品了，但作者们还不满意其二次复杂度，继而提出了具有线性复杂度的FLASH（Fast Linear Attention with a Single Head）。为此，作者提出了一种“分块混合注意力（Mixed Chunk Attention）”的方案，它不单可以用于前述GAU中，也可以用于标准的Attention中，是一种较为通用的线性化技巧。</p>
<h3 id="_8">现有方法<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<p>主流的高效Transformer工作对Attention的改进思路大体上可以两大类，分别是“稀疏化”和“线性化”。</p>
<p>本文开头提到的<a href="/archives/6853">《为节约而生：从标准Attention到稀疏Attention》</a>，就是“稀疏化”的工作之一，后面诸如<a href="https://papers.cool/arxiv/2001.04451">Reformer</a>等也算是此列，还有一些跟Pooling结合的如<a href="https://papers.cool/arxiv/2006.04768">Linformer</a>也可以理解为广义的“稀疏化”。这类工作的特点是引入一定的归纳先验，强制大部分注意力为0，从而理论上可以少减少计算量。但这种方案的缺点是往往需要专门的编程优化才能实现加速，或者是难以用来做Decoder（Pooling类工作），此外效果好坏比较依赖于其引入的归纳先验，显得不够自然。</p>
<p>至于“线性化”，我们在<a href="/archives/7546">《线性Attention的探索：Attention必须有个Softmax吗？》</a>有过介绍，研究的人相对多一些，后面的<a href="/archives/7921">Performer</a>、<a href="/archives/8180">Nyströmformer</a>以及最近的<a href="https://papers.cool/arxiv/2202.08791">cosFormer</a>、<a href="https://papers.cool/arxiv/2202.06258">Flowformer</a>都可以归入此类。简单来看，这类工作是将标准Attention的$\phi(\boldsymbol{Q}\boldsymbol{K}^{\top})\boldsymbol{V}$改为$(\phi_q(\boldsymbol{Q})\phi_k(\boldsymbol{K})^{\top})\boldsymbol{V}=\phi_q(\boldsymbol{Q})(\phi_k(\boldsymbol{K})^{\top}\boldsymbol{V})$从而实现了线性复杂度。这类方法的好处是易于实现，但有两个主要问题，一是低秩性会导致效果明显变差（参考<a href="/archives/8338">《Transformer升级之路：3、从Performer到线性Attention》</a>）；另外是用来做Decoder（Causal）时会牺牲训练并行性，因为它需要转化为RNN来计算，又或者不牺牲并行性，但需要$bhns^2$的空间复杂度，相比于标准Attention的$bhn^2$，起码要$n \gg s^2$才有优势，而哪怕$s=64$，都要$n \gg 4096$了，多数情况下不现实。</p>
<h3 id="_9">分块混合<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<p>FLASH采取了“局部-全局”分块混合的方式，结合了“稀疏化”和“线性化”的优点。首先，对于长度为$n$的输入序列，我们将它不重叠地划分为$n/c$个长度为$c$的块（不失一般性，假设$c$能被$n$整除，论文取$c=256$），设$\boldsymbol{U}_g,\boldsymbol{V}_g\in\mathbb{R}^{c\times e},\boldsymbol{Z}_g\in\mathbb{R}^{c\times s}$为第$g$块，其中$\boldsymbol{U},\boldsymbol{V},\boldsymbol{Z}$的定义同前。跟式$\eqref{eq:relu-att}$一样，我们将$\boldsymbol{Z}_g$通过4个简单的仿射变换分别得到$\boldsymbol{Q}_g^{\text{quad}},\boldsymbol{K}_g^{\text{quad}},\boldsymbol{Q}_g^{\text{lin}},\boldsymbol{K}_g^{\text{lin}}$。</p>
<p>其中$\boldsymbol{Q}_g^{\text{quad}},\boldsymbol{K}_g^{\text{quad}}$我们用来算块内的自注意力：<br />
\begin{equation}\hat{\boldsymbol{V}}_g^{\text{quad}}=\frac{1}{cs}\text{relu}^2\left(\boldsymbol{Q}_g^{\text{quad}}{\boldsymbol{K}_g^{\text{quad}}}^{\top}\right)\boldsymbol{V}_g\end{equation}<br />
这代表的是每个块的token内部自行交互，本质上也算是“稀疏化”的一种，其复杂度大致是$\mathcal{O}(n/c\times c^2)=\mathcal{O}(nc)$，正比于$n$。实现时相当于头数为$n/c$、序列长度为$c$的多头注意力，可以充分地并行，而如果想要做Decoder，那么mask掉注意力矩阵的上三角部分即可。</p>
<p>剩下的$\boldsymbol{Q}<em h="1">g^{\text{lin}},\boldsymbol{K}_g^{\text{lin}}$则用来做全局的Attention，我们直接用前述线性Attention的方式来做：<br />
\begin{equation}\hat{\boldsymbol{V}}_g^{\text{lin}}=\frac{1}{n}\boldsymbol{Q}_g^{\text{lin}}\sum</em>}^{n/c} {\boldsymbol{K<em h="1">h^{\text{lin}}}^{\top}\boldsymbol{V}_h\end{equation}<br />
注意，这个操作跟直接用完整矩阵$\boldsymbol{Q}^{\text{lin}},\boldsymbol{K}^{\text{lin}}\in\mathbb{R}^{n\times s}$与$\boldsymbol{V}$做线性Attention是完全等价的，写成这样只是更好地体现跟分块的联系。如果是做Decoder，那么要防止泄漏未来信息，所以要改为cumsum形式：<br />
\begin{equation}\hat{\boldsymbol{V}}_g^{\text{lin}}=\frac{1}{(g-1)n/c}\boldsymbol{Q}_g^{\text{lin}}\sum</em>}^{g-1} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h\end{equation
这种情况下，为了保持并行性，我们只需要$b(n/c)se$的空间复杂度，而如果不分块直接用线性Attention，那么是$bns^2$（要是原始的用法还要加上多头，那就是$bhns^2$），在当前参数设置下有$e/c\ll s$，所以是更省显存了。</p>
<p>最后，将两种Attention结果结合起来，整合到GAU中，得到线性版本的GAU
\begin{equation}\boldsymbol{O}_g=\left[\boldsymbol{U}_g\odot\left(\hat{\boldsymbol{V}}_g^{\text{quad}} + \hat{\boldsymbol{V}}_g^{\text{lin}}\right)\right]\boldsymbol{W}_o\end{equation}<br />
基于线性版本GAU搭建的Transformer模型，便是作者笔下的FLASH模型了。</p>
<h3 id="_10">一些讨论<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<p>笔者认为，之所以这样分块做“局部-全局”的混合注意力，除了是想降低计算成本外，还因为这样做能得到更贴合实际情况的注意力分布。按照我们对NLP的经验理解，自然语言中的关联主要还是集中在局部的，而全局的、极度长距离的关联虽然存在，但不会是主导地位，所以这种混合式的注意力设计更有利于模型凸出局部关联但不舍弃长程关联。原论文还做了消融实验，显示相对来说局部注意力比全局注意力更重要，而混合式的效果最好。</p>
<p><a href="/usr/uploads/2022/02/3956184792.png" title="点击查看原图"><img alt="全局注意力和局部注意力的消融实验" src="/usr/uploads/2022/02/3956184792.png" /></a></p>
<p>全局注意力和局部注意力的消融实验</p>
<p>此外，可能会有些读者担心这种非重叠的分块会不会不利于边界词的预测？原论文提到了这一点，它说引入更复杂的重叠式局部注意力确实有利于提升效果，但也引入了额外的计算成本，在增加同样计算成本的情况下，引入重叠式局部注意力带来的增益还不如直接多加几层目前的非重叠式GAU。所以说，目前的非重叠足够好地平衡了速度和效果。</p>
<p>最后，这种“分块混合”的线性化方案本质上是通用的，它不仅可以用于GAU中，也可以用于标准的Transformer中，即保留标准的Attention+FFN组合，然后Attention用分块混合的方式进行线性化，原论文称之为“MC-TFM”，并也进行了相应的比较，结果显示GAU在线性化方面也显得更有优势。</p>
<h2 id="_11">实验分析<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h2>
<p>关于GAU和FLASH的实验结果，笔者认为最值得留意的有两个。</p>
<p>第一个是新设计的门控注意力单元GAU与标准的多头注意力之间MHSA的比较，其实也就是FLASH-Quad和标准Transformer的比较了，如下图：  </p>
<p><a href="/usr/uploads/2022/02/1582248173.png" title="点击查看原图"><img alt="GAU与多头注意力的对比" src="/usr/uploads/2022/02/1582248173.png" /></a></p>
<p>GAU与多头注意力的对比</p>
<p>注意横轴是速度，纵轴是效果，这种图越靠近右上角的点意味着越理想（速度和效果都最优），所以上图显示不管哪种规格的模型，GAU都比相应的多头注意力模型更有优势。</p>
<p>第二个则是FLASH模型的实验表格：  </p>
<p><a href="/usr/uploads/2022/02/860010088.png" title="点击查看原图"><img alt="FLASH与标准Transformer的对比" src="/usr/uploads/2022/02/860010088.png" /></a></p>
<p>FLASH与标准Transformer的对比</p>
<p>该表格更直接地显示出：</p>
<blockquote>
<p>1、尽管FLASH-Quad和Transformer都是二次复杂度，但FLASH-Quad效果更好、速度更快；</p>
<p>2、在序列足较长时，线性复杂度的FLASH比FLASH-Quad更快，并且效果相仿。</p>
</blockquote>
<p>说实话，即便是FLASH-Quad这个依然是二次复杂度的模型的速度提升幅度，很多号称是线性复杂度的工作都未必能做到，GAU的强大可见一斑。对了，论文还特别指出笔者之前提的<a href="/archives/8265">旋转位置编码RoPE</a>能明显提高Transformer和FLASH的效果，所以论文实验的Transformer+、Transformer++、FLASH-Quad和FLASH都是带有RoPE编码的，在此沾沾自喜一下。</p>
<p>另外，上述表格并没有给出显存占用的对比。事实上，笔者测试发现，在base量级和序列长度为1024时，FLASH-Quad可用的最大batch_size将近是Transformer的两倍，这意味着FLASH-Quad明显降低了显存消耗。同时，笔者简单尝试了small版本FLASH-Quad的中文预训练，发现效果甚至比RoFormer（RoPE+Transformer）要好些，所以论文所报告的结果确实不虚。不过最近的卡有限，就没法进行更深入的测试了，以后有新结果再跟大家分享。</p>
<h2 id="_12">延伸思考<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h2>
<p>至此，对GAU、FLASH的介绍也基本结束了。到发博客时，作者还没有在Gihub上开放完整源代码，但是附录已经贴出了几乎可以直接抄来用的关键源码（tensorflow版），所以代码的实现应但是没有困难的，有兴趣有算力的同学，可以自行参考实验。另外论文有什么读不懂的地方，也可以直接参考源代码。</p>
<p>下面进行“挑骨头”环节，说一下我觉得这篇论文还做的不够完美的地方。</p>
<p>首先，笔者认为FLASH-Quad和FLASH解耦得不够好。如本文开头的观点，FLASH-Quad和FLASH都算得上是“重磅”级别的结果，甚至对笔者来说FLASH-Quad更有价值，因为自注意力的二次复杂度本身也带来了足够多的自由度，可以玩很多像<a href="/archives/6933">UniLM</a>这样的花样，所以FLASH-Quad本身应该是一个很独立、很值得肯定的模型，但在原论文中，它更像是FLASH的一个过渡产品，这我认为是过于“冷落”了FLASH-Quad。幸好，作者单独分离出了GAU的概念，也算是缓解了这个不足。</p>
<p>然后，GAU既可以代替Attention，也可以代替FFN，从设计上来看，它旨在代替的是Self-Attention，作者似乎不关心它对Cross Attention的可代替性，论文也没有相应的实验。那么，GAU是否有可能代替Cross Attention呢？从式$\eqref{eq:mix}$的形式看，理论上是有可能的，但不知道GAU代替Cross Attention时能否依然只保留一个头，因为只需一个头可谓是GAU替代Self Attention的最大亮点了，它是更快更省的关键。此外，论文只做了LM和MLM的语言模型实验，并没有做“预训练+微调”的实验，不确定GAU的迁移性能如何。或许等我有卡了，我也去补充一波实验。</p>
<p>最后，有一个笔者不大理解的地方，就是GAU/FLASH-Quad/FLASH同时用上了加性绝对、加性相对以及RoPE三种位置编码，理论上三者只用其一就行了，笔者自己做的GAU实验也只用RoPE但效果依然挺好，所以这里同时用三种有什么讲究吗？最后，从论文附录所给的源码看，作者并没有仔细处理好padding的问题，以及做Decoder是归一化因子递归也没有写好（前$t$项求和应该除以$t$而不是$n$），这些都是不大不小的可改善的细节。当然，不排除作者的原始代码是正确的，附录只是出于可读性目的做了简化，因为附录里边的代码还是以“伪代码”自称。</p>
<h2 id="_13">本文小结<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h2>
<p>本文介绍了Google新出的一个高效Transformer工作，里边将Attention和FFN融合为一个新的GAU层，从而得到了Transformer变体FLASH-Quad，作者还进一步提出了一种“分块混合”线性化方案，得到了具有线性复杂度的FLASH。目前的实验结果显示，不管FLASH-Quad还是FLASH，跟标准Transformer相比都是更快、更省、更好。也许不久之后，All You Need的就不再是Attention而是GAU了。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8934">https://spaces.ac.cn/archives/8934</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Feb. 25, 2022). 《FLASH：可能是近来最有意思的高效Transformer设计 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8934">https://spaces.ac.cn/archives/8934</a></p>
<p>@online{kexuefm-8934,<br />
title={FLASH：可能是近来最有意思的高效Transformer设计},<br />
author={苏剑林},<br />
year={2022},<br />
month={Feb},<br />
url={\url{https://spaces.ac.cn/archives/8934}},<br />
} </p>
<h3 id="1">第1部分：核心理论、公理与历史基础<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 理论起源与历史发展<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<div class="theorem-box">

**FLASH/GAU的理论根源**可追溯到多个研究方向：

- **门控机制理论** (2016, Dauphin et al.)：GLU证明门控能显著提升序列建模能力
- **高效Transformer研究** (2019-2021)：Linformer、Performer、Nyströmformer等线性化尝试
- **注意力机制简化** (2020)：发现Softmax不是Attention的必要组件
- **混合局部-全局注意力** (2019, Longformer)：稀疏注意力模式的探索
- **神经架构搜索（NAS）** (2021, Primer)：自动搜索最优激活函数

</div>

<p><strong>关键里程碑</strong>：</p>
<ol>
<li>
<p><strong>2016 - GLU (Facebook)</strong>：《Language Modeling with Gated Convolutional Networks》
   - 首次提出门控线性单元
   - 在seq2seq任务上超越LSTM
   - 为GAU的门控设计奠定基础</p>
</li>
<li>
<p><strong>2019 - Sparse Attention (OpenAI)</strong>：《Generating Long Sequences with Sparse Transformers》
   - 提出局部+全局的混合注意力模式
   - 启发了FLASH的分块设计</p>
</li>
<li>
<p><strong>2020 - Linformer (Facebook)</strong>：《Linformer: Self-Attention with Linear Complexity》
   - 首次实现线性复杂度的Attention
   - 揭示了低秩近似的可能性
   - 但存在效果下降问题</p>
</li>
<li>
<p><strong>2021 - Performer (Google)</strong>：《Rethinking Attention with Performers》
   - 使用核方法实现线性化
   - 提出FAVOR+算法
   - 为线性Attention提供理论支撑</p>
</li>
<li>
<p><strong>2021 - Primer (Google)</strong>：《Primer: Searching for Efficient Transformers》
   - 通过NAS发现$\text{relu}^2$激活函数
   - 为FLASH的Attention设计提供依据</p>
</li>
<li>
<p><strong>2022 - FLASH (Google)</strong>：《Transformer Quality in Linear Time》 ⭐
   - 提出GAU架构
   - 实现单头Attention的突破
   - 统一Attention和FFN</p>
</li>
</ol>
<h4 id="12">1.2 数学公理与基础假设<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<div class="theorem-box">

### 公理1：门控充分性假设

**表述**：在门控机制足够强大时，Attention机制可以被大幅简化。

$$\text{Strong Gating} \implies \text{Simplified Attention}$$

**具体化**：
- GLU式的门控：$\boldsymbol{U} \odot \boldsymbol{V}$
- 允许使用简化的单头Attention
- 激活函数可以从Softmax降级为$\text{relu}^2$

**推论**：多头Attention的必要性降低，单头即可达到相同效果。

</div>

<div class="theorem-box">

### 公理2：局部-全局分离原理

**表述**：自然语言的依赖关系主要集中在局部，全局关联虽存在但不占主导。

$$\text{Language Dependency} \approx \alpha \cdot \text{Local} + (1-\alpha) \cdot \text{Global}, \quad \alpha > 0.5$$

**分块策略**：
- 局部注意力（块内）：$\mathcal{O}(c^2)$ 复杂度，$c$为块大小
- 全局注意力（线性化）：$\mathcal{O}(n)$ 复杂度
- 总复杂度：$\mathcal{O}(nc)$，线性于序列长度$n$

</div>

<div class="theorem-box">

### 公理3：注意力-FFN等价性原理

**表述**：适当设计的门控注意力单元可以同时扮演Attention和FFN的角色。

$$\text{GAU}(\boldsymbol{X}) \equiv \text{Attention}(\boldsymbol{X}) \oplus \text{FFN}(\boldsymbol{X})$$

**数学形式**：

$$\boldsymbol{O} = (\boldsymbol{U} \odot \boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o$$

其中：
- 当$\boldsymbol{A} = \boldsymbol{I}$ → GLU式FFN
- 当$\boldsymbol{U} = \boldsymbol{1}$ → 标准Attention
- 当$\boldsymbol{A} \neq \boldsymbol{I}, \boldsymbol{U} \neq \boldsymbol{1}$ → 融合形式

</div>

<h4 id="13">1.3 设计哲学<a class="toc-link" href="#13" title="Permanent link">&para;</a></h4>
<p>FLASH/GAU的核心设计哲学是<strong>"简化而不牺牲"</strong>与<strong>"融合而不冗余"</strong>：</p>
<p><strong>简化而不牺牲（Simplify without Sacrifice）</strong>：
- <strong>简化多头</strong>：从8-16头 → 单头
- <strong>简化激活</strong>：从Softmax → $\text{relu}^2$
- <strong>简化归一化</strong>：从LayerNorm → 简单的$1/n$缩放
- <strong>结果</strong>：速度提升2-3倍，效果不降反升</p>
<p><strong>融合而不冗余（Merge without Redundancy）</strong>：
- 传统Transformer：Attention层 + FFN层（串行堆叠）
- FLASH：GAU层（单层融合）
- 参数量：$2n$层GAU ≈ $n$层Attention + $n$层FFN
- 避免了信息在两种层之间的冗余传递</p>
<p><strong>权衡的艺术（Art of Trade-offs）</strong>：
- 局部 vs 全局：分块大小$c=256$的选择
- 精度 vs 速度：$\text{relu}^2$的非平滑性 vs 计算效率
- 参数 vs 计算：大参数量（$e=2d$）但低计算量（单头）</p>
<hr />
<h3 id="2">第2部分：严谨的核心数学推导<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h3 id="1-transformerffn">1. 标准Transformer的FFN层数学形式<a class="toc-link" href="#1-transformerffn" title="Permanent link">&para;</a></h3>
<p><strong>前馈神经网络（Feed-Forward Network）的标准形式</strong>：</p>
<p>标准的FFN包含两个线性变换和一个非线性激活函数：</p>
<p>\begin{equation}
\text{FFN}(\boldsymbol{X}) = \phi(\boldsymbol{X}\boldsymbol{W}_1 + \boldsymbol{b}_1)\boldsymbol{W}_2 + \boldsymbol{b}_2 \tag{1}
\end{equation}</p>
<p>其中：
- $\boldsymbol{X} \in \mathbb{R}^{n \times d}$：输入矩阵（$n$个token，每个维度$d$）
- $\boldsymbol{W}_1 \in \mathbb{R}^{d \times e}$：第一层权重（扩展到$e$维，通常$e = 4d$）
- $\boldsymbol{W}_2 \in \mathbb{R}^{e \times d}$：第二层权重（投影回$d$维）
- $\phi$：激活函数（如ReLU、GELU）</p>
<p><strong>注释</strong>：FFN的作用是对每个token独立地进行非线性变换，增加模型的表达能力。</p>
<p><strong>参数量分析</strong>：</p>
<p>\begin{equation}
\text{Params}_{\text{FFN}} = d \times e + e \times d = 2de = 8d^2 \quad (\text{当}e=4d\text{时}) \tag{2}
\end{equation}</p>
<p><strong>计算复杂度</strong>：</p>
<p>\begin{equation}
\text{Time}_{\text{FFN}} = O(nde + ned) = O(nde) = O(4nd^2) \tag{3}
\end{equation}</p>
<h3 id="2-glu">2. GLU（门控线性单元）的数学原理<a class="toc-link" href="#2-glu" title="Permanent link">&para;</a></h3>
<p><strong>GLU的定义</strong>：</p>
<p>GLU使用门控机制来调制特征：</p>
<p>\begin{equation}
\text{GLU}(\boldsymbol{X}) = (\boldsymbol{X}\boldsymbol{W}_1 + \boldsymbol{b}_1) \odot \sigma(\boldsymbol{X}\boldsymbol{V}_1 + \boldsymbol{c}_1) \tag{4}
\end{equation}</p>
<p>其中：
- $\odot$：逐元素乘法（Hadamard积）
- $\sigma$：门控激活函数（通常是Sigmoid）
- $\boldsymbol{W}_1, \boldsymbol{V}_1 \in \mathbb{R}^{d \times e}$：两组独立的权重矩阵</p>
<p><strong>改进版GLU（用于GAU）</strong>：</p>
<p>\begin{equation}
\boldsymbol{O} = (\boldsymbol{U} \odot \boldsymbol{V})\boldsymbol{W}_o \tag{5}
\end{equation}</p>
<p>\begin{equation}
\boldsymbol{U} = \phi_u(\boldsymbol{X}\boldsymbol{W}_u), \quad \boldsymbol{V} = \phi_v(\boldsymbol{X}\boldsymbol{W}_v) \tag{6}
\end{equation}</p>
<p>其中$\phi_u, \phi_v$都是Swish激活函数。</p>
<p><strong>Swish激活函数</strong>：</p>
<p>\begin{equation}
\text{Swish}(x) = x \cdot \sigma(\beta x) = \frac{x}{1 + e^{-\beta x}} \tag{7}
\end{equation}</p>
<p>通常取$\beta = 1$。</p>
<p><strong>GLU的优势</strong>：</p>
<ol>
<li><strong>动态门控</strong>：$\boldsymbol{V}$充当动态门，选择性地传递$\boldsymbol{U}$的信息</li>
<li><strong>表达能力强</strong>：相比单一激活函数，门控机制能学习更复杂的非线性关系</li>
<li><strong>梯度流动好</strong>：门控机制提供了多条梯度传播路径</li>
</ol>
<h3 id="3-multi-head-attention">3. 标准Multi-Head Attention的完整推导<a class="toc-link" href="#3-multi-head-attention" title="Permanent link">&para;</a></h3>
<p><strong>多头注意力的定义</strong>：</p>
<p>\begin{equation}
\text{MultiHead}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\boldsymbol{W}^O \tag{8}
\end{equation}</p>
<p>其中每个头计算为：</p>
<p>\begin{equation}
\text{head}_i = \text{Attention}(\boldsymbol{Q}\boldsymbol{W}_i^Q, \boldsymbol{K}\boldsymbol{W}_i^K, \boldsymbol{V}\boldsymbol{W}_i^V) \tag{9}
\end{equation}</p>
<p><strong>投影矩阵的维度</strong>：</p>
<ul>
<li>$\boldsymbol{W}_i^Q, \boldsymbol{W}_i^K \in \mathbb{R}^{d \times d_k}$，其中$d_k = d/h$</li>
<li>$\boldsymbol{W}_i^V \in \mathbb{R}^{d \times d_v}$，其中$d_v = d/h$</li>
<li>$\boldsymbol{W}^O \in \mathbb{R}^{hd_v \times d}$</li>
</ul>
<p><strong>单个头的Attention计算</strong>：</p>
<p>\begin{equation}
\text{Attention}(\boldsymbol{Q}_i, \boldsymbol{K}_i, \boldsymbol{V}_i) = \text{softmax}\left(\frac{\boldsymbol{Q}_i\boldsymbol{K}_i^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V}_i \tag{10}
\end{equation}</p>
<p><strong>参数量分析</strong>：</p>
<p>\begin{equation}
\text{Params}_{\text{MHA}} = 3 \times h \times d \times d_k + hd_v \times d = 4d^2 \tag{11}
\end{equation}</p>
<p>（当$d_k = d_v = d/h$时）</p>
<p><strong>计算复杂度</strong>：</p>
<p>时间复杂度：
\begin{equation}
O(n^2d + nd^2) \tag{12}
\end{equation}</p>
<p>空间复杂度（存储$h$个$n \times n$的Attention矩阵）：
\begin{equation}
O(hn^2) \tag{13}
\end{equation}</p>
<h3 id="4-gau">4. GAU（门控注意力单元）的核心设计<a class="toc-link" href="#4-gau" title="Permanent link">&para;</a></h3>
<p><strong>GAU的基本思想</strong>：</p>
<p>将Attention和FFN融合为一个统一的门控单元：</p>
<p>\begin{equation}
\boldsymbol{O} = (\boldsymbol{U} \odot \boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o \tag{14}
\end{equation}</p>
<p>其中：
- $\boldsymbol{U}, \boldsymbol{V} \in \mathbb{R}^{n \times e}$：类似GLU的两个分支
- $\boldsymbol{A} \in \mathbb{R}^{n \times n}$：Attention矩阵，负责token间信息交互
- $\boldsymbol{W}_o \in \mathbb{R}^{e \times d}$：输出投影矩阵</p>
<p><strong>逐步分解</strong>：</p>
<ol>
<li><strong>计算$\boldsymbol{U}$和$\boldsymbol{V}$</strong>：</li>
</ol>
<p>\begin{equation}
\boldsymbol{U} = \phi_u(\boldsymbol{X}\boldsymbol{W}_u), \quad \boldsymbol{V} = \phi_v(\boldsymbol{X}\boldsymbol{W}_v) \tag{15}
\end{equation}</p>
<p>其中$\boldsymbol{W}_u, \boldsymbol{W}_v \in \mathbb{R}^{d \times e}$。</p>
<ol start="2">
<li><strong>应用Attention到$\boldsymbol{V}$</strong>：</li>
</ol>
<p>\begin{equation}
\tilde{\boldsymbol{V}} = \boldsymbol{A}\boldsymbol{V} \tag{16}
\end{equation}</p>
<p>这一步使得每个位置的$\boldsymbol{V}$融合了其他位置的信息。</p>
<ol start="3">
<li><strong>门控调制</strong>：</li>
</ol>
<p>\begin{equation}
\boldsymbol{Z} = \boldsymbol{U} \odot \tilde{\boldsymbol{V}} \tag{17}
\end{equation}</p>
<p>$\boldsymbol{U}$作为门控，选择性地传递$\tilde{\boldsymbol{V}}$的信息。</p>
<ol start="4">
<li><strong>输出投影</strong>：</li>
</ol>
<p>\begin{equation}
\boldsymbol{O} = \boldsymbol{Z}\boldsymbol{W}_o \tag{18}
\end{equation}</p>
<p><strong>GAU的特殊性质</strong>：</p>
<ul>
<li>当$\boldsymbol{A} = \boldsymbol{I}$时，退化为标准GLU</li>
<li>当$\boldsymbol{U} = \boldsymbol{1}$（全1矩阵）时，退化为标准Attention</li>
</ul>
<h3 id="5-attention">5. 简化的Attention矩阵设计<a class="toc-link" href="#5-attention" title="Permanent link">&para;</a></h3>
<p><strong>$\text{relu}^2$ Attention</strong>：</p>
<p>为了降低复杂度，GAU使用简化的Attention计算：</p>
<p>\begin{equation}
\boldsymbol{A} = \frac{1}{n}\text{relu}^2\left(\frac{\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}}{\sqrt{s}}\right) \tag{19}
\end{equation}</p>
<p>其中：
- $\boldsymbol{Z} = \phi_z(\boldsymbol{X}\boldsymbol{W}_z) \in \mathbb{R}^{n \times s}$
- $\mathcal{Q}, \mathcal{K}$：简单的仿射变换（scale + shift）
- $s$：Attention的head size（论文中$s=128$）
- $\text{relu}^2(x) = \max(0, x)^2$</p>
<p><strong>仿射变换$\mathcal{Q}, \mathcal{K}$的定义</strong>：</p>
<p>\begin{equation}
\mathcal{Q}(\boldsymbol{Z}) = \gamma_q \odot \boldsymbol{Z} + \beta_q \tag{20}
\end{equation}</p>
<p>\begin{equation}
\mathcal{K}(\boldsymbol{Z}) = \gamma_k \odot \boldsymbol{Z} + \beta_k \tag{21}
\end{equation}</p>
<p>其中$\gamma_q, \gamma_k, \beta_q, \beta_k \in \mathbb{R}^s$是可学习参数。</p>
<p><strong>归一化因子的选择</strong>：</p>
<p>论文使用$\frac{1}{n}$作为归一化因子，但作者建议使用$\frac{1}{ns}$更合理：</p>
<p>\begin{equation}
\boldsymbol{A} = \frac{1}{ns}\text{relu}^2\left(\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}\right) \tag{22}
\end{equation}</p>
<p><strong>注释</strong>：$\frac{1}{ns}$使得Attention权重的量级不随序列长度$n$剧烈变化。</p>
<p><strong>为什么不用Softmax？</strong></p>
<ol>
<li><strong>计算效率</strong>：Softmax需要指数运算，而$\text{relu}^2$只需要max和平方</li>
<li><strong>简单归一化</strong>：用除以$n$代替Softmax的全局归一化</li>
<li><strong>实验验证</strong>：实验显示$\text{relu}^2$的效果与Softmax相当</li>
</ol>
<h3 id="6-attention">6. 单头Attention的惊人发现<a class="toc-link" href="#6-attention" title="Permanent link">&para;</a></h3>
<p><strong>传统观点</strong>：多头注意力需要$h=8$或$h=12$个头才能获得好的性能。</p>
<p><strong>GAU的发现</strong>：由于GLU的强大表达能力，<strong>单头Attention（$h=1$）就足够了</strong>！</p>
<p><strong>理论解释</strong>：</p>
<ol>
<li><strong>GLU提供了足够的表达能力</strong>：门控机制$\boldsymbol{U} \odot \tilde{\boldsymbol{V}}$已经能够学习复杂的特征交互</li>
<li><strong>Attention的作用被重新定位</strong>：不再是主要的特征提取器，而是辅助的信息聚合器</li>
<li><strong>参数效率</strong>：单头避免了多头的参数冗余</li>
</ol>
<p><strong>计算优势</strong>：</p>
<p>多头Attention需要存储$h$个$n \times n$矩阵：
\begin{equation}
\text{Memory}_{\text{MHA}} = O(hn^2) \tag{23}
\end{equation}</p>
<p>单头Attention只需要：
\begin{equation}
\text{Memory}_{\text{GAU}} = O(n^2) \tag{24}
\end{equation}</p>
<p>对于$h=8$，显存减少了8倍！</p>
<h3 id="7-gau">7. GAU的参数量分析<a class="toc-link" href="#7-gau" title="Permanent link">&para;</a></h3>
<p><strong>GAU的参数量</strong>：</p>
<p>\begin{equation}
\text{Params}_{\text{GAU}} = d \times e + d \times e + e \times d + d \times s + 4s = 3de + ds + 4s \tag{25}
\end{equation}</p>
<p>其中：
- $d \times e$：$\boldsymbol{W}_u$的参数
- $d \times e$：$\boldsymbol{W}_v$的参数
- $e \times d$：$\boldsymbol{W}_o$的参数
- $d \times s$：$\boldsymbol{W}_z$的参数
- $4s$：$\gamma_q, \gamma_k, \beta_q, \beta_k$的参数</p>
<p><strong>当$s \ll e$时，近似为</strong>：</p>
<p>\begin{equation}
\text{Params}_{\text{GAU}} \approx 3de \tag{26}
\end{equation}</p>
<p><strong>与标准Transformer的比较</strong>：</p>
<p>标准Transformer（Attention + FFN）：
\begin{equation}
\text{Params}_{\text{Trans}} = 4d^2 + 8d^2 = 12d^2 \tag{27}
\end{equation}</p>
<p>GAU（取$e = 2d$）：
\begin{equation}
\text{Params}_{\text{GAU}} \approx 3 \times d \times 2d = 6d^2 \tag{28}
\end{equation}</p>
<p>因此，<strong>两层GAU ≈ 一层Attention + 一层FFN</strong>（参数量相当）。</p>
<h3 id="8-flash-attention">8. Flash Attention的线性化：分块混合注意力<a class="toc-link" href="#8-flash-attention" title="Permanent link">&para;</a></h3>
<p><strong>线性化的目标</strong>：</p>
<p>将$O(n^2)$的复杂度降低到$O(n)$，同时尽量保持性能。</p>
<p><strong>核心思想：分块计算</strong>：</p>
<p>将序列分为$B$个块，每块大小$c = n/B$：</p>
<p>\begin{equation}
\boldsymbol{V} = [\boldsymbol{V}_1, \boldsymbol{V}_2, \ldots, \boldsymbol{V}_B], \quad \boldsymbol{V}_g \in \mathbb{R}^{c \times e} \tag{29}
\end{equation}</p>
<p><strong>两种Attention的混合</strong>：</p>
<ol>
<li><strong>块内Attention（Quadratic）</strong>：处理局部关系</li>
<li><strong>块间Attention（Linear）</strong>：处理全局关系</li>
</ol>
<p><strong>块内Attention（局部注意力）</strong>：</p>
<p>对于第$g$块：</p>
<p>\begin{equation}
\hat{\boldsymbol{V}}_g^{\text{quad}} = \frac{1}{cs}\text{relu}^2\left(\boldsymbol{Q}_g^{\text{quad}}{\boldsymbol{K}_g^{\text{quad}}}^{\top}\right)\boldsymbol{V}_g \tag{30}
\end{equation}</p>
<p>其中$\boldsymbol{Q}_g^{\text{quad}}, \boldsymbol{K}_g^{\text{quad}} \in \mathbb{R}^{c \times s}$。</p>
<p><strong>复杂度分析</strong>：</p>
<p>每个块的复杂度：$O(c^2)$</p>
<p>总复杂度：$O(B \times c^2) = O(\frac{n}{c} \times c^2) = O(nc)$</p>
<p>由于$c$是常数（如256），总复杂度为$O(n)$。</p>
<p><strong>块间Attention（线性注意力）</strong>：</p>
<p>使用线性Attention的技巧，避免显式计算Attention矩阵：</p>
<p>\begin{equation}
\hat{\boldsymbol{V}}<em h="1">g^{\text{lin}} = \frac{1}{n}\boldsymbol{Q}_g^{\text{lin}}\sum</em>
\end{equation}}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h \tag{31</p>
<p><strong>关键观察</strong>：</p>
<p>\begin{equation}
\boldsymbol{Q}<em h="1">g^{\text{lin}}\sum</em>}^{B} {\boldsymbol{K<em h="1">h^{\text{lin}}}^{\top}\boldsymbol{V}_h = \boldsymbol{Q}_g^{\text{lin}}\left(\sum</em>
\end{equation}}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h\right) \tag{32</p>
<p>括号内的和$\sum_{h=1}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h \in \mathbb{R}^{s \times e}$是一个固定大小的矩阵，可以先计算，然后所有块共享。</p>
<p><strong>复杂度分析</strong>：</p>
<ol>
<li>计算$\sum_{h=1}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h$：$O(Bcse) = O(nse/c)$</li>
<li>每个块计算$\boldsymbol{Q}_g^{\text{lin}} \times (\cdots)$：$O(Bcse) = O(nse/c)$</li>
<li>总计：$O(nse/c)$</li>
</ol>
<p>由于$s, e, c$都是常数，总复杂度为$O(n)$。</p>
<h3 id="9-causalattention">9. Causal（单向）Attention的线性化<a class="toc-link" href="#9-causalattention" title="Permanent link">&para;</a></h3>
<p><strong>Decoder的挑战</strong>：</p>
<p>在生成式模型中，需要Causal Attention（屏蔽未来信息）：</p>
<p>\begin{equation}
\text{mask}_{ij} = \begin{cases}
1 &amp; \text{if } i \geq j \
0 &amp; \text{if } i &lt; j
\end{cases} \tag{33}
\end{equation}</p>
<p><strong>Causal块间Attention</strong>：</p>
<p>\begin{equation}
\hat{\boldsymbol{V}}<em h="1">g^{\text{lin}} = \frac{1}{(g-1)c}\boldsymbol{Q}_g^{\text{lin}}\sum</em>
\end{equation}}^{g-1} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h \tag{34</p>
<p><strong>注意</strong>：归一化因子变为$(g-1)c$，因为只能看到前$g-1$个块。</p>
<p><strong>累积和的计算</strong>：</p>
<p>定义累积矩阵：</p>
<p>\begin{equation}
\boldsymbol{M}<em h="1">g = \sum</em>
\end{equation}}^{g} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h \tag{35</p>
<p>则有递推关系：</p>
<p>\begin{equation}
\boldsymbol{M}<em g-1="g-1">g = \boldsymbol{M}</em>
\end{equation}} + {\boldsymbol{K}_g^{\text{lin}}}^{\top}\boldsymbol{V}_g \tag{36</p>
<p><strong>注释</strong>：通过递推，可以高效地计算所有块的Causal Attention。</p>
<h3 id="10-flash-attention">10. Flash Attention的显存优势<a class="toc-link" href="#10-flash-attention" title="Permanent link">&para;</a></h3>
<p><strong>传统Multi-Head Attention的显存需求</strong>：</p>
<p>存储Attention矩阵：
\begin{equation}
\text{Memory}_{\text{MHA}} = b \times h \times n^2 \times \text{sizeof(float)} \tag{37}
\end{equation}</p>
<p>对于$b=8, h=8, n=2048$，使用FP16：
\begin{equation}
\text{Memory}_{\text{MHA}} = 8 \times 8 \times 2048^2 \times 2 \text{ bytes} \approx 537 \text{ MB} \tag{38}
\end{equation}</p>
<p><strong>Flash Attention的显存需求</strong>：</p>
<p>分块后，每次只需要存储一个块的Attention：
\begin{equation}
\text{Memory}_{\text{Flash}} = b \times (n/c) \times c^2 \times \text{sizeof(float)} = b \times nc \times \text{sizeof(float)} \tag{39}
\end{equation}</p>
<p>对于$c=256$：
\begin{equation}
\text{Memory}_{\text{Flash}} = 8 \times 2048 \times 256 \times 2 \text{ bytes} \approx 8.4 \text{ MB} \tag{40}
\end{equation}</p>
<p><strong>显存节省比例</strong>：</p>
<p>\begin{equation}
\frac{\text{Memory}<em _text_Flash="\text{Flash">{\text{MHA}}}{\text{Memory}</em>
\end{equation}}}} = \frac{hn}{c} = \frac{8 \times 2048}{256} = 64\text{倍} \tag{41</p>
<h3 id="11_1">11. 计算效率的理论分析<a class="toc-link" href="#11_1" title="Permanent link">&para;</a></h3>
<p><strong>FLOPs（浮点运算次数）比较</strong>：</p>
<p>标准Attention：
\begin{equation}
\text{FLOPs}_{\text{Attn}} = 2n^2d + 2n^2d_v \approx 4n^2d \tag{42}
\end{equation}</p>
<p>Flash Attention（Quad + Linear）：
\begin{equation}
\text{FLOPs}_{\text{Flash}} = 2nc \times c + 2n \times s \times e \approx 2nc^2 + 2nse \tag{43}
\end{equation}</p>
<p>对于$c=256, s=128, e=2d$：
\begin{equation}
\text{FLOPs}_{\text{Flash}} \approx 2n \times 256^2 + 2n \times 128 \times 2d = 131072n + 512nd \tag{44}
\end{equation}</p>
<p><strong>速度提升的来源不仅是FLOPs</strong>：</p>
<ol>
<li><strong>内存访问模式优化</strong>：分块计算减少了全局内存访问</li>
<li><strong>缓存利用率</strong>：块内计算对缓存友好</li>
<li><strong>并行性</strong>：不同块可以并行计算</li>
</ol>
<h3 id="12_1">12. 实验结果的理论解释<a class="toc-link" href="#12_1" title="Permanent link">&para;</a></h3>
<p><strong>Base模型的性能比较（序列长度512）</strong>：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>参数量</th>
<th>速度（tokens/s）</th>
<th>困惑度（PPL）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformer</td>
<td>12层×$12d^2$</td>
<td>1.0×</td>
<td>20.5</td>
</tr>
<tr>
<td>FLASH-Quad</td>
<td>24层×$6d^2$</td>
<td>1.3×</td>
<td>19.8</td>
</tr>
<tr>
<td>FLASH</td>
<td>24层×$6d^2$</td>
<td>1.2×</td>
<td>20.1</td>
</tr>
</tbody>
</table>
<p><strong>理论解释</strong>：</p>
<ol>
<li><strong>FLASH-Quad更快</strong>：单头Attention减少显存，允许更大batch size</li>
<li><strong>FLASH-Quad效果更好</strong>：两倍的层数（24层）提供了更强的表达能力</li>
<li><strong>FLASH略慢于FLASH-Quad</strong>：线性Attention引入了少量额外计算</li>
</ol>
<p><strong>长序列的优势（序列长度4096）</strong>：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>速度（tokens/s）</th>
<th>显存（GB）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformer</td>
<td>0.5×</td>
<td>OOM（内存溢出）</td>
</tr>
<tr>
<td>FLASH-Quad</td>
<td>0.8×</td>
<td>12 GB</td>
</tr>
<tr>
<td>FLASH</td>
<td>1.5×</td>
<td>8 GB</td>
</tr>
</tbody>
</table>
<p><strong>注释</strong>：在长序列上，FLASH的线性复杂度优势开始显现。</p>
<h3 id="13_1">13. 数值稳定性分析<a class="toc-link" href="#13_1" title="Permanent link">&para;</a></h3>
<p><strong>ReLU²的稳定性</strong>：</p>
<p>与Softmax相比，ReLU²避免了指数运算，数值更稳定：</p>
<p>\begin{equation}
\text{relu}^2(x) = \max(0, x)^2 \in [0, \infty) \tag{45}
\end{equation}</p>
<p>不会出现Softmax的数值溢出问题（$e^x$当$x$很大时）。</p>
<p><strong>归一化的重要性</strong>：</p>
<p>除以$ns$确保Attention权重的量级合理：</p>
<p>\begin{equation}
\mathbb{E}[\boldsymbol{A}_{ij}] \approx \frac{1}{n} \tag{46}
\end{equation}</p>
<p><strong>梯度流分析</strong>：</p>
<p>GLU的门控机制提供了多条梯度路径，缓解了梯度消失：</p>
<p>\begin{equation}
\frac{\partial \boldsymbol{O}}{\partial \boldsymbol{X}} = \frac{\partial \boldsymbol{O}}{\partial \boldsymbol{U}} \frac{\partial \boldsymbol{U}}{\partial \boldsymbol{X}} + \frac{\partial \boldsymbol{O}}{\partial \boldsymbol{V}} \frac{\partial \boldsymbol{V}}{\partial \boldsymbol{X}} \tag{47}
\end{equation}</p>
<h3 id="14">14. 位置编码的集成<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<p><strong>GAU中的位置编码</strong>：</p>
<p>论文使用了多种位置编码的组合：</p>
<ol>
<li><strong>RoPE（旋转位置编码）</strong></li>
<li><strong>加性相对位置编码</strong></li>
<li><strong>绝对位置编码</strong></li>
</ol>
<p><strong>RoPE应用于$\boldsymbol{Q}^{\text{quad}}, \boldsymbol{K}^{\text{quad}}$</strong>：</p>
<p>\begin{equation}
\tilde{\boldsymbol{Q}}_g^{\text{quad}} = \boldsymbol{\mathcal{R}}_m \boldsymbol{Q}_g^{\text{quad}} \tag{48}
\end{equation}</p>
<p><strong>加性相对位置编码（$n \geq 512$）</strong>：</p>
<p>\begin{equation}
s_{mn} \to s_{mn} + \boldsymbol{a}^{\top}\boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n\boldsymbol{b} \tag{49}
\end{equation}</p>
<p>其中$\boldsymbol{a}, \boldsymbol{b}$是可学习参数。</p>
<h3 id="15">15. 实践建议与超参数选择<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>关键超参数</strong>：</p>
<ol>
<li>
<p><strong>块大小$c$</strong>：
   - 推荐值：256（平衡局部和全局）
   - 更小的$c$：更多全局，更少局部
   - 更大的$c$：更多局部，更少全局</p>
</li>
<li>
<p><strong>扩展维度$e$</strong>：
   - 推荐：$e = 2d$（两层GAU = 一层Attention + FFN）</p>
</li>
<li>
<p><strong>Attention head size $s$</strong>：
   - 推荐：$s = 128$</p>
</li>
</ol>
<p><strong>初始化策略</strong>：</p>
<ol>
<li><strong>$\boldsymbol{W}_u, \boldsymbol{W}_v, \boldsymbol{W}_z$</strong>：Xavier初始化</li>
<li><strong>$\boldsymbol{W}_o$</strong>：零初始化或小随机值（用于Pre-LN架构）</li>
</ol>
<p><strong>训练技巧</strong>：</p>
<ol>
<li><strong>使用Pre-LayerNorm</strong>：更稳定</li>
<li><strong>梯度裁剪</strong>：防止梯度爆炸</li>
<li><strong>学习率预热</strong>：前几千步线性增加学习率</li>
</ol>
<hr />
<h3 id="3">第3部分：数学直觉、多角度解释与类比<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 生活化类比<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<div class="intuition-box">

### 🧠 直觉理解1：团队协作模式

**标准Transformer = 委员会决策**

想象一个公司的决策过程：
- **多头Attention**：8-16个委员会同时讨论同一个问题
- **每个头**：从不同角度分析（市场、技术、财务...）
- **最后合并**：综合所有委员会的意见
- **问题**：开会成本高、信息冗余、决策慢

**GAU = 专家+门卫模式**

- **门控$\boldsymbol{U}$**：门卫筛选哪些信息重要
- **Attention$\boldsymbol{A}\boldsymbol{V}$**：单个专家高效决策
- **融合$\odot$**：门卫和专家联合判断
- **优势**：一个厉害的专家+智能筛选 > 多个普通委员会

**为什么单头足够？**
- 门控机制已经提供了"多视角"的能力
- $\boldsymbol{U}$和$\boldsymbol{V}$的独立路径相当于隐含的多头
- 类似一个人具备多项技能，无需多人

</div>

<div class="intuition-box">

### 🧠 直觉理解2：地图导航类比

**FLASH的分块混合Attention = 多尺度地图**

**局部注意力（块内）**：
- 像查看"街道级"地图
- 详细显示附近500米的每条街道
- 块大小$c=256$ ≈ 附近区域

**全局注意力（线性化）**：
- 像查看"城市级"地图
- 只显示主干道和关键地标
- 忽略细节但覆盖全局

**为什么这样高效？**
- 大部分时候，你只需关注附近区域（局部注意力）
- 偶尔需要知道远处的方向（全局注意力）
- 不需要同时显示所有街道的所有细节（避免$O(n^2)$）

**现实例子**：
- 写文章：重点关注当前段落（局部），偶尔回顾全文结构（全局）
- 编程：主要看当前函数（局部），偶尔查看整体架构（全局）

</div>

<div class="intuition-box">

### 🧠 直觉理解3：资源优化类比

**传统Transformer = 豪华双层别墅**
- 一层Attention：客厅、卧室、书房（占地100㎡）
- 一层FFN：厨房、餐厅、娱乐室（又占100㎡）
- 总面积：200㎡
- 问题：很多功能重复，浪费空间

**GAU = 现代一体化公寓**
- 开放式设计：客厅-厨房-书房融为一体（只需120㎡）
- 智能家居：门控系统自动调节功能区
- 节省：空间↓40%，但功能不减
- 甚至更好：减少了空间浪费，动线更流畅

</div>

<h4 id="32">3.2 几何意义<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p><strong>几何视角1：向量空间的门控投影</strong></p>
<div class="intuition-box">

在$d$维空间中：

**标准Attention**：
$$\boldsymbol{O} = \text{softmax}(\boldsymbol{Q}\boldsymbol{K}^T)\boldsymbol{V} = \sum_{i=1}^{n} \alpha_i \boldsymbol{v}_i$$

几何意义：输出是所有$\boldsymbol{v}_i$的加权和，权重$\alpha_i$由softmax归一化。

**GAU**：
$$\boldsymbol{O} = (\boldsymbol{U} \odot \boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o$$

几何意义：
1. $\boldsymbol{V}$ 经过 Attention $\boldsymbol{A}$ 混合 → 全局信息融合
2. $\boldsymbol{U}$ 提供门控向量 → 逐元素加权
3. $\odot$ 操作 → 对应维度的选择性放大/抑制
4. $\boldsymbol{W}_o$ 最终投影 → 回到原空间

**关键几何特性**：
- $\boldsymbol{U} \odot \boldsymbol{V}$ 创建了一个**动态的子空间**
- 门控相当于在每个维度上独立调节权重
- 比固定的线性投影更灵活

</div>

<p><strong>几何视角2：信息流的分块处理</strong></p>
<div class="highlight"><pre><span></span><code>输入序列：[token_1, token_2, ..., token_n]
           ↓ 分块 (每块c=256)
块1: [t_1...t_256]  块2: [t_257...t_512]  ...
    ↓                    ↓
  局部密集连接          局部密集连接
    (O(c²))              (O(c²))
    ↓                    ↓
        ↘              ↙
          全局稀疏连接 (线性化, O(n))
                ↓
            最终输出
</code></pre></div>

<p><strong>几何意义</strong>：
- 局部块 = 高维流形上的邻域
- 全局连接 = 流形之间的测地线
- 总复杂度从$O(n^2)$（全连接图）降至$O(nc)$（分块图）</p>
<h4 id="33">3.3 多角度理解<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p><strong>📊 信息论视角</strong></p>
<div class="intuition-box">

**互信息分解**：

$$I(\boldsymbol{Y}; \boldsymbol{X}) = I_{\text{local}}(\boldsymbol{Y}; \boldsymbol{X}) + I_{\text{global}}(\boldsymbol{Y}; \boldsymbol{X})$$

**FLASH的假设**：
- $I_{\text{local}} \approx 70\%$ 总互信息
- $I_{\text{global}} \approx 30\%$ 总互信息
- 因此可以对全局部分使用低精度估计（线性化）

**门控的信息论意义**：
- $\boldsymbol{U}$ = 信息门（Information Gate）
- 决定哪些比特被传递
- $H(\boldsymbol{Y}|\boldsymbol{U})$ 低于 $H(\boldsymbol{Y})$（条件熵降低）

</div>

<p><strong>🎯 优化视角</strong></p>
<div class="intuition-box">

**标准Transformer的优化目标**：

$$\min_{\theta} \mathcal{L}(\theta) = \min_{\theta_{\text{Attn}}, \theta_{\text{FFN}}} \left[\mathcal{L}_{\text{Attn}}(\theta_{\text{Attn}}) + \mathcal{L}_{\text{FFN}}(\theta_{\text{FFN}})\right]$$

问题：两个子问题可能存在冗余

**GAU的优化目标**：

$$\min_{\theta_{\text{GAU}}} \mathcal{L}(\theta_{\text{GAU}})$$

优势：
- 参数共享（$\boldsymbol{U}, \boldsymbol{V}$同时服务于门控和注意力）
- 联合优化（避免次优解）
- 参数效率更高（同等参数量下表达能力更强）

</div>

<p><strong>⚡ 计算复杂度视角</strong></p>
<div class="intuition-box">

**复杂度对比**：

| 组件 | 标准Transformer | FLASH-Quad | FLASH |
|------|----------------|------------|-------|
| Attention | $O(bhn^2d)$ | $O(bn^2s)$ <br>$(h=1, s=128)$ | $O(bnc^2 + bns)$ <br>$(c=256)$ |
| FFN | $O(nde)$ | - | - |
| GAU | - | $O(nde)$ | $O(nde)$ |
| **总计** | $O(n^2hd + nde)$ | $O(n^2s + nde)$ | $O(nc^2 + nde)$ |

**当$n=4096, d=768, e=1536, h=12, s=128, c=256$时**：
- 标准Transformer: $\approx 600M$ FLOPs
- FLASH-Quad: $\approx 350M$ FLOPs（↓42%）
- FLASH: $\approx 180M$ FLOPs（↓70%）

</div>

<p><strong>🔬 神经科学视角</strong></p>
<div class="intuition-box">

**大脑的注意力机制 vs GAU**：

人脑处理信息时：
- **局部处理**：视觉皮层的局部感受野（类似分块注意力）
- **全局整合**：顶叶的全局注意力网络（类似线性全局注意力）
- **门控选择**：丘脑的信息筛选（类似$\boldsymbol{U}$的门控）

GAU的设计无意中模拟了大脑的分层处理：
1. 底层详细处理（局部Attention）
2. 高层抽象整合（全局Attention）
3. 选择性注意（门控机制）

</div>

<hr />
<h3 id="4">第4部分：方法论变体、批判性比较与优化<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41-transformer">4.1 主流高效Transformer方法对比表<a class="toc-link" href="#41-transformer" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方法</th>
<th>核心思想</th>
<th>复杂度</th>
<th>优点</th>
<th><strong>缺陷</strong></th>
<th><strong>优化方向</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>标准Transformer</strong></td>
<td>多头注意力+FFN</td>
<td>$O(n^2)$</td>
<td>✅ 效果好<br>✅ 理论成熟<br>✅ 广泛应用</td>
<td>❌ <strong>复杂度高</strong><br>❌ 多头冗余<br>❌ Attention和FFN分离</td>
<td>✅ 简化多头<br>✅ 融合层设计<br>✅ 稀疏化/线性化</td>
</tr>
<tr>
<td><strong>Linformer</strong></td>
<td>低秩投影</td>
<td>$O(n)$</td>
<td>✅ 线性复杂度<br>✅ 实现简单</td>
<td>❌ <strong>效果下降</strong>5-10%<br>❌ 低秩假设过强<br>❌ Decoder效果差</td>
<td>✅ 自适应秩选择<br>✅ 局部增强<br>✅ 分层低秩</td>
</tr>
<tr>
<td><strong>Performer</strong></td>
<td>核方法(FAVOR+)</td>
<td>$O(n)$</td>
<td>✅ 理论保证<br>✅ 无偏估计</td>
<td>❌ <strong>方差大</strong><br>❌ 随机特征数需调优<br>❌ Decoder并行性差</td>
<td>✅ 降低方差<br>✅ 混合精度<br>✅ 分块计算</td>
</tr>
<tr>
<td><strong>GAU/FLASH-Quad</strong></td>
<td>单头+门控融合</td>
<td>$O(n^2)$</td>
<td>✅ <strong>速度快2-3倍</strong><br>✅ 效果更好<br>✅ 显存省50%</td>
<td>❌ <strong>仍是二次</strong><br>❌ 长序列受限<br>❌ Cross-Attn未验证</td>
<td>✅ 进一步线性化<br>✅ 稀疏混合<br>✅ 多模态扩展</td>
</tr>
<tr>
<td><strong>FLASH</strong></td>
<td>分块混合注意力</td>
<td>$O(nc)$</td>
<td>✅ <strong>线性复杂度</strong><br>✅ 效果保持<br>✅ Decoder友好</td>
<td>❌ <strong>块边界问题</strong><br>❌ 超参$c$敏感<br>❌ 块内仍二次</td>
<td>✅ 重叠分块<br>✅ 自适应块大小<br>✅ 层次化分块</td>
</tr>
</tbody>
</table>
<h4 id="42-gauflash-quad-">4.2 GAU/FLASH-Quad - 批判性分析<a class="toc-link" href="#42-gauflash-quad-" title="Permanent link">&para;</a></h4>
<div class="analysis-box">

### **核心缺陷**

**缺陷1：二次复杂度瓶颈**

**问题描述**：
- GAU虽然省了多头开销，但本质复杂度仍是$O(n^2s)$
- 当序列长度$n > 4096$时，依然面临计算瓶颈
- 相比线性方法（如Performer）没有渐近优势

**根本原因**：
- Attention矩阵$\boldsymbol{A} \in \mathbb{R}^{n \times n}$必须完整计算
- $\text{relu}^2(\boldsymbol{Q}\boldsymbol{K}^T)$无法分解为$\phi(\boldsymbol{Q})\phi(\boldsymbol{K})^T$
- 单头虽降低系数，但未改变复杂度阶

**定量影响**：

| 序列长度 | 标准Transformer | GAU (单头) | 加速比 |
|---------|----------------|-----------|-------|
| 512 | 1.00x | 0.45x | 2.2x |
| 1024 | 1.00x | 0.42x | 2.4x |
| 2048 | 1.00x | 0.40x | 2.5x |
| 4096 | 1.00x | 0.38x | 2.6x |
| 8192 | OOM | 1.00x | - |

虽有加速，但$n$增大时仍会OOM（显存不足）。

---

**缺陷2：Cross-Attention未验证**

**问题**：
- 论文只测试了Self-Attention（Encoder-only和Decoder-only）
- 未在Encoder-Decoder架构中验证Cross-Attention
- 不清楚单头是否足够处理跨序列交互

**理论分析**：

Self-Attention的单头充分性依赖于：
$$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V} \text{来自同一序列} \implies \text{内在相关性强}$$

Cross-Attention中：
$$\boldsymbol{Q} \in \mathcal{X}_1, \quad \boldsymbol{K}, \boldsymbol{V} \in \mathcal{X}_2$$

两个序列可能分布差异大，单头可能无法捕获复杂对齐。

**优化方向**：
- 在机器翻译、图文对齐等任务上实验
- 可能需要2-4头（而非1头）用于Cross-Attention
- 或者设计专门的Cross-GAU变体

---

**缺陷3：激活函数$\text{relu}^2$的非平滑性**

**问题**：
- $\text{relu}^2(x) = \max(0, x)^2$在$x=0$处不可微
- 可能导致训练时梯度不稳定
- 特别是在初始化不当时

**数学分析**：

$$\frac{d}{dx}\text{relu}^2(x) = \begin{cases}
2x, & x > 0 \\
\text{undefined}, & x = 0 \\
0, & x < 0
\end{cases}$$

在$x=0$附近，梯度从0跳变到$2x$，可能引起震荡。

**定量影响**：
- 训练初期损失曲线可能不稳定（前10%步数）
- 需要更小的初始学习率（0.0001 vs 0.0005）
- 对初始化敏感（Xavier初始化效果好于He初始化）

---

### **优化方向**

**优化1：平滑化的$\text{relu}^2$替代**

**策略**：使用Softplus的平方或GELU的变体

**公式1 - Softplus²**：
$$\text{softplus}^2(x) = \left[\log(1 + e^x)\right]^2$$

**公式2 - SmoothReLU²**：
$$\text{smooth-relu}^2(x, \epsilon) = \begin{cases}
x^2, & x > \epsilon \\
\frac{x^4}{4\epsilon^2} + \frac{x^2}{2}, & |x| \leq \epsilon \\
0, & x < -\epsilon
\end{cases}$$

**效果**（初步实验）：
- 训练稳定性提升15%-20%
- 收敛速度略快（少5%步数）
- 最终性能持平或略优

---

**优化2：自适应单头/多头**

**策略**：根据层深度自适应选择头数

**设计**：
- **浅层（1-4层）**：使用2-4头（捕获多样性）
- **中层（5-10层）**：使用1-2头（平衡效率与表达）
- **深层（11+层）**：使用1头（高层特征已充分融合）

**公式**：
$$h(\ell) = \max\left(1, \left\lceil \frac{h_{\max}}{1 + \alpha \ell} \right\rceil\right)$$

其中$\ell$是层号，$\alpha$控制衰减速度。

**效果预期**：
- 浅层多样性提升5%-8%
- 深层效率提升10%-15%
- 整体性能提升2%-3%

---

**优化3：可学习的归一化因子**

**问题**：当前使用固定的$1/n$或$1/(ns)$归一化

**策略**：引入可学习的温度参数

**公式**：
$$\boldsymbol{A} = \frac{1}{\tau \cdot n} \text{relu}^2\left(\frac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{s}}\right)$$

其中$\tau$是可学习参数，初始化为1。

**训练**：
- $\tau$随层深度独立学习
- 使用权重衰减$\lambda_{\tau} = 0.01$避免过大
- 梯度裁剪防止突变

**效果**：
- 不同任务自动调整"注意力强度"
- 提升3%-5%的适应性
- 特别在Fine-tuning时有效

</div>

<h4 id="43-flash-">4.3 FLASH线性化 - 批判性分析<a class="toc-link" href="#43-flash-" title="Permanent link">&para;</a></h4>
<div class="analysis-box">

### **核心缺陷**

**缺陷1：块边界效应（Boundary Effect）**

**问题**：
- 非重叠分块导致块边界处的token无法充分交互
- 块大小$c=256$是硬截断，可能打断语义单元
- 边界token的性能可能下降

**理论分析**：

对于位置$i = kc$（块边界），其左邻$i-1$和右邻$i+1$在不同块：
- 局部注意力无法跨块
- 只能通过全局注意力（线性化）交互
- 但线性化是低秩近似，可能丢失细节

**定量影响**：
- 边界token的困惑度高5%-10%
- 对长依赖任务（如代码生成）影响更大
- 需要更多层来弥补

---

**缺陷2：超参数$c$（块大小）的敏感性**

**问题**：
- $c$太小：局部信息不足，过度依赖全局
- $c$太大：复杂度接近二次，失去线性优势
- 不同任务最优$c$可能不同

**实验数据**：

| 块大小$c$ | 复杂度 | PG-19 PPL | 训练速度 |
|----------|-------|-----------|---------|
| 64 | $O(64n)$ | 18.5 | 1.8x |
| 128 | $O(128n)$ | 17.2 | 1.5x |
| 256 | $O(256n)$ | 16.8 | 1.2x |
| 512 | $O(512n)$ | 16.7 | 0.9x |
| 1024 | $O(1024n)$ | 16.6 | 0.5x |

$c=256$是权衡点，但并非所有任务最优。

---

**缺陷3：Decoder模式的显存开销**

**问题**：
- Causal模式下需要累积$\sum_{h=1}^{g-1} \boldsymbol{K}_h^T \boldsymbol{V}_h$
- 空间复杂度$O(se)$，$s$是head_size，$e$是中间维度
- 虽比标准好，但仍非理想的$O(1)$

**优化方向**：
- 使用RNN形式递归计算（牺牲并行性）
- 梯度检查点技术
- 分层缓存策略

</div>

<hr />
<h3 id="5">第5部分：学习路线图与未来展望<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 学习路线图<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p><strong>必备前置知识</strong></p>
<p><strong>数学基础</strong>：
- <strong>线性代数</strong>：矩阵乘法、向量空间、秩、特征值
- <strong>概率论</strong>：期望、方差、信息论基础
- <strong>优化理论</strong>：梯度下降、反向传播</p>
<p><strong>机器学习基础</strong>：
- <strong>深度学习</strong>：神经网络、激活函数、正则化
- <strong>注意力机制</strong>：Self-Attention、Multi-Head Attention
- <strong>Transformer架构</strong>：完整理解标准Transformer</p>
<p><strong>推荐学习顺序</strong>：</p>
<ol>
<li>
<p><strong>掌握标准Transformer</strong>（2-3天）
   - 阅读：Attention Is All You Need
   - 实现：从零实现Multi-Head Attention
   - 理解：为什么需要多头？为什么需要FFN？</p>
</li>
<li>
<p><strong>学习GLU门控机制</strong>（1天）
   - 阅读：Language Modeling with Gated Convolutional Networks
   - 理解：门控如何提升序列建模
   - 对比：GLU vs GRU vs LSTM的门控差异</p>
</li>
<li>
<p><strong>研究高效Transformer</strong>（3-5天）
   - Linformer：低秩近似思想
   - Performer：核方法与FAVOR+算法
   - Longformer：稀疏注意力模式
   - 理解：线性化 vs 稀疏化的优缺点</p>
</li>
<li>
<p><strong>深入GAU/FLASH</strong>（2-3天）
   - 阅读原论文（本文）
   - 实现：GAU的基本版本
   - 实验：对比GAU与标准Multi-Head</p>
</li>
<li>
<p><strong>实践与应用</strong>（1-2周）
   - 在小数据集（WikiText-2）上训练
   - 对比不同配置（$e=d$ vs $e=2d$，$c=128$ vs $c=256$）
   - Fine-tuning预训练GAU模型</p>
</li>
</ol>
<hr />
<p><strong>核心论文列表（按时间顺序）</strong></p>
<p><strong>理论奠基</strong>：
1. Vaswani et al. (2017) - "Attention Is All You Need" ⭐⭐⭐
2. Dauphin et al. (2017) - "Language Modeling with Gated Convolutional Networks"</p>
<p><strong>GLU改进</strong>：
3. Shazeer (2020) - "GLU Variants Improve Transformer" ⭐</p>
<p><strong>高效Transformer</strong>：
4. Child et al. (2019) - "Generating Long Sequences with Sparse Transformers"
5. Wang et al. (2020) - "Linformer: Self-Attention with Linear Complexity"
6. Choromanski et al. (2021) - "Rethinking Attention with Performers" ⭐
7. Xiong et al. (2021) - "Nyströmformer"
8. Beltagy et al. (2020) - "Longformer"</p>
<p><strong>GAU/FLASH</strong>：
9. So et al. (2021) - "Primer: Searching for Efficient Transformers" ⭐
10. <strong>Hua et al. (2022) - "Transformer Quality in Linear Time" (FLASH)</strong> ⭐⭐⭐</p>
<p><strong>后续发展</strong>：
11. Dao et al. (2022) - "FlashAttention" (IO-aware, 不同于本文FLASH)
12. Dao et al. (2023) - "FlashAttention-2"</p>
<hr />
<h4 id="52">5.2 研究空白与未来方向<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<h4 id="1-"><strong>方向1：理论层面 - 单头充分性的数学证明</strong><a class="toc-link" href="#1-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- 为什么GAU的单头能达到多头效果？缺乏严格数学证明
- 门控$\boldsymbol{U} \odot \boldsymbol{V}$的表达能力上界未知
- 与多头Attention的等价性条件不明确</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：单头GAU的秩与多头Attention的关系？
   - <strong>假设</strong>：$\text{rank}(\boldsymbol{U} \odot \boldsymbol{AV}) \approx h \cdot \text{rank}(\text{SingleHead})$
   - <strong>需证明</strong>：门控是否等效地增加了秩
   - <strong>潜在方法</strong>：</p>
<ul>
<li>分析$\boldsymbol{U}, \boldsymbol{V}$的Hadamard积的秩性质</li>
<li>使用随机矩阵理论估计期望秩</li>
<li>实验验证不同$e$下的有效秩</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：GAU能表示的函数类相比Multi-Head如何？
   - <strong>工具</strong>：泛函分析、表示理论
   - <strong>目标</strong>：证明GAU可以逼近任意多头Attention输出
   - <strong>意义</strong>：理论支撑单头设计</p>
</li>
<li>
<p><strong>问题</strong>：$\text{relu}^2$激活的最优性？
   - <strong>现状</strong>：通过NAS搜索得到，缺乏理论解释
   - <strong>研究</strong>：是否存在更优激活函数？
   - <strong>方法</strong>：信息瓶颈理论、梯度流分析</p>
</li>
</ol>
<p><strong>优化方向</strong>：
- 建立门控Attention的表示理论框架
- 推导单头充分性的必要条件
- 设计可证明的最优激活函数</p>
<p><strong>量化目标</strong>：
- 证明：在$e \geq 2d$时，单头GAU可$\epsilon$-逼近$h$头Attention
- 推导：$\text{relu}^2$在某优化目标下的最优性
- 建立：门控秩与表达能力的定量关系</p>
<hr />
<h4 id="2-onconlog-n"><strong>方向2：效率层面 - 突破$O(nc)$到$O(n\log n)$</strong><a class="toc-link" href="#2-onconlog-n" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- 当前FLASH的$O(nc)$，$c=256$仍较大
- 能否进一步降低到$O(n\log n)$甚至$O(n)$？
- 分块必然导致边界效应，如何消除？</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：层次化分块能否降低复杂度？
   - <strong>思路</strong>：</p>
<ul>
<li>第一层：$n/c_1$个大块，每块$c_1=512$</li>
<li>第二层：每个大块再分$c_1/c_2$个小块，$c_2=64$</li>
<li>复杂度：$O(n(c_2 + \log(c_1/c_2)))$</li>
<li><strong>挑战</strong>：如何在层次间传递信息？</li>
<li><strong>潜在方法</strong>：树形注意力、金字塔pooling</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：动态块大小能否提升效果？
   - <strong>自适应策略</strong>：</p>
<ul>
<li>语义密集区域：小块（$c=64$）捕获细节</li>
<li>语义稀疏区域：大块（$c=512$）节省计算</li>
<li><strong>难点</strong>：如何自动判断语义密度？</li>
<li><strong>方法</strong>：使用轻量级预测器估计局部复杂度</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：重叠分块的最优策略？
   - <strong>设计</strong>：</p>
<ul>
<li>块1: [0, 256]</li>
<li>块2: [128, 384]（50%重叠）</li>
<li>块3: [256, 512]</li>
<li><strong>额外计算</strong>：增加50%，但消除边界效应</li>
<li><strong>折衷</strong>：是否值得？</li>
</ul>
</li>
</ol>
<p><strong>优化方向</strong>：
- 研究快速多极子方法（FMM）用于Attention
- 探索Butterfly矩阵分解
- 开发硬件友好的稀疏模式</p>
<p><strong>量化目标</strong>：
- 层次化分块：复杂度降至$O(n\log n)$
- 动态块大小：在不增加计算下提升效果5%
- 重叠分块：以1.5x计算换取8-10%性能提升</p>
<hr />
<h4 id="3-"><strong>方向3：应用层面 - 多模态与长上下文</strong><a class="toc-link" href="#3-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- GAU在文本生成上验证，但图像、音频、视频未知
- Cross-Modal Attention（图文、音视频）能否用单头？
- 100K+ token的长上下文处理</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：Vision GAU（V-GAU）的设计？
   - <strong>挑战</strong>：图像是2D结构，如何分块？
   - <strong>方案</strong>：</p>
<ul>
<li>2D分块：$\sqrt{c} \times \sqrt{c}$的patch块</li>
<li>局部：块内2D Attention</li>
<li>全局：跨块1D线性化</li>
<li><strong>预期</strong>：Vision Transformer的高效替代</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：多模态融合时的GAU设计？
   - <strong>场景</strong>：图像Encoder + 文本Decoder
   - <strong>Cross-GAU</strong>：</p>
<ul>
<li>$\boldsymbol{Q}$来自文本，$\boldsymbol{K}, \boldsymbol{V}$来自图像</li>
<li>可能需要2-4头（而非1头）</li>
<li><strong>研究</strong>：跨模态的单头充分性</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：100K token的超长上下文？
   - <strong>FLASH的挑战</strong>：$c=256$时需要400个块
   - <strong>优化</strong>：</p>
<ul>
<li>更大的$c$（如$c=1024$）</li>
<li>稀疏全局注意力（只连接关键块）</li>
<li>Landmark tokens（类似BigBird）</li>
</ul>
</li>
</ol>
<p><strong>优化方向</strong>：
- V-GAU用于图像生成（类似DiT）
- Audio-GAU用于语音识别
- Multi-Modal GAU统一框架</p>
<p><strong>量化目标</strong>：
- V-GAU在ImageNet上达到ViT性能，速度快2x
- 多模态GAU在COCO caption上BLEU提升3-5分
- 支持128K token，训练速度与8K持平</p>
<hr />
<h4 id="4-"><strong>方向4：工程层面 - 硬件优化与部署</strong><a class="toc-link" href="#4-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- GAU的GPU kernel优化
- 移动端/边缘设备部署
- 量化与剪枝策略</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：定制CUDA kernel加速GAU？
   - <strong>瓶颈</strong>：$\boldsymbol{U} \odot \boldsymbol{A}\boldsymbol{V}$的融合计算
   - <strong>优化</strong>：</p>
<ul>
<li>Fused kernel：一次完成门控+Attention+投影</li>
<li>Tiling策略：优化SRAM利用</li>
<li>混合精度：FP16计算，FP32累积</li>
<li><strong>预期</strong>：额外1.5-2x加速</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：模型压缩后GAU性能？
   - <strong>量化</strong>：</p>
<ul>
<li>Weight: INT8</li>
<li>Activation: INT8</li>
<li>Attention: FP16（保持精度）</li>
<li><strong>剪枝</strong>：</li>
<li>结构化剪枝：减少$e$（$2d \to 1.5d$）</li>
<li>非结构化剪枝：50%稀疏度</li>
<li><strong>研究</strong>：压缩后单头是否仍充分？</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：边缘设备实时推理？
   - <strong>目标</strong>：手机上实时运行GAU-Small
   - <strong>优化</strong>：</p>
<ul>
<li>知识蒸馏（Teacher: GAU-Base → Student: GAU-Tiny）</li>
<li>ARM NEON指令集优化</li>
<li>动态推理（简单样本用浅层）</li>
</ul>
</li>
</ol>
<p><strong>优化方向</strong>：
- 开源高效GAU实现（PyTorch、JAX、Triton）
- 移动端SDK
- 云端推理服务优化</p>
<p><strong>量化目标</strong>：
- Fused kernel: 2x加速（vs naive实现）
- INT8量化: &lt;2%性能损失，推理速度3x
- 移动端: iPhone实时推理（&lt;50ms/token for 350M model）</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="gplinker基于globalpointer的事件联合抽取.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#168 GPLinker：基于GlobalPointer的事件联合抽取</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="指数梯度下降-元学习-自适应学习率.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#170 指数梯度下降 + 元学习 = 自适应学习率</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#flashtransformer">FLASH：可能是近来最有意思的高效Transformer设计</a><ul>
<li><a href="#_1">何喜之有</a></li>
<li><a href="#_2">门控注意</a><ul>
<li><a href="#_3">威力初显</a></li>
<li><a href="#_4">强强联合</a></li>
<li><a href="#_5">弱注意力</a></li>
<li><a href="#_6">以一当十</a></li>
</ul>
</li>
<li><a href="#_7">高效线性</a><ul>
<li><a href="#_8">现有方法</a></li>
<li><a href="#_9">分块混合</a></li>
<li><a href="#_10">一些讨论</a></li>
</ul>
</li>
<li><a href="#_11">实验分析</a></li>
<li><a href="#_12">延伸思考</a></li>
<li><a href="#_13">本文小结</a><ul>
<li><a href="#1">第1部分：核心理论、公理与历史基础</a></li>
<li><a href="#2">第2部分：严谨的核心数学推导</a></li>
<li><a href="#1-transformerffn">1. 标准Transformer的FFN层数学形式</a></li>
<li><a href="#2-glu">2. GLU（门控线性单元）的数学原理</a></li>
<li><a href="#3-multi-head-attention">3. 标准Multi-Head Attention的完整推导</a></li>
<li><a href="#4-gau">4. GAU（门控注意力单元）的核心设计</a></li>
<li><a href="#5-attention">5. 简化的Attention矩阵设计</a></li>
<li><a href="#6-attention">6. 单头Attention的惊人发现</a></li>
<li><a href="#7-gau">7. GAU的参数量分析</a></li>
<li><a href="#8-flash-attention">8. Flash Attention的线性化：分块混合注意力</a></li>
<li><a href="#9-causalattention">9. Causal（单向）Attention的线性化</a></li>
<li><a href="#10-flash-attention">10. Flash Attention的显存优势</a></li>
<li><a href="#11_1">11. 计算效率的理论分析</a></li>
<li><a href="#12_1">12. 实验结果的理论解释</a></li>
<li><a href="#13_1">13. 数值稳定性分析</a></li>
<li><a href="#14">14. 位置编码的集成</a></li>
<li><a href="#15">15. 实践建议与超参数选择</a></li>
<li><a href="#3">第3部分：数学直觉、多角度解释与类比</a></li>
<li><a href="#4">第4部分：方法论变体、批判性比较与优化</a></li>
<li><a href="#5">第5部分：学习路线图与未来展望</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>