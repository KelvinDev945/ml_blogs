<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸 | ML & Math Blog Posts</title>
    <meta name="description" content="L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸&para;
原文链接: https://spaces.ac.cn/archives/7681
发布日期: 

L2正则是机器学习常用的一种防止过拟合的方法（应该也是一道经常遇到的面试题）。简单来说，它就是希望权重的模长尽可能小一点，从而能抵御的扰动多一点，最终提高模型的泛化性能。但是读者可能也会发现，L2正则的表现通常没有理论上说的那么好，很多...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=模型">模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #82 L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#82</span>
                L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/7681" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 模型</span>
                </a>
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=深度学习" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 深度学习</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="l2">L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸<a class="toc-link" href="#l2" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/7681">https://spaces.ac.cn/archives/7681</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>L2正则是机器学习常用的一种防止过拟合的方法（应该也是一道经常遇到的面试题）。简单来说，它就是希望权重的模长尽可能小一点，从而能抵御的扰动多一点，最终提高模型的泛化性能。但是读者可能也会发现，L2正则的表现通常没有理论上说的那么好，很多时候加了可能还有负作用。最近的一篇文章<a href="https://papers.cool/arxiv/2008.02965">《Improve Generalization and Robustness of Neural Networks via Weight Scale Shifting Invariant Regularizations》</a>从“权重尺度偏移”这个角度分析了L2正则的弊端，并提出了新的WEISSI正则项。整个分析过程颇有意思，在这里与大家分享一下。</p>
<h2 id="_1">相关内容<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>这一节中我们先简单回顾一下L2正则，然后介绍它与权重衰减的联系以及与之相关的<a href="https://papers.cool/arxiv/1711.05101">AdamW优化器</a>。</p>
<h3 id="l2_1">L2正则的理解<a class="toc-link" href="#l2_1" title="Permanent link">&para;</a></h3>
<p>为什么要添加L2正则？这个问题可能有多个答案。有从Ridge回归角度回答的，有从贝叶斯推断角度回答的，这里给出从扰动敏感性的角度的理解。</p>
<p>对于两个（列）向量$\boldsymbol{w},\boldsymbol{x}$，我们有柯西不等式$\left|\boldsymbol{w}^{\top}\boldsymbol{x}\right|\leq \Vert\boldsymbol{w}\Vert_2\Vert\boldsymbol{x}\Vert_2$。根据这个结果，我们就可以证明<br />
\begin{equation}\Vert\boldsymbol{W}(\boldsymbol{x}<em task="task">2 - \boldsymbol{x}_1)\Vert_2\leq \Vert\boldsymbol{W}\Vert_2\Vert\boldsymbol{x}_2 - \boldsymbol{x}_1\Vert_2\end{equation}<br />
这里的$\Vert\boldsymbol{W}\Vert_2^2$等于矩阵$\boldsymbol{W}$的所有元素的平方和。证明并不困难，有兴趣的读者自行完成。这个结果告诉我们：$\boldsymbol{W}\boldsymbol{x}$的变化量，可以被$\Vert\boldsymbol{W}\Vert_2$和$\Vert\boldsymbol{x}_2 - \boldsymbol{x}_1\Vert_2$控制住，因此如果我们希望$\Vert\boldsymbol{x}_2 - \boldsymbol{x}_1\Vert_2$很小时$\boldsymbol{W}\boldsymbol{x}$的变化量也尽可能小，那么我们可以降低$\Vert\boldsymbol{W}\Vert_2$，这时候就可以往任务目标$\mathcal{L}</em>\Vert_2^2$。不难发现，这其实就是L2正则。这个角度的相关讨论还可以参考}$里边加入一个正则项$\mathcal{L}_{reg}=\Vert\boldsymbol{W<a href="/archives/6051">《深度学习中的Lipschitz约束：泛化与生成模型》</a>（不过要注意两篇文章的记号略有不同）。</p>
<h3 id="adamw">AdamW优化器<a class="toc-link" href="#adamw" title="Permanent link">&para;</a></h3>
<p>在使用SGD进行优化时，假设原来的迭代为$\boldsymbol{\theta}<em t-1="t-1">{t}=\boldsymbol{\theta}</em>} - \varepsilon\boldsymbol{g<em t="t">{t}$，那么不难证明加入L2正则$\Vert\boldsymbol{\theta}\Vert_2^2$后变成了<br />
\begin{equation}\boldsymbol{\theta}</em>}=(1-\varepsilon\lambda)\boldsymbol{\theta<em t="t">{t-1} - \varepsilon\boldsymbol{g}</em>}\end{equation
由于$0 &lt; 1-\varepsilon\lambda &lt; 1$，所以这会使得整个优化过程中参数$\boldsymbol{\theta}$有“收缩”到0的倾向，这样的改动称为“权重衰减（Weight Decay）”。</p>
<p>不过，L2正则与权重衰减的等价性仅仅是在SGD优化器下成立，如果用了自适应学习率优化器如Adagrad、Adam等，那么两者不等价，在自适应学习率优化器中，L2正则的作用约等于往优化过程里边加入$-\varepsilon\lambda\text{sign}(\boldsymbol{\theta}<em t-1="t-1">{t-1})$而不是$-\varepsilon\lambda\boldsymbol{\theta}</em>$，也就是说每个元素的惩罚都很均匀，而不是绝对值更大的元素惩罚更大，这部分抵消了L2正则的作用。论文<a href="https://papers.cool/arxiv/1711.05101">《Decoupled Weight Decay Regularization》</a>首次强调了这个问题，并且提出了改进的AdamW优化器。</p>
<h2 id="_2">新的正则<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>在这一节中，我们将指出常见的深度学习模型中往往存在“权重尺度偏移（Weight Scale Shif）”现象，这个现象可能会导致了L2正则的作用没那么明显。进一步地，我们可以构建一个新的正则项，它具有跟L2类似的作用，但是与权重尺度偏移现象更加协调，理论上来说会更加有效。</p>
<h3 id="_3">权重尺度偏移<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h3>
<p>我们知道深度学习模型的基本结构就是“线性变换+非线性激活函数”，而现在最常用的激活函数之一是$\text{relu}(x)=\max(x,0)$。有意思的是，这两者都满足“正齐次性”，也就是对于$\varepsilon \geq 0$，我们有$\varepsilon\phi(x)=\phi(\varepsilon x)$恒成立。对于其他的激活函数如SoftPlus、GELU、Swish等，其实它们都是$\text{relu}$的光滑近似，因此可以认为它们是近似满足“正齐次性”。</p>
<p>“正齐次性”使得深度学习模型对于权重尺度偏移具有一定的不变性。具体来说，假设一个$L$层的模型：<br />
\begin{equation}\begin{aligned}
\boldsymbol{h}<em L-1="L-1">L =&amp; \phi(\boldsymbol{W}_L \boldsymbol{h}</em>} + \boldsymbol{b<em L-1="L-1">L) \\
=&amp; \phi(\boldsymbol{W}_L \phi(\boldsymbol{W}</em>} \boldsymbol{h<em L-1="L-1">{L-2} + \boldsymbol{b}</em>}) + \boldsymbol{b<em L-1="L-1">L) \\
=&amp; \cdots\\
=&amp; \phi(\boldsymbol{W}_L \phi(\boldsymbol{W}</em>} \phi(\cdots\phi(\boldsymbol{W<em L-1="L-1">1\boldsymbol{x} + \boldsymbol{b}_1)\cdots) + \boldsymbol{b}</em>}) + \boldsymbol{b<em l="1">L)
\end{aligned}\end{equation}<br />
假设每个参数引入偏移$\boldsymbol{W}_l = \gamma_l\tilde{\boldsymbol{W}}_l,\boldsymbol{b}_l = \gamma_l\tilde{\boldsymbol{b}}_l$，那么根据正齐次性可得<br />
\begin{equation}\begin{aligned}
\boldsymbol{h}_L =&amp; \left(\prod</em>}^L \gamma_l\right)\phi(\tilde{\boldsymbol{W}<em L-1="L-1">L \boldsymbol{h}</em>} + \tilde{\boldsymbol{b}<em l="1">L) \\
=&amp; \cdots\\
=&amp; \left(\prod</em>}^L \gamma_l\right) \phi(\tilde{\boldsymbol{W}<em L-1="L-1">L \phi(\tilde{\boldsymbol{W}}</em>} \phi(\cdots\phi(\tilde{\boldsymbol{W}<em L-1="L-1">1\boldsymbol{x} + \tilde{\boldsymbol{b}}_1)\cdots) + \tilde{\boldsymbol{b}}</em>}) + \tilde{\boldsymbol{b}<em l="1">L)
\end{aligned}\end{equation}<br />
如果$\prod\limits</em>}^L \gamma_l = 1$，那么参数为$\{\boldsymbol{W<em l="1">l,\boldsymbol{b}_l\}$就跟参数为$\{\tilde{\boldsymbol{W}}_l,\tilde{\boldsymbol{b}}_l\}$的模型完全等价了。换句话说，模型对于$\prod\limits</em>^L \gamma_l = 1$的权重尺度偏移具有不变性（WEIght-Scale-Shift-Invariance，WEISSI）。</p>
<h3 id="l2_2">与L2正则不协调<a class="toc-link" href="#l2_2" title="Permanent link">&para;</a></h3>
<p>刚才我们说只要尺度偏移满足$\prod\limits_{l=1}^L \gamma_l = 1$，那么两组参数对应的模型就等价了，但问题是它们对应的L2正则却不等价：<br />
\begin{equation}\sum_{l=1}^L \Vert\boldsymbol{W}<em l="1">l\Vert_2^2=\sum</em>}^L \gamma_l^2\Vert\tilde{\boldsymbol{W}<em l="1">l\Vert_2^2\neq \sum</em>^L
\Vert\tilde{\boldsymbol{W}}<em l="1">l\Vert_2^2\end{equation}<br />
并且可以证明，如果固定$\Vert\boldsymbol{W}_1\Vert_2,\Vert\boldsymbol{W}_2\Vert_2,\dots,\Vert\boldsymbol{W}_L\Vert_2$，并且保持约束$\prod\limits</em>^L}^L \gamma_l = 1$，那么$\sum\limits_{l=1
\Vert\tilde{\boldsymbol{W}}<em l="1">l\Vert_2^2$的最小值在<br />
\begin{equation}\Vert\tilde{\boldsymbol{W}_1}\Vert_2=\Vert\tilde{\boldsymbol{W}}_2\Vert_2=\dots=\Vert\tilde{\boldsymbol{W}}_L\Vert_2=\left(\prod</em>}^L \Vert\boldsymbol{W<em task="task">l\Vert_2\right)^{1/L}\end{equation}<br />
事实上，这就体现了L2正则的低效性。试想一下，假如我们已经训练得到一组参数$\{\boldsymbol{W}_l,\boldsymbol{b}_l\}$，这组参数泛化性能可能不大好，于是我们希望L2正则能帮助优化器找到一组更好参数（牺牲一点$\mathcal{L}</em>$，它跟原来参数的模型完全等价（没有提升泛化性能），但是L2正则还更小（L2正则起作用了）。说白了，就是L2正则确实起作用了，但没有提升模型泛化性能，没有达到使用L2正则的初衷。}$，降低一点$\mathcal{L}_{reg}$）。但是，上述结果告诉我们，由于权重尺度偏移不变性的存在，模型完全可以找到一组新的参数$\{\tilde{\boldsymbol{W}}_l,\tilde{\boldsymbol{b}}_l\</p>
<h3 id="weissi">WEISSI正则<a class="toc-link" href="#weissi" title="Permanent link">&para;</a></h3>
<p>上述问题的根源在于，模型对权重尺度偏移具有不变性，但是L2正则对权重尺度偏移没有不变性。如果我们能找到一个新的正则项，它有类似的作用，同时还对权重尺度偏移不变，那么就能解决这个问题了。个人感觉原论文对这部分的讲解并不够清晰，下面的推导以笔者的个人理解为主。</p>
<p>我们考虑如下的一般形式的正则项<br />
\begin{equation}\mathcal{L}<em l="1">{reg}=\sum</em>}^L \varphi(\Vert\boldsymbol{W<em reg="reg">l\Vert_2)\end{equation}<br />
对于L2正则来说，$\varphi(x)=x^2$，只要$\varphi(x)$是关于$x$在$[0,+\infty)$上的单调递增函数，那么就能保证优化目标是缩小$\Vert\boldsymbol{W}_l\Vert$。要注意我们希望正则项具有尺度偏移不变性，并不需要$\varphi(\gamma x) = \varphi(x)$，而只需要<br />
\begin{equation}\frac{d}{dx}\varphi(\gamma x)=\frac{d}{dx}\varphi(x)\label{eq:varphi}\end{equation}<br />
因为优化过程只需要用到它的梯度。可能有的读者都能直接看出它的一个解了，其实就是对数函数$\varphi(x) = \log x$。所以新提出来的正则项就是<br />
\begin{equation}\mathcal{L}</em>}=\sum_{l=1}^L \log\Vert\boldsymbol{W<em l="1">l\Vert_2=\log \left(\prod</em>}^L \Vert\boldsymbol{W<em reg="reg">l\Vert_2\right)\end{equation}<br />
除此之外，原论文可能担心上述正则项惩罚力度还不够，还对参数方向加了个L1的惩罚，总的形式为：<br />
\begin{equation}\mathcal{L}</em>}=\lambda_1\sum_{l=1}^L \log\Vert\boldsymbol{W<em l="1">l\Vert_2 + \lambda_2\sum</em>}^L \big\Vert\boldsymbol{W}_l\big/\Vert\boldsymbol{W}_l\Vert_2\big\Vert_1\end{equation</p>
<h3 id="_4">实验效果简述<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h3>
<p>按惯例展示一下原论文的是实验结果，当然既然作者都整理成文了，显然说明是有正面结果的：  </p>
<p><a href="/usr/uploads/2020/08/4113446272.png" title="点击查看原图"><img alt="原论文对WEISSI正则的实验结果之一" src="/usr/uploads/2020/08/4113446272.png" /></a></p>
<p>原论文对WEISSI正则的实验结果之一</p>
<p>对于我们来说，无非就是知道有这么个新的选择，炼丹的时候多一种尝试罢了。毕竟正则项这种东西，没有什么理论能保证它一定能起作用，还是用了才能知道结果，别人说得再漂亮也不一定有用。</p>
<h2 id="_5">文章小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文介绍了神经网络模型中的权重尺度偏移不变性的现象，并指出它与L2正则的不协调性，继而提出了作用类似但能够解决不协调性的正则项。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/7681">https://spaces.ac.cn/archives/7681</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Aug. 14, 2020). 《L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/7681">https://spaces.ac.cn/archives/7681</a></p>
<p>@online{kexuefm-7681,
title={L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸},<br />
author={苏剑林},<br />
year={2020},<br />
month={Aug},<br />
url={\url{https://spaces.ac.cn/archives/7681}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="有限内存下全局打乱几百g文件python.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#81 有限内存下全局打乱几百G文件（Python）</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="观测iss.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#83 观测ISS</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#l2">L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸</a><ul>
<li><a href="#_1">相关内容</a><ul>
<li><a href="#l2_1">L2正则的理解</a></li>
<li><a href="#adamw">AdamW优化器</a></li>
</ul>
</li>
<li><a href="#_2">新的正则</a><ul>
<li><a href="#_3">权重尺度偏移</a></li>
<li><a href="#l2_2">与L2正则不协调</a></li>
<li><a href="#weissi">WEISSI正则</a></li>
<li><a href="#_4">实验效果简述</a></li>
</ul>
</li>
<li><a href="#_5">文章小结</a></li>
<li><a href="#_6">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>