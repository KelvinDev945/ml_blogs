<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>那个屠榜的T5模型，现在可以在中文上玩玩了 | ML & Math Blog Posts</title>
    <meta name="description" content="那个屠榜的T5模型，现在可以在中文上玩玩了&para;
原文链接: https://spaces.ac.cn/archives/7867
发布日期: 

不知道大家对Google去年的屠榜之作T5还有没有印象？就是那个打着“万事皆可Seq2Seq”的旗号、最大搞了110亿参数、一举刷新了GLUE、SuperGLUE等多个NLP榜单的模型，而且过去一年了，T5仍然是SuperGLUE榜单上的第一，目...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #36 那个屠榜的T5模型，现在可以在中文上玩玩了
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#36</span>
                那个屠榜的T5模型，现在可以在中文上玩玩了
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/7867" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=文本生成" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 文本生成</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="t5">那个屠榜的T5模型，现在可以在中文上玩玩了<a class="toc-link" href="#t5" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/7867">https://spaces.ac.cn/archives/7867</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>不知道大家对Google去年的屠榜之作T5还有没有印象？就是那个打着“万事皆可Seq2Seq”的旗号、最大搞了110亿参数、一举刷新了GLUE、SuperGLUE等多个NLP榜单的模型，而且过去一年了，T5仍然是<a href="https://super.gluebenchmark.com/">SuperGLUE</a>榜单上的第一，目前还稳妥地拉开着第二名2%的差距。然而，对于中文界的朋友来说，T5可能没有什么存在感，原因很简单：没有中文版T5可用。不过这个现状要改变了，因为Google最近放出了多国语言版的T5（mT5），里边当然是包含了中文语言。虽然不是纯正的中文版，但也能凑合着用一下。</p>
<p><a href="/usr/uploads/2020/11/4211830191.png" title="点击查看原图"><img alt="“万事皆可Seq2Seq”的T5" src="/usr/uploads/2020/11/4211830191.png" /></a></p>
<p>“万事皆可Seq2Seq”的T5</p>
<p>本文将会对T5模型做一个简单的回顾与介绍，然后再介绍一下如何在bert4keras中调用mT5模型来做中文任务。作为一个原生的Seq2Seq预训练模型，mT5在文本生成任务上的表现还是相当不错的，非常值得一试。</p>
<h2 id="t5_1">T5<a class="toc-link" href="#t5_1" title="Permanent link">&para;</a></h2>
<p>跟BERT一样，T5也是Google出品的预训练模型，来自论文为<a href="https://papers.cool/arxiv/1910.10683">《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》</a>，Github为<a href="https://github.com/google-research/text-to-text-transfer-transformer">text-to-text-transfer-transformer</a>。T5的理念就是“万事皆可Seq2Seq”，它使用了标准的Encoder-Decoder模型，并且构建了无监督/有监督的文本生成预训练任务，最终将效果推向了一个新高度。</p>
<h3 id="_1">训练<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h3>
<p>T5的预训练包含无监督和有监督两部分。无监督部分使用的是Google构建的近800G的语料（论文称之为C4），而训练目标则跟BERT类似，只不过改成了Seq2Seq版本，我们可以将它看成一个高级版的完形填空问题：</p>
<blockquote>
<p><strong>输入：</strong> 明月几时有，[M0]问青天，不知[M1]，今夕是何年？我欲[M2]归去，又恐琼楼玉宇，高处[M3]；起舞[M4]清影，何似在人间。<br />
<strong>输出：</strong>[M0]把酒[M1]天上宫阙[M2]乘风[M3]不胜寒[M4]弄</p>
</blockquote>
<p>而有监督部分，则是收集了常见的NLP监督任务数据，并也统一转化为SeqSeq任务来训练。比如情感分类可以这样转化：</p>
<blockquote>
<p><strong>输入：</strong> 识别该句子的情感倾向：这趟北京之旅我感觉很不错。<br />
<strong>输出：</strong> 正面</p>
</blockquote>
<p>主题分类可以这样转化：</p>
<blockquote>
<p><strong>输入：</strong> 下面是一则什么新闻？八个月了，终于又能在赛场上看到女排姑娘们了。<br />
<strong>输出：</strong> 体育</p>
</blockquote>
<p>阅读理解可以这样转化：</p>
<blockquote>
<p><strong>输入：</strong> 阅读理解：特朗普与拜登共同竞选下一任美国总统。根据上述信息回答问题：特朗普是哪国人？<br />
<strong>输出：</strong> 美国</p>
</blockquote>
<p>可以看到，这种转化跟GPT2、GPT3、PET的思想都是一致的，都是希望用文字把我们要做的任务表达出来，然后都转化为文字的预测，读者还可以翻看旧作<a href="/archives/7764">《必须要GPT3吗？不，BERT的MLM模型也能小样本学习》</a>了解相关内容。总的来说，在我们的内部实验里边，模型够大、数据够多以及有监督预训练都是T5成功的关键因素，“万事皆可Seq2Seq”则提供了有效地融合这些关键因素的方案。</p>
<h3 id="_2">结果<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h3>
<p>T5的主要战绩汇总如下表：  </p>
<p><a href="/usr/uploads/2020/11/358540148.png" title="点击查看原图"><img alt="T5的战绩汇总" src="/usr/uploads/2020/11/358540148.png" /></a></p>
<p>T5的战绩汇总</p>
<p>除了屠了多个榜单之外，T5还对整个训练流程中很多可调的超参数都调试了一遍，比如模型架构究竟用标准的Encoder-Decoder好还是UniLM那种结构好，无监督预训练任务究竟是BERT的方式好还是其他方式好，随机Mask的比例是不是15%最好，等等，最后给出了如下的表格，并还很遗憾地表达了一句“其实我们觉得T5的实验做得还不是很充分”，颇有一种“走别人的路，让别人无路可走”的感觉。当然，不管怎样，这些炼丹结果还是值得每一位要做语言模型的同学好好看看，或许能让我们少走一些弯路。</p>
<p><a href="/usr/uploads/2020/11/8208956.png" title="点击查看原图"><img alt="T5那巨细无遗的“炼丹宝典”（点击可以看大图）" src="/usr/uploads/2020/11/8208956.png" /></a></p>
<p>T5那巨细无遗的“炼丹宝典”（点击可以看大图）</p>
<h2 id="mt5">mT5<a class="toc-link" href="#mt5" title="Permanent link">&para;</a></h2>
<p>至于mT5，即Multilingual T5，T5的多国语言版，出自最近的论文<a href="https://papers.cool/arxiv/2010.11934">《mT5: A massively multilingual pre-trained text-to-text transformer》</a>，Github为<a href="https://github.com/google-research/multilingual-t5">multilingual-t5</a>，这也是将多语种NLP任务的榜单推到了一个新高度了。当然，对我们来说，最重要的是mT5里边包含了中文，因此我们终于有机会在中文任务中尝试下T5了。</p>
<h3 id="t511">T5.1.1<a class="toc-link" href="#t511" title="Permanent link">&para;</a></h3>
<p>总的来说，mT5跟T5一脉相承的，整体基本一样，但在模型结构方面，mT5使用的是T5.1.1方案，在此对它做个基本的介绍。</p>
<p>很多人都不知道的是，自从在去年10月发布后，T5在今年还经历了一次低调的小升级，具体细节可以查看<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md">Github链接</a>，官方把升级前的T5称为T5.1.0，而升级后的叫做T5.1.1。它主要的改动来自论文<a href="https://papers.cool/arxiv/2002.05202">《GLU Variants Improve Transformer》</a>，主要是借用了<a href="https://papers.cool/arxiv/1612.08083">《Language Modeling with Gated Convolutional Networks》</a>的GLU（Gated Linear Unit）来增强FFN部分的效果。具体来说，原来T5的FFN为（T5没有Bias）<br />
\begin{equation}\text{FFN}(x)=\text{relu}(xW_1)W_2\end{equation}<br />
现在改为了<br />
\begin{equation}\text{FFN}_{\text{GEGLU}}(x)=\big(\text{gelu}(xW_1)\otimes xW_2\big)W_3\end{equation}<br />
也就是把relu激活的第一个变化层改为了gelu激活的门控线性单元，这样FFN层增加了50%参数，但是从论文效果看效果明显增加。此外，T5.1.1还对Embedding层做了改动，原来在T5.1.0中，Encoder和Decoder的Embedding层、Decoder最后预测概率分布的Softmax层都是共享同一个Embedding矩阵的，现在T5.1.1只让Encoder和Decoder的Embedding层共享，而Decoder最后预测概率分布的Softmax层则用了一个独立的Embedding矩阵，当然这会让参数量大大增加，但Google的结论说这样做效果会更好，其结论被总结在最近的论文<a href="https://papers.cool/arxiv/2010.12821">《Rethinking embedding coupling in pre-trained language models》</a>中。还有最后一点改动，T5.1.1在预训练阶段去掉了Dropout，而只有在下游微调阶段才使用Dropout。</p>
<p>经过这些调整后，Google重新训练并开放了全系列的T5.1.1模型，其下载地址可以在刚才的Github链接找到，注意T5.1.1只做了无监督预训练，但效果依然相当出色。由于T5.1.1提升明显，所以mT5也就继续使用了T5.1.1结构了</p>
<h3 id="_3">结果<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h3>
<p>mT5其实就是重新构建了多国语言版的数据集mC4，然后使用T5.1.1方案训练了一波，技术路线上没有什么明显的创新。关于训练细节，大家观察下原论文就好，论文其实也不长，毕竟T5已经把路都给铺好了。</p>
<p>至于mT5的战绩，主要就是集中在下面这张表内了：  </p>
<p><a href="/usr/uploads/2020/11/3733950397.png" title="点击查看原图"><img alt="mT5的“战绩”" src="/usr/uploads/2020/11/3733950397.png" /></a></p>
<p>mT5的“战绩”</p>
<p>读者可能会有疑问，这种多国语言版的该用什么方式评测？简单的话，我们可以直接在此基础上finetune一个跨语种的机器翻译任务，看看效果的提升。但事实上，对于多国语言版模型，研究人员更关心的是它在跨语种任务上的Zero Shot表现，说白了，就是同一种任务，在一个语种上进行finetune，其模型能不能直接用于其余语种？这也是上图中“Cross-lingual zero-shot transfer (models fine-tuned on English data only)”的含义了，可以看到，mT5的表现还是相当出色的。</p>
<h2 id="_4">实践<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>终于到了大家喜闻乐见的实践时间了，这里我们简单介绍一下在bert4keras上使用mT5模型来做中文文本生成任务的流程和技巧。bert4keras从0.9.1版本开始支持调用mT5模型，如果要进行下述实验的读者，请先将bert4keras升级到0.9.1版或以上。</p>
<p><strong>Github链接：<a href="https://github.com/bojone/t5_in_bert4keras">https://github.com/bojone/t5_in_bert4keras</a></strong></p>
<h3 id="_5">基本<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h3>
<p>用bert4keras把mT5模型加载到keras中的基本代码为</p>
<div class="highlight"><pre><span></span><code><span class="gh">#</span> 模型路径
config_path = &#39;/root/kg/bert/mt5/mt5_small/t5_config.json&#39;
checkpoint_path = &#39;/root/kg/bert/mt5/mt5_small/model.ckpt-1000000&#39;
spm_path = &#39;/root/kg/bert/mt5/sentencepiece.model&#39;

<span class="gh">#</span> 加载分词器
tokenizer = SpTokenizer(spm_path, token_start=None, token_end=&#39;&lt;/s&gt;&#39;)

<span class="gh">#</span> 加载模型
t5 = build_transformer_model(
    config_path=config_path,
    checkpoint_path=checkpoint_path,
    model=&#39;t5.1.1&#39;,
    return_keras_model=False,
    name=&#39;T5&#39;,
)

encoder = t5.encoder
decoder = t5.decoder
model = t5.model
</code></pre></div>

<p>可以看到跟在bert4keras中加载BERT没太大区别，其中<code>t5_config.json</code>的构建、<code>model.ckpt-1000000</code>的下载在Github上都有详细介绍，大家请移步去看。完整代码（训练和解码细节）在Github上也可以找到，这里就不展开了。</p>
<p>值得一提的是，对于中文来说，tokenizer给出的结果是带有词的，即对于中文来说mT5是以词为单位的，只不过词颗粒度会比较少。这进一步说明了我们之前的工作<a href="/archives/7758">《提速不掉点：基于词颗粒度的中文WoBERT</a>》的改进方向是正确的。</p>
<h3 id="_6">中文<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h3>
<p>相信看本博客的读者多数都只关心中文任务，部分读者可能也会关心英文任务，应该鲜有读者会关心中英文以外的任务了。然而，mT5涵盖了101种语言，总词表有25万，而且它采用的T5.1.1结构的Softmax还不共享参数，这就导致了Embedding层占用了相当多的参数量，比如mT5 small的参数量为3亿，其中Embedding相关的就占了2.5亿，关键是里边的大部分参数我们都用不上，纯粹是不必要的浪费。因此，对于主要关心中文任务的我们来说，有必要精简一下这个Embedding层了。</p>
<p>对模型的精简很简单，只需要在两个Embedding矩阵中删除不需要的行就行了，关键在于如何决定要保留的token，以及如何得到一个精简后的sentencepiece模型。决定要保留的token，简单来想就是把中文的token保留下来，但是也不只是中文，英文的也要保留一部分，看上去似乎只是一个正则表达式的问题，实际上没那么简单，用英文字母的也不一定是英语，用中文字的也不一定是中文，这是个让人纠结的事情。于是笔者想了另外一个办法：用这个25万token的tokenizer对笔者收集的几十G中文语料分词，统计分词结果，然后按照词频选择前面的部分（最后保留了3万多个token）。这样虽然费时一些，但是比较靠谱，能确保把我们比较需要的token保留下来。决定词表后，就要修改得到一个新的sentencepiece模型，这也有点麻烦，但最终经过搜索后还是把这个事情解决了，处理方法都分享在Github上。</p>
<p>经过这样处理后，要构建新的模型，则只需要多加三行代码<code>keep_tokens</code>相关的代码，所需要的显存就大大降低，并且中文生成的效果基本不变了：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 模型路径</span>
<span class="n">config_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;/root/kg/bert/mt5/mt5_base/t5_config.json&#39;</span>
<span class="n">checkpoint_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;/root/kg/bert/mt5/mt5_base/model.ckpt-1000000&#39;</span>
<span class="n">spm_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;/root/kg/bert/mt5/sentencepiece_cn.model&#39;</span>
<span class="n">keep_tokens_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;/root/kg/bert/mt5/sentencepiece_cn_keep_tokens.json&#39;</span>

<span class="c1"># 加载分词器</span>
<span class="n">tokenizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SpTokenizer</span><span class="p">(</span><span class="n">spm_path</span><span class="p">,</span><span class="w"> </span><span class="n">token_start</span><span class="o">=</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">token_end</span><span class="o">=</span><span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">)</span>
<span class="n">keep_tokens</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">open</span><span class="p">(</span><span class="n">keep_tokens_path</span><span class="p">))</span>

<span class="c1"># 加载模型</span>
<span class="n">t5</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">build_transformer_model</span><span class="p">(</span>
<span class="w">    </span><span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
<span class="w">    </span><span class="n">checkpoint_path</span><span class="o">=</span><span class="n">checkpoint_path</span><span class="p">,</span>
<span class="w">    </span><span class="n">keep_tokens</span><span class="o">=</span><span class="n">keep_tokens</span><span class="p">,</span>
<span class="w">    </span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;t5.1.1&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="n">return_keras_model</span><span class="o">=</span><span class="n">False</span><span class="p">,</span>
<span class="w">    </span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;T5&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">encoder</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t5</span><span class="o">.</span><span class="n">encoder</span>
<span class="n">decoder</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t5</span><span class="o">.</span><span class="n">decoder</span>
<span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">t5</span><span class="o">.</span><span class="n">model</span>
</code></pre></div>

<h3 id="_7">效果<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<p>最后，大家应该是关心折腾了这么久，生成效果究竟有没有提升，有没有使用的价值？这样说吧，用mT5 small版本finetune出来的CSL标题生成模型，BLEU指标能持平基于WoBERT的UniLM模型，并且解码速度快130%；而用mT5 base版本finetune出来的CSL标题生成模型，指标能超过基于WoBERT的UniLM模型1%以上，并且解码速度也能快60%。</p>
<p>\begin{array}{c}
\text{CSL摘要生成实验结果 (beam size=1)}\\
{\begin{array}{c|cccc|c}
\hline
&amp; \text{Rouge-L} &amp; \text{Rouge-1} &amp; \text{Rouge-2} &amp; \text{BLEU} &amp; \text{解码速度}\\
\hline
\text{BERT base} &amp; 63.81 &amp; 65.45 &amp; 54.91 &amp; 45.52 &amp; \text{1x}\\
\text{WoBERT base} &amp; 66.38 &amp; 68.22 &amp; 57.83 &amp; 47.76 &amp; \text{1.1x}\\
\hline
\text{mT5 small} &amp; 65.14 &amp; 67.08 &amp; 56.71 &amp; 47.69 &amp; \text{2.3x}\\
\text{mT5 base} &amp; \textbf{66.81} &amp; \textbf{68.94} &amp; \textbf{58.49} &amp; \textbf{49.49} &amp; \text{1.6x}\\
\hline
\end{array}}<br />
\end{array}</p>
<p>说白了，确实是又快又好。至于设备要求，平时跑过BERT base的同学，基本都应该能跑起mT5 small/base版，甚至large版也可以尝试一下，至于XL和XXL，那就比较难搞了，建议还是放弃吧。更多的惊喜，还是大家自己去挖掘吧～～对了，顺便需要提醒一下，微调T5模型的时候，学习率要比微调BERT大10倍以上才行（即$10^{-4}$级别，BERT一般是$10^{-5}$级别），这是两者模型架构差异决定的。</p>
<h2 id="_8">小结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>本文回顾了一下Google去年发布的T5模型，然后介绍了最近发布的多国语言版的mT5，最后介绍了如何在bert4keras中微调mT5来做中文任务，结果显示mT5在中文生成上有着很不错的表现，值得做文本生成任务的同学一试。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/7867">https://spaces.ac.cn/archives/7867</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Nov. 06, 2020). 《那个屠榜的T5模型，现在可以在中文上玩玩了 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/7867">https://spaces.ac.cn/archives/7867</a></p>
<p>@online{kexuefm-7867,<br />
title={那个屠榜的T5模型，现在可以在中文上玩玩了},<br />
author={苏剑林},<br />
year={2020},<br />
month={Nov},<br />
url={\url{https://spaces.ac.cn/archives/7867}},<br />
} </p>
<hr />
<h2 id="_9">公式推导与注释<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="wgan的成功可能跟wasserstein距离没啥关系.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#35 WGAN的成功，可能跟Wasserstein距离没啥关系</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="曾被嫌弃的预训练任务nsp做出了优秀的zero-shot效果.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#37 曾被嫌弃的预训练任务NSP，做出了优秀的Zero Shot效果</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#t5">那个屠榜的T5模型，现在可以在中文上玩玩了</a><ul>
<li><a href="#t5_1">T5</a><ul>
<li><a href="#_1">训练</a></li>
<li><a href="#_2">结果</a></li>
</ul>
</li>
<li><a href="#mt5">mT5</a><ul>
<li><a href="#t511">T5.1.1</a></li>
<li><a href="#_3">结果</a></li>
</ul>
</li>
<li><a href="#_4">实践</a><ul>
<li><a href="#_5">基本</a></li>
<li><a href="#_6">中文</a></li>
<li><a href="#_7">效果</a></li>
</ul>
</li>
<li><a href="#_8">小结</a></li>
<li><a href="#_9">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>