<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪 | ML & Math Blog Posts</title>
    <meta name="description" content="生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪&para;
原文链接: https://spaces.ac.cn/archives/9164
发布日期: 

到目前为止，笔者给出了生成扩散模型DDPM的两种推导，分别是《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》中的通俗类比方案和《生成扩散模型漫谈（二）：DDPM = 自回归式VAE》中的变分自编码器方案。两种方案可谓各有特点，...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=概率">概率</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #8 生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#8</span>
                生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/9164" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=概率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 概率</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=DDPM" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> DDPM</span>
                </a>
                
                <a href="../index.html?tags=扩散" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 扩散</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="ddpm">生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪<a class="toc-link" href="#ddpm" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9164">https://spaces.ac.cn/archives/9164</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>到目前为止，笔者给出了生成扩散模型DDPM的两种推导，分别是<a href="/archives/9119">《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》</a>中的通俗类比方案和<a href="/archives/9152">《生成扩散模型漫谈（二）：DDPM = 自回归式VAE》</a>中的变分自编码器方案。两种方案可谓各有特点，前者更为直白易懂，但无法做更多的理论延伸和定量理解，后者理论分析上更加完备一些，但稍显形式化，启发性不足。</p>
<p><a href="/usr/uploads/2022/07/3685027055.jpeg" title="点击查看原图"><img alt="贝叶斯定理（来自维基百科）" src="/usr/uploads/2022/07/3685027055.jpeg" /></a></p>
<p>贝叶斯定理（来自维基百科）</p>
<p>在这篇文章中，我们再分享DDPM的一种推导，它主要利用到了贝叶斯定理来简化计算，整个过程的“推敲”味道颇浓，很有启发性。不仅如此，它还跟我们后面将要介绍的<a href="https://papers.cool/arxiv/2010.02502">DDIM模型</a>有着紧密的联系。</p>
<h2 id="_1">模型绘景<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>再次回顾，DDPM建模的是如下变换流程：<br />
\begin{equation}\boldsymbol{x} = \boldsymbol{x}<em T-1="T-1">0 \rightleftharpoons \boldsymbol{x}_1 \rightleftharpoons \boldsymbol{x}_2 \rightleftharpoons \cdots \rightleftharpoons \boldsymbol{x}</em>} \rightleftharpoons \boldsymbol{x}_T = \boldsymbol{z}\end{equation<br />
其中，正向就是将样本数据$\boldsymbol{x}$逐渐变为随机噪声$\boldsymbol{z}$的过程，反向就是将随机噪声$\boldsymbol{z}$逐渐变为样本数据$\boldsymbol{x}$的过程，反向过程就是我们希望得到的“生成模型”。</p>
<p>正向过程很简单，每一步是<br />
\begin{equation}\boldsymbol{x}<em t-1="t-1">t = \alpha_t \boldsymbol{x}</em>} + \beta_t \boldsymbol{\varepsilon<em t-1="t-1">t,\quad \boldsymbol{\varepsilon}_t\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\end{equation}<br />
或者写成$p(\boldsymbol{x}_t|\boldsymbol{x}</em>})=\mathcal{N}(\boldsymbol{x<em t-1="t-1">t;\alpha_t \boldsymbol{x}</em>)$。在约束$\alpha_t^2 + \beta_t^2 = 1$之下，我们有},\beta_t^2 \boldsymbol{I<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{x}<em t-1="t-1">t =&amp;\, \alpha_t \boldsymbol{x}</em>} + \beta_t \boldsymbol{\varepsilon<em t-1="t-1">t \\<br />
=&amp;\, \alpha_t \big(\alpha</em>} \boldsymbol{x<em t-1="t-1">{t-2} + \beta</em>} \boldsymbol{\varepsilon<em t-1="t-1">{t-1}\big) + \beta_t \boldsymbol{\varepsilon}_t \\<br />
=&amp;\,\cdots\\<br />
=&amp;\,(\alpha_t\cdots\alpha_1) \boldsymbol{x}_0 + \underbrace{(\alpha_t\cdots\alpha_2)\beta_1 \boldsymbol{\varepsilon}_1 + (\alpha_t\cdots\alpha_3)\beta_2 \boldsymbol{\varepsilon}_2 + \cdots + \alpha_t\beta</em>} \boldsymbol{\varepsilon<em _mathcal_N="\mathcal{N" _sim="\sim">{t-1} + \beta_t \boldsymbol{\varepsilon}_t}</em>}(\boldsymbol{0}, (1-\alpha_t^2\cdots\alpha_1^2)\boldsymbol{I})<br />
\end{aligned}\end{equation}<br />
从而可以求出$p(\boldsymbol{x}_t|\boldsymbol{x}_0)=\mathcal{N}(\boldsymbol{x}_t;\bar{\alpha}_t \boldsymbol{x}_0,\bar{\beta}_t^2 \boldsymbol{I})$，其中$\bar{\alpha}_t = \alpha_1\cdots\alpha_t$，而$\bar{\beta}_t = \sqrt{1-\bar{\alpha}_t^2}$。</p>
<p>DDPM要做的事情，就是从上述信息中求出反向过程所需要的$p(\boldsymbol{x}<em T-1="T-1">{t-1}|\boldsymbol{x}_t)$，这样我们就能实现从任意一个$\boldsymbol{x}_T=\boldsymbol{z}$出发，逐步采样出$\boldsymbol{x}</em>$。},\boldsymbol{x}_{T-2},\cdots,\boldsymbol{x}_1$，最后得到随机生成的样本数据$\boldsymbol{x}_0=\boldsymbol{x</p>
<h2 id="_2">请贝叶斯<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>下面我们请出伟大的<a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">贝叶斯定理</a>。事实上，直接根据贝叶斯定理我们有<br />
\begin{equation}p(\boldsymbol{x}<em t-1="t-1">{t-1}|\boldsymbol{x}_t) = \frac{p(\boldsymbol{x}_t|\boldsymbol{x}</em>})p(\boldsymbol{x<em t-1="t-1">{t-1})}{p(\boldsymbol{x}_t)}\label{eq:bayes}\end{equation}<br />
然而，我们并不知道$p(\boldsymbol{x}</em>}),p(\boldsymbol{x<em t-1="t-1">t)$的表达式，所以此路不通。但我们可以退而求其次，在给定$\boldsymbol{x}_0$的条件下使用贝叶斯定理：<br />
\begin{equation}p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0) = \frac{p(\boldsymbol{x}_t|\boldsymbol{x}</em>})p(\boldsymbol{x<em t-1="t-1">{t-1}|\boldsymbol{x}_0)}{p(\boldsymbol{x}_t|\boldsymbol{x}_0)}\end{equation}<br />
这样修改自然是因为$p(\boldsymbol{x}_t|\boldsymbol{x}</em>}),p(\boldsymbol{x<em t-1="t-1">{t-1}|\boldsymbol{x}_0),p(\boldsymbol{x}_t|\boldsymbol{x}_0)$都是已知的，所以上式是可计算的，代入各自的表达式得到：<br />
\begin{equation}p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0) = \mathcal{N}\left(\boldsymbol{x}</em>};\frac{\alpha_t\bar{\beta<em t-1="t-1">{t-1}^2}{\bar{\beta}_t^2}\boldsymbol{x}_t + \frac{\bar{\alpha}</em>}\beta_t^2}{\bar{\beta<em t-1="t-1">t^2}\boldsymbol{x}_0,\frac{\bar{\beta}</em>}^2\beta_t^2}{\bar{\beta}_t^2} \boldsymbol{I}\right)\label{eq:p-xt-x0}\end{equation</p>
<blockquote>
<p><strong>推导：</strong> 上式的推导过程并不难，就是常规的展开整理而已，当然我们也可以找点技巧加快计算。首先，代入各自的表达式，可以发现指数部分除掉$-1/2$因子外，结果是：<br />
 \begin{equation}\frac{\Vert \boldsymbol{x}<em t-1="t-1">t - \alpha_t \boldsymbol{x}</em>}\Vert^2}{\beta_t^2} + \frac{\Vert \boldsymbol{x<em t-1="t-1">{t-1} - \bar{\alpha}</em>}\boldsymbol{x<em t-1="t-1">0\Vert^2}{\bar{\beta}</em>}^2} - \frac{\Vert \boldsymbol{x<em t-1="t-1">t - \bar{\alpha}_t \boldsymbol{x}_0\Vert^2}{\bar{\beta}_t^2}\end{equation}<br />
 它关于$\boldsymbol{x}</em>}$是二次的，因此最终的分布必然也是正态分布，我们只需要求出其均值和协方差。不难看出，展开式中$\Vert \boldsymbol{x<em t-1="t-1">{t-1}\Vert^2$项的系数是<br />
 \begin{equation}\frac{\alpha_t^2}{\beta_t^2} + \frac{1}{\bar{\beta}</em>}^2} = \frac{\alpha_t^2\bar{\beta<em t-1="t-1">{t-1}^2 + \beta_t^2}{\bar{\beta}</em>}^2 \beta_t^2} = \frac{\alpha_t^2(1-\bar{\alpha<em t-1="t-1">{t-1}^2) + \beta_t^2}{\bar{\beta}</em>}^2 \beta_t^2} = \frac{1-\bar{\alpha<em t-1="t-1">t^2}{\bar{\beta}</em>}^2 \beta_t^2} = \frac{\bar{\beta<em t-1="t-1">t^2}{\bar{\beta}</em>}^2 \beta_t^2}\end{equation<br />
 所以整理好的结果必然是$\frac{\bar{\beta}<em t-1="t-1">t^2}{\bar{\beta}</em>}^2 \beta_t^2}\Vert \boldsymbol{x<em t-1="t-1">{t-1} - \tilde{\boldsymbol{\mu}}(\boldsymbol{x}_t, \boldsymbol{x}_0)\Vert^2$的形式，这意味着协方差矩阵是$\frac{\bar{\beta}</em>}^2 \beta_t^2}{\bar{\beta<em t-1="t-1">t^2}\boldsymbol{I}$。另一边，把一次项系数拿出来是$-2\left(\frac{\alpha_t}{\beta_t^2}\boldsymbol{x}_t + \frac{\bar{\alpha}</em>}}{\bar{\beta<em t-1="t-1">{t-1}^2}\boldsymbol{x}_0 \right)$，除以$\frac{-2\bar{\beta}_t^2}{\bar{\beta}</em>$后便可以得到}^2 \beta_t^2<br />
 \begin{equation}\tilde{\boldsymbol{\mu}}(\boldsymbol{x}<em t-1="t-1">t, \boldsymbol{x}_0)=\frac{\alpha_t\bar{\beta}</em>}^2}{\bar{\beta<em t-1="t-1">t^2}\boldsymbol{x}_t + \frac{\bar{\alpha}</em>}\beta_t^2}{\bar{\beta<em t-1="t-1">t^2}\boldsymbol{x}_0 \end{equation}<br />
 这就得到了$p(\boldsymbol{x}</em>$。}|\boldsymbol{x}_t, \boldsymbol{x}_0)$的所有信息了，结果正是式$\eqref{eq:p-xt-x0</p>
</blockquote>
<h2 id="_3">去噪过程<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>现在我们得到了$p(\boldsymbol{x}<em t-1="t-1">{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0)$，它有显式的解，但并非我们想要的最终答案，因为我们只想通过$\boldsymbol{x}_t$来预测$\boldsymbol{x}</em>_0$是我们最终想要生成的结果。接下来，一个“异想天开”的想法是}$，而不能依赖$\boldsymbol{x}_0$，$\boldsymbol{x</p>
<blockquote>
<p>如果我们能够通过$\boldsymbol{x}<em t-1="t-1">t$来预测$\boldsymbol{x}_0$，那么不就可以消去$p(\boldsymbol{x}</em>_t$了吗？}|\boldsymbol{x}_t, \boldsymbol{x}_0)$中的$\boldsymbol{x}_0$，使得它只依赖于$\boldsymbol{x</p>
</blockquote>
<p>说干就干，我们用$\bar{\boldsymbol{\mu}}(\boldsymbol{x}<em t-1="t-1">t)$来预估$\boldsymbol{x}_0$，损失函数为$\Vert \boldsymbol{x}_0 - \bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)\Vert^2$。训练完成后，我们就认为<br />
\begin{equation}p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t) \approx p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0=\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)) = \mathcal{N}\left(\boldsymbol{x}</em>}; \frac{\alpha_t\bar{\beta<em t-1="t-1">{t-1}^2}{\bar{\beta}_t^2}\boldsymbol{x}_t + \frac{\bar{\alpha}</em>}\beta_t^2}{\bar{\beta<em t-1="t-1">t^2}\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t),\frac{\bar{\beta}</em>}^2\beta_t^2}{\bar{\beta}_t^2} \boldsymbol{I}\right)\label{eq:p-xt}\end{equation<br />
在$\Vert \boldsymbol{x}_0 - \bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)\Vert^2$中，$\boldsymbol{x}_0$代表原始数据，$\boldsymbol{x}_t$代表带噪数据，所以这实际上在训练一个去噪模型，这也就是DDPM的第一个“D”的含义（Denoising）。</p>
<p>具体来说，$p(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t|\boldsymbol{x}_0)=\mathcal{N}(\boldsymbol{x}_t;\bar{\alpha}_t \boldsymbol{x}_0,\bar{\beta}_t^2 \boldsymbol{I})$意味着$\boldsymbol{x}_t = \bar{\alpha}_t \boldsymbol{x}_0 + \bar{\beta}_t \boldsymbol{\varepsilon},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$，或者写成$\boldsymbol{x}_0 = \frac{1}{\bar{\alpha}_t}\left(\boldsymbol{x}_t - \bar{\beta}_t \boldsymbol{\varepsilon}\right)$，这启发我们将$\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)$参数化为<br />
\begin{equation}\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t) = \frac{1}{\bar{\alpha}_t}\left(\boldsymbol{x}_t - \bar{\beta}_t \boldsymbol{\epsilon}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)\right)\label{eq:bar-mu}\end{equation}<br />
此时损失函数变为<br />
\begin{equation}\Vert \boldsymbol{x}_0 - \bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)\Vert^2 = \frac{\bar{\beta}_t^2}{\bar{\alpha}_t^2}\left\Vert\boldsymbol{\varepsilon} - \boldsymbol{\epsilon}</em>}}(\bar{\alpha<em t-1="t-1">t \boldsymbol{x}_0 + \bar{\beta}_t \boldsymbol{\varepsilon}, t)\right\Vert^2\end{equation}<br />
省去前面的系数，就得到DDPM原论文所用的损失函数了。可以发现，本文是直接得出了从$\boldsymbol{x}_t$到$\boldsymbol{x}_0$的去噪过程，而不是像之前两篇文章那样，通过$\boldsymbol{x}_t$到$\boldsymbol{x}</em>$的去噪过程再加上积分变换来推导，相比之下本文的推导可谓更加一步到位了。</p>
<p>另一边，我们将式$\eqref{eq:bar-mu}$代入到式$\eqref{eq:p-xt}$中，化简得到<br />
\begin{equation}<br />
p(\boldsymbol{x}<em t-1="t-1">{t-1}|\boldsymbol{x}_t) \approx p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0=\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)) = \mathcal{N}\left(\boldsymbol{x}</em>}; \frac{1}{\alpha_t}\left(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t - \frac{\beta_t^2}{\bar{\beta}_t}\boldsymbol{\epsilon}</em>}}(\boldsymbol{x<em t-1="t-1">t, t)\right),\frac{\bar{\beta}</em>}^2\beta_t^2}{\bar{\beta<em _boldsymbol_theta="\boldsymbol{\theta">t^2} \boldsymbol{I}\right)\end{equation}<br />
这就是反向的采样过程所用的分布，连同采样过程所用的方差也一并确定下来了。至此，DDPM推导完毕～（注：出于推导的流畅性考虑，本文的$\boldsymbol{\epsilon}</em>$跟前两篇介绍不一样，反而跟DDPM原论文一致。）}</p>
<blockquote>
<p><strong>推导：</strong> 将式$\eqref{eq:bar-mu}$代入到式$\eqref{eq:p-xt}$的主要化简难度就是计算<br />
 \begin{equation}\begin{aligned}\frac{\alpha_t\bar{\beta}<em t-1="t-1">{t-1}^2}{\bar{\beta}_t^2} + \frac{\bar{\alpha}</em>}\beta_t^2}{\bar{\alpha<em t-1="t-1">t\bar{\beta}_t^2} =&amp;\, \frac{\alpha_t\bar{\beta}</em>}^2 + \beta_t^2/\alpha_t}{\bar{\beta<em t-1="t-1">t^2} = \frac{\alpha_t^2(1-\bar{\alpha}</em>}^2) + \beta_t^2}{\alpha_t\bar{\beta}_t^2} = \frac{1-\bar{\alpha}_t^2}{\alpha_t\bar{\beta}_t^2} = \frac{1}{\alpha_t<br />
 \end{aligned}\end{equation}</p>
</blockquote>
<h2 id="_4">预估修正<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>不知道读者有没有留意到一个有趣的地方：我们要做的事情，就是想将$\boldsymbol{x}<em t-1="t-1">T$慢慢地变为$\boldsymbol{x}_0$，而我们在借用$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0)$近似$p(\boldsymbol{x}</em>_0$”这一步，要是能预估准的话，那就直接一步到位了，还需要逐步采样吗？}|\boldsymbol{x}_t)$时，却包含了“用$\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)$来预估$\boldsymbol{x</p>
<p>真实情况是，“用$\bar{\boldsymbol{\mu}}(\boldsymbol{x}<em t-1="t-1">t)$来预估$\boldsymbol{x}_0$”当然不会太准的，至少开始的相当多步内不会太准。它仅仅起到了一个前瞻性的预估作用，然后我们只用$p(\boldsymbol{x}</em>_t)$来推进一小步，这就是很多数值算法中的“预估-修正”思想，即我们用一个粗糙的解往前推很多步，然后利用这个粗糙的结果将最终结果推进一小步，以此来逐步获得更为精细的解。}|\boldsymbol{x</p>
<p>由此我们还可以联想到Hinton三年前提出的<a href="https://papers.cool/arxiv/1907.08610">《Lookahead Optimizer: k steps forward, 1 step back》</a>，它同样也包含了预估（k steps forward）和修正（1 step back）两部分，原论文将其诠释为“快（Fast）-慢（Slow）”权重的相互结合，快权重就是预估得到的结果，慢权重则是基于预估所做的修正结果。如果愿意，我们也可以用同样的方式去诠释DDPM的“预估-修正”过程～</p>
<h2 id="_5">遗留问题<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>最后，在使用贝叶斯定理一节中，我们说式$\eqref{eq:bayes}$没法直接用的原因是$p(\boldsymbol{x}_{t-1})$和$p(\boldsymbol{x}_t)$均不知道。因为根据定义，我们有<br />
\begin{equation}p(\boldsymbol{x}_t) = \int p(\boldsymbol{x}_t|\boldsymbol{x}_0)\tilde{p}(\boldsymbol{x}_0)d\boldsymbol{x}_0\end{equation}<br />
其中$p(\boldsymbol{x}_t|\boldsymbol{x}_0)$是知道的，而数据分布$\tilde{p}(\boldsymbol{x}_0)$无法提前预知，所以不能进行计算。不过，有两个特殊的例子，是可以直接将两者算出来的，这里我们也补充计算一下，其结果也正好是上一篇文章遗留的方差选取问题的答案。</p>
<p>第一个例子是整个数据集只有一个样本，不失一般性，假设该样本为$\boldsymbol{0}$，此时$\tilde{p}(\boldsymbol{x}<em t-1="t-1">0)$为狄拉克分布$\delta(\boldsymbol{x}_0)$，可以直接算出$p(\boldsymbol{x}_t)=p(\boldsymbol{x}_t|\boldsymbol{0})$。继而代入式$\eqref{eq:bayes}$，可以发现结果正好是$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t,\boldsymbol{x}_0)$取$\boldsymbol{x}_0=\boldsymbol{0}$的特例，即<br />
\begin{equation}p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t) = p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0=\boldsymbol{0}) = \mathcal{N}\left(\boldsymbol{x}</em>};\frac{\alpha_t\bar{\beta<em t-1="t-1">{t-1}^2}{\bar{\beta}_t^2}\boldsymbol{x}_t,\frac{\bar{\beta}</em>}^2\beta_t^2}{\bar{\beta<em t-1="t-1">t^2} \boldsymbol{I}\right)\end{equation}<br />
我们主要关心其方差为$\frac{\bar{\beta}</em>$，这便是采样方差的选择之一。}^2\beta_t^2}{\bar{\beta}_t^2</p>
<p>第二个例子是数据集服从标准正态分布，即$\tilde{p}(\boldsymbol{x}<em t-1="t-1">0)=\mathcal{N}(\boldsymbol{x}_0;\boldsymbol{0},\boldsymbol{I})$。前面我们说了$p(\boldsymbol{x}_t|\boldsymbol{x}_0)=\mathcal{N}(\boldsymbol{x}_t;\bar{\alpha}_t \boldsymbol{x}_0,\bar{\beta}_t^2 \boldsymbol{I})$意味着$\boldsymbol{x}_t = \bar{\alpha}_t \boldsymbol{x}_0 + \bar{\beta}_t \boldsymbol{\varepsilon},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$，而此时根据假设还有$\boldsymbol{x}_0\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$，所以由正态分布的叠加性，$\boldsymbol{x}_t$正好也服从标准正态分布。将标准正态分布的概率密度代入式$\eqref{eq:bayes}$后，结果的指数部分除掉$-1/2$因子外，结果是：<br />
\begin{equation}\frac{\Vert \boldsymbol{x}_t - \alpha_t \boldsymbol{x}</em>}\Vert^2}{\beta_t^2} + \Vert \boldsymbol{x<em t-1="t-1">{t-1}\Vert^2 - \Vert \boldsymbol{x}_t\Vert^2\end{equation}<br />
跟推导$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t,\boldsymbol{x}_0)$的过程类似，可以得到上述指数对应于<br />
\begin{equation}p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t) = \mathcal{N}\left(\boldsymbol{x}</em>};\alpha_t\boldsymbol{x}_t,\beta_t^2 \boldsymbol{I}\right)\end{equation<br />
我们同样主要关心其方差为$\beta_t^2$，这便是采样方差的另一个选择。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文分享了DDPM的一种颇有“推敲”味道的推导，它借助贝叶斯定理来直接推导反向的生成过程，相比之前的“拆楼-建楼”类比和变分推断理解更加一步到位。同时，它也更具启发性，跟接下来要介绍的DDIM有很密切的联系。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9164">https://spaces.ac.cn/archives/9164</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jul. 19, 2022). 《生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9164">https://spaces.ac.cn/archives/9164</a></p>
<p>@online{kexuefm-9164,<br />
title={生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪},<br />
author={苏剑林},<br />
year={2022},<br />
month={Jul},<br />
url={\url{https://spaces.ac.cn/archives/9164}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="让mathjax更好地兼容谷歌翻译和延时加载.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#7 让MathJax更好地兼容谷歌翻译和延时加载</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="relugeluswish的一个恒等式.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#9 ReLU/GeLU/Swish的一个恒等式</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#ddpm">生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪</a><ul>
<li><a href="#_1">模型绘景</a></li>
<li><a href="#_2">请贝叶斯</a></li>
<li><a href="#_3">去噪过程</a></li>
<li><a href="#_4">预估修正</a></li>
<li><a href="#_5">遗留问题</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>