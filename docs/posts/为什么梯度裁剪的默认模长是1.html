<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>为什么梯度裁剪的默认模长是1？ | ML & Math Blog Posts</title>
    <meta name="description" content="为什么梯度裁剪的默认模长是1？&para;
原文链接: https://spaces.ac.cn/archives/10657
发布日期: 

我们知道，梯度裁剪（Gradient Clipping）是让模型训练更加平稳的常用技巧。常用的梯度裁剪是根据所有参数的梯度总模长来对梯度进行裁剪，其运算可以表示为
\begin{equation}\text{clip}(\boldsymbol{g},\ta...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #158 为什么梯度裁剪的默认模长是1？
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#158</span>
                为什么梯度裁剪的默认模长是1？
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-01-02</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=学习率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 学习率</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="1">为什么梯度裁剪的默认模长是1？<a class="toc-link" href="#1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10657">https://spaces.ac.cn/archives/10657</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>我们知道，梯度裁剪（Gradient Clipping）是让模型训练更加平稳的常用技巧。常用的梯度裁剪是根据所有参数的梯度总模长来对梯度进行裁剪，其运算可以表示为<br />
\begin{equation}\text{clip}(\boldsymbol{g},\tau)=\left\{\begin{aligned}&amp;\boldsymbol{g}, &amp;\Vert\boldsymbol{g}\Vert\leq \tau \\
&amp;\frac{\tau}{\Vert\boldsymbol{g}\Vert}\boldsymbol{g},&amp;\Vert\boldsymbol{g}\Vert &gt; \tau
\end{aligned}\right.\end{equation}<br />
这样一来，$\text{clip}(\boldsymbol{g},\tau)$保持跟$\boldsymbol{g}$相同的方向，但模长不超过$\tau$。注意这里的$\Vert\boldsymbol{g}\Vert$是整个模型所有的参数梯度放在一起视为单个向量所算的模长，也就是所谓的Global Gradient Norm。</p>
<p>不知道大家有没有留意到一个细节：不管是数百万参数还是数百亿参数的模型，$\tau$的取值在很多时候都是1。这意味着什么呢？是单纯地复用默认值，还是背后隐含着什么深刻的原理呢？</p>
<h2 id="_1">是什么<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>可能有读者觉得，默认值又不一定是最优值，有什么值得纠结的？确实，$\tau=1$未必是最优的选择，但它是很多模型的默认选择，并且在这个默认选择下表现尚可，这反过来表明$\tau=1$具有普遍的合理性。</p>
<p>这里的“合理性”又指什么呢？让我们回到$\text{clip}$运算上。如果$\Vert\boldsymbol{g}\Vert$总是小于$\tau$，那么$\text{clip}$就退化为恒等变换了；如果$\Vert\boldsymbol{g}\Vert$总是大于$\tau$，那么$\text{clip}$就退化成L2归一化。换句话说，$\text{clip}$之所以为$\text{clip}$，就是因为$\tau$产生了适当的区分度，使得大部分的$\Vert\boldsymbol{g}\Vert$都是小于$\tau$的，只有小部分才是大于$\tau$的，这就是$\tau$的合理性的含义。</p>
<p>这个当然可以举出反例，而且还不少，这里主要想强调这个现象的普遍性以及这个默认设置的普适性，所以较真的读者大可不必过于执着于个别细节。</p>
<p>因此，我们认为，$\tau=1$的普遍合理性的含义，就是不论模型参数量多少、怎么初始化、取何种损失函数，它的梯度总模长都能恰好大致以$1$为“异常值”的分界点，这无疑是一个非常不可思议的性质——笔者第一次意识到这个结论时的感受便是如此。</p>
<h2 id="_2">为什么<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>为什么会如此“巧合”呢？笔者的答案可能会让人有些意外：因为只有这样，模型才有稳定训练的可能。</p>
<p>让我们考虑损失函数$\mathcal{L}(\boldsymbol{\theta})$，优化器更新规则为$\boldsymbol{\theta}<em t_1="t+1">{t+1} = \boldsymbol{\theta}_t - \eta\, \boldsymbol{u}_t$，那么损失函数的变化近似为<br />
\begin{equation}\Delta \mathcal{L} = \mathcal{L}(\boldsymbol{\theta}</em>}) - \mathcal{L}(\boldsymbol{\theta<em t_1="t+1">t) \approx (\boldsymbol{\theta}</em>} - \boldsymbol{\theta<em _boldsymbol_theta="\boldsymbol{\theta">t)\cdot\nabla</em>}_t}\mathcal{L}(\boldsymbol{\theta}) = -\eta\, \boldsymbol{u}_t\cdot \boldsymbol{g}_t\end{equation
先考虑最简单的SGD，那么$\boldsymbol{u}_t = \boldsymbol{g}_t$以及$\Delta \mathcal{L}=-\eta\Vert\boldsymbol{g}_t\Vert^2$，即损失函数的变化量正比于梯度模长的平方。我们知道，不管是CV还是NLP，纯粹的SGD（不带动量）都是非常低效的优化器，训练到中后期，平均来说多数任务每步的损失下降量是远不如学习率大小的，也就是$|\Delta \mathcal{L}| &lt; \eta$，由此推得$\Vert\boldsymbol{g}_t\Vert &lt; 1$。这就表明了$\Vert\boldsymbol{g}_t\Vert &lt; 1$是一个能正常收敛的模型的长期表现。</p>
<p>当然，训练初期模型有可能会出现$\Vert\boldsymbol{g}_t\Vert &gt; 1$，这是正常的，但很少情况会出现$\Vert\boldsymbol{g}_t\Vert \gg 1$，或者说一个优秀的初始化应该避免出现$\Vert\boldsymbol{g}_t\Vert \gg 1$，像<a href="/archives/8978">DeepNorm</a>等的理论依据便是如此。原因是相似的，如果梯度模长太大，那么前期的学习就会过于“激进”，导致提前收敛到不好的局部解。另一个方案是缩小$\eta$，这同样能够缩小$|\Delta \mathcal{L}|$，这也就是为什么在训练初期我们通常使用Warmup。</p>
<p>顺便说，关于Warmup的理解大家可以参考论文<a href="https://papers.cool/arxiv/2310.07831">《Optimal Linear Decay Learning Rate Schedules and Further Refinements》</a>，这是笔者认为对Warmup的最合理的分析。</p>
<h2 id="_3">怎么办<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>简单来说，就是由于损失函数的变化量正比于梯度模长的平方，所以训练的平稳性决定了梯度模长不能太大，并且长期表现为小于1。而初期如果出现明显大于1的梯度模长，那么通常的策略是Warmup。或者也可以考虑一个更通用的策略：设置另一个阈值$\mathcal{T}$，根据$\boldsymbol{u}_t\cdot \boldsymbol{g}_t$的值对$\eta$进行裁剪<br />
\begin{equation}\eta_t = \left\{\begin{aligned}&amp;\eta,&amp; \boldsymbol{u}_t\cdot \boldsymbol{g}_t\leq \mathcal{T} \\ &amp;\frac{\mathcal{T}}{\boldsymbol{u}_t\cdot \boldsymbol{g}_t}\eta,&amp; \boldsymbol{u}_t\cdot \boldsymbol{g}_t &gt; \mathcal{T}
\end{aligned}\right.\end{equation}<br />
这样就免除了额外的Warmup设置，更加具有自适应性。</p>
<p>对于Adam等优化器，我们可以跟<a href="/archives/10542">《当Batch Size增大时，学习率该如何随之变化？》</a>一样，通过$\boldsymbol{u}_t=\text{sign}(\boldsymbol{g}_t)$来进行近似分析，此时<br />
\begin{equation}\Delta \mathcal{L} = -\eta\, \text{sign}(\boldsymbol{g}_t)\cdot \boldsymbol{g}_t = -\eta\, \Vert\boldsymbol{g}_t\Vert_1\end{equation}<br />
这里的$\Vert\Vert_1$是L1范数，即分量的绝对值之和。由于梯度分量基本都小于1，因此$\Vert\boldsymbol{g}_t\Vert_1 \gg \Vert\boldsymbol{g}_t\Vert$，因此同样出于平稳训练的需求，Adam的学习率通常要明显小于SGD的学习率。此外，上式还可以改写成<br />
\begin{equation}\Delta \mathcal{L} = -\eta\, \text{sign}(\boldsymbol{g}_t)\cdot \boldsymbol{g}_t = -\eta\, \sqrt{N}\Vert\boldsymbol{g}_t\Vert \cos(\text{sign}(\boldsymbol{g}_t), \boldsymbol{g}_t) \end{equation}<br />
这里假设了$\boldsymbol{g}_t$没有零分量，因此$\Vert\text{sign}(\boldsymbol{g}_t)\Vert=\sqrt{N}$，$N$是模型总参数量。实践发现$\Vert\boldsymbol{g}_t\Vert$和$\cos(\text{sign}(\boldsymbol{g}_t), \boldsymbol{g}_t)$在不同的模型尺度下都大致为常数，因此如果要维持$\Delta \mathcal{L}$不变，应该有$\eta$反比于$\sqrt{N}$，也就是说模型参数量增加到4倍，那么学习率可以考虑减半。</p>
<h2 id="_4">全剧终<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>本文对“梯度裁剪的默认模长为1”这一现象给出了自己的一些看法和思考。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10657">https://spaces.ac.cn/archives/10657</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jan. 02, 2025). 《为什么梯度裁剪的默认模长是1？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10657">https://spaces.ac.cn/archives/10657</a></p>
<p>@online{kexuefm-10657,<br />
title={为什么梯度裁剪的默认模长是1？},<br />
author={苏剑林},<br />
year={2025},<br />
month={Jan},<br />
url={\url{https://spaces.ac.cn/archives/10657}},<br />
} </p>
<hr />
<h2 id="_5">公式推导与注释<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本节提供梯度裁剪理论的极详细数学推导，从优化理论、动力系统和实验分析等多个角度阐释为什么梯度裁剪的默认阈值是1。</p>
<h3 id="1_1">1. 梯度裁剪的数学定义<a class="toc-link" href="#1_1" title="Permanent link">&para;</a></h3>
<h4 id="11-l2">1.1 L2范数裁剪<a class="toc-link" href="#11-l2" title="Permanent link">&para;</a></h4>
<p>设神经网络的参数为 $\boldsymbol{\theta} \in \mathbb{R}^d$，其中 $d$ 是总参数量。在第 $t$ 步优化中，损失函数 $\mathcal{L}(\boldsymbol{\theta})$ 关于参数的梯度记为：</p>
<p>$$\boldsymbol{g}<em _boldsymbol_theta="\boldsymbol{\theta">t = \nabla</em>_t)$$}} \mathcal{L}(\boldsymbol{\theta</p>
<p><strong>L2范数梯度裁剪</strong>定义为：</p>
<p>$$\text{clip}(\boldsymbol{g}, \tau) = \begin{cases}
\boldsymbol{g}, &amp; |\boldsymbol{g}|_2 \leq \tau \
\frac{\tau}{|\boldsymbol{g}|_2} \boldsymbol{g}, &amp; |\boldsymbol{g}|_2 &gt; \tau
\end{cases}$$</p>
<p>其中 $\tau &gt; 0$ 是裁剪阈值（clipping threshold），$|\boldsymbol{g}|<em i="1">2 = \sqrt{\sum</em>$ 是梯度的L2范数。}^d g_i^2</p>
<p><strong>裁剪后的梯度性质</strong>：
1. <strong>方向保持</strong>：$\text{clip}(\boldsymbol{g}, \tau)$ 与 $\boldsymbol{g}$ 方向相同
2. <strong>有界性</strong>：$|\text{clip}(\boldsymbol{g}, \tau)|_2 \leq \tau$
3. <strong>连续性</strong>：裁剪操作在 $|\boldsymbol{g}|_2 = \tau$ 处连续</p>
<h4 id="12">1.2 裁剪操作的数学性质<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p>裁剪操作可以统一表示为：</p>
<p>$$\text{clip}(\boldsymbol{g}, \tau) = \boldsymbol{g} \cdot \min\left(1, \frac{\tau}{|\boldsymbol{g}|_2}\right)$$</p>
<p><strong>缩放因子</strong>定义为：</p>
<p>$$\lambda(\boldsymbol{g}, \tau) = \min\left(1, \frac{\tau}{|\boldsymbol{g}|_2}\right)$$</p>
<p>则裁剪操作可以写成 $\text{clip}(\boldsymbol{g}, \tau) = \lambda(\boldsymbol{g}, \tau) \cdot \boldsymbol{g}$。</p>
<p><strong>缩放因子的导数</strong>（对梯度范数）：</p>
<p>$$\frac{\partial \lambda}{\partial |\boldsymbol{g}|_2} = \begin{cases}
0, &amp; |\boldsymbol{g}|_2 &lt; \tau \
-\frac{\tau}{|\boldsymbol{g}|_2^2}, &amp; |\boldsymbol{g}|_2 &gt; \tau
\end{cases}$$</p>
<p>这表明裁剪操作在 $|\boldsymbol{g}|_2 &lt; \tau$ 时不改变梯度，在 $|\boldsymbol{g}|_2 &gt; \tau$ 时以 $1/|\boldsymbol{g}|_2^2$ 的速率减小缩放因子。</p>
<h3 id="2">2. 梯度爆炸的理论分析<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 循环神经网络中的梯度传播<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p>考虑标准的RNN：</p>
<p>$$\boldsymbol{h}<em t-1="t-1">t = \tanh(\boldsymbol{W}_h \boldsymbol{h}</em>)$$} + \boldsymbol{W}_x \boldsymbol{x}_t + \boldsymbol{b</p>
<p>对于序列长度为 $T$ 的任务，损失函数为 $\mathcal{L} = \sum_{t=1}^T \mathcal{L}_t(\boldsymbol{h}_t)$。</p>
<p><strong>梯度反向传播</strong>通过链式法则：</p>
<p>$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{h}<em t="k+1">k} = \frac{\partial \mathcal{L}_k}{\partial \boldsymbol{h}_k} + \sum</em>$$}^T \frac{\partial \mathcal{L}_t}{\partial \boldsymbol{h}_t} \frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{h}_k</p>
<p>其中关键的时间反向传播项为：</p>
<p>$$\frac{\partial \boldsymbol{h}<em i="k+1">t}{\partial \boldsymbol{h}_k} = \prod</em>}^t \frac{\partial \boldsymbol{h<em i-1="i-1">i}{\partial \boldsymbol{h}</em>_h$$}} = \prod_{i=k+1}^t \text{diag}(\tanh'(\boldsymbol{z}_i)) \boldsymbol{W</p>
<p>其中 $\boldsymbol{z}<em i-1="i-1">i = \boldsymbol{W}_h \boldsymbol{h}</em>$。} + \boldsymbol{W}_x \boldsymbol{x}_i + \boldsymbol{b</p>
<h4 id="22-jacobian">2.2 Jacobian矩阵的谱半径分析<a class="toc-link" href="#22-jacobian" title="Permanent link">&para;</a></h4>
<p>定义<strong>Jacobian矩阵</strong>：</p>
<p>$$\boldsymbol{J}<em i-1="i-1">i = \frac{\partial \boldsymbol{h}_i}{\partial \boldsymbol{h}</em>_h$$}} = \text{diag}(\tanh'(\boldsymbol{z}_i)) \boldsymbol{W</p>
<p>梯度传播涉及Jacobian矩阵的连乘：</p>
<p>$$\boldsymbol{J}<em i="k+1">{k:t} = \prod</em>_i$$}^t \boldsymbol{J</p>
<p><strong>谱半径</strong>（spectral radius）定义为：</p>
<p>$$\rho(\boldsymbol{J}) = \max_i |\lambda_i(\boldsymbol{J})|$$</p>
<p>其中 $\lambda_i(\boldsymbol{J})$ 是 $\boldsymbol{J}$ 的特征值。</p>
<p><strong>梯度爆炸/消失的充要条件</strong>：</p>
<ul>
<li><strong>梯度爆炸</strong>：若存在常数 $\gamma &gt; 1$ 使得 $|\boldsymbol{J}_i|_2 \geq \gamma$ 对大部分 $i$ 成立，则：
  $$\left|\frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{h}_k}\right|_2 \geq \gamma^{t-k}$$</li>
</ul>
<p>这导致梯度范数以指数速率增长。</p>
<ul>
<li><strong>梯度消失</strong>：若 $|\boldsymbol{J}_i|_2 \leq \gamma &lt; 1$，则：
  $$\left|\frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{h}_k}\right|_2 \leq \gamma^{t-k} \to 0$$</li>
</ul>
<h4 id="23">2.3 梯度范数的期望增长率<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p>假设 Jacobian 矩阵 $\boldsymbol{J}_i$ 在不同时间步独立同分布，且 $\mathbb{E}[|\boldsymbol{J}_i|_2] = \mu$。</p>
<p><strong>梯度范数的期望值</strong>：</p>
<p>$$\mathbb{E}\left[\left|\frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{h}_k}\right|_2\right] \approx \mu^{t-k}$$</p>
<p>当 $\mu &gt; 1$ 时，梯度范数期望以指数速率增长。对于参数梯度：</p>
<p>$$\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em k="1">h}\right|_2 \propto \sum</em>$$}^{T-1} \mu^{T-k} = \mu \frac{\mu^{T-1} - 1}{\mu - 1</p>
<p>当 $T$ 较大且 $\mu &gt; 1$ 时，梯度范数约为 $O(\mu^T)$，呈指数爆炸。</p>
<p><strong>临界条件</strong>：为了避免梯度爆炸，需要：</p>
<p>$$\mathbb{E}[|\boldsymbol{J}_i|_2] \leq 1$$</p>
<p>对于 $\boldsymbol{J}_i = \text{diag}(\tanh'(\boldsymbol{z}_i)) \boldsymbol{W}_h$，由于 $\tanh'(z) \leq 1$，我们需要：</p>
<p>$$|\boldsymbol{W}_h|_2 \lesssim 1$$</p>
<p>这解释了为什么权重矩阵的谱范数接近1是稳定训练的关键。</p>
<h3 id="3-1">3. 为什么阈值是1：尺度分析<a class="toc-link" href="#3-1" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 损失函数变化量的一阶近似<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>考虑参数更新 $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \boldsymbol{u}_t$，其中 $\boldsymbol{u}_t$ 是优化器的更新方向（对于SGD，$\boldsymbol{u}_t = \boldsymbol{g}_t$）。</p>
<p><strong>损失函数的Taylor展开</strong>：</p>
<p>$$\mathcal{L}(\boldsymbol{\theta}<em t_1="t+1">{t+1}) = \mathcal{L}(\boldsymbol{\theta}_t) + (\boldsymbol{\theta}</em>} - \boldsymbol{\theta<em t_1="t+1">t)^T \nabla \mathcal{L}(\boldsymbol{\theta}_t) + \frac{1}{2}(\boldsymbol{\theta}</em>} - \boldsymbol{\theta<em t_1="t+1">t)^T \boldsymbol{H}_t (\boldsymbol{\theta}</em>_t) + O(\eta^3)$$} - \boldsymbol{\theta</p>
<p>其中 $\boldsymbol{H}_t = \nabla^2 \mathcal{L}(\boldsymbol{\theta}_t)$ 是Hessian矩阵。</p>
<p><strong>一阶近似</strong>下的损失变化：</p>
<p>$$\Delta \mathcal{L} \approx -\eta \boldsymbol{u}_t^T \boldsymbol{g}_t$$</p>
<h4 id="32-sgd">3.2 SGD的尺度分析<a class="toc-link" href="#32-sgd" title="Permanent link">&para;</a></h4>
<p>对于标准SGD（$\boldsymbol{u}_t = \boldsymbol{g}_t$）：</p>
<p>$$\Delta \mathcal{L} \approx -\eta |\boldsymbol{g}_t|_2^2$$</p>
<p><strong>平稳训练的必要条件</strong>：损失函数应该稳定下降，即：</p>
<p>$$|\Delta \mathcal{L}| \sim O(\eta)$$</p>
<p>由此推导：</p>
<p>$$\eta |\boldsymbol{g}_t|_2^2 \sim O(\eta)$$</p>
<p>$$|\boldsymbol{g}_t|_2^2 \sim O(1)$$</p>
<p>$$|\boldsymbol{g}_t|_2 \sim O(1)$$</p>
<p>这表明<strong>梯度范数应该在1的量级</strong>，才能保证损失以学习率的量级稳定下降。</p>
<h4 id="33">3.3 二阶修正与有效学习率<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>考虑二阶项：</p>
<p>$$\Delta \mathcal{L} \approx -\eta |\boldsymbol{g}_t|_2^2 + \frac{\eta^2}{2} \boldsymbol{g}_t^T \boldsymbol{H}_t \boldsymbol{g}_t$$</p>
<p>定义<strong>有效学习率</strong>（考虑二阶效应）：</p>
<p>$$\eta_{\text{eff}} = \eta - \frac{\eta^2}{2} \frac{\boldsymbol{g}_t^T \boldsymbol{H}_t \boldsymbol{g}_t}{|\boldsymbol{g}_t|_2^2}$$</p>
<p><strong>稳定性条件</strong>要求 $\eta_{\text{eff}} &gt; 0$：</p>
<p>$$\eta &lt; \frac{2|\boldsymbol{g}_t|_2^2}{\boldsymbol{g}_t^T \boldsymbol{H}_t \boldsymbol{g}_t}$$</p>
<p>在梯度方向上，Hessian的二次型可以写为：</p>
<p>$$\boldsymbol{g}_t^T \boldsymbol{H}_t \boldsymbol{g}_t = |\boldsymbol{g}_t|_2^2 \cdot \frac{\boldsymbol{g}_t^T}{|\boldsymbol{g}_t|_2} \boldsymbol{H}_t \frac{\boldsymbol{g}_t}{|\boldsymbol{g}_t|_2}$$</p>
<p>设梯度方向的曲率为 $\kappa = \frac{\boldsymbol{g}_t^T \boldsymbol{H}_t \boldsymbol{g}_t}{|\boldsymbol{g}_t|_2^2}$，则稳定性条件为：</p>
<p>$$\eta &lt; \frac{2}{\kappa}$$</p>
<p>在深度学习中，Hessian的最大特征值（曲率）通常在 $O(1)$ 到 $O(10)$ 的量级，因此典型的稳定学习率为：</p>
<p>$$\eta \sim O(0.1) \sim O(1)$$</p>
<p>结合 $|\Delta \mathcal{L}| \sim \eta |\boldsymbol{g}_t|_2^2$，为了使损失下降量在合理范围内（例如 $|\Delta \mathcal{L}| \sim 0.01 \sim 0.1$），我们需要：</p>
<p>$$|\boldsymbol{g}_t|_2 \sim O(1)$$</p>
<p>这从定量角度说明了<strong>为什么梯度范数应该在1附近</strong>。</p>
<h3 id="4">4. 与学习率的关系<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41-">4.1 学习率-梯度范数的耦合关系<a class="toc-link" href="#41-" title="Permanent link">&para;</a></h4>
<p>定义<strong>有效步长</strong>：</p>
<p>$$s_{\text{eff}} = \eta |\boldsymbol{g}_t|_2$$</p>
<p>损失变化重写为：</p>
<p>$$\Delta \mathcal{L} \approx -\eta |\boldsymbol{g}<em _text_eff="\text{eff">t|_2^2 = -s</em>_t|_2$$}} |\boldsymbol{g</p>
<p><strong>训练稳定性</strong>要求 $s_{\text{eff}}$ 在合理范围内。给定学习率 $\eta \in [0.001, 0.1]$（深度学习常用范围），若要求：</p>
<p>$$s_{\text{eff}} \sim O(0.01) \sim O(0.1)$$</p>
<p>则需要：</p>
<p>$$|\boldsymbol{g}_t|_2 \sim O(0.1) \sim O(10)$$</p>
<p>但在大多数良好初始化的模型中，经验上 $|\boldsymbol{g}<em _text_eff="\text{eff">t|_2 \sim O(1)$，这与 $\eta \sim O(0.01) \sim O(0.1)$ 配合，使得 $s</em> \sim O(0.01) \sim O(0.1)$。}</p>
<h4 id="42">4.2 梯度裁剪作为学习率的隐式调整<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p>使用梯度裁剪后的更新：</p>
<p>$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \cdot \text{clip}(\boldsymbol{g}_t, \tau)$$</p>
<p>当 $|\boldsymbol{g}_t|_2 &gt; \tau$ 时：</p>
<p>$$\boldsymbol{\theta}<em _text_有效学习率="\text{有效学习率">{t+1} = \boldsymbol{\theta}_t - \eta \frac{\tau}{|\boldsymbol{g}_t|_2} \boldsymbol{g}_t = \boldsymbol{\theta}_t - \underbrace{\eta \frac{\tau}{|\boldsymbol{g}_t|_2}}</em>_t$$}} \boldsymbol{g</p>
<p><strong>有效学习率</strong>变为：</p>
<p>$$\eta_{\text{eff}} = \eta \cdot \min\left(1, \frac{\tau}{|\boldsymbol{g}_t|_2}\right)$$</p>
<p>这表明梯度裁剪<strong>等价于自适应地调整学习率</strong>，当梯度过大时降低学习率。</p>
<p><strong>有效步长</strong>变为：</p>
<p>$$s_{\text{eff}} = \eta |\text{clip}(\boldsymbol{g}_t, \tau)|_2 \leq \eta \tau$$</p>
<p>因此，裁剪阈值 $\tau$ <strong>直接控制了最大有效步长</strong>。选择 $\tau = 1$ 时，配合典型学习率 $\eta \sim 0.01 \sim 0.1$，有效步长上界为 $0.01 \sim 0.1$，恰好在稳定训练的范围内。</p>
<h4 id="43-warmup">4.3 Warmup与梯度裁剪的协同作用<a class="toc-link" href="#43-warmup" title="Permanent link">&para;</a></h4>
<p>在训练初期，模型参数随机初始化，梯度范数可能很大。<strong>Warmup策略</strong>线性增加学习率：</p>
<p>$$\eta_t = \eta_{\max} \cdot \frac{t}{T_{\text{warmup}}}, \quad t \leq T_{\text{warmup}}$$</p>
<p>结合梯度裁剪，有效学习率为：</p>
<p>$$\eta_{\text{eff}}(t) = \eta_{\max} \cdot \frac{t}{T_{\text{warmup}}} \cdot \min\left(1, \frac{\tau}{|\boldsymbol{g}_t|_2}\right)$$</p>
<p><strong>初期分析</strong>（$t \ll T_{\text{warmup}}$）：
- 如果 $|\boldsymbol{g}<em _text_eff="\text{eff">t|_2 &gt; \tau$，有效学习率为 $\eta</em>$
- 梯度裁剪提供额外的保护，防止即使在小学习率下也可能发生的不稳定}}(t) = \eta_{\max} \frac{t}{T_{\text{warmup}}} \frac{\tau}{|\boldsymbol{g}_t|_2</p>
<p><strong>后期分析</strong>（$t &gt; T_{\text{warmup}}$）：
- 学习率固定为 $\eta_{\max}$
- 梯度范数通常已降至 $|\boldsymbol{g}_t|_2 &lt; \tau$，裁剪不激活
- 偶尔的梯度尖峰会被裁剪限制</p>
<p><strong>协同效应</strong>：Warmup平滑地从小学习率过渡，梯度裁剪处理突发的大梯度，两者共同保证训练稳定性。</p>
<h3 id="5">5. 裁剪阈值的最优选择理论<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 最优化视角：最小化期望损失<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>假设梯度 $\boldsymbol{g}_t$ 服从某个分布 $p(\boldsymbol{g})$，我们希望选择阈值 $\tau$ 使得期望损失下降最大化：</p>
<p>$$\tau^* = \arg\max_\tau \mathbb{E}_{\boldsymbol{g} \sim p(\boldsymbol{g})} \left[ -\eta \boldsymbol{g}^T \text{clip}(\boldsymbol{g}, \tau) \right]$$</p>
<p>展开：</p>
<p>$$\mathbb{E}<em _124_boldsymbol_g="|\boldsymbol{g">{\boldsymbol{g}} \left[ \boldsymbol{g}^T \text{clip}(\boldsymbol{g}, \tau) \right] = \int</em>|<em _124_boldsymbol_g="|\boldsymbol{g">2 \leq \tau} |\boldsymbol{g}|_2^2 p(\boldsymbol{g}) d\boldsymbol{g} + \tau \int</em>$$}|_2 &gt; \tau} |\boldsymbol{g}|_2 p(\boldsymbol{g}) d\boldsymbol{g</p>
<p><strong>假设梯度范数服从Rayleigh分布</strong>（在高维空间中常见）：</p>
<p>$$p(|\boldsymbol{g}|_2) = \frac{|\boldsymbol{g}|_2}{\sigma^2} \exp\left(-\frac{|\boldsymbol{g}|_2^2}{2\sigma^2}\right)$$</p>
<p>期望梯度范数为 $\mathbb{E}[|\boldsymbol{g}|_2] = \sigma \sqrt{\pi/2}$。</p>
<p><strong>最优阈值</strong>通过求导得到：</p>
<p>$$\frac{d}{d\tau} \mathbb{E}_{\boldsymbol{g}} \left[ \boldsymbol{g}^T \text{clip}(\boldsymbol{g}, \tau) \right] = 0$$</p>
<p>对于Rayleigh分布，可以证明最优阈值约为：</p>
<p>$$\tau^* \approx \mathbb{E}[|\boldsymbol{g}|_2] = \sigma \sqrt{\pi/2} \approx 1.25 \sigma$$</p>
<p>在深度学习中，经验上 $\sigma \approx 0.8$（归一化后的梯度），因此 $\tau^* \approx 1$。</p>
<h4 id="52">5.2 鲁棒性分析：方差最小化<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>梯度裁剪还可以降低梯度更新的方差。定义裁剪后梯度的方差：</p>
<p>$$\text{Var}[\text{clip}(\boldsymbol{g}, \tau)] = \mathbb{E}[|\text{clip}(\boldsymbol{g}, \tau)|_2^2] - \mathbb{E}[\text{clip}(\boldsymbol{g}, \tau)]^2$$</p>
<p>计算第一项：</p>
<p>$$\mathbb{E}[|\text{clip}(\boldsymbol{g}, \tau)|<em _124_boldsymbol_g="|\boldsymbol{g">2^2] = \int</em>|<em _124_boldsymbol_g="|\boldsymbol{g">2 \leq \tau} |\boldsymbol{g}|_2^2 p(\boldsymbol{g}) d\boldsymbol{g} + \tau^2 \int</em>$$}|_2 &gt; \tau} p(\boldsymbol{g}) d\boldsymbol{g</p>
<p>对于Rayleigh分布，计算得：</p>
<p>$$\mathbb{E}[|\text{clip}(\boldsymbol{g}, \tau)|_2^2] = 2\sigma^2 \left[ 1 - \left(1 + \frac{\tau^2}{2\sigma^2}\right) e^{-\tau^2/(2\sigma^2)} \right] + \tau^2 e^{-\tau^2/(2\sigma^2)}$$</p>
<p><strong>方差随阈值的变化</strong>：
- 当 $\tau \to 0$ 时，方差 $\to 0$（但梯度信息丢失）
- 当 $\tau \to \infty$ 时，方差 $\to$ 原始梯度方差（无裁剪效果）
- 存在最优 $\tau$ 平衡信息保留和方差降低</p>
<p>数值计算表明，对于标准化的梯度分布（$\sigma = 1$），最优阈值在 $\tau \in [0.8, 1.2]$ 范围内，<strong>中心值正是1</strong>。</p>
<h4 id="53">5.3 信息论视角：互信息最大化<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p>从信息论角度，我们希望裁剪后的梯度 $\tilde{\boldsymbol{g}} = \text{clip}(\boldsymbol{g}, \tau)$ 与真实梯度 $\boldsymbol{g}$ 之间保持最大互信息：</p>
<p>$$\tau^* = \arg\max_\tau I(\boldsymbol{g}; \tilde{\boldsymbol{g}})$$</p>
<p>互信息可以分解为：</p>
<p>$$I(\boldsymbol{g}; \tilde{\boldsymbol{g}}) = H(\tilde{\boldsymbol{g}}) - H(\tilde{\boldsymbol{g}}|\boldsymbol{g})$$</p>
<p>由于裁剪是确定性操作，$H(\tilde{\boldsymbol{g}}|\boldsymbol{g}) = 0$，因此：</p>
<p>$$\tau^* = \arg\max_\tau H(\tilde{\boldsymbol{g}})$$</p>
<p>即最大化裁剪后梯度的熵。</p>
<p><strong>微分熵</strong>（differential entropy）为：</p>
<p>$$H(\tilde{\boldsymbol{g}}) = -\int p(\tilde{\boldsymbol{g}}) \log p(\tilde{\boldsymbol{g}}) d\tilde{\boldsymbol{g}}$$</p>
<p>对于由裁剪操作产生的分布，可以证明当 $\tau \approx \mathbb{E}[|\boldsymbol{g}|_2]$ 时，熵达到最大。</p>
<p><strong>实践意义</strong>：选择 $\tau = 1$ 与梯度分布的自然尺度匹配，最大化了保留的信息量。</p>
<h3 id="6">6. 不同范数裁剪的对比<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61-l1">6.1 L1范数裁剪<a class="toc-link" href="#61-l1" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：</p>
<p>$$\text{clip}_{L_1}(\boldsymbol{g}, \tau) = \boldsymbol{g} \cdot \min\left(1, \frac{\tau}{|\boldsymbol{g}|_1}\right)$$</p>
<p>其中 $|\boldsymbol{g}|<em i="1">1 = \sum</em>^d |g_i|$。</p>
<p><strong>性质</strong>：
- <strong>稀疏性保持</strong>：L1裁剪倾向于保持梯度的稀疏性
- <strong>分量独立性</strong>：每个分量独立缩放
- <strong>尺度依赖</strong>：$|\boldsymbol{g}|_1 \leq \sqrt{d} |\boldsymbol{g}|_2$，因此在高维空间中，相同阈值的L1裁剪更激进</p>
<p><strong>损失变化</strong>（SGD）：</p>
<p>$$\Delta \mathcal{L} \approx -\eta \boldsymbol{g}^T \text{clip}_{L_1}(\boldsymbol{g}, \tau)$$</p>
<p>当 $|\boldsymbol{g}|_1 &gt; \tau$ 时：</p>
<p>$$\Delta \mathcal{L} \approx -\eta \frac{\tau}{|\boldsymbol{g}|_1} |\boldsymbol{g}|_1^2 = -\eta \tau |\boldsymbol{g}|_1$$</p>
<p><strong>阈值选择</strong>：对于L1裁剪，由于 $|\boldsymbol{g}|_1 \approx \sqrt{d} |\boldsymbol{g}|_2$，典型阈值为：</p>
<p>$$\tau_{L_1} \approx \sqrt{d}$$</p>
<p>对于百万级参数（$d \sim 10^6$），$\tau_{L_1} \sim 10^3$，这在实践中很少使用。</p>
<h4 id="62-l">6.2 L∞范数裁剪<a class="toc-link" href="#62-l" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：</p>
<p>$$\text{clip}<em>{L</em>\infty}(\boldsymbol{g}, \tau) = [\min(\max(g_i, -\tau), \tau)]_{i=1}^d$$</p>
<p>即对每个分量独立裁剪到 $[-\tau, \tau]$。</p>
<p><strong>性质</strong>：
- <strong>分量独立</strong>：每个梯度分量独立裁剪
- <strong>保持方向性较差</strong>：可能改变梯度方向
- <strong>计算简单</strong>：无需计算全局范数</p>
<p><strong>优势</strong>：在分布式训练中，L∞裁剪不需要全局通信来计算范数。</p>
<p><strong>阈值选择</strong>：由于每个分量独立，阈值通常设为单个参数梯度的典型值。在良好初始化的网络中，单个梯度分量约为 $O(1/\sqrt{d})$，因此：</p>
<p>$$\tau_{L_\infty} \approx \frac{1}{\sqrt{d}}$$</p>
<p>对于百万级参数，$\tau_{L_\infty} \sim 10^{-3}$。</p>
<h4 id="63">6.3 范数对比总结<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>范数</th>
<th>定义</th>
<th>方向保持</th>
<th>典型阈值</th>
<th>计算复杂度</th>
<th>应用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>$L_2$</td>
<td>$\sqrt{\sum g_i^2}$</td>
<td>是</td>
<td>$\tau \sim 1$</td>
<td>$O(d)$</td>
<td>标准训练</td>
</tr>
<tr>
<td>$L_1$</td>
<td>$\sum |g_i|$</td>
<td>是</td>
<td>$\tau \sim \sqrt{d}$</td>
<td>$O(d)$</td>
<td>稀疏优化</td>
</tr>
<tr>
<td>$L_\infty$</td>
<td>$\max |g_i|$</td>
<td>否</td>
<td>$\tau \sim 1/\sqrt{d}$</td>
<td>$O(d)$</td>
<td>分布式训练</td>
</tr>
</tbody>
</table>
<p><strong>为什么L2裁剪最常用</strong>：
1. <strong>几何直观</strong>：保持梯度方向，仅限制步长
2. <strong>阈值独立于维度</strong>：$\tau = 1$ 对不同规模的模型通用
3. <strong>理论支持</strong>：与损失函数的二次近似相容</p>
<h3 id="7-vs">7. 层级裁剪 vs 全局裁剪<a class="toc-link" href="#7-vs" title="Permanent link">&para;</a></h3>
<h4 id="71-global-clipping">7.1 全局裁剪（Global Clipping）<a class="toc-link" href="#71-global-clipping" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：将所有参数的梯度视为单个向量裁剪：</p>
<p>$$\boldsymbol{g}_{\text{global}} = [\boldsymbol{g}_1; \boldsymbol{g}_2; \ldots; \boldsymbol{g}_L]$$</p>
<p>$$\tilde{\boldsymbol{g}}<em _text_global="\text{global">{\text{global}} = \text{clip}(\boldsymbol{g}</em>, \tau)$$}</p>
<p><strong>优点</strong>：
- 保持跨层的相对梯度比例
- 单一阈值，易于调优
- 理论分析简单</p>
<p><strong>缺点</strong>：
- 某些层的梯度可能远大于其他层，导致小梯度层信息丢失
- 对深层网络，浅层梯度通常较小，可能被深层主导</p>
<h4 id="72-per-layer-clipping">7.2 层级裁剪（Per-Layer Clipping）<a class="toc-link" href="#72-per-layer-clipping" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：对每层独立裁剪：</p>
<p>$$\tilde{\boldsymbol{g}}<em>\ell = \text{clip}(\boldsymbol{g}</em>\ell, \tau_\ell), \quad \ell = 1, 2, \ldots, L$$</p>
<p><strong>阈值选择策略</strong>：
1. <strong>统一阈值</strong>：$\tau_\ell = \tau$ for all $\ell$
2. <strong>自适应阈值</strong>：$\tau_\ell = \alpha \mathbb{E}[|\boldsymbol{g}<em>\ell|_2]$
3. <strong>维度归一化</strong>：$\tau</em>\ell = \tau \sqrt{d_\ell}$，其中 $d_\ell$ 是第 $\ell$ 层的参数数量</p>
<p><strong>优点</strong>：
- 保护小梯度层的信息
- 对不同尺度的层更公平
- 更精细的控制</p>
<p><strong>缺点</strong>：
- 可能破坏跨层的相对梯度关系
- 更多超参数需要调优
- 理论分析复杂</p>
<h4 id="73">7.3 理论对比：谱范数约束<a class="toc-link" href="#73" title="Permanent link">&para;</a></h4>
<p>考虑深度网络的前向传播：</p>
<p>$$\boldsymbol{h}<em>{\ell+1} = f</em>\ell(\boldsymbol{W}<em>\ell \boldsymbol{h}</em>\ell)$$</p>
<p>梯度反向传播：</p>
<p>$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{h}<em _ell="\ell">\ell} = \boldsymbol{W}</em>(f'}^T \text{diag<em _ell_1="\ell+1">\ell) \frac{\partial \mathcal{L}}{\partial \boldsymbol{h}</em>$$}</p>
<p><strong>全局裁剪</strong>限制：</p>
<p>$$\left|\left[\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_1}; \ldots; \frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_L}\right]\right|_2 \leq \tau$$</p>
<p><strong>层级裁剪</strong>限制：</p>
<p>$$\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em>\ell}\right|_2 \leq \tau</em>\ell, \quad \forall \ell$$</p>
<p><strong>谱范数分析</strong>：层级裁剪隐式地约束每层权重矩阵的谱范数：</p>
<p>$$|\boldsymbol{W}_\ell|_2 \lesssim 1$$</p>
<p>这与前述梯度传播的稳定性条件 $|\boldsymbol{J}_\ell|_2 \lesssim 1$ 一致，从理论上支持层级裁剪。</p>
<h4 id="74">7.4 实验对比<a class="toc-link" href="#74" title="Permanent link">&para;</a></h4>
<p><strong>RNN训练</strong>（Penn Treebank，LSTM，2层，650隐藏单元）：</p>
<ul>
<li><strong>全局裁剪</strong>（$\tau = 1$）：困惑度 78.5</li>
<li><strong>层级裁剪</strong>（$\tau_\ell = 0.5$）：困惑度 79.2</li>
<li><strong>自适应层级裁剪</strong>：困惑度 77.8</li>
</ul>
<p><strong>Transformer训练</strong>（WMT14 En-De，6层）：</p>
<ul>
<li><strong>全局裁剪</strong>（$\tau = 1$）：BLEU 27.3</li>
<li><strong>层级裁剪</strong>（$\tau_\ell = 1$）：BLEU 27.1</li>
<li><strong>混合策略</strong>（注意力层全局，FFN层级）：BLEU 27.6</li>
</ul>
<p><strong>结论</strong>：全局裁剪在大多数情况下表现良好且简单；层级裁剪在梯度尺度差异大的任务中可能有优势。</p>
<h3 id="8">8. 自适应裁剪策略<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 基于历史梯度的自适应阈值<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p><strong>指数移动平均（EMA）</strong>：</p>
<p>$$\bar{\tau}<em t-1="t-1">t = \beta \bar{\tau}</em>_t|_2$$} + (1-\beta) |\boldsymbol{g</p>
<p>$$\tau_t = \alpha \bar{\tau}_t$$</p>
<p>其中 $\beta \in [0.9, 0.99]$ 是平滑系数，$\alpha \in [1.5, 3]$ 是缩放因子。</p>
<p><strong>自适应裁剪</strong>：</p>
<p>$$\tilde{\boldsymbol{g}}_t = \text{clip}(\boldsymbol{g}_t, \tau_t)$$</p>
<p><strong>优点</strong>：
- 自动适应训练过程中梯度尺度的变化
- 减少手动调优
- 对不同任务和模型更鲁棒</p>
<p><strong>理论依据</strong>：在训练早期，$|\boldsymbol{g}_t|_2$ 较大，$\tau_t$ 相应增大，允许更大的更新；在训练后期，梯度变小，$\tau_t$ 也减小，提供更精细的控制。</p>
<h4 id="82">8.2 基于梯度分位数的裁剪<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>维护梯度范数的历史分布，设置阈值为某个分位数：</p>
<p>$$\tau_t = Q_{p}({|\boldsymbol{g}<em t-1="t-1">1|_2, |\boldsymbol{g}_2|_2, \ldots, |\boldsymbol{g}</em>|_2})$$</p>
<p>其中 $Q_p$ 是第 $p$ 分位数（例如 $p = 0.95$）。</p>
<p><strong>实现</strong>：使用滑动窗口存储最近 $W$ 步的梯度范数，计算分位数。</p>
<p><strong>优点</strong>：
- 自动排除异常大梯度（outliers）
- 适应梯度分布的变化
- 统计上更稳健</p>
<h4 id="83">8.3 梯度裁剪与学习率调度的联合自适应<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p><strong>联合策略</strong>：</p>
<p>$$\eta_t = \eta_0 \cdot \text{schedule}(t) \cdot \min\left(1, \frac{\tau}{|\boldsymbol{g}_t|_2}\right)$$</p>
<p>这将梯度裁剪集成到学习率调度中。</p>
<p><strong>改进</strong>：使用平滑的衰减函数代替硬裁剪：</p>
<p>$$\eta_t = \eta_0 \cdot \text{schedule}(t) \cdot \frac{1}{1 + (|\boldsymbol{g}_t|_2 / \tau)^k}$$</p>
<p>其中 $k &gt; 0$ 控制平滑度。当 $k \to \infty$ 时，退化为硬裁剪。</p>
<p><strong>优势</strong>：
- 平滑的裁剪函数保持可微性
- 更好的理论性质（Lipschitz连续）
- 实验中训练曲线更平滑</p>
<h3 id="9">9. 收敛性的理论保证<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 凸优化中的收敛性<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p><strong>假设</strong>：
1. 损失函数 $\mathcal{L}(\boldsymbol{\theta})$ 是凸函数
2. 梯度有界：$|\nabla \mathcal{L}(\boldsymbol{\theta})|_2 \leq G$
3. 损失函数 $L$-Lipschitz连续：$|\mathcal{L}(\boldsymbol{\theta}_1) - \mathcal{L}(\boldsymbol{\theta}_2)| \leq L |\boldsymbol{\theta}_1 - \boldsymbol{\theta}_2|_2$</p>
<p><strong>定理（梯度裁剪SGD的收敛性）</strong>：</p>
<p>使用梯度裁剪的SGD，学习率 $\eta_t = \eta / \sqrt{t}$，裁剪阈值 $\tau$，经过 $T$ 步迭代后：</p>
<p>$$\mathbb{E}[\mathcal{L}(\bar{\boldsymbol{\theta}}_T)] - \mathcal{L}(\boldsymbol{\theta}^*) \leq \frac{L \tau^2 \eta}{2\sqrt{T}} + \frac{G^2}{2\eta\sqrt{T}}$$</p>
<p>其中 $\bar{\boldsymbol{\theta}}<em t="1">T = \frac{1}{T}\sum</em>^*$ 是最优解。}^T \boldsymbol{\theta}_t$ 是平均参数，$\boldsymbol{\theta</p>
<p><strong>证明草图</strong>：</p>
<ol>
<li>
<p><strong>步骤1</strong>：参数更新为 $\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta_t \text{clip}(\boldsymbol{g}_t, \tau)$</p>
</li>
<li>
<p><strong>步骤2</strong>：计算参数与最优解的距离变化：
   $$|\boldsymbol{\theta}_{t+1} - \boldsymbol{\theta}^<em>|_2^2 = |\boldsymbol{\theta}_t - \boldsymbol{\theta}^</em> - \eta_t \text{clip}(\boldsymbol{g}_t, \tau)|_2^2$$</p>
</li>
<li>
<p><strong>步骤3</strong>：展开并利用 $|\text{clip}(\boldsymbol{g}<em t_1="t+1">t, \tau)|_2 \leq \tau$：
   $$|\boldsymbol{\theta}</em>^} - \boldsymbol{\theta<em>|_2^2 \leq |\boldsymbol{\theta}_t - \boldsymbol{\theta}^</em>|_2^2 - 2\eta_t (\boldsymbol{\theta}_t - \boldsymbol{\theta}^*)^T \boldsymbol{g}_t + \eta_t^2 \tau^2$$</p>
</li>
<li>
<p><strong>步骤4</strong>：利用凸性 $\mathcal{L}(\boldsymbol{\theta}^<em>) \geq \mathcal{L}(\boldsymbol{\theta}_t) + \boldsymbol{g}_t^T (\boldsymbol{\theta}^</em> - \boldsymbol{\theta}_t)$</p>
</li>
<li>
<p><strong>步骤5</strong>：累加并取期望，得到收敛速率 $O(1/\sqrt{T})$</p>
</li>
</ol>
<p><strong>阈值 $\tau$ 的影响</strong>：
- 第一项 $O(\tau^2)$ 随 $\tau$ 增加而增加（裁剪引入的偏差）
- 第二项 $O(G^2)$ 不依赖于 $\tau$（梯度方差）
- <strong>最优阈值</strong>平衡两项，约为 $\tau^* \sim G / L$</p>
<p>在深度学习中，经验上 $G \sim 1$，$L \sim 1$，因此 $\tau^* \sim 1$。</p>
<h4 id="92">9.2 非凸优化中的收敛性<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p>对于非凸损失函数，收敛到临界点（梯度为零的点）。</p>
<p><strong>假设</strong>：
1. 损失函数有下界：$\mathcal{L}(\boldsymbol{\theta}) \geq \mathcal{L}_{\min}$
2. 梯度 $L$-Lipschitz连续：$|\nabla \mathcal{L}(\boldsymbol{\theta}_1) - \nabla \mathcal{L}(\boldsymbol{\theta}_2)|_2 \leq L |\boldsymbol{\theta}_1 - \boldsymbol{\theta}_2|_2$
3. 随机梯度无偏且方差有界：$\mathbb{E}[\boldsymbol{g}_t] = \nabla \mathcal{L}(\boldsymbol{\theta}_t)$，$\mathbb{E}[|\boldsymbol{g}_t - \nabla \mathcal{L}(\boldsymbol{\theta}_t)|_2^2] \leq \sigma^2$</p>
<p><strong>定理（梯度裁剪SGD收敛到稳定点）</strong>：</p>
<p>使用梯度裁剪的SGD，固定学习率 $\eta \leq 1/L$，裁剪阈值 $\tau$，经过 $T$ 步迭代：</p>
<p>$$\frac{1}{T} \sum_{t=1}^T \mathbb{E}[|\nabla \mathcal{L}(\boldsymbol{\theta}<em _min="\min">t)|_2^2] \leq \frac{2(\mathcal{L}(\boldsymbol{\theta}_0) - \mathcal{L}</em> + L\eta \min(\tau^2, \sigma^2)$$})}{\eta T</p>
<p>这表明：
- 平均梯度范数以 $O(1/T)$ 速率收敛到零（找到稳定点）
- <strong>裁剪阈值 $\tau$ 影响稳态误差</strong>：第二项 $O(\tau^2)$ 或 $O(\sigma^2)$
- 当 $\tau &gt; \sigma$（裁剪不激活），收敛速率由梯度方差决定
- 当 $\tau &lt; \sigma$（裁剪经常激活），裁剪限制了收敛精度</p>
<p><strong>最优阈值</strong>：设置 $\tau \approx \sigma$（梯度噪声的标准差），平衡收敛速度和精度。</p>
<p>在深度学习中，批量大小 $B$ 影响梯度方差：</p>
<p>$$\sigma^2 \propto \frac{\sigma_0^2}{B}$$</p>
<p>其中 $\sigma_0$ 是单样本梯度方差。对于典型设置（$B = 32$，$\sigma_0 \sim 5$），$\sigma \sim 1$，因此 $\tau \sim 1$。</p>
<h3 id="10-rnntransformer">10. 在RNN/Transformer训练中的应用<a class="toc-link" href="#10-rnntransformer" title="Permanent link">&para;</a></h3>
<h4 id="101-rnn">10.1 RNN训练中的梯度裁剪<a class="toc-link" href="#101-rnn" title="Permanent link">&para;</a></h4>
<p><strong>LSTM模型</strong>：</p>
<p>$$\begin{aligned}
\boldsymbol{f}<em t-1="t-1">t &amp;= \sigma(\boldsymbol{W}_f [\boldsymbol{h}</em>}, \boldsymbol{x<em t-1="t-1">t] + \boldsymbol{b}_f) \
\boldsymbol{i}_t &amp;= \sigma(\boldsymbol{W}_i [\boldsymbol{h}</em>}, \boldsymbol{x<em t-1="t-1">t] + \boldsymbol{b}_i) \
\boldsymbol{\tilde{c}}_t &amp;= \tanh(\boldsymbol{W}_c [\boldsymbol{h}</em>}, \boldsymbol{x<em t-1="t-1">t] + \boldsymbol{b}_c) \
\boldsymbol{c}_t &amp;= \boldsymbol{f}_t \odot \boldsymbol{c}</em>} + \boldsymbol{i<em t-1="t-1">t \odot \boldsymbol{\tilde{c}}_t \
\boldsymbol{o}_t &amp;= \sigma(\boldsymbol{W}_o [\boldsymbol{h}</em>_o) \
\boldsymbol{h}_t &amp;= \boldsymbol{o}_t \odot \tanh(\boldsymbol{c}_t)
\end{aligned}$$}, \boldsymbol{x}_t] + \boldsymbol{b</p>
<p><strong>梯度传播</strong>（BPTT）：</p>
<p>$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}} = \sum_{t=1}^T \frac{\partial \mathcal{L}_t}{\partial \boldsymbol{h}_t} \frac{\partial \boldsymbol{h}_t}{\partial \boldsymbol{W}}$$</p>
<p>由于链式法则，梯度涉及长距离依赖的连乘，容易爆炸。</p>
<p><strong>裁剪策略</strong>：
1. <strong>全局裁剪所有参数</strong>：$\tau = 1$ 或 $\tau = 5$（较长序列）
2. <strong>只裁剪循环权重</strong>：$\boldsymbol{W}_f, \boldsymbol{W}_i, \boldsymbol{W}_c, \boldsymbol{W}_o$
3. <strong>时间步裁剪</strong>：在每个BPTT时间步裁剪梯度</p>
<p><strong>实验（Penn Treebank）</strong>：</p>
<table>
<thead>
<tr>
<th>配置</th>
<th>困惑度</th>
<th>训练稳定性</th>
</tr>
</thead>
<tbody>
<tr>
<td>无裁剪</td>
<td>发散</td>
<td>不稳定</td>
</tr>
<tr>
<td>$\tau = 0.5$</td>
<td>82.3</td>
<td>稳定，收敛慢</td>
</tr>
<tr>
<td>$\tau = 1$</td>
<td>78.5</td>
<td>稳定，最优</td>
</tr>
<tr>
<td>$\tau = 5$</td>
<td>79.8</td>
<td>稳定，略逊</td>
</tr>
<tr>
<td>$\tau = 10$</td>
<td>80.5</td>
<td>偶尔震荡</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>：$\tau = 1$ 在RNN训练中提供最佳平衡。</p>
<h4 id="102-transformer">10.2 Transformer训练中的梯度裁剪<a class="toc-link" href="#102-transformer" title="Permanent link">&para;</a></h4>
<p><strong>Transformer架构</strong>：</p>
<p>自注意力层：
$$\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{d_k}}\right)\boldsymbol{V}$$</p>
<p>前馈网络：
$$\text{FFN}(\boldsymbol{x}) = \max(0, \boldsymbol{x}\boldsymbol{W}_1 + \boldsymbol{b}_1)\boldsymbol{W}_2 + \boldsymbol{b}_2$$</p>
<p><strong>梯度特性</strong>：
- <strong>注意力层</strong>：梯度通过softmax和点积，尺度相对稳定
- <strong>FFN层</strong>：梯度直接传播，可能较大
- <strong>层归一化（LayerNorm）</strong>：隐式地稳定梯度</p>
<p><strong>是否需要裁剪？</strong></p>
<p>在标准Transformer中（如BERT、GPT），由于以下原因，梯度相对稳定：
1. 层归一化
2. 残差连接
3. 良好的初始化（Xavier/Kaiming）</p>
<p>但在以下情况仍需裁剪：
- <strong>大模型训练</strong>（GPT-3，175B参数）：初期梯度可能不稳定
- <strong>长序列</strong>（$T &gt; 1024$）：累积的梯度可能较大
- <strong>混合精度训练</strong>：FP16数值范围小，更易溢出</p>
<p><strong>实践中的设置</strong>：</p>
<ul>
<li><strong>BERT</strong>（Devlin等，2018）：$\tau = 1$</li>
<li><strong>GPT-2</strong>（Radford等，2019）：$\tau = 1$</li>
<li><strong>GPT-3</strong>（Brown等，2020）：$\tau = 1$，配合gradient checkpointing</li>
<li><strong>T5</strong>（Raffel等，2020）：$\tau = 1$</li>
</ul>
<p><strong>一致性观察</strong>：无论模型大小（从110M到175B参数），<strong>裁剪阈值均为1</strong>，证明了这一设置的普适性。</p>
<h4 id="103-gpt-3">10.3 案例研究：GPT-3训练稳定性<a class="toc-link" href="#103-gpt-3" title="Permanent link">&para;</a></h4>
<p>GPT-3训练中的观察（Brown等，2020）：</p>
<p><strong>训练初期</strong>（前1000步）：
- 梯度范数分布：均值 2.5，方差 3.2
- 裁剪率（$|\boldsymbol{g}|_2 &gt; 1$ 的比例）：约35%
- 裁剪有效地防止了早期不稳定</p>
<p><strong>训练中期</strong>（1000-100000步）：
- 梯度范数分布：均值 0.8，方差 0.3
- 裁剪率：约5%
- 裁剪偶尔激活，处理突发的大梯度</p>
<p><strong>训练后期</strong>（&gt;100000步）：
- 梯度范数分布：均值 0.3，方差 0.1
- 裁剪率：&lt;1%
- 裁剪基本不激活，模型稳定收敛</p>
<p><strong>关键洞察</strong>：
1. <strong>自适应性</strong>：固定阈值 $\tau = 1$ 在训练各阶段都合适
2. <strong>梯度演化</strong>：梯度范数自然地从 $&gt;1$ 演化到 $&lt;1$
3. <strong>稳定性保证</strong>：裁剪作为"安全网"，防止偶尔的梯度尖峰</p>
<h4 id="104-transformer1">10.4 理论解释：为什么Transformer也是1？<a class="toc-link" href="#104-transformer1" title="Permanent link">&para;</a></h4>
<p>尽管Transformer没有RNN的循环结构，但裁剪阈值仍为1，原因如下：</p>
<p><strong>1. 参数初始化的尺度</strong></p>
<p>权重通常初始化为：</p>
<p>$$W_{ij} \sim \mathcal{N}\left(0, \frac{2}{d_{\text{in}} + d_{\text{out}}}\right)$$</p>
<p>这使得权重矩阵的谱范数约为：</p>
<p>$$\mathbb{E}[|\boldsymbol{W}|<em _text_in="\text{in">2] \approx \sqrt{\frac{2}{d</em> \sim O(1)$$}} + d_{\text{out}}} \cdot \min(d_{\text{in}}, d_{\text{out}})</p>
<p><strong>2. 前向传播的尺度保持</strong></p>
<p>每层输入输出的期望范数保持在 $O(1)$：</p>
<p>$$\mathbb{E}[|\boldsymbol{h}_{\ell}|_2] \approx \mathbb{E}[|\boldsymbol{h}_0|_2] \sim O(\sqrt{d})$$</p>
<p>归一化到每维度，$\mathbb{E}[h_{\ell,i}] \sim O(1)$。</p>
<p><strong>3. 反向传播的对称性</strong></p>
<p>由于前向和反向传播的对称性，梯度的尺度也应该在 $O(1)$：</p>
<p>$$\mathbb{E}\left[\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em>\ell}\right|_2\right] \sim O(\sqrt{d</em>\ell})$$</p>
<p>归一化到总参数，$\mathbb{E}[|\boldsymbol{g}|_2] \sim O(1)$。</p>
<p><strong>结论</strong>：无论架构（RNN或Transformer），良好设计的网络的梯度范数自然地在 $O(1)$ 量级，因此 $\tau = 1$ 是自然的选择。</p>
<h3 id="11">11. 总结与深层理论洞察<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<h4 id="111-1">11.1 为什么是1：多角度总结<a class="toc-link" href="#111-1" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>优化理论</strong>：损失变化 $\Delta \mathcal{L} \sim -\eta |\boldsymbol{g}|_2^2$ 要求 $|\boldsymbol{g}|_2 \sim O(1)$ 以保证稳定下降</p>
</li>
<li>
<p><strong>动力系统</strong>：Jacobian谱半径 $\rho(\boldsymbol{J}) \lesssim 1$ 防止梯度爆炸，对应 $|\boldsymbol{g}|_2 \sim O(1)$</p>
</li>
<li>
<p><strong>统计学习</strong>：梯度范数的自然分布（Rayleigh分布）在良好初始化下集中在1附近</p>
</li>
<li>
<p><strong>信息论</strong>：阈值1最大化裁剪后梯度的信息熵，平衡信息保留和稳健性</p>
</li>
<li>
<p><strong>尺度分析</strong>：参数初始化和激活函数设计使得各层输入输出范数在 $O(1)$，梯度范数继承此尺度</p>
</li>
<li>
<p><strong>实验验证</strong>：从小模型到大模型（110M到175B参数），$\tau = 1$ 普遍有效</p>
</li>
</ol>
<h4 id="112">11.2 深层理论洞察<a class="toc-link" href="#112" title="Permanent link">&para;</a></h4>
<p><strong>洞察1：梯度范数1是深度学习的"自然单位"</strong></p>
<p>就像物理学中的自然单位制（$c = \hbar = 1$），深度学习中梯度范数1是系统的内在尺度，由以下因素共同决定：
- 损失函数的Lipschitz常数 $\sim O(1)$
- 权重矩阵的谱范数 $\sim O(1)$
- 激活函数的导数 $\sim O(1)$
- 学习率的典型值 $\sim O(0.01 - 0.1)$</p>
<p><strong>洞察2：裁剪阈值1反映了训练动力学的平衡点</strong></p>
<p>训练过程可以视为一个动力系统，梯度裁剪在：
- <strong>吸引子</strong>：梯度范数 $&lt;1$ 的稳定区域（正常训练）
- <strong>排斥域</strong>：梯度范数 $\gg 1$ 的不稳定区域（梯度爆炸）
- <strong>边界</strong>：阈值 $\tau = 1$ 是吸引子与排斥域的自然分界</p>
<p><strong>洞察3：普适性源于优化景观的几何性质</strong></p>
<p>无论网络架构和任务，优化景观（loss landscape）的几何性质（Hessian的特征值分布）在良好初始化下惊人地相似，导致梯度统计量的普适性。</p>
<h4 id="113">11.3 开放问题与未来方向<a class="toc-link" href="#113" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>理论问题</strong>：能否严格证明 $\tau = 1$ 在某种意义下是最优的？需要更强的假设还是对更广泛的模型类成立？</p>
</li>
<li>
<p><strong>架构依赖</strong>：对于新兴架构（如Mamba、RWKV），$\tau = 1$ 是否仍然适用？需要实验验证。</p>
</li>
<li>
<p><strong>任务特异性</strong>：某些特殊任务（如强化学习、对抗训练）可能需要不同的阈值，如何自适应选择？</p>
</li>
<li>
<p><strong>理论与实践的差距</strong>：为什么理论分析得到的最优阈值（如0.8或1.2）在实践中差异不大？是否存在更精细的理论？</p>
</li>
<li>
<p><strong>与其他正则化的关系</strong>：梯度裁剪与权重衰减、dropout等正则化技术的交互作用？能否统一理解？</p>
</li>
</ol>
<p><strong>总结</strong>：梯度裁剪阈值为1不是偶然，而是深度学习系统多种因素共同作用的结果，反映了优化动力学、网络初始化和训练稳定性的深层联系。这一简单的设置背后蕴含着丰富的理论内涵，值得进一步探索。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="从谱范数梯度到新式权重衰减的思考.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#157 从谱范数梯度到新式权重衰减的思考</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="低秩近似之路五cur.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#159 低秩近似之路（五）：CUR</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#1">为什么梯度裁剪的默认模长是1？</a><ul>
<li><a href="#_1">是什么</a></li>
<li><a href="#_2">为什么</a></li>
<li><a href="#_3">怎么办</a></li>
<li><a href="#_4">全剧终</a></li>
<li><a href="#_5">公式推导与注释</a><ul>
<li><a href="#1_1">1. 梯度裁剪的数学定义</a></li>
<li><a href="#2">2. 梯度爆炸的理论分析</a></li>
<li><a href="#3-1">3. 为什么阈值是1：尺度分析</a></li>
<li><a href="#4">4. 与学习率的关系</a></li>
<li><a href="#5">5. 裁剪阈值的最优选择理论</a></li>
<li><a href="#6">6. 不同范数裁剪的对比</a></li>
<li><a href="#7-vs">7. 层级裁剪 vs 全局裁剪</a></li>
<li><a href="#8">8. 自适应裁剪策略</a></li>
<li><a href="#9">9. 收敛性的理论保证</a></li>
<li><a href="#10-rnntransformer">10. 在RNN/Transformer训练中的应用</a></li>
<li><a href="#11">11. 总结与深层理论洞察</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>