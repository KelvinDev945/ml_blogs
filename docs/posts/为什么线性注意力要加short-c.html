<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>为什么线性注意力要加Short C... | ML & Math Blog Posts</title>
    <meta name="description" content="为什么线性注意力要加Short C...&para;
原文链接: https://spaces.ac.cn/archives/11320
发布日期: 

如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考《线性注意力简史：从模仿、创新到反哺》）模型都给$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}$加上了Short Co...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=线性">线性</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #348 为什么线性注意力要加Short C...
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#348</span>
                为什么线性注意力要加Short C...
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-10-05</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=线性" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 线性</span>
                </a>
                
                <a href="../index.html?tags=RNN" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> RNN</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="short-c">为什么线性注意力要加Short C...<a class="toc-link" href="#short-c" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11320">https://spaces.ac.cn/archives/11320</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>如果读者有关注模型架构方面的进展，那么就会发现，比较新的线性Attention（参考<a href="/archives/11033">《线性注意力简史：从模仿、创新到反哺》</a>）模型都给$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}$加上了Short Conv，比如下图所示的<a href="https://papers.cool/arxiv/2406.06484">DeltaNet</a>：  </p>
<p><a href="/usr/uploads/2025/10/175536171.png" title="点击查看原图"><img alt="DeltaNet中的Short Conv" src="/usr/uploads/2025/10/175536171.png" /></a></p>
<p>DeltaNet中的Short Conv</p>
<p>为什么要加这个Short Conv呢？直观理解可能是增加模型深度、增强模型的Token-Mixing能力等，说白了就是补偿线性化导致的表达能力下降。这个说法当然是大差不差，但它属于“万能模版”式的回答，我们更想对它的生效机制有更准确的认知。</p>
<p>接下来，笔者将给出自己的一个理解（更准确说应该是猜测）。</p>
<h2 id="_1">测试训练<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>从<a href="/archives/11033">《线性注意力简史：从模仿、创新到反哺》</a>我们知道，目前的新式线性Attention背后的核心思想都是<a href="https://papers.cool/arxiv/2407.04620">TTT（Test Time Training）</a>或者说Online Learning。TTT基于优化器更新与RNN迭代的相似性，通过优化器来构建（不一定是线性的）RNN模型，诸如DeltaNet、GDN、Comba等线性Attention都可以看成它的特例。</p>
<p>具体来说，TTT将$\boldsymbol{K},\boldsymbol{V}$视为成对的语料$(\boldsymbol{k}<em t-1="t-1">1, \boldsymbol{v}_1),(\boldsymbol{k}_2, \boldsymbol{v}_2),\cdots,(\boldsymbol{k}_t, \boldsymbol{v}_t)$，我们用它训练一个模型$\boldsymbol{v} = \boldsymbol{f}(\boldsymbol{S}_t;\boldsymbol{k})$，然后输出$\boldsymbol{o}_t = \boldsymbol{f}(\boldsymbol{S}_t;\boldsymbol{q}_t)$，其中$\boldsymbol{S}_t$是模型参数，用SGD更新：<br />
\begin{equation} \boldsymbol{S}_t = \boldsymbol{S}</em>} - \eta_t\nabla_{\boldsymbol{S<em t-1="t-1">{t-1}}\mathcal{L}(\boldsymbol{f}(\boldsymbol{S}</em>};\boldsymbol{k<em t-1="t-1">t), \boldsymbol{v}_t)\end{equation}<br />
当然，如果我们愿意，也可以考虑其他优化器，比如<a href="https://papers.cool/arxiv/2505.23884">《Test-Time Training Done Right》</a>尝试了Muon优化器。除了可以改变优化器外，可以灵活改变的地方还有模型$\boldsymbol{v} = \boldsymbol{f}(\boldsymbol{S}_t;\boldsymbol{k})$的架构以及损失函数$\mathcal{L}(\boldsymbol{f}(\boldsymbol{S}</em>_t)$。此外，我们还可以考虑以Chunk为单位的Mini-batch TTT。};\boldsymbol{k}_t), \boldsymbol{v</p>
<p>不难想象，理论上TTT的灵活性非常高，可以构建任意复杂的RNN模型。当架构选择线性模型$\boldsymbol{v} = \boldsymbol{S}_t\boldsymbol{k}$且损失函数选择平方误差时，结果对应DeltaNet；如果我们加上一些正则项，那么可以衍生出GDN等变体。</p>
<h2 id="_2">灵魂拷问<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>将TTT放在前面，主要是想表明，当前主流的线性Attention的底层逻辑跟TTT一样，核心都是语料对$(\boldsymbol{k}_1, \boldsymbol{v}_1),(\boldsymbol{k}_2, \boldsymbol{v}_2),\cdots,(\boldsymbol{k}_t, \boldsymbol{v}_t)$的Online Learning。这就很自然地引申出一个疑问：为什么要这样做？这样做究竟学出个什么来？</p>
<p>要回答这个问题，首先得反思一下“我们究竟想要什么”。按照Softmax Attention的特点，我们想要的应该是根据$(\boldsymbol{k}_1, \boldsymbol{v}_1),(\boldsymbol{k}_2, \boldsymbol{v}_2),\cdots,(\boldsymbol{k}_t, \boldsymbol{v}_t)$和$\boldsymbol{q}_t$算出一个$\boldsymbol{o}_t$来，这个过程理想情况下应该依赖于全体$(\boldsymbol{k},\boldsymbol{v})$。同时，我们还希望能常数复杂度实现这个目标，所以一个直观想法是先将$(\boldsymbol{k},\boldsymbol{v})$压缩成（与$t$无关的）固定大小的State，然后再读取这个State。</p>
<p>怎么实现这个压缩呢？TTT的想法是：设计一个模型$\boldsymbol{v} = \boldsymbol{f}(\boldsymbol{S}_t;\boldsymbol{k})$，然后用这些$(\boldsymbol{k},\boldsymbol{v})$对去“训练”这个模型，训练完成后，模型某种意义上就“背下”了这些$(\boldsymbol{k},\boldsymbol{v})$对，这就相当于就全体$(\boldsymbol{k},\boldsymbol{v})$压缩到了固定大小的模型权重$\boldsymbol{S}_t$中。至于$\boldsymbol{q}
_t$怎么利用$\boldsymbol{S}_t$，直接将它代入模型中得到$\boldsymbol{o}
_t = \boldsymbol{f}(\boldsymbol{S}_t;\boldsymbol{q}_t)$是一个比较自然的选择，但原则上我们也可以设计别的利用方式。</p>
<p>也就是说，TTT的核心任务是利用 <em>“训练模型”约等于“背诵训练集”</em> 这件事情，来实现$\boldsymbol{K},\boldsymbol{V}$的压缩。然而， <em>“训练模型”约等于“背诵训练集”</em> 这件事情并不是那么平凡，它有一些前提条件。</p>
<h2 id="_3">键值同源<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>举个例子，如果我们取$\boldsymbol{K}=\boldsymbol{V}$，那么TTT这套框架理论上就失效了，因为这时候模型$\boldsymbol{v} = \boldsymbol{f}(\boldsymbol{S}_t;\boldsymbol{k})$的最优解就是恒等变换，它是一个平凡解，这相当于没记住任何东西。像DeltaNet这种在线更新的或许还能抢救一下，而像<a href="https://papers.cool/arxiv/2506.05233">MesaNet</a>这种基于准确解的就真的直接输出单位阵$\boldsymbol{I}$了。</p>
<p>可能有读者会反问：好端端地为啥要考虑$\boldsymbol{K}=\boldsymbol{V}$这种不科学的选择呢？的确，$\boldsymbol{K}=\boldsymbol{V}$是比较极端的选择，这里只是作为一个例子，说明 <em>“训练模型”约等于“背诵训练集”</em> 并不是随意成立的。其次，我们在<a href="/archives/10907">《Transformer升级之路：20、MLA好在哪里?（上）》</a>已经验证过，对于Softmax Attention来说，$\boldsymbol{K}=\boldsymbol{V}$也能取得不错的结果。</p>
<p>这说明，$\boldsymbol{K}=\boldsymbol{V}$并不是Attention机制的本质障碍，但在TTT框架里边它却能导致模型失效，这是因为$\boldsymbol{K},\boldsymbol{V}$完全重合，那么它们俩之间的回归就没有东西可学了。类似地，我们可以想象，$\boldsymbol{K},\boldsymbol{V}$的信息重合度越高，那么它们之间可学的东西就越少，换言之TTT对“训练集”的记忆程度就越低。</p>
<p>在一般的Attention机制中，$\boldsymbol{q}_t,\boldsymbol{k}_t,\boldsymbol{v}_t$都是由同一个输入$\boldsymbol{x}_t$经过不同的线性投影得到的，换句话说$\boldsymbol{k}_t,\boldsymbol{v}_t$具有相同的源头$\boldsymbol{x}_t$，这就始终有种“自己预测自己”的感觉，可学的东西有限。</p>
<h2 id="_4">卷积救场<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>怎么让TTT在键值同源甚至$\boldsymbol{K}=\boldsymbol{V}$时能学出更有价值的结果呢？其实答案很早就有了——可以追溯到Word2Vec甚至更早的年代——那就是不要“预测自己”，而是“预测周围”。</p>
<p>以<a href="https://papers.cool/arxiv/1301.3781">Word2Vec</a>为例，我们知道它的训练方式是“中心词预测上下文”；之前流行的<a href="https://papers.cool/arxiv/1810.04805">BERT</a>，预训练方式是MLM，它是某些词Mask掉来预测这些词，可以说是“上下文预测中心词”；现在主流的LLM，训练任务则是NTP（Next Token Predict），根据上文预测下一个词。很明显，它们的共同特点都是不预测自己，而是预测周围。</p>
<p>所以，想要改进TTT，那么就要改变$(\boldsymbol{k}<em t-1="t-1">t,\boldsymbol{v}_t)$这样的“自己预测自己”的配对方式，考虑到当前LLM以NTP为主，在TTT中我们也可以考虑NTP，比如$(\boldsymbol{k}</em>},\boldsymbol{v<em t-1="t-1">t)$来构建语料对，即用$\boldsymbol{k}</em>$也能学出非平凡的结果。此时TTT内外都是NTP任务，具有漂亮的一致性。}$来预测$\boldsymbol{v}_t$，这样即便$\boldsymbol{K}=\boldsymbol{V</p>
<p>不过，只用$\boldsymbol{k}<em t-1="t-1">{t-1}$预测$\boldsymbol{v}_t$的话，似乎把$\boldsymbol{k}_t$浪费掉了，所以进一步的想法是把$\boldsymbol{k}</em>}$和$\boldsymbol{k<em t-1="t-1">t$以某种方式混合起来再预测$\boldsymbol{v}_t$。到这里，大家可能反应过来了，“把$\boldsymbol{k}</em>$加Short Conv，是将TTT的训练目标从“预测自己”转化为NTP，让TTT至少有能力学出一个n-gram模型。}$和$\boldsymbol{k}_t$以某种方式混合起来”这不就相当于kernel_size=2的Conv嘛！所以，给$\boldsymbol{K</p>
<p>至于给$\boldsymbol{Q},\boldsymbol{V}$加Short Conv，则完全是顺带的，根据飞来阁（FLA群）的消息，给$\boldsymbol{Q},\boldsymbol{V}$加虽然也有一点作用，但远不如给$\boldsymbol{K}$加Short Conv带来的提升，这也算是佐证了我们的猜测。</p>
<h2 id="_5">文章小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>这篇文章对“为什么线性注意力要加Short Conv”这个问题给出了一个闭门造车的理解。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11320">https://spaces.ac.cn/archives/11320</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Oct. 05, 2025). 《为什么线性注意力要加Short Conv？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11320">https://spaces.ac.cn/archives/11320</a></p>
<p>@online{kexuefm-11320,<br />
title={为什么线性注意力要加Short Conv？},<br />
author={苏剑林},<br />
year={2025},<br />
month={Oct},<br />
url={\url{https://spaces.ac.cn/archives/11320}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="adamw的weight-rms的渐近估计.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#347 AdamW的Weight RMS的渐近估计</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="为什么线性注意力要加short-conv.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#349 为什么线性注意力要加Short Conv？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#short-c">为什么线性注意力要加Short C...</a><ul>
<li><a href="#_1">测试训练</a></li>
<li><a href="#_2">灵魂拷问</a></li>
<li><a href="#_3">键值同源</a></li>
<li><a href="#_4">卷积救场</a></li>
<li><a href="#_5">文章小结</a></li>
<li><a href="#_6">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>