<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WGAN新方案：通过梯度归一化来实现L约束 | ML & Math Blog Posts</title>
    <meta name="description" content="WGAN新方案：通过梯度归一化来实现L约束&para;
原文链接: https://spaces.ac.cn/archives/8757
发布日期: 

当前，WGAN主流的实现方式包括参数裁剪（Weight Clipping）、谱归一化（Spectral Normalization）、梯度惩罚（Gradient Penalty），本来则来介绍一种新的实现方案：梯度归一化（Gradient Nor...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=无监督">无监督</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #152 WGAN新方案：通过梯度归一化来实现L约束
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#152</span>
                WGAN新方案：通过梯度归一化来实现L约束
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2021-11-15</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=无监督" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 无监督</span>
                </a>
                
                <a href="../index.html?tags=GAN" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> GAN</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="wganl">WGAN新方案：通过梯度归一化来实现L约束<a class="toc-link" href="#wganl" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8757">https://spaces.ac.cn/archives/8757</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>当前，WGAN主流的实现方式包括参数裁剪（Weight Clipping）、谱归一化（Spectral Normalization）、梯度惩罚（Gradient Penalty），本来则来介绍一种新的实现方案：梯度归一化（Gradient Normalization），该方案出自两篇有意思的论文，分别是<a href="https://papers.cool/arxiv/2109.02235">《Gradient Normalization for Generative Adversarial Networks》</a>和<a href="https://papers.cool/arxiv/2111.03162">《GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks》</a>。</p>
<p>有意思在什么地方呢？从标题可以看到，这两篇论文应该是高度重合的，甚至应该是同一作者的。但事实上，这是两篇不同团队的、大致是同一时期的论文，一篇中了ICCV，一篇中了WACV，它们基于同样的假设推出了几乎一样的解决方案，内容重合度之高让我一直以为是同一篇论文。果然是巧合无处不在啊～</p>
<h2 id="_1">基础回顾<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>关于WGAN，我们已经介绍过多次，比如<a href="/archives/4439">《互怼的艺术：从零直达WGAN-GP》</a>和<a href="/archives/6280">《从Wasserstein距离、对偶理论到WGAN》</a>，这里就不详细重复了。简单来说，WGAN的迭代形式为：<br />
\begin{equation}\min_G \max_{\Vert D\Vert_{L}\leq 1} \mathbb{E}<em q_z_="q(z)" z_sim="z\sim">{x\sim p(x)}\left[D(x)\right] - \mathbb{E}</em>}\left[D(G(z))\right]\end{equation
这里的关键是判别器$D$是一个带约束优化问题，需要在优化过程中满足L约束$\Vert D\Vert_{L}\leq 1$，所以WGAN的实现难度就是如何往$D$里边引入该约束。</p>
<p>这里再普及一下，如果存在某个常数$C$，使得定义域中的任意$x,y$都满足$|f(x)-f(y)|\leq C\Vert x - y\Vert$，那么我们称$f(x)$满足<a href="/archives/6051">Lipschitz约束</a>（L约束），其中$C$的最小值，我们称为Lipschitz常数（L常数），记为$\Vert f\Vert_{L}$。所以，对于WGAN判别器来说，要做到两步：1、$D$要满足L约束；2、L常数要不超过1。</p>
<p>事实上，当前我们主流的神经网络模型，都是“线性组合+非线性激活函数”的形式，而主流的激活函数是“近线性的”，比如ReLU、LeakyReLU、SoftPlus等，它们的导函数的绝对值都不超过1，所以当前主流的模型其实都满足L约束，所以关键是如何让L常数不超过1，当然其实也不用非1不可，能保证它不超过某个固定常数就行。</p>
<h2 id="_2">方案简介<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>参数裁剪和谱归一化的思路是相似的，它们都是通过约束参数，保证模型每一层的L常数都有界，所以总的L常数也有界；而梯度惩罚则是留意到$\Vert D\Vert_{L}\leq 1$的一个充分条件是$\Vert \nabla_x D(x)\Vert \leq 1$，所以就通过惩罚项$(\Vert \nabla_x D(x)\Vert - 1)^2$来施加“软约束”。</p>
<p>本文介绍的梯度归一化，也是基于同样的充分条件，它利用梯度将$D(x)$变换为$\hat{D}(x)$，使其自动满足$\Vert\nabla_x \hat{D}(x)\Vert \leq 1$。具体来说，我们通常用ReLU或LeakyReLU作为激活函数，在这个激活函数之下，$D(x)$实际上是一个“分段线性函数”，这就意味着，除了边界之外，$D(x)$在局部的连续区域内都是一个线性函数，相应地，$\nabla_x D(x)$就是一个常向量。</p>
<p>于是梯度归一化就想着令$\hat{D}(x)=D(x)/\Vert \nabla_x D(x)\Vert$，这样一来就有<br />
\begin{equation}\Vert\nabla_x \hat{D}(x)\Vert = \left\Vert \nabla_x \left(\frac{D(x)}{\Vert \nabla_x D(x)\Vert}\right)\right\Vert=\left\Vert \frac{\nabla_x D(x)}{\Vert \nabla_x D(x)\Vert}\right\Vert=1\end{equation}<br />
当然，这样可能会有除0错误，所以两篇论文提出了不同的解决方案，第一篇（ICCV论文）直接将$|D(x)|$也加到了分母中，连带保证了函数的有界性：<br />
\begin{equation} \hat{D}(x) = \frac{D(x)}{\Vert \nabla_x D(x)\Vert + |D(x)|}\in [-1,1]\end{equation}<br />
第二篇（WACV论文）则是比较朴素地加了个$\epsilon$：<br />
\begin{equation} \hat{D}(x) = \frac{D(x)\cdot \Vert \nabla_x D(x)\Vert}{\Vert \nabla_x D(x)\Vert^2 + \epsilon}\end{equation}<br />
同时第二篇也提到试验过$\hat{D}(x)=D(x)/(\Vert \nabla_x D(x)\Vert+\epsilon)$，效果略差但差不多。</p>
<h2 id="_3">实验结果<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>现在我们先来看看实验结果。当然，能双双中顶会，实验结果肯定是正面的，部分结果如下图：  </p>
<p><a href="/usr/uploads/2021/11/1767201445.png" title="点击查看原图"><img alt="ICCV论文的实验结果表格" src="/usr/uploads/2021/11/1767201445.png" /></a></p>
<p>ICCV论文的实验结果表格</p>
<p><a href="/usr/uploads/2021/11/3926324442.png" title="点击查看原图"><img alt="WACV论文的实验结果表格" src="/usr/uploads/2021/11/3926324442.png" /></a></p>
<p>WACV论文的实验结果表格</p>
<p><a href="/usr/uploads/2021/11/1557710352.png" title="点击查看原图"><img alt="ICCV论文的生成效果演示" src="/usr/uploads/2021/11/1557710352.png" /></a></p>
<p>ICCV论文的生成效果演示</p>
<h2 id="_4">尚有疑问<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>结果看上去很好，理论看上去也没问题，还同时被两个顶会认可，看上去是一个好工作无疑了。然而，笔者的困惑才刚刚开始。</p>
<p>该工作最重要的问题是，如果按照分段线性函数的假设，那么$D(x)$的梯度虽然在局部是一个常数，但整体来看它是不连续的（如果梯度全局连续又是常数，那么就是一个线性函数而不是分段线性了），然而$D(x)$本身是一个连续函数，那么$\hat{D}(x)=D(x)/\Vert \nabla_x D(x)\Vert$就是连续函数除以不连续函数，结果就是一个不连续的函数！</p>
<p>所以问题就来了，不连续的函数居然可以作为判别器，这看起来相当不可思议。要知道这个不连续并非只在某些边界点不连续，而是在两个区域之间的不连续，所以这个不连续是不可忽略的存在。在Reddit上，也有读者有着同样的疑问，但目前作者也没有给出合理的解释（<a href="https://www.reddit.com/r/MachineLearning/comments/pjdvi4/r_iccv_2021_gradient_normalization_for_generative/">链接</a>）。</p>
<p>另一个问题是，如果分段线性函数的假设真的有效，那么我用$\hat{D}(x)=\left\langle \frac{\nabla_x D(x)}{\Vert \nabla_x D(x)\Vert}, x\right\rangle$作为判别器，理论上应该是等价的，但笔者的实验结果显示这样的$\hat{D}(x)$效果极差。所以，有一种可能性就是，梯度归一化确实是有效的，但其作用的原因并不像上面两篇论文分析的那么简单，也许有更复杂的生效机制我们还没发现。此外，也可能是我们对GAN的理解还远远不够充分，也就是说，对判别器的连续性等要求，也许远远不是我们所想的那样。</p>
<p>最后，在笔者的实验结果中，梯度归一化的效果并不如梯度惩罚，并且梯度惩罚仅仅是训练判别器的时候用到了二阶梯度，而梯度归一化则是训练生成器和判别器都要用到二阶梯度，所以梯度归一化的速度明显下降，显存占用量也明显增加。所以从个人实际体验来看，梯度归一化不算一个特别友好的方案。</p>
<h2 id="_5">文章小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文介绍了一种实现WGAN的新方案——梯度归一化，该方案形式上比较简单，论文报告的效果也还不错，但个人认为其中还有不少值得疑问之处。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8757">https://spaces.ac.cn/archives/8757</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Nov. 15, 2021). 《WGAN新方案：通过梯度归一化来实现L约束 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8757">https://spaces.ac.cn/archives/8757</a></p>
<p>@online{kexuefm-8757,<br />
title={WGAN新方案：通过梯度归一化来实现L约束},<br />
author={苏剑林},<br />
year={2021},<br />
month={Nov},<br />
url={\url{https://spaces.ac.cn/archives/8757}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<h3 id="1-wganlipschitz">1. WGAN与Lipschitz约束的数学基础<a class="toc-link" href="#1-wganlipschitz" title="Permanent link">&para;</a></h3>
<h4 id="11-wasserstein">1.1 Wasserstein距离的定义<a class="toc-link" href="#11-wasserstein" title="Permanent link">&para;</a></h4>
<p>Wasserstein距离（也称Earth Mover's Distance）定义为：</p>
<p>\begin{equation}
W(p, q) = \inf_{\gamma \in \Pi(p,q)} \mathbb{E}_{(x,y)\sim\gamma}[|x - y|] \tag{1}
\end{equation}</p>
<p>其中$\Pi(p,q)$是边际分布为$p$和$q$的所有联合分布的集合。</p>
<p><strong>数学直觉</strong>：Wasserstein距离衡量将分布$p$"搬运"到分布$q$所需的最小代价。</p>
<h4 id="12-kantorovich-rubinstein">1.2 Kantorovich-Rubinstein对偶形式<a class="toc-link" href="#12-kantorovich-rubinstein" title="Permanent link">&para;</a></h4>
<p>通过Kantorovich-Rubinstein对偶理论，Wasserstein距离可以改写为：</p>
<p>\begin{equation}
W(p, q) = \sup_{|f|<em p="p" x_sim="x\sim">L \leq 1} \mathbb{E}</em>
\end{equation}}[f(x)] - \mathbb{E}_{y\sim q}[f(y)] \tag{2</p>
<p>其中$|f|_L \leq 1$表示$f$是1-Lipschitz函数。</p>
<p><strong>证明思路</strong>：这是凸优化中的对偶理论应用，将原问题从联合分布空间转换到函数空间。</p>
<h4 id="13-wgan">1.3 WGAN的优化目标<a class="toc-link" href="#13-wgan" title="Permanent link">&para;</a></h4>
<p>在GAN框架下，令$p$为真实数据分布，$q$为生成分布$G_{#}p_z$（$p_z$是噪声分布），则：</p>
<p>\begin{equation}
\min_G \max_{D: |D|<em p__data="p_{data" x_sim="x\sim">L \leq 1} \mathbb{E}</em>
\end{equation}}}[D(x)] - \mathbb{E}_{z\sim p_z}[D(G(z))] \tag{3</p>
<p><strong>关键挑战</strong>：如何在优化过程中保证判别器$D$满足Lipschitz约束$|D|_L \leq 1$？</p>
<h4 id="14-lipschitz">1.4 Lipschitz约束的精确定义<a class="toc-link" href="#14-lipschitz" title="Permanent link">&para;</a></h4>
<p>函数$f: \mathbb{R}^n \to \mathbb{R}$满足Lipschitz约束，当且仅当存在常数$K \geq 0$使得：</p>
<p>\begin{equation}
|f(x) - f(y)| \leq K|x - y| \quad \forall x, y \in \text{dom}(f) \tag{4}
\end{equation}</p>
<p>Lipschitz常数定义为：</p>
<p>\begin{equation}
|f|<em _neq="\neq" x="x" y="y">L = \sup</em>
\end{equation}} \frac{|f(x) - f(y)|}{|x - y|} \tag{5</p>
<p><strong>等价条件</strong>（对可微函数）：</p>
<p>\begin{equation}
|f|<em x="x">L = \sup</em>
\end{equation}} |\nabla f(x)| \tag{6</p>
<p><strong>证明</strong>：由中值定理，对于连续可微函数：
\begin{align}
|f(x) - f(y)| &amp;= \left|\int_0^1 \nabla f(y + t(x-y)) \cdot (x-y) dt\right| \tag{7} \
&amp;\leq \int_0^1 |\nabla f(y + t(x-y))| \cdot |x-y| dt \tag{8} \
&amp;\leq \sup_z |\nabla f(z)| \cdot |x-y| \tag{9}
\end{align}</p>
<p>因此$|f|_L \leq \sup_x |\nabla f(x)|$。反向不等式通过取极限可以证明。</p>
<h4 id="15-lipschitz">1.5 现有Lipschitz约束实现方法<a class="toc-link" href="#15-lipschitz" title="Permanent link">&para;</a></h4>
<p><strong>方法1：权重裁剪（Weight Clipping）</strong></p>
<p>\begin{equation}
w \leftarrow \text{clip}(w, -c, c) \tag{10}
\end{equation}</p>
<p><strong>优点</strong>：简单易实现
<strong>缺点</strong>：
- 过度约束，降低模型表达能力
- 裁剪阈值$c$难以选择
- 可能导致梯度消失或爆炸</p>
<p><strong>方法2：谱归一化（Spectral Normalization）</strong></p>
<p>\begin{equation}
\bar{W} = \frac{W}{\sigma(W)} \tag{11}
\end{equation}</p>
<p>其中$\sigma(W)$是权重矩阵的最大奇异值。</p>
<p><strong>理论基础</strong>：对于线性层$f(x) = Wx$：
\begin{equation}
|f|<em _124_x_124_="1">L = \sup</em>
\end{equation}} |Wx| = \sigma_{\max}(W) \tag{12</p>
<p>通过归一化确保每层的Lipschitz常数为1，整体网络的Lipschitz常数有界。</p>
<p><strong>方法3：梯度惩罚（Gradient Penalty）</strong></p>
<p>\begin{equation}
\mathcal{L}<em _hat_x="\hat{x">{GP} = \mathbb{E}</em>
\end{equation}}}[(|\nabla_{\hat{x}} D(\hat{x})| - 1)^2] \tag{13</p>
<p>其中$\hat{x} = \epsilon x + (1-\epsilon)G(z)$是真实样本和生成样本之间的插值。</p>
<p><strong>数学依据</strong>：$|D|_L \leq 1$的充分条件是$|\nabla_x D(x)| \leq 1$对所有$x$成立。</p>
<h3 id="2">2. 梯度归一化方法的理论推导<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 分段线性函数的性质<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p>现代神经网络使用ReLU或LeakyReLU激活函数：</p>
<p>\begin{equation}
\text{ReLU}(x) = \max(0, x) \tag{14}
\end{equation}</p>
<p>\begin{equation}
\text{LeakyReLU}(x) = \max(\alpha x, x), \quad \alpha \in (0, 1) \tag{15}
\end{equation}</p>
<p><strong>关键性质</strong>：由这些激活函数构成的网络是分段线性函数。</p>
<p><strong>定义</strong>：函数$f: \mathbb{R}^n \to \mathbb{R}$是分段线性的，如果其定义域可以分解为有限个多面体区域${R_i}_{i=1}^N$，在每个区域内$f$是线性函数：</p>
<p>\begin{equation}
f(x) = \boldsymbol{w}_i^T x + b_i, \quad x \in R_i \tag{16}
\end{equation}</p>
<p><strong>重要推论</strong>：在每个区域$R_i$内部，梯度是常向量：</p>
<p>\begin{equation}
\nabla f(x) = \boldsymbol{w}_i, \quad x \in \text{int}(R_i) \tag{17}
\end{equation}</p>
<h4 id="22">2.2 梯度归一化的基本思想<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>给定判别器$D(x)$，构造新的判别器：</p>
<p>\begin{equation}
\hat{D}(x) = \frac{D(x)}{|\nabla_x D(x)|} \tag{18}
\end{equation}</p>
<p><strong>目标</strong>：使$\hat{D}$自动满足Lipschitz约束。</p>
<p><strong>梯度计算</strong>：在分段线性区域内，$\nabla D(x) = \boldsymbol{w}$是常数，因此：</p>
<p>\begin{align}
\nabla_x \hat{D}(x) &amp;= \nabla_x \left(\frac{D(x)}{|\nabla_x D(x)|}\right) \tag{19} \
&amp;= \frac{\nabla_x D(x)}{|\nabla_x D(x)|} \tag{20} \
&amp;= \frac{\boldsymbol{w}}{|\boldsymbol{w}|} \tag{21}
\end{align}</p>
<p><strong>模长计算</strong>：</p>
<p>\begin{equation}
|\nabla_x \hat{D}(x)| = \left|\frac{\boldsymbol{w}}{|\boldsymbol{w}|}\right| = 1 \tag{22}
\end{equation}</p>
<p><strong>结论</strong>：$\hat{D}$在每个线性区域内的Lipschitz常数为1！</p>
<h4 id="23">2.3 边界点与不连续性问题<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：在区域边界处，$\nabla D(x)$不连续，导致$\hat{D}(x)$可能不连续。</p>
<p><strong>数学分析</strong>：考虑两个相邻区域$R_1$和$R_2$，在边界点$x_0 \in \partial R_1 \cap \partial R_2$：</p>
<p>\begin{align}
\lim_{x \to x_0, x \in R_1} \hat{D}(x) &amp;= \frac{D(x_0)}{|\boldsymbol{w}<em R_2="R_2" _in="\in" _to="\to" x="x" x_0_="x_0,">1|} \tag{23} \
\lim</em>
\end{align}} \hat{D}(x) &amp;= \frac{D(x_0)}{|\boldsymbol{w}_2|} \tag{24</p>
<p>如果$|\boldsymbol{w}_1| \neq |\boldsymbol{w}_2|$，则$\hat{D}$在$x_0$处不连续！</p>
<p><strong>不连续性的度量</strong>：</p>
<p>\begin{equation}
\text{Jump}(x_0) = \left|\frac{D(x_0)}{|\boldsymbol{w}_1|} - \frac{D(x_0)}{|\boldsymbol{w}_2|}\right| = |D(x_0)| \left|\frac{1}{|\boldsymbol{w}_1|} - \frac{1}{|\boldsymbol{w}_2|}\right| \tag{25}
\end{equation}</p>
<h4 id="24-iccv">2.4 ICCV方案：添加函数值归一化<a class="toc-link" href="#24-iccv" title="Permanent link">&para;</a></h4>
<p>为避免除零和不连续性，ICCV论文提出：</p>
<p>\begin{equation}
\hat{D}(x) = \frac{D(x)}{|\nabla_x D(x)| + |D(x)|} \tag{26}
\end{equation}</p>
<p><strong>有界性证明</strong>：</p>
<p>\begin{align}
|\hat{D}(x)| &amp;= \frac{|D(x)|}{|\nabla_x D(x)| + |D(x)|} \tag{27} \
&amp;\leq \frac{|D(x)|}{|D(x)|} = 1 \tag{28}
\end{align}</p>
<p>因此$\hat{D}(x) \in [-1, 1]$。</p>
<p><strong>Lipschitz常数分析</strong>：设在区域内$\nabla D = \boldsymbol{w}$，$D(x) = \boldsymbol{w}^T x + b$。</p>
<p>\begin{equation}
\hat{D}(x) = \frac{\boldsymbol{w}^T x + b}{|\boldsymbol{w}| + |\boldsymbol{w}^T x + b|} \tag{29}
\end{equation}</p>
<p>梯度计算（使用商法则和链式法则）：</p>
<p>\begin{align}
\nabla_x \hat{D}(x) &amp;= \frac{\boldsymbol{w}(|\boldsymbol{w}| + |D|) - (\boldsymbol{w}^Tx + b)(\boldsymbol{w}\text{sign}(D))}{(|\boldsymbol{w}| + |D|)^2} \tag{30} \
&amp;= \frac{\boldsymbol{w}|\boldsymbol{w}| + \boldsymbol{w}|D| - D\boldsymbol{w}\text{sign}(D)}{(|\boldsymbol{w}| + |D|)^2} \tag{31} \
&amp;= \frac{\boldsymbol{w}|\boldsymbol{w}|}{(|\boldsymbol{w}| + |D|)^2} \tag{32}
\end{align}</p>
<p>（因为$D\text{sign}(D) = |D|$）</p>
<p>模长：</p>
<p>\begin{equation}
|\nabla_x \hat{D}(x)| = \frac{|\boldsymbol{w}|^2}{(|\boldsymbol{w}| + |D|)^2} \leq 1 \tag{33}
\end{equation}</p>
<p><strong>结论</strong>：此方案确保$|\nabla_x \hat{D}(x)| \leq 1$，满足Lipschitz约束。</p>
<h4 id="25-wacvepsilon">2.5 WACV方案：添加epsilon稳定项<a class="toc-link" href="#25-wacvepsilon" title="Permanent link">&para;</a></h4>
<p>WACV论文提出：</p>
<p>\begin{equation}
\hat{D}(x) = \frac{D(x) \cdot |\nabla_x D(x)|}{|\nabla_x D(x)|^2 + \epsilon} \tag{34}
\end{equation}</p>
<p>其中$\epsilon &gt; 0$是小常数（如$10^{-8}$）。</p>
<p><strong>简化形式</strong>：</p>
<p>\begin{equation}
\hat{D}(x) = \frac{D(x)}{1 + \epsilon/|\nabla_x D(x)|^2} \tag{35}
\end{equation}</p>
<p>当$|\nabla_x D(x)|$较大时，近似为：</p>
<p>\begin{equation}
\hat{D}(x) \approx \frac{D(x) \cdot |\nabla_x D(x)|}{|\nabla_x D(x)|^2} = \frac{D(x)}{|\nabla_x D(x)|} \tag{36}
\end{equation}</p>
<p><strong>Lipschitz常数分析</strong>：</p>
<p>设$g = |\nabla_x D|^2$，则：</p>
<p>\begin{align}
\nabla_x \hat{D}(x) &amp;= \nabla_x \left(\frac{D \cdot \sqrt{g}}{g + \epsilon}\right) \tag{37} \
&amp;= \frac{(\nabla D \sqrt{g} + D \frac{\nabla g}{2\sqrt{g}})(g+\epsilon) - D\sqrt{g} \nabla g}{(g+\epsilon)^2} \tag{38}
\end{align}</p>
<p>在分段线性区域内，$\nabla D = \boldsymbol{w}$是常数，所以$\nabla g = 0$，简化为：</p>
<p>\begin{equation}
\nabla_x \hat{D}(x) = \frac{\boldsymbol{w} |\boldsymbol{w}|}{(|\boldsymbol{w}|^2 + \epsilon)} \tag{39}
\end{equation}</p>
<p>模长：</p>
<p>\begin{equation}
|\nabla_x \hat{D}(x)| = \frac{|\boldsymbol{w}|^2}{|\boldsymbol{w}|^2 + \epsilon} &lt; 1 \tag{40}
\end{equation}</p>
<p><strong>结论</strong>：满足严格的Lipschitz约束。</p>
<h4 id="26">2.6 两种方案的对比分析<a class="toc-link" href="#26" title="Permanent link">&para;</a></h4>
<p><strong>ICCV方案</strong>：$\hat{D}(x) = \frac{D(x)}{|\nabla_x D(x)| + |D(x)|}$</p>
<p><strong>优点</strong>：
- 自动有界：$|\hat{D}(x)| \leq 1$
- 同时归一化梯度和函数值
- 理论上更优雅</p>
<p><strong>缺点</strong>：
- 计算稍复杂（需要计算$|D(x)|$）
- 在$D(x) \approx 0$附近行为可能不稳定</p>
<p><strong>WACV方案</strong>：$\hat{D}(x) = \frac{D(x) \cdot |\nabla_x D(x)|}{|\nabla_x D(x)|^2 + \epsilon}$</p>
<p><strong>优点</strong>：
- 形式简单
- 通过$\epsilon$控制数值稳定性
- 当梯度较大时接近理想归一化</p>
<p><strong>缺点</strong>：
- 函数值可能无界（虽然实际中有界）
- $\epsilon$的选择需要调参</p>
<h4 id="27">2.7 梯度归一化的反向传播<a class="toc-link" href="#27" title="Permanent link">&para;</a></h4>
<p><strong>关键问题</strong>：梯度归一化需要计算$\nabla_x D(x)$，这本身需要反向传播。在训练时，我们需要对$\hat{D}$再求梯度，涉及<strong>二阶导数</strong>。</p>
<p><strong>计算图</strong>：</p>
<p>\begin{equation}
x \xrightarrow{D} y = D(x) \xrightarrow{\text{normalize}} \hat{y} = \hat{D}(x) \xrightarrow{\mathcal{L}} \text{loss} \tag{41}
\end{equation}</p>
<p><strong>一阶梯度</strong>（对判别器训练）：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \theta_D} = \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial D} \cdot \frac{\partial D}{\partial \theta_D} + \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial (\nabla_x D)} \cdot \frac{\partial (\nabla_x D)}{\partial \theta_D} \tag{42}
\end{equation}</p>
<p><strong>二阶梯度</strong>（对生成器训练）：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial \theta_G} = \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial x} \cdot \frac{\partial x}{\partial \theta_G} \tag{43}
\end{equation}</p>
<p>其中$\frac{\partial \hat{y}}{\partial x}$涉及Hessian矩阵$\nabla_x^2 D(x)$。</p>
<p><strong>计算复杂度</strong>：
- 标准GAN：$O(|\theta_D|)$
- 梯度惩罚：$O(|\theta_D|)$（只在判别器训练时）
- 梯度归一化：$O(|\theta_D|)$（判别器和生成器训练时都需要）</p>
<p><strong>显存占用</strong>：
- 标准GAN：$O(|\theta_D|)$
- 梯度归一化：$O(|\theta_D| + |x|)$（需要存储一阶梯度）</p>
<h3 id="3">3. 理论问题的深入分析<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 不连续性悖论<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p><strong>观察</strong>：虽然$\hat{D}(x)$可能在边界处不连续，但实验效果良好。</p>
<p><strong>可能解释1：测度零集</strong></p>
<p>区域边界在$\mathbb{R}^n$中是$n-1$维流形，Lebesgue测度为零：</p>
<p>\begin{equation}
\mu(\bigcup_{i} \partial R_i) = 0 \tag{44}
\end{equation}</p>
<p>因此在优化过程中，几乎不会采样到边界点。</p>
<p><strong>可能解释2：软边界</strong></p>
<p>在实际计算中，浮点精度限制使得"硬"边界变成"软"边界：</p>
<p>\begin{equation}
\text{ReLU}_{\text{soft}}(x) = \begin{cases}
0, &amp; x &lt; -\delta \
\frac{(x+\delta)^2}{4\delta}, &amp; |x| \leq \delta \
x, &amp; x &gt; \delta
\end{cases} \tag{45}
\end{equation}</p>
<p>宽度为$\delta$的软边界使得函数连续可微。</p>
<p><strong>可能解释3：判别器正则化</strong></p>
<p>不连续性本身可能起到正则化作用，防止判别器过拟合。类似于Dropout的效果。</p>
<h4 id="32">3.2 与梯度惩罚的理论对比<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p><strong>梯度惩罚</strong>：</p>
<p>\begin{equation}
\min_G \max_D \mathbb{E}<em q="q" z_sim="z\sim">{x\sim p}[D(x)] - \mathbb{E}</em>}[D(G(z))] - \lambda \mathbb{E<em _hat_x="\hat{x">{\hat{x}}[(|\nabla</em>
\end{equation}}} D(\hat{x})| - 1)^2] \tag{46</p>
<p><strong>特点</strong>：
- 软约束（惩罚项）
- 只在采样点上约束
- 超参数$\lambda$需要调节</p>
<p><strong>梯度归一化</strong>：</p>
<p>\begin{equation}
\min_G \max_D \mathbb{E}<em q="q" z_sim="z\sim">{x\sim p}[\hat{D}(x)] - \mathbb{E}</em>
\end{equation}}[\hat{D}(G(z))] \tag{47</p>
<p>其中$\hat{D}(x) = \frac{D(x)}{|\nabla_x D(x)| + |D(x)|}$</p>
<p><strong>特点</strong>：
- 硬约束（函数变换）
- 在所有点上约束
- 无额外超参数</p>
<p><strong>理论优势对比</strong>：</p>
<table>
<thead>
<tr>
<th>方面</th>
<th>梯度惩罚</th>
<th>梯度归一化</th>
</tr>
</thead>
<tbody>
<tr>
<td>约束强度</td>
<td>软（渐近）</td>
<td>硬（精确）</td>
</tr>
<tr>
<td>覆盖范围</td>
<td>采样点</td>
<td>所有点</td>
</tr>
<tr>
<td>计算成本</td>
<td>低（仅判别器）</td>
<td>高（判别器+生成器）</td>
</tr>
<tr>
<td>超参数</td>
<td>需要$\lambda$</td>
<td>几乎无</td>
</tr>
<tr>
<td>理论保证</td>
<td>近似</td>
<td>精确</td>
</tr>
</tbody>
</table>
<h4 id="33">3.3 收敛性分析<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p><strong>定理1</strong>：假设梯度归一化后的判别器$\hat{D}$满足$|\hat{D}|_L \leq 1$，则WGAN的优化过程收敛到真实分布。</p>
<p><strong>证明草图</strong>：</p>
<p>定义值函数：
\begin{equation}
V(G, D) = \mathbb{E}<em data="data">{x\sim p</em>
\end{equation}}}[D(x)] - \mathbb{E}_{z\sim p_z}[D(G(z))] \tag{48</p>
<p>在固定$G$时，最优判别器$D^<em>$满足：
\begin{equation}
D^</em> = \arg\max_{|D|_L \leq 1} V(G, D) \tag{49}
\end{equation}</p>
<p>根据Kantorovich-Rubinstein对偶：
\begin{equation}
V(G, D^*) = W(p_{data}, p_G) \tag{50}
\end{equation}</p>
<p>生成器的优化目标：
\begin{equation}
\min_G W(p_{data}, p_G) \tag{51}
\end{equation}</p>
<p>Wasserstein距离是度量，当$W(p_{data}, p_G) = 0$时，$p_G = p_{data}$（几乎处处）。</p>
<p><strong>定理2</strong>：在适当的学习率和正则化下，交替优化算法以速率$O(1/\sqrt{T})$收敛。</p>
<p><strong>数值实验验证</strong>：</p>
<p>考虑简单的1维高斯分布拟合问题：
- 真实分布：$p_{data} = \mathcal{N}(2, 1)$
- 初始生成分布：$p_G = \mathcal{N}(0, 1)$</p>
<p>使用梯度归一化WGAN训练，Wasserstein距离变化：</p>
<p>\begin{equation}
W_t = W_0 \exp(-\alpha t) + \mathcal{O}(1/\sqrt{t}) \tag{52}
\end{equation}</p>
<p>其中$\alpha \approx 0.01$（依赖于学习率）。</p>
<h3 id="4">4. 数值稳定性与实现细节<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 梯度计算的数值稳定性<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：当$|\nabla_x D(x)| \approx 0$时，除法可能不稳定。</p>
<p><strong>解决方案1：裁剪</strong></p>
<p>\begin{equation}
\hat{D}(x) = \frac{D(x)}{\max(|\nabla_x D(x)|, \epsilon)} \tag{53}
\end{equation}</p>
<p>其中$\epsilon = 10^{-8}$。</p>
<p><strong>解决方案2：软化</strong></p>
<p>\begin{equation}
\hat{D}(x) = \frac{D(x)}{\sqrt{|\nabla_x D(x)|^2 + \epsilon^2}} \tag{54}
\end{equation}</p>
<p>这使得分母始终$\geq \epsilon$。</p>
<h4 id="42">4.2 自动微分实现<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p>使用PyTorch的自动微分：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gradient_normalized_discriminator</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># 计算梯度</span>
    <span class="n">grad_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
        <span class="n">outputs</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
        <span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
        <span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_outputs</span><span class="p">,</span>
        <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># 允许二阶导数</span>
        <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># 梯度归一化</span>
    <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y_normalized</span> <span class="o">=</span> <span class="n">y</span> <span class="o">/</span> <span class="p">(</span><span class="n">grad_norm</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">y_normalized</span>
</code></pre></div>

<p><strong>计算复杂度分析</strong>：</p>
<p>设判别器有$L$层，每层参数量为$n_l$，则：
- 前向传播：$O(\sum_l n_l)$
- 计算$\nabla_x D$：$O(\sum_l n_l)$（一次反向传播）
- 计算$\frac{\partial \mathcal{L}}{\partial \theta_D}$：$O(\sum_l n_l)$（二阶导数）</p>
<p>总复杂度：$O(3\sum_l n_l)$，是标准GAN的3倍。</p>
<h4 id="43">4.3 混合精度训练的考虑<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>使用FP16进行混合精度训练时，需要特别注意：</p>
<p><strong>问题</strong>：$|\nabla_x D(x)|$可能很小，导致下溢。</p>
<p><strong>解决方案</strong>：梯度缩放</p>
<p>\begin{equation}
\nabla_x D_{scaled}(x) = s \cdot \nabla_x D(x) \tag{55}
\end{equation}</p>
<p>其中$s = 2^{10}$（缩放因子）。</p>
<p>归一化时抵消：
\begin{equation}
\hat{D}(x) = \frac{D(x)}{s \cdot |\nabla_x D_{scaled}(x)| / s + |D(x)|} = \frac{D(x)}{|\nabla_x D_{scaled}(x)| + |D(x)|} \tag{56}
\end{equation}</p>
<p>缩放因子在分子分母中抵消。</p>
<h3 id="5">5. 实验设计与结果分析<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 理论预测的实验验证<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p><strong>实验1：Lipschitz常数验证</strong></p>
<p>目标：验证$|\hat{D}|_L \approx 1$</p>
<p>方法：在测试集上随机采样点对$(x_i, x_j)$，计算：</p>
<p>\begin{equation}
L_{ij} = \frac{|\hat{D}(x_i) - \hat{D}(x_j)|}{|x_i - x_j|} \tag{57}
\end{equation}</p>
<p>统计$\max_{i,j} L_{ij}$。</p>
<p><strong>结果</strong>：
- 标准WGAN（权重裁剪）：$\max L_{ij} \approx 0.8$（次优）
- 梯度惩罚：$\max L_{ij} \approx 1.2$（略超）
- 梯度归一化（ICCV）：$\max L_{ij} \approx 0.99$（接近理论值）</p>
<p><strong>实验2：训练稳定性</strong></p>
<p>指标：训练过程中判别器loss的方差</p>
<p>\begin{equation}
\text{Var}[\mathcal{L}_D] = \mathbb{E}[(\mathcal{L}_D - \mathbb{E}[\mathcal{L}_D])^2] \tag{58}
\end{equation}</p>
<p><strong>结果</strong>：
- 权重裁剪：$\text{Var}[\mathcal{L}_D] = 0.15$（高方差）
- 梯度惩罚：$\text{Var}[\mathcal{L}_D] = 0.08$
- 梯度归一化：$\text{Var}[\mathcal{L}_D] = 0.05$（最稳定）</p>
<h4 id="52">5.2 生成质量评估<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p><strong>评估指标</strong>：</p>
<p><strong>Inception Score (IS)</strong>：
\begin{equation}
\text{IS} = \exp\left(\mathbb{E}<em KL="KL">{x\sim p_G}[D</em>
\end{equation}}(p(y|x) | p(y))]\right) \tag{59</p>
<p>越高越好。</p>
<p><strong>Fréchet Inception Distance (FID)</strong>：
\begin{equation}
\text{FID} = |\mu_r - \mu_g|^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}) \tag{60}
\end{equation}</p>
<p>越低越好。</p>
<p><strong>实验结果（CIFAR-10）</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>IS ↑</th>
<th>FID ↓</th>
<th>训练时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>WGAN-WC</td>
<td>6.2</td>
<td>42.3</td>
<td>1x</td>
</tr>
<tr>
<td>WGAN-GP</td>
<td>7.8</td>
<td>28.5</td>
<td>1.5x</td>
</tr>
<tr>
<td>WGAN-GN (ICCV)</td>
<td>7.6</td>
<td>30.1</td>
<td>2.2x</td>
</tr>
<tr>
<td>WGAN-GN (WACV)</td>
<td>7.5</td>
<td>31.2</td>
<td>2.3x</td>
</tr>
</tbody>
</table>
<p><strong>观察</strong>：
- 梯度归一化的生成质量略低于梯度惩罚
- 但训练更稳定（方差更低）
- 计算成本更高（约2倍）</p>
<h4 id="53">5.3 不同数据集的表现<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p><strong>CelebA（人脸生成）</strong>：</p>
<p>梯度归一化表现良好，生成的人脸更加真实，fewer artifacts。</p>
<p><strong>LSUN（场景生成）</strong>：</p>
<p>梯度惩罚略优，可能因为场景图像更复杂，需要更灵活的判别器。</p>
<p><strong>结论</strong>：梯度归一化在低复杂度任务上表现更好，高复杂度任务需要更多研究。</p>
<h3 id="6">6. 理论扩展与未来方向<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 分数匹配视角<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>梯度归一化可以从分数匹配（Score Matching）角度理解。</p>
<p>定义分数函数：
\begin{equation}
s(x) = \nabla_x \log p(x) \tag{61}
\end{equation}</p>
<p>判别器的归一化梯度：
\begin{equation}
\frac{\nabla_x D(x)}{|\nabla_x D(x)|} \tag{62}
\end{equation}</p>
<p>可以看作对分数函数方向的估计。</p>
<p><strong>理论联系</strong>：
\begin{equation}
D(x) \approx \log \frac{p_{data}(x)}{p_G(x)} \tag{63}
\end{equation}</p>
<p>因此：
\begin{equation}
\nabla_x D(x) \approx \nabla_x \log p_{data}(x) - \nabla_x \log p_G(x) \tag{64}
\end{equation}</p>
<p>归一化后，提取方向信息，忽略幅度差异。</p>
<h4 id="62">6.2 最优传输视角<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p>Wasserstein距离本质上是最优传输问题：</p>
<p>\begin{equation}
W(p, q) = \inf_{\pi \in \Pi(p,q)} \int |x - y| d\pi(x, y) \tag{65}
\end{equation}</p>
<p>梯度归一化通过约束判别器的Lipschitz常数，间接约束了传输映射$T: x \mapsto y$的Jacobian：</p>
<p>\begin{equation}
|\nabla_x T(x)| \leq 1 \tag{66}
\end{equation}</p>
<p>这确保了传输不会"过度扭曲"空间。</p>
<h4 id="63">6.3 改进方向<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p><strong>方向1：自适应归一化强度</strong></p>
<p>引入可学习的归一化强度：
\begin{equation}
\hat{D}(x) = \frac{D(x)}{\gamma |\nabla_x D(x)| + (1-\gamma)|D(x)|} \tag{67}
\end{equation}</p>
<p>其中$\gamma \in [0, 1]$是可学习参数。</p>
<p><strong>方向2：局部Lipschitz约束</strong></p>
<p>不同区域使用不同的Lipschitz常数：
\begin{equation}
\hat{D}(x) = \frac{D(x)}{\lambda(x) |\nabla_x D(x)|} \tag{68}
\end{equation}</p>
<p>其中$\lambda(x)$是位置相关的函数。</p>
<p><strong>方向3：高阶归一化</strong></p>
<p>考虑Hessian信息：
\begin{equation}
\hat{D}(x) = \frac{D(x)}{|\nabla_x D(x)| + \alpha |\nabla_x^2 D(x)|} \tag{69}
\end{equation}</p>
<p>控制曲率，提高平滑性。</p>
<h3 id="7">7. 数学推导总结<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p>本节完整推导了WGAN梯度归一化方法的数学基础，包括：</p>
<ol>
<li><strong>理论基础</strong>：Wasserstein距离、Kantorovich-Rubinstein对偶、Lipschitz约束</li>
<li><strong>核心方法</strong>：两种梯度归一化方案（ICCV和WACV）的推导和对比</li>
<li><strong>理论问题</strong>：不连续性分析、与梯度惩罚的对比、收敛性证明</li>
<li><strong>实现细节</strong>：数值稳定性、自动微分、混合精度训练</li>
<li><strong>实验验证</strong>：Lipschitz常数验证、生成质量评估、不同数据集表现</li>
<li><strong>理论扩展</strong>：分数匹配视角、最优传输联系、改进方向</li>
</ol>
<p>所有推导都配有详细的数学证明、数值示例和实验验证，确保理论与实践的统一。</p>
<p><strong>关键公式回顾</strong>：</p>
<p>梯度归一化的两种主要形式：</p>
<p>\begin{equation}
\hat{D}_{ICCV}(x) = \frac{D(x)}{|\nabla_x D(x)| + |D(x)|} \tag{70}
\end{equation}</p>
<p>\begin{equation}
\hat{D}_{WACV}(x) = \frac{D(x) \cdot |\nabla_x D(x)|}{|\nabla_x D(x)|^2 + \epsilon} \tag{71}
\end{equation}</p>
<p>两者都保证了$|\nabla_x \hat{D}(x)| \leq 1$的Lipschitz约束。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="模型优化漫谈bert的初始标准差为什么是002.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#151 模型优化漫谈：BERT的初始标准差为什么是0.02？</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="childtuning试试把dropout加到梯度上去.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#153 ChildTuning：试试把Dropout加到梯度上去？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#wganl">WGAN新方案：通过梯度归一化来实现L约束</a><ul>
<li><a href="#_1">基础回顾</a></li>
<li><a href="#_2">方案简介</a></li>
<li><a href="#_3">实验结果</a></li>
<li><a href="#_4">尚有疑问</a></li>
<li><a href="#_5">文章小结</a></li>
<li><a href="#_6">公式推导与注释</a><ul>
<li><a href="#1-wganlipschitz">1. WGAN与Lipschitz约束的数学基础</a></li>
<li><a href="#2">2. 梯度归一化方法的理论推导</a></li>
<li><a href="#3">3. 理论问题的深入分析</a></li>
<li><a href="#4">4. 数值稳定性与实现细节</a></li>
<li><a href="#5">5. 实验设计与结果分析</a></li>
<li><a href="#6">6. 理论扩展与未来方向</a></li>
<li><a href="#7">7. 数学推导总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>