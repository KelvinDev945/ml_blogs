<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SVD的导数 | ML & Math Blog Posts</title>
    <meta name="description" content="SVD的导数&para;
原文链接: https://spaces.ac.cn/archives/10878
发布日期: 

SVD（Singular Value Decomposition，奇异值分解）是常见的矩阵分解算法，相信很多读者都已经对它有所了解，此前我们在《低秩近似之路（二）：SVD》也专门介绍过它。然而，读者是否想到，SVD竟然还可以求导呢？笔者刚了解到这一结论时也颇感意外，因为直觉...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=微积分">微积分</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #78 SVD的导数
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#78</span>
                SVD的导数
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/10878" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=微积分" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 微积分</span>
                </a>
                
                <a href="../index.html?tags=分析" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 分析</span>
                </a>
                
                <a href="../index.html?tags=矩阵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 矩阵</span>
                </a>
                
                <a href="../index.html?tags=SVD" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> SVD</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="svd">SVD的导数<a class="toc-link" href="#svd" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10878">https://spaces.ac.cn/archives/10878</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>SVD（Singular Value Decomposition，奇异值分解）是常见的矩阵分解算法，相信很多读者都已经对它有所了解，此前我们在<a href="/archives/10407">《低秩近似之路（二）：SVD》</a>也专门介绍过它。然而，读者是否想到，SVD竟然还可以求导呢？笔者刚了解到这一结论时也颇感意外，因为直觉上“分解”往往都是不可导的。但事实是，SVD在一般情况下确实可导，这意味着理论上我们可以将SVD嵌入到模型中，并用基于梯度的优化器来端到端训练。</p>
<p>问题来了，既然SVD可导，那么它的导函数长什么样呢？接下来，我们将参考文献<a href="https://j-towns.github.io/papers/svd-derivative.pdf">《Differentiating the Singular Value Decomposition》</a>，逐步推导SVD的求导公式。</p>
<h2 id="_1">推导基础<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>假设$\boldsymbol{W}$是满秩的$n\times n$矩阵，且全体奇异值两两不等，这是比较容易讨论的情形，后面我们也会讨论哪些条件可以放宽一点。接着，我们设$\boldsymbol{W}$的SVD为：<br />
\begin{equation}\boldsymbol{W} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}\end{equation}<br />
所谓SVD求导，实际上就是设法分别求出$\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}$关于$\boldsymbol{W}$的梯度或微分。为此，我们先对上式两边求微分<br />
\begin{equation}d\boldsymbol{W} = (d\boldsymbol{U})\boldsymbol{\Sigma}\boldsymbol{V}^{\top} + \boldsymbol{U}(d\boldsymbol{\Sigma})\boldsymbol{V}^{\top} + \boldsymbol{U}\boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\end{equation}<br />
左乘$\boldsymbol{U}^{\top}$、右乘$\boldsymbol{V}$，并利用$\boldsymbol{U}^{\top}\boldsymbol{U} = \boldsymbol{I}, \boldsymbol{V}^{\top}\boldsymbol{V} = \boldsymbol{I}$得到<br />
\begin{equation}\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V} = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}\label{eq:core}\end{equation}<br />
这便是后面推导的基础。注意对恒等式$\boldsymbol{U}^{\top}\boldsymbol{U} = \boldsymbol{I}, \boldsymbol{V}^{\top}\boldsymbol{V} = \boldsymbol{I}$两端求微分，我们还可以得到<br />
\begin{equation}(d\boldsymbol{U})^{\top}\boldsymbol{U} + \boldsymbol{U}^{\top}(d\boldsymbol{U}) = \boldsymbol{0},\quad (d\boldsymbol{V})^{\top}\boldsymbol{V} + \boldsymbol{V}^{\top}(d\boldsymbol{V}) = \boldsymbol{0}\end{equation}<br />
这表明$\boldsymbol{U}^{\top}(d\boldsymbol{U})$和$(d\boldsymbol{V})^{\top}\boldsymbol{V}$都是反对称矩阵。</p>
<h2 id="_2">奇异值<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>反对称矩阵的特点是对角线元素都是零，而$\boldsymbol{\Sigma}$是对角阵，非对角线元素都是零，这启发我们可能需要分别处理处理对角线和非对角线元素。</p>
<p>首先，我们定义矩阵$\boldsymbol{I}$和$\bar{\boldsymbol{I}}$：$\boldsymbol{I}$就是单位阵，即对角线元素全是1，非对角线元素全是0；$\bar{\boldsymbol{I}}$则是单位阵的互补阵，即对角线元素全是0，而非对角线元素全是1。利用$\boldsymbol{I}$、$\bar{\boldsymbol{I}}$和Hadamard积$\otimes$，我们可以分别提取出式$\eqref{eq:core}$的对角线和非对角线部分：<br />
\begin{align}<br />
\boldsymbol{I}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}) =&amp;\, \boldsymbol{I}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}) = d\boldsymbol{\Sigma} \label{eq:value} \\[8pt]<br />
\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}) =&amp;\, \bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}) = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}\label{eq:vector}<br />
\end{align}<br />
现在我们先来看式$\eqref{eq:value}$，它可以等价地写成<br />
\begin{equation}d\sigma_i = \boldsymbol{u}_i^{\top}(d\boldsymbol{W})\boldsymbol{v}_i\label{eq:d-sigma}\end{equation}<br />
这就是第$i$个奇异值$\sigma_i$的微分，其中$\boldsymbol{u}_i,\boldsymbol{v}_i$分别是$\boldsymbol{U},\boldsymbol{V}$的第$i$列。<a href="/archives/10648">《从谱范数梯度到新式权重衰减的思考》</a>中讨论的谱范数梯度，实际上就只是这里$i=1$时的特例。</p>
<h2 id="_3">奇异向量<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>然后我们再来看式$\eqref{eq:vector}$：<br />
\begin{equation}\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}) = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}\label{eq:vector-1}\end{equation}<br />
转置一下<br />
\begin{equation}\begin{aligned}<br />
\bar{\boldsymbol{I}}\otimes(\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}) =&amp;\, \boldsymbol{\Sigma}(d\boldsymbol{U})^{\top}\boldsymbol{U} + \boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Sigma} \\[6pt]<br />
=&amp;\, -\boldsymbol{\Sigma}\boldsymbol{U}^{\top}(d\boldsymbol{U}) - (d\boldsymbol{V})^{\top}\boldsymbol{V}\boldsymbol{\Sigma}<br />
\end{aligned}\label{eq:vector-2}\end{equation}<br />
第二个等号利用了“$\boldsymbol{U}^{\top}(d\boldsymbol{U})$和$(d\boldsymbol{V})^{\top}\boldsymbol{V}$都是反对称矩阵”这一事实。式$\eqref{eq:vector-1}$和式$\eqref{eq:vector-2}$就是关于$d\boldsymbol{U},d\boldsymbol{V}$的线性方程组，我们要从中解出$d\boldsymbol{U},d\boldsymbol{V}$。</p>
<p>求解思路就是普通的消元法。首先，由$\eqref{eq:vector-1}\times\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\times\eqref{eq:vector-2}$得到<br />
\begin{equation}\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}) = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma}^2 - \boldsymbol{\Sigma}^2\boldsymbol{U}^{\top}(d\boldsymbol{U})\end{equation}<br />
这里利用了对角阵$\boldsymbol{\Sigma}$满足$\boldsymbol{\Sigma}(\bar{\boldsymbol{I}}\otimes \boldsymbol{M}) = \bar{\boldsymbol{I}}\otimes (\boldsymbol{\Sigma}\boldsymbol{M})$以及$(\bar{\boldsymbol{I}}\otimes \boldsymbol{M})\boldsymbol{\Sigma} = \bar{\boldsymbol{I}}\otimes (\boldsymbol{M}\boldsymbol{\Sigma})$的事实。我们知道，左（右）乘一个对角阵，等于矩阵的每一行（列）都乘以对角阵上相应的元素，所以如果我们定义矩阵$\boldsymbol{E}$，其中$\boldsymbol{E}<em i_j="i,j">{i,j} = \sigma_j^2 - \sigma_i^2$，那么$\boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma}^2 - \boldsymbol{\Sigma}^2\boldsymbol{U}^{\top}(d\boldsymbol{U}) = \boldsymbol{E}\otimes (\boldsymbol{U}^{\top}(d\boldsymbol{U}))$，于是上式可以写成<br />
\begin{equation}\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}) = \boldsymbol{E}\otimes (\boldsymbol{U}^{\top}(d\boldsymbol{U}))\label{eq:dU-0}\end{equation}<br />
继而可以解得<br />
\begin{equation}d\boldsymbol{U} = \boldsymbol{U}(\boldsymbol{F}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}))\label{eq:dU}\end{equation}<br />
类似地，由$\boldsymbol{\Sigma}\times \eqref{eq:vector-1} + \eqref{eq:vector-2}\times \boldsymbol{\Sigma}$解得：<br />
\begin{equation}d\boldsymbol{V} = \boldsymbol{V}(\boldsymbol{F}\otimes(\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}))\label{eq:dV}\end{equation}<br />
式$\eqref{eq:dU},\eqref{eq:dV}$便是特征向量的微分。其中<br />
\begin{equation}\boldsymbol{F}</em>} = \left\{\begin{aligned<br />
&amp;\, 1/(\sigma_j^2 - \sigma_i^2), &amp;\, i\neq j \\<br />
&amp;\, 0, &amp;\, i = j<br />
\end{aligned}\right.\end{equation}</p>
<h2 id="_4">梯度（一）<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>微分有了，怎么把梯度求出来呢？这还真有点麻烦。倒不是技术上的麻烦，而是表示上的麻烦，比如$\boldsymbol{W},\boldsymbol{U}$是一个$n\times n$矩阵，那么$\boldsymbol{U}$关于$\boldsymbol{W}$的完整梯度就是一个$n\times n\times n\times n$的4阶张量，而高阶张量是大多数人包括笔者都不熟悉的内容。</p>
<p>为了绕开高阶张量的麻烦，我们有两个方案。首先，从编程角度看，我们根本没必要求出梯度的形式，而是根据微分的结果写出等价的前向形式，然后交给框架的自动求导就行。比如，从式$\eqref{eq:d-sigma}$我们可以断定$\sigma_i$的梯度等于$\newcommand{\sg}[1]{\color{skyblue}{#1}} \sg{\boldsymbol{u}<em _boldsymbol_W="\boldsymbol{W">i}^{\top} \boldsymbol{W} \sg{\boldsymbol{v}_i}$的梯度，即<br />
\begin{equation}\nabla</em>}} \sigma_i = \nabla_{\boldsymbol{W}} (\sg{\boldsymbol{u<em _boldsymbol_W="\boldsymbol{W">i}^{\top} \boldsymbol{W} \sg{\boldsymbol{v}_i})\end{equation}<br />
这里将符号颜色改为$\sg{\blacksquare}$色代表stop_gradient算子，避免公式过于臃肿。刚好$\sigma_i$又等于$\boldsymbol{u}_i^{\top}\boldsymbol{W}\boldsymbol{v}_i$，所以我们只需要把代码中出现$\sigma_i$的地方，都替换成$\sg{\boldsymbol{u}_i}^{\top} \boldsymbol{W} \sg{\boldsymbol{v}_i}$，那么就可以自动获得正确的梯度。一般地，我们有<br />
\begin{equation}\nabla</em>}} \boldsymbol{\Sigma} = \nabla_{\boldsymbol{W}} (\boldsymbol{I}\otimes(\sg{\boldsymbol{U}}^{\top} \boldsymbol{W} \sg{\boldsymbol{V}}))\end{equation<br />
即所有$\boldsymbol{\Sigma}$换成$\boldsymbol{I}\otimes(\sg{\boldsymbol{U}}^{\top} \boldsymbol{W} \sg{\boldsymbol{V}})$即可。</p>
<p>同理，从式$\eqref{eq:dU}$我们知道<br />
\begin{equation}\nabla_{\boldsymbol{W}}\boldsymbol{U} = \nabla_{\boldsymbol{W}}(\sg{\boldsymbol{U}}(\sg{\boldsymbol{F}}\otimes(\sg{\boldsymbol{U}}^{\top}\boldsymbol{W}\sg{\boldsymbol{V}\boldsymbol{\Sigma}} + \sg{\boldsymbol{\Sigma}\boldsymbol{V}}^{\top}\boldsymbol{W}^{\top}\sg{\boldsymbol{U}})))\end{equation}<br />
可以验证$\boldsymbol{U}(\boldsymbol{F}\otimes(\boldsymbol{U}^{\top}\boldsymbol{W}\boldsymbol{V}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}\boldsymbol{W}^{\top}\boldsymbol{U}))$刚好是零矩阵，所以我们只需要将代码中所有出现$\boldsymbol{U}$的地方，都替换成<br />
\begin{equation}\boldsymbol{U} \quad \to \quad \sg{\boldsymbol{U}} + \sg{\boldsymbol{U}}(\sg{\boldsymbol{F}}\otimes(\sg{\boldsymbol{U}}^{\top}\boldsymbol{W}\sg{\boldsymbol{V}\boldsymbol{\Sigma}} + \sg{\boldsymbol{\Sigma}\boldsymbol{V}}^{\top}\boldsymbol{W}^{\top}\sg{\boldsymbol{U}}))\end{equation}<br />
那就能保持正确的前向结果，同时获得正确的梯度。基于同样的原理，$\boldsymbol{V}$的替换格式是<br />
\begin{equation}\boldsymbol{V} \quad \to \quad \sg{\boldsymbol{V}} + \sg{\boldsymbol{V}}(\sg{\boldsymbol{F}}\otimes(\sg{\boldsymbol{V}}^{\top}\boldsymbol{W}^{\top}\sg{\boldsymbol{U}\boldsymbol{\Sigma}} + \sg{\boldsymbol{\Sigma}\boldsymbol{U}}^{\top}\boldsymbol{W}\sg{\boldsymbol{V}}))\end{equation}</p>
<h2 id="_5">梯度（二）<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>第二个方案是直接求出损失函数关于$\boldsymbol{W}$的梯度。具体来说，假设损失函数是$\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}$的函数，记为$\mathcal{L}(\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V})$，我们直接求$\nabla_{\boldsymbol{W}}\mathcal{L}$，它是一个矩阵，可以用$\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V},\nabla_{\boldsymbol{U}}\mathcal{L},\nabla_{\boldsymbol{\Sigma}}\mathcal{L},\nabla_{\boldsymbol{V}}\mathcal{L}$表示出来，这些量也都只是矩阵，所以不用涉及到高阶张量。</p>
<p>在上一节中，我们已经求出了具有相同梯度的$\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}$的等效函数，除开被stop_gradient的部分外，这些等效函数关于$\boldsymbol{W}$都是线性的，所以此时问题本质上已经变成了线性复合函数的梯度。我们之前在<a href="/archives/10366#%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC">《低秩近似之路（一）：伪逆》</a>的“矩阵求导”一节便已经讨论过相关方法。具体来说，我们有<br />
\begin{align}<br />
\boldsymbol{X} = \boldsymbol{A}\boldsymbol{B}\boldsymbol{C} &amp;\,\quad\Rightarrow\quad \nabla_{\boldsymbol{B}}f(\boldsymbol{X}) = \boldsymbol{A}^{\top}(\nabla_{\boldsymbol{X}}f(\boldsymbol{X}))\boldsymbol{C}^{\top} \\[8pt]<br />
\boldsymbol{X} = \boldsymbol{A}\boldsymbol{B}^{\top}\boldsymbol{C} &amp;\,\quad\Rightarrow\quad \nabla_{\boldsymbol{B}}f(\boldsymbol{X}) = \boldsymbol{C}(\nabla_{\boldsymbol{X}}f(\boldsymbol{X}))^{\top}\boldsymbol{A} \\[8pt]<br />
\boldsymbol{X} = \boldsymbol{A}\otimes\boldsymbol{B} &amp;\,\quad\Rightarrow\quad \nabla_{\boldsymbol{B}}f(\boldsymbol{X}) = \boldsymbol{A}\otimes \nabla_{\boldsymbol{X}}f(\boldsymbol{X})<br />
\end{align}<br />
利用这些基本公式，以及复合函数求导的链式法则，我们可以写出<br />
\begin{equation}\begin{aligned}<br />
\nabla_{\boldsymbol{W}}\mathcal{L} \quad = \qquad &amp;\,\boldsymbol{U}(\boldsymbol{F}\otimes(\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{U}}\mathcal{L}) - (\nabla_{\boldsymbol{U}}\mathcal{L})^{\top}\boldsymbol{U}))\boldsymbol{\Sigma}\boldsymbol{V}^{\top} \\[6pt]<br />
+ &amp;\,\boldsymbol{U}(\boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L}))\boldsymbol{V}^{\top} \\[6pt]<br />
+ &amp;\,\boldsymbol{U}\boldsymbol{\Sigma}(\boldsymbol{F}\otimes(\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L}) - (\nabla_{\boldsymbol{V}}\mathcal{L})^{\top}\boldsymbol{V})))\boldsymbol{V}^{\top}<br />
\end{aligned}\end{equation}<br />
整个过程就是反复地利用基本公式和链式法则，以及$\boldsymbol{F}^{\top} = -\boldsymbol{F}$，原则上没有难度，就是需要谨慎地集中注意力，建议读者亲自动手完成这个推导过程，这是一道相当实用的矩阵求导练习题。最后引入两个记号<br />
\begin{equation}\newcommand{\sym}[1]{\color{red}{[}#1\color{red}{]<em skew="skew">{sym}}} \newcommand{\skew}[1]{\color{red}{[}#1\color{red}{]</em>}}} \sym{\boldsymbol{X}} = \frac{1}{2}(\boldsymbol{X} + \boldsymbol{X}^{\top}),\qquad \skew{\boldsymbol{X}} = \frac{1}{2}(\boldsymbol{X} - \boldsymbol{X}^{\top})\end{equation<br />
我们可以将梯度结果简写成<br />
\begin{equation}\nabla_{\boldsymbol{W}}\mathcal{L} = \boldsymbol{U}\Big(2(\boldsymbol{F}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{U}}\mathcal{L})})\boldsymbol{\Sigma} + \boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L}) + 2\boldsymbol{\Sigma}(\boldsymbol{F}\otimes\skew{\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L})})\Big)\boldsymbol{V}^{\top}<br />
\label{eq:w-grad-l}\end{equation}</p>
<h2 id="_6">梯度（三）<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>现在可以牛刀小试一下，求$\boldsymbol{O}=\mathop{\text{msign}}(\boldsymbol{W})=\boldsymbol{U}\boldsymbol{V}^{\top}$的梯度，其中$\mathop{\text{msign}}$的概念我们在<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>介绍Muon时已经讨论过。</p>
<p>根据$\mathop{\text{msign}}$的定义，我们有<br />
\begin{equation}\nabla_{\boldsymbol{U}}\mathcal{L} = (\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V},\qquad \nabla_{\boldsymbol{V}}\mathcal{L} = (\nabla_{\boldsymbol{O}}\mathcal{L})^{\top} \boldsymbol{U}\end{equation}<br />
代入式$\eqref{eq:w-grad-l}$得到<br />
\begin{equation}\begin{aligned}<br />
\nabla_{\boldsymbol{W}}\mathcal{L} =&amp;\, 2\boldsymbol{U}\Big((\boldsymbol{F}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}})\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(\boldsymbol{F}\otimes\skew{\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})^{\top}\boldsymbol{U}})\Big)\boldsymbol{V}^{\top} \\[6pt]<br />
=&amp;\, 2\boldsymbol{U}\Big((\boldsymbol{F}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}})\boldsymbol{\Sigma} - \boldsymbol{\Sigma}(\boldsymbol{F}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}})\Big)\boldsymbol{V}^{\top} \\[6pt]<br />
=&amp;\, 2\boldsymbol{U}\big(\boldsymbol{G}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}}\big)\boldsymbol{V}^{\top}<br />
\end{aligned}\end{equation}<br />
其中$\boldsymbol{G}_{i,j} = 1/(\sigma_i + \sigma_j)$。</p>
<h2 id="_7">梯度（四）<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>最后让我们来考虑一个常用的特殊例子，当$\boldsymbol{W}$还是正定对称矩阵时，它的SVD具有$\boldsymbol{V}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$的形式，即$\boldsymbol{U}=\boldsymbol{V}$。我们重复前面的推导，先对两边求微分<br />
\begin{equation}d\boldsymbol{W} = (d\boldsymbol{V})\boldsymbol{\Sigma}\boldsymbol{V}^{\top} + \boldsymbol{V}(d\boldsymbol{\Sigma})\boldsymbol{V}^{\top} + \boldsymbol{V}\boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\end{equation}<br />
然后左乘$\boldsymbol{V}^{\top}$、右乘$\boldsymbol{V}$：<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V} =&amp;\, \boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V} \\[6pt]<br />
=&amp;\, \boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} - \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{V})<br />
\end{aligned}\end{equation}<br />
由此可见<br />
\begin{gather}<br />
d\boldsymbol{\Sigma} = \boldsymbol{I}\otimes(\boldsymbol{V}(d\boldsymbol{W})\boldsymbol{V}^{\top}) \\[8pt]<br />
\boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Sigma} - \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{V}) = \bar{\boldsymbol{I}}\otimes(\boldsymbol{V}(d\boldsymbol{W})\boldsymbol{V}^{\top})<br />
\end{gather}<br />
由第二式可以进一步解得<br />
\begin{equation}d\boldsymbol{V} = \boldsymbol{V}(\boldsymbol{K}^{\top}\otimes(\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V}))\end{equation}<br />
其中<br />
\begin{equation}\boldsymbol{K}<em _boldsymbol_W="\boldsymbol{W">{i,j} = \left\{\begin{aligned}<br />
&amp;\, 1/(\sigma_i - \sigma_j), &amp;\, i\neq j \\<br />
&amp;\, 0, &amp;\, i = j<br />
\end{aligned}\right.\end{equation}<br />
根据这个结果，我们有<br />
\begin{equation}\nabla</em>}}\mathcal{L} = \boldsymbol{V}(\boldsymbol{K}^{\top}\otimes(\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L})) + \boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L}))\boldsymbol{V}^{\top} \end{equation<br />
注意陷阱！上式其实是错误的，它是链式法则的结果，但链式法则只适用于无约束求导，而这里带有约束$\boldsymbol{W}=\boldsymbol{W}^{\top}$，正确的梯度应该还要包含对称化：<br />
\begin{equation}\begin{aligned}<br />
\nabla_{\boldsymbol{W}}\mathcal{L} =&amp;\, \boldsymbol{V}\Big(\sym{\boldsymbol{K}^{\top}\otimes(\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L}))} + \boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L})\Big)\boldsymbol{V}^{\top} \\[6pt]<br />
=&amp;\, \boldsymbol{V}\Big(\boldsymbol{K}^{\top}\otimes\skew{\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L})} + \boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L})\Big)\boldsymbol{V}^{\top}\end{aligned}\end{equation}<br />
另一个陷阱是直接将$\boldsymbol{U}=\boldsymbol{V}$代入式$\eqref{eq:w-grad-l}$来推导，这将会导致$\boldsymbol{K}$项翻倍，原因是式$\eqref{eq:w-grad-l}$区分了$\nabla_{\boldsymbol{U}}\mathcal{L}$和$\nabla_{\boldsymbol{V}}\mathcal{L}$，而在正定对称假设下，$\boldsymbol{U},\boldsymbol{V}$是相同的，对$\boldsymbol{V}$求梯度实际上无形中求了原本的$\nabla_{\boldsymbol{U}}\mathcal{L},\nabla_{\boldsymbol{V}}\mathcal{L}$之和，所以会导致重复计算。相关文献还可以参考<a href="https://openaccess.thecvf.com/content_iccv_2015/html/Ionescu_Matrix_Backpropagation_for_ICCV_2015_paper.html">《Matrix Backpropagation for Deep Networks With Structured Layers》</a>。</p>
<h2 id="_8">数值问题<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>可能有些读者疑问：我只能保证初始化矩阵的奇异值两两不等，怎么保证经过训练后的矩阵仍然满足这个条件呢？答案是梯度自然会帮我们保证。从式$\eqref{eq:dU}$和式$\eqref{eq:dV}$可以看出，它的梯度包含了$\boldsymbol{F}$，而由$\boldsymbol{F}_{i,j} = \frac{1}{\sigma_j^2 - \sigma_i^2}$可知，一旦两个奇异值接近，那么梯度将会非常大，所以优化器会自动把它们推开。</p>
<p>不过，这个特性也给实际训练带来了数值不稳定性，主要体现在$\sigma_i,\sigma_j$接近时的梯度爆炸。对此，论文<a href="https://papers.cool/arxiv/1906.09023">《Backpropagation-Friendly Eigendecomposition》</a>提出用“幂迭代（Power Iteration）”替代精确的SVD。后来，论文<a href="https://papers.cool/arxiv/2104.03821">《Robust Differentiable SVD》</a>证明了它理论上等于对$\boldsymbol{F}_{i,j} = -\frac{1}{（\sigma_i + \sigma_j)(\sigma_i - \sigma_j）}$中的$\frac{1}{\sigma_i - \sigma_j}$做泰勒近似（假设$\sigma_j &lt; \sigma_i$）：<br />
\begin{equation}\frac{1}{\sigma_i - \sigma_j} = \frac{1}{\sigma_i}\frac{1}{1-(\sigma_j/\sigma_i)}\approx \frac{1}{\sigma_i}\left(1 + \left(\frac{\sigma_j}{\sigma_i}\right) + \left(\frac{\sigma_j}{\sigma_i}\right)^2 + \cdots + \left(\frac{\sigma_j}{\sigma_i}\right)^N \right)\end{equation}<br />
使用泰勒近似后，至少对于$\sigma_j \to \sigma_i$的情况不会出现梯度爆炸了。再后来，作者在<a href="https://papers.cool/arxiv/2105.02498">《Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?》</a>将其推广到更一般的Padé近似。这一系列工作笔者并不是太熟悉，因此就不过多展开了。</p>
<p>只是笔者有一个疑问，如果只是单纯为了避免数值爆炸，似乎没必要上这些工具，直接给$\sigma_j/\sigma_i$加个截断不就好了？比如<br />
\begin{equation}\frac{1}{1-(\sigma_j/\sigma_i)}\approx \frac{1}{1-\min(\sigma_j/\sigma_i, 0.99)}\end{equation}<br />
这样不就简单避免了它趋于无穷？还是笔者有什么认识不到位的地方？如果读者了解相关背景，敬请在评论区指正，谢谢。</p>
<h2 id="_9">一般结果<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>到目前为止，上面出现的所有矩阵都是$n\times n$矩阵，因为这是在文章开头的假设“$\boldsymbol{W}$是满秩的$n\times n$矩阵，且全体奇异值两两不等”下进行的推导。这一节我们来讨论该条件可以放宽到什么程度。</p>
<p>简单来说，SVD可导的条件是“全体非零奇异值两两不等”，换言之，方阵可以去掉，满秩也可以去掉，但是非零奇异值仍须互不相同。因为一旦有相等的奇异值，那么SVD就不唯一，这从根本上破坏了可导性。当然，如果只要部分导数，那我们还可以放宽条件，比如只想要谱范数即$\sigma_1$的导数，那么只需要$\sigma_1 &gt; \sigma_2$。</p>
<p>那么，放宽条件后，微分结果怎样变化呢？我们不妨一般地设$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，秩为$r$，SVD为<br />
\begin{equation}\boldsymbol{W} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top},\quad\boldsymbol{U}\in\mathbb{R}^{n\times n},\boldsymbol{\Sigma}\in\mathbb{R}^{n\times m},\boldsymbol{V}\in\mathbb{R}^{m\times m}\end{equation}<br />
之所以允许零奇异值，是因为此时我们只关心$d\boldsymbol{U}<em :r_=":r]" _:_="[:,">{[:, :r]}$和$d\boldsymbol{V}</em>}$，在非零奇异值两两不等的假设下，它们是可以唯一确定的。我们从式$\eqref{eq:dU-0}$出发，直到式$\eqref{eq:dU-0}$的推导都是通用的，从式$\eqref{eq:dU-0}$也可以看出为什么要拒绝相同奇异值，因为$\sigma_i = \sigma_j$时$\sigma_j - \sigma_i$就无法求逆了。在式$\eqref{eq:dU-0}$中只保留跟$d\boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, :r]}$相关的部分，我们得到<br />
\begin{equation}\bar{\boldsymbol{I}}</em>}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:, :r]} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}</em>}) = \boldsymbol{E<em :r_=":r]" _:_="[:,">{[:, :r]}\otimes (\boldsymbol{U}^{\top}(d\boldsymbol{U}</em>}))\end{equation<br />
这里$\boldsymbol{E}$的定义依然是$\boldsymbol{E}<em :r_=":r]" _:_="[:,">{i,j} = \sigma_j^2 - \sigma_i^2$，但$\boldsymbol{E}</em>$正好排除了所有0，所以可以顺利求逆，结果是<br />
\begin{equation}d\boldsymbol{U}<em :r_=":r]" _:_="[:,">{[:, :r]} = \boldsymbol{U}(\boldsymbol{F}</em>}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:, :r]} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}</em>}))\end{equation<br />
进一步地，做如下分块<br />
\begin{equation}\boldsymbol{U} = \begin{pmatrix}\boldsymbol{U}<em _:_r:_="[:,r:]">{[:,:r]} &amp; \boldsymbol{U}</em>}\end{pmatrix},\quad \boldsymbol{F<em :r_=":r]" _:r_="[:r,">{[:, :r]} = \begin{pmatrix} \boldsymbol{F}</em>} \\ \boldsymbol{F<em :r_=":r]" _:_="[:,">{[r:, :r]}\end{pmatrix}\end{equation}<br />
再加上$\boldsymbol{V}\boldsymbol{\Sigma}</em>} = \boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>$，可以得到<br />
\begin{equation}\begin{aligned}<br />
d\boldsymbol{U}<em :r_=":r]" _:_="[:,">{[:, :r]} =&amp;\, \boldsymbol{U}</em>}(\boldsymbol{F<em :r_=":r]" _:_="[:,">{[:r, :r]}\otimes(\boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>} + \boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:r, :r]}\boldsymbol{V}</em>}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U<em _:_="[:," r:_="r:]">{[:, :r]})) \\[6pt]<br />
&amp;\,\qquad + \boldsymbol{U}</em>}(\boldsymbol{F<em _:_="[:," r:_="r:]">{[r:, :r]}\otimes(\boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>))<br />
\end{aligned}\label{eq:dU-r}\end{equation}<br />
根据假设，当$i &gt; r$时$\sigma_i=0$，所以$\boldsymbol{F}<em :r_=":r]" _r:_="[r:,">{[r:, :r]}$的每一行都是$(\sigma_1^{-2},\sigma_2^{-2},\cdots,\sigma_r^{-2})$，于是$\boldsymbol{F}</em>}\otimes$等于右乘$\boldsymbol{\Sigma<em :r_=":r]" _r:_="[r:,">{[:r, :r]}^{-2}$，因此<br />
\begin{equation}\boldsymbol{F}</em>}\otimes(\boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, r:]}^{\top}(d\boldsymbol{W})\boldsymbol{V}</em>}\boldsymbol{\Sigma<em _:_="[:," r:_="r:]">{[:r, :r]}) = \boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>}^{-1}\end{equation<br />
最后利用$\boldsymbol{U}\boldsymbol{U}^{\top} = \boldsymbol{I}$和$\boldsymbol{U} = (\boldsymbol{U}<em _:_r:_="[:,r:]">{[:,:r]},\boldsymbol{U}</em>})$，得到$\boldsymbol{U<em _:_="[:," r:_="r:]">{[:, r:]}\boldsymbol{U}</em>}^{\top} = \boldsymbol{I} - \boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, :r]}\boldsymbol{U}</em>$得}^{\top}$，代入式$\eqref{eq:dU-r<br />
\begin{equation}\begin{aligned}<br />
d\boldsymbol{U}<em :r_=":r]" _:_="[:,">{[:, :r]} =&amp;\, \boldsymbol{U}</em>}(\boldsymbol{F<em :r_=":r]" _:_="[:,">{[:r, :r]}\otimes(\boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>} + \boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:r, :r]}\boldsymbol{V}</em>}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, :r]})) \\[6pt]<br />
&amp;\,\qquad \color{orange}{+ (\boldsymbol{I} - \boldsymbol{U}</em>}\boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, :r]}^{\top})(d\boldsymbol{W})\boldsymbol{V}</em>}\boldsymbol{\Sigma<em _:_="[:," r:_="r:]">{[:r, :r]}^{-1}}<br />
\end{aligned}\end{equation}<br />
这就将$d\boldsymbol{U}</em>}$表示成了$\boldsymbol{U<em :r_=":r]" _:r_="[:r,">{[:, :r]},\boldsymbol{\Sigma}</em>},\boldsymbol{V<em :r_=":r]" _:_="[:,">{[:, :r]}$的函数，在非零奇异值两两不等的假设下，三者是唯一确定的，所以$d\boldsymbol{U}</em>$多出了橙色这一项。类似地}$是唯一确定的，相比式$\eqref{eq:dU<br />
\begin{equation}\begin{aligned}<br />
d\boldsymbol{V}<em :r_=":r]" _:_="[:,">{[:, :r]} =&amp;\, \boldsymbol{V}</em>}(\boldsymbol{F<em :r_=":r]" _:_="[:,">{[:r, :r]}\otimes(\boldsymbol{V}</em>}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>} + \boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:r, :r]}\boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:_="[:,">{[:, :r]})) \\[6pt]<br />
&amp;\,\qquad \color{orange}{+ (\boldsymbol{I} - \boldsymbol{V}</em>}\boldsymbol{V<em :r_=":r]" _:_="[:,">{[:, :r]}^{\top})(d\boldsymbol{W})^{\top}\boldsymbol{U}</em>}\boldsymbol{\Sigma}_{[:r, :r]}^{-1}<br />
\end{aligned}\end{equation}</p>
<h2 id="_10">文章小结<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>本文较为详细地推导了SVD的求导公式。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10878">https://spaces.ac.cn/archives/10878</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 26, 2025). 《SVD的导数 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10878">https://spaces.ac.cn/archives/10878</a></p>
<p>@online{kexuefm-10878,<br />
title={SVD的导数},<br />
author={苏剑林},<br />
year={2025},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/10878}},<br />
} </p>
<hr />
<h2 id="_11">公式推导与注释<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈十统一扩散模型理论篇.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#77 生成扩散模型漫谈（十）：统一扩散模型（理论篇）</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="moe环游记2不患寡而患不均.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#79 MoE环游记：2、不患寡而患不均</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#svd">SVD的导数</a><ul>
<li><a href="#_1">推导基础</a></li>
<li><a href="#_2">奇异值</a></li>
<li><a href="#_3">奇异向量</a></li>
<li><a href="#_4">梯度（一）</a></li>
<li><a href="#_5">梯度（二）</a></li>
<li><a href="#_6">梯度（三）</a></li>
<li><a href="#_7">梯度（四）</a></li>
<li><a href="#_8">数值问题</a></li>
<li><a href="#_9">一般结果</a></li>
<li><a href="#_10">文章小结</a></li>
<li><a href="#_11">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>