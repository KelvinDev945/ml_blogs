<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SVD的导数 | ML & Math Blog Posts</title>
    <meta name="description" content="SVD的导数&para;
原文链接: https://spaces.ac.cn/archives/10878
发布日期: 

SVD（Singular Value Decomposition，奇异值分解）是常见的矩阵分解算法，相信很多读者都已经对它有所了解，此前我们在《低秩近似之路（二）：SVD》也专门介绍过它。然而，读者是否想到，SVD竟然还可以求导呢？笔者刚了解到这一结论时也颇感意外，因为直觉...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #172 SVD的导数
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#172</span>
                SVD的导数
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-04-26</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=微积分" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 微积分</span>
                </a>
                
                <a href="../index.html?tags=分析" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 分析</span>
                </a>
                
                <a href="../index.html?tags=矩阵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 矩阵</span>
                </a>
                
                <a href="../index.html?tags=SVD" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> SVD</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="svd">SVD的导数<a class="toc-link" href="#svd" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10878">https://spaces.ac.cn/archives/10878</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>SVD（Singular Value Decomposition，奇异值分解）是常见的矩阵分解算法，相信很多读者都已经对它有所了解，此前我们在<a href="/archives/10407">《低秩近似之路（二）：SVD》</a>也专门介绍过它。然而，读者是否想到，SVD竟然还可以求导呢？笔者刚了解到这一结论时也颇感意外，因为直觉上“分解”往往都是不可导的。但事实是，SVD在一般情况下确实可导，这意味着理论上我们可以将SVD嵌入到模型中，并用基于梯度的优化器来端到端训练。</p>
<p>问题来了，既然SVD可导，那么它的导函数长什么样呢？接下来，我们将参考文献<a href="https://j-towns.github.io/papers/svd-derivative.pdf">《Differentiating the Singular Value Decomposition》</a>，逐步推导SVD的求导公式。</p>
<h2 id="_1">推导基础<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>假设$\boldsymbol{W}$是满秩的$n\times n$矩阵，且全体奇异值两两不等，这是比较容易讨论的情形，后面我们也会讨论哪些条件可以放宽一点。接着，我们设$\boldsymbol{W}$的SVD为：<br />
\begin{equation}\boldsymbol{W} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}\end{equation}<br />
所谓SVD求导，实际上就是设法分别求出$\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}$关于$\boldsymbol{W}$的梯度或微分。为此，我们先对上式两边求微分<br />
\begin{equation}d\boldsymbol{W} = (d\boldsymbol{U})\boldsymbol{\Sigma}\boldsymbol{V}^{\top} + \boldsymbol{U}(d\boldsymbol{\Sigma})\boldsymbol{V}^{\top} + \boldsymbol{U}\boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\end{equation}<br />
左乘$\boldsymbol{U}^{\top}$、右乘$\boldsymbol{V}$，并利用$\boldsymbol{U}^{\top}\boldsymbol{U} = \boldsymbol{I}, \boldsymbol{V}^{\top}\boldsymbol{V} = \boldsymbol{I}$得到<br />
\begin{equation}\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V} = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}\label{eq:core}\end{equation}<br />
这便是后面推导的基础。注意对恒等式$\boldsymbol{U}^{\top}\boldsymbol{U} = \boldsymbol{I}, \boldsymbol{V}^{\top}\boldsymbol{V} = \boldsymbol{I}$两端求微分，我们还可以得到<br />
\begin{equation}(d\boldsymbol{U})^{\top}\boldsymbol{U} + \boldsymbol{U}^{\top}(d\boldsymbol{U}) = \boldsymbol{0},\quad (d\boldsymbol{V})^{\top}\boldsymbol{V} + \boldsymbol{V}^{\top}(d\boldsymbol{V}) = \boldsymbol{0}\end{equation}<br />
这表明$\boldsymbol{U}^{\top}(d\boldsymbol{U})$和$(d\boldsymbol{V})^{\top}\boldsymbol{V}$都是反对称矩阵。</p>
<h2 id="_2">奇异值<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>反对称矩阵的特点是对角线元素都是零，而$\boldsymbol{\Sigma}$是对角阵，非对角线元素都是零，这启发我们可能需要分别处理处理对角线和非对角线元素。</p>
<p>首先，我们定义矩阵$\boldsymbol{I}$和$\bar{\boldsymbol{I}}$：$\boldsymbol{I}$就是单位阵，即对角线元素全是1，非对角线元素全是0；$\bar{\boldsymbol{I}}$则是单位阵的互补阵，即对角线元素全是0，而非对角线元素全是1。利用$\boldsymbol{I}$、$\bar{\boldsymbol{I}}$和Hadamard积$\otimes$，我们可以分别提取出式$\eqref{eq:core}$的对角线和非对角线部分：<br />
\begin{align}
\boldsymbol{I}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}) =&amp;\, \boldsymbol{I}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}) = d\boldsymbol{\Sigma} \label{eq:value} \\[8pt]
\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}) =&amp;\, \bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}) = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}\label{eq:vector}
\end{align}<br />
现在我们先来看式$\eqref{eq:value}$，它可以等价地写成<br />
\begin{equation}d\sigma_i = \boldsymbol{u}_i^{\top}(d\boldsymbol{W})\boldsymbol{v}_i\label{eq:d-sigma}\end{equation}<br />
这就是第$i$个奇异值$\sigma_i$的微分，其中$\boldsymbol{u}_i,\boldsymbol{v}_i$分别是$\boldsymbol{U},\boldsymbol{V}$的第$i$列。<a href="/archives/10648">《从谱范数梯度到新式权重衰减的思考》</a>中讨论的谱范数梯度，实际上就只是这里$i=1$时的特例。</p>
<h2 id="_3">奇异向量<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>然后我们再来看式$\eqref{eq:vector}$：<br />
\begin{equation}\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}) = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}\label{eq:vector-1}\end{equation}<br />
转置一下<br />
\begin{equation}\begin{aligned}
\bar{\boldsymbol{I}}\otimes(\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}) =&amp;\, \boldsymbol{\Sigma}(d\boldsymbol{U})^{\top}\boldsymbol{U} + \boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Sigma} \\[6pt]
=&amp;\, -\boldsymbol{\Sigma}\boldsymbol{U}^{\top}(d\boldsymbol{U}) - (d\boldsymbol{V})^{\top}\boldsymbol{V}\boldsymbol{\Sigma}
\end{aligned}\label{eq:vector-2}\end{equation}<br />
第二个等号利用了“$\boldsymbol{U}^{\top}(d\boldsymbol{U})$和$(d\boldsymbol{V})^{\top}\boldsymbol{V}$都是反对称矩阵”这一事实。式$\eqref{eq:vector-1}$和式$\eqref{eq:vector-2}$就是关于$d\boldsymbol{U},d\boldsymbol{V}$的线性方程组，我们要从中解出$d\boldsymbol{U},d\boldsymbol{V}$。</p>
<p>求解思路就是普通的消元法。首先，由$\eqref{eq:vector-1}\times\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\times\eqref{eq:vector-2}$得到<br />
\begin{equation}\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}) = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma}^2 - \boldsymbol{\Sigma}^2\boldsymbol{U}^{\top}(d\boldsymbol{U})\end{equation}<br />
这里利用了对角阵$\boldsymbol{\Sigma}$满足$\boldsymbol{\Sigma}(\bar{\boldsymbol{I}}\otimes \boldsymbol{M}) = \bar{\boldsymbol{I}}\otimes (\boldsymbol{\Sigma}\boldsymbol{M})$以及$(\bar{\boldsymbol{I}}\otimes \boldsymbol{M})\boldsymbol{\Sigma} = \bar{\boldsymbol{I}}\otimes (\boldsymbol{M}\boldsymbol{\Sigma})$的事实。我们知道，左（右）乘一个对角阵，等于矩阵的每一行（列）都乘以对角阵上相应的元素，所以如果我们定义矩阵$\boldsymbol{E}$，其中$\boldsymbol{E}<em i_j="i,j">{i,j} = \sigma_j^2 - \sigma_i^2$，那么$\boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma}^2 - \boldsymbol{\Sigma}^2\boldsymbol{U}^{\top}(d\boldsymbol{U}) = \boldsymbol{E}\otimes (\boldsymbol{U}^{\top}(d\boldsymbol{U}))$，于是上式可以写成<br />
\begin{equation}\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}) = \boldsymbol{E}\otimes (\boldsymbol{U}^{\top}(d\boldsymbol{U}))\label{eq:dU-0}\end{equation}<br />
继而可以解得<br />
\begin{equation}d\boldsymbol{U} = \boldsymbol{U}(\boldsymbol{F}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}))\label{eq:dU}\end{equation}<br />
类似地，由$\boldsymbol{\Sigma}\times \eqref{eq:vector-1} + \eqref{eq:vector-2}\times \boldsymbol{\Sigma}$解得：<br />
\begin{equation}d\boldsymbol{V} = \boldsymbol{V}(\boldsymbol{F}\otimes(\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}))\label{eq:dV}\end{equation}<br />
式$\eqref{eq:dU},\eqref{eq:dV}$便是特征向量的微分。其中<br />
\begin{equation}\boldsymbol{F}</em>} = \left\{\begin{aligned
&amp;\, 1/(\sigma_j^2 - \sigma_i^2), &amp;\, i\neq j \\
&amp;\, 0, &amp;\, i = j
\end{aligned}\right.\end{equation}</p>
<h2 id="_4">梯度（一）<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>微分有了，怎么把梯度求出来呢？这还真有点麻烦。倒不是技术上的麻烦，而是表示上的麻烦，比如$\boldsymbol{W},\boldsymbol{U}$是一个$n\times n$矩阵，那么$\boldsymbol{U}$关于$\boldsymbol{W}$的完整梯度就是一个$n\times n\times n\times n$的4阶张量，而高阶张量是大多数人包括笔者都不熟悉的内容。</p>
<p>为了绕开高阶张量的麻烦，我们有两个方案。首先，从编程角度看，我们根本没必要求出梯度的形式，而是根据微分的结果写出等价的前向形式，然后交给框架的自动求导就行。比如，从式$\eqref{eq:d-sigma}$我们可以断定$\sigma_i$的梯度等于$\newcommand{\sg}[1]{\color{skyblue}{#1}} \sg{\boldsymbol{u}<em _boldsymbol_W="\boldsymbol{W">i}^{\top} \boldsymbol{W} \sg{\boldsymbol{v}_i}$的梯度，即<br />
\begin{equation}\nabla</em>}} \sigma_i = \nabla_{\boldsymbol{W}} (\sg{\boldsymbol{u<em _boldsymbol_W="\boldsymbol{W">i}^{\top} \boldsymbol{W} \sg{\boldsymbol{v}_i})\end{equation}<br />
这里将符号颜色改为$\sg{\blacksquare}$色代表stop_gradient算子，避免公式过于臃肿。刚好$\sigma_i$又等于$\boldsymbol{u}_i^{\top}\boldsymbol{W}\boldsymbol{v}_i$，所以我们只需要把代码中出现$\sigma_i$的地方，都替换成$\sg{\boldsymbol{u}_i}^{\top} \boldsymbol{W} \sg{\boldsymbol{v}_i}$，那么就可以自动获得正确的梯度。一般地，我们有<br />
\begin{equation}\nabla</em>}} \boldsymbol{\Sigma} = \nabla_{\boldsymbol{W}} (\boldsymbol{I}\otimes(\sg{\boldsymbol{U}}^{\top} \boldsymbol{W} \sg{\boldsymbol{V}}))\end{equation
即所有$\boldsymbol{\Sigma}$换成$\boldsymbol{I}\otimes(\sg{\boldsymbol{U}}^{\top} \boldsymbol{W} \sg{\boldsymbol{V}})$即可。</p>
<p>同理，从式$\eqref{eq:dU}$我们知道<br />
\begin{equation}\nabla_{\boldsymbol{W}}\boldsymbol{U} = \nabla_{\boldsymbol{W}}(\sg{\boldsymbol{U}}(\sg{\boldsymbol{F}}\otimes(\sg{\boldsymbol{U}}^{\top}\boldsymbol{W}\sg{\boldsymbol{V}\boldsymbol{\Sigma}} + \sg{\boldsymbol{\Sigma}\boldsymbol{V}}^{\top}\boldsymbol{W}^{\top}\sg{\boldsymbol{U}})))\end{equation}<br />
可以验证$\boldsymbol{U}(\boldsymbol{F}\otimes(\boldsymbol{U}^{\top}\boldsymbol{W}\boldsymbol{V}\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}\boldsymbol{W}^{\top}\boldsymbol{U}))$刚好是零矩阵，所以我们只需要将代码中所有出现$\boldsymbol{U}$的地方，都替换成<br />
\begin{equation}\boldsymbol{U} \quad \to \quad \sg{\boldsymbol{U}} + \sg{\boldsymbol{U}}(\sg{\boldsymbol{F}}\otimes(\sg{\boldsymbol{U}}^{\top}\boldsymbol{W}\sg{\boldsymbol{V}\boldsymbol{\Sigma}} + \sg{\boldsymbol{\Sigma}\boldsymbol{V}}^{\top}\boldsymbol{W}^{\top}\sg{\boldsymbol{U}}))\end{equation}<br />
那就能保持正确的前向结果，同时获得正确的梯度。基于同样的原理，$\boldsymbol{V}$的替换格式是<br />
\begin{equation}\boldsymbol{V} \quad \to \quad \sg{\boldsymbol{V}} + \sg{\boldsymbol{V}}(\sg{\boldsymbol{F}}\otimes(\sg{\boldsymbol{V}}^{\top}\boldsymbol{W}^{\top}\sg{\boldsymbol{U}\boldsymbol{\Sigma}} + \sg{\boldsymbol{\Sigma}\boldsymbol{U}}^{\top}\boldsymbol{W}\sg{\boldsymbol{V}}))\end{equation}</p>
<h2 id="_5">梯度（二）<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>第二个方案是直接求出损失函数关于$\boldsymbol{W}$的梯度。具体来说，假设损失函数是$\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}$的函数，记为$\mathcal{L}(\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V})$，我们直接求$\nabla_{\boldsymbol{W}}\mathcal{L}$，它是一个矩阵，可以用$\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V},\nabla_{\boldsymbol{U}}\mathcal{L},\nabla_{\boldsymbol{\Sigma}}\mathcal{L},\nabla_{\boldsymbol{V}}\mathcal{L}$表示出来，这些量也都只是矩阵，所以不用涉及到高阶张量。</p>
<p>在上一节中，我们已经求出了具有相同梯度的$\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}$的等效函数，除开被stop_gradient的部分外，这些等效函数关于$\boldsymbol{W}$都是线性的，所以此时问题本质上已经变成了线性复合函数的梯度。我们之前在<a href="/archives/10366#%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC">《低秩近似之路（一）：伪逆》</a>的“矩阵求导”一节便已经讨论过相关方法。具体来说，我们有<br />
\begin{align}
\boldsymbol{X} = \boldsymbol{A}\boldsymbol{B}\boldsymbol{C} &amp;\,\quad\Rightarrow\quad \nabla_{\boldsymbol{B}}f(\boldsymbol{X}) = \boldsymbol{A}^{\top}(\nabla_{\boldsymbol{X}}f(\boldsymbol{X}))\boldsymbol{C}^{\top} \\[8pt]
\boldsymbol{X} = \boldsymbol{A}\boldsymbol{B}^{\top}\boldsymbol{C} &amp;\,\quad\Rightarrow\quad \nabla_{\boldsymbol{B}}f(\boldsymbol{X}) = \boldsymbol{C}(\nabla_{\boldsymbol{X}}f(\boldsymbol{X}))^{\top}\boldsymbol{A} \\[8pt]
\boldsymbol{X} = \boldsymbol{A}\otimes\boldsymbol{B} &amp;\,\quad\Rightarrow\quad \nabla_{\boldsymbol{B}}f(\boldsymbol{X}) = \boldsymbol{A}\otimes \nabla_{\boldsymbol{X}}f(\boldsymbol{X})
\end{align}<br />
利用这些基本公式，以及复合函数求导的链式法则，我们可以写出<br />
\begin{equation}\begin{aligned}
\nabla_{\boldsymbol{W}}\mathcal{L} \quad = \qquad &amp;\,\boldsymbol{U}(\boldsymbol{F}\otimes(\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{U}}\mathcal{L}) - (\nabla_{\boldsymbol{U}}\mathcal{L})^{\top}\boldsymbol{U}))\boldsymbol{\Sigma}\boldsymbol{V}^{\top} \\[6pt]
+ &amp;\,\boldsymbol{U}(\boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L}))\boldsymbol{V}^{\top} \\[6pt]
+ &amp;\,\boldsymbol{U}\boldsymbol{\Sigma}(\boldsymbol{F}\otimes(\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L}) - (\nabla_{\boldsymbol{V}}\mathcal{L})^{\top}\boldsymbol{V})))\boldsymbol{V}^{\top}
\end{aligned}\end{equation}<br />
整个过程就是反复地利用基本公式和链式法则，以及$\boldsymbol{F}^{\top} = -\boldsymbol{F}$，原则上没有难度，就是需要谨慎地集中注意力，建议读者亲自动手完成这个推导过程，这是一道相当实用的矩阵求导练习题。最后引入两个记号<br />
\begin{equation}\newcommand{\sym}[1]{\color{red}{[}#1\color{red}{]<em skew="skew">{sym}}} \newcommand{\skew}[1]{\color{red}{[}#1\color{red}{]</em>}}} \sym{\boldsymbol{X}} = \frac{1}{2}(\boldsymbol{X} + \boldsymbol{X}^{\top}),\qquad \skew{\boldsymbol{X}} = \frac{1}{2}(\boldsymbol{X} - \boldsymbol{X}^{\top})\end{equation
我们可以将梯度结果简写成
\begin{equation}\nabla_{\boldsymbol{W}}\mathcal{L} = \boldsymbol{U}\Big(2(\boldsymbol{F}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{U}}\mathcal{L})})\boldsymbol{\Sigma} + \boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L}) + 2\boldsymbol{\Sigma}(\boldsymbol{F}\otimes\skew{\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L})})\Big)\boldsymbol{V}^{\top}<br />
\label{eq:w-grad-l}\end{equation}</p>
<h2 id="_6">梯度（三）<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>现在可以牛刀小试一下，求$\boldsymbol{O}=\mathop{\text{msign}}(\boldsymbol{W})=\boldsymbol{U}\boldsymbol{V}^{\top}$的梯度，其中$\mathop{\text{msign}}$的概念我们在<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>介绍Muon时已经讨论过。</p>
<p>根据$\mathop{\text{msign}}$的定义，我们有<br />
\begin{equation}\nabla_{\boldsymbol{U}}\mathcal{L} = (\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V},\qquad \nabla_{\boldsymbol{V}}\mathcal{L} = (\nabla_{\boldsymbol{O}}\mathcal{L})^{\top} \boldsymbol{U}\end{equation}<br />
代入式$\eqref{eq:w-grad-l}$得到<br />
\begin{equation}\begin{aligned}
\nabla_{\boldsymbol{W}}\mathcal{L} =&amp;\, 2\boldsymbol{U}\Big((\boldsymbol{F}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}})\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(\boldsymbol{F}\otimes\skew{\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})^{\top}\boldsymbol{U}})\Big)\boldsymbol{V}^{\top} \\[6pt]
=&amp;\, 2\boldsymbol{U}\Big((\boldsymbol{F}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}})\boldsymbol{\Sigma} - \boldsymbol{\Sigma}(\boldsymbol{F}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}})\Big)\boldsymbol{V}^{\top} \\[6pt]
=&amp;\, 2\boldsymbol{U}\big(\boldsymbol{G}\otimes\skew{\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}}\big)\boldsymbol{V}^{\top}
\end{aligned}\end{equation}<br />
其中$\boldsymbol{G}_{i,j} = 1/(\sigma_i + \sigma_j)$。</p>
<h2 id="_7">梯度（四）<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>最后让我们来考虑一个常用的特殊例子，当$\boldsymbol{W}$还是正定对称矩阵时，它的SVD具有$\boldsymbol{V}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$的形式，即$\boldsymbol{U}=\boldsymbol{V}$。我们重复前面的推导，先对两边求微分<br />
\begin{equation}d\boldsymbol{W} = (d\boldsymbol{V})\boldsymbol{\Sigma}\boldsymbol{V}^{\top} + \boldsymbol{V}(d\boldsymbol{\Sigma})\boldsymbol{V}^{\top} + \boldsymbol{V}\boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\end{equation}<br />
然后左乘$\boldsymbol{V}^{\top}$、右乘$\boldsymbol{V}$：<br />
\begin{equation}\begin{aligned}
\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V} =&amp;\, \boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V} \\[6pt]
=&amp;\, \boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} - \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{V})
\end{aligned}\end{equation}<br />
由此可见<br />
\begin{gather}
d\boldsymbol{\Sigma} = \boldsymbol{I}\otimes(\boldsymbol{V}(d\boldsymbol{W})\boldsymbol{V}^{\top}) \\[8pt]
\boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Sigma} - \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{V}) = \bar{\boldsymbol{I}}\otimes(\boldsymbol{V}(d\boldsymbol{W})\boldsymbol{V}^{\top})
\end{gather}<br />
由第二式可以进一步解得<br />
\begin{equation}d\boldsymbol{V} = \boldsymbol{V}(\boldsymbol{K}^{\top}\otimes(\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V}))\end{equation}<br />
其中<br />
\begin{equation}\boldsymbol{K}<em _boldsymbol_W="\boldsymbol{W">{i,j} = \left\{\begin{aligned}
&amp;\, 1/(\sigma_i - \sigma_j), &amp;\, i\neq j \\
&amp;\, 0, &amp;\, i = j
\end{aligned}\right.\end{equation}<br />
根据这个结果，我们有<br />
\begin{equation}\nabla</em>}}\mathcal{L} = \boldsymbol{V}(\boldsymbol{K}^{\top}\otimes(\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L})) + \boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L}))\boldsymbol{V}^{\top} \end{equation
注意陷阱！上式其实是错误的，它是链式法则的结果，但链式法则只适用于无约束求导，而这里带有约束$\boldsymbol{W}=\boldsymbol{W}^{\top}$，正确的梯度应该还要包含对称化：<br />
\begin{equation}\begin{aligned}
\nabla_{\boldsymbol{W}}\mathcal{L} =&amp;\, \boldsymbol{V}\Big(\sym{\boldsymbol{K}^{\top}\otimes(\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L}))} + \boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L})\Big)\boldsymbol{V}^{\top} \\[6pt]
=&amp;\, \boldsymbol{V}\Big(\boldsymbol{K}^{\top}\otimes\skew{\boldsymbol{V}^{\top}(\nabla_{\boldsymbol{V}}\mathcal{L})} + \boldsymbol{I}\otimes(\nabla_{\boldsymbol{\Sigma}}\mathcal{L})\Big)\boldsymbol{V}^{\top}\end{aligned}\end{equation}<br />
另一个陷阱是直接将$\boldsymbol{U}=\boldsymbol{V}$代入式$\eqref{eq:w-grad-l}$来推导，这将会导致$\boldsymbol{K}$项翻倍，原因是式$\eqref{eq:w-grad-l}$区分了$\nabla_{\boldsymbol{U}}\mathcal{L}$和$\nabla_{\boldsymbol{V}}\mathcal{L}$，而在正定对称假设下，$\boldsymbol{U},\boldsymbol{V}$是相同的，对$\boldsymbol{V}$求梯度实际上无形中求了原本的$\nabla_{\boldsymbol{U}}\mathcal{L},\nabla_{\boldsymbol{V}}\mathcal{L}$之和，所以会导致重复计算。相关文献还可以参考<a href="https://openaccess.thecvf.com/content_iccv_2015/html/Ionescu_Matrix_Backpropagation_for_ICCV_2015_paper.html">《Matrix Backpropagation for Deep Networks With Structured Layers》</a>。</p>
<h2 id="_8">数值问题<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>可能有些读者疑问：我只能保证初始化矩阵的奇异值两两不等，怎么保证经过训练后的矩阵仍然满足这个条件呢？答案是梯度自然会帮我们保证。从式$\eqref{eq:dU}$和式$\eqref{eq:dV}$可以看出，它的梯度包含了$\boldsymbol{F}$，而由$\boldsymbol{F}_{i,j} = \frac{1}{\sigma_j^2 - \sigma_i^2}$可知，一旦两个奇异值接近，那么梯度将会非常大，所以优化器会自动把它们推开。</p>
<p>不过，这个特性也给实际训练带来了数值不稳定性，主要体现在$\sigma_i,\sigma_j$接近时的梯度爆炸。对此，论文<a href="https://papers.cool/arxiv/1906.09023">《Backpropagation-Friendly Eigendecomposition》</a>提出用“幂迭代（Power Iteration）”替代精确的SVD。后来，论文<a href="https://papers.cool/arxiv/2104.03821">《Robust Differentiable SVD》</a>证明了它理论上等于对$\boldsymbol{F}_{i,j} = -\frac{1}{（\sigma_i + \sigma_j)(\sigma_i - \sigma_j）}$中的$\frac{1}{\sigma_i - \sigma_j}$做泰勒近似（假设$\sigma_j &lt; \sigma_i$）：<br />
\begin{equation}\frac{1}{\sigma_i - \sigma_j} = \frac{1}{\sigma_i}\frac{1}{1-(\sigma_j/\sigma_i)}\approx \frac{1}{\sigma_i}\left(1 + \left(\frac{\sigma_j}{\sigma_i}\right) + \left(\frac{\sigma_j}{\sigma_i}\right)^2 + \cdots + \left(\frac{\sigma_j}{\sigma_i}\right)^N \right)\end{equation}<br />
使用泰勒近似后，至少对于$\sigma_j \to \sigma_i$的情况不会出现梯度爆炸了。再后来，作者在<a href="https://papers.cool/arxiv/2105.02498">《Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?》</a>将其推广到更一般的Padé近似。这一系列工作笔者并不是太熟悉，因此就不过多展开了。</p>
<p>只是笔者有一个疑问，如果只是单纯为了避免数值爆炸，似乎没必要上这些工具，直接给$\sigma_j/\sigma_i$加个截断不就好了？比如<br />
\begin{equation}\frac{1}{1-(\sigma_j/\sigma_i)}\approx \frac{1}{1-\min(\sigma_j/\sigma_i, 0.99)}\end{equation}<br />
这样不就简单避免了它趋于无穷？还是笔者有什么认识不到位的地方？如果读者了解相关背景，敬请在评论区指正，谢谢。</p>
<h2 id="_9">一般结果<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>到目前为止，上面出现的所有矩阵都是$n\times n$矩阵，因为这是在文章开头的假设“$\boldsymbol{W}$是满秩的$n\times n$矩阵，且全体奇异值两两不等”下进行的推导。这一节我们来讨论该条件可以放宽到什么程度。</p>
<p>简单来说，SVD可导的条件是“全体非零奇异值两两不等”，换言之，方阵可以去掉，满秩也可以去掉，但是非零奇异值仍须互不相同。因为一旦有相等的奇异值，那么SVD就不唯一，这从根本上破坏了可导性。当然，如果只要部分导数，那我们还可以放宽条件，比如只想要谱范数即$\sigma_1$的导数，那么只需要$\sigma_1 &gt; \sigma_2$。</p>
<p>那么，放宽条件后，微分结果怎样变化呢？我们不妨一般地设$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，秩为$r$，SVD为<br />
\begin{equation}\boldsymbol{W} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top},\quad\boldsymbol{U}\in\mathbb{R}^{n\times n},\boldsymbol{\Sigma}\in\mathbb{R}^{n\times m},\boldsymbol{V}\in\mathbb{R}^{m\times m}\end{equation}<br />
之所以允许零奇异值，是因为此时我们只关心$d\boldsymbol{U}<em :r_=":r]" _:_="[:,">{[:, :r]}$和$d\boldsymbol{V}</em>}$，在非零奇异值两两不等的假设下，它们是可以唯一确定的。我们从式$\eqref{eq:dU-0}$出发，直到式$\eqref{eq:dU-0}$的推导都是通用的，从式$\eqref{eq:dU-0}$也可以看出为什么要拒绝相同奇异值，因为$\sigma_i = \sigma_j$时$\sigma_j - \sigma_i$就无法求逆了。在式$\eqref{eq:dU-0}$中只保留跟$d\boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, :r]}$相关的部分，我们得到<br />
\begin{equation}\bar{\boldsymbol{I}}</em>}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:, :r]} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}</em>}) = \boldsymbol{E<em :r_=":r]" _:_="[:,">{[:, :r]}\otimes (\boldsymbol{U}^{\top}(d\boldsymbol{U}</em>}))\end{equation
这里$\boldsymbol{E}$的定义依然是$\boldsymbol{E}<em :r_=":r]" _:_="[:,">{i,j} = \sigma_j^2 - \sigma_i^2$，但$\boldsymbol{E}</em>$正好排除了所有0，所以可以顺利求逆，结果是<br />
\begin{equation}d\boldsymbol{U}<em :r_=":r]" _:_="[:,">{[:, :r]} = \boldsymbol{U}(\boldsymbol{F}</em>}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}\boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:, :r]} + \boldsymbol{\Sigma}\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}</em>}))\end{equation
进一步地，做如下分块
\begin{equation}\boldsymbol{U} = \begin{pmatrix}\boldsymbol{U}<em _:_r:_="[:,r:]">{[:,:r]} &amp; \boldsymbol{U}</em>}\end{pmatrix},\quad \boldsymbol{F<em :r_=":r]" _:r_="[:r,">{[:, :r]} = \begin{pmatrix} \boldsymbol{F}</em>} \\ \boldsymbol{F<em :r_=":r]" _:_="[:,">{[r:, :r]}\end{pmatrix}\end{equation}<br />
再加上$\boldsymbol{V}\boldsymbol{\Sigma}</em>} = \boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>$，可以得到<br />
\begin{equation}\begin{aligned}
d\boldsymbol{U}<em :r_=":r]" _:_="[:,">{[:, :r]} =&amp;\, \boldsymbol{U}</em>}(\boldsymbol{F<em :r_=":r]" _:_="[:,">{[:r, :r]}\otimes(\boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>} + \boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:r, :r]}\boldsymbol{V}</em>}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U<em _:_="[:," r:_="r:]">{[:, :r]})) \\[6pt]
&amp;\,\qquad + \boldsymbol{U}</em>}(\boldsymbol{F<em _:_="[:," r:_="r:]">{[r:, :r]}\otimes(\boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>))
\end{aligned}\label{eq:dU-r}\end{equation}<br />
根据假设，当$i &gt; r$时$\sigma_i=0$，所以$\boldsymbol{F}<em :r_=":r]" _r:_="[r:,">{[r:, :r]}$的每一行都是$(\sigma_1^{-2},\sigma_2^{-2},\cdots,\sigma_r^{-2})$，于是$\boldsymbol{F}</em>}\otimes$等于右乘$\boldsymbol{\Sigma<em :r_=":r]" _r:_="[r:,">{[:r, :r]}^{-2}$，因此<br />
\begin{equation}\boldsymbol{F}</em>}\otimes(\boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, r:]}^{\top}(d\boldsymbol{W})\boldsymbol{V}</em>}\boldsymbol{\Sigma<em _:_="[:," r:_="r:]">{[:r, :r]}) = \boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>}^{-1}\end{equation
最后利用$\boldsymbol{U}\boldsymbol{U}^{\top} = \boldsymbol{I}$和$\boldsymbol{U} = (\boldsymbol{U}<em _:_r:_="[:,r:]">{[:,:r]},\boldsymbol{U}</em>})$，得到$\boldsymbol{U<em _:_="[:," r:_="r:]">{[:, r:]}\boldsymbol{U}</em>}^{\top} = \boldsymbol{I} - \boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, :r]}\boldsymbol{U}</em>$得}^{\top}$，代入式$\eqref{eq:dU-r<br />
\begin{equation}\begin{aligned}
d\boldsymbol{U}<em :r_=":r]" _:_="[:,">{[:, :r]} =&amp;\, \boldsymbol{U}</em>}(\boldsymbol{F<em :r_=":r]" _:_="[:,">{[:r, :r]}\otimes(\boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>} + \boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:r, :r]}\boldsymbol{V}</em>}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, :r]})) \\[6pt]
&amp;\,\qquad \color{orange}{+ (\boldsymbol{I} - \boldsymbol{U}</em>}\boldsymbol{U<em :r_=":r]" _:_="[:,">{[:, :r]}^{\top})(d\boldsymbol{W})\boldsymbol{V}</em>}\boldsymbol{\Sigma<em _:_="[:," r:_="r:]">{[:r, :r]}^{-1}}
\end{aligned}\end{equation}<br />
这就将$d\boldsymbol{U}</em>}$表示成了$\boldsymbol{U<em :r_=":r]" _:r_="[:r,">{[:, :r]},\boldsymbol{\Sigma}</em>},\boldsymbol{V<em :r_=":r]" _:_="[:,">{[:, :r]}$的函数，在非零奇异值两两不等的假设下，三者是唯一确定的，所以$d\boldsymbol{U}</em>$多出了橙色这一项。类似地}$是唯一确定的，相比式$\eqref{eq:dU<br />
\begin{equation}\begin{aligned}
d\boldsymbol{V}<em :r_=":r]" _:_="[:,">{[:, :r]} =&amp;\, \boldsymbol{V}</em>}(\boldsymbol{F<em :r_=":r]" _:_="[:,">{[:r, :r]}\otimes(\boldsymbol{V}</em>}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U<em :r_=":r]" _:r_="[:r,">{[:, :r]}\boldsymbol{\Sigma}</em>} + \boldsymbol{\Sigma<em :r_=":r]" _:_="[:,">{[:r, :r]}\boldsymbol{U}</em>}^{\top}(d\boldsymbol{W})\boldsymbol{V<em :r_=":r]" _:_="[:,">{[:, :r]})) \\[6pt]
&amp;\,\qquad \color{orange}{+ (\boldsymbol{I} - \boldsymbol{V}</em>}\boldsymbol{V<em :r_=":r]" _:_="[:,">{[:, :r]}^{\top})(d\boldsymbol{W})^{\top}\boldsymbol{U}</em>}\boldsymbol{\Sigma}_{[:r, :r]}^{-1}
\end{aligned}\end{equation}</p>
<h2 id="_10">文章小结<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>本文较为详细地推导了SVD的求导公式。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10878">https://spaces.ac.cn/archives/10878</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 26, 2025). 《SVD的导数 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10878">https://spaces.ac.cn/archives/10878</a></p>
<p>@online{kexuefm-10878,<br />
title={SVD的导数},<br />
author={苏剑林},<br />
year={2025},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/10878}},<br />
} </p>
<hr />
<h2 id="_11">公式推导与注释<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h2>
<p>本节将对SVD导数进行极详细的数学推导，从多个角度深入理解这一重要结果。我们将涵盖微积分、矩阵分析、梯度计算等多个视角，并讨论退化情况、扰动理论、数值稳定性等关键问题。</p>
<h3 id="svd_1">一、SVD分解的微分理论基础<a class="toc-link" href="#svd_1" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 矩阵微分的基本概念<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>对于矩阵函数$\boldsymbol{W}(t)$，其微分定义为：
$$d\boldsymbol{W} = \lim_{\epsilon\to 0}\frac{\boldsymbol{W}(t+\epsilon) - \boldsymbol{W}(t)}{\epsilon}dt$$</p>
<p>这个定义可以推广到矩阵值函数。关键性质包括：</p>
<p><strong>性质1（乘积法则）</strong>：对于矩阵乘积$\boldsymbol{W} = \boldsymbol{A}\boldsymbol{B}\boldsymbol{C}$，有：
$$d\boldsymbol{W} = (d\boldsymbol{A})\boldsymbol{B}\boldsymbol{C} + \boldsymbol{A}(d\boldsymbol{B})\boldsymbol{C} + \boldsymbol{A}\boldsymbol{B}(d\boldsymbol{C})$$</p>
<p><strong>性质2（转置法则）</strong>：$(d\boldsymbol{A})^{\top} = d(\boldsymbol{A}^{\top})$</p>
<p><strong>性质3（正交约束）</strong>：若$\boldsymbol{U}^{\top}\boldsymbol{U} = \boldsymbol{I}$，对两边求微分：
$$\boldsymbol{0} = d(\boldsymbol{U}^{\top}\boldsymbol{U}) = (d\boldsymbol{U})^{\top}\boldsymbol{U} + \boldsymbol{U}^{\top}(d\boldsymbol{U})$$</p>
<p>这表明$\boldsymbol{U}^{\top}(d\boldsymbol{U})$是反对称矩阵（skew-symmetric），即：
$$[\boldsymbol{U}^{\top}(d\boldsymbol{U})]<em ji="ji">{ij} = -[\boldsymbol{U}^{\top}(d\boldsymbol{U})]</em>$$</p>
<p>特别地，对角线元素必为零：$[\boldsymbol{U}^{\top}(d\boldsymbol{U})]_{ii} = 0$。</p>
<h4 id="12-svd">1.2 从SVD分解到微分方程<a class="toc-link" href="#12-svd" title="Permanent link">&para;</a></h4>
<p>给定SVD分解$\boldsymbol{W} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，对两边求微分：
$$d\boldsymbol{W} = (d\boldsymbol{U})\boldsymbol{\Sigma}\boldsymbol{V}^{\top} + \boldsymbol{U}(d\boldsymbol{\Sigma})\boldsymbol{V}^{\top} + \boldsymbol{U}\boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}$$</p>
<p><strong>关键变换</strong>：左乘$\boldsymbol{U}^{\top}$，右乘$\boldsymbol{V}$，利用正交性$\boldsymbol{U}^{\top}\boldsymbol{U} = \boldsymbol{I}$和$\boldsymbol{V}^{\top}\boldsymbol{V} = \boldsymbol{I}$：
$$\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V} = \boldsymbol{U}^{\top}(d\boldsymbol{U})\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}(d\boldsymbol{V})^{\top}\boldsymbol{V}$$</p>
<p>记$\boldsymbol{\Omega}_U = \boldsymbol{U}^{\top}(d\boldsymbol{U})$和$\boldsymbol{\Omega}_V = (d\boldsymbol{V})^{\top}\boldsymbol{V}$，它们都是反对称矩阵。上式变为：
$$\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V} = \boldsymbol{\Omega}_U\boldsymbol{\Sigma} + d\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{\Omega}_V$$</p>
<p>这是关于$\boldsymbol{\Omega}_U$、$d\boldsymbol{\Sigma}$、$\boldsymbol{\Omega}_V$的Sylvester型方程。</p>
<h4 id="13">1.3 对角与非对角分离技术<a class="toc-link" href="#13" title="Permanent link">&para;</a></h4>
<p>利用Hadamard积（element-wise product）$\otimes$，我们可以分离方程的对角和非对角部分。</p>
<p>定义：
- $\boldsymbol{I}$：单位矩阵（对角元素为1，非对角元素为0）
- $\bar{\boldsymbol{I}}$：单位矩阵的补（对角元素为0，非对角元素为1）</p>
<p>显然$\boldsymbol{I} + \bar{\boldsymbol{I}} = \boldsymbol{J}$（全1矩阵）。</p>
<p><strong>对角部分</strong>：由于$\boldsymbol{\Omega}_U$和$\boldsymbol{\Omega}_V$的对角元素为零，对角阵$\boldsymbol{\Sigma}$的非对角元素为零：
$$\boldsymbol{I}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}) = \boldsymbol{I}\otimes(d\boldsymbol{\Sigma}) = d\boldsymbol{\Sigma}$$</p>
<p>写成分量形式：
$$d\sigma_i = [\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}]_{ii} = \boldsymbol{u}_i^{\top}(d\boldsymbol{W})\boldsymbol{v}_i$$</p>
<p>这就是<strong>奇异值的微分公式</strong>。</p>
<p><strong>非对角部分</strong>：
$$\bar{\boldsymbol{I}}\otimes(\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}) = \bar{\boldsymbol{I}}\otimes(\boldsymbol{\Omega}_U\boldsymbol{\Sigma} + \boldsymbol{\Sigma}\boldsymbol{\Omega}_V)$$</p>
<p>展开为分量形式（$i\neq j$）：
$$[\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}]<em ij="ij">{ij} = [\boldsymbol{\Omega}_U]</em>}\sigma_j + \sigma_i[\boldsymbol{\Omega<em ij="ij">V]</em>$$</p>
<h3 id="_12">二、奇异值梯度的深入分析<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 奇异值梯度的几何意义<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p>从$d\sigma_i = \boldsymbol{u}<em jk="jk">i^{\top}(d\boldsymbol{W})\boldsymbol{v}_i$可以立即得出：
$$\frac{\partial \sigma_i}{\partial W</em>$$}} = u_{ji}v_{ki</p>
<p>以矩阵形式：
$$\nabla_{\boldsymbol{W}}\sigma_i = \boldsymbol{u}_i\boldsymbol{v}_i^{\top}$$</p>
<p><strong>几何解释</strong>：这是一个秩1矩阵，其方向由左右奇异向量张成。物理意义是：沿着$\boldsymbol{u}_i\boldsymbol{v}_i^{\top}$方向扰动矩阵$\boldsymbol{W}$，会最大程度地增加第$i$个奇异值。</p>
<h4 id="22">2.2 谱范数的梯度（最大奇异值）<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>谱范数定义为$|\boldsymbol{W}|<em _boldsymbol_W="\boldsymbol{W">2 = \sigma_1 = \max_i \sigma_i$。其梯度为：
$$\nabla</em>$$}}|\boldsymbol{W}|_2 = \boldsymbol{u}_1\boldsymbol{v}_1^{\top</p>
<p>这在深度学习中非常重要，特别是在<strong>谱归一化（Spectral Normalization）</strong>中。</p>
<p><strong>应用：谱归一化层</strong></p>
<p>在GAN等模型中，为了Lipschitz约束，需要对权重矩阵进行谱归一化：
$$\bar{\boldsymbol{W}} = \frac{\boldsymbol{W}}{|\boldsymbol{W}|_2}$$</p>
<p>其梯度为（使用商法则）：
$$\nabla_{\boldsymbol{W}}\mathcal{L}(\bar{\boldsymbol{W}}) = \frac{1}{\sigma_1}(\nabla_{\bar{\boldsymbol{W}}}\mathcal{L}) - \frac{1}{\sigma_1^2}(\nabla_{\bar{\boldsymbol{W}}}\mathcal{L}:\boldsymbol{W})\boldsymbol{u}_1\boldsymbol{v}_1^{\top}$$</p>
<p>其中$\boldsymbol{A}:\boldsymbol{B} = \text{tr}(\boldsymbol{A}^{\top}\boldsymbol{B})$是Frobenius内积。</p>
<h4 id="23">2.3 核范数的梯度（所有奇异值之和）<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p>核范数（nuclear norm）定义为$|\boldsymbol{W}|_<em> = \sum_i \sigma_i$。其梯度为：
$$\nabla_{\boldsymbol{W}}|\boldsymbol{W}|_</em> = \sum_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top} = \boldsymbol{U}\boldsymbol{V}^{\top}$$</p>
<p>这恰好是矩阵的"符号函数"$\text{msign}(\boldsymbol{W})$，在低秩优化中扮演重要角色。</p>
<h3 id="_13">三、奇异向量梯度的求解<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<h4 id="31-sylvester">3.1 Sylvester方程的求解策略<a class="toc-link" href="#31-sylvester" title="Permanent link">&para;</a></h4>
<p>回到非对角方程（$i\neq j$）：
$$[\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}]<em ij="ij">{ij} = [\boldsymbol{\Omega}_U]</em>}\sigma_j + \sigma_i[\boldsymbol{\Omega<em ij="ij">V]</em>$$</p>
<p><strong>目标</strong>：求解$\boldsymbol{\Omega}_U$和$\boldsymbol{\Omega}_V$。</p>
<p><strong>方法1：直接消元</strong></p>
<p>利用转置关系。对原方程转置：
$$[\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}]<em ij="ij">{ij} = [\boldsymbol{\Omega}_V^{\top}]</em>}\sigma_j + \sigma_i[\boldsymbol{\Omega<em ij="ij">U^{\top}]</em>$$</p>
<p>由反对称性$\boldsymbol{\Omega}<em ij="ij">U^{\top} = -\boldsymbol{\Omega}_U$和$\boldsymbol{\Omega}_V^{\top} = -\boldsymbol{\Omega}_V$：
$$[\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}]</em>} = -[\boldsymbol{\Omega<em ij="ij">V]</em>}\sigma_j - \sigma_i[\boldsymbol{\Omega<em ij="ij">U]</em>$$</p>
<p>现在我们有两个方程消除一个未知数。</p>
<p><strong>消去$\boldsymbol{\Omega}_V$</strong>：原方程乘以$\sigma_i$加上转置方程乘以$\sigma_j$：
$$\sigma_i[\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}]<em ij="ij">{ij} + \sigma_j[\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}]</em>} = [\boldsymbol{\Omega<em ij="ij">U]</em>}(\sigma_i\sigma_j - \sigma_i\sigma_j) + \sigma_i^2[\boldsymbol{\Omega<em ij="ij">V]</em>} - \sigma_j^2[\boldsymbol{\Omega<em ij="ij">V]</em>$$</p>
<p>简化为：
$$\sigma_i[\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V}]<em ij="ij">{ij} + \sigma_j[\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}]</em>} = (\sigma_i^2 - \sigma_j^2)[\boldsymbol{\Omega<em ij="ij">V]</em>$$</p>
<p>因此：
$$[\boldsymbol{\Omega}<em ij="ij">V]</em>]} = \frac{\sigma_i[\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V<em ij="ij">{ij} + \sigma_j[\boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}]</em>$$}}{\sigma_i^2 - \sigma_j^2</p>
<p>以矩阵形式（$i\neq j$）：
$$\boldsymbol{\Omega}_V = \boldsymbol{F}\otimes(\boldsymbol{\Sigma}\boldsymbol{U}^{\top}(d\boldsymbol{W})\boldsymbol{V} + \boldsymbol{V}^{\top}(d\boldsymbol{W})^{\top}\boldsymbol{U}\boldsymbol{\Sigma})$$</p>
<p>其中$\boldsymbol{F}<em ii="ii">{ij} = \frac{1}{\sigma_j^2 - \sigma_i^2}$（$i\neq j$），$\boldsymbol{F}</em> = 0$。</p>
<p><strong>恢复$d\boldsymbol{V}$</strong>：由$\boldsymbol{\Omega}_V = (d\boldsymbol{V})^{\top}\boldsymbol{V}$，两边右乘$\boldsymbol{V}^{\top}$：
$$(d\boldsymbol{V})^{\top} = \boldsymbol{\Omega}_V\boldsymbol{V}^{\top}$$</p>
<p>转置得：
$$d\boldsymbol{V} = \boldsymbol{V}\boldsymbol{\Omega}_V^{\top}$$</p>
<p>类似地可得$d\boldsymbol{U} = \boldsymbol{U}\boldsymbol{\Omega}_U^{\top}$。</p>
<h4 id="32">3.2 矩阵乘法的简化表示<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>注意到：
$$\boldsymbol{\Omega}_U\boldsymbol{\Sigma} - \boldsymbol{\Sigma}\boldsymbol{\Omega}_U = \boldsymbol{E}\otimes\boldsymbol{\Omega}_U$$</p>
<p>其中$\boldsymbol{E}<em ij="ij">{ij} = \sigma_j - \sigma_i$。这是因为：
$$[\boldsymbol{\Omega}_U\boldsymbol{\Sigma}]</em>} = [\boldsymbol{\Omega<em ij="ij">U]</em>\sigma_j$$
$$[\boldsymbol{\Sigma}\boldsymbol{\Omega}<em ij="ij">U]</em>} = \sigma_i[\boldsymbol{\Omega<em ij="ij">U]</em>$$
$$[\boldsymbol{\Omega}<em ij="ij">U\boldsymbol{\Sigma} - \boldsymbol{\Sigma}\boldsymbol{\Omega}_U]</em>} = (\sigma_j - \sigma_i)[\boldsymbol{\Omega<em ij="ij">U]</em>$$</p>
<p>类似地：
$$\boldsymbol{\Omega}_U\boldsymbol{\Sigma}^2 - \boldsymbol{\Sigma}^2\boldsymbol{\Omega}_U = (\sigma_j^2 - \sigma_i^2)\boldsymbol{\Omega}_U = \boldsymbol{E}_2\otimes\boldsymbol{\Omega}_U$$</p>
<p>其中$\boldsymbol{E}_2 = \boldsymbol{E}\odot\boldsymbol{E}$（这里$\odot$表示element-wise乘法，实际上就是平方）。</p>
<h3 id="_14">四、退化情况：重复奇异值的处理<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 重复奇异值的不可导性<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p><strong>定理</strong>：若$\boldsymbol{W}$存在重复奇异值（即$\sigma_i = \sigma_j$对某些$i\neq j$），则SVD分解不唯一，导数不存在。</p>
<p><strong>证明</strong>：假设$\sigma_i = \sigma_j$，对应的奇异向量为$\boldsymbol{u}_i, \boldsymbol{u}_j$和$\boldsymbol{v}_i, \boldsymbol{v}_j$。那么对任意正交矩阵$\boldsymbol{Q}\in\mathbb{R}^{2\times 2}$：
$$\boldsymbol{W} = \cdots + \sigma_i\boldsymbol{u}_i\boldsymbol{v}_i^{\top} + \sigma_j\boldsymbol{u}_j\boldsymbol{v}_j^{\top} + \cdots$$</p>
<p>可以替换为：
$$\boldsymbol{W} = \cdots + \sigma_i(\boldsymbol{u}_i, \boldsymbol{u}_j)\boldsymbol{Q}\begin{pmatrix}\boldsymbol{v}_i^{\top} \ \boldsymbol{v}_j^{\top}\end{pmatrix} + \cdots$$</p>
<p>这给出无穷多个等价的SVD分解。由于分解不唯一，导数无法定义。</p>
<p>从公式角度，$\boldsymbol{F}_{ij} = \frac{1}{\sigma_j^2 - \sigma_i^2}$在$\sigma_i = \sigma_j$时出现$0/0$型不定式，无法计算。</p>
<h4 id="42-weyl">4.2 扰动视角：Weyl不等式<a class="toc-link" href="#42-weyl" title="Permanent link">&para;</a></h4>
<p><strong>Weyl扰动定理</strong>：设$\boldsymbol{W}$和$\boldsymbol{W} + \boldsymbol{E}$的奇异值分别为$\sigma_1\geq\cdots\geq\sigma_n$和$\tilde{\sigma}_1\geq\cdots\geq\tilde{\sigma}_n$，则：
$$|\sigma_i - \tilde{\sigma}_i| \leq |\boldsymbol{E}|_2$$</p>
<p>更精确的Weyl不等式：
$$|\sigma_i(\boldsymbol{W}) - \sigma_i(\boldsymbol{W}+\boldsymbol{E})| \leq \sigma_1(\boldsymbol{E})$$</p>
<p><strong>推论</strong>：若$\sigma_i &gt; \sigma_{i+1}$且间隙$\delta = \sigma_i - \sigma_{i+1} &gt; 0$，那么当$|\boldsymbol{E}|_2 &lt; \delta/2$时，扰动后的第$i$个奇异值仍然与其他奇异值分离，保持良定性。</p>
<h4 id="43">4.3 部分导数的存在性<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>即使存在重复奇异值，某些部分导数仍然存在：</p>
<p><strong>命题</strong>：若仅要求$\nabla_{\boldsymbol{W}}\sigma_1$（最大奇异值的梯度），则只需$\sigma_1 &gt; \sigma_2$，而不需要所有奇异值两两不等。</p>
<p><strong>证明</strong>：$d\sigma_1 = \boldsymbol{u}<em ij="ij">1^{\top}(d\boldsymbol{W})\boldsymbol{v}_1$的推导只依赖于对角元素，不涉及$\boldsymbol{F}</em>$的计算。即使$\sigma_2 = \sigma_3$，也不影响$\sigma_1$的梯度。</p>
<p>类似地，前$k$个奇异值$\sigma_1,\ldots,\sigma_k$的梯度存在，只需这$k$个奇异值互异，而不管$\sigma_{k+1},\ldots,\sigma_n$是否重复。</p>
<h3 id="_15">五、隐函数定理的应用<a class="toc-link" href="#_15" title="Permanent link">&para;</a></h3>
<h4 id="51-svd">5.1 SVD作为隐函数<a class="toc-link" href="#51-svd" title="Permanent link">&para;</a></h4>
<p>将SVD视为隐函数关系：
$$\boldsymbol{G}(\boldsymbol{W}, \boldsymbol{U}, \boldsymbol{\Sigma}, \boldsymbol{V}) = \boldsymbol{W} - \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} = \boldsymbol{0}$$</p>
<p>加上正交约束：
$$\boldsymbol{H}_U(\boldsymbol{U}) = \boldsymbol{U}^{\top}\boldsymbol{U} - \boldsymbol{I} = \boldsymbol{0}$$
$$\boldsymbol{H}_V(\boldsymbol{V}) = \boldsymbol{V}^{\top}\boldsymbol{V} - \boldsymbol{I} = \boldsymbol{0}$$</p>
<p><strong>隐函数定理</strong>：若Jacobian矩阵$\frac{\partial(\boldsymbol{G}, \boldsymbol{H}_U, \boldsymbol{H}_V)}{\partial(\boldsymbol{U}, \boldsymbol{\Sigma}, \boldsymbol{V})}$可逆，则存在局部微分映射：
$$\boldsymbol{U} = \boldsymbol{U}(\boldsymbol{W}),\quad \boldsymbol{\Sigma} = \boldsymbol{\Sigma}(\boldsymbol{W}),\quad \boldsymbol{V} = \boldsymbol{V}(\boldsymbol{W})$$</p>
<h4 id="52">5.2 可逆性条件<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>Jacobian可逆等价于奇异值两两不等。这可以从Sylvester方程的可解性看出：</p>
<p>考虑线性方程$\boldsymbol{A}\boldsymbol{X} - \boldsymbol{X}\boldsymbol{B} = \boldsymbol{C}$，其解存在且唯一当且仅当$\boldsymbol{A}$和$\boldsymbol{B}$无公共特征值。</p>
<p>在我们的情况下，$\boldsymbol{A} = \text{diag}(\sigma_1^2,\ldots,\sigma_n^2)$，$\boldsymbol{B} = \text{diag}(\sigma_1^2,\ldots,\sigma_n^2)$，公共特征值意味着$\sigma_i^2 = \sigma_j^2$，即$\sigma_i = \sigma_j$（因奇异值非负）。</p>
<p>因此，<strong>奇异值两两不等</strong>是SVD可微的充要条件。</p>
<h3 id="_16">六、变分法推导<a class="toc-link" href="#_16" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 优化视角<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>从优化角度理解SVD：最大奇异值可以写成优化问题：
$$\sigma_1 = \max_{|\boldsymbol{u}|=1, |\boldsymbol{v}|=1} \boldsymbol{u}^{\top}\boldsymbol{W}\boldsymbol{v}$$</p>
<p><strong>变分法</strong>：对优化问题求导，设Lagrangian：
$$\mathcal{L}(\boldsymbol{u}, \boldsymbol{v}, \lambda, \mu) = \boldsymbol{u}^{\top}\boldsymbol{W}\boldsymbol{v} - \frac{\lambda}{2}(\boldsymbol{u}^{\top}\boldsymbol{u} - 1) - \frac{\mu}{2}(\boldsymbol{v}^{\top}\boldsymbol{v} - 1)$$</p>
<p>KKT条件：
$$\frac{\partial\mathcal{L}}{\partial\boldsymbol{u}} = \boldsymbol{W}\boldsymbol{v} - \lambda\boldsymbol{u} = \boldsymbol{0} \quad\Rightarrow\quad \boldsymbol{W}\boldsymbol{v} = \lambda\boldsymbol{u}$$
$$\frac{\partial\mathcal{L}}{\partial\boldsymbol{v}} = \boldsymbol{W}^{\top}\boldsymbol{u} - \mu\boldsymbol{v} = \boldsymbol{0} \quad\Rightarrow\quad \boldsymbol{W}^{\top}\boldsymbol{u} = \mu\boldsymbol{v}$$</p>
<p>由$\boldsymbol{W}\boldsymbol{v} = \lambda\boldsymbol{u}$得$\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{v} = \lambda\boldsymbol{W}^{\top}\boldsymbol{u} = \lambda\mu\boldsymbol{v}$，故$\lambda\mu = \sigma_1^2$，$\lambda = \mu = \sigma_1$。</p>
<h4 id="62-boldsymbolw">6.2 对$\boldsymbol{W}$的灵敏度分析<a class="toc-link" href="#62-boldsymbolw" title="Permanent link">&para;</a></h4>
<p>对KKT条件关于$\boldsymbol{W}$求微分：
$$d\boldsymbol{W}\cdot\boldsymbol{v} + \boldsymbol{W}\cdot d\boldsymbol{v} = (d\sigma_1)\boldsymbol{u} + \sigma_1(d\boldsymbol{u})$$
$$(d\boldsymbol{W})^{\top}\boldsymbol{u} + \boldsymbol{W}^{\top}(d\boldsymbol{u}) = (d\sigma_1)\boldsymbol{v} + \sigma_1(d\boldsymbol{v})$$</p>
<p>左乘第一式以$\boldsymbol{u}^{\top}$：
$$\boldsymbol{u}^{\top}(d\boldsymbol{W})\boldsymbol{v} + \boldsymbol{u}^{\top}\boldsymbol{W}(d\boldsymbol{v}) = d\sigma_1 + \sigma_1\boldsymbol{u}^{\top}(d\boldsymbol{u})$$</p>
<p>由正交约束$\boldsymbol{u}^{\top}(d\boldsymbol{u}) = 0$和$\boldsymbol{u}^{\top}\boldsymbol{W} = \sigma_1\boldsymbol{v}^{\top}$：
$$\boldsymbol{u}^{\top}(d\boldsymbol{W})\boldsymbol{v} + \sigma_1\boldsymbol{v}^{\top}(d\boldsymbol{v}) = d\sigma_1$$</p>
<p>又$\boldsymbol{v}^{\top}(d\boldsymbol{v}) = 0$，故：
$$d\sigma_1 = \boldsymbol{u}^{\top}(d\boldsymbol{W})\boldsymbol{v}$$</p>
<p>这再次验证了奇异值微分公式！</p>
<h3 id="_17">七、数值稳定性与实现算法<a class="toc-link" href="#_17" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 病态问题：奇异值接近的情况<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p>当$\sigma_i\approx\sigma_j$时，$\boldsymbol{F}_{ij} = \frac{1}{\sigma_j^2 - \sigma_i^2}$会变得极大，导致数值不稳定。</p>
<p><strong>问题根源</strong>：$\boldsymbol{F}<em ij="ij">{ij}$的大小决定了梯度的magnitude。定义条件数：
$$\kappa</em>$$} = \frac{\max(\sigma_i, \sigma_j)}{|\sigma_i - \sigma_j|</p>
<p>当$\kappa_{ij}\to\infty$时，问题变为病态。</p>
<h4 id="72-taylor">7.2 Taylor近似方法<a class="toc-link" href="#72-taylor" title="Permanent link">&para;</a></h4>
<p>对于$\sigma_i &gt; \sigma_j$，可以进行Taylor展开：
$$\frac{1}{\sigma_i^2 - \sigma_j^2} = \frac{1}{\sigma_i^2}\cdot\frac{1}{1 - (\sigma_j/\sigma_i)^2}$$</p>
<p>设$r = \sigma_j/\sigma_i &lt; 1$，则：
$$\frac{1}{1 - r^2} = 1 + r^2 + r^4 + \cdots = \sum_{k=0}^{\infty}r^{2k}$$</p>
<p>截断至$N$阶：
$$\frac{1}{\sigma_i^2 - \sigma_j^2}\approx\frac{1}{\sigma_i^2}\sum_{k=0}^{N}\left(\frac{\sigma_j}{\sigma_i}\right)^{2k}$$</p>
<p><strong>优点</strong>：当$r$接近1时，级数每项大小相近，不会出现单个巨大的项。</p>
<h4 id="73-pade">7.3 Padé近似<a class="toc-link" href="#73-pade" title="Permanent link">&para;</a></h4>
<p>Padé近似是有理函数逼近：
$$\frac{1}{1-r^2}\approx\frac{P_m(r^2)}{Q_n(r^2)}$$</p>
<p>其中$P_m$和$Q_n$是多项式。例如$(m, n) = (1, 1)$时：
$$\frac{1}{1-r^2}\approx\frac{1 + \alpha r^2}{1 + \beta r^2}$$</p>
<p>通过匹配Taylor系数确定$\alpha, \beta$。Padé近似在整个区间上比Taylor多项式更精确。</p>
<h4 id="74">7.4 截断与正则化<a class="toc-link" href="#74" title="Permanent link">&para;</a></h4>
<p>最简单的方法是截断$\boldsymbol{F}<em ij="ij">{ij}$：
$$\tilde{\boldsymbol{F}}</em>
\boldsymbol{F}_{ij}, &amp; |\sigma_i^2 - \sigma_j^2| &gt; \epsilon \
0, &amp; \text{otherwise}
\end{cases}$$} = \begin{cases</p>
<p>或使用软截断（Huber型）：
$$\tilde{\boldsymbol{F}}_{ij} = \frac{\sigma_j^2 - \sigma_i^2}{(\sigma_j^2 - \sigma_i^2)^2 + \epsilon^2}$$</p>
<p>这在$\sigma_i\approx\sigma_j$时提供平滑的过渡。</p>
<h4 id="75">7.5 幂迭代方法<a class="toc-link" href="#75" title="Permanent link">&para;</a></h4>
<p>对于最大奇异值$\sigma_1$及其奇异向量$\boldsymbol{u}_1, \boldsymbol{v}_1$，可以用幂迭代避免完整SVD：</p>
<p><strong>算法</strong>：
1. 初始化：随机$\boldsymbol{v}^{(0)}$，归一化
2. 迭代：
   $$\boldsymbol{u}^{(k)} = \frac{\boldsymbol{W}\boldsymbol{v}^{(k-1)}}{|\boldsymbol{W}\boldsymbol{v}^{(k-1)}|}$$
   $$\boldsymbol{v}^{(k)} = \frac{\boldsymbol{W}^{\top}\boldsymbol{u}^{(k)}}{|\boldsymbol{W}^{\top}\boldsymbol{u}^{(k)}|}$$
3. 收敛后：$\sigma_1 = |\boldsymbol{W}\boldsymbol{v}^{(\infty)}|$</p>
<p><strong>梯度计算</strong>：将幂迭代嵌入计算图，自动微分自然处理梯度传播。这避免了显式计算$\boldsymbol{F}_{ij}$。</p>
<p><strong>优点</strong>：
- 只计算需要的奇异值/向量
- 数值稳定（避免近似奇异值的除法）
- 可微且可嵌入端到端训练</p>
<h3 id="_18">八、深度学习中的应用<a class="toc-link" href="#_18" title="Permanent link">&para;</a></h3>
<h4 id="81-spectral-normalization">8.1 谱归一化（Spectral Normalization）<a class="toc-link" href="#81-spectral-normalization" title="Permanent link">&para;</a></h4>
<p>在GAN中，判别器需要满足Lipschitz连续性以稳定训练。对权重矩阵$\boldsymbol{W}$：
$$\bar{\boldsymbol{W}} = \frac{\boldsymbol{W}}{\sigma_1(\boldsymbol{W})}$$</p>
<p>则$|\bar{\boldsymbol{W}}|_2 = 1$，保证$|f(\boldsymbol{x}) - f(\boldsymbol{y})| \leq |\boldsymbol{x} - \boldsymbol{y}|$。</p>
<p><strong>反向传播</strong>：
$$\frac{\partial\mathcal{L}}{\partial\boldsymbol{W}} = \frac{1}{\sigma_1}\frac{\partial\mathcal{L}}{\partial\bar{\boldsymbol{W}}} - \frac{1}{\sigma_1^2}\left(\frac{\partial\mathcal{L}}{\partial\bar{\boldsymbol{W}}}:\boldsymbol{W}\right)\boldsymbol{u}_1\boldsymbol{v}_1^{\top}$$</p>
<p>其中第二项是对归一化因子的修正。</p>
<p><strong>实践技巧</strong>：
- 使用幂迭代估计$\sigma_1$（通常1-2次迭代足够）
- 在训练过程中复用上一步的$\boldsymbol{u}_1, \boldsymbol{v}_1$作为初始值
- 仅在判别器中应用，生成器不需要</p>
<h4 id="82">8.2 低秩正则化<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>在矩阵分解、推荐系统中，常用核范数正则化：
$$\min_{\boldsymbol{W}} \mathcal{L}(\boldsymbol{W}) + \lambda|\boldsymbol{W}|_*$$</p>
<p>梯度为：
$$\nabla_{\boldsymbol{W}}\mathcal{L} + \lambda\boldsymbol{U}\boldsymbol{V}^{\top}$$</p>
<p>这促进$\boldsymbol{W}$的低秩结构。</p>
<h4 id="83-orthogonal-layers">8.3 正交化层（Orthogonal Layers）<a class="toc-link" href="#83-orthogonal-layers" title="Permanent link">&para;</a></h4>
<p>在某些架构中（如Transformer的attention），希望保持权重正交性以避免梯度消失/爆炸。</p>
<p>可以参数化为：
$$\boldsymbol{W} = \boldsymbol{U}\boldsymbol{V}^{\top}$$</p>
<p>其中$\boldsymbol{U}, \boldsymbol{V}$是正交矩阵。通过SVD投影实现：
$$\boldsymbol{W}<em _text_raw="\text{raw">{\text{proj}} = \boldsymbol{U}\boldsymbol{V}^{\top},\quad \text{where } \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} = \text{SVD}(\boldsymbol{W}</em>)$$}</p>
<p>梯度计算需要用到本文推导的公式。</p>
<h3 id="_19">九、与特征值分解导数的对比<a class="toc-link" href="#_19" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 对称矩阵的特殊情况<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p>对于对称正定矩阵$\boldsymbol{W} = \boldsymbol{W}^{\top}$，SVD退化为特征值分解：
$$\boldsymbol{W} = \boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}$$</p>
<p>其中$\boldsymbol{\Lambda} = \text{diag}(\lambda_1,\ldots,\lambda_n)$是特征值对角阵。</p>
<p><strong>微分方程</strong>：
$$\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V} = \boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Lambda} + d\boldsymbol{\Lambda} + \boldsymbol{\Lambda}(d\boldsymbol{V})^{\top}\boldsymbol{V}$$</p>
<p>利用反对称性：
$$\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V} = \boldsymbol{V}^{\top}(d\boldsymbol{V})\boldsymbol{\Lambda} + d\boldsymbol{\Lambda} - \boldsymbol{\Lambda}\boldsymbol{V}^{\top}(d\boldsymbol{V})$$</p>
<p><strong>对角部分</strong>：
$$d\lambda_i = \boldsymbol{v}_i^{\top}(d\boldsymbol{W})\boldsymbol{v}_i$$</p>
<p><strong>非对角部分</strong>（$i\neq j$）：
$$[\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V}]<em ij="ij">{ij} = [\boldsymbol{V}^{\top}(d\boldsymbol{V})]</em>(\lambda_j - \lambda_i)$$</p>
<p>解得：
$$[\boldsymbol{V}^{\top}(d\boldsymbol{V})]<em ij="ij">{ij} = \frac{[\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V}]</em>$$}}{\lambda_j - \lambda_i</p>
<p>定义$\boldsymbol{K}<em ii="ii">{ij} = \frac{1}{\lambda_j - \lambda_i}$（$i\neq j$），$\boldsymbol{K}</em> = 0$，则：
$$d\boldsymbol{V} = \boldsymbol{V}(\boldsymbol{K}^{\top}\otimes(\boldsymbol{V}^{\top}(d\boldsymbol{W})\boldsymbol{V}))$$</p>
<h4 id="92">9.2 关键区别<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>特征值分解（对称矩阵）</th>
<th>SVD（一般矩阵）</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\boldsymbol{W} = \boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{\top}$</td>
<td>$\boldsymbol{W} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$</td>
</tr>
<tr>
<td>单个正交矩阵$\boldsymbol{V}$</td>
<td>两个正交矩阵$\boldsymbol{U}, \boldsymbol{V}$</td>
</tr>
<tr>
<td>$\boldsymbol{K}_{ij} = \frac{1}{\lambda_j - \lambda_i}$</td>
<td>$\boldsymbol{F}_{ij} = \frac{1}{\sigma_j^2 - \sigma_i^2}$</td>
</tr>
<tr>
<td>要求$\lambda_i\neq\lambda_j$</td>
<td>要求$\sigma_i\neq\sigma_j$</td>
</tr>
<tr>
<td>受约束$\boldsymbol{W} = \boldsymbol{W}^{\top}$</td>
<td>无约束</td>
</tr>
</tbody>
</table>
<p><strong>注意</strong>：在对称情况下，将$\boldsymbol{U} = \boldsymbol{V}$, $\sigma_i = \lambda_i$直接代入SVD公式会<strong>导致错误</strong>，因为：
1. 对$\boldsymbol{V}$求导时，实际上$\boldsymbol{U}$和$\boldsymbol{V}$都在变，导致重复计算
2. 对称约束$\boldsymbol{W} = \boldsymbol{W}^{\top}$需要额外的对称化操作</p>
<p>正确做法是从头推导对称情况，或在最终梯度上应用对称化$\text{sym}(\cdot) = \frac{1}{2}(\cdot + \cdot^{\top})$。</p>
<h3 id="_20">十、理论深化：扰动展开与高阶导数<a class="toc-link" href="#_20" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 一阶扰动理论<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p>考虑扰动$\boldsymbol{W}(\epsilon) = \boldsymbol{W}_0 + \epsilon\boldsymbol{E}$，其SVD为$\boldsymbol{U}(\epsilon)\boldsymbol{\Sigma}(\epsilon)\boldsymbol{V}(\epsilon)^{\top}$。</p>
<p><strong>Taylor展开</strong>：
$$\sigma_i(\epsilon) = \sigma_i(0) + \epsilon\sigma_i'(0) + O(\epsilon^2)$$
$$\boldsymbol{u}_i(\epsilon) = \boldsymbol{u}_i(0) + \epsilon\boldsymbol{u}_i'(0) + O(\epsilon^2)$$
$$\boldsymbol{v}_i(\epsilon) = \boldsymbol{v}_i(0) + \epsilon\boldsymbol{v}_i'(0) + O(\epsilon^2)$$</p>
<p>从$\boldsymbol{W}(\epsilon) = \boldsymbol{U}(\epsilon)\boldsymbol{\Sigma}(\epsilon)\boldsymbol{V}(\epsilon)^{\top}$：
$$\boldsymbol{W}_0 + \epsilon\boldsymbol{E} = (\boldsymbol{U}_0 + \epsilon\boldsymbol{U}_0')(\boldsymbol{\Sigma}_0 + \epsilon\boldsymbol{\Sigma}_0')(\boldsymbol{V}_0 + \epsilon\boldsymbol{V}_0')^{\top} + O(\epsilon^2)$$</p>
<p>展开并比较$\epsilon$的系数：
$$\boldsymbol{E} = \boldsymbol{U}_0'\boldsymbol{\Sigma}_0\boldsymbol{V}_0^{\top} + \boldsymbol{U}_0\boldsymbol{\Sigma}_0'\boldsymbol{V}_0^{\top} + \boldsymbol{U}_0\boldsymbol{\Sigma}_0(\boldsymbol{V}_0')^{\top}$$</p>
<p>这与微分公式$d\boldsymbol{W} = (d\boldsymbol{U})\boldsymbol{\Sigma}\boldsymbol{V}^{\top} + \cdots$完全一致（识别$d\boldsymbol{U} = \epsilon\boldsymbol{U}_0'$等）。</p>
<h4 id="102">10.2 二阶导数<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p>二阶导数$\frac{\partial^2\sigma_i}{\partial W_{jk}\partial W_{lm}}$涉及奇异向量的导数，计算复杂。</p>
<p>从$\frac{\partial\sigma_i}{\partial W_{jk}} = u_{ji}v_{ki}$求导：
$$\frac{\partial^2\sigma_i}{\partial W_{lm}\partial W_{jk}} = \frac{\partial u_{ji}}{\partial W_{lm}}v_{ki} + u_{ji}\frac{\partial v_{ki}}{\partial W_{lm}}$$</p>
<p>利用本文推导的$\frac{\partial\boldsymbol{u}_i}{\partial\boldsymbol{W}}$和$\frac{\partial\boldsymbol{v}_i}{\partial\boldsymbol{W}}$（以4阶张量形式），可以计算出二阶导数。</p>
<p><strong>应用</strong>：二阶优化方法（如Newton法）需要Hessian矩阵。对于包含SVD的目标函数，可以用二阶导数构造精确的Hessian。</p>
<h3 id="_21">十一、数值验证与测试<a class="toc-link" href="#_21" title="Permanent link">&para;</a></h3>
<h4 id="111">11.1 有限差分检验<a class="toc-link" href="#111" title="Permanent link">&para;</a></h4>
<p>验证梯度公式的标准方法是有限差分：
$$\frac{\partial f}{\partial W_{ij}}\approx\frac{f(\boldsymbol{W} + \epsilon\boldsymbol{E}<em ij="ij">{ij}) - f(\boldsymbol{W} - \epsilon\boldsymbol{E}</em>$$})}{2\epsilon</p>
<p>其中$\boldsymbol{E}_{ij}$是第$(i,j)$位置为1其余为0的矩阵。</p>
<p><strong>测试案例</strong>：</p>
<pre class="highlight"><code class="language-python">import numpy as np

def test_svd_gradient():
    np.random.seed(42)
    n = 5
    W = np.random.randn(n, n)
    U, S, Vt = np.linalg.svd(W)

    # 解析梯度：最大奇异值
    grad_analytical = np.outer(U[:, 0], Vt[0, :])

    # 数值梯度
    epsilon = 1e-7
    grad_numerical = np.zeros_like(W)
    for i in range(n):
        for j in range(n):
            E = np.zeros_like(W)
            E[i, j] = 1
            s1_plus = np.linalg.svd(W + epsilon * E, compute_uv=False)[0]
            s1_minus = np.linalg.svd(W - epsilon * E, compute_uv=False)[0]
            grad_numerical[i, j] = (s1_plus - s1_minus) / (2 * epsilon)

    # 比较
    error = np.linalg.norm(grad_analytical - grad_numerical)
    print(f&quot;Gradient error: {error}&quot;)
    assert error &lt; 1e-5
</code></pre>

<h4 id="112">11.2 自动微分对比<a class="toc-link" href="#112" title="Permanent link">&para;</a></h4>
<p>现代深度学习框架（PyTorch, JAX）提供自动微分。可以对比手动实现与框架自动计算的梯度：</p>
<pre class="highlight"><code class="language-python">import torch

def svd_spectral_norm_manual(W):
    &quot;&quot;&quot;手动实现谱归一化的梯度&quot;&quot;&quot;
    U, S, V = torch.svd(W)
    sigma1 = S[0]
    u1 = U[:, 0:1]
    v1 = V[:, 0:1]
    W_normalized = W / sigma1
    return W_normalized, u1, v1, sigma1

# 对比自动微分
W = torch.randn(5, 5, requires_grad=True)
W_norm_manual, u1, v1, s1 = svd_spectral_norm_manual(W)
loss_manual = W_norm_manual.sum()
loss_manual.backward()
grad_manual = W.grad.clone()

# 使用自动微分
W.grad.zero_()
W_norm_auto = W / torch.norm(W, p=2)
loss_auto = W_norm_auto.sum()
loss_auto.backward()
grad_auto = W.grad

print(&quot;Gradient difference:&quot;, torch.norm(grad_manual - grad_auto))
</code></pre>

<h3 id="_22">十二、总结与展望<a class="toc-link" href="#_22" title="Permanent link">&para;</a></h3>
<h4 id="121">12.1 核心要点回顾<a class="toc-link" href="#121" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>可导条件</strong>：SVD可导当且仅当所有非零奇异值两两不等</li>
<li><strong>奇异值梯度</strong>：$\nabla_{\boldsymbol{W}}\sigma_i = \boldsymbol{u}_i\boldsymbol{v}_i^{\top}$</li>
<li><strong>奇异向量梯度</strong>：涉及矩阵$\boldsymbol{F}_{ij} = \frac{1}{\sigma_j^2 - \sigma_i^2}$的Hadamard积</li>
<li><strong>数值稳定性</strong>：接近奇异值导致梯度爆炸，需用Taylor/Padé近似或幂迭代</li>
<li><strong>深度学习应用</strong>：谱归一化、低秩正则化、正交化层</li>
</ol>
<h4 id="122">12.2 理论意义<a class="toc-link" href="#122" title="Permanent link">&para;</a></h4>
<p>SVD求导展示了<strong>微分几何</strong>与<strong>数值线性代数</strong>的深刻联系：
- SVD定义了矩阵流形上的坐标系
- 奇异值/向量的变化受Riemann度量约束
- 梯度公式反映了流形的几何结构</p>
<h4 id="123">12.3 开放问题<a class="toc-link" href="#123" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>自适应正则化</strong>：如何根据奇异值间隙自动调整$\boldsymbol{F}_{ij}$的正则化强度？</li>
<li><strong>高阶优化</strong>：如何高效计算SVD的Hessian矩阵用于二阶优化？</li>
<li><strong>随机SVD的梯度</strong>：对于大规模矩阵，随机SVD算法的梯度如何计算？</li>
<li><strong>非凸优化</strong>：含SVD项的非凸优化问题的全局收敛性如何保证？</li>
</ol>
<h4 id="124">12.4 实践建议<a class="toc-link" href="#124" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>优先使用框架自动微分</strong>：除非对性能有极致要求，否则让PyTorch/JAX处理SVD梯度</li>
<li><strong>监控条件数</strong>：训练过程中跟踪$\min_{i\neq j}|\sigma_i - \sigma_j|$，过小时调整正则化</li>
<li><strong>幂迭代足够用</strong>：大多数情况下，1-2次幂迭代即可获得足够精确的$\sigma_1, \boldsymbol{u}_1, \boldsymbol{v}_1$</li>
<li><strong>避免完整SVD</strong>：如果只需要少数奇异值，使用Lanczos或Arnoldi迭代而非完整分解</li>
</ul>
<p>通过本节的详细推导，我们从多个角度理解了SVD导数的本质，掌握了理论基础、数值算法和实际应用。这些知识不仅在深度学习中重要，在优化、信号处理、控制理论等领域也有广泛应用。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="矩阵的有效秩effective-rank.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#171 矩阵的有效秩（Effective Rank）</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="一道概率不等式盯着它到显然成立为止.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#173 一道概率不等式：盯着它到显然成立为止！</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#svd">SVD的导数</a><ul>
<li><a href="#_1">推导基础</a></li>
<li><a href="#_2">奇异值</a></li>
<li><a href="#_3">奇异向量</a></li>
<li><a href="#_4">梯度（一）</a></li>
<li><a href="#_5">梯度（二）</a></li>
<li><a href="#_6">梯度（三）</a></li>
<li><a href="#_7">梯度（四）</a></li>
<li><a href="#_8">数值问题</a></li>
<li><a href="#_9">一般结果</a></li>
<li><a href="#_10">文章小结</a></li>
<li><a href="#_11">公式推导与注释</a><ul>
<li><a href="#svd_1">一、SVD分解的微分理论基础</a></li>
<li><a href="#_12">二、奇异值梯度的深入分析</a></li>
<li><a href="#_13">三、奇异向量梯度的求解</a></li>
<li><a href="#_14">四、退化情况：重复奇异值的处理</a></li>
<li><a href="#_15">五、隐函数定理的应用</a></li>
<li><a href="#_16">六、变分法推导</a></li>
<li><a href="#_17">七、数值稳定性与实现算法</a></li>
<li><a href="#_18">八、深度学习中的应用</a></li>
<li><a href="#_19">九、与特征值分解导数的对比</a></li>
<li><a href="#_20">十、理论深化：扰动展开与高阶导数</a></li>
<li><a href="#_21">十一、数值验证与测试</a></li>
<li><a href="#_22">十二、总结与展望</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>