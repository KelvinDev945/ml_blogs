<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>随机分词浅探：从Viterbi Decoding到Viterbi Sampling | ML & Math Blog Posts</title>
    <meta name="description" content="随机分词浅探：从Viterbi Decoding到Viterbi Sampling&para;
原文链接: https://spaces.ac.cn/archives/9768
发布日期: 

上一篇文章《大词表语言模型在续写任务上的一个问题及对策》发布后，很快就有读者指出可以在训练阶段引入带有随机性的分词结果来解决同样的问题，并且已经有论文和实现。经过进一步查阅学习，笔者发现这是一个名为Subw...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=概率">概率</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #109 随机分词浅探：从Viterbi Decoding到Viterbi Sampling
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#109</span>
                随机分词浅探：从Viterbi Decoding到Viterbi Sampling
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2023-09-16</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=概率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 概率</span>
                </a>
                
                <a href="../index.html?tags=随机" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 随机</span>
                </a>
                
                <a href="../index.html?tags=分词" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 分词</span>
                </a>
                
                <a href="../index.html?tags=新词发现" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 新词发现</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="viterbi-decodingviterbi-sampling">随机分词浅探：从Viterbi Decoding到Viterbi Sampling<a class="toc-link" href="#viterbi-decodingviterbi-sampling" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9768">https://spaces.ac.cn/archives/9768</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>上一篇文章<a href="/archives/9762">《大词表语言模型在续写任务上的一个问题及对策》</a>发布后，很快就有读者指出可以在训练阶段引入带有随机性的分词结果来解决同样的问题，并且已经有论文和实现。经过进一步查阅学习，笔者发现这是一个名为<a href="https://papers.cool/arxiv/1804.10959">Subword Regularization</a>的技巧，最早应用在NMT（机器翻译）中，目前SentencePiece也有相应的实现。看起来这个技巧确实能缓解前述问题，甚至有助于增强语言模型的容错能力，所以就有了将它加进去<a href="/archives/9752">BytePiece</a>的想法。</p>
<p>那么问题来了，如何将确定性分词改为随机性分词呢？BytePiece是基于Unigram模型的，它通过Viterbi算法找最大概率的分词方案，既然有概率，是否就可以自然地导出随机采样？本文来讨论这个问题，并分享自己的解决方案。</p>
<h2 id="_1">要点分析<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>现阶段，Unigram分词是直接输出最大概率的切分方案，通常这是一个确定性的输出。具体来说，假设$\boldsymbol{w}=(w_1,w_2,\cdots,w_k)$代表一个切分方案，对应的打分为$P(\boldsymbol{w})=p(w_1)p(w_2)\cdots p(w_k)$，$\Omega(S)$代表句子$S$所有可能的切分方案的集合，那么分词算法可以描述为<br />
\begin{equation}\boldsymbol{w}^* = \mathop{\text{argmax}}_{\boldsymbol{w}\in \Omega(S)}P(\boldsymbol{w})\end{equation}<br />
这可以通过Viterbi算法在线性时间内来完成，所以这个过程我们也称之为“Viterbi Decoding”。看起来，Unigram模型天然带有概率，所以似乎并不难将它改为依概率采样的形式，但细想之下才发现这并非一个平凡的问题，有很多细节上的困难需要克服。</p>
<p>笔者设想是模仿自回归语言模型设计一个递归采样流程，但这里最困难的地方是如何尽量保持原来的候选切分方案的排序不变，或者就算不能保持所有的排序不变，也至少满足最大概率不变，即Viterbi解码的最大概率路径$\boldsymbol{w}^*$应该对应所设计的递归采样算法的最大概率采样结果。由于所有切分方案$\Omega(S)$构成一个有向无环图（DAG，Directed Acyclic Graph），笔者一开始以为直接在有向无环图上随机游走是一个可行方案，但再思考后发现很难设计适当的转移概率来保证最大概率路径不变（因为同一起点的不同边不是平权的，不能简单按照边的频率为权重做采样）。</p>
<h2 id="_2">已有方案<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>由于一时半会没有新想法，所以笔者决定去翻翻“参考答案”——看看Subword Regularization的原始论文<a href="https://papers.cool/arxiv/1804.10959">《Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates》</a>是怎么做的。</p>
<p>然而，这个“标准答案”却让笔者有点哭笑不得。原来Subword Regularization的思路非常简单直接：先搜索出$P(\boldsymbol{w})$最大的$n$个分词方案$\boldsymbol{w}^<em>_1,\boldsymbol{w}^</em><em j="1">2,\cdots,\boldsymbol{w}^<em>_n$（$n$-best segmentations），然后构建如下分布<br />
\begin{equation}p_i = \frac{P(\boldsymbol{w}^</em>_i)^{\alpha}}{\sum\limits</em>}^n P(\boldsymbol{w}^*_j)^{\alpha}}\end{equation<br />
对这$n$个方案进行依概率采样，其中$\alpha &gt; 0$是一个超参数。该算法已经集成在SentencePiece中，读者可以自行测试（使用方法参考<a href="https://github.com/google/sentencepiece/tree/master#subword-regularization-and-bpe-dropout">这里</a>）。</p>
<p>问题是，“简单直接”不代表着“高效”，尽管搜索top-$n$个分词方案最优方案的复杂度也是线性的（有兴趣的读者可以自行找找N-best Viterbi的资料)，但明显比只找top1的Viterbi Decoding要大很多（理论上是$n$倍复杂度），所以直接的后果是开启了随机采样后，会比确定性的分词要慢很多，所以这并非是笔者心中的理想采样方法。</p>
<h2 id="_3">个人思路<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>思路再度陷入了僵局。一筹莫展之际，笔者决定把思路再捋一捋：我们的目标是想要找到复杂度类似Viterbi Decoding的随机采样算法，既然如此，Viterbi Decoding本身应该是一个不错的突破点。于是，笔者再次翻开了分词代码，<a href="https://github.com/bojone/bytepiece/blob/b65716b76938b3ac4124661a3367fc1c270373fa/bytepiece/faster.pyx">当时的分词函数</a>长这个样：</p>
<div class="highlight"><pre><span></span><code><span class="s s-Atom">def</span> <span class="k">_</span><span class="nf">tokenize</span><span class="p">(</span><span class="s s-Atom">self</span><span class="p">,</span> <span class="s s-Atom">bytes</span> <span class="s s-Atom">text</span><span class="p">)</span><span class="o">:</span>
    <span class="s s-Atom">cdef</span> <span class="s s-Atom">int</span> <span class="s s-Atom">e</span><span class="p">,</span> <span class="s s-Atom">k</span><span class="p">,</span> <span class="s s-Atom">s</span>
    <span class="s s-Atom">cdef</span> <span class="s s-Atom">double</span> <span class="s s-Atom">v</span><span class="p">,</span> <span class="s s-Atom">score</span>
    <span class="s s-Atom">cdef</span> <span class="s s-Atom">list</span> <span class="s s-Atom">routes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="nv">None</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[(</span><span class="o">-</span><span class="nv">INFINITY</span><span class="p">,</span> <span class="nv">None</span><span class="p">)</span> <span class="s s-Atom">for</span> <span class="k">_</span> <span class="s s-Atom">in</span> <span class="s s-Atom">text</span><span class="p">]</span>
    <span class="s s-Atom">cdef</span> <span class="s s-Atom">list</span> <span class="s s-Atom">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="s s-Atom">for</span> <span class="s s-Atom">e</span><span class="p">,</span> <span class="p">(</span><span class="s s-Atom">k</span><span class="p">,</span> <span class="s s-Atom">v</span><span class="p">)</span> <span class="s s-Atom">in</span> <span class="s s-Atom">self</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">automaton</span><span class="p">.</span><span class="nf">iter</span><span class="p">(</span><span class="s s-Atom">text</span><span class="p">)</span><span class="o">:</span>
        <span class="s s-Atom">s</span><span class="p">,</span> <span class="s s-Atom">e</span> <span class="o">=</span> <span class="s s-Atom">e</span> <span class="o">-</span> <span class="s s-Atom">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s s-Atom">e</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="s s-Atom">score</span> <span class="o">=</span> <span class="s s-Atom">routes</span><span class="p">[</span><span class="s s-Atom">s</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s s-Atom">v</span>
        <span class="s s-Atom">if</span> <span class="s s-Atom">score</span> <span class="o">&gt;</span> <span class="s s-Atom">routes</span><span class="p">[</span><span class="s s-Atom">e</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">:</span>
            <span class="s s-Atom">routes</span><span class="p">[</span><span class="s s-Atom">e</span><span class="p">]</span> <span class="o">=</span> <span class="s s-Atom">score</span><span class="p">,</span> <span class="s s-Atom">s</span>
    <span class="s s-Atom">while</span> <span class="s s-Atom">text</span><span class="p">:</span>
        <span class="s s-Atom">s</span> <span class="o">=</span> <span class="s s-Atom">routes</span><span class="p">[</span><span class="s s-Atom">e</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        <span class="s s-Atom">tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="s s-Atom">text</span><span class="p">[</span><span class="s s-Atom">s</span><span class="p">:</span><span class="s s-Atom">e</span><span class="p">])</span>
        <span class="s s-Atom">text</span><span class="p">,</span> <span class="s s-Atom">e</span> <span class="o">=</span> <span class="s s-Atom">text</span><span class="p">[</span><span class="o">:</span><span class="s s-Atom">s</span><span class="p">],</span> <span class="s s-Atom">s</span>
    <span class="s s-Atom">return</span> <span class="s s-Atom">tokens</span><span class="p">[</span><span class="o">::-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

<p>反复读了几遍，总算有了灵感：Viterbi Decoding的关键是<code>if score &gt; routes[e][0]:</code>这一句，它代表保留截止到当前位置的最优切分方案，其中<code>score</code>是新切分方案分数（概率对数），<code>routes[e][0]</code>是历史最优分数，如果新方案更优则覆盖。这让笔者联想到了<a href="/archives/8084">MCMC算法</a>的接受率设计，如果在这里引入随机采样，不就可以将分词结果随机化了？</p>
<p>我们用$r\in \{1, 0\}$表示接受/拒绝新方案，由于这一步只是一个二元选择，所以将它概率化也非常简单：<br />
\begin{equation}<br />
r_i = \left\{\begin{aligned}&amp;\,1\,, \,\, s_i &gt; s_{i-1} \\<br />
&amp;\,0\,, \,\, \text{else}\end{aligned}\right.\qquad\longrightarrow\qquad<br />
r_i = \left\{\begin{aligned}&amp;\,1\,, \,\, \varepsilon &lt; \sigma(\alpha(s_i - s_{i-1})) \\<br />
&amp;\,0\,, \,\, \text{else}\end{aligned}\right.<br />
\end{equation}<br />
这里$\varepsilon\sim U[0,1]$是均匀随机数，$\alpha &gt; 0$是超参数，$\sigma(t)=1/(1+e^{-t})$是Sigmoid函数，$s_i,s_{i-1}$分别是新旧方案的得分（概率对数）。不难发现，左端的确定性采样对应$\alpha\to\infty$的随机性采样。</p>
<p>这样，在Viterbi解码的基础上我们得到了一个非常自然、非常轻量级的随机采样算法，这里称之为“Viterbi Sampling”，实现它只需要将<code>if score &gt; routes[e][0]:</code>这一判据换成带随机数的版本。由于Sigmoid函数的单调性，当$s_i &gt; s_{i-1}$时，它自然会给新方案分配更大的概率，所以很明显原来的的最大概率切分在Viterbi Sampling之下也是最大概率结果，并且当$s_i - s_{i-1}$越大，$\sigma(\alpha(s_i - s_{i-1}))$也越大，这意味着原本得分越大的方案被采样到的概率也越高，一定程度上保持了切分方案的排序不变（尽管还没有证明一定严格保序，但从应用角度看，近似保序就够了）。</p>
<h2 id="_4">简单测试<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>从0.4.0版本开始，Viterbi Sampling就内置在BytePiece的分词函数中，只需要在<code>tokenizer.tokenize</code>或者<code>tokenizer.encode</code>时加入大于0的alpha参数，结果就是随机的：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">bytepiece</span>
<span class="k">assert</span> <span class="n">bytepiece</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;=</span> <span class="s1">&#39;0.4.0&#39;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">bytepiece</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span><span class="s1">&#39;bytepiece_160k.model&#39;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天天气不错&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>  <span class="c1"># alpha默认值为-1，alpha≤0 都代表确定性分词</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>

<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6&#39;, b&#39;\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8&#39;, b&#39;\x8d&#39;, b&#39;\xe9\x94&#39;, b&#39;\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9&#39;, b&#39;\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb&#39;, b&#39;\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9&#39;, b&#39;\xe6\xb0\x94\xe4\xb8\x8d&#39;, b&#39;\xe9\x94&#39;, b&#39;\x99&#39;]</span>
</code></pre></div>

<p>下面对比一下SentencePiece的Subword Regularization和BytePiece的Viterbi Sampling的速度（随机性分词时都设$\alpha=0.1$）：<br />
\begin{array}{c|cc}<br />
\hline<br />
&amp; \text{确定性分词} &amp; \text{随机性分词} &amp; \\<br />
\hline<br />
\text{SP-BPE} &amp; \text{1.36M bytes/sec} &amp; \text{1.25M bytes/sec} \\<br />
\text{SP-Unigram} &amp; \text{5.65M bytes/sec} &amp; \text{1.28M bytes/sec} \\<br />
\text{BytePiece} &amp; \text{1.95M bytes/sec} &amp; \text{1.36M bytes/sec}\\<br />
\hline<br />
\end{array}<br />
可以看到，Subword Regularization（“SP-Unigram”这一行）开启之后，分词速度不到原来的1/4，这表明Subword Regularization的采样算法是相当低效的。相比之下，本文提出的Viterbi Sampling只下降了30%左右，效率显然更高，下降的部分在于随机数的生成和Sigmoid函数的计算，如果能进一步优化这两部分，速度还能进一步提升。至于BPE模型，它的随机分词叫做<a href="https://papers.cool/arxiv/1910.13267">BPE Dropout</a>，这是专属于BPE模型的方法，有兴趣的读者自行了解，这里就不介绍了。</p>
<h2 id="_5">文章小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文主要探讨了将Unigram分词模型的确定性分词改为随机性分词的策略。尽管已有名为“Subword Regularization”的方法可以实现这一目标，但其效率相对较低。为此，笔者提出了一种更高效的采样算法Viterbi Sampling，它仅需对确定性的Viterbi Decoding进行简单的修改，从而基本保持了原有的效率。实验证明，新的算法采样速度明显超越了Subword Regularization。相应的实现已经内置在BytePiece最新版中。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9768">https://spaces.ac.cn/archives/9768</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 16, 2023). 《随机分词浅探：从Viterbi Decoding到Viterbi Sampling 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9768">https://spaces.ac.cn/archives/9768</a></p>
<p>@online{kexuefm-9768,<br />
title={随机分词浅探：从Viterbi Decoding到Viterbi Sampling},<br />
author={苏剑林},<br />
year={2023},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/9768}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="大词表语言模型在续写任务上的一个问题及对策.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#108 大词表语言模型在续写任务上的一个问题及对策</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="自然数集中-n-ab-c-时-a-b-c-的最小值.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#110 自然数集中 N = ab + c 时 a + b + c 的最小值</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#viterbi-decodingviterbi-sampling">随机分词浅探：从Viterbi Decoding到Viterbi Sampling</a><ul>
<li><a href="#_1">要点分析</a></li>
<li><a href="#_2">已有方案</a></li>
<li><a href="#_3">个人思路</a></li>
<li><a href="#_4">简单测试</a></li>
<li><a href="#_5">文章小结</a></li>
<li><a href="#_6">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>