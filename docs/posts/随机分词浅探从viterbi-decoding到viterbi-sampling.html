<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>随机分词浅探：从Viterbi Decoding到Viterbi Sampling | ML & Math Blog Posts</title>
    <meta name="description" content="随机分词浅探：从Viterbi Decoding到Viterbi Sampling&para;
原文链接: https://spaces.ac.cn/archives/9768
发布日期: 

上一篇文章《大词表语言模型在续写任务上的一个问题及对策》发布后，很快就有读者指出可以在训练阶段引入带有随机性的分词结果来解决同样的问题，并且已经有论文和实现。经过进一步查阅学习，笔者发现这是一个名为Subw...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=概率">概率</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #249 随机分词浅探：从Viterbi Decoding到Viterbi Sampling
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#249</span>
                随机分词浅探：从Viterbi Decoding到Viterbi Sampling
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2023-09-16</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=概率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 概率</span>
                </a>
                
                <a href="../index.html?tags=随机" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 随机</span>
                </a>
                
                <a href="../index.html?tags=分词" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 分词</span>
                </a>
                
                <a href="../index.html?tags=新词发现" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 新词发现</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="viterbi-decodingviterbi-sampling">随机分词浅探：从Viterbi Decoding到Viterbi Sampling<a class="toc-link" href="#viterbi-decodingviterbi-sampling" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9768">https://spaces.ac.cn/archives/9768</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>上一篇文章<a href="/archives/9762">《大词表语言模型在续写任务上的一个问题及对策》</a>发布后，很快就有读者指出可以在训练阶段引入带有随机性的分词结果来解决同样的问题，并且已经有论文和实现。经过进一步查阅学习，笔者发现这是一个名为<a href="https://papers.cool/arxiv/1804.10959">Subword Regularization</a>的技巧，最早应用在NMT（机器翻译）中，目前SentencePiece也有相应的实现。看起来这个技巧确实能缓解前述问题，甚至有助于增强语言模型的容错能力，所以就有了将它加进去<a href="/archives/9752">BytePiece</a>的想法。</p>
<p>那么问题来了，如何将确定性分词改为随机性分词呢？BytePiece是基于Unigram模型的，它通过Viterbi算法找最大概率的分词方案，既然有概率，是否就可以自然地导出随机采样？本文来讨论这个问题，并分享自己的解决方案。</p>
<h2 id="_1">要点分析<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>现阶段，Unigram分词是直接输出最大概率的切分方案，通常这是一个确定性的输出。具体来说，假设$\boldsymbol{w}=(w_1,w_2,\cdots,w_k)$代表一个切分方案，对应的打分为$P(\boldsymbol{w})=p(w_1)p(w_2)\cdots p(w_k)$，$\Omega(S)$代表句子$S$所有可能的切分方案的集合，那么分词算法可以描述为<br />
\begin{equation}\boldsymbol{w}^* = \mathop{\text{argmax}}_{\boldsymbol{w}\in \Omega(S)}P(\boldsymbol{w})\end{equation}<br />
这可以通过Viterbi算法在线性时间内来完成，所以这个过程我们也称之为“Viterbi Decoding”。看起来，Unigram模型天然带有概率，所以似乎并不难将它改为依概率采样的形式，但细想之下才发现这并非一个平凡的问题，有很多细节上的困难需要克服。</p>
<p>笔者设想是模仿自回归语言模型设计一个递归采样流程，但这里最困难的地方是如何尽量保持原来的候选切分方案的排序不变，或者就算不能保持所有的排序不变，也至少满足最大概率不变，即Viterbi解码的最大概率路径$\boldsymbol{w}^*$应该对应所设计的递归采样算法的最大概率采样结果。由于所有切分方案$\Omega(S)$构成一个有向无环图（DAG，Directed Acyclic Graph），笔者一开始以为直接在有向无环图上随机游走是一个可行方案，但再思考后发现很难设计适当的转移概率来保证最大概率路径不变（因为同一起点的不同边不是平权的，不能简单按照边的频率为权重做采样）。</p>
<h2 id="_2">已有方案<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>由于一时半会没有新想法，所以笔者决定去翻翻“参考答案”——看看Subword Regularization的原始论文<a href="https://papers.cool/arxiv/1804.10959">《Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates》</a>是怎么做的。</p>
<p>然而，这个“标准答案”却让笔者有点哭笑不得。原来Subword Regularization的思路非常简单直接：先搜索出$P(\boldsymbol{w})$最大的$n$个分词方案$\boldsymbol{w}^<em>_1,\boldsymbol{w}^</em><em j="1">2,\cdots,\boldsymbol{w}^<em>_n$（$n$-best segmentations），然后构建如下分布<br />
\begin{equation}p_i = \frac{P(\boldsymbol{w}^</em>_i)^{\alpha}}{\sum\limits</em>}^n P(\boldsymbol{w}^*_j)^{\alpha}}\end{equation
对这$n$个方案进行依概率采样，其中$\alpha &gt; 0$是一个超参数。该算法已经集成在SentencePiece中，读者可以自行测试（使用方法参考<a href="https://github.com/google/sentencepiece/tree/master#subword-regularization-and-bpe-dropout">这里</a>）。</p>
<p>问题是，“简单直接”不代表着“高效”，尽管搜索top-$n$个分词方案最优方案的复杂度也是线性的（有兴趣的读者可以自行找找N-best Viterbi的资料)，但明显比只找top1的Viterbi Decoding要大很多（理论上是$n$倍复杂度），所以直接的后果是开启了随机采样后，会比确定性的分词要慢很多，所以这并非是笔者心中的理想采样方法。</p>
<h2 id="_3">个人思路<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>思路再度陷入了僵局。一筹莫展之际，笔者决定把思路再捋一捋：我们的目标是想要找到复杂度类似Viterbi Decoding的随机采样算法，既然如此，Viterbi Decoding本身应该是一个不错的突破点。于是，笔者再次翻开了分词代码，<a href="https://github.com/bojone/bytepiece/blob/b65716b76938b3ac4124661a3367fc1c270373fa/bytepiece/faster.pyx">当时的分词函数</a>长这个样：</p>
<div class="highlight"><pre><span></span><code><span class="s s-Atom">def</span> <span class="k">_</span><span class="nf">tokenize</span><span class="p">(</span><span class="s s-Atom">self</span><span class="p">,</span> <span class="s s-Atom">bytes</span> <span class="s s-Atom">text</span><span class="p">)</span><span class="o">:</span>
    <span class="s s-Atom">cdef</span> <span class="s s-Atom">int</span> <span class="s s-Atom">e</span><span class="p">,</span> <span class="s s-Atom">k</span><span class="p">,</span> <span class="s s-Atom">s</span>
    <span class="s s-Atom">cdef</span> <span class="s s-Atom">double</span> <span class="s s-Atom">v</span><span class="p">,</span> <span class="s s-Atom">score</span>
    <span class="s s-Atom">cdef</span> <span class="s s-Atom">list</span> <span class="s s-Atom">routes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="nv">None</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[(</span><span class="o">-</span><span class="nv">INFINITY</span><span class="p">,</span> <span class="nv">None</span><span class="p">)</span> <span class="s s-Atom">for</span> <span class="k">_</span> <span class="s s-Atom">in</span> <span class="s s-Atom">text</span><span class="p">]</span>
    <span class="s s-Atom">cdef</span> <span class="s s-Atom">list</span> <span class="s s-Atom">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="s s-Atom">for</span> <span class="s s-Atom">e</span><span class="p">,</span> <span class="p">(</span><span class="s s-Atom">k</span><span class="p">,</span> <span class="s s-Atom">v</span><span class="p">)</span> <span class="s s-Atom">in</span> <span class="s s-Atom">self</span><span class="p">.</span><span class="k">_</span><span class="s s-Atom">automaton</span><span class="p">.</span><span class="nf">iter</span><span class="p">(</span><span class="s s-Atom">text</span><span class="p">)</span><span class="o">:</span>
        <span class="s s-Atom">s</span><span class="p">,</span> <span class="s s-Atom">e</span> <span class="o">=</span> <span class="s s-Atom">e</span> <span class="o">-</span> <span class="s s-Atom">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="s s-Atom">e</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="s s-Atom">score</span> <span class="o">=</span> <span class="s s-Atom">routes</span><span class="p">[</span><span class="s s-Atom">s</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s s-Atom">v</span>
        <span class="s s-Atom">if</span> <span class="s s-Atom">score</span> <span class="o">&gt;</span> <span class="s s-Atom">routes</span><span class="p">[</span><span class="s s-Atom">e</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">:</span>
            <span class="s s-Atom">routes</span><span class="p">[</span><span class="s s-Atom">e</span><span class="p">]</span> <span class="o">=</span> <span class="s s-Atom">score</span><span class="p">,</span> <span class="s s-Atom">s</span>
    <span class="s s-Atom">while</span> <span class="s s-Atom">text</span><span class="p">:</span>
        <span class="s s-Atom">s</span> <span class="o">=</span> <span class="s s-Atom">routes</span><span class="p">[</span><span class="s s-Atom">e</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
        <span class="s s-Atom">tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="s s-Atom">text</span><span class="p">[</span><span class="s s-Atom">s</span><span class="p">:</span><span class="s s-Atom">e</span><span class="p">])</span>
        <span class="s s-Atom">text</span><span class="p">,</span> <span class="s s-Atom">e</span> <span class="o">=</span> <span class="s s-Atom">text</span><span class="p">[</span><span class="o">:</span><span class="s s-Atom">s</span><span class="p">],</span> <span class="s s-Atom">s</span>
    <span class="s s-Atom">return</span> <span class="s s-Atom">tokens</span><span class="p">[</span><span class="o">::-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

<p>反复读了几遍，总算有了灵感：Viterbi Decoding的关键是<code>if score &gt; routes[e][0]:</code>这一句，它代表保留截止到当前位置的最优切分方案，其中<code>score</code>是新切分方案分数（概率对数），<code>routes[e][0]</code>是历史最优分数，如果新方案更优则覆盖。这让笔者联想到了<a href="/archives/8084">MCMC算法</a>的接受率设计，如果在这里引入随机采样，不就可以将分词结果随机化了？</p>
<p>我们用$r\in \{1, 0\}$表示接受/拒绝新方案，由于这一步只是一个二元选择，所以将它概率化也非常简单：<br />
\begin{equation}
r_i = \left\{\begin{aligned}&amp;\,1\,, \,\, s_i &gt; s_{i-1} \\
&amp;\,0\,, \,\, \text{else}\end{aligned}\right.\qquad\longrightarrow\qquad<br />
r_i = \left\{\begin{aligned}&amp;\,1\,, \,\, \varepsilon &lt; \sigma(\alpha(s_i - s_{i-1})) \\
&amp;\,0\,, \,\, \text{else}\end{aligned}\right.<br />
\end{equation}<br />
这里$\varepsilon\sim U[0,1]$是均匀随机数，$\alpha &gt; 0$是超参数，$\sigma(t)=1/(1+e^{-t})$是Sigmoid函数，$s_i,s_{i-1}$分别是新旧方案的得分（概率对数）。不难发现，左端的确定性采样对应$\alpha\to\infty$的随机性采样。</p>
<p>这样，在Viterbi解码的基础上我们得到了一个非常自然、非常轻量级的随机采样算法，这里称之为“Viterbi Sampling”，实现它只需要将<code>if score &gt; routes[e][0]:</code>这一判据换成带随机数的版本。由于Sigmoid函数的单调性，当$s_i &gt; s_{i-1}$时，它自然会给新方案分配更大的概率，所以很明显原来的的最大概率切分在Viterbi Sampling之下也是最大概率结果，并且当$s_i - s_{i-1}$越大，$\sigma(\alpha(s_i - s_{i-1}))$也越大，这意味着原本得分越大的方案被采样到的概率也越高，一定程度上保持了切分方案的排序不变（尽管还没有证明一定严格保序，但从应用角度看，近似保序就够了）。</p>
<h2 id="_4">简单测试<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>从0.4.0版本开始，Viterbi Sampling就内置在BytePiece的分词函数中，只需要在<code>tokenizer.tokenize</code>或者<code>tokenizer.encode</code>时加入大于0的alpha参数，结果就是随机的：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">bytepiece</span>
<span class="k">assert</span> <span class="n">bytepiece</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;=</span> <span class="s1">&#39;0.4.0&#39;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">bytepiece</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span><span class="s1">&#39;bytepiece_160k.model&#39;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天天气不错&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>  <span class="c1"># alpha默认值为-1，alpha≤0 都代表确定性分词</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>

<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6&#39;, b&#39;\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8&#39;, b&#39;\x8d&#39;, b&#39;\xe9\x94&#39;, b&#39;\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9&#39;, b&#39;\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb&#39;, b&#39;\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9&#39;, b&#39;\xe6\xb0\x94\xe4\xb8\x8d&#39;, b&#39;\xe9\x94&#39;, b&#39;\x99&#39;]</span>
</code></pre></div>

<p>下面对比一下SentencePiece的Subword Regularization和BytePiece的Viterbi Sampling的速度（随机性分词时都设$\alpha=0.1$）：<br />
\begin{array}{c|cc}
\hline
&amp; \text{确定性分词} &amp; \text{随机性分词} &amp; \\
\hline
\text{SP-BPE} &amp; \text{1.36M bytes/sec} &amp; \text{1.25M bytes/sec} \\
\text{SP-Unigram} &amp; \text{5.65M bytes/sec} &amp; \text{1.28M bytes/sec} \\
\text{BytePiece} &amp; \text{1.95M bytes/sec} &amp; \text{1.36M bytes/sec}\\
\hline
\end{array}<br />
可以看到，Subword Regularization（“SP-Unigram”这一行）开启之后，分词速度不到原来的1/4，这表明Subword Regularization的采样算法是相当低效的。相比之下，本文提出的Viterbi Sampling只下降了30%左右，效率显然更高，下降的部分在于随机数的生成和Sigmoid函数的计算，如果能进一步优化这两部分，速度还能进一步提升。至于BPE模型，它的随机分词叫做<a href="https://papers.cool/arxiv/1910.13267">BPE Dropout</a>，这是专属于BPE模型的方法，有兴趣的读者自行了解，这里就不介绍了。</p>
<h2 id="_5">文章小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文主要探讨了将Unigram分词模型的确定性分词改为随机性分词的策略。尽管已有名为“Subword Regularization”的方法可以实现这一目标，但其效率相对较低。为此，笔者提出了一种更高效的采样算法Viterbi Sampling，它仅需对确定性的Viterbi Decoding进行简单的修改，从而基本保持了原有的效率。实验证明，新的算法采样速度明显超越了Subword Regularization。相应的实现已经内置在BytePiece最新版中。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9768">https://spaces.ac.cn/archives/9768</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 16, 2023). 《随机分词浅探：从Viterbi Decoding到Viterbi Sampling 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9768">https://spaces.ac.cn/archives/9768</a></p>
<p>@online{kexuefm-9768,<br />
title={随机分词浅探：从Viterbi Decoding到Viterbi Sampling},<br />
author={苏剑林},<br />
year={2023},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/9768}},<br />
} </p>
<hr />
<h2 id="_6">完整数学推导与理论分析<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本节将详细推导Viterbi算法的数学基础，包括HMM模型、动态规划、前向后向算法，以及Viterbi Sampling的完整理论。</p>
<h3 id="hmm">一、隐马尔可夫模型(HMM)基础<a class="toc-link" href="#hmm" title="Permanent link">&para;</a></h3>
<h4 id="11-hmm">1.1 HMM的三要素<a class="toc-link" href="#11-hmm" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：隐马尔可夫模型是关于时序的概率模型，描述一个隐藏的马尔可夫链随机生成不可观测的状态序列，再由各个状态生成观测序列的过程。</p>
<div class="definition-box">

**HMM的三要素**：

设状态集合$\mathcal{S} = \{s_1, s_2, \ldots, s_N\}$，观测集合$\mathcal{O} = \{o_1, o_2, \ldots, o_M\}$。

1. **初始状态概率**：$\boldsymbol{\pi} = (\pi_1, \ldots, \pi_N)$
   \begin{equation}
   \pi_i = P(q_1 = s_i), \quad \sum_{i=1}^{N} \pi_i = 1 \tag{1}
   \end{equation}

2. **状态转移概率**：$\mathbf{A} = (a_{ij})_{N \times N}$
   \begin{equation}
   a_{ij} = P(q_{t+1} = s_j | q_t = s_i), \quad \sum_{j=1}^{N} a_{ij} = 1 \tag{2}
   \end{equation}

3. **发射概率（观测概率）**：$\mathbf{B} = (b_j(o_k))$
   \begin{equation}
   b_j(o_k) = P(x_t = o_k | q_t = s_j), \quad \sum_{k=1}^{M} b_j(o_k) = 1 \tag{3}
   \end{equation}

HMM记为$\lambda = (\mathbf{A}, \mathbf{B}, \boldsymbol{\pi})$。

</div>

<p><strong>马尔可夫性假设</strong>：</p>
<ol>
<li>
<p><strong>齐次马尔可夫假设</strong>：状态转移概率与时间无关
   \begin{equation}
   P(q_t | q_{t-1}, \ldots, q_1) = P(q_t | q_{t-1}) \tag{4}
   \end{equation}</p>
</li>
<li>
<p><strong>观测独立性假设</strong>：观测只依赖于当前状态
   \begin{equation}
   P(x_t | q_1, \ldots, q_T, x_1, \ldots, x_{t-1}, x_{t+1}, \ldots, x_T) = P(x_t | q_t) \tag{5}
   \end{equation}</p>
</li>
</ol>
<h4 id="12-hmmunigram">1.2 HMM的应用：Unigram分词<a class="toc-link" href="#12-hmmunigram" title="Permanent link">&para;</a></h4>
<p>在Unigram分词中，HMM的对应关系：</p>
<ul>
<li><strong>观测序列</strong>：原始文本（字节序列）$\boldsymbol{c} = (c_1, c_2, \ldots, c_L)$</li>
<li><strong>状态序列</strong>：切分方案（词序列）$\boldsymbol{w} = (w_1, w_2, \ldots, w_K)$</li>
<li><strong>状态转移</strong>：从一个词到下一个词（实际上是独立的，$a_{ij} = 1$）</li>
<li><strong>发射概率</strong>：词的概率$P(w_i)$</li>
</ul>
<p><strong>简化的Unigram模型</strong>：</p>
<p>由于假设词之间独立（unigram），不存在真正的状态转移，只有发射概率：
\begin{equation}
P(\boldsymbol{w}) = \prod_{i=1}^{K} P(w_i) \tag{6}
\end{equation}</p>
<p><strong>分词问题的形式化</strong>：</p>
<p>给定字节序列$\boldsymbol{c} = c_1c_2\cdots c_L$，找到概率最大的切分方案：
\begin{equation}
\boldsymbol{w}^* = \arg\max_{\boldsymbol{w} \in \Omega(\boldsymbol{c})} P(\boldsymbol{w}) \tag{7}
\end{equation}</p>
<p>其中$\Omega(\boldsymbol{c})$是所有能够完整覆盖$\boldsymbol{c}$的切分方案的集合。</p>
<h3 id="viterbi">二、Viterbi算法：动态规划解码<a class="toc-link" href="#viterbi" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 问题设定<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p><strong>目标</strong>：给定观测序列$\boldsymbol{x} = (x_1, \ldots, x_T)$和HMM参数$\lambda$，找到最可能的状态序列：
\begin{equation}
\boldsymbol{q}^* = \arg\max_{\boldsymbol{q}} P(\boldsymbol{q} | \boldsymbol{x}, \lambda) = \arg\max_{\boldsymbol{q}} P(\boldsymbol{q}, \boldsymbol{x} | \lambda) \tag{8}
\end{equation}</p>
<p><strong>朴素方法的问题</strong>：</p>
<p>遍历所有可能的状态序列，共$N^T$种，复杂度指数级！</p>
<h4 id="22">2.2 动态规划的思想<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p><strong>关键观察</strong>：最优路径的任何子路径也是最优的（最优子结构性质）。</p>
<p>定义变量$\delta_t(i)$：
\begin{equation}
\delta_t(i) = \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, q_t = s_i, x_1, \ldots, x_t | \lambda) \tag{9}
\end{equation}</p>
<p><strong>含义</strong>：到时刻$t$，以状态$s_i$结尾，观测序列为$x_1, \ldots, x_t$的所有路径中，概率最大的路径的概率。</p>
<h4 id="23-viterbi">2.3 Viterbi递推公式<a class="toc-link" href="#23-viterbi" title="Permanent link">&para;</a></h4>
<p><strong>初始化</strong>（$t = 1$）：
\begin{equation}
\delta_1(i) = \pi_i \cdot b_i(x_1), \quad i = 1, \ldots, N \tag{10}
\end{equation}
\begin{equation}
\psi_1(i) = 0 \tag{11}
\end{equation}</p>
<p><strong>递推</strong>（$t = 2, \ldots, T$）：
\begin{equation}
\delta_t(j) = \max_{1 \leq i \leq N} [\delta_{t-1}(i) \cdot a_{ij}] \cdot b_j(x_t) \tag{12}
\end{equation}
\begin{equation}
\psi_t(j) = \arg\max_{1 \leq i \leq N} [\delta_{t-1}(i) \cdot a_{ij}] \tag{13}
\end{equation}</p>
<p>其中$\psi_t(j)$记录了到达状态$j$的最优前驱状态。</p>
<p><strong>证明递推公式的正确性</strong>：</p>
<p>\begin{equation}
\begin{aligned}
\delta_t(j) &amp;= \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, q_t = s_j, x_1, \ldots, x_t | \lambda) \
&amp;= \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, x_1, \ldots, x_{t-1} | \lambda) \
&amp;\quad \times P(q_t = s_j | q_{t-1}) \times P(x_t | q_t = s_j) \
&amp;= \max_{1 \leq i \leq N} \left[ \max_{q_1, \ldots, q_{t-2}} P(q_1, \ldots, q_{t-2}, q_{t-1} = s_i, x_1, \ldots, x_{t-1}) \right] \
&amp;\quad \times a_{ij} \times b_j(x_t) \
&amp;= \max_{1 \leq i \leq N} [\delta_{t-1}(i) \cdot a_{ij}] \cdot b_j(x_t)
\end{aligned} \tag{14}
\end{equation}</p>
<p><strong>终止</strong>：
\begin{equation}
P^<em> = \max_{1 \leq i \leq N} \delta_T(i) \tag{15}
\end{equation}
\begin{equation}
q_T^</em> = \arg\max_{1 \leq i \leq N} \delta_T(i) \tag{16}
\end{equation}</p>
<p><strong>回溯</strong>（$t = T-1, T-2, \ldots, 1$）：
\begin{equation}
q_t^<em> = \psi_{t+1}(q_{t+1}^</em>) \tag{17}
\end{equation}</p>
<h4 id="24-viterbi">2.4 对数空间的Viterbi算法<a class="toc-link" href="#24-viterbi" title="Permanent link">&para;</a></h4>
<p><strong>数值稳定性问题</strong>：连乘很多小概率会导致下溢。</p>
<p><strong>解决方法</strong>：在对数空间进行计算。</p>
<p>定义：
\begin{equation}
\tilde{\delta}_t(i) = \log \delta_t(i) \tag{18}
\end{equation}</p>
<p>递推公式变为：
\begin{equation}
\tilde{\delta}<em 1="1" N="N" _leq="\leq" i="i">t(j) = \max</em>} [\tilde{\delta<em ij="ij">{t-1}(i) + \log a</em>
\end{equation}}] + \log b_j(x_t) \tag{19</p>
<p><strong>关键变化</strong>：
- 乘法 → 加法
- $\max$操作保持不变</p>
<h3 id="unigramviterbi">三、应用于Unigram分词的Viterbi算法<a class="toc-link" href="#unigramviterbi" title="Permanent link">&para;</a></h3>
<h4 id="31-dag">3.1 分词的DAG表示<a class="toc-link" href="#31-dag" title="Permanent link">&para;</a></h4>
<p>给定字节序列$\boldsymbol{c} = c_1c_2\cdots c_L$和词表$\mathcal{V}$。</p>
<p><strong>构建DAG（有向无环图）</strong>：
- <strong>节点</strong>：位置$0, 1, 2, \ldots, L$
- <strong>边</strong>：如果$\overline{c_i c_{i+1} \cdots c_j} \in \mathcal{V}$，则存在边$(i-1) \to j$
- <strong>边权</strong>：$\log P(\overline{c_i \cdots c_j})$</p>
<p><strong>最优路径</strong>：从节点0到节点$L$的最大权重路径。</p>
<h4 id="32-viterbiforward-dp">3.2 前向Viterbi算法（Forward DP）<a class="toc-link" href="#32-viterbiforward-dp" title="Permanent link">&para;</a></h4>
<p>定义$S^*(i)$：从位置0到位置$i$的最优路径得分。</p>
<p><strong>初始化</strong>：
\begin{equation}
S^*(0) = 0 \tag{20}
\end{equation}</p>
<p><strong>递推</strong>（$i = 1, 2, \ldots, L$）：
\begin{equation}
S^<em>(i) = \max_{j &lt; i, \overline{c_{j+1}\cdots c_i} \in \mathcal{V}} [S^</em>(j) + \log P(\overline{c_{j+1}\cdots c_i})] \tag{21}
\end{equation}</p>
<p>同时记录前驱：
\begin{equation}
\text{prev}(i) = \arg\max_{j &lt; i, \overline{c_{j+1}\cdots c_i} \in \mathcal{V}} [S^*(j) + \log P(\overline{c_{j+1}\cdots c_i})] \tag{22}
\end{equation}</p>
<p><strong>回溯</strong>：
\begin{equation}
i \gets L; \quad \text{tokens} = [] \
\text{while } i &gt; 0: \
\quad j = \text{prev}(i) \
\quad \text{tokens.append}(\overline{c_{j+1}\cdots c_i}) \
\quad i \gets j \tag{23}
\end{equation}</p>
<h4 id="33-ac">3.3 AC自动机加速<a class="toc-link" href="#33-ac" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：对于每个位置$i$，需要遍历所有$j &lt; i$检查是否存在词，复杂度$O(L^2)$。</p>
<p><strong>AC自动机（Aho-Corasick Automaton）</strong>：</p>
<p>预处理词表，构建失配指针，可以在$O(L)$时间内找到所有词的终止位置。</p>
<p><strong>算法流程</strong>：
1. 预处理：构建AC自动机（$O(|\mathcal{V}| \cdot \bar{L}_w)$，$\bar{L}_w$是平均词长）
2. 扫描：用AC自动机扫描文本，找到所有可能的词$(s, e, w)$（起始、结束、词本身）
3. DP：按结束位置$e$排序，依次计算$S^*(e)$</p>
<p><strong>复杂度</strong>：$O(L \cdot m)$，其中$m$是词表中最大词长。</p>
<h4 id="34">3.4 伪代码实现<a class="toc-link" href="#34" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">viterbi_decoding</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">prob</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    text: 输入字节序列</span>
<span class="sd">    vocab: 词表（Trie或AC自动机）</span>
<span class="sd">    prob: 词的对数概率函数，prob(w) = log P(w)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># 初始化</span>
    <span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)]</span> <span class="o">*</span> <span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># 使用AC自动机找到所有可能的词</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">find_all_words</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>  <span class="c1"># [(start, end, word), ...]</span>

    <span class="c1"># 按结束位置分组</span>
    <span class="n">candidates_by_end</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
        <span class="n">candidates_by_end</span><span class="p">[</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">word</span><span class="p">))</span>

    <span class="c1"># 动态规划</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">candidates_by_end</span><span class="p">[</span><span class="n">e</span><span class="p">]:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+</span> <span class="n">prob</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">S</span><span class="p">[</span><span class="n">e</span><span class="p">]:</span>
                <span class="n">S</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                <span class="n">prev</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span>

    <span class="c1"># 回溯</span>
    <span class="k">if</span> <span class="n">S</span><span class="p">[</span><span class="n">L</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># 无法切分</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">L</span>
    <span class="k">while</span> <span class="n">pos</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">word</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">s</span>

    <span class="k">return</span> <span class="n">tokens</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

<h3 id="-">四、前向-后向算法对比<a class="toc-link" href="#-" title="Permanent link">&para;</a></h3>
<h4 id="41-forward-algorithm">4.1 前向算法（Forward Algorithm）<a class="toc-link" href="#41-forward-algorithm" title="Permanent link">&para;</a></h4>
<p><strong>目的</strong>：计算观测序列的概率$P(\boldsymbol{x}|\lambda)$。</p>
<p>定义前向变量：
\begin{equation}
\alpha_t(i) = P(x_1, \ldots, x_t, q_t = s_i | \lambda) \tag{24}
\end{equation}</p>
<p><strong>递推</strong>：
\begin{equation}
\alpha_t(j) = \left[\sum_{i=1}^{N} \alpha_{t-1}(i) a_{ij}\right] b_j(x_t) \tag{25}
\end{equation}</p>
<p><strong>与Viterbi的对比</strong>：
- 前向：$\sum$（求和，所有路径）
- Viterbi：$\max$（最大值，最优路径）</p>
<h4 id="42-backward-algorithm">4.2 后向算法（Backward Algorithm）<a class="toc-link" href="#42-backward-algorithm" title="Permanent link">&para;</a></h4>
<p>定义后向变量：
\begin{equation}
\beta_t(i) = P(x_{t+1}, \ldots, x_T | q_t = s_i, \lambda) \tag{26}
\end{equation}</p>
<p><strong>递推</strong>（从$T$到$1$）：
\begin{equation}
\beta_t(i) = \sum_{j=1}^{N} a_{ij} b_j(x_{t+1}) \beta_{t+1}(j) \tag{27}
\end{equation}</p>
<h4 id="43-">4.3 前向-后向的应用<a class="toc-link" href="#43-" title="Permanent link">&para;</a></h4>
<p><strong>边缘概率</strong>：
\begin{equation}
P(q_t = s_i | \boldsymbol{x}, \lambda) = \frac{\alpha_t(i) \beta_t(i)}{P(\boldsymbol{x}|\lambda)} = \frac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^{N} \alpha_T(j)} \tag{28}
\end{equation}</p>
<p><strong>边缘概率 vs Viterbi路径</strong>：
- 边缘概率：考虑所有路径，给出每个位置每个状态的概率
- Viterbi：只考虑最优路径</p>
<h3 id="viterbi-decodingviterbi-sampling_1">五、从Viterbi Decoding到Viterbi Sampling<a class="toc-link" href="#viterbi-decodingviterbi-sampling_1" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 采样的动机<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p><strong>Viterbi Decoding的局限</strong>：
1. <strong>确定性</strong>：总是输出同一个结果
2. <strong>过度自信</strong>：即使多个切分方案概率接近，也只返回一个
3. <strong>训练-推理不一致</strong>：训练时可能需要多样性</p>
<p><strong>Viterbi Sampling的目标</strong>：
- 依概率采样切分方案
- 概率高的方案被采样到的概率大
- 保持Viterbi Decoding的效率（线性复杂度）</p>
<h4 id="52-n-best">5.2 朴素采样方法：N-best搜索<a class="toc-link" href="#52-n-best" title="Permanent link">&para;</a></h4>
<p><strong>Subword Regularization的做法</strong>：</p>
<ol>
<li>找到概率最高的$n$个切分方案$\boldsymbol{w}_1^<em>, \ldots, \boldsymbol{w}_n^</em>$</li>
<li>计算权重：
   \begin{equation}
   p_i = \frac{P(\boldsymbol{w}_i^<em>)^\alpha}{\sum_{j=1}^{n} P(\boldsymbol{w}_j^</em>)^\alpha} \tag{29}
   \end{equation}</li>
<li>按$p_i$进行采样</li>
</ol>
<p><strong>N-best Viterbi算法</strong>：</p>
<p>维护每个位置的top-$n$路径，复杂度$O(nLm)$，其中$m$是最大词长。</p>
<p><strong>缺点</strong>：
- 效率低（$n$倍复杂度）
- 需要选择$n$（太小：覆盖不足；太大：浪费）</p>
<h4 id="53-viterbi-sampling">5.3 Viterbi Sampling的核心思想<a class="toc-link" href="#53-viterbi-sampling" title="Permanent link">&para;</a></h4>
<p><strong>问题重新表述</strong>：</p>
<p>Viterbi Decoding的判据是：
\begin{equation}
\text{if } \text{score}<em _text_old="\text{old">{\text{new}} &gt; \text{score}</em>
\end{equation}}} \text{ then accept new} \tag{30</p>
<p><strong>随机化判据</strong>：
\begin{equation}
\text{if } \varepsilon &lt; \sigma(\alpha(\text{score}<em _text_old="\text{old">{\text{new}} - \text{score}</em>
\end{equation}}})) \text{ then accept new} \tag{31</p>
<p>其中：
- $\varepsilon \sim U[0,1]$：均匀随机数
- $\sigma(x) = 1/(1 + e^{-x})$：Sigmoid函数
- $\alpha &gt; 0$：温度参数</p>
<p><strong>性质</strong>：
1. 当$\text{score}<em _text_old="\text{old">{\text{new}} &gt; \text{score}</em>$时，接受概率$&gt; 0.5$
2. 分数差距越大，接受概率越极端（接近0或1）
3. $\alpha \to \infty$时，退化为Viterbi Decoding}</p>
<h4 id="54-viterbi-sampling">5.4 Viterbi Sampling的递推公式<a class="toc-link" href="#54-viterbi-sampling" title="Permanent link">&para;</a></h4>
<p><strong>修改Viterbi递推</strong>：</p>
<p>原本：
\begin{equation}
\text{if } S[j] + s(w) &gt; S[i] \text{ then} \
\quad S[i] = S[j] + s(w) \
\quad \text{prev}[i] = (j, w) \tag{32}
\end{equation}</p>
<p>随机化为：
\begin{equation}
\text{score}<em _text_old="\text{old">{\text{new}} = S[j] + s(w) \
\text{score}</em> = S[i] \
p_{\text{accept}} = \sigma(\alpha(\text{score}}<em _text_old="\text{old">{\text{new}} - \text{score}</em>)) \
\text{if } \varepsilon &lt; p_{\text{accept}} \text{ then} \
\quad S[i] = \text{score}_{\text{new}} \
\quad \text{prev}[i] = (j, w) \tag{33}
\end{equation}}</p>
<p><strong>初始化</strong>：
\begin{equation}
S[i] = -\infty, \quad i = 1, \ldots, L \tag{34}
\end{equation}</p>
<h4 id="55">5.5 保序性分析<a class="toc-link" href="#55" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：如果切分方案$\boldsymbol{w}_1$的得分高于$\boldsymbol{w}_2$，即$P(\boldsymbol{w}_1) &gt; P(\boldsymbol{w}_2)$，那么在采样中$\boldsymbol{w}_1$被采到的概率是否也更高？</p>
<p><strong>Viterbi Sampling的保序性</strong>：</p>
<p>不是严格保序的，但近似保序。</p>
<p><strong>分析</strong>：考虑两条路径$A$和$B$，得分分别为$s_A$和$s_B$，$s_A &gt; s_B$。</p>
<p>如果两条路径在位置$i$竞争：
\begin{equation}
P(\text{选择}A) = \sigma(\alpha(s_A - s_B)) &gt; 0.5 \tag{35}
\end{equation}</p>
<p>但是，如果路径涉及多步竞争，每步都有随机性，最终的排序可能改变。</p>
<p><strong>实证观察</strong>：在实际应用中，得分高的方案确实更常被采样到，尤其是当$\alpha$较大时。</p>
<h3 id="_7">六、数值稳定性与实现技巧<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<h4 id="61-logsumexp">6.1 LogSumExp技巧<a class="toc-link" href="#61-logsumexp" title="Permanent link">&para;</a></h4>
<p>前向算法涉及到求和：
\begin{equation}
\alpha_t(j) = \left[\sum_{i=1}^{N} \alpha_{t-1}(i) a_{ij}\right] b_j(x_t) \tag{36}
\end{equation}</p>
<p>在对数空间：
\begin{equation}
\log \alpha_t(j) = \log \left[\sum_{i=1}^{N} e^{\log \alpha_{t-1}(i) + \log a_{ij}}\right] + \log b_j(x_t) \tag{37}
\end{equation}</p>
<p>直接计算$\sum e^{x_i}$可能溢出。使用LogSumExp：
\begin{equation}
\text{LSE}(x_1, \ldots, x_N) = x_{\max} + \log \sum_{i=1}^{N} e^{x_i - x_{\max}} \tag{38}
\end{equation}</p>
<p>其中$x_{\max} = \max_i x_i$。</p>
<h4 id="62-sigmoid">6.2 Sigmoid的数值稳定计算<a class="toc-link" href="#62-sigmoid" title="Permanent link">&para;</a></h4>
<p>Sigmoid函数：
\begin{equation}
\sigma(x) = \frac{1}{1 + e^{-x}} \tag{39}
\end{equation}</p>
<p><strong>问题</strong>：
- 当$x$很大时，$e^{-x} \to 0$，但计算$e^{-x}$可能下溢
- 当$x$很小时，$e^{-x} \to \infty$，上溢</p>
<p><strong>稳定计算</strong>：
\begin{equation}
\sigma(x) = \begin{cases}
\frac{1}{1 + e^{-x}}, &amp; x \geq 0 \
\frac{e^x}{1 + e^x}, &amp; x &lt; 0
\end{cases} \tag{40}
\end{equation}</p>
<p><strong>推导</strong>：
\begin{equation}
\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{e^x + 1} = \frac{e^x}{1 + e^x} \tag{41}
\end{equation}</p>
<p>两种形式等价，但数值稳定性不同。</p>
<h4 id="63">6.3 随机数生成的效率<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p>Viterbi Sampling每次DP更新都需要一个随机数$\varepsilon \sim U[0,1]$。</p>
<p><strong>优化方法</strong>：
1. <strong>批量生成</strong>：预先生成一大批随机数
2. <strong>快速RNG</strong>：使用Xorshift等快速随机数生成器
3. <strong>向量化</strong>：使用SIMD指令并行生成</p>
<p><strong>Python示例</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 预生成随机数</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random_pool</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">rand_idx</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_random</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">rand_idx</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">random_pool</span><span class="p">[</span><span class="n">rand_idx</span><span class="p">]</span>
    <span class="n">rand_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">rand_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">random_pool</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span>
</code></pre></div>

<h3 id="_8">七、采样质量分析<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 期望值分析<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：Viterbi Sampling采样到某个切分方案$\boldsymbol{w}$的概率是多少？</p>
<p>这是一个复杂的问题，因为采样过程涉及多步随机决策。</p>
<p><strong>近似分析</strong>：</p>
<p>假设DP过程中，每个位置独立做决策（这是一个简化假设）。设位置$i$有$k_i$个候选词结尾，得分分别为$s_1^{(i)}, \ldots, s_{k_i}^{(i)}$。</p>
<p>采样到得分为$s_j^{(i)}$的候选的概率约为：
\begin{equation}
p_j^{(i)} \approx \frac{e^{\alpha s_j^{(i)}}}{\sum_{\ell=1}^{k_i} e^{\alpha s_\ell^{(i)}}} \tag{42}
\end{equation}</p>
<p>这类似于Softmax！</p>
<p>整条路径被采样到的概率（近似）：
\begin{equation}
P_{\text{sample}}(\boldsymbol{w}) \approx \prod_{i \in \boldsymbol{w}} p_j^{(i)} \tag{43}
\end{equation}</p>
<h4 id="72">7.2 与真实概率的关系<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p><strong>理想情况</strong>：我们希望采样概率正比于真实概率：
\begin{equation}
P_{\text{sample}}(\boldsymbol{w}) \propto P(\boldsymbol{w}) = e^{s(\boldsymbol{w})} \tag{44}
\end{equation}</p>
<p>其中$s(\boldsymbol{w}) = \sum_{w_i \in \boldsymbol{w}} \log P(w_i)$。</p>
<p><strong>Viterbi Sampling的偏差</strong>：</p>
<p>由于多次二选一的稀释效应（后面的候选"占便宜"），Viterbi Sampling的采样概率与真实概率不完全一致。</p>
<p><strong>文章的改进方向</strong>：
- 使用水塘采样（Reservoir Sampling）
- 缓存累积概率
- 详见下一篇文章《随机分词再探》</p>
<h4 id="73-alpha">7.3 温度参数$\alpha$的影响<a class="toc-link" href="#73-alpha" title="Permanent link">&para;</a></h4>
<p><strong>$\alpha \to 0$</strong>：
\begin{equation}
\sigma(\alpha \Delta s) \to \begin{cases}
1, &amp; \Delta s &gt; 0 \
0.5, &amp; \Delta s = 0 \
0, &amp; \Delta s &lt; 0
\end{cases} \tag{45}
\end{equation}</p>
<p>接近均匀采样（所有正分数差都以概率0.5接受）。</p>
<p><strong>$\alpha \to \infty$</strong>：
\begin{equation}
\sigma(\alpha \Delta s) \to \begin{cases}
1, &amp; \Delta s &gt; 0 \
0.5, &amp; \Delta s = 0 \
0, &amp; \Delta s &lt; 0
\end{cases} \tag{46}
\end{equation}</p>
<p>退化为Viterbi Decoding（总是选择分数高的）。</p>
<p><strong>$\alpha = 0.1$（经验值）</strong>：
- 保持一定的随机性
- 同时偏向高分方案
- 在训练中引入正则化效果</p>
<h3 id="bpe-dropout">八、与BPE Dropout的对比<a class="toc-link" href="#bpe-dropout" title="Permanent link">&para;</a></h3>
<h4 id="81-bpe-dropout">8.1 BPE Dropout<a class="toc-link" href="#81-bpe-dropout" title="Permanent link">&para;</a></h4>
<p>BPE（Byte Pair Encoding）使用贪心merge策略，每次选择频率最高的字节对合并。</p>
<p><strong>BPE Dropout</strong>：在merge过程中，以概率$p$随机跳过某些merge操作。</p>
<p><strong>算法</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">bpe_dropout</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">merge_rules</span><span class="p">,</span> <span class="n">p_dropout</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">rule</span> <span class="ow">in</span> <span class="n">merge_rules</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">p_dropout</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">apply_merge</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">rule</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span>
</code></pre></div>

<h4 id="82">8.2 对比<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方面</th>
<th>Viterbi Sampling</th>
<th>BPE Dropout</th>
</tr>
</thead>
<tbody>
<tr>
<td>适用模型</td>
<td>Unigram</td>
<td>BPE</td>
</tr>
<tr>
<td>随机化方式</td>
<td>DP过程中随机选择</td>
<td>Merge过程中随机跳过</td>
</tr>
<tr>
<td>概率保证</td>
<td>近似按概率采样</td>
<td>无明确概率保证</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>中等</td>
<td>简单</td>
</tr>
<tr>
<td>效率</td>
<td>与确定性版本相近</td>
<td>与确定性版本相同</td>
</tr>
</tbody>
</table>
<h4 id="83-subword-regularization">8.3 Subword Regularization对比<a class="toc-link" href="#83-subword-regularization" title="Permanent link">&para;</a></h4>
<p><strong>方法</strong>：找top-$n$个切分方案，按概率加权采样。</p>
<p><strong>优点</strong>：
- 精确按概率采样
- 理论保证强</p>
<p><strong>缺点</strong>：
- 效率低（$n$倍复杂度）
- 需要调参$n$</p>
<p><strong>Viterbi Sampling的优势</strong>：
- 接近原始效率
- 自动适应（无需调$n$）
- 实现简单</p>
<h3 id="_9">九、实际应用与效果<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 训练数据增强<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p><strong>应用场景</strong>：训练语言模型时，每个epoch使用不同的分词结果。</p>
<p><strong>效果</strong>：
- 增强模型鲁棒性
- 减少对特定分词的过拟合
- 改善out-of-vocabulary (OOV)处理</p>
<p><strong>示例</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 训练时</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="c1"># 每次使用随机分词</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div>

<h4 id="92">9.2 速度对比<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p>根据文章的实验数据：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>确定性速度</th>
<th>随机性速度</th>
<th>速度比</th>
</tr>
</thead>
<tbody>
<tr>
<td>SP-BPE</td>
<td>1.36M bytes/sec</td>
<td>1.25M bytes/sec</td>
<td>92%</td>
</tr>
<tr>
<td><strong>SP-Unigram</strong></td>
<td><strong>5.65M bytes/sec</strong></td>
<td><strong>1.28M bytes/sec</strong></td>
<td><strong>23%</strong></td>
</tr>
<tr>
<td>BytePiece (Viterbi Sampling)</td>
<td>1.95M bytes/sec</td>
<td>1.36M bytes/sec</td>
<td><strong>70%</strong></td>
</tr>
</tbody>
</table>
<p><strong>观察</strong>：
- Subword Regularization (SP-Unigram随机版)速度下降到23%
- Viterbi Sampling保持70%的速度
- <strong>3倍加速</strong>！</p>
<h4 id="93">9.3 分词多样性<a class="toc-link" href="#93" title="Permanent link">&para;</a></h4>
<p><strong>实验</strong>（从文章）：</p>
<div class="highlight"><pre><span></span><code><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天天气不错&#39;</span>

<span class="c1"># 确定性分词</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>

<span class="c1"># Viterbi Sampling (alpha=0.1)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6&#39;, b&#39;\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8&#39;, b&#39;\x8d&#39;, b&#39;\xe9\x94&#39;, b&#39;\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9&#39;, b&#39;\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb&#39;, b&#39;\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9&#39;, b&#39;\xe6\xb0\x94\xe4\xb8\x8d&#39;, b&#39;\xe9\x94&#39;, b&#39;\x99&#39;]</span>
</code></pre></div>

<p><strong>分析</strong>：
- 确定性版本总是输出同一结果
- 随机版本产生多种切分
- 某些常见切分出现频率更高（如"今天"、"天气"）</p>
<h3 id="_10">十、理论保证与局限性<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<h4 id="101-viterbi-sampling">10.1 Viterbi Sampling的理论性质<a class="toc-link" href="#101-viterbi-sampling" title="Permanent link">&para;</a></h4>
<p><strong>性质1（近似保序）</strong>：高分切分方案被采样到的概率更大。</p>
<p><strong>性质2（效率）</strong>：复杂度与Viterbi Decoding相同，都是$O(Lm)$。</p>
<p><strong>性质3（可控随机性）</strong>：通过$\alpha$控制随机性强度。</p>
<h4 id="102">10.2 未解决的问题<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p><strong>问题1</strong>：采样概率的精确表达式是什么？</p>
<p>目前只有近似分析，缺乏精确的概率分布。</p>
<p><strong>问题2</strong>：如何保证采样的无偏性？</p>
<p>Viterbi Sampling由于稀释效应，不是严格按$P(\boldsymbol{w})$采样。</p>
<p><strong>解决方向</strong>：
- 水塘采样（下一篇文章）
- 基于CDF的逆变换采样</p>
<h4 id="103">10.3 适用场景<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p><strong>适合</strong>：
- 需要快速随机分词
- 训练数据增强
- 轻量级正则化</p>
<p><strong>不适合</strong>：
- 需要精确概率分布的场景
- 理论分析要求严格无偏采样</p>
<h3 id="_11">十一、总结<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<p>Viterbi算法是序列标注和分词问题的核心算法，通过动态规划在线性时间内找到最优路径。</p>
<p><strong>Viterbi Decoding</strong>：
\begin{equation}
S^<em>(i) = \max_{j &lt; i} [S^</em>(j) + \log P(w_{j+1:i})] \tag{47}
\end{equation}</p>
<p><strong>Viterbi Sampling（初步版本）</strong>：
\begin{equation}
\text{accept new if } \varepsilon &lt; \sigma(\alpha \cdot \Delta s) \tag{48}
\end{equation}</p>
<p><strong>优势</strong>：
- ✅ 保持线性复杂度
- ✅ 引入随机性
- ✅ 实现简单
- ✅ 速度快（相比Subword Regularization）</p>
<p><strong>局限</strong>：
- ❌ 采样概率不精确
- ❌ 稀释效应（后续候选占便宜）
- ❌ 理论保证不足</p>
<p><strong>下一步</strong>：在下一篇文章《随机分词再探》中，我们将解决稀释效应，通过水塘采样实现完美采样，达到与Subword Regularization等价的效果，同时保持高效率。</p>
<hr />
<h2 id="_12">完整数学推导与理论分析<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h2>
<p>本节将详细推导Viterbi算法的数学基础，包括HMM模型、动态规划、前向后向算法，以及Viterbi Sampling的完整理论。</p>
<h3 id="hmm_1">一、隐马尔可夫模型(HMM)基础<a class="toc-link" href="#hmm_1" title="Permanent link">&para;</a></h3>
<h4 id="11-hmm_1">1.1 HMM的三要素<a class="toc-link" href="#11-hmm_1" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：隐马尔可夫模型是关于时序的概率模型，描述一个隐藏的马尔可夫链随机生成不可观测的状态序列，再由各个状态生成观测序列的过程。</p>
<div class="definition-box">

**HMM的三要素**：

设状态集合$\mathcal{S} = \{s_1, s_2, \ldots, s_N\}$，观测集合$\mathcal{O} = \{o_1, o_2, \ldots, o_M\}$。

1. **初始状态概率**：$\boldsymbol{\pi} = (\pi_1, \ldots, \pi_N)$
   \begin{equation}
   \pi_i = P(q_1 = s_i), \quad \sum_{i=1}^{N} \pi_i = 1 \tag{1}
   \end{equation}

2. **状态转移概率**：$\mathbf{A} = (a_{ij})_{N \times N}$
   \begin{equation}
   a_{ij} = P(q_{t+1} = s_j | q_t = s_i), \quad \sum_{j=1}^{N} a_{ij} = 1 \tag{2}
   \end{equation}

3. **发射概率（观测概率）**：$\mathbf{B} = (b_j(o_k))$
   \begin{equation}
   b_j(o_k) = P(x_t = o_k | q_t = s_j), \quad \sum_{k=1}^{M} b_j(o_k) = 1 \tag{3}
   \end{equation}

HMM记为$\lambda = (\mathbf{A}, \mathbf{B}, \boldsymbol{\pi})$。

</div>

<p><strong>马尔可夫性假设</strong>：</p>
<ol>
<li>
<p><strong>齐次马尔可夫假设</strong>：状态转移概率与时间无关
   \begin{equation}
   P(q_t | q_{t-1}, \ldots, q_1) = P(q_t | q_{t-1}) \tag{4}
   \end{equation}</p>
</li>
<li>
<p><strong>观测独立性假设</strong>：观测只依赖于当前状态
   \begin{equation}
   P(x_t | q_1, \ldots, q_T, x_1, \ldots, x_{t-1}, x_{t+1}, \ldots, x_T) = P(x_t | q_t) \tag{5}
   \end{equation}</p>
</li>
</ol>
<h4 id="12-hmmunigram_1">1.2 HMM的应用：Unigram分词<a class="toc-link" href="#12-hmmunigram_1" title="Permanent link">&para;</a></h4>
<p>在Unigram分词中，HMM的对应关系：</p>
<ul>
<li><strong>观测序列</strong>：原始文本（字节序列）$\boldsymbol{c} = (c_1, c_2, \ldots, c_L)$</li>
<li><strong>状态序列</strong>：切分方案（词序列）$\boldsymbol{w} = (w_1, w_2, \ldots, w_K)$</li>
<li><strong>状态转移</strong>：从一个词到下一个词（实际上是独立的，$a_{ij} = 1$）</li>
<li><strong>发射概率</strong>：词的概率$P(w_i)$</li>
</ul>
<p><strong>简化的Unigram模型</strong>：</p>
<p>由于假设词之间独立（unigram），不存在真正的状态转移，只有发射概率：
\begin{equation}
P(\boldsymbol{w}) = \prod_{i=1}^{K} P(w_i) \tag{6}
\end{equation}</p>
<p><strong>分词问题的形式化</strong>：</p>
<p>给定字节序列$\boldsymbol{c} = c_1c_2\cdots c_L$，找到概率最大的切分方案：
\begin{equation}
\boldsymbol{w}^* = \arg\max_{\boldsymbol{w} \in \Omega(\boldsymbol{c})} P(\boldsymbol{w}) \tag{7}
\end{equation}</p>
<p>其中$\Omega(\boldsymbol{c})$是所有能够完整覆盖$\boldsymbol{c}$的切分方案的集合。</p>
<h3 id="viterbi_1">二、Viterbi算法：动态规划解码<a class="toc-link" href="#viterbi_1" title="Permanent link">&para;</a></h3>
<h4 id="21_1">2.1 问题设定<a class="toc-link" href="#21_1" title="Permanent link">&para;</a></h4>
<p><strong>目标</strong>：给定观测序列$\boldsymbol{x} = (x_1, \ldots, x_T)$和HMM参数$\lambda$，找到最可能的状态序列：
\begin{equation}
\boldsymbol{q}^* = \arg\max_{\boldsymbol{q}} P(\boldsymbol{q} | \boldsymbol{x}, \lambda) = \arg\max_{\boldsymbol{q}} P(\boldsymbol{q}, \boldsymbol{x} | \lambda) \tag{8}
\end{equation}</p>
<p><strong>朴素方法的问题</strong>：</p>
<p>遍历所有可能的状态序列，共$N^T$种，复杂度指数级！</p>
<h4 id="22_1">2.2 动态规划的思想<a class="toc-link" href="#22_1" title="Permanent link">&para;</a></h4>
<p><strong>关键观察</strong>：最优路径的任何子路径也是最优的（最优子结构性质）。</p>
<p>定义变量$\delta_t(i)$：
\begin{equation}
\delta_t(i) = \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, q_t = s_i, x_1, \ldots, x_t | \lambda) \tag{9}
\end{equation}</p>
<p><strong>含义</strong>：到时刻$t$，以状态$s_i$结尾，观测序列为$x_1, \ldots, x_t$的所有路径中，概率最大的路径的概率。</p>
<h4 id="23-viterbi_1">2.3 Viterbi递推公式<a class="toc-link" href="#23-viterbi_1" title="Permanent link">&para;</a></h4>
<p><strong>初始化</strong>（$t = 1$）：
\begin{equation}
\delta_1(i) = \pi_i \cdot b_i(x_1), \quad i = 1, \ldots, N \tag{10}
\end{equation}
\begin{equation}
\psi_1(i) = 0 \tag{11}
\end{equation}</p>
<p><strong>递推</strong>（$t = 2, \ldots, T$）：
\begin{equation}
\delta_t(j) = \max_{1 \leq i \leq N} [\delta_{t-1}(i) \cdot a_{ij}] \cdot b_j(x_t) \tag{12}
\end{equation}
\begin{equation}
\psi_t(j) = \arg\max_{1 \leq i \leq N} [\delta_{t-1}(i) \cdot a_{ij}] \tag{13}
\end{equation}</p>
<p>其中$\psi_t(j)$记录了到达状态$j$的最优前驱状态。</p>
<p><strong>证明递推公式的正确性</strong>：</p>
<p>\begin{equation}
\begin{aligned}
\delta_t(j) &amp;= \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, q_t = s_j, x_1, \ldots, x_t | \lambda) \
&amp;= \max_{q_1, \ldots, q_{t-1}} P(q_1, \ldots, q_{t-1}, x_1, \ldots, x_{t-1} | \lambda) \
&amp;\quad \times P(q_t = s_j | q_{t-1}) \times P(x_t | q_t = s_j) \
&amp;= \max_{1 \leq i \leq N} \left[ \max_{q_1, \ldots, q_{t-2}} P(q_1, \ldots, q_{t-2}, q_{t-1} = s_i, x_1, \ldots, x_{t-1}) \right] \
&amp;\quad \times a_{ij} \times b_j(x_t) \
&amp;= \max_{1 \leq i \leq N} [\delta_{t-1}(i) \cdot a_{ij}] \cdot b_j(x_t)
\end{aligned} \tag{14}
\end{equation}</p>
<p><strong>终止</strong>：
\begin{equation}
P^<em> = \max_{1 \leq i \leq N} \delta_T(i) \tag{15}
\end{equation}
\begin{equation}
q_T^</em> = \arg\max_{1 \leq i \leq N} \delta_T(i) \tag{16}
\end{equation}</p>
<p><strong>回溯</strong>（$t = T-1, T-2, \ldots, 1$）：
\begin{equation}
q_t^<em> = \psi_{t+1}(q_{t+1}^</em>) \tag{17}
\end{equation}</p>
<h4 id="24-viterbi_1">2.4 对数空间的Viterbi算法<a class="toc-link" href="#24-viterbi_1" title="Permanent link">&para;</a></h4>
<p><strong>数值稳定性问题</strong>：连乘很多小概率会导致下溢。</p>
<p><strong>解决方法</strong>：在对数空间进行计算。</p>
<p>定义：
\begin{equation}
\tilde{\delta}_t(i) = \log \delta_t(i) \tag{18}
\end{equation}</p>
<p>递推公式变为：
\begin{equation}
\tilde{\delta}<em 1="1" N="N" _leq="\leq" i="i">t(j) = \max</em>} [\tilde{\delta<em ij="ij">{t-1}(i) + \log a</em>
\end{equation}}] + \log b_j(x_t) \tag{19</p>
<p><strong>关键变化</strong>：
- 乘法 → 加法
- $\max$操作保持不变</p>
<h3 id="unigramviterbi_1">三、应用于Unigram分词的Viterbi算法<a class="toc-link" href="#unigramviterbi_1" title="Permanent link">&para;</a></h3>
<h4 id="31-dag_1">3.1 分词的DAG表示<a class="toc-link" href="#31-dag_1" title="Permanent link">&para;</a></h4>
<p>给定字节序列$\boldsymbol{c} = c_1c_2\cdots c_L$和词表$\mathcal{V}$。</p>
<p><strong>构建DAG（有向无环图）</strong>：
- <strong>节点</strong>：位置$0, 1, 2, \ldots, L$
- <strong>边</strong>：如果$\overline{c_i c_{i+1} \cdots c_j} \in \mathcal{V}$，则存在边$(i-1) \to j$
- <strong>边权</strong>：$\log P(\overline{c_i \cdots c_j})$</p>
<p><strong>最优路径</strong>：从节点0到节点$L$的最大权重路径。</p>
<h4 id="32-viterbiforward-dp_1">3.2 前向Viterbi算法（Forward DP）<a class="toc-link" href="#32-viterbiforward-dp_1" title="Permanent link">&para;</a></h4>
<p>定义$S^*(i)$：从位置0到位置$i$的最优路径得分。</p>
<p><strong>初始化</strong>：
\begin{equation}
S^*(0) = 0 \tag{20}
\end{equation}</p>
<p><strong>递推</strong>（$i = 1, 2, \ldots, L$）：
\begin{equation}
S^<em>(i) = \max_{j &lt; i, \overline{c_{j+1}\cdots c_i} \in \mathcal{V}} [S^</em>(j) + \log P(\overline{c_{j+1}\cdots c_i})] \tag{21}
\end{equation}</p>
<p>同时记录前驱：
\begin{equation}
\text{prev}(i) = \arg\max_{j &lt; i, \overline{c_{j+1}\cdots c_i} \in \mathcal{V}} [S^*(j) + \log P(\overline{c_{j+1}\cdots c_i})] \tag{22}
\end{equation}</p>
<p><strong>回溯</strong>：
\begin{equation}
i \gets L; \quad \text{tokens} = [] \
\text{while } i &gt; 0: \
\quad j = \text{prev}(i) \
\quad \text{tokens.append}(\overline{c_{j+1}\cdots c_i}) \
\quad i \gets j \tag{23}
\end{equation}</p>
<h4 id="33-ac_1">3.3 AC自动机加速<a class="toc-link" href="#33-ac_1" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：对于每个位置$i$，需要遍历所有$j &lt; i$检查是否存在词，复杂度$O(L^2)$。</p>
<p><strong>AC自动机（Aho-Corasick Automaton）</strong>：</p>
<p>预处理词表，构建失配指针，可以在$O(L)$时间内找到所有词的终止位置。</p>
<p><strong>算法流程</strong>：
1. 预处理：构建AC自动机（$O(|\mathcal{V}| \cdot \bar{L}_w)$，$\bar{L}_w$是平均词长）
2. 扫描：用AC自动机扫描文本，找到所有可能的词$(s, e, w)$（起始、结束、词本身）
3. DP：按结束位置$e$排序，依次计算$S^*(e)$</p>
<p><strong>复杂度</strong>：$O(L \cdot m)$，其中$m$是词表中最大词长。</p>
<h4 id="34_1">3.4 伪代码实现<a class="toc-link" href="#34_1" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">viterbi_decoding</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">prob</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    text: 输入字节序列</span>
<span class="sd">    vocab: 词表（Trie或AC自动机）</span>
<span class="sd">    prob: 词的对数概率函数，prob(w) = log P(w)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># 初始化</span>
    <span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)]</span> <span class="o">*</span> <span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># 使用AC自动机找到所有可能的词</span>
    <span class="n">candidates</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">find_all_words</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>  <span class="c1"># [(start, end, word), ...]</span>

    <span class="c1"># 按结束位置分组</span>
    <span class="n">candidates_by_end</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">:</span>
        <span class="n">candidates_by_end</span><span class="p">[</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">word</span><span class="p">))</span>

    <span class="c1"># 动态规划</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">candidates_by_end</span><span class="p">[</span><span class="n">e</span><span class="p">]:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+</span> <span class="n">prob</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">S</span><span class="p">[</span><span class="n">e</span><span class="p">]:</span>
                <span class="n">S</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                <span class="n">prev</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span>

    <span class="c1"># 回溯</span>
    <span class="k">if</span> <span class="n">S</span><span class="p">[</span><span class="n">L</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># 无法切分</span>

    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">L</span>
    <span class="k">while</span> <span class="n">pos</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">word</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">s</span>

    <span class="k">return</span> <span class="n">tokens</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>

<h3 id="-_1">四、前向-后向算法对比<a class="toc-link" href="#-_1" title="Permanent link">&para;</a></h3>
<h4 id="41-forward-algorithm_1">4.1 前向算法（Forward Algorithm）<a class="toc-link" href="#41-forward-algorithm_1" title="Permanent link">&para;</a></h4>
<p><strong>目的</strong>：计算观测序列的概率$P(\boldsymbol{x}|\lambda)$。</p>
<p>定义前向变量：
\begin{equation}
\alpha_t(i) = P(x_1, \ldots, x_t, q_t = s_i | \lambda) \tag{24}
\end{equation}</p>
<p><strong>递推</strong>：
\begin{equation}
\alpha_t(j) = \left[\sum_{i=1}^{N} \alpha_{t-1}(i) a_{ij}\right] b_j(x_t) \tag{25}
\end{equation}</p>
<p><strong>与Viterbi的对比</strong>：
- 前向：$\sum$（求和，所有路径）
- Viterbi：$\max$（最大值，最优路径）</p>
<h4 id="42-backward-algorithm_1">4.2 后向算法（Backward Algorithm）<a class="toc-link" href="#42-backward-algorithm_1" title="Permanent link">&para;</a></h4>
<p>定义后向变量：
\begin{equation}
\beta_t(i) = P(x_{t+1}, \ldots, x_T | q_t = s_i, \lambda) \tag{26}
\end{equation}</p>
<p><strong>递推</strong>（从$T$到$1$）：
\begin{equation}
\beta_t(i) = \sum_{j=1}^{N} a_{ij} b_j(x_{t+1}) \beta_{t+1}(j) \tag{27}
\end{equation}</p>
<h4 id="43-_1">4.3 前向-后向的应用<a class="toc-link" href="#43-_1" title="Permanent link">&para;</a></h4>
<p><strong>边缘概率</strong>：
\begin{equation}
P(q_t = s_i | \boldsymbol{x}, \lambda) = \frac{\alpha_t(i) \beta_t(i)}{P(\boldsymbol{x}|\lambda)} = \frac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^{N} \alpha_T(j)} \tag{28}
\end{equation}</p>
<p><strong>边缘概率 vs Viterbi路径</strong>：
- 边缘概率：考虑所有路径，给出每个位置每个状态的概率
- Viterbi：只考虑最优路径</p>
<h3 id="viterbi-decodingviterbi-sampling_2">五、从Viterbi Decoding到Viterbi Sampling<a class="toc-link" href="#viterbi-decodingviterbi-sampling_2" title="Permanent link">&para;</a></h3>
<h4 id="51_1">5.1 采样的动机<a class="toc-link" href="#51_1" title="Permanent link">&para;</a></h4>
<p><strong>Viterbi Decoding的局限</strong>：
1. <strong>确定性</strong>：总是输出同一个结果
2. <strong>过度自信</strong>：即使多个切分方案概率接近，也只返回一个
3. <strong>训练-推理不一致</strong>：训练时可能需要多样性</p>
<p><strong>Viterbi Sampling的目标</strong>：
- 依概率采样切分方案
- 概率高的方案被采样到的概率大
- 保持Viterbi Decoding的效率（线性复杂度）</p>
<h4 id="52-n-best_1">5.2 朴素采样方法：N-best搜索<a class="toc-link" href="#52-n-best_1" title="Permanent link">&para;</a></h4>
<p><strong>Subword Regularization的做法</strong>：</p>
<ol>
<li>找到概率最高的$n$个切分方案$\boldsymbol{w}_1^<em>, \ldots, \boldsymbol{w}_n^</em>$</li>
<li>计算权重：
   \begin{equation}
   p_i = \frac{P(\boldsymbol{w}_i^<em>)^\alpha}{\sum_{j=1}^{n} P(\boldsymbol{w}_j^</em>)^\alpha} \tag{29}
   \end{equation}</li>
<li>按$p_i$进行采样</li>
</ol>
<p><strong>N-best Viterbi算法</strong>：</p>
<p>维护每个位置的top-$n$路径，复杂度$O(nLm)$，其中$m$是最大词长。</p>
<p><strong>缺点</strong>：
- 效率低（$n$倍复杂度）
- 需要选择$n$（太小：覆盖不足；太大：浪费）</p>
<h4 id="53-viterbi-sampling_1">5.3 Viterbi Sampling的核心思想<a class="toc-link" href="#53-viterbi-sampling_1" title="Permanent link">&para;</a></h4>
<p><strong>问题重新表述</strong>：</p>
<p>Viterbi Decoding的判据是：
\begin{equation}
\text{if } \text{score}<em _text_old="\text{old">{\text{new}} &gt; \text{score}</em>
\end{equation}}} \text{ then accept new} \tag{30</p>
<p><strong>随机化判据</strong>：
\begin{equation}
\text{if } \varepsilon &lt; \sigma(\alpha(\text{score}<em _text_old="\text{old">{\text{new}} - \text{score}</em>
\end{equation}}})) \text{ then accept new} \tag{31</p>
<p>其中：
- $\varepsilon \sim U[0,1]$：均匀随机数
- $\sigma(x) = 1/(1 + e^{-x})$：Sigmoid函数
- $\alpha &gt; 0$：温度参数</p>
<p><strong>性质</strong>：
1. 当$\text{score}<em _text_old="\text{old">{\text{new}} &gt; \text{score}</em>$时，接受概率$&gt; 0.5$
2. 分数差距越大，接受概率越极端（接近0或1）
3. $\alpha \to \infty$时，退化为Viterbi Decoding}</p>
<h4 id="54-viterbi-sampling_1">5.4 Viterbi Sampling的递推公式<a class="toc-link" href="#54-viterbi-sampling_1" title="Permanent link">&para;</a></h4>
<p><strong>修改Viterbi递推</strong>：</p>
<p>原本：
\begin{equation}
\text{if } S[j] + s(w) &gt; S[i] \text{ then} \
\quad S[i] = S[j] + s(w) \
\quad \text{prev}[i] = (j, w) \tag{32}
\end{equation}</p>
<p>随机化为：
\begin{equation}
\text{score}<em _text_old="\text{old">{\text{new}} = S[j] + s(w) \
\text{score}</em> = S[i] \
p_{\text{accept}} = \sigma(\alpha(\text{score}}<em _text_old="\text{old">{\text{new}} - \text{score}</em>)) \
\text{if } \varepsilon &lt; p_{\text{accept}} \text{ then} \
\quad S[i] = \text{score}_{\text{new}} \
\quad \text{prev}[i] = (j, w) \tag{33}
\end{equation}}</p>
<p><strong>初始化</strong>：
\begin{equation}
S[i] = -\infty, \quad i = 1, \ldots, L \tag{34}
\end{equation}</p>
<h4 id="55_1">5.5 保序性分析<a class="toc-link" href="#55_1" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：如果切分方案$\boldsymbol{w}_1$的得分高于$\boldsymbol{w}_2$，即$P(\boldsymbol{w}_1) &gt; P(\boldsymbol{w}_2)$，那么在采样中$\boldsymbol{w}_1$被采到的概率是否也更高？</p>
<p><strong>Viterbi Sampling的保序性</strong>：</p>
<p>不是严格保序的，但近似保序。</p>
<p><strong>分析</strong>：考虑两条路径$A$和$B$，得分分别为$s_A$和$s_B$，$s_A &gt; s_B$。</p>
<p>如果两条路径在位置$i$竞争：
\begin{equation}
P(\text{选择}A) = \sigma(\alpha(s_A - s_B)) &gt; 0.5 \tag{35}
\end{equation}</p>
<p>但是，如果路径涉及多步竞争，每步都有随机性，最终的排序可能改变。</p>
<p><strong>实证观察</strong>：在实际应用中，得分高的方案确实更常被采样到，尤其是当$\alpha$较大时。</p>
<h3 id="_13">六、数值稳定性与实现技巧<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<h4 id="61-logsumexp_1">6.1 LogSumExp技巧<a class="toc-link" href="#61-logsumexp_1" title="Permanent link">&para;</a></h4>
<p>前向算法涉及到求和：
\begin{equation}
\alpha_t(j) = \left[\sum_{i=1}^{N} \alpha_{t-1}(i) a_{ij}\right] b_j(x_t) \tag{36}
\end{equation}</p>
<p>在对数空间：
\begin{equation}
\log \alpha_t(j) = \log \left[\sum_{i=1}^{N} e^{\log \alpha_{t-1}(i) + \log a_{ij}}\right] + \log b_j(x_t) \tag{37}
\end{equation}</p>
<p>直接计算$\sum e^{x_i}$可能溢出。使用LogSumExp：
\begin{equation}
\text{LSE}(x_1, \ldots, x_N) = x_{\max} + \log \sum_{i=1}^{N} e^{x_i - x_{\max}} \tag{38}
\end{equation}</p>
<p>其中$x_{\max} = \max_i x_i$。</p>
<h4 id="62-sigmoid_1">6.2 Sigmoid的数值稳定计算<a class="toc-link" href="#62-sigmoid_1" title="Permanent link">&para;</a></h4>
<p>Sigmoid函数：
\begin{equation}
\sigma(x) = \frac{1}{1 + e^{-x}} \tag{39}
\end{equation}</p>
<p><strong>问题</strong>：
- 当$x$很大时，$e^{-x} \to 0$，但计算$e^{-x}$可能下溢
- 当$x$很小时，$e^{-x} \to \infty$，上溢</p>
<p><strong>稳定计算</strong>：
\begin{equation}
\sigma(x) = \begin{cases}
\frac{1}{1 + e^{-x}}, &amp; x \geq 0 \
\frac{e^x}{1 + e^x}, &amp; x &lt; 0
\end{cases} \tag{40}
\end{equation}</p>
<p><strong>推导</strong>：
\begin{equation}
\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{e^x + 1} = \frac{e^x}{1 + e^x} \tag{41}
\end{equation}</p>
<p>两种形式等价，但数值稳定性不同。</p>
<h4 id="63_1">6.3 随机数生成的效率<a class="toc-link" href="#63_1" title="Permanent link">&para;</a></h4>
<p>Viterbi Sampling每次DP更新都需要一个随机数$\varepsilon \sim U[0,1]$。</p>
<p><strong>优化方法</strong>：
1. <strong>批量生成</strong>：预先生成一大批随机数
2. <strong>快速RNG</strong>：使用Xorshift等快速随机数生成器
3. <strong>向量化</strong>：使用SIMD指令并行生成</p>
<p><strong>Python示例</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 预生成随机数</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">random_pool</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">rand_idx</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_random</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">rand_idx</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">random_pool</span><span class="p">[</span><span class="n">rand_idx</span><span class="p">]</span>
    <span class="n">rand_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">rand_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">random_pool</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span>
</code></pre></div>

<h3 id="_14">七、采样质量分析<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h3>
<h4 id="71_1">7.1 期望值分析<a class="toc-link" href="#71_1" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：Viterbi Sampling采样到某个切分方案$\boldsymbol{w}$的概率是多少？</p>
<p>这是一个复杂的问题，因为采样过程涉及多步随机决策。</p>
<p><strong>近似分析</strong>：</p>
<p>假设DP过程中，每个位置独立做决策（这是一个简化假设）。设位置$i$有$k_i$个候选词结尾，得分分别为$s_1^{(i)}, \ldots, s_{k_i}^{(i)}$。</p>
<p>采样到得分为$s_j^{(i)}$的候选的概率约为：
\begin{equation}
p_j^{(i)} \approx \frac{e^{\alpha s_j^{(i)}}}{\sum_{\ell=1}^{k_i} e^{\alpha s_\ell^{(i)}}} \tag{42}
\end{equation}</p>
<p>这类似于Softmax！</p>
<p>整条路径被采样到的概率（近似）：
\begin{equation}
P_{\text{sample}}(\boldsymbol{w}) \approx \prod_{i \in \boldsymbol{w}} p_j^{(i)} \tag{43}
\end{equation}</p>
<h4 id="72_1">7.2 与真实概率的关系<a class="toc-link" href="#72_1" title="Permanent link">&para;</a></h4>
<p><strong>理想情况</strong>：我们希望采样概率正比于真实概率：
\begin{equation}
P_{\text{sample}}(\boldsymbol{w}) \propto P(\boldsymbol{w}) = e^{s(\boldsymbol{w})} \tag{44}
\end{equation}</p>
<p>其中$s(\boldsymbol{w}) = \sum_{w_i \in \boldsymbol{w}} \log P(w_i)$。</p>
<p><strong>Viterbi Sampling的偏差</strong>：</p>
<p>由于多次二选一的稀释效应（后面的候选"占便宜"），Viterbi Sampling的采样概率与真实概率不完全一致。</p>
<p><strong>文章的改进方向</strong>：
- 使用水塘采样（Reservoir Sampling）
- 缓存累积概率
- 详见下一篇文章《随机分词再探》</p>
<h4 id="73-alpha_1">7.3 温度参数$\alpha$的影响<a class="toc-link" href="#73-alpha_1" title="Permanent link">&para;</a></h4>
<p><strong>$\alpha \to 0$</strong>：
\begin{equation}
\sigma(\alpha \Delta s) \to \begin{cases}
1, &amp; \Delta s &gt; 0 \
0.5, &amp; \Delta s = 0 \
0, &amp; \Delta s &lt; 0
\end{cases} \tag{45}
\end{equation}</p>
<p>接近均匀采样（所有正分数差都以概率0.5接受）。</p>
<p><strong>$\alpha \to \infty$</strong>：
\begin{equation}
\sigma(\alpha \Delta s) \to \begin{cases}
1, &amp; \Delta s &gt; 0 \
0.5, &amp; \Delta s = 0 \
0, &amp; \Delta s &lt; 0
\end{cases} \tag{46}
\end{equation}</p>
<p>退化为Viterbi Decoding（总是选择分数高的）。</p>
<p><strong>$\alpha = 0.1$（经验值）</strong>：
- 保持一定的随机性
- 同时偏向高分方案
- 在训练中引入正则化效果</p>
<h3 id="bpe-dropout_1">八、与BPE Dropout的对比<a class="toc-link" href="#bpe-dropout_1" title="Permanent link">&para;</a></h3>
<h4 id="81-bpe-dropout_1">8.1 BPE Dropout<a class="toc-link" href="#81-bpe-dropout_1" title="Permanent link">&para;</a></h4>
<p>BPE（Byte Pair Encoding）使用贪心merge策略，每次选择频率最高的字节对合并。</p>
<p><strong>BPE Dropout</strong>：在merge过程中，以概率$p$随机跳过某些merge操作。</p>
<p><strong>算法</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">bpe_dropout</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">merge_rules</span><span class="p">,</span> <span class="n">p_dropout</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">rule</span> <span class="ow">in</span> <span class="n">merge_rules</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">p_dropout</span><span class="p">:</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">apply_merge</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">rule</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span>
</code></pre></div>

<h4 id="82_1">8.2 对比<a class="toc-link" href="#82_1" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方面</th>
<th>Viterbi Sampling</th>
<th>BPE Dropout</th>
</tr>
</thead>
<tbody>
<tr>
<td>适用模型</td>
<td>Unigram</td>
<td>BPE</td>
</tr>
<tr>
<td>随机化方式</td>
<td>DP过程中随机选择</td>
<td>Merge过程中随机跳过</td>
</tr>
<tr>
<td>概率保证</td>
<td>近似按概率采样</td>
<td>无明确概率保证</td>
</tr>
<tr>
<td>实现复杂度</td>
<td>中等</td>
<td>简单</td>
</tr>
<tr>
<td>效率</td>
<td>与确定性版本相近</td>
<td>与确定性版本相同</td>
</tr>
</tbody>
</table>
<h4 id="83-subword-regularization_1">8.3 Subword Regularization对比<a class="toc-link" href="#83-subword-regularization_1" title="Permanent link">&para;</a></h4>
<p><strong>方法</strong>：找top-$n$个切分方案，按概率加权采样。</p>
<p><strong>优点</strong>：
- 精确按概率采样
- 理论保证强</p>
<p><strong>缺点</strong>：
- 效率低（$n$倍复杂度）
- 需要调参$n$</p>
<p><strong>Viterbi Sampling的优势</strong>：
- 接近原始效率
- 自动适应（无需调$n$）
- 实现简单</p>
<h3 id="_15">九、实际应用与效果<a class="toc-link" href="#_15" title="Permanent link">&para;</a></h3>
<h4 id="91_1">9.1 训练数据增强<a class="toc-link" href="#91_1" title="Permanent link">&para;</a></h4>
<p><strong>应用场景</strong>：训练语言模型时，每个epoch使用不同的分词结果。</p>
<p><strong>效果</strong>：
- 增强模型鲁棒性
- 减少对特定分词的过拟合
- 改善out-of-vocabulary (OOV)处理</p>
<p><strong>示例</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 训练时</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="c1"># 每次使用随机分词</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div>

<h4 id="92_1">9.2 速度对比<a class="toc-link" href="#92_1" title="Permanent link">&para;</a></h4>
<p>根据文章的实验数据：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>确定性速度</th>
<th>随机性速度</th>
<th>速度比</th>
</tr>
</thead>
<tbody>
<tr>
<td>SP-BPE</td>
<td>1.36M bytes/sec</td>
<td>1.25M bytes/sec</td>
<td>92%</td>
</tr>
<tr>
<td><strong>SP-Unigram</strong></td>
<td><strong>5.65M bytes/sec</strong></td>
<td><strong>1.28M bytes/sec</strong></td>
<td><strong>23%</strong></td>
</tr>
<tr>
<td>BytePiece (Viterbi Sampling)</td>
<td>1.95M bytes/sec</td>
<td>1.36M bytes/sec</td>
<td><strong>70%</strong></td>
</tr>
</tbody>
</table>
<p><strong>观察</strong>：
- Subword Regularization (SP-Unigram随机版)速度下降到23%
- Viterbi Sampling保持70%的速度
- <strong>3倍加速</strong>！</p>
<h4 id="93_1">9.3 分词多样性<a class="toc-link" href="#93_1" title="Permanent link">&para;</a></h4>
<p><strong>实验</strong>（从文章）：</p>
<div class="highlight"><pre><span></span><code><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天天气不错&#39;</span>

<span class="c1"># 确定性分词</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>

<span class="c1"># Viterbi Sampling (alpha=0.1)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6&#39;, b&#39;\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9\xe6\xb0\x94&#39;, b&#39;\xe4\xb8&#39;, b&#39;\x8d&#39;, b&#39;\xe9\x94&#39;, b&#39;\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9&#39;, b&#39;\xe6\xb0\x94&#39;, b&#39;\xe4\xb8\x8d\xe9\x94\x99&#39;]</span>
<span class="c1"># [b&#39;\xe4\xbb&#39;, b&#39;\x8a\xe5\xa4\xa9&#39;, b&#39;\xe5\xa4\xa9&#39;, b&#39;\xe6\xb0\x94\xe4\xb8\x8d&#39;, b&#39;\xe9\x94&#39;, b&#39;\x99&#39;]</span>
</code></pre></div>

<p><strong>分析</strong>：
- 确定性版本总是输出同一结果
- 随机版本产生多种切分
- 某些常见切分出现频率更高（如"今天"、"天气"）</p>
<h3 id="_16">十、理论保证与局限性<a class="toc-link" href="#_16" title="Permanent link">&para;</a></h3>
<h4 id="101-viterbi-sampling_1">10.1 Viterbi Sampling的理论性质<a class="toc-link" href="#101-viterbi-sampling_1" title="Permanent link">&para;</a></h4>
<p><strong>性质1（近似保序）</strong>：高分切分方案被采样到的概率更大。</p>
<p><strong>性质2（效率）</strong>：复杂度与Viterbi Decoding相同，都是$O(Lm)$。</p>
<p><strong>性质3（可控随机性）</strong>：通过$\alpha$控制随机性强度。</p>
<h4 id="102_1">10.2 未解决的问题<a class="toc-link" href="#102_1" title="Permanent link">&para;</a></h4>
<p><strong>问题1</strong>：采样概率的精确表达式是什么？</p>
<p>目前只有近似分析，缺乏精确的概率分布。</p>
<p><strong>问题2</strong>：如何保证采样的无偏性？</p>
<p>Viterbi Sampling由于稀释效应，不是严格按$P(\boldsymbol{w})$采样。</p>
<p><strong>解决方向</strong>：
- 水塘采样（下一篇文章）
- 基于CDF的逆变换采样</p>
<h4 id="103_1">10.3 适用场景<a class="toc-link" href="#103_1" title="Permanent link">&para;</a></h4>
<p><strong>适合</strong>：
- 需要快速随机分词
- 训练数据增强
- 轻量级正则化</p>
<p><strong>不适合</strong>：
- 需要精确概率分布的场景
- 理论分析要求严格无偏采样</p>
<h3 id="_17">十一、总结<a class="toc-link" href="#_17" title="Permanent link">&para;</a></h3>
<p>Viterbi算法是序列标注和分词问题的核心算法，通过动态规划在线性时间内找到最优路径。</p>
<p><strong>Viterbi Decoding</strong>：
\begin{equation}
S^<em>(i) = \max_{j &lt; i} [S^</em>(j) + \log P(w_{j+1:i})] \tag{47}
\end{equation}</p>
<p><strong>Viterbi Sampling（初步版本）</strong>：
\begin{equation}
\text{accept new if } \varepsilon &lt; \sigma(\alpha \cdot \Delta s) \tag{48}
\end{equation}</p>
<p><strong>优势</strong>：
- ✅ 保持线性复杂度
- ✅ 引入随机性
- ✅ 实现简单
- ✅ 速度快（相比Subword Regularization）</p>
<p><strong>局限</strong>：
- ❌ 采样概率不精确
- ❌ 稀释效应（后续候选占便宜）
- ❌ 理论保证不足</p>
<p><strong>下一步</strong>：在下一篇文章《随机分词再探》中，我们将解决稀释效应，通过水塘采样实现完美采样，达到与Subword Regularization等价的效果，同时保持高效率。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="大词表语言模型在续写任务上的一个问题及对策.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#248 大词表语言模型在续写任务上的一个问题及对策</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="自然数集中-n-ab-c-时-a-b-c-的最小值.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#250 自然数集中 N = ab + c 时 a + b + c 的最小值</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#viterbi-decodingviterbi-sampling">随机分词浅探：从Viterbi Decoding到Viterbi Sampling</a><ul>
<li><a href="#_1">要点分析</a></li>
<li><a href="#_2">已有方案</a></li>
<li><a href="#_3">个人思路</a></li>
<li><a href="#_4">简单测试</a></li>
<li><a href="#_5">文章小结</a></li>
<li><a href="#_6">完整数学推导与理论分析</a><ul>
<li><a href="#hmm">一、隐马尔可夫模型(HMM)基础</a></li>
<li><a href="#viterbi">二、Viterbi算法：动态规划解码</a></li>
<li><a href="#unigramviterbi">三、应用于Unigram分词的Viterbi算法</a></li>
<li><a href="#-">四、前向-后向算法对比</a></li>
<li><a href="#viterbi-decodingviterbi-sampling_1">五、从Viterbi Decoding到Viterbi Sampling</a></li>
<li><a href="#_7">六、数值稳定性与实现技巧</a></li>
<li><a href="#_8">七、采样质量分析</a></li>
<li><a href="#bpe-dropout">八、与BPE Dropout的对比</a></li>
<li><a href="#_9">九、实际应用与效果</a></li>
<li><a href="#_10">十、理论保证与局限性</a></li>
<li><a href="#_11">十一、总结</a></li>
</ul>
</li>
<li><a href="#_12">完整数学推导与理论分析</a><ul>
<li><a href="#hmm_1">一、隐马尔可夫模型(HMM)基础</a></li>
<li><a href="#viterbi_1">二、Viterbi算法：动态规划解码</a></li>
<li><a href="#unigramviterbi_1">三、应用于Unigram分词的Viterbi算法</a></li>
<li><a href="#-_1">四、前向-后向算法对比</a></li>
<li><a href="#viterbi-decodingviterbi-sampling_2">五、从Viterbi Decoding到Viterbi Sampling</a></li>
<li><a href="#_13">六、数值稳定性与实现技巧</a></li>
<li><a href="#_14">七、采样质量分析</a></li>
<li><a href="#bpe-dropout_1">八、与BPE Dropout的对比</a></li>
<li><a href="#_15">九、实际应用与效果</a></li>
<li><a href="#_16">十、理论保证与局限性</a></li>
<li><a href="#_17">十一、总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>