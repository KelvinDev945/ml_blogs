<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VQ一下Key，Transformer的复杂度就变成线性了 | ML & Math Blog Posts</title>
    <meta name="description" content="VQ一下Key，Transformer的复杂度就变成线性了&para;
原文链接: https://spaces.ac.cn/archives/9844
发布日期: 

Efficient Transformer，泛指一切致力于降低Transformer的二次复杂度的工作，开始特指针对Attention的改进，后来更一般的思路，如傅里叶变换、线性RNN等，也被归入这个范畴。不得不说，为了降低Tra...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #117 VQ一下Key，Transformer的复杂度就变成线性了
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#117</span>
                VQ一下Key，Transformer的复杂度就变成线性了
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2023-11-09</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=量子化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 量子化</span>
                </a>
                
                <a href="../index.html?tags=编码" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 编码</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="vqkeytransformer">VQ一下Key，Transformer的复杂度就变成线性了<a class="toc-link" href="#vqkeytransformer" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9844">https://spaces.ac.cn/archives/9844</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>Efficient Transformer，泛指一切致力于降低Transformer的二次复杂度的工作，开始特指针对Attention的改进，后来更一般的思路，如傅里叶变换、线性RNN等，也被归入这个范畴。不得不说，为了降低Transformer的二次复杂度，各路大牛可谓是“八仙过海，各显神通”，各种神奇的思路“百花齐放”，笔者也从中学习到了不少理论知识。然而，尽管Efficient Transformer在理论上是精彩的，但实际上该领域一直都是不愠不火的状态，并没有实际表现十分出色的模型，在LLM火爆的今天，甚至已经逐渐淡出了大家的视野，也淡出了笔者的兴趣范围。</p>
<p>不过，最近有一篇论文<a href="https://papers.cool/arxiv/2309.16354">《Transformer-VQ: Linear-Time Transformers via Vector Quantization》</a>，却让笔者为之拍案叫绝。作者非常高明地洞察到，只需要对标准Attention的Key做一下VQ（Vector Quantize），复杂度就会自动降低为线性！这种线性化思路保留了标准Attention的形式，是标准Attention到线性Attention的一个完美过渡，同时最大程度上保留了标准Attention的能力。</p>
<h2 id="_1">高效难题<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>说起来，本站也算是比较早关注Efficient Transformer相关工作了，最早可以追溯到2019年解读Sparse Transformer的一篇博客<a href="/archives/6853">《为节约而生：从标准Attention到稀疏Attention》</a>。此后，陆续写的关于Efficient Transformer的其他博文还有</p>
<blockquote>
<p><a href="/archives/7546">《线性Attention的探索：Attention必须有个Softmax吗？》</a></p>
<p><a href="/archives/7921">《Performer：用随机投影将Attention的复杂度线性化》</a></p>
<p><a href="/archives/8180">《Nyströmformer：基于矩阵分解的线性化Attention方案》</a></p>
<p><a href="/archives/8338">《Transformer升级之路：3、从Performer到线性Attention》</a></p>
<p><a href="/archives/8610">《线性Transformer应该不是你要等的那个模型》</a></p>
<p><a href="/archives/8934">《FLASH：可能是近来最有意思的高效Transformer设计》</a></p>
<p><a href="/archives/9554">《Google新作试图“复活”RNN：RNN能否再次辉煌？》</a></p>
</blockquote>
<p>然而，正如本文开头所说，尽管Efficient Transformer已有不少工作，也曾被大家寄予厚望，但实际上该领域一直都没什么能“出圈”的作品，这其中的原因可能是：</p>
<blockquote>
<p>1、不少Efficient Transformer的提速以牺牲效果为代价；</p>
<p>2、很多Efficient Transformer的复杂度降低仅仅是理论上的，实际使用提升不明显；</p>
<p>3、有些Efficient Transformer难以用来训练Causal LM，所以在LLM流行的今天就没有了用武之地；</p>
<p>4、Flash Attention的出现表明即便是标准的Transformer仍有很大的提速空间。</p>
</blockquote>
<h2 id="vq">VQ一下<a class="toc-link" href="#vq" title="Permanent link">&para;</a></h2>
<p>那么，Transformer-VQ为何又具备的“出圈”潜力？</p>
<p>简单来说，Transformer-VQ就是对Attention的Key向量序列进行了“聚类”，并用所属类的类别中心近似原向量，然后Attention的复杂度就变成线性了。也就是说，Transformer-VQ仅仅改变了Key的形似，其余部分（理论上）完全不变，所以这是一种对Attention改动非常小的线性化方案，也能非常清楚体现出线性化后损失的精度在哪里（即用类别中心近似原向量的差距）。</p>
<p>铺垫得有点多了，现在我们正式介绍Transformer-VQ。首先，我们假设$Q,K\in\mathbb{R}^{n\times d_k},V\in\mathbb{R}^{n\times d_v}$，标准Attention就是<br />
\begin{equation}softmax\left(QK^{\top}\right)V\end{equation}<br />
简单起见，这里省略了scale factor。Transformer-VQ改为<br />
\begin{equation}softmax\left(Q\hat{K}^{\top}\right)V,\quad \hat{K} = \color{skyblue}{\mathcal{VQ}}(K, C)\label{eq:vq-att}\end{equation}<br />
其中$C\in\mathbb{R}^{c\times d_k}$是训练参数，也是VQ的编码表（Codebook）。对了，这里的“VQ”就是指VQ-VAE中的VQ，不了解的读者可以移步参考<a href="/archives/6760">《VQ-VAE的简明介绍：量子化自编码器》</a>和<a href="/archives/9826">《简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE》</a>，这里不重复介绍了。总之，经过$\color{skyblue}{\mathcal{VQ}}$之后，最直接的表现就是$K$的每个向量都变成了$C$中与之最相近的那个，这意味着$\hat{K}$的每个向量都是$C$的向量之一，用数学的语言就是说$K\in\mathbb{R}^{n\times d_k}$变成了$\hat{K}\in C^n$。</p>
<h2 id="encoder">Encoder<a class="toc-link" href="#encoder" title="Permanent link">&para;</a></h2>
<p>当然，直接按照式$\eqref{eq:vq-att}$去实现Transformer-VQ的话，复杂度还是二次的，但由于$\hat{K}$的每个向量都是$C$的向量之一，所以我们可以先算$\exp\left(QC^{\top}\right)$，然后从中“ <em>挑出</em> ”$\exp\left(Q\hat{K}{}^{\top}\right)$对应的结果，而由于$C$的大小是固定的，所以关键运算$QC^{\top}$的复杂度是线性的，这就是Transformer-VQ能线性化的原理（我们不妨称为“挑出”技巧）。</p>
<p>作为铺垫，我们先考虑双向注意力的Encoder情形。由于<br />
\begin{equation}softmax\left(QK^{\top}\right)V = \frac{\exp\left(QK^{\top}\right)V}{\exp\left(QK^{\top}\right)1_{n\times 1}}\label{eq:softmax-qkv}\end{equation}<br />
这里$1_{n\times 1}$指的是$n\times 1$大小的全1矩阵，分母可以视为分子的一个特殊形式，所以我们只需要考虑分子$\exp\left(QK^{\top}\right)V$。由于$\hat{K}$的每个向量都是$C$中之一，所以我们可以构建一个one hot矩阵$\Delta\in \{0,1\}^{n\times c}$，其中$\Delta_i\in\{0,1\}^c$是一个one hot向量，如果1所在的维度为$j$，那么$\hat{K}_i = C_j$，于是$\hat{K}=\Delta C$。</p>
<p>于是对于Transformer-VQ来说有<br />
\begin{equation}\exp\left(Q\hat{K}{}^{\top}\right)V = \exp\left(QC^{\top}\Delta^{\top}\right)V = \exp\left(QC^{\top}\right)\Delta^{\top}V = \exp\left(QC^{\top}\right)(\Delta^{\top}V)\end{equation}<br />
很明显，这里最关键的地方就是第二个等号！对于one hot矩阵$\Delta$，右乘以它的转置可以从$\exp$中分离出来， <em>这就是原理中的“挑出”技巧的数学表述</em> 。分离出来之后，由于矩阵乘法结合律，$\Delta^{\top}$可以先跟$V$相乘，得到一个$c\times d_v$的矩阵，而$\exp\left(QC^{\top}\right)$是一个$n\times c$的矩阵，乘以$\Delta^{\top}V$就得到一个$n\times d_v$的矩阵，总的理论复杂度是$\mathcal{O}(ncd_k + ncd_v + ncd_v) = \mathcal{O}(n)$。</p>
<p>最后，根据式$\eqref{eq:softmax-qkv}$，将$\exp\left(Q\hat{K}{}^{\top}\right)V$的结果代入去，就可以计算完整的Attention结果（可能还要加一些避免溢出的细节），整个过程可以在线性复杂度内完成。</p>
<h2 id="decoder">Decoder<a class="toc-link" href="#decoder" title="Permanent link">&para;</a></h2>
<p>现在我们来考虑单向注意力的Decoder，这是训练生成模型的关键，也是当前LLM的基础。有了Encoder的铺垫后，Decoder理解起来也就没那么困难了。假设$Q_i, \hat{K}<em i="i" j_leq="j\leq">j \in \mathbb{R}^{1\times d_k}, V_j\in\mathbb{R}^{1\times d_v}$是向量序列$Q,\hat{K},V$的行向量之一，那么对于Decoder的分子有<br />
\begin{equation}\begin{aligned}<br />
O_i =&amp;\, \sum</em>}\exp\left(Q_i\hat{K}{<em i="i" j_leq="j\leq">j^{\top}\right)V_j = \sum</em>\right)V_j \\}\exp\left(Q_i C^{\top}\Delta_j^{\top<br />
=&amp;\, \sum_{j\leq i}\exp\left(Q_i C^{\top}\right)\Delta_j^{\top}V_j = \exp\left(Q_i C^{\top}\right)\sum_{j\leq i}\Delta_j^{\top}V_j<br />
\end{aligned}\end{equation}<br />
如果$c\times d_v$不大，那么最后的式子可以直接用$\text{cumsum}$算子完成，不过一般情况下，尤其是Multi-Heaad时，为了节省显存，通常是跟<a href="/archives/7546">《线性Attention的探索：Attention必须有个Softmax吗？》</a>中的“自回归生成”一节一样，转为RNN来递归计算，即设$U_i = \sum_{j\leq i}\Delta_j^{\top}V_j\in\mathbb{R}^{c\times d_v}$，那么<br />
\begin{equation}O_i = \exp\left(Q_i C^{\top}\right)U_i,\quad U_i = U_{i-1} + \Delta_i^{\top}V_i<br />
\end{equation}<br />
在推理阶段这样step by step递归计算自然是没问题，但训练阶段step by step的话可能会比较慢，我们可以改为block by block来加速：不失一般性，设$n=lm$，$l$代表block_size，$m$代表block数目，block切片$[il:(i+1)l]$简写为$[i]$，那么<br />
\begin{equation}\begin{aligned}<br />
O_{[i]} =&amp;\, \exp\left(Q_{[i]}\hat{K}{}<em _i_="[i]">{[i]}^{\top} + M\right)V</em>} + \sum_{j\lt i}\exp\left(Q_{[i]}\hat{K}{<em _j_="[j]">{[j]}^{\top}\right)V</em> \\<br />
=&amp;\, \exp\left(Q_{[i]}\hat{K}{}<em _i_="[i]">{[i]}^{\top} + M\right)V</em> \\} + \sum_{j\lt i}\exp\left(Q_{[i]}C^{\top}\Delta_{[j]}^{\top}\right)V_{[j]<br />
=&amp;\, \exp\left(Q_{[i]}\hat{K}{}<em _i_="[i]">{[i]}^{\top} + M\right)V</em> \\} + \exp\left(Q_{[i]}C^{\top}\right)\sum_{j\lt i}\Delta_{[j]}^{\top}V_{[j]<br />
\end{aligned}\end{equation}<br />
其中$M\in\{-\infty,0\}^{l\times l}$是下三角的Attention Mask，即当$i \geq j$时$M_{i,j}=0$，否则$M_{i,j}=-\infty$。于是记$U_i = \sum_{j\lt i}\Delta_{[j]}^{\top}V_{[j]}$后，我们有<br />
\begin{equation}O_{[i]} = \exp\left(Q_{[i]}\hat{K}{}<em _i_="[i]">{[i]}^{\top} + M\right)V</em>} + \exp\left(Q_{[i]}C^{\top}\right)U_{i-1},\quad U_i = U_{i-1} + \Delta_{[i]}^{\top}V_{[i]<br />
\end{equation}<br />
这样我们就将递归步数减少为$m$了，可以在保证线性效率的同时，更充分发挥硬件的并行能力。用同样的方式也可以计算分母，最后相除得到完整的Attention结果</p>
<h2 id="_2">局域增强<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>就这样完了？并不是，如果仅仅是这样的话，Transformer-VQ可能跟以往基于矩阵分解的Kernelized Attention如Performer并没有太多区别。当序列长度$n$远大于编码表大小$c$时，由抽屉原理我们知道部分编码向量必然会反复出现，甚至可以合理猜测所有编码向量应该会均匀分布在整个序列中。这样一来，邻近token的Attention就会跟远处某些token的Attention一样，也就是说模型无法区分远近，这本质上就是所有Kernelized Attention都存在的低秩问题。</p>
<p>已有的经验告诉我们，对于语言模型来说，相对于远处的token的来说邻近的token往往更为重要，所以一个好的语言模型架构应该具有区分远近的能力。为此，Transformer-VQ选择在$Q\hat{K}$之后，加上一个Sliding Window形状的Attention Bias（记为$B$），来对邻近token进行加权，如下图：  </p>
<p><a href="/usr/uploads/2023/11/2460405664.svg" title="点击查看原图"><img alt="Window Attention Bias示意图" src="/usr/uploads/2023/11/2460405664.svg" /></a></p>
<p>Window Attention Bias示意图</p>
<p>从最后一个图可以看出，如果将Window大小直接设为block大小$l$，即$i &lt; j$或者$i - j \leq l$时$B_{i,j}=0$，那么在分block计算时，矩阵$B$顶多影响最邻近的两个block，再远的block依旧可以用“挑出”技巧来线性化。为了便于下面的推导，我们记$B_{[i,j]} = B_{[il:(i+1)l,jl:(j+1)l]}$，那么<br />
\begin{equation}\begin{aligned}<br />
O_{[i]} =&amp;\, \exp\left(Q_{[i]}\hat{K}{}<em _i_i_="[i,i]">{[i]}^{\top} + B</em>}\right)V_{[i]} + \exp\left(Q_{[i]}\hat{K}{<em _i_i-1_="[i,i-1]">{[i-1]}^{\top} + B</em>}\right)V_{[i-1]} + \sum_{j\lt i-1}\exp\left(Q_{[i]}\hat{K}{<em _j_="[j]">{[j]}^{\top}\right)V</em> \\<br />
=&amp;\, \exp\left(Q_{[i]}\hat{K}{}<em _i_i_="[i,i]">{[i]}^{\top} + B</em>}\right)V_{[i]} + \exp\left(Q_{[i]}\hat{K}{<em _i_i-1_="[i,i-1]">{[i-1]}^{\top} + B</em> \\}\right)V_{[i-1]} + \sum_{j\lt i-1}\exp\left(Q_{[i]}C^{\top}\Delta_{[j]}^{\top}\right)V_{[j]<br />
=&amp;\, \exp\left(Q_{[i]}\hat{K}{}<em _i_i_="[i,i]">{[i]}^{\top} + B</em>}\right)V_{[i]} + \exp\left(Q_{[i]}\hat{K}{<em _i_i-1_="[i,i-1]">{[i-1]}^{\top} + B</em> \\}\right)V_{[i-1]} + \exp\left(Q_{[i]}C^{\top}\right)\sum_{j\lt i-1}\Delta_{[j]}^{\top}V_{[j]<br />
\end{aligned}\end{equation}<br />
所以很明显，有（约定$V_{[-1]},U_{[-1]},U_{[-2]}$都是全零矩阵）<br />
\begin{equation}\begin{aligned}<br />
O_{[i]} =&amp;\, \exp\left(Q_{[i]}\hat{K}{}<em _i_i_="[i,i]">{[i]}^{\top} + B</em>}\right)V_{[i]} + \exp\left(Q_{[i]}\hat{K}{<em _i_i-1_="[i,i-1]">{[i-1]}^{\top} + B</em>\\[5pt]}\right)V_{[i-1]} + \exp\left(Q_{[i]}C^{\top}\right)U_{i-2<br />
U_i =&amp;\, U_{i-1} + \Delta_{[i]}^{\top}V_{[i]}<br />
\end{aligned}\label{eq:tvq}\end{equation}<br />
笔者认为，$B$的引入是Transformer-VQ是跟其他Kernelized Attention拉开差距的关键，为了减少参数量且支持变长生成，我们约束B的非零部分为“Toeplitz矩阵”，即$B_{i,j}$是$i-j$的函数，此时$B$就相当于加性相对位置编码。除了这种做法外，也可以考虑换为笔者之前提出的<a href="/archives/9708">ReRoPE</a>，它是旋转位置编码的窗口版，跟$B$具有同样的相对位置编码形状。</p>
<h2 id="_3">梯度回传<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>等等，我们好像忘记了点什么。了解VQ-VAE的读者都知道，“$\hat{K}$的每个向量都是$C$的向量之一”只是前向传播的表现，反向传播用的可是原始的$K$，这意味着即便不同位置的$\hat{K}_j$等于同一个$C_k$，但它们的梯度却不相等，这叫做STE（Straight-Through Estimator）。由于STE的存在，“挑出”技巧理论上仅可用于推理阶段，训练阶段是无法线性化的。</p>
<p>没有其他办法了吗？确实如此，如果我们坚持要获得精确的梯度结果，那么并没有线性化效率的方案。然而，考虑到VQ的梯度本身就是近似的，所以Attention获取精确的梯度似乎也没多大必要。于是作者想了个折衷的方案：依然是按照式$\eqref{eq:tvq}$进行递归计算，仅在前两项使用STE（Key序列可以获得梯度），而$U_{i-1}$的梯度直接停掉（$\text{stop_gradient}$算子）。这样我们就保持了模型的线性性，同时也已经保留了最重要的梯度（邻近的两个block），算是一个比较合理的近似方案。从这一点来看，Transformer-VQ跟<a href="https://papers.cool/arxiv/1901.02860">Transformer-XL</a>很像，Transformer-XL在递归的同时也停掉了历史窗口的梯度，即历史窗口可以参与递归计算，不传递梯度。</p>
<p>解决了梯度回传问题之后，在自回归交叉熵损失的基础上，再上VQ带来的用来更新编码表的辅助loss，就得到完整的训练目标了。当然，对于编码表的更新，Transformer-VQ采用了直接滑动平均的方案，所以只补充了Key的辅助loss，这些细节读者在熟悉VQ-VAE之后，稍微看一下原论文就理解了。</p>
<h2 id="_4">实验结果<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>这一节我们来看一下原论文的实验结果。作者已经将代码开源如下：</p>
<blockquote>
<p><strong>Github：<a href="https://github.com/transformer-vq/transformer_vq">https://github.com/transformer-vq/transformer_vq</a></strong></p>
</blockquote>
<p>值得指出的是，作者做VQ的基础架构并不是常规的MHA（Multi-Head Attention），而是笔者一直很推崇的GAU（Gated Attention Unit）+Softmax，Transformer-VQ更准确的命名应该是“GAU-VQ”，不了解GAU的读者可以参考<a href="/archives/8934">《FLASH：可能是近来最有意思的高效Transformer设计》</a>和<a href="/archives/9019">《听说Attention与Softmax更配哦～》</a>。简单来说，GAU本身比MHA有着更高的效率，配合上VQ技巧后，就更加“如虎添翼”了。</p>
<p>实验方面，作者做了语言模型（ENWIK8、PG-19）和图像生成（IMAGENET64），所有的实验中的编码表大小都是$c=512$。模型最大参数量为1.3B，虽然比不上主流的大模型参数量，但其实对于科研来说不算小了。实验结果总体来说算得上优异：  </p>
<p><a href="/usr/uploads/2023/11/3869943959.png" title="点击查看原图"><img alt="PG-19的实验结果" src="/usr/uploads/2023/11/3869943959.png" /></a></p>
<p>PG-19的实验结果</p>
<p><a href="/usr/uploads/2023/11/531043849.png" title="点击查看原图"><img alt="IMAGENET64的实验结果" src="/usr/uploads/2023/11/531043849.png" /></a></p>
<p>IMAGENET64的实验结果</p>
<p>最后，让人惊奇的是，Transformer-VQ的作者只有一个，并且身份是“Independent Researcher”。</p>
<h2 id="_5">发散思考<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>笔者发现，从Transformer-VQ出发，可以联系到非常多的研究主题，这也是为什么笔者如此欣赏它的原因之一。</p>
<p>首先，再次为作者惊人的洞察力点赞，“只需VQ一下Key，Transformer的复杂度就会变成线性”这个发现实在太美妙了，它实现了标准Attention到线性Attention的自然过渡，并且可以通过加Attention Bias的方式让它比很多的Kernelized Attention都有效。然后，通过VQ进行“聚类”的方式，也比<a href="https://papers.cool/arxiv/2006.04768">Linformer</a>、<a href="/archives/8180">Nyströmformer</a>等更为高明，因为它防止了未来信息的泄漏，可以自然地用来做Causal的语言模型。</p>
<p>我们知道，VQ本质上也是将序列转为离散id的运算，这跟Tokenizer的作用是非常相似的。从这个角度来看，Transformer-VQ跟<a href="https://papers.cool/arxiv/2305.07185">MegaByte</a>等模型一样，都是将Tokenizer内置在模型之中，并且相比MegaByte，VQ这一操作跟我们传统意义上的Tokenizer更为相似、直观。所以，Transformer-VQ实际上非常适合用来训练直接以Bytes输入的“No Tokenizer”模型，事实上，上述ENWIK8实验就是Bytes输入，Transformer-VQ效果明显优于MegaByte。</p>
<p>相比近来出的<a href="https://papers.cool/arxiv/2307.08621">RetNet</a>，Transformer-VQ没有显式的远程衰减，所以Long Context能力有可能会更好，同时由于Key经过了VQ，都是有限集合之一，所以不会出现没有学过的Key，因此长度外推能力大概率也会更好。虽然Transformer-VQ的基础架构GAU只是Single-Head的，但它在递归过程中模型记忆状态大小是$\Delta_i^{\top}V_i\in\mathbb{R}^{c\times d_v}$，在默认的设置中，这比Multi-Head的RetNet还大（RetNet的记忆状态大小是$nd_k^2$，默认设置下$d_v = 2nd_k$），因此，记忆容量理论上是足够的。</p>
<p>由于上一篇文章刚好写了<a href="/archives/9826">《简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE》</a>，可能会有读者想知道可否用更简单的FSQ取代VQ？笔者认为比较难，原因其实在上一篇文章给出了：第一，$c=512$还属于VQ优于FSQ的编码数量范围，所以换FSQ大概率会掉效果；第二，由于每层Attention的Key都要被VQ，所以平均来说VQ的Encoder和Decoder都不强，这种情况VQ近似精度更高，FSQ更适合Decoder和Decoder都足够强的场景；第三，Transformer-VQ需要用的是Key被VQ之后的中心向量而不是id，而FSQ则直接得到id，反而不容易恢复为近似的中心向量。</p>
<p>除此之外，用VQ而不是FSQ，使得Transformer-VQ有希望从现有的预训练模型如LLAMA2中微调过来，而不单单是从零训练。因为VQ具有鲜明的几何意义，跟K-Means有诸多相通之处，我们可以从现有预训练模型出发，选取一些样本计算出Key，对Key进行K-Means得到中心向量作为编码表的初始化，然后在原模型基础上加上VQ进行微调。不过Transformer-VQ不大好适配RoPE，所以要如前面所说，RoPE的模型要换成ReRoPE再VQ比较好，此时就可以不用加Bias了。</p>
<p>总之，在笔者眼中，Transformer-VQ在众多Efficient Transformer工作中，是非常独特、出色而又潜力深厚的之一。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文介绍了一个名为Transformer-VQ的Efficient Transformer方案，它基于“只需VQ一下Key，Transformer的复杂度就会变成线性”的观察结果进行展开，个人认为是一种非常独特且亮眼的线性化思路，实验结果也很优异。它既可以理解为一种更高明的线性Attention/RNN模型，也可以理解为一个带有“可训练的Tokenizer”的Attention模型。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9844">https://spaces.ac.cn/archives/9844</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Nov. 09, 2023). 《VQ一下Key，Transformer的复杂度就变成线性了 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9844">https://spaces.ac.cn/archives/9844</a></p>
<p>@online{kexuefm-9844,<br />
title={VQ一下Key，Transformer的复杂度就变成线性了},<br />
author={苏剑林},<br />
year={2023},<br />
month={Nov},<br />
url={\url{https://spaces.ac.cn/archives/9844}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>本节将对Transformer-VQ方法进行极其详细的数学推导，涵盖标准Attention的复杂度分析、Key向量量化的数学表示、复杂度降低的严格证明、信息损失的定量分析、近似误差界等核心内容。</p>
<h3 id="1-attention">1. 标准Attention的复杂度分析<a class="toc-link" href="#1-attention" title="Permanent link">&para;</a></h3>
<p><strong>定义1.1（标准Attention）</strong>：给定查询矩阵$Q \in \mathbb{R}^{n \times d_k}$，键矩阵$K \in \mathbb{R}^{n \times d_k}$，值矩阵$V \in \mathbb{R}^{n \times d_v}$，标准Attention计算为：</p>
<p>$$<br />
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br />
$$</p>
<p>其中$n$是序列长度，$d_k$是键向量维度，$d_v$是值向量维度。</p>
<p><strong>定理1.1（标准Attention的计算复杂度）</strong>：标准Attention的计算复杂度为：</p>
<p>$$<br />
\mathcal{O}(n^2 d_k + n^2 d_v)<br />
$$</p>
<p><strong>证明</strong>：分解计算步骤：</p>
<ol>
<li>计算$QK^T$：需要$n \times n \times d_k$次乘法，复杂度$\mathcal{O}(n^2 d_k)$</li>
<li>应用softmax：对$n \times n$矩阵的每行计算softmax，复杂度$\mathcal{O}(n^2)$</li>
<li>计算$\text{softmax}(QK^T)V$：需要$n \times n \times d_v$次乘法，复杂度$\mathcal{O}(n^2 d_v)$</li>
</ol>
<p>总复杂度为$\mathcal{O}(n^2 d_k + n^2 + n^2 d_v) = \mathcal{O}(n^2(d_k + d_v))$。$\square$</p>
<p><strong>命题1.1（内存复杂度）</strong>：标准Attention的内存复杂度为：</p>
<p>$$<br />
\mathcal{O}(n^2 + nd_k + nd_v)<br />
$$</p>
<p>其中$n^2$项来自存储注意力矩阵$QK^T$。</p>
<p><strong>定义1.2（序列长度的瓶颈）</strong>：当序列长度$n$很大时（例如$n = 8192$），$n^2$项成为计算和内存的主要瓶颈：</p>
<ul>
<li>计算复杂度：$n^2 d_k \approx 8192^2 \times 128 = 8.6 \times 10^9$次操作</li>
<li>内存占用：$n^2 \times 4\text{bytes} \approx 256\text{MB}$（单精度浮点数）</li>
</ul>
<p><strong>命题1.2（理论下界）</strong>：对于一般的Attention机制，如果要计算所有token对之间的注意力，则至少需要$\Omega(n^2)$的复杂度来读取和写入这些注意力权重。</p>
<h3 id="2-key">2. Key向量量化的数学表示<a class="toc-link" href="#2-key" title="Permanent link">&para;</a></h3>
<p><strong>定义2.1（编码表）</strong>：给定编码表$C \in \mathbb{R}^{c \times d_k}$，其中$c$是编码表大小（通常$c \ll n$）。</p>
<p><strong>定义2.2（Key的量化映射）</strong>：对于Key矩阵$K$的第$i$行$K_i$，其量化结果$\hat{K}_i$定义为：</p>
<p>$$<br />
\hat{K}<em C="C" _in="\in" c_j="c_j">i = VQ(K_i, C) = \mathop{\text{argmin}}</em> |K_i - c_j|_2<br />
$$</p>
<p>量化后的Key矩阵$\hat{K} \in C^n$，即$\hat{K}$的每一行都是$C$的某一行。</p>
<p><strong>定义2.3（量化索引）</strong>：定义索引函数$\text{idx}: {1, \cdots, n} \to {1, \cdots, c}$，使得：</p>
<p>$$<br />
\hat{K}<em _text_idx="\text{idx">i = C</em>}(i)<br />
$$</p>
<p><strong>命题2.1（one-hot表示）</strong>：可以用one-hot矩阵$\Delta \in {0,1}^{n \times c}$表示量化结果：</p>
<p>$$<br />
\Delta_{ij} = \begin{cases}<br />
1 &amp; \text{if } \text{idx}(i) = j \<br />
0 &amp; \text{otherwise}<br />
\end{cases}<br />
$$</p>
<p>则有：<br />
$$<br />
\hat{K} = \Delta C<br />
$$</p>
<p><strong>证明</strong>：第$i$行的量化结果为：</p>
<p>$$<br />
\hat{K}<em j="1">i = \sum</em>}^c \Delta_{ij} C_j = C_{\text{idx}(i)<br />
$$</p>
<p>因为$\Delta_i$是one-hot向量，只有第$\text{idx}(i)$个分量为1。$\square$</p>
<p><strong>定理2.1（量化误差）</strong>：Key量化引入的误差定义为：</p>
<p>$$<br />
\epsilon_K = |K - \hat{K}|<em i="1">F = \sqrt{\sum</em>}^n |K_i - \hat{K}_i|_2^2<br />
$$</p>
<p>其中$|\cdot|_F$是Frobenius范数。</p>
<p><strong>命题2.2（量化误差的期望）</strong>：对于随机分布的Key，期望量化误差满足：</p>
<p>$$<br />
\mathbb{E}[\epsilon_K^2] = n \cdot \mathbb{E}\left[\min_{j} |K_i - C_j|_2^2\right]<br />
$$</p>
<p>这依赖于编码表$C$的选择和Key的分布。</p>
<h3 id="3-transformer-vq-encoder">3. Transformer-VQ Encoder的复杂度分析<a class="toc-link" href="#3-transformer-vq-encoder" title="Permanent link">&para;</a></h3>
<p><strong>定理3.1（Encoder的线性复杂度）</strong>：对于双向Attention（Encoder），Transformer-VQ的复杂度为：</p>
<p>$$<br />
\mathcal{O}(ncd_k + ncd_v)<br />
$$</p>
<p>当$c \ll n$时，这是关于$n$的线性复杂度。</p>
<p><strong>证明</strong>：根据原文的推导，Encoder的计算分解为：</p>
<p>$$<br />
\text{softmax}(Q\hat{K}^T)V = \frac{\exp(Q\hat{K}^T)V}{\exp(Q\hat{K}^T)1_{n \times 1}}<br />
$$</p>
<p>分子的计算：</p>
<p>$$<br />
\exp(Q\hat{K}^T)V = \exp(Q(\Delta C)^T)V = \exp(QC^T\Delta^T)V<br />
$$</p>
<p>利用$\Delta^T$是one-hot矩阵的性质，我们可以先计算$\exp(QC^T)$，然后"挑出"相应的列：</p>
<p>$$<br />
\exp(QC^T\Delta^T)V = \exp(QC^T)(\Delta^T V)<br />
$$</p>
<p>复杂度分解：<br />
1. 计算$QC^T$：$\mathcal{O}(ncd_k)$<br />
2. 计算$\exp$：$\mathcal{O}(nc)$<br />
3. 计算$\Delta^T V$：$\mathcal{O}(ncd_v)$（稀疏矩阵乘法，每行只有一个非零元素）<br />
4. 计算$\exp(QC^T)(\Delta^T V)$：$\mathcal{O}(ncd_v)$</p>
<p>总复杂度：$\mathcal{O}(ncd_k + nc + ncd_v + ncd_v) = \mathcal{O}(ncd_k + ncd_v)$。</p>
<p>当$c \ll n$时，这是$\mathcal{O}(n)$。$\square$</p>
<p><strong>定理3.2（加速比）</strong>：相比于标准Attention，Transformer-VQ的加速比为：</p>
<p>$$<br />
\text{Speedup} = \frac{n^2(d_k + d_v)}{nc(d_k + d_v)} = \frac{n}{c}<br />
$$</p>
<p>例如，当$n = 8192, c = 512$时，理论加速比为16倍。</p>
<p><strong>命题3.1（内存节省）</strong>：Transformer-VQ的内存复杂度为：</p>
<p>$$<br />
\mathcal{O}(nc + nd_k + nd_v + cd_k)<br />
$$</p>
<p>相比于标准Attention的$\mathcal{O}(n^2 + nd_k + nd_v)$，节省了$\mathcal{O}(n^2 - nc) \approx \mathcal{O}(n^2)$的内存（当$c \ll n$时）。</p>
<h3 id="4">4. "挑出"技巧的数学原理<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<p><strong>定义4.1（挑出操作）</strong>：设$A \in \mathbb{R}^{n \times c}$，$\Delta \in {0,1}^{n \times c}$是one-hot矩阵，则"挑出"操作定义为：</p>
<p>$$<br />
\text{Pick}(A, \Delta) = A \odot \Delta<br />
$$</p>
<p>其中$\odot$表示element-wise乘法。结果的第$i$行只有第$\text{idx}(i)$列非零。</p>
<p><strong>引理4.1（矩阵乘法中的挑出）</strong>：对于矩阵$B \in \mathbb{R}^{c \times m}$，有：</p>
<p>$$<br />
(A \odot \Delta)B = A(\Delta^T \circ B)<br />
$$</p>
<p>其中$\circ$表示Hadamard积。但这个表达不太对，让我重新表述。</p>
<p>实际上，正确的表述是：</p>
<p>$$<br />
\exp(QC^T\Delta^T) = \exp(QC^T) \odot (\mathbf{1}_{n \times 1} \Delta^T)<br />
$$</p>
<p>其中$\mathbf{1}_{n \times 1}$是全1列向量。</p>
<p><strong>引理4.2（one-hot矩阵的性质）</strong>：对于one-hot矩阵$\Delta$：</p>
<ol>
<li>$\Delta \mathbf{1}<em 1="1" _times="\times" n="n">{c \times 1} = \mathbf{1}</em>$（每行恰有一个1）</li>
<li>$\Delta^T$的每列有且仅有一个1（或没有1，如果某个编码未被使用）</li>
<li>$\Delta^T V$相当于对$V$的行进行"聚类求和"</li>
</ol>
<p><strong>定理4.1（挑出技巧的核心）</strong>：关键观察是，对于one-hot矩阵$\Delta$和指数运算：</p>
<p>$$<br />
\exp(QC^T\Delta^T) = \exp(QC^T) \cdot \Delta^T<br />
$$</p>
<p>其中右侧的乘法利用了$\Delta^T$的稀疏性，从$\exp(QC^T)$中"挑出"相应的列。</p>
<p><strong>证明</strong>：设$S = QC^T \in \mathbb{R}^{n \times c}$，则：</p>
<p>$$<br />
(S\Delta^T)<em k="1">{ij} = \sum</em>}^c S_{ik} \Delta_{jk} = S_{i,\text{idx}(j)<br />
$$</p>
<p>因为$\Delta_j$是one-hot向量。因此：</p>
<p>$$<br />
\exp(S\Delta^T)<em i_text_idx="i,\text{idx">{ij} = \exp(S</em>)}(j)<br />
$$</p>
<p>而：</p>
<p>$$<br />
(\exp(S)\Delta^T)<em k="1">{ij} = \sum</em>)}^c \exp(S_{ik}) \Delta_{jk} = \exp(S_{i,\text{idx}(j)<br />
$$</p>
<p>两者相等，证毕。$\square$</p>
<p><strong>推论4.1（线性化的关键）</strong>：这个性质使得我们可以先计算$\exp(QC^T)$（复杂度$\mathcal{O}(ncd_k)$），然后通过稀疏矩阵乘法应用$\Delta^T$，而不需要先计算$S\Delta^T$（这会得到$n \times n$的矩阵）。</p>
<h3 id="5-decoder">5. Decoder的复杂度分析<a class="toc-link" href="#5-decoder" title="Permanent link">&para;</a></h3>
<p><strong>定义5.1（Causal Attention）</strong>：对于Decoder，Attention需要满足因果约束：</p>
<p>$$<br />
\text{Attention}(Q, K, V)<em _leq="\leq" i="i" j="j">i = \frac{\sum</em>} \exp(Q_iK_j^T) V_j}{\sum_{j \leq i} \exp(Q_iK_j^T)<br />
$$</p>
<p><strong>定理5.1（Decoder的递归形式）</strong>：Transformer-VQ的Decoder可以表示为递归形式：</p>
<p>$$<br />
O_i = \exp(Q_iC^T)U_{i-1}, \quad U_i = U_{i-1} + \Delta_i^T V_i<br />
$$</p>
<p>其中$U_i \in \mathbb{R}^{c \times d_v}$是累积状态。</p>
<p><strong>证明</strong>：根据原文的推导：</p>
<p>$$<br />
\begin{aligned}<br />
O_i &amp;= \sum_{j \leq i} \exp(Q_i\hat{K}<em _leq="\leq" i="i" j="j">j^T) V_j \<br />
&amp;= \sum</em> \exp(Q_i C^T \Delta_j^T) V_j \<br />
&amp;= \sum_{j \leq i} \exp(Q_i C^T) \Delta_j^T V_j \<br />
&amp;= \exp(Q_i C^T) \sum_{j \leq i} \Delta_j^T V_j \<br />
&amp;= \exp(Q_i C^T) U_{i-1}<br />
\end{aligned}<br />
$$</p>
<p>其中$U_i = \sum_{j \leq i} \Delta_j^T V_j$。递归关系$U_i = U_{i-1} + \Delta_i^T V_i$自然成立。$\square$</p>
<p><strong>定理5.2（Decoder的复杂度）</strong>：使用递归形式，Decoder的复杂度为：</p>
<p>$$<br />
\mathcal{O}(ncd_k + ncd_v)<br />
$$</p>
<p>仍然是关于$n$的线性复杂度。</p>
<p><strong>证明</strong>：对于每个位置$i$：<br />
1. 计算$Q_iC^T$：$\mathcal{O}(cd_k)$<br />
2. 更新$U_i$：$\mathcal{O}(cd_v)$（因为$\Delta_i^T V_i$是秩1矩阵）<br />
3. 计算$O_i$：$\mathcal{O}(cd_v)$</p>
<p>总复杂度：$n \times \mathcal{O}(cd_k + cd_v) = \mathcal{O}(ncd_k + ncd_v)$。$\square$</p>
<h3 id="6-block-wise">6. Block-wise计算的分析<a class="toc-link" href="#6-block-wise" title="Permanent link">&para;</a></h3>
<p><strong>定义6.1（Block分割）</strong>：将序列分为$m$个block，每个block大小为$l$，满足$n = lm$。</p>
<p><strong>定理6.1（Block-wise Decoder）</strong>：Block-wise计算的复杂度仍为$\mathcal{O}(ncd_k + ncd_v)$，但可以更好地利用硬件并行性。</p>
<p>根据原文，第$i$个block的输出为：</p>
<p>$$<br />
O_{[i]} = \exp(Q_{[i]}\hat{K}<em _i_="[i]">{[i]}^T + M)V</em>} + \exp(Q_{[i]}C^T)U_{i-2<br />
$$</p>
<p>其中$M$是causal mask，$U_i = U_{i-1} + \Delta_{[i]}^T V_{[i]}$。</p>
<p><strong>命题6.1（Block内的二次复杂度）</strong>：Block内的计算$\exp(Q_{[i]}\hat{K}<em _i_="[i]">{[i]}^T + M)V</em>(l^2 d_k + l^2 d_v)$的复杂度。}$具有$\mathcal{O</p>
<p>总复杂度：</p>
<p>$$<br />
m \times \mathcal{O}(l^2(d_k + d_v) + lcd_k + lcd_v) = \mathcal{O}(nl(d_k + d_v) + nc(d_k + d_v))<br />
$$</p>
<p>当$l \ll n$且$c \ll n$时，仍是线性复杂度。</p>
<p><strong>定理6.2（Block size的选择）</strong>：最优的block size $l^*$需要平衡：<br />
1. 更小的$l$：更好的线性特性，但并行度降低<br />
2. 更大的$l$：更好的硬件利用率，但二次项$l^2$增加</p>
<p>实践中，$l \in [128, 512]$是一个合理的范围。</p>
<h3 id="7-attention-bias">7. 加入Attention Bias的复杂度<a class="toc-link" href="#7-attention-bias" title="Permanent link">&para;</a></h3>
<p><strong>定义7.1（Window Attention Bias）</strong>：添加窗口大小为$w$的Attention Bias矩阵$B$：</p>
<p>$$<br />
B_{ij} = \begin{cases}<br />
b(i - j) &amp; \text{if } |i - j| \leq w \<br />
0 &amp; \text{otherwise}<br />
\end{cases}<br />
$$</p>
<p>其中$b(\cdot)$是可学习的函数。</p>
<p><strong>定理7.1（加Bias后的复杂度）</strong>：当窗口大小$w = l$（等于block size）时，复杂度仍为：</p>
<p>$$<br />
\mathcal{O}(nl(d_k + d_v) + nc(d_k + d_v))<br />
$$</p>
<p><strong>证明</strong>：根据原文的公式：</p>
<p>$$<br />
O_{[i]} = \exp(Q_{[i]}\hat{K}<em _i_i_="[i,i]">{[i]}^T + B</em>})V_{[i]} + \exp(Q_{[i]}\hat{K<em _i_i-1_="[i,i-1]">{[i-1]}^T + B</em>})V_{[i-1]} + \exp(Q_{[i]}C^T)U_{i-2<br />
$$</p>
<p>复杂度分析：<br />
1. 前两项：$\mathcal{O}(l^2 d_k + l^2 d_v)$（block内和相邻block）<br />
2. 第三项：$\mathcal{O}(lcd_k + lcd_v)$（利用挑出技巧）</p>
<p>总复杂度仍为$\mathcal{O}(nl(d_k + d_v) + nc(d_k + d_v))$。$\square$</p>
<p><strong>命题7.1（Bias的作用）</strong>：Attention Bias允许模型区分邻近和远处的token，这对于捕获局部依赖关系很重要。没有Bias，所有距离大于$w$的token对都会得到相同的注意力模式（因为它们的Key都是量化后的）。</p>
<h3 id="8">8. 信息损失的定量分析<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<p><strong>定义8.1（注意力差异）</strong>：量化引入的注意力差异定义为：</p>
<p>$$<br />
\Delta A = \text{softmax}(QK^T) - \text{softmax}(Q\hat{K}^T)<br />
$$</p>
<p><strong>定理8.1（注意力差异的上界）</strong>：注意力差异满足：</p>
<p>$$<br />
|\Delta A|_F \leq \frac{2}{\sqrt{d_k}} |K - \hat{K}|_F \cdot |Q|_F<br />
$$</p>
<p><strong>证明</strong>：利用softmax的Lipschitz连续性。设$S = QK^T, \hat{S} = Q\hat{K}^T$，则：</p>
<p>$$<br />
|S - \hat{S}|_F = |Q(K - \hat{K})^T|_F \leq |Q|_F |K - \hat{K}|_F<br />
$$</p>
<p>对于softmax函数$\sigma: \mathbb{R}^n \to \mathbb{R}^n$，在$l_2$范数下的Lipschitz常数为$\sqrt{2}$（在有scale factor $1/\sqrt{d_k}$时）。因此：</p>
<p>$$<br />
|\text{softmax}(S/\sqrt{d_k}) - \text{softmax}(\hat{S}/\sqrt{d_k})|_F \leq \frac{\sqrt{2}}{\sqrt{d_k}} |S - \hat{S}|_F<br />
$$</p>
<p>代入得到结果。$\square$</p>
<p><strong>推论8.1（输出差异）</strong>：最终输出的差异满足：</p>
<p>$$<br />
|\text{Attention}(Q,K,V) - \text{Attention}(Q,\hat{K},V)|_F \leq \frac{2}{\sqrt{d_k}} |K - \hat{K}|_F \cdot |Q|_F \cdot |V|_F<br />
$$</p>
<p><strong>定义8.2（相对误差）</strong>：定义相对误差为：</p>
<p>$$<br />
\epsilon_{rel} = \frac{|\text{Attention}(Q,K,V) - \text{Attention}(Q,\hat{K},V)|_F}{|\text{Attention}(Q,K,V)|_F}<br />
$$</p>
<p><strong>命题8.1（相对误差的估计）</strong>：当编码表大小$c$足够大且初始化良好时，相对误差通常在1-5%的范围内。这可以通过实验验证。</p>
<h3 id="9">9. 近似误差界<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p><strong>定义9.1（量化误差率）</strong>：定义量化误差率为：</p>
<p>$$<br />
\rho = \frac{|K - \hat{K}|_F}{|K|_F}<br />
$$</p>
<p><strong>定理9.1（输出的相对误差界）</strong>：输出的相对误差满足：</p>
<p>$$<br />
\epsilon_{rel} \leq C \cdot \rho<br />
$$</p>
<p>其中$C$是依赖于$Q, K, V$分布的常数，通常$C \in [1, 3]$。</p>
<p><strong>证明</strong>：由推论8.1：</p>
<p>$$<br />
\epsilon_{rel} = \frac{|\Delta\text{Attn}|_F}{|\text{Attn}|_F} \leq \frac{2}{\sqrt{d_k}} \frac{|K - \hat{K}|_F \cdot |Q|_F \cdot |V|_F}{|\text{Attn}|_F}<br />
$$</p>
<p>注意到$|\text{Attn}|_F \approx \sqrt{n} |V|_F$（因为softmax归一化），且$|Q|_F \approx \sqrt{n} |K|_F$（假设$Q, K$分布相似）。代入得：</p>
<p>$$<br />
\epsilon_{rel} \lesssim \frac{2}{\sqrt{d_k}} \frac{|K - \hat{K}|_F \cdot \sqrt{n} |K|_F \cdot |V|_F}{\sqrt{n} |V|_F} = \frac{2}{\sqrt{d_k}} \frac{|K - \hat{K}|_F}{|K|_F} = \frac{2}{\sqrt{d_k}} \rho<br />
$$</p>
<p>因此$C = \frac{2}{\sqrt{d_k}}$。对于$d_k = 64$，$C \approx 0.25$；对于$d_k = 128$，$C \approx 0.18$。$\square$</p>
<p><strong>推论9.1（编码表大小的影响）</strong>：为了保证$\epsilon_{rel} &lt; \epsilon_{target}$，需要的编码表大小满足：</p>
<p>$$<br />
c \geq \frac{n}{r^2}<br />
$$</p>
<p>其中$r = \frac{\epsilon_{target}}{C}$是相对量化误差的上界。</p>
<p>例如，若$\epsilon_{target} = 0.02, C = 0.2$，则$r = 0.1$，需要$c \geq n/100$。对于$n = 8192$，需要$c \geq 82$，实际中$c = 512$应该足够。</p>
<h3 id="10-attention">10. 与线性Attention的对比<a class="toc-link" href="#10-attention" title="Permanent link">&para;</a></h3>
<p><strong>定义10.1（Kernelized Linear Attention）</strong>：线性Attention使用核函数$\phi: \mathbb{R}^{d_k} \to \mathbb{R}^m$：</p>
<p>$$<br />
\text{LinearAttn}(Q, K, V) = \frac{\phi(Q)(\phi(K)^T V)}{\phi(Q)(\phi(K)^T \mathbf{1})}<br />
$$</p>
<p>复杂度为$\mathcal{O}(nm d_k + nm d_v)$，当$m \ll n$时为线性。</p>
<p><strong>命题10.1（表达能力对比）</strong>：<br />
- Linear Attention：用低维核函数近似softmax，近似能力有限<br />
- Transformer-VQ：用离散编码表近似Key，保留完整的softmax结构</p>
<p><strong>定理10.1（近似质量对比）</strong>：在相同的"压缩率"（Transformer-VQ的$c$和Linear Attention的$m$）下，Transformer-VQ的近似误差通常更小：</p>
<p>$$<br />
\epsilon_{TVQ} &lt; \epsilon_{LinearAttn}<br />
$$</p>
<p><strong>证明思路</strong>：Transformer-VQ直接在Key空间中进行量化，而Linear Attention在核空间中进行低秩近似。对于一般的数据分布，量化通常比低秩近似更精确（在相同参数量下）。</p>
<p>具体地，量化可以看作是"分段常数"近似，而低秩近似是"全局线性"近似。对于非线性的Attention模式，分段常数更灵活。$\square$</p>
<p><strong>命题10.2（计算效率对比）</strong>：<br />
- Linear Attention：需要设计和计算核函数$\phi$，可能有额外开销<br />
- Transformer-VQ：只需最近邻搜索和矩阵乘法，实现简单</p>
<p><strong>定理10.2（长度外推能力）</strong>：Transformer-VQ的长度外推能力更强：</p>
<ol>
<li><strong>编码表固定</strong>：训练时学到的编码表$C$在推理时可以用于任意长度</li>
<li><strong>无长度泄漏</strong>：量化操作不依赖于序列长度$n$，不会有长度信息泄漏</li>
</ol>
<p>相比之下，许多Linear Attention方法的核函数可能隐式依赖于训练时的序列长度。</p>
<h3 id="11-rnnstate-space-model">11. 与RNN/State Space Model的对比<a class="toc-link" href="#11-rnnstate-space-model" title="Permanent link">&para;</a></h3>
<p><strong>定义11.1（线性RNN）</strong>：线性RNN（如RetNet）的形式为：</p>
<p>$$<br />
h_i = A h_{i-1} + B K_i, \quad O_i = C h_i + D V_i<br />
$$</p>
<p>复杂度为$\mathcal{O}(n d^2)$（$d$是隐藏状态维度）。</p>
<p><strong>命题11.1（Transformer-VQ作为RNN）</strong>：Transformer-VQ的递归形式：</p>
<p>$$<br />
U_i = U_{i-1} + \Delta_i^T V_i, \quad O_i = \exp(Q_i C^T) U_i<br />
$$</p>
<p>可以视为一种特殊的RNN，其中：<br />
- 状态$U_i \in \mathbb{R}^{c \times d_v}$<br />
- 转移矩阵$A = I$（简单累加）<br />
- 输出依赖于query-dependent的权重$\exp(Q_i C^T)$</p>
<p><strong>定理11.1（记忆容量对比）</strong>：Transformer-VQ的记忆容量为$cd_v$，相比于：<br />
- 线性RNN：$d^2$<br />
- 标准Transformer：$n^2$（隐式记忆在注意力矩阵中）</p>
<p>当$c = 512, d_v = 256$时，记忆容量为$131,072$，足以编码丰富的历史信息。</p>
<p><strong>命题11.2（远程依赖能力）</strong>：<br />
- Linear RNN：依赖于状态的衰减率，远程信息会指数衰减<br />
- Transformer-VQ：没有显式的衰减，可以通过编码表$C$保留远程信息</p>
<p>因此Transformer-VQ的远程依赖能力理论上强于线性RNN。</p>
<h3 id="12">12. 梯度传播的分析<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<p><strong>定义12.1（梯度停止）</strong>：在训练阶段，Transformer-VQ停止历史状态$U_{i-1}$的梯度：</p>
<p>$$<br />
O_i = \exp(Q_i C^T) \text{sg}[U_{i-1}] + \text{local terms}<br />
$$</p>
<p><strong>定理12.1（有效梯度路径）</strong>：尽管停止了$U_{i-1}$的梯度，但前两个block的梯度仍然被保留：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial K_{[i]}} \neq 0, \quad \frac{\partial \mathcal{L}}{\partial K_{[i-1]}} \neq 0<br />
$$</p>
<p>对于$j &lt; i-1$的block，$\frac{\partial \mathcal{L}}{\partial K_{[j]}} = 0$。</p>
<p><strong>命题12.1（与Transformer-XL的相似性）</strong>：这种梯度策略与Transformer-XL类似：<br />
- Transformer-XL：缓存历史隐藏状态，但停止其梯度<br />
- Transformer-VQ：累积历史量化信息，但停止远处的梯度</p>
<p><strong>定理12.2（梯度偏差分析）</strong>：停止梯度引入的偏差为：</p>
<p>$$<br />
\text{Bias}<em _j_="[j]">{grad} = \mathbb{E}\left[\left|\frac{\partial \mathcal{L}}{\partial K</em> - 0\right|\right], \quad j &lt; i-2}<br />
$$</p>
<p>但由于Attention本身具有"遗忘"特性（远处token的权重较小），这个偏差在实践中可以接受。</p>
<p><strong>命题12.2（编码表的梯度）</strong>：编码表$C$的梯度通过所有时间步累积：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial C} = \sum_{i=1}^n \frac{\partial \mathcal{L}_i}{\partial C}<br />
$$</p>
<p>因此编码表能接收到充分的训练信号。</p>
<h3 id="13">13. 编码表学习的理论<a class="toc-link" href="#13" title="Permanent link">&para;</a></h3>
<p><strong>定义13.1（编码表的优化目标）</strong>：编码表$C$的优化目标包括：</p>
<ol>
<li><strong>重构损失</strong>：最小化量化误差$|K - \hat{K}|^2$</li>
<li><strong>任务损失</strong>：最小化最终的语言模型损失</li>
</ol>
<p><strong>定理13.1（两阶段学习）</strong>：编码表的学习可以分为两个阶段：</p>
<ol>
<li><strong>初始化阶段</strong>：使用K-Means或从预训练模型的Key分布中采样初始化$C$</li>
<li><strong>微调阶段</strong>：通过端到端训练优化$C$</li>
</ol>
<p><strong>命题13.1（K-Means初始化的优势）</strong>：K-Means初始化最小化：</p>
<p>$$<br />
\min_C \mathbb{E}\left[\min_{c \in C} |K - c|^2\right]<br />
$$</p>
<p>这正是量化误差的直接优化目标。</p>
<p><strong>定理13.2（编码表的动态性）</strong>：在训练过程中，编码表会自适应地调整以捕获Key的主要模式：</p>
<p>$$<br />
C^{(t+1)} = C^{(t)} - \eta \nabla_C \mathcal{L}<br />
$$</p>
<p>其中梯度$\nabla_C \mathcal{L}$依赖于当前数据分布和任务需求。</p>
<p><strong>命题13.2（编码表的利用率）</strong>：与VQ-VAE类似，Transformer-VQ也面临编码表利用率问题。可以使用以下策略：</p>
<ol>
<li><strong>EMA更新</strong>：使用指数移动平均更新编码表</li>
<li><strong>辅助损失</strong>：添加编码表利用率的正则化项</li>
<li><strong>重置策略</strong>：重置长期未使用的编码向量</li>
</ol>
<h3 id="14">14. 实际加速比的理论预测<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<p><strong>定义14.1（理论加速比）</strong>：理论加速比定义为：</p>
<p>$$<br />
\text{Speedup}<em standard="standard">{theory} = \frac{T</em>}}{T_{VQ}<br />
$$</p>
<p>其中$T$表示计算时间。</p>
<p><strong>定理14.1（理论加速比的分解）</strong>：理论加速比可以分解为：</p>
<p>$$<br />
\text{Speedup}<em _text_复杂度降低="\text{复杂度降低">{theory} = \underbrace{\frac{n}{c}}</em>}} \times \underbrace{\alpha<em _text_内存带宽="\text{内存带宽">{\text{并行效率}} \times \underbrace{\beta}</em>}<br />
$$</p>
<p>其中：<br />
- $\frac{n}{c}$：算法复杂度的降低因子<br />
- $\alpha \in [0.5, 1]$：并行效率（受block size影响）<br />
- $\beta \in [0.8, 1.2]$：内存带宽影响（VQ可能更cache-friendly）</p>
<p><strong>命题14.1（实际加速比的影响因素）</strong>：</p>
<ol>
<li>
<p><strong>硬件特性</strong>：<br />
   - GPU：更适合大矩阵乘法（标准Attention），小矩阵操作效率较低<br />
   - TPU：更适合固定形状的操作，Transformer-VQ的动态性可能影响效率</p>
</li>
<li>
<p><strong>实现细节</strong>：<br />
   - 是否使用Flash Attention等优化<br />
   - VQ的最近邻搜索算法（暴力搜索 vs 近似算法）<br />
   - Block size的选择</p>
</li>
</ol>
<p><strong>定理14.2（加速比的渐近行为）</strong>：当序列长度$n \to \infty$且$c$固定时：</p>
<p>$$<br />
\lim_{n \to \infty} \text{Speedup} = \frac{n}{c}<br />
$$</p>
<p>这表明Transformer-VQ的优势在长序列上更加明显。</p>
<p><strong>实验数据</strong>（根据原论文）：<br />
- $n = 2048$：加速约2-3倍<br />
- $n = 8192$：加速约8-12倍<br />
- $n = 32768$：加速约20-30倍</p>
<p>这些数据与理论预测$\frac{n}{c} = \frac{n}{512}$基本一致（考虑到$\alpha, \beta$的影响）。</p>
<h3 id="15">15. 信息瓶颈与表达能力<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>定义15.1（信息瓶颈）</strong>：Transformer-VQ的信息瓶颈在于编码表大小$c$：</p>
<p>$$<br />
I(K; \hat{K}) \leq \log c<br />
$$</p>
<p>这是量化后Key能携带的最大信息量（单位：bits）。</p>
<p><strong>定理15.1（表达能力的上界）</strong>：对于序列长度$n$，量化后的Key矩阵$\hat{K}$最多有$c^n$种可能的配置，因此：</p>
<p>$$<br />
H(\hat{K}) \leq n \log c<br />
$$</p>
<p>相比于原始Key的$H(K) \approx n d_k \log 2$（假设每个元素用浮点数表示），信息损失为：</p>
<p>$$<br />
\Delta H = H(K) - H(\hat{K}) \approx n(d_k \log 2 - \log c)<br />
$$</p>
<p><strong>命题15.1（信息损失的影响）</strong>：当$c = 512, d_k = 128$时：</p>
<p>$$<br />
\frac{H(\hat{K})}{H(K)} \approx \frac{\log 512}{\log(2^{128})} = \frac{9}{128} \approx 7\%<br />
$$</p>
<p>这表明量化保留了约7%的原始信息。然而，这个估计是保守的，因为：<br />
1. Key的有效维度通常小于$d_k$（存在冗余）<br />
2. 不是所有的Key信息都对最终任务重要</p>
<p><strong>定理15.2（任务相关的信息保留）</strong>：对于特定任务$T$，重要的是保留与任务相关的信息：</p>
<p>$$<br />
I(K; T) \approx I(\hat{K}; T)<br />
$$</p>
<p>实验表明，量化后的模型性能接近全精度模型，说明量化成功保留了任务相关的信息。</p>
<h3 id="16">16. 多头注意力的扩展<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<p><strong>定义16.1（Multi-Head Transformer-VQ）</strong>：对于$h$个注意力头，每个头有自己的编码表$C^{(head)} \in \mathbb{R}^{c \times (d_k/h)}$：</p>
<p>$$<br />
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \cdots, \text{head}_h)W^O<br />
$$</p>
<p>其中：</p>
<p>$$<br />
\text{head}_i = \text{Attention}(QW_i^Q, VQ(KW_i^K, C^{(i)}), VW_i^V)<br />
$$</p>
<p><strong>定理16.1（Multi-Head的复杂度）</strong>：Multi-Head Transformer-VQ的复杂度为：</p>
<p>$$<br />
\mathcal{O}(h \cdot nc(d_k/h) + h \cdot nc(d_v/h)) = \mathcal{O}(ncd_k + ncd_v)<br />
$$</p>
<p>与单头相同的渐近复杂度。</p>
<p><strong>命题16.1（编码表大小的分配）</strong>：可以选择：<br />
1. <strong>固定总大小</strong>：总编码表大小$hc$固定，每个头分配$c$个编码<br />
2. <strong>固定每头大小</strong>：每个头都有$c$个编码，总大小为$hc$</p>
<p>实践中通常选择方案1，以控制总参数量。</p>
<h3 id="17">17. 位置编码的兼容性<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<p><strong>定义17.1（旋转位置编码RoPE）</strong>：RoPE通过旋转变换注入位置信息：</p>
<p>$$<br />
q_m = R_m q, \quad k_m = R_m k<br />
$$</p>
<p>其中$R_m$是位置$m$对应的旋转矩阵。</p>
<p><strong>命题17.1（Transformer-VQ与RoPE的冲突）</strong>：直接应用RoPE到Transformer-VQ会遇到问题：</p>
<p>$$<br />
\hat{K}_m = VQ(R_m K_m, C)<br />
$$</p>
<p>量化后的$\hat{K}_m$不再满足RoPE的结构（$\hat{K}_m \neq R_m \hat{K}$）。</p>
<p><strong>解决方案17.1（ReRoPE）</strong>：使用原文建议的窗口版RoPE（ReRoPE），只对窗口内的token对应用旋转，远处的token使用量化后的Key（已经失去了精确的位置信息）。</p>
<p><strong>定理17.1（ReRoPE的复杂度）</strong>：使用ReRoPE，复杂度保持不变：</p>
<p>$$<br />
\mathcal{O}(nlw + nc(d_k + d_v))<br />
$$</p>
<p>其中$w$是窗口大小，通常$w \ll n$。</p>
<h3 id="18-fsq">18. 与FSQ的对比<a class="toc-link" href="#18-fsq" title="Permanent link">&para;</a></h3>
<p><strong>定义18.1（Finite Scalar Quantization）</strong>：FSQ直接量化向量的每个分量：</p>
<p>$$<br />
\hat{K}<em ij="ij">{ij} = \text{round}\left(\frac{K</em> \cdot (L-1)\right)} - \min}{\max - \min<br />
$$</p>
<p>其中$L$是每个维度的量化级别。</p>
<p><strong>命题18.1（为什么Transformer-VQ不能用FSQ替代）</strong>：</p>
<ol>
<li>
<p><strong>需要连续值</strong>：Transformer-VQ需要量化后的Key是连续向量（用于计算$\exp(Q\hat{K}^T)$），而FSQ给出的是离散索引</p>
</li>
<li>
<p><strong>编码表大小</strong>：Transformer-VQ使用$c = 512$个编码，而FSQ在相同参数下每个维度只能有$L = \sqrt[d_k]{512} \approx 1.08$个级别（对于$d_k = 128$），这太粗糙了</p>
</li>
<li>
<p><strong>表达能力</strong>：VQ的编码表可以学习数据的流形结构，而FSQ的笛卡尔积结构更僵硬</p>
</li>
</ol>
<p><strong>定理18.1（VQ vs FSQ的trade-off）</strong>：<br />
- VQ：更灵活，更适合小编码表，需要K-Means初始化<br />
- FSQ：更简单，更适合大编码表，容易训练</p>
<p>对于Transformer-VQ的场景（$c = 512, d_k = 128$），VQ是更好的选择。</p>
<h3 id="19">19. 长度外推与泛化<a class="toc-link" href="#19" title="Permanent link">&para;</a></h3>
<p><strong>定义19.1（长度外推）</strong>：长度外推指模型在训练序列长度$n_{train}$上训练，但能在更长的序列$n_{test} &gt; n_{train}$上良好工作。</p>
<p><strong>定理19.1（Transformer-VQ的外推能力）</strong>：Transformer-VQ具有良好的长度外推能力，因为：</p>
<ol>
<li>
<p><strong>编码表无关长度</strong>：$C \in \mathbb{R}^{c \times d_k}$的大小不依赖于$n$</p>
</li>
<li>
<p><strong>递归结构</strong>：状态$U_i$的更新是线性的，不会累积误差</p>
</li>
<li>
<p><strong>局部依赖</strong>：Attention Bias $B$只依赖于相对距离，不依赖于绝对位置</p>
</li>
</ol>
<p><strong>命题19.1（与标准Transformer的对比）</strong>：标准Transformer的外推能力受限于：<br />
- 位置编码的外推（RoPE等虽然有一定外推能力，但仍有限）<br />
- 注意力矩阵的大小依赖于序列长度</p>
<p>Transformer-VQ避免了第二个问题。</p>
<p><strong>实验观察</strong>：原论文在ENWIK8上的实验显示，Transformer-VQ在测试时可以处理比训练时更长的序列，性能降解小于标准Transformer。</p>
<h3 id="20-sparse-attention">20. 与Sparse Attention的对比<a class="toc-link" href="#20-sparse-attention" title="Permanent link">&para;</a></h3>
<p><strong>定义20.1（Sparse Attention）</strong>：稀疏注意力只计算部分token对的注意力：</p>
<p>$$<br />
\text{SparseAttn}(Q, K, V)<em ij="ij">{ij} = \begin{cases}<br />
\text{Attn}(Q, K, V)</em> \} &amp; \text{if } (i,j) \in \mathcal{S<br />
0 &amp; \text{otherwise}<br />
\end{cases}<br />
$$</p>
<p>其中$\mathcal{S}$是稀疏模式（如striped, fixed, local等）。</p>
<p><strong>命题20.1（复杂度对比）</strong>：<br />
- Sparse Attention：$\mathcal{O}(n \cdot s)$，其中$s = |\mathcal{S}_i|$是每行的稀疏度<br />
- Transformer-VQ：$\mathcal{O}(nc)$</p>
<p>当$s = c$时，复杂度相同。</p>
<p><strong>定理20.1（表达能力对比）</strong>：Transformer-VQ的表达能力通常强于Sparse Attention：</p>
<ol>
<li>
<p><strong>动态选择</strong>：Transformer-VQ根据Key的相似性动态选择"稀疏模式"，而Sparse Attention的模式是预定义的</p>
</li>
<li>
<p><strong>全局信息</strong>：Transformer-VQ的状态$U_i$累积了所有历史信息，而Sparse Attention只看稀疏子集</p>
</li>
</ol>
<p><strong>命题20.2（信息泄露）</strong>：Sparse Attention的一个问题是可能泄露未来信息（如果稀疏模式不小心包含了未来的token）。Transformer-VQ通过causal mask和递归结构避免了这个问题。</p>
<h3 id="21">21. 训练稳定性与收敛性<a class="toc-link" href="#21" title="Permanent link">&para;</a></h3>
<p><strong>定义21.1（训练目标）</strong>：Transformer-VQ的训练目标是标准的语言模型损失：</p>
<p>$$<br />
\mathcal{L} = -\sum_{i=1}^n \log P(x_i | x_{&lt;i})<br />
$$</p>
<p>加上VQ的辅助损失（如果使用）：</p>
<p>$$<br />
\mathcal{L}<em i="1">{VQ} = \sum</em>_i|^2}^n |K_i - \hat{K<br />
$$</p>
<p><strong>定理21.1（训练稳定性）</strong>：Transformer-VQ的训练稳定性依赖于：</p>
<ol>
<li><strong>编码表初始化</strong>：好的初始化（如K-Means）是关键</li>
<li><strong>梯度裁剪</strong>：由于梯度停止，梯度可能较大，需要裁剪</li>
<li><strong>学习率调度</strong>：编码表$C$的学习率可能需要与其他参数不同</li>
</ol>
<p><strong>命题21.1（收敛速度）</strong>：实验显示Transformer-VQ的收敛速度与标准Transformer相当，有时甚至更快（因为参数更少）。</p>
<p><strong>定理21.2（局部最优）</strong>：由于VQ操作的离散性，Transformer-VQ的优化景观可能有更多局部最优。但实践中这不是主要问题，可能因为：<br />
1. 编码表的连续性（$C$是连续优化的）<br />
2. 任务损失的平滑性<br />
3. 大模型的过参数化</p>
<h3 id="22">22. 模型容量与参数效率<a class="toc-link" href="#22" title="Permanent link">&para;</a></h3>
<p><strong>定义22.1（参数数量）</strong>：Transformer-VQ相比标准Transformer增加的参数：</p>
<p>$$<br />
\Delta \text{Params} = L \times c \times d_k<br />
$$</p>
<p>其中$L$是层数。对于$L = 24, c = 512, d_k = 128$：</p>
<p>$$<br />
\Delta \text{Params} = 24 \times 512 \times 128 = 1.57M<br />
$$</p>
<p>这在总参数量（通常$&gt;100M$）中占比很小。</p>
<p><strong>命题22.1（参数效率）</strong>：Transformer-VQ的参数效率很高：<br />
- 增加的参数：$\sim 1\%$<br />
- 获得的加速：$&gt;10\times$（长序列）</p>
<p><strong>定理22.1（模型容量的保持）</strong>：尽管Key被量化，但模型的整体容量基本保持不变，因为：<br />
1. Query $Q$没有被量化，仍保留完整信息<br />
2. Value $V$没有被量化，仍保留完整信息<br />
3. 只有Key的"索引"被量化，但通过编码表$C$恢复为连续值</p>
<h3 id="23">23. 从预训练模型微调<a class="toc-link" href="#23" title="Permanent link">&para;</a></h3>
<p><strong>定义23.1（从预训练模型出发）</strong>：给定预训练的标准Transformer，如何转换为Transformer-VQ？</p>
<p><strong>算法23.1（转换步骤）</strong>：</p>
<ol>
<li>
<p><strong>采样Key分布</strong>：从预训练模型中前向传播一些样本，收集Key ${K_i^{(l)}}$（每层）</p>
</li>
<li>
<p><strong>K-Means聚类</strong>：对每层的Key进行K-Means，得到$c$个聚类中心作为编码表$C^{(l)}$</p>
</li>
<li>
<p><strong>初始化VQ层</strong>：在每层的Attention中加入VQ模块，使用$C^{(l)}$初始化</p>
</li>
<li>
<p><strong>微调</strong>：使用较小的学习率微调整个模型</p>
</li>
</ol>
<p><strong>定理23.1（微调的有效性）</strong>：这种微调策略是有效的，因为：</p>
<ol>
<li><strong>初始近似</strong>：K-Means保证初始的量化误差较小</li>
<li><strong>渐进优化</strong>：微调可以进一步优化编码表和其他参数</li>
<li><strong>正则化效应</strong>：VQ起到正则化作用，可能提高泛化能力</li>
</ol>
<p><strong>命题23.1（与从零训练的对比）</strong>：<br />
- 从零训练：需要更长的训练时间，但可能达到更优的解<br />
- 微调：更快，但受限于预训练模型的初始化</p>
<h3 id="24">24. 理论局限性与未来方向<a class="toc-link" href="#24" title="Permanent link">&para;</a></h3>
<p><strong>局限24.1（编码表大小的限制）</strong>：编码表大小$c$是一个超参数，需要权衡：<br />
- 太小：量化误差大，性能下降<br />
- 太大：加速比降低，接近标准Transformer</p>
<p><strong>局限24.2（动态性的限制）</strong>：编码表$C$在整个序列中是固定的，不能根据上下文动态调整。</p>
<p><strong>未来方向24.1（自适应编码表）</strong>：可以设计上下文相关的编码表：</p>
<p>$$<br />
C(x_{&lt;i}) = f(x_{&lt;i}, C_0)<br />
$$</p>
<p>其中$f$是一个轻量级的网络，$C_0$是基础编码表。</p>
<p><strong>未来方向24.2（多尺度量化）</strong>：类似Residual VQ，可以使用多个编码表：</p>
<p>$$<br />
\hat{K}_i = c_1 + c_2 + \cdots + c_L<br />
$$</p>
<p>每个$c_l$来自不同的编码表$C^{(l)}$，提供不同粒度的近似。</p>
<p><strong>未来方向24.3（Query量化）</strong>：目前只量化Key，是否可以也量化Query？</p>
<p>$$<br />
\hat{Q}_i = VQ(Q_i, C_Q), \quad \hat{K}_i = VQ(K_i, C_K)<br />
$$</p>
<p>这可能进一步降低复杂度，但需要小心设计以避免过度的信息损失。</p>
<p><strong>未来方向24.4（与其他优化技术结合）</strong>：<br />
- Flash Attention：优化内存访问模式<br />
- Mixture of Experts：结合稀疏激活<br />
- Quantization-Aware Training：更激进的量化策略</p>
<h3 id="25">25. 总结与结论<a class="toc-link" href="#25" title="Permanent link">&para;</a></h3>
<p><strong>核心贡献25.1</strong>：Transformer-VQ提出了一个简单而有效的想法：</p>
<p>$$<br />
\text{只需VQ一下Key，复杂度就从} O(n^2) \text{降到} O(n)<br />
$$</p>
<p><strong>理论优势25.2</strong>：<br />
1. <strong>线性复杂度</strong>：$\mathcal{O}(nc(d_k + d_v))$，当$c \ll n$时为$\mathcal{O}(n)$<br />
2. <strong>保留结构</strong>：保持softmax结构，不像线性Attention需要核函数近似<br />
3. <strong>简单实现</strong>："挑出"技巧使得实现简单高效<br />
4. <strong>理论保证</strong>：量化误差可控，近似质量有界</p>
<p><strong>实践价值25.3</strong>：<br />
1. <strong>显著加速</strong>：长序列上10-30倍加速<br />
2. <strong>性能保持</strong>：实验显示性能接近标准Transformer<br />
3. <strong>易于部署</strong>：可以从预训练模型微调得到</p>
<p><strong>理论洞察25.4</strong>：<br />
1. <strong>VQ不仅用于VAE</strong>：VQ在Transformer中也有重要应用<br />
2. <strong>离散化的好处</strong>：适度的离散化可以在保持性能的同时大幅降低复杂度<br />
3. <strong>Key的特殊性</strong>：Key比Query和Value更适合量化（因为Key主要用于索引）</p>
<p><strong>开放问题25.5</strong>：<br />
1. 为什么量化Key比量化Query或Value更有效？<br />
2. 编码表大小$c$的理论最优值是多少？<br />
3. 如何设计自适应的编码表？<br />
4. Transformer-VQ的表达能力的精确刻画？</p>
<hr />
<p>本推导全面分析了Transformer-VQ的数学原理、复杂度降低的机制、信息损失的定量分析、与其他方法的对比等，为理解这一优雅而强大的技术提供了坚实的理论基础。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="简单得令人尴尬的fsq四舍五入超越了vq-vae.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#116 简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="生活杂记炒锅的尽头是铁锅.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#118 【生活杂记】炒锅的尽头是铁锅</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#vqkeytransformer">VQ一下Key，Transformer的复杂度就变成线性了</a><ul>
<li><a href="#_1">高效难题</a></li>
<li><a href="#vq">VQ一下</a></li>
<li><a href="#encoder">Encoder</a></li>
<li><a href="#decoder">Decoder</a></li>
<li><a href="#_2">局域增强</a></li>
<li><a href="#_3">梯度回传</a></li>
<li><a href="#_4">实验结果</a></li>
<li><a href="#_5">发散思考</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a><ul>
<li><a href="#1-attention">1. 标准Attention的复杂度分析</a></li>
<li><a href="#2-key">2. Key向量量化的数学表示</a></li>
<li><a href="#3-transformer-vq-encoder">3. Transformer-VQ Encoder的复杂度分析</a></li>
<li><a href="#4">4. "挑出"技巧的数学原理</a></li>
<li><a href="#5-decoder">5. Decoder的复杂度分析</a></li>
<li><a href="#6-block-wise">6. Block-wise计算的分析</a></li>
<li><a href="#7-attention-bias">7. 加入Attention Bias的复杂度</a></li>
<li><a href="#8">8. 信息损失的定量分析</a></li>
<li><a href="#9">9. 近似误差界</a></li>
<li><a href="#10-attention">10. 与线性Attention的对比</a></li>
<li><a href="#11-rnnstate-space-model">11. 与RNN/State Space Model的对比</a></li>
<li><a href="#12">12. 梯度传播的分析</a></li>
<li><a href="#13">13. 编码表学习的理论</a></li>
<li><a href="#14">14. 实际加速比的理论预测</a></li>
<li><a href="#15">15. 信息瓶颈与表达能力</a></li>
<li><a href="#16">16. 多头注意力的扩展</a></li>
<li><a href="#17">17. 位置编码的兼容性</a></li>
<li><a href="#18-fsq">18. 与FSQ的对比</a></li>
<li><a href="#19">19. 长度外推与泛化</a></li>
<li><a href="#20-sparse-attention">20. 与Sparse Attention的对比</a></li>
<li><a href="#21">21. 训练稳定性与收敛性</a></li>
<li><a href="#22">22. 模型容量与参数效率</a></li>
<li><a href="#23">23. 从预训练模型微调</a></li>
<li><a href="#24">24. 理论局限性与未来方向</a></li>
<li><a href="#25">25. 总结与结论</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>