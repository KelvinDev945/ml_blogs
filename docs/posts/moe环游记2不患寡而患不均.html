<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MoE环游记：2、不患寡而患不均 | ML & Math Blog Posts</title>
    <meta name="description" content="MoE环游记：2、不患寡而患不均&para;
原文链接: https://spaces.ac.cn/archives/10735
发布日期: 

在上一篇文章《MoE环游记：1、从几何意义出发》中，我们介绍了MoE的一个几何诠释，旨在通过Dense模型的最佳逼近出发来推导和理解MoE。同时在文末我们也说了，给出MoE的计算公式仅仅是开始，训练一个实际有效的MoE模型还有很多细节补，比如本文要讨论的...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #183 MoE环游记：2、不患寡而患不均
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#183</span>
                MoE环游记：2、不患寡而患不均
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-02-21</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=损失函数" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 损失函数</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=稀疏" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 稀疏</span>
                </a>
                
                <a href="../index.html?tags=moe" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> moe</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="moe2">MoE环游记：2、不患寡而患不均<a class="toc-link" href="#moe2" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10735">https://spaces.ac.cn/archives/10735</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在上一篇文章<a href="/archives/10699">《MoE环游记：1、从几何意义出发》</a>中，我们介绍了MoE的一个几何诠释，旨在通过Dense模型的最佳逼近出发来推导和理解MoE。同时在文末我们也说了，给出MoE的计算公式仅仅是开始，训练一个实际有效的MoE模型还有很多细节补，比如本文要讨论的负载均衡（Load Balance）问题。</p>
<p>负载均衡，即“不患寡而患不均”，说白了就是让每个Expert都在干活，并且都在干尽可能一样多的活，避免某些Expert浪费算力。负载均衡既是充分利用训练算力的需求，也是尽可能发挥MoE大参数量潜力的需求。</p>
<h2 id="_1">需求分析<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>我们知道，MoE的基本形式是<br />
\begin{equation}\boldsymbol{y} = \sum_{i\in \mathop{\text{argtop}}_k \boldsymbol{\rho}} \rho_i \boldsymbol{e}_i\end{equation}<br />
对于传统MoE，$\boldsymbol{\rho}$是一个概率分布（Router），$\boldsymbol{e}_i=\boldsymbol{v}_i$，$\boldsymbol{v}_i$是一个小型FFN（Expert）的输出；而对于我们上一篇推导的几何MoE，$\boldsymbol{\rho}$没有归一化的要求，它预测的是Expert的模长，而$\boldsymbol{e}_i=\boldsymbol{v}_i/\Vert\boldsymbol{v}_i\Vert$预测的是Expert的方向。</p>
<p>不管哪种格式的MoE，实际表现都差不多，只是理解视角的不同。但要注意，虽然MoE的公式给人的感觉是“每遇到一个Token，就去找相应的Expert来计算”，但实际训练时其实是反过来的：先给每个Expert分配好相应的算力，然后将Token分配（Route）到所属的Expert中并行计算，这也就为什么负责打分的$\boldsymbol{\rho}$被称为Router。</p>
<p>这样一来，如果Expert的分配不均衡，就可能出现如下局面：某些Expert（Dead Expert）几乎一直闲置，浪费算力；某些Expert要处理的Token太多，根本忙不过来，只能Token Drop（即放弃处理部分Token）。从理论上来说，出现Dead Expert意味着MoE没有达到预期的参数量，即花了大参数量的显存，结果只训出来小参数量的效果。</p>
<p>所以，不管是从训练还是性能角度看，我们都希望保证Expert的负载均衡。</p>
<h2 id="_2">辅助损失<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>促进负载均衡的常规思路是添加与之相关的损失函数，我们通常称之为“Aux Loss（Auxiliary Loss）”，目前主流用的Aux Loss最早可以追溯到2020年的<a href="https://papers.cool/arxiv/2006.16668">《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》</a>。</p>
<p>介绍Aux Loss之前，我们需要先引入一些新概念。首先，我们已经提到对于一般的MoE来说，$\boldsymbol{\rho}$未必是概率分布，我们将归一化的$\boldsymbol{\rho}$记为$\boldsymbol{p}=[p_1,p_2,\cdots,p_n]$，以及它Top-$k$版为$\boldsymbol{f}=[f_1,f_2,\cdots,f_n]$，其中<br />
\begin{equation}p_i = \frac{\rho_i}{\sum_{i=1}^n \rho_i},\qquad f_i = \left\{\begin{aligned}1/k, \quad i\in \mathop{\text{argtop}}\nolimits_k \boldsymbol{\rho} \\<br />
0, \quad i\not\in \mathop{\text{argtop}}\nolimits_k \boldsymbol{\rho}\end{aligned}\right.\end{equation}<br />
接着我们定义$\boldsymbol{P}=\mathbb{E}[\boldsymbol{p}],\boldsymbol{F}=\mathbb{E}[\boldsymbol{f}]$，这里的$\mathbb{E}$是指对所有样本的所有Token做平均。不难看出，$\boldsymbol{F}$就是Expert当前的负载分布，而$\boldsymbol{P}$则相当于$\boldsymbol{F}$的一个光滑近似。</p>
<p>有了这些记号，我们就可以写出Aux Loss为：<br />
\begin{equation}\mathcal{L}<em i="1">{\text{aux}} = \boldsymbol{F}\cdot \boldsymbol{P} = \sum</em>}^n F_i P_i\label{eq:aux-loss}\end{equation<br />
一般文献定义Aux Loss会多乘一个$n$，即它们的Aux Loss等于这里的$n \mathcal{L}_{\text{aux}}$。此外，有些大型MoE可能会按设备来算Aux Loss，以达到设备内的均衡，减少设备间的通信，这些就各自发挥了。但也有较新的实验显示，强行局部均衡极有可能影响模型最终效果。</p>
<h2 id="_3">直通估计<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>不知道大家有没有发现一个奇怪的现象：不管是最早出处、后续文献还是科普文章，总之笔者阅读过的资料中，对Aux Loss的引用都是不加证明的，似乎大家都公认上述Aux Loss能促进均衡是一件显然成立的事情。可真有这么显然易得吗？</p>
<p>反正笔者是没看出来，所以接下来笔者给出式$\eqref{eq:aux-loss}$的一种推导思路，由此思路我们还可以自定义其他形式的Aux Loss。首先，定义均匀分布$\boldsymbol{Q}=(1/n,1/n,\cdots,1/n)$，刚才我们说了$\boldsymbol{F}$就是当前负载分布，因此负载均衡等价于$\boldsymbol{F}=\boldsymbol{Q}$，那么下式就是一个比较直观的Aux Loss：<br />
\begin{equation}\mathcal{L}<em i="1">{\text{aux}} = \frac{1}{2}\Vert\boldsymbol{F} - \boldsymbol{Q}\Vert^2 = \frac{1}{2}\sum</em>}^n (F_i - 1/n)^2\label{eq:aux-loss-2}\end{equation<br />
问题是$\boldsymbol{F}$是由$\mathop{\text{argtop}}<em _text_aux="\text{aux">k$出来的，这意味着上式并不是一个能直接用的可导目标。怎么解决这个问题呢？答案是<a href="/archives/6760#%E8%87%AA%E8%A1%8C%E8%AE%BE%E8%AE%A1%E6%A2%AF%E5%BA%A6">STE（Straight-Through Estimator）</a>技巧，分别设计前向传播和反向传播的函数。具体来说，$\boldsymbol{F}$不可导，$\boldsymbol{P}$作为它的光滑近似是可导的，那么我们在反向传播的时候将$\boldsymbol{F}$替换成$\boldsymbol{P}$就行了，即<br />
\begin{equation}\mathcal{L}</em>}} = \frac{1}{2}\Vert \boldsymbol{P} + \text{sg}[\boldsymbol{F}-\boldsymbol{P}] - \boldsymbol{Q}\Vert^2 = \frac{1}{2}\sum_{i=1}^n (P_i + \text{sg}[F_i - P_i] - 1/n)^2\label{eq:aux-loss-3}\end{equation<br />
其中$\text{sg}[]$是stop gradient算子，特点是保持前向输出不变，但强制梯度为零。这样改动之后，$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{aux}}$就是一个切实可行的Aux Loss了，我们可以试求一下它的梯度：<br />
\begin{equation}\begin{aligned}<br />
\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">{\text{aux}} =&amp;\, \frac{1}{2}\nabla</em>[F_i - P_i] - 1/n)^2 \\}}\sum_{i=1}^n (P_i + \text{sg<br />
=&amp;\, \sum_{i=1}^n (P_i + \text{sg}[F_i - P_i] - 1/n) \nabla_{\boldsymbol{\theta}}(P_i + \text{sg}[F_i - P_i] - 1/n)\\<br />
=&amp;\, \sum_{i=1}^n (F_i - 1/n) \nabla_{\boldsymbol{\theta}}P_i = \nabla_{\boldsymbol{\theta}}\sum_{i=1}^n (F_i - 1/n) P_i\\<br />
=&amp;\, \nabla_{\boldsymbol{\theta}}\left(\sum_{i=1}^n F_i P_i\right)<br />
\end{aligned}\end{equation}<br />
这里$\boldsymbol{\theta}$是模型参数。最后的结果表明式$\eqref{eq:aux-loss-3}$的梯度等于式$\eqref{eq:aux-loss}$梯度，这意味着用式$\eqref{eq:aux-loss}$作为Aux Loss跟式$\eqref{eq:aux-loss-3}$在梯度上是等价的，所以就出现了式$\eqref{eq:aux-loss}$的Aux Loss。</p>
<p>然而，式$\eqref{eq:aux-loss}$只有等效梯度的意义，但没有Loss的意义，不算一个真正的Loss，比如当$\boldsymbol{F} = \boldsymbol{P}$时我们可以算出式$\eqref{eq:aux-loss}$等于$1/n$，但实际上我们可以构造出一个不等于$\boldsymbol{P}$的$\boldsymbol{F}$让它小于$1/n$，所以式$\eqref{eq:aux-loss}$并不是像正常的Loss一样越小越好，最小值也不是$\boldsymbol{F} = \boldsymbol{P}$时取到。</p>
<h2 id="_4">一般形式<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>上述推导实际上提供了构建Aux Loss的一般思路：<strong>首先基于$\boldsymbol{F}$构建符合要求的损失，然后在实现时将$\boldsymbol{F}$替换成$\boldsymbol{P} + \text{sg}[\boldsymbol{F}-\boldsymbol{P}]$。</strong> 比如，我们知道最大熵也可以将分布推向均衡，因此也可以用熵的相反数来构建Aux Loss：<br />
\begin{equation}\mathcal{L}<em i="1">{\text{aux}} = \sum</em>}^n (P_i + \text{sg}[F_i - P_i])\log(P_i + \text{sg}[F_i - P_i])\end{equation<br />
上式就可以直接用作代码实现，当然如果我们追求简化，也可以类似地求梯度，结果将是<br />
\begin{equation}\nabla_{\boldsymbol{\theta}}\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{aux}} = \nabla</em>}}\sum_{i=1}^n(P_i + \text{sg}[F_i - P_i]) \log(P_i + \text{sg}[F_i - P_i]) = \nabla_{\boldsymbol{\theta}}\sum_{i=1}^n P_i \log F_i\end{equation<br />
两次简化梯度的过程中，我们都用到了如下恒等式<br />
\begin{equation}\sum_{i=1}^n \nabla_{\boldsymbol{\theta}}P_i = \nabla_{\boldsymbol{\theta}}\sum_{i=1}^n P_i = \nabla_{\boldsymbol{\theta}}1 = \boldsymbol{0}\end{equation}<br />
这依赖于$\boldsymbol{P}$是一个概率分布，以及目标分布$\boldsymbol{Q}$是均匀分布的事实。而如果我们不追求简化后的等价结果，而是直接用$\boldsymbol{F}\to \boldsymbol{P} + \text{sg}[\boldsymbol{F}-\boldsymbol{P}]$形式的Aux Loss，那么可以不受这两个约束。</p>
<p>比如，$\boldsymbol{P}$作为$\boldsymbol{F}$光滑近似这一点，我们只用到了“$P_i$大$F_i$通常也大”的性质，所以用非归一化的$\mathbb{E}[\boldsymbol{\rho}]$作为$\boldsymbol{P}$通常也没问题，这一点在一些特殊场景（例如有正有负的$\boldsymbol{\rho}$）可能会比较关键，因为此时无法归一化为概率分布。又比如目标$\Vert\boldsymbol{F} - \boldsymbol{Q}\Vert^2$，显然能将$\boldsymbol{F}$推向任意我们想要的、不一定是均匀的目标分布$\boldsymbol{Q}$。</p>
<h2 id="_5">文章小结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文介绍了MoE的负载均衡问题，并给出了一种构建Aux Loss的一般思路。除了Aux Loss外，促进负载均衡还有一些其他方案，我们下回再谈。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10735">https://spaces.ac.cn/archives/10735</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Feb. 21, 2025). 《MoE环游记：2、不患寡而患不均 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10735">https://spaces.ac.cn/archives/10735</a></p>
<p>@online{kexuefm-10735,<br />
title={MoE环游记：2、不患寡而患不均},<br />
author={苏剑林},<br />
year={2025},<br />
month={Feb},<br />
url={\url{https://spaces.ac.cn/archives/10735}},<br />
} </p>
<hr />
<h2 id="_6">公式推导与注释<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本节将对MoE负载均衡问题进行深入的数学推导与分析，包括MoE的数学定义、负载均衡损失的理论基础、均匀分布的必要性证明、熵正则化的作用机制、不同平衡策略的数学对比，以及收敛性与稳定性的理论分析。</p>
<h3 id="1-moe">1. MoE的数学定义与路由机制<a class="toc-link" href="#1-moe" title="Permanent link">&para;</a></h3>
<h4 id="11-moe">1.1 基本MoE架构的数学表示<a class="toc-link" href="#11-moe" title="Permanent link">&para;</a></h4>
<p>对于一个包含$n$个专家的MoE系统，给定输入$\boldsymbol{x} \in \mathbb{R}^d$，MoE的输出定义为：</p>
<p>$$<br />
\boldsymbol{y} = \sum_{i=1}^{n} g_i(\boldsymbol{x}) \boldsymbol{e}_i(\boldsymbol{x})<br />
$$</p>
<p>其中：<br />
- $g_i(\boldsymbol{x})$是路由网络（Router）的输出，决定第$i$个专家的权重<br />
- $\boldsymbol{e}<em i="1">i(\boldsymbol{x})$是第$i$个专家网络的输出<br />
- $\sum</em>) \geq 0$（概率分布约束）}^{n} g_i(\boldsymbol{x}) = 1$且$g_i(\boldsymbol{x</p>
<p><strong>推导1：Router的Softmax表示</strong></p>
<p>Router通常通过一个线性变换后接Softmax实现：</p>
<p>$$<br />
\boldsymbol{\rho} = \boldsymbol{W}_r \boldsymbol{x} + \boldsymbol{b}_r<br />
$$</p>
<p>其中$\boldsymbol{W}_r \in \mathbb{R}^{n \times d}$，$\boldsymbol{b}_r \in \mathbb{R}^n$。然后对$\boldsymbol{\rho}$应用Softmax：</p>
<p>$$<br />
g_i(\boldsymbol{x}) = \frac{e^{\rho_i}}{\sum_{j=1}^{n} e^{\rho_j}}<br />
$$</p>
<p>这保证了$\sum_{i=1}^{n} g_i(\boldsymbol{x}) = 1$。</p>
<h4 id="12-top-kmoe">1.2 Top-K稀疏化MoE<a class="toc-link" href="#12-top-kmoe" title="Permanent link">&para;</a></h4>
<p>为了减少计算量，实际应用中采用Top-K稀疏化：</p>
<p>$$<br />
\boldsymbol{y} = \sum_{i \in \mathop{\text{argtop}}_k \boldsymbol{\rho}} \rho_i \boldsymbol{e}_i<br />
$$</p>
<p><strong>推导2：Top-K操作的数学定义</strong></p>
<p>定义Top-K选择函数：</p>
<p>$$<br />
\mathcal{I}<em i_1="i_1">k(\boldsymbol{\rho}) = {i_1, i_2, \ldots, i_k} \quad \text{满足} \quad \rho</em>_k} \geq \rho_{i_2} \geq \cdots \geq \rho_{i_k} \geq \rho_j, \forall j \notin \mathcal{I<br />
$$</p>
<p>则Top-K稀疏MoE可以重写为：</p>
<p>$$<br />
\boldsymbol{y} = \sum_{i=1}^{n} \mathbb{1}_{i \in \mathcal{I}_k(\boldsymbol{\rho})} \cdot \rho_i \boldsymbol{e}_i<br />
$$</p>
<p>其中$\mathbb{1}_{i \in \mathcal{I}_k(\boldsymbol{\rho})}$是指示函数。</p>
<h4 id="13-moe">1.3 几何MoE的解释<a class="toc-link" href="#13-moe" title="Permanent link">&para;</a></h4>
<p><strong>推导3：几何视角下的MoE分解</strong></p>
<p>在几何MoE中，我们将专家输出分解为模长和方向：</p>
<p>$$<br />
\boldsymbol{e}_i = \rho_i \frac{\boldsymbol{v}_i}{|\boldsymbol{v}_i|}<br />
$$</p>
<p>其中：<br />
- $\rho_i$预测的是专家贡献的幅度（模长）<br />
- $\frac{\boldsymbol{v}_i}{|\boldsymbol{v}_i|}$预测的是专家贡献的方向（单位向量）</p>
<p>这样，MoE的输出变为：</p>
<p>$$<br />
\boldsymbol{y} = \sum_{i \in \mathop{\text{argtop}}_k \boldsymbol{\rho}} \rho_i \frac{\boldsymbol{v}_i}{|\boldsymbol{v}_i|}<br />
$$</p>
<p><strong>推导4：非归一化Router的有效性</strong></p>
<p>几何MoE不要求$\boldsymbol{\rho}$归一化，因为我们只关心相对大小：</p>
<p>$$<br />
\mathop{\text{argtop}}_k \boldsymbol{\rho} = \mathop{\text{argtop}}_k (c\boldsymbol{\rho}) \quad \forall c &gt; 0<br />
$$</p>
<p>这给予了模型更大的表达自由度。</p>
<h3 id="2">2. 负载均衡问题的数学分析<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 负载分布的定义<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p><strong>推导5：归一化Router分数$\boldsymbol{p}$</strong></p>
<p>对于任意Router输出$\boldsymbol{\rho}$，定义归一化版本：</p>
<p>$$<br />
p_i = \frac{\rho_i}{\sum_{j=1}^{n} \rho_j}<br />
$$</p>
<p>显然$\sum_{i=1}^{n} p_i = 1$，$\boldsymbol{p}$形成一个概率分布。</p>
<p><strong>推导6：Top-K离散分布$\boldsymbol{f}$</strong></p>
<p>定义Top-K后的离散分布：</p>
<p>$$<br />
f_i = \begin{cases}<br />
\frac{1}{k}, &amp; i \in \mathop{\text{argtop}}_k \boldsymbol{\rho} \<br />
0, &amp; i \notin \mathop{\text{argtop}}_k \boldsymbol{\rho}<br />
\end{cases}<br />
$$</p>
<p>验证归一化：$\sum_{i=1}^{n} f_i = k \cdot \frac{1}{k} = 1$ ✓</p>
<p><strong>推导7：全局负载分布</strong></p>
<p>对所有样本的所有Token求平均，得到期望负载分布：</p>
<p>$$<br />
\boldsymbol{F} = \mathbb{E}<em _text_tokens="\text{tokens">{\text{tokens}}[\boldsymbol{f}], \quad \boldsymbol{P} = \mathbb{E}</em>]}}[\boldsymbol{p<br />
$$</p>
<p>其中$\boldsymbol{P}$是$\boldsymbol{F}$的光滑近似，因为：</p>
<p>$$<br />
\lim_{\tau \to 0} \boldsymbol{P}(\boldsymbol{\rho}/\tau) = \boldsymbol{F}(\boldsymbol{\rho})<br />
$$</p>
<h4 id="22">2.2 负载不均衡的代价<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p><strong>推导8：Dead Expert的参数浪费</strong></p>
<p>如果专家$i$的负载$F_i \approx 0$，则该专家的参数$\boldsymbol{\theta}_i$几乎不参与训练：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\theta}_i} = \mathbb{E}\left[\frac{\partial \mathcal{L}}{\partial \boldsymbol{e}_i} \frac{\partial \boldsymbol{e}_i}{\partial \boldsymbol{\theta}_i}\right] \approx 0 \quad \text{当} \quad F_i \to 0<br />
$$</p>
<p>设模型总参数为$M$，若$m$个专家处于Dead状态，则有效参数量降为：</p>
<p>$$<br />
M_{\text{eff}} = M - m \cdot |\boldsymbol{\theta}_i| \ll M<br />
$$</p>
<p><strong>推导9：Token Drop的损失</strong></p>
<p>当某个专家负载过高时，会发生Token Drop。设专家$i$的容量为$C_i$，但分配的Token数为$T_i &gt; C_i$，则丢弃比例为：</p>
<p>$$<br />
\text{Drop Rate}_i = \frac{T_i - C_i}{T_i} = 1 - \frac{C_i}{T_i}<br />
$$</p>
<p>丢弃Token会导致信息损失，影响模型性能。</p>
<h3 id="3-aux-loss">3. 辅助损失（Aux Loss）的数学推导<a class="toc-link" href="#3-aux-loss" title="Permanent link">&para;</a></h3>
<h4 id="31-aux-loss">3.1 标准Aux Loss的形式<a class="toc-link" href="#31-aux-loss" title="Permanent link">&para;</a></h4>
<p><strong>推导10：GShard Aux Loss</strong></p>
<p>GShard提出的辅助损失为：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{aux}} = n \sum</em> F_i P_i}^{n<br />
$$</p>
<p>或不带系数$n$的版本（本文采用）：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{aux}} = \sum</em>}^{n} F_i P_i = \boldsymbol{F} \cdot \boldsymbol{P<br />
$$</p>
<p><strong>推导11：为什么这个形式能促进均衡？</strong></p>
<p>定义均匀分布$\boldsymbol{Q} = (\frac{1}{n}, \frac{1}{n}, \ldots, \frac{1}{n})$。如果$\boldsymbol{F} = \boldsymbol{Q}$（完全均衡），则：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{aux}} = \sum</em>}^{n} \frac{1}{n} P_i = \frac{1}{n} \sum_{i=1}^{n} P_i = \frac{1}{n<br />
$$</p>
<p>但这不是最小值！考虑极端不均衡情况，如$F_1 = 1, F_i = 0 (i&gt;1)$，此时：</p>
<p>$$<br />
\mathcal{L}_{\text{aux}} = P_1 \leq 1<br />
$$</p>
<p>所以$\mathcal{L}_{\text{aux}}$本身不是标准的"越小越好"的损失。</p>
<h4 id="32-steaux-loss">3.2 通过直通估计器（STE）理解Aux Loss<a class="toc-link" href="#32-steaux-loss" title="Permanent link">&para;</a></h4>
<p><strong>推导12：基于均方误差的真实损失</strong></p>
<p>更自然的负载均衡损失应该是：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{true}} = \frac{1}{2} \sum</em>\right)^2}^{n} (F_i - Q_i)^2 = \frac{1}{2} \sum_{i=1}^{n} \left(F_i - \frac{1}{n<br />
$$</p>
<p>这确实是"越小越好"的损失，最小值为0（当$\boldsymbol{F} = \boldsymbol{Q}$时）。</p>
<p><strong>推导13：STE替换$\boldsymbol{F} \to \boldsymbol{P}$</strong></p>
<p>由于$\boldsymbol{F}$包含不可微的$\text{argtop}_k$操作，我们用STE技巧：</p>
<p>$$<br />
\tilde{F}_i = P_i + \text{sg}[F_i - P_i]<br />
$$</p>
<p>其中$\text{sg}[\cdot]$是stop gradient算子：<br />
- 前向传播：$\text{sg}[x] = x$<br />
- 反向传播：$\frac{\partial \text{sg}[x]}{\partial x} = 0$</p>
<p>将$F_i$替换为$\tilde{F}_i$：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{ste}} = \frac{1}{2} \sum</em>\right)^2}^{n} \left(P_i + \text{sg}[F_i - P_i] - \frac{1}{n<br />
$$</p>
<p><strong>推导14：梯度推导</strong></p>
<p>计算$\mathcal{L}_{\text{ste}}$关于$\boldsymbol{\theta}$的梯度：</p>
<p>$$<br />
\begin{aligned}<br />
\nabla_{\boldsymbol{\theta}} \mathcal{L}<em i="1">{\text{ste}} &amp;= \sum</em>[F_i - P_i]) \}^{n} \left(P_i + \text{sg}[F_i - P_i] - \frac{1}{n}\right) \nabla_{\boldsymbol{\theta}} (P_i + \text{sg<br />
&amp;= \sum_{i=1}^{n} \left(P_i + \text{sg}[F_i - P_i] - \frac{1}{n}\right) \nabla_{\boldsymbol{\theta}} P_i \<br />
&amp;= \sum_{i=1}^{n} \left(F_i - \frac{1}{n}\right) \nabla_{\boldsymbol{\theta}} P_i<br />
\end{aligned}<br />
$$</p>
<p>在第二步我们用了$\nabla_{\boldsymbol{\theta}} \text{sg}[x] = 0$。</p>
<p><strong>推导15：简化为点积形式</strong></p>
<p>继续推导：</p>
<p>$$<br />
\begin{aligned}<br />
\nabla_{\boldsymbol{\theta}} \mathcal{L}<em i="1">{\text{ste}} &amp;= \sum</em> P_i \}^{n} F_i \nabla_{\boldsymbol{\theta}} P_i - \frac{1}{n} \sum_{i=1}^{n} \nabla_{\boldsymbol{\theta}<br />
&amp;= \sum_{i=1}^{n} F_i \nabla_{\boldsymbol{\theta}} P_i - \frac{1}{n} \nabla_{\boldsymbol{\theta}} \left(\sum_{i=1}^{n} P_i\right) \<br />
&amp;= \sum_{i=1}^{n} F_i \nabla_{\boldsymbol{\theta}} P_i \quad (\text{因为} \sum_{i=1}^{n} P_i = 1) \<br />
&amp;= \nabla_{\boldsymbol{\theta}} \sum_{i=1}^{n} F_i P_i<br />
\end{aligned}<br />
$$</p>
<p>这证明了$\mathcal{L}<em i="1">{\text{aux}} = \sum</em> F_i P_i$的梯度等价性！}^{n</p>
<h3 id="4">4. 均匀分布的理论分析<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 均匀分布的信息论意义<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p><strong>推导16：熵与均匀分布</strong></p>
<p>分布$\boldsymbol{F}$的熵定义为：</p>
<p>$$<br />
H(\boldsymbol{F}) = -\sum_{i=1}^{n} F_i \log F_i<br />
$$</p>
<p>对于固定的$n$，熵在均匀分布时达到最大：</p>
<p>$$<br />
H(\boldsymbol{Q}) = -\sum_{i=1}^{n} \frac{1}{n} \log \frac{1}{n} = \log n<br />
$$</p>
<p><strong>推导17：熵最大性的证明</strong></p>
<p>使用拉格朗日乘数法，要最大化$H(\boldsymbol{F})$约束于$\sum_{i=1}^{n} F_i = 1$：</p>
<p>$$<br />
\mathcal{L} = -\sum_{i=1}^{n} F_i \log F_i + \lambda \left(\sum_{i=1}^{n} F_i - 1\right)<br />
$$</p>
<p>对$F_i$求偏导并令其为零：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial F_i} = -\log F_i - 1 + \lambda = 0<br />
$$</p>
<p>得到$\log F_i = \lambda - 1$，即$F_i = e^{\lambda - 1}$对所有$i$都相同。结合约束条件$\sum_{i=1}^{n} F_i = 1$，得到$F_i = \frac{1}{n}$。</p>
<p><strong>推导18：KL散度与均匀分布</strong></p>
<p>从当前分布$\boldsymbol{F}$到目标均匀分布$\boldsymbol{Q}$的KL散度：</p>
<p>$$<br />
D_{KL}(\boldsymbol{F} | \boldsymbol{Q}) = \sum_{i=1}^{n} F_i \log \frac{F_i}{Q_i} = \sum_{i=1}^{n} F_i \log(n F_i)<br />
$$</p>
<p>展开：</p>
<p>$$<br />
D_{KL}(\boldsymbol{F} | \boldsymbol{Q}) = \log n + \sum_{i=1}^{n} F_i \log F_i = \log n - H(\boldsymbol{F})<br />
$$</p>
<p>所以最大化熵等价于最小化KL散度。</p>
<h4 id="42">4.2 均匀分布的容量利用<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p><strong>推导19：有效容量分析</strong></p>
<p>假设每个专家容量为$C$，总容量$C_{\text{total}} = nC$。在分布$\boldsymbol{F}$下，期望利用的容量为：</p>
<p>$$<br />
C_{\text{used}} = \sum_{i=1}^{n} \min(F_i \cdot T, C)<br />
$$</p>
<p>其中$T$是总Token数。当$\boldsymbol{F}$均匀时，$F_i = \frac{1}{n}$，如果$\frac{T}{n} \leq C$，则：</p>
<p>$$<br />
C_{\text{used}} = \sum_{i=1}^{n} \frac{T}{n} = T<br />
$$</p>
<p>所有Token都被处理，无Drop。</p>
<p><strong>推导20：不均匀分布的容量损失</strong></p>
<p>假设分布为$(F_1, F_2, \ldots, F_n) = (0.5, 0.5, 0, \ldots, 0)$（两个专家分担所有负载）。设$C = \frac{T}{n}$，则：</p>
<p>$$<br />
C_{\text{used}} = 2 \cdot \min(0.5T, C) = 2C = \frac{2T}{n}<br />
$$</p>
<p>浪费了$\frac{n-2}{n}$的容量！当$n$很大时，这是巨大的浪费。</p>
<h3 id="5">5. 熵正则化的理论分析<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51-aux-loss">5.1 基于熵的Aux Loss<a class="toc-link" href="#51-aux-loss" title="Permanent link">&para;</a></h4>
<p><strong>推导21：熵正则化损失</strong></p>
<p>另一种促进均衡的方式是直接最大化熵（或最小化负熵）：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{entropy}} = -H(\boldsymbol{F}) = \sum</em> F_i \log F_i}^{n<br />
$$</p>
<p>使用STE技巧：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{entropy-ste}} = \sum</em>[F_i - P_i])}^{n} (P_i + \text{sg}[F_i - P_i]) \log(P_i + \text{sg<br />
$$</p>
<p><strong>推导22：熵损失的梯度</strong></p>
<p>计算梯度：</p>
<p>$$<br />
\begin{aligned}<br />
\nabla_{\boldsymbol{\theta}} \mathcal{L}<em i="1">{\text{entropy-ste}} &amp;= \sum</em>[F_i - P_i])] \}^{n} \nabla_{\boldsymbol{\theta}} [(P_i + \text{sg}[F_i - P_i]) \log(P_i + \text{sg<br />
&amp;= \sum_{i=1}^{n} (\log(P_i + \text{sg}[F_i - P_i]) + 1) \nabla_{\boldsymbol{\theta}} P_i \<br />
&amp;= \sum_{i=1}^{n} (\log F_i + 1) \nabla_{\boldsymbol{\theta}} P_i<br />
\end{aligned}<br />
$$</p>
<p><strong>推导23：梯度简化</strong></p>
<p>利用$\sum_{i=1}^{n} \nabla_{\boldsymbol{\theta}} P_i = 0$：</p>
<p>$$<br />
\begin{aligned}<br />
\nabla_{\boldsymbol{\theta}} \mathcal{L}<em i="1">{\text{entropy-ste}} &amp;= \sum</em> P_i \}^{n} \log F_i \nabla_{\boldsymbol{\theta}} P_i + \sum_{i=1}^{n} \nabla_{\boldsymbol{\theta}<br />
&amp;= \sum_{i=1}^{n} \log F_i \nabla_{\boldsymbol{\theta}} P_i \<br />
&amp;= \nabla_{\boldsymbol{\theta}} \sum_{i=1}^{n} P_i \log F_i<br />
\end{aligned}<br />
$$</p>
<p>所以可以使用简化形式：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{entropy-simple}} = \sum</em> P_i \log F_i}^{n<br />
$$</p>
<h4 id="52">5.2 熵正则与点积形式的关系<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p><strong>推导24：泰勒展开分析</strong></p>
<p>在$\boldsymbol{F} \approx \boldsymbol{Q}$附近，对$F_i = \frac{1}{n} + \epsilon_i$展开：</p>
<p>$$<br />
\log F_i = \log\left(\frac{1}{n} + \epsilon_i\right) = \log \frac{1}{n} + \log\left(1 + n\epsilon_i\right) \approx \log \frac{1}{n} + n\epsilon_i<br />
$$</p>
<p>代入熵损失：</p>
<p>$$<br />
\begin{aligned}<br />
\mathcal{L}<em i="1">{\text{entropy-simple}} &amp;= \sum</em> P_i \log F_i \}^{n<br />
&amp;\approx \sum_{i=1}^{n} P_i \left(\log \frac{1}{n} + n\epsilon_i\right) \<br />
&amp;= -\log n + n \sum_{i=1}^{n} P_i \epsilon_i<br />
\end{aligned}<br />
$$</p>
<p>而点积形式在同样的展开下：</p>
<p>$$<br />
\sum_{i=1}^{n} F_i P_i = \sum_{i=1}^{n} \left(\frac{1}{n} + \epsilon_i\right) P_i = \frac{1}{n} + \sum_{i=1}^{n} \epsilon_i P_i<br />
$$</p>
<p>两者在一阶近似下本质相同！</p>
<h3 id="6">6. 不同平衡策略的数学对比<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61-l2">6.1 L2正则化策略<a class="toc-link" href="#61-l2" title="Permanent link">&para;</a></h4>
<p><strong>推导25：L2距离损失</strong></p>
<p>$$<br />
\mathcal{L}<em i="1">{L2} = \frac{1}{2} |\boldsymbol{F} - \boldsymbol{Q}|^2 = \frac{1}{2} \sum</em>\right)^2}^{n} \left(F_i - \frac{1}{n<br />
$$</p>
<p>展开：</p>
<p>$$<br />
\mathcal{L}<em i="1">{L2} = \frac{1}{2} \sum</em>}^{n} F_i^2 - \frac{1}{n} \sum_{i=1}^{n} F_i + \frac{1}{2n<br />
$$</p>
<p>由于$\sum_{i=1}^{n} F_i = 1$，简化为：</p>
<p>$$<br />
\mathcal{L}<em i="1">{L2} = \frac{1}{2} \sum</em>}^{n} F_i^2 - \frac{1}{n} + \frac{1}{2n} = \frac{1}{2} \sum_{i=1}^{n} F_i^2 - \frac{1}{2n<br />
$$</p>
<p><strong>推导26：L2损失与点积形式的关系</strong></p>
<p>注意到：</p>
<p>$$<br />
\sum_{i=1}^{n} F_i^2 = \sum_{i=1}^{n} F_i \cdot F_i<br />
$$</p>
<p>如果$\boldsymbol{P} \approx \boldsymbol{F}$，则：</p>
<p>$$<br />
\mathcal{L}<em i="1">{L2} \approx \frac{1}{2} \sum</em>}^{n} F_i P_i - \frac{1}{2n<br />
$$</p>
<p>相差一个常数项！</p>
<h4 id="62-l1">6.2 L1正则化策略<a class="toc-link" href="#62-l1" title="Permanent link">&para;</a></h4>
<p><strong>推导27：L1距离损失</strong></p>
<p>$$<br />
\mathcal{L}<em i="1">{L1} = |\boldsymbol{F} - \boldsymbol{Q}|_1 = \sum</em>\right|}^{n} \left|F_i - \frac{1}{n<br />
$$</p>
<p>这个损失对离群专家（$F_i$远离$\frac{1}{n}$）的惩罚更强。</p>
<p><strong>推导28：L1的次梯度</strong></p>
<p>L1损失的次梯度为：</p>
<p>$$<br />
\partial \mathcal{L}<em i="1">{L1} = \sum</em> P_i}^{n} \text{sign}\left(F_i - \frac{1}{n}\right) \nabla_{\boldsymbol{\theta}<br />
$$</p>
<p>其中$\text{sign}(x) = \begin{cases} 1, &amp; x &gt; 0 \ [-1, 1], &amp; x = 0 \ -1, &amp; x &lt; 0 \end{cases}$</p>
<h4 id="63">6.3 最大负载最小化<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p><strong>推导29：Minimax目标</strong></p>
<p>另一种策略是最小化最大负载：</p>
<p>$$<br />
\mathcal{L}<em i="1,\ldots,n">{\max} = \max</em> F_i<br />
$$</p>
<p>这直接针对"最繁忙"的专家。可以用光滑近似：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\max-soft} = \text{LogSumExp}(\boldsymbol{F}) = \log \sum</em>}^{n} e^{F_i<br />
$$</p>
<p>当温度系数很大时，这接近$\max F_i$。</p>
<h3 id="7">7. 收敛性与稳定性分析<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71-aux-loss">7.1 Aux Loss的收敛性<a class="toc-link" href="#71-aux-loss" title="Permanent link">&para;</a></h4>
<p><strong>推导30：梯度下降更新</strong></p>
<p>设总损失为：</p>
<p>$$<br />
\mathcal{L}<em _text_task="\text{task">{\text{total}} = \mathcal{L}</em>}} + \alpha \mathcal{L}_{\text{aux}<br />
$$</p>
<p>其中$\alpha$是Aux Loss的权重。梯度下降更新：</p>
<p>$$<br />
\boldsymbol{\theta}<em _boldsymbol_theta="\boldsymbol{\theta">{t+1} = \boldsymbol{\theta}_t - \eta \nabla</em>}} \mathcal{L}_{\text{total}<br />
$$</p>
<p><strong>推导31：收敛条件分析</strong></p>
<p>假设$\mathcal{L}_{\text{task}}$是$L$-光滑的，即：</p>
<p>$$<br />
|\nabla \mathcal{L}<em _text_task="\text{task">{\text{task}}(\boldsymbol{\theta}_1) - \nabla \mathcal{L}</em>_2|}}(\boldsymbol{\theta}_2)| \leq L |\boldsymbol{\theta}_1 - \boldsymbol{\theta<br />
$$</p>
<p>那么在学习率$\eta \leq \frac{1}{L}$下，梯度下降收敛。</p>
<p>对于Aux Loss项$\alpha \sum_{i=1}^{n} F_i P_i$，其梯度为：</p>
<p>$$<br />
\alpha \sum_{i=1}^{n} F_i \nabla_{\boldsymbol{\theta}} P_i<br />
$$</p>
<p>由于$P_i = \frac{e^{\rho_i}}{\sum_j e^{\rho_j}}$是Softmax，其Jacobian有界，因此Aux Loss也是光滑的。</p>
<p><strong>推导32：稳定性分析——方差界</strong></p>
<p>负载分布的方差定义为：</p>
<p>$$<br />
\text{Var}(\boldsymbol{F}) = \sum_{i=1}^{n} \left(F_i - \frac{1}{n}\right)^2 = \sum_{i=1}^{n} F_i^2 - \frac{1}{n}<br />
$$</p>
<p>最小化Aux Loss $\sum_{i=1}^{n} F_i P_i$时，如果$\boldsymbol{P} \approx \boldsymbol{F}$，则相当于最小化$\sum_{i=1}^{n} F_i^2$，即减小方差。</p>
<p><strong>推导33：Lyapunov稳定性</strong></p>
<p>定义Lyapunov函数：</p>
<p>$$<br />
V(\boldsymbol{F}) = \frac{1}{2} |\boldsymbol{F} - \boldsymbol{Q}|^2<br />
$$</p>
<p>其时间导数（沿梯度流）：</p>
<p>$$<br />
\frac{dV}{dt} = (\boldsymbol{F} - \boldsymbol{Q})^T \frac{d\boldsymbol{F}}{dt}<br />
$$</p>
<p>如果优化正确进行，$\frac{d\boldsymbol{F}}{dt}$应该指向减小$V$的方向，即：</p>
<p>$$<br />
\frac{dV}{dt} &lt; 0 \quad \text{当} \quad \boldsymbol{F} \neq \boldsymbol{Q}<br />
$$</p>
<p>这保证了系统收敛到均匀分布。</p>
<h4 id="72">7.2 动态稳定性<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p><strong>推导34：负载波动分析</strong></p>
<p>在训练过程中，负载$F_i^{(t)}$在时间步$t$可能有波动。定义波动幅度：</p>
<p>$$<br />
\Delta F_i^{(t)} = F_i^{(t)} - F_i^{(t-1)}<br />
$$</p>
<p>总波动为：</p>
<p>$$<br />
\Omega^{(t)} = \sum_{i=1}^{n} |\Delta F_i^{(t)}|<br />
$$</p>
<p>稳定的训练应该有$\Omega^{(t)} \to 0$当$t \to \infty$。</p>
<p><strong>推导35：指数移动平均的稳定效果</strong></p>
<p>为了减少波动，可以使用EMA更新$\boldsymbol{F}$：</p>
<p>$$<br />
\boldsymbol{F}^{(t)} = \beta \boldsymbol{F}^{(t-1)} + (1-\beta) \boldsymbol{f}^{(t)}<br />
$$</p>
<p>其中$\boldsymbol{f}^{(t)}$是第$t$步的瞬时负载，$\beta \in [0, 1)$是动量系数。</p>
<p>这个EMA的方差为：</p>
<p>$$<br />
\text{Var}(\boldsymbol{F}^{(t)}) = (1-\beta)^2 \sum_{s=0}^{t} \beta^{2s} \text{Var}(\boldsymbol{f}^{(t-s)})<br />
$$</p>
<p>当$t$足够大时，稳态方差为：</p>
<p>$$<br />
\text{Var}(\boldsymbol{F}^{(\infty)}) = \frac{1-\beta}{1+\beta} \text{Var}(\boldsymbol{f})<br />
$$</p>
<p>$\beta$越大，方差越小，越稳定。</p>
<h3 id="8">8. 实验结果的理论解释<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81-aux-loss">8.1 Aux Loss权重的影响<a class="toc-link" href="#81-aux-loss" title="Permanent link">&para;</a></h4>
<p><strong>推导36：权重$\alpha$的权衡</strong></p>
<p>总损失为：</p>
<p>$$<br />
\mathcal{L} = \mathcal{L}<em _text_aux="\text{aux">{\text{task}} + \alpha \mathcal{L}</em>}<br />
$$</p>
<p>过小的$\alpha$：负载不均衡，$\boldsymbol{F}$偏离$\boldsymbol{Q}$<br />
过大的$\alpha$：过度强调均衡，牺牲任务性能</p>
<p>最优$\alpha^*$应平衡两者：</p>
<p>$$<br />
\alpha^* = \arg\min_{\alpha} \mathbb{E}[\mathcal{L}_{\text{task}}(\alpha)] + \lambda |\boldsymbol{F}(\alpha) - \boldsymbol{Q}|^2<br />
$$</p>
<p>其中$\lambda$是超参数，控制对均衡的重视程度。</p>
<p><strong>推导37：帕累托前沿分析</strong></p>
<p>定义两个目标：<br />
- $J_1 = \mathcal{L}_{\text{task}}$（任务性能，越小越好）<br />
- $J_2 = |\boldsymbol{F} - \boldsymbol{Q}|^2$（负载方差，越小越好）</p>
<p>帕累托最优解满足：不存在其他解能同时改善$J_1$和$J_2$。</p>
<p>不同的$\alpha$对应帕累托前沿上的不同点。</p>
<h4 id="82">8.2 训练动态的理论预测<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p><strong>推导38：早期训练阶段</strong></p>
<p>初始时，Router权重接近随机，$\boldsymbol{P}^{(0)} \approx \boldsymbol{Q}$（近似均匀）。此时：</p>
<p>$$<br />
\mathcal{L}<em i="1">{\text{aux}}^{(0)} = \sum</em>}^{n} F_i^{(0)} P_i^{(0)} \approx \sum_{i=1}^{n} F_i^{(0)} \cdot \frac{1}{n} = \frac{1}{n<br />
$$</p>
<p>随着训练，Router开始专门化，$\boldsymbol{P}$偏离均匀，如果没有Aux Loss，$\boldsymbol{F}$也会偏离。</p>
<p><strong>推导39：中期训练阶段</strong></p>
<p>Aux Loss开始发挥作用，将$\boldsymbol{F}$拉向$\boldsymbol{Q}$。此时存在竞争：<br />
- 任务损失希望Router专门化<br />
- Aux Loss希望Router均匀化</p>
<p>平衡点取决于$\alpha$。</p>
<p><strong>推导40：后期训练阶段</strong></p>
<p>当模型接近收敛，$\boldsymbol{F}$应该稳定在接近$\boldsymbol{Q}$的位置。此时Aux Loss的梯度：</p>
<p>$$<br />
\nabla_{\boldsymbol{\theta}} \mathcal{L}<em i="1">{\text{aux}} = \sum</em> \nabla P_i = 0}^{n} F_i \nabla P_i \approx \frac{1}{n} \sum_{i=1}^{n<br />
$$</p>
<p>Aux Loss的影响减弱，模型主要优化任务损失。</p>
<h3 id="9">9. 总结与扩展<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p>通过以上40个详细推导，我们从数学角度深入理解了MoE负载均衡问题：</p>
<ol>
<li><strong>MoE架构</strong>：建立了严格的数学定义，包括标准MoE和几何MoE</li>
<li><strong>负载问题</strong>：量化了负载不均衡的代价（Dead Expert、Token Drop）</li>
<li><strong>Aux Loss设计</strong>：通过STE技巧推导了标准Aux Loss的梯度等价性</li>
<li><strong>均匀分布</strong>：从信息论（熵）和容量利用角度证明了均匀分布的必要性</li>
<li><strong>熵正则化</strong>：展示了基于熵的Aux Loss与点积形式的联系</li>
<li><strong>策略对比</strong>：比较了L1、L2、熵、Minimax等不同平衡策略</li>
<li><strong>收敛性</strong>：分析了Aux Loss的收敛条件和Lyapunov稳定性</li>
<li><strong>实验理论</strong>：解释了Aux Loss权重、训练动态等实验现象</li>
</ol>
<p>这些推导为理解和改进MoE负载均衡提供了坚实的理论基础。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈二十九用ddpm来离散编码.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#182 生成扩散模型漫谈（二十九）：用DDPM来离散编码</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="muon续集为什么我们选择尝试muon.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#184 Muon续集：为什么我们选择尝试Muon？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#moe2">MoE环游记：2、不患寡而患不均</a><ul>
<li><a href="#_1">需求分析</a></li>
<li><a href="#_2">辅助损失</a></li>
<li><a href="#_3">直通估计</a></li>
<li><a href="#_4">一般形式</a></li>
<li><a href="#_5">文章小结</a></li>
<li><a href="#_6">公式推导与注释</a><ul>
<li><a href="#1-moe">1. MoE的数学定义与路由机制</a></li>
<li><a href="#2">2. 负载均衡问题的数学分析</a></li>
<li><a href="#3-aux-loss">3. 辅助损失（Aux Loss）的数学推导</a></li>
<li><a href="#4">4. 均匀分布的理论分析</a></li>
<li><a href="#5">5. 熵正则化的理论分析</a></li>
<li><a href="#6">6. 不同平衡策略的数学对比</a></li>
<li><a href="#7">7. 收敛性与稳定性分析</a></li>
<li><a href="#8">8. 实验结果的理论解释</a></li>
<li><a href="#9">9. 总结与扩展</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>