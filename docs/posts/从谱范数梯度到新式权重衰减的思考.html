<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>从谱范数梯度到新式权重衰减的思考 | ML & Math Blog Posts</title>
    <meta name="description" content="从谱范数梯度到新式权重衰减的思考&para;
原文链接: https://spaces.ac.cn/archives/10648
发布日期: 

在文章《Muon优化器赏析：从向量到矩阵的本质跨越》中，我们介绍了一个名为“Muon”的新优化器，其中一个理解视角是作为谱范数正则下的最速梯度下降，这似乎揭示了矩阵参数的更本质的优化方向。众所周知，对于矩阵参数我们经常也会加权重衰减（Weight Dec...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #303 从谱范数梯度到新式权重衰减的思考
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#303</span>
                从谱范数梯度到新式权重衰减的思考
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-12-25</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=矩阵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 矩阵</span>
                </a>
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=谱范数" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 谱范数</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">从谱范数梯度到新式权重衰减的思考<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10648">https://spaces.ac.cn/archives/10648</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在文章<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>中，我们介绍了一个名为“Muon”的新优化器，其中一个理解视角是作为谱范数正则下的最速梯度下降，这似乎揭示了矩阵参数的更本质的优化方向。众所周知，对于矩阵参数我们经常也会加权重衰减（Weight Decay），它可以理解为$F$范数平方的梯度，那么从Muon的视角看，通过谱范数平方的梯度来构建新的权重衰减，会不会能起到更好的效果呢？</p>
<p>那么问题来了，谱范数的梯度或者说导数长啥样呢？用它来设计的新权重衰减又是什么样的？接下来我们围绕这些问题展开。</p>
<h2 id="_2">基础回顾<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>谱范数（Spectral Norm），又称“$2$范数”，是最常用的矩阵范数之一，相比更简单的$F$范数（Frobenius Norm），它往往能揭示一些与矩阵乘法相关的更本质的信号，这是因为它定义上就跟矩阵乘法相关：对于矩阵参数$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，它的谱范数定义为<br />
\begin{equation}\Vert\boldsymbol{W}\Vert_2 \triangleq \max_{\Vert\boldsymbol{x}\Vert=1} \Vert\boldsymbol{W}\boldsymbol{x}\Vert\end{equation}<br />
这里$\boldsymbol{x}\in\mathbb{R}^m$是列向量，右端的$\Vert\Vert$是向量的模长（欧氏范数）。换个角度看，谱范数就是使得下面不等式对$\forall \boldsymbol{x}\in\mathbb{R}^m$恒成立的最小常数$C$：<br />
\begin{equation}\Vert\boldsymbol{W}\boldsymbol{x}\Vert \leq C\Vert\boldsymbol{x}\Vert\end{equation}<br />
不难证明，当$C$取$F$范数$\Vert W\Vert_F$时，上式也是恒成立的，所以可以写出$\Vert \boldsymbol{W}\Vert_2\leq \Vert \boldsymbol{W}\Vert_F$（因为$\Vert \boldsymbol{W}\Vert_F$只是让上式恒成立的其中一个$C$，而$\Vert \boldsymbol{W}\Vert_2$则是最小的那个$C$）。这个结论也表明，如果我们想要控制输出的幅度，以谱范数作为正则项要比$F$范数更为精准。</p>
<p>早在6年前的<a href="/archives/6051">《深度学习中的Lipschitz约束：泛化与生成模型》</a>中，我们就讨论过谱范数，当时的应用场景有两个：一是WGAN对判别器明确提出了Lipschitz约束，而实现方式之一就是基于谱范数的归一化；二是有一些工作表明，谱范数作为正则项，相比$F$范数正则有更好的性能。</p>
<h2 id="_3">梯度推导<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>现在让我们进入正题，尝试推导谱范数的梯度$\nabla_{\boldsymbol{W}} \Vert\boldsymbol{W}\Vert_2$。我们知道，谱范数在数值上等于它的最大奇异值，对此我们在<a href="/archives/10407">《低秩近似之路（二）：SVD》</a>的“<a href="/archives/10407#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0">矩阵范数</a>”一节有过证明。这意味着，如果$\boldsymbol{W}$可以SVD为$\sum\limits_{i=1}^{\min(n,m)}\sigma_i \boldsymbol{u}<em _min_n_m_="\min(n,m)">i\boldsymbol{v}_i^{\top}$，那么<br />
\begin{equation}\Vert\boldsymbol{W}\Vert_2 = \sigma_1 = \boldsymbol{u}_1^{\top}\boldsymbol{W}\boldsymbol{v}_1\end{equation}<br />
其中$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma</em>$的奇异值。对两边求微分，我们得到} \geq 0$是$\boldsymbol{W<br />
\begin{equation}d\Vert\boldsymbol{W}\Vert_2 = d\boldsymbol{u}<em i="1">1^{\top}\boldsymbol{W}\boldsymbol{v}_1 + \boldsymbol{u}_1^{\top}d\boldsymbol{W}\boldsymbol{v}_1 + \boldsymbol{u}_1^{\top}\boldsymbol{W}d\boldsymbol{v}_1\end{equation}<br />
留意到<br />
\begin{equation}d\boldsymbol{u}_1^{\top}\boldsymbol{W}\boldsymbol{v}_1 = d\boldsymbol{u}_1^{\top}\sum</em>}^{\min(n,m)}\sigma_i \boldsymbol{u<em _boldsymbol_W="\boldsymbol{W">i\boldsymbol{v}_i^{\top}\boldsymbol{v}_1 = d\boldsymbol{u}_1^{\top}\sigma_1 \boldsymbol{u}_1 = \frac{1}{2}\sigma_1 d(\Vert\boldsymbol{u}_1\Vert^2)=0\end{equation}<br />
同理$\boldsymbol{u}_1^{\top}\boldsymbol{W}d\boldsymbol{v}_1=0$，所以<br />
\begin{equation}d\Vert\boldsymbol{W}\Vert_2 = \boldsymbol{u}_1^{\top}d\boldsymbol{W}\boldsymbol{v}_1 = \text{Tr}((\boldsymbol{u}_1 \boldsymbol{v}_1^{\top})^{\top} d\boldsymbol{W}) \quad\Rightarrow\quad \nabla</em>}}\Vert\boldsymbol{W}\Vert_2 = \boldsymbol{u}_1 \boldsymbol{v}_1^{\top}\end{equation
注意，这个证明过程有一个关键条件是$\sigma_1 &gt; \sigma_2$，因为如果$\sigma_1=\sigma_2$的话，$\Vert\boldsymbol{W}\Vert_2$既可以表示成$\boldsymbol{u}_1^{\top}\boldsymbol{W}\boldsymbol{v}_1$又可以表示成$\boldsymbol{u}_2^{\top}\boldsymbol{W}\boldsymbol{v}_2$，用同样方法求出的梯度分别是$\boldsymbol{u}_1 \boldsymbol{v}_1^{\top}$和$\boldsymbol{u}_2 \boldsymbol{v}_2^{\top}$，结果不唯一意味着梯度不存在。当然，从实践角度看，两个数完全相等的概率是很小的，因此可以忽略这一点。</p>
<p>（注：这里的证明过程参考了Stack Exchange上的<a href="https://math.stackexchange.com/a/3000223">回答</a>，但该回答里面没有证明$d\boldsymbol{u}_1^{\top}\boldsymbol{W}\boldsymbol{v}_1=0$和$\boldsymbol{u}_1^{\top}\boldsymbol{W}d\boldsymbol{v}_1=0$，这部分由笔者补充完整。）</p>
<h2 id="_4">权重衰减<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>根据这个结果以及链式法则，我们有<br />
\begin{equation}\nabla_{\boldsymbol{W}}\left(\frac{1}{2}\Vert\boldsymbol{W}\Vert_2^2\right) = \Vert\boldsymbol{W}\Vert_2\nabla_{\boldsymbol{W}}\Vert\boldsymbol{W}\Vert_2 = \sigma_1 \boldsymbol{u}<em _boldsymbol_W="\boldsymbol{W">1 \boldsymbol{v}_1^{\top}\label{eq:grad-2-2}\end{equation}<br />
对比$F$范数下的结果：<br />
\begin{equation}\nabla</em>}}\left(\frac{1}{2}\Vert\boldsymbol{W}\Vert_F^2\right) = \boldsymbol{W} = \sum_{i=1}^{\min(n,m)}\sigma_i \boldsymbol{u}_i \boldsymbol{v}_i^{\top}\end{equation
这样对比着看就很清晰了：$F$范数平方作为正则项所得出的权重衰减，同时惩罚全体奇异值；而谱范数平方对应的权重衰减，只惩罚最大奇异值。如果我们目的是压缩输出的大小，那么压缩最大奇异值是“刚刚好”的做法，压缩全体奇异值虽然可能达到相近的目的，但同时也可能压缩参数的表达能力。</p>
<p>根据“<a href="/archives/10407#%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86">Eckart-Young-Mirsky定理</a>”，式$\eqref{eq:grad-2-2}$最右侧的结果还有一个含义，就是$\boldsymbol{W}$矩阵的“最优1秩近似”。也就是说，谱范数的权重衰减将每一步减去它自身的操作，改为每一步减去它的最优1秩近似，弱化了惩罚力度，当然某种程度上也让惩罚更加“直击本质”。</p>
<h2 id="_5">数值计算<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>对于实践来说，最关键的问题来了：怎么计算$\sigma_1 \boldsymbol{u}_1 \boldsymbol{v}_1^{\top}$呢？SVD当然是最简单直接的方案，但计算复杂度无疑也是最高的，我们必须找到更高效的计算途径。</p>
<p>不失一般性，设$n\geq m$。首先注意到<br />
\begin{equation}\sigma_1 \boldsymbol{u}<em i="1">1 \boldsymbol{v}_1^{\top} = \sum</em>}^m\sigma_i \boldsymbol{u}_i \boldsymbol{v}_i^{\top} \boldsymbol{v}_1 \boldsymbol{v}_1^{\top} = \boldsymbol{W}\boldsymbol{v}_1 \boldsymbol{v}_1^{\top}\end{equation
由此可见计算$\sigma_1 \boldsymbol{u}_1 \boldsymbol{v}_1^{\top}$只需要知道$\boldsymbol{v}_1$，然后根据我们在<a href="/archives/10407#%E5%A5%87%E5%BC%82%E5%88%86%E8%A7%A3">《低秩近似之路（二）：SVD》</a>中的讨论，$\boldsymbol{v}_1$实际上是矩阵$\boldsymbol{W}^{\top}\boldsymbol{W}$的最大特征值对应的特征向量。这样一来，我们便将问题从一般矩阵$\boldsymbol{W}$的SVD转化成了实对称矩阵$\boldsymbol{W}^{\top}\boldsymbol{W}$的特征值分解，这其实已经降低复杂度了，因为特征值分解通常要比SVD明显快。</p>
<p>如果还觉得慢，那么我们就需要请出很多特征值分解算法背后的原理——“<a href="https://en.wikipedia.org/wiki/Power_iteration">幂迭代（Power Iteration）</a>”：</p>
<blockquote>
<p>当$\sigma_1 &gt; \sigma_2$时，迭代 \begin{equation}\boldsymbol{x}_{t+1} = \frac{\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{x}_t}{\Vert\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{x}_t\Vert}\end{equation} 以$(\sigma_2/\sigma_1)^{2t}$的速度收敛至$\boldsymbol{v}_1$。</p>
</blockquote>
<p>幂迭代每步只需要算两次“矩阵-向量”乘法，复杂度是$\mathcal{O}(nm)$，$t$步迭代的总复杂度是$\mathcal{O}(tnm)$，非常理想，缺点是$\sigma_1,\sigma_2$接近时收敛会比较慢。但幂迭代的实际表现往往比理论想象更好用，早期很多工作甚至只迭代一次就得到不错的效果，因为$\sigma_1,\sigma_2$接近表明两者及其特征向量一定程度上可替换，而幂迭代即便没完全收敛，得到的也是两者特征向量的一个平均，这也完全够用了。</p>
<h2 id="_6">迭代证明<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>这一节我们来完成幂迭代的证明。不难看出，幂迭代可以等价地写成<br />
\begin{equation}\lim_{t\to\infty} \frac{(\boldsymbol{W}^{\top}\boldsymbol{W})^t \boldsymbol{x}<em i="1">0}{\Vert(\boldsymbol{W}^{\top}\boldsymbol{W})^t \boldsymbol{x}_0\Vert} = \boldsymbol{v}_1\end{equation}<br />
为了证明这个极限，我们从$\boldsymbol{W}=\sum\limits</em>}^m\sigma_i \boldsymbol{u<em i="1">i\boldsymbol{v}_i^{\top}$出发，代入计算可得<br />
\begin{equation}\boldsymbol{W}^{\top}\boldsymbol{W} = \sum</em>}^m\sigma_i^2 \boldsymbol{v<em i="1">i\boldsymbol{v}_i^{\top},\qquad(\boldsymbol{W}^{\top}\boldsymbol{W})^t = \sum</em>}^m\sigma_i^{2t} \boldsymbol{v<em j="1">i\boldsymbol{v}_i^{\top}\end{equation}<br />
由于$\boldsymbol{v}_1,\boldsymbol{v}_2,\cdots,\boldsymbol{v}_m$是$\mathbb{R}^m$的一组标准正交基，所以$\boldsymbol{x}_0$可以写成$\sum\limits</em>}^m c_j \boldsymbol{v<em i="1">j$，于是我们有<br />
\begin{equation}(\boldsymbol{W}^{\top}\boldsymbol{W})^t \boldsymbol{x}_0 = \sum</em>}^m\sigma_i^{2t} \boldsymbol{v<em j="1">i\boldsymbol{v}_i^{\top}\sum</em>}^m c_j \boldsymbol{v<em i="1">j = \sum</em>}^m\sum_{j=1}^m c_j\sigma_i^{2t} \boldsymbol{v<em>i\underbrace{\boldsymbol{v}_i^{\top} \boldsymbol{v}_j}</em>}} = \sum_{i=1}^m c_i\sigma_i^{2t} \boldsymbol{v<em i="1">i\end{equation}<br />
以及<br />
\begin{equation}\Vert(\boldsymbol{W}^{\top}\boldsymbol{W})^t \boldsymbol{x}_0\Vert = \left\Vert \sum</em>}^m c_i\sigma_i^{2t} \boldsymbol{v<em i="1">i\right\Vert = \sqrt{\sum</em>}^m c_i^2\sigma_i^{4t}}\end{equation
由于随机初始化的缘故，$c_1=0$的概率是非常小的，所以我们可以认为$c_1\neq 0$，那么
\begin{equation}\frac{(\boldsymbol{W}^{\top}\boldsymbol{W})^t \boldsymbol{x}<em i="1">0}{\Vert(\boldsymbol{W}^{\top}\boldsymbol{W})^t \boldsymbol{x}_0\Vert} = \frac{\sum\limits</em>}^m c_i\sigma_i^{2t} \boldsymbol{v<em i="1">i}{\sqrt{\sum\limits</em>}^m c_i^2\sigma_i^{4t}}} = \frac{\boldsymbol{v<em i="2">1 + \sum\limits</em>}^m (c_i/c_1)(\sigma_i/\sigma_1)^{2t} \boldsymbol{v<em i="2">i}{\sqrt{1 + \sum\limits</em>}^m (c_i/c_1)^2(\sigma_i/\sigma_1)^{4t}}}\end{equation<br />
当$\sigma_1 &gt; \sigma_2$时，所有的$\sigma_i/\sigma_1(i\geq 2)$都小于1，因此当$t\to \infty$时对应项都变成了0，最后的极限是$\boldsymbol{v}_1$。</p>
<h2 id="_7">相关工作<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>最早提出谱范数正则的论文，应该是2017年的<a href="https://papers.cool/arxiv/1705.10941">《Spectral Norm Regularization for Improving the Generalizability of Deep Learning》</a>，里边对比了权重衰减、对抗训练、谱范数正则等方法，发现谱范数正则在泛化性能方面表现最好。</p>
<p>论文当时的做法，并不是像本文一样求出$\nabla_{\boldsymbol{W}}\Vert\boldsymbol{W}\Vert_2^2 = 2\sigma_1\boldsymbol{u}_1 \boldsymbol{v}_1^{\top}$，而是直接通过幂迭代来估计$\Vert\boldsymbol{W}\Vert_2$，然后将$\Vert\boldsymbol{W}\Vert_2^2$加权到损失函数中，让优化器自己去求梯度，这样做效率上稍差一些，并且也不好以权重衰减的形式跟优化器解耦开来。本文的做法相对来说更加灵活一些，允许我们像AdamW一样，将权重衰减独立于主损失函数的优化之外。</p>
<p>当然，从今天LLM的视角来看，当初的这些实验最大问题就是规模都太小了，很难有足够的说服力，不过鉴于谱范数的Muon优化器“珠玉在前”，笔者认为还是值得重新思考和尝试一下谱范数权重衰减。当然，不管是$F$范数还是谱范数的权重衰减，这些面向“泛化”的技术往往也有一些运气成份在里边，大家平常心期待就好。</p>
<p>个人在语言模型的初步实验结果显示，Loss层面可能会有微弱的提升（希望不是幻觉，当然再不济也没有出现变差的现象）。实验过程就是用幂迭代求出$\boldsymbol{v}_1$的近似值（初始化为全一向量，迭代10次），然后将原来的权重衰减$-\lambda \boldsymbol{W}$改为$-\lambda \boldsymbol{W}\boldsymbol{v}_1\boldsymbol{v}_1^{\top}$，$\lambda$的取值不做改变。</p>
<h2 id="_8">文章小结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>本文推导了谱范数的梯度，由此导出了一种新的权重衰减，并分享了笔者对它的思考。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10648">https://spaces.ac.cn/archives/10648</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Dec. 25, 2024). 《从谱范数梯度到新式权重衰减的思考 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10648">https://spaces.ac.cn/archives/10648</a></p>
<p>@online{kexuefm-10648,<br />
title={从谱范数梯度到新式权重衰减的思考},<br />
author={苏剑林},<br />
year={2024},<br />
month={Dec},<br />
url={\url{https://spaces.ac.cn/archives/10648}},<br />
} </p>
<hr />
<h2 id="_9">公式推导与注释<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>本节将从多个角度深入推导谱范数及其在优化中的应用，包括矩阵理论基础、优化理论分析、泛化理论以及实际应用。</p>
<h3 id="1">1. 谱范数的定义与性质<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 多种等价定义<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>谱范数有多种等价的定义方式，每种定义都揭示了其不同的几何或代数意义。</p>
<p><strong>定义1（算子范数）</strong>：对于矩阵$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，
$$\Vert\boldsymbol{W}\Vert_2 = \max_{\boldsymbol{x}\in\mathbb{R}^m,\Vert\boldsymbol{x}\Vert_2=1} \Vert\boldsymbol{W}\boldsymbol{x}\Vert_2$$</p>
<p><strong>定义2（最大奇异值）</strong>：设$\boldsymbol{W}$的SVD为$\boldsymbol{W}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，则
$$\Vert\boldsymbol{W}\Vert_2 = \sigma_{\max}(\boldsymbol{W}) = \sigma_1$$</p>
<p><strong>定义3（谱半径形式）</strong>：
$$\Vert\boldsymbol{W}\Vert_2 = \sqrt{\lambda_{\max}(\boldsymbol{W}^{\top}\boldsymbol{W})} = \sqrt{\lambda_{\max}(\boldsymbol{W}\boldsymbol{W}^{\top})}$$</p>
<p><strong>证明定义等价性</strong>：</p>
<p>首先证明定义1与定义2的等价性。设$\boldsymbol{W}=\sum_{i=1}^{r}\sigma_i\boldsymbol{u}<em i="1">i\boldsymbol{v}_i^{\top}$，其中$r=\text{rank}(\boldsymbol{W})$，$\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_r&gt;0$。对于任意单位向量$\boldsymbol{x}$：
$$\Vert\boldsymbol{W}\boldsymbol{x}\Vert_2^2 = \left\Vert\sum</em>}^{r}\sigma_i\boldsymbol{u<em i="1">i(\boldsymbol{v}_i^{\top}\boldsymbol{x})\right\Vert_2^2 = \sum</em>)^2$$}^{r}\sigma_i^2(\boldsymbol{v}_i^{\top}\boldsymbol{x</p>
<p>由于${\boldsymbol{v}<em i="1">1,\ldots,\boldsymbol{v}_r}$是标准正交向量，且$\Vert\boldsymbol{x}\Vert_2=1$，我们有$\sum</em>}^{r}(\boldsymbol{v<em i="1">i^{\top}\boldsymbol{x})^2\leq 1$。因此：
$$\Vert\boldsymbol{W}\boldsymbol{x}\Vert_2^2 = \sum</em>}^{r}\sigma_i^2(\boldsymbol{v<em i="1">i^{\top}\boldsymbol{x})^2 \leq \sigma_1^2\sum</em>)^2 \leq \sigma_1^2$$}^{r}(\boldsymbol{v}_i^{\top}\boldsymbol{x</p>
<p>等号在$\boldsymbol{x}=\boldsymbol{v}<em _Vert_boldsymbol_x="\Vert\boldsymbol{x">1$时取得，因此$\max</em>\Vert_2 = \sigma_1$。}\Vert_2=1}\Vert\boldsymbol{W}\boldsymbol{x</p>
<p>接下来证明定义2与定义3的等价性。注意到$\boldsymbol{W}^{\top}\boldsymbol{W}$的特征值为$\sigma_i^2(i=1,\ldots,r)$和$m-r$个零。因此：
$$\lambda_{\max}(\boldsymbol{W}^{\top}\boldsymbol{W}) = \sigma_1^2 \quad\Rightarrow\quad \sqrt{\lambda_{\max}(\boldsymbol{W}^{\top}\boldsymbol{W})} = \sigma_1$$</p>
<h4 id="12">1.2 基本性质<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p><strong>性质1（次可乘性）</strong>：对于相容维度的矩阵$\boldsymbol{A},\boldsymbol{B}$，
$$\Vert\boldsymbol{A}\boldsymbol{B}\Vert_2 \leq \Vert\boldsymbol{A}\Vert_2\Vert\boldsymbol{B}\Vert_2$$</p>
<p><strong>证明</strong>：对于任意单位向量$\boldsymbol{x}$，
$$\Vert\boldsymbol{A}\boldsymbol{B}\boldsymbol{x}\Vert_2 \leq \Vert\boldsymbol{A}\Vert_2\Vert\boldsymbol{B}\boldsymbol{x}\Vert_2 \leq \Vert\boldsymbol{A}\Vert_2\Vert\boldsymbol{B}\Vert_2\Vert\boldsymbol{x}\Vert_2 = \Vert\boldsymbol{A}\Vert_2\Vert\boldsymbol{B}\Vert_2$$</p>
<p>对$\boldsymbol{x}$取最大值即得结论。</p>
<p><strong>性质2（酉不变性）</strong>：对于酉矩阵$\boldsymbol{U},\boldsymbol{V}$（满足$\boldsymbol{U}^{\top}\boldsymbol{U}=\boldsymbol{I},\boldsymbol{V}^{\top}\boldsymbol{V}=\boldsymbol{I}$），
$$\Vert\boldsymbol{U}\boldsymbol{W}\boldsymbol{V}\Vert_2 = \Vert\boldsymbol{W}\Vert_2$$</p>
<p><strong>证明</strong>：由于酉矩阵的谱范数为1（$\Vert\boldsymbol{U}\Vert_2=\Vert\boldsymbol{V}\Vert_2=1$），且$\Vert\boldsymbol{U}^{-1}\Vert_2=\Vert\boldsymbol{U}^{\top}\Vert_2=1$，利用次可乘性：
$$\Vert\boldsymbol{U}\boldsymbol{W}\boldsymbol{V}\Vert_2 \leq \Vert\boldsymbol{U}\Vert_2\Vert\boldsymbol{W}\Vert_2\Vert\boldsymbol{V}\Vert_2 = \Vert\boldsymbol{W}\Vert_2$$
$$\Vert\boldsymbol{W}\Vert_2 = \Vert\boldsymbol{U}^{\top}(\boldsymbol{U}\boldsymbol{W}\boldsymbol{V})\boldsymbol{V}^{\top}\Vert_2 \leq \Vert\boldsymbol{U}\boldsymbol{W}\boldsymbol{V}\Vert_2$$</p>
<p>两边夹定得$\Vert\boldsymbol{U}\boldsymbol{W}\boldsymbol{V}\Vert_2 = \Vert\boldsymbol{W}\Vert_2$。</p>
<p><strong>性质3（与Frobenius范数的关系）</strong>：
$$\Vert\boldsymbol{W}\Vert_2 \leq \Vert\boldsymbol{W}\Vert_F \leq \sqrt{r}\Vert\boldsymbol{W}\Vert_2$$</p>
<p>其中$r=\text{rank}(\boldsymbol{W})$。</p>
<p><strong>证明</strong>：由于$\Vert\boldsymbol{W}\Vert_F^2 = \sum_{i=1}^{r}\sigma_i^2$且$\sigma_1\geq\sigma_i$对所有$i$成立，
$$\Vert\boldsymbol{W}\Vert_F^2 = \sum_{i=1}^{r}\sigma_i^2 \leq r\sigma_1^2 = r\Vert\boldsymbol{W}\Vert_2^2$$</p>
<p>另一方面，$\sigma_1^2\leq\sum_{i=1}^{r}\sigma_i^2$，因此$\Vert\boldsymbol{W}\Vert_2\leq\Vert\boldsymbol{W}\Vert_F$。</p>
<h3 id="2">2. 谱范数梯度的完整推导<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 基于变分方法的推导<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p>考虑优化问题：
$$\Vert\boldsymbol{W}\Vert_2 = \max_{\Vert\boldsymbol{x}\Vert_2=1} \Vert\boldsymbol{W}\boldsymbol{x}\Vert_2 = \max_{\Vert\boldsymbol{x}\Vert_2=1} \sqrt{\boldsymbol{x}^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{x}}$$</p>
<p>等价地（平方后）：
$$\Vert\boldsymbol{W}\Vert_2^2 = \max_{\Vert\boldsymbol{x}\Vert_2=1} \boldsymbol{x}^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{x}$$</p>
<p>设最优解为$\boldsymbol{x}^*=\boldsymbol{v}_1$，则$\Vert\boldsymbol{W}\Vert_2^2 = \boldsymbol{v}_1^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{v}_1 = \sigma_1^2$。</p>
<p>使用包络定理（Envelope Theorem），对于参数化的优化问题$f(\boldsymbol{W})=\max_{\boldsymbol{x}\in\mathcal{X}(\boldsymbol{W})}g(\boldsymbol{x},\boldsymbol{W})$，其梯度为：
$$\nabla_{\boldsymbol{W}}f(\boldsymbol{W}) = \nabla_{\boldsymbol{W}}g(\boldsymbol{x}^<em>(\boldsymbol{W}),\boldsymbol{W})\Big|_{\boldsymbol{x}^</em>=\boldsymbol{v}_1}$$</p>
<p>这里$g(\boldsymbol{x},\boldsymbol{W})=\boldsymbol{x}^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{x}$，因此：
$$\nabla_{\boldsymbol{W}}(\boldsymbol{v}_1^{\top}\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{v}_1) = 2\boldsymbol{W}\boldsymbol{v}_1\boldsymbol{v}_1^{\top}$$</p>
<p>而$\boldsymbol{W}\boldsymbol{v}<em _boldsymbol_W="\boldsymbol{W">1 = \sigma_1\boldsymbol{u}_1$，所以：
$$\nabla</em>$$}}\Vert\boldsymbol{W}\Vert_2^2 = 2\sigma_1\boldsymbol{u}_1\boldsymbol{v}_1^{\top</p>
<p>从而：
$$\nabla_{\boldsymbol{W}}\Vert\boldsymbol{W}\Vert_2 = \frac{1}{2\sigma_1}\nabla_{\boldsymbol{W}}\Vert\boldsymbol{W}\Vert_2^2 = \boldsymbol{u}_1\boldsymbol{v}_1^{\top}$$</p>
<h4 id="22">2.2 基于矩阵微分的详细推导<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>考虑函数$f(\boldsymbol{W})=\Vert\boldsymbol{W}\Vert_2=\sigma_1$。利用SVD，$\sigma_1 = \boldsymbol{u}_1^{\top}\boldsymbol{W}\boldsymbol{v}_1$。对$\boldsymbol{W}$求微分：
$$d\sigma_1 = d(\boldsymbol{u}_1^{\top}\boldsymbol{W}\boldsymbol{v}_1) = (d\boldsymbol{u}_1)^{\top}\boldsymbol{W}\boldsymbol{v}_1 + \boldsymbol{u}_1^{\top}(d\boldsymbol{W})\boldsymbol{v}_1 + \boldsymbol{u}_1^{\top}\boldsymbol{W}(d\boldsymbol{v}_1)$$</p>
<p><strong>关键引理</strong>：在$\sigma_1&gt;\sigma_2$的假设下，$(d\boldsymbol{u}_1)^{\top}\boldsymbol{W}\boldsymbol{v}_1=0$且$\boldsymbol{u}_1^{\top}\boldsymbol{W}(d\boldsymbol{v}_1)=0$。</p>
<p><strong>证明</strong>：由于$\boldsymbol{v}_1$是$\boldsymbol{W}^{\top}\boldsymbol{W}$对应于特征值$\sigma_1^2$的单位特征向量，满足：
$$\boldsymbol{W}^{\top}\boldsymbol{W}\boldsymbol{v}_1 = \sigma_1^2\boldsymbol{v}_1,\quad \boldsymbol{v}_1^{\top}\boldsymbol{v}_1=1$$</p>
<p>对第一式求微分：
$$d(\boldsymbol{W}^{\top}\boldsymbol{W})\boldsymbol{v}_1 + \boldsymbol{W}^{\top}\boldsymbol{W}(d\boldsymbol{v}_1) = 2\sigma_1(d\sigma_1)\boldsymbol{v}_1 + \sigma_1^2(d\boldsymbol{v}_1)$$</p>
<p>即：
$$(d\boldsymbol{W})^{\top}\boldsymbol{W}\boldsymbol{v}_1 + \boldsymbol{W}^{\top}(d\boldsymbol{W})\boldsymbol{v}_1 + \boldsymbol{W}^{\top}\boldsymbol{W}(d\boldsymbol{v}_1) = 2\sigma_1(d\sigma_1)\boldsymbol{v}_1 + \sigma_1^2(d\boldsymbol{v}_1)$$</p>
<p>注意到$\boldsymbol{W}\boldsymbol{v}_1=\sigma_1\boldsymbol{u}_1$，上式左边第一项为：
$$(d\boldsymbol{W})^{\top}\sigma_1\boldsymbol{u}_1 = \sigma_1\boldsymbol{u}_1^{\top}(d\boldsymbol{W})$$</p>
<p>对第二式$\boldsymbol{v}_1^{\top}\boldsymbol{v}_1=1$求微分：
$$2\boldsymbol{v}_1^{\top}(d\boldsymbol{v}_1) = 0 \quad\Rightarrow\quad (d\boldsymbol{v}_1) \perp \boldsymbol{v}_1$$</p>
<p>类似地，$\boldsymbol{u}_1^{\top}(d\boldsymbol{u}_1)=0$。</p>
<p>用$\boldsymbol{u}_1^{\top}$左乘$\boldsymbol{W}\boldsymbol{v}_1=\sigma_1\boldsymbol{u}_1$并求微分：
$$\boldsymbol{u}_1^{\top}(d\boldsymbol{W})\boldsymbol{v}_1 + \boldsymbol{u}_1^{\top}\boldsymbol{W}(d\boldsymbol{v}_1) + (d\boldsymbol{u}_1)^{\top}\boldsymbol{W}\boldsymbol{v}_1 = (d\sigma_1)\boldsymbol{u}_1^{\top}\boldsymbol{u}_1 + \sigma_1(d\boldsymbol{u}_1)^{\top}\boldsymbol{u}_1$$</p>
<p>最右侧利用$\boldsymbol{u}_1^{\top}(d\boldsymbol{u}_1)=0$得到$(d\sigma_1)$。</p>
<p>现在计算$(d\boldsymbol{u}<em i="1">1)^{\top}\boldsymbol{W}\boldsymbol{v}_1$。由$\boldsymbol{W}=\sum</em>}^{r}\sigma_i\boldsymbol{u<em i="1">i\boldsymbol{v}_i^{\top}$：
$$(d\boldsymbol{u}_1)^{\top}\boldsymbol{W}\boldsymbol{v}_1 = (d\boldsymbol{u}_1)^{\top}\left(\sum</em>_1) = 0$$}^{r}\sigma_i\boldsymbol{u}_i\boldsymbol{v}_i^{\top}\right)\boldsymbol{v}_1 = (d\boldsymbol{u}_1)^{\top}\sigma_1\boldsymbol{u}_1 = \sigma_1\boldsymbol{u}_1^{\top}(d\boldsymbol{u</p>
<p>类似地，$\boldsymbol{u}_1^{\top}\boldsymbol{W}(d\boldsymbol{v}_1)=\sigma_1\boldsymbol{v}_1^{\top}(d\boldsymbol{v}_1)=0$。</p>
<p>因此：
$$d\sigma_1 = \boldsymbol{u}_1^{\top}(d\boldsymbol{W})\boldsymbol{v}_1 = \text{Tr}(\boldsymbol{u}_1^{\top}(d\boldsymbol{W})\boldsymbol{v}_1) = \text{Tr}(\boldsymbol{v}_1\boldsymbol{u}_1^{\top}d\boldsymbol{W}) = \text{Tr}((\boldsymbol{u}_1\boldsymbol{v}_1^{\top})^{\top}d\boldsymbol{W})$$</p>
<p>根据微分与梯度的关系$df=\text{Tr}((\nabla f)^{\top}dX)$，得：
$$\nabla_{\boldsymbol{W}}\Vert\boldsymbol{W}\Vert_2 = \boldsymbol{u}_1\boldsymbol{v}_1^{\top}$$</p>
<h4 id="23-sigma_1sigma_2">2.3 次微分视角（当$\sigma_1=\sigma_2$时）<a class="toc-link" href="#23-sigma_1sigma_2" title="Permanent link">&para;</a></h4>
<p>当$\sigma_1=\sigma_2=\cdots=\sigma_k&gt;\sigma_{k+1}$时，谱范数在该点不可微，但存在次微分（subdifferential）：
$$\partial\Vert\boldsymbol{W}\Vert_2 = \text{conv}\left{\boldsymbol{u}_i\boldsymbol{v}_j^{\top} : 1\leq i,j\leq k\right}$$</p>
<p>即所有形如$\sum_{i,j=1}^{k}\alpha_{ij}\boldsymbol{u}<em ij="ij">i\boldsymbol{v}_j^{\top}$的矩阵，其中$\alpha</em>=1$。}\geq 0$，$\sum_{i,j}\alpha_{ij</p>
<p>实践中遇到重奇异值的概率为零（浮点数集合的测度为零），因此可以忽略这一退化情况。</p>
<h3 id="3-frobenius">3. 与Frobenius范数的深入对比<a class="toc-link" href="#3-frobenius" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 梯度结构的几何意义<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p><strong>Frobenius范数</strong>：
$$\Vert\boldsymbol{W}\Vert_F^2 = \sum_{i=1}^{r}\sigma_i^2 \quad\Rightarrow\quad \nabla_{\boldsymbol{W}}\left(\frac{1}{2}\Vert\boldsymbol{W}\Vert_F^2\right) = \boldsymbol{W} = \sum_{i=1}^{r}\sigma_i\boldsymbol{u}_i\boldsymbol{v}_i^{\top}$$</p>
<p>这是一个<strong>全秩</strong>的梯度（假设$\boldsymbol{W}$满秩），包含所有奇异方向的信息。</p>
<p><strong>谱范数</strong>：
$$\Vert\boldsymbol{W}\Vert_2^2 = \sigma_1^2 \quad\Rightarrow\quad \nabla_{\boldsymbol{W}}\left(\frac{1}{2}\Vert\boldsymbol{W}\Vert_2^2\right) = \sigma_1\boldsymbol{u}_1\boldsymbol{v}_1^{\top}$$</p>
<p>这是一个<strong>秩1</strong>的梯度，只针对最大奇异值方向。</p>
<p><strong>几何解释</strong>：
- Frobenius范数权重衰减：$\boldsymbol{W}\leftarrow\boldsymbol{W}-\eta\lambda\boldsymbol{W}=(1-\eta\lambda)\boldsymbol{W}$，对所有奇异值进行<strong>等比例缩放</strong>。
- 谱范数权重衰减：$\boldsymbol{W}\leftarrow\boldsymbol{W}-\eta\lambda\sigma_1\boldsymbol{u}_1\boldsymbol{v}_1^{\top}$，<strong>选择性地</strong>减小最大奇异值。</p>
<h4 id="32">3.2 效率对比<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>计算$\nabla_{\boldsymbol{W}}\Vert\boldsymbol{W}\Vert_F^2$：直接返回$\boldsymbol{W}$，复杂度$\mathcal{O}(1)$。</p>
<p>计算$\nabla_{\boldsymbol{W}}\Vert\boldsymbol{W}\Vert_2^2$：需要求$\boldsymbol{v}_1$（和$\sigma_1$）。
- <strong>精确方法</strong>：完整SVD，复杂度$\mathcal{O}(\min(n^2m,nm^2))$。
- <strong>近似方法</strong>：幂迭代$t$次，复杂度$\mathcal{O}(tnm)$，通常$t\ll\min(n,m)$。</p>
<p>实践中，$t=1\sim 10$次幂迭代通常足够，使得谱范数权重衰减的额外计算开销可接受。</p>
<h4 id="33">3.3 作用机制的理论分析<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>考虑一个线性层$\boldsymbol{y}=\boldsymbol{W}\boldsymbol{x}$，输入为$\boldsymbol{x}\sim\mathcal{N}(\mathbf{0},\boldsymbol{I})$。输出的协方差矩阵为：
$$\mathbb{E}[\boldsymbol{y}\boldsymbol{y}^{\top}] = \mathbb{E}[\boldsymbol{W}\boldsymbol{x}\boldsymbol{x}^{\top}\boldsymbol{W}^{\top}] = \boldsymbol{W}\boldsymbol{W}^{\top}$$</p>
<p>其特征值为$\sigma_1^2,\ldots,\sigma_r^2$。</p>
<p><strong>Frobenius权重衰减</strong>通过缩放$\boldsymbol{W}$来均匀减小所有$\sigma_i$，可能导致：
- <strong>过度正则化</strong>：小的$\sigma_i$被进一步压缩，丢失信息。
- <strong>表达能力下降</strong>：降低矩阵的有效秩。</p>
<p><strong>谱范数权重衰减</strong>只减小$\sigma_1$，保留其他方向：
- <strong>精准控制</strong>：确保$\Vert\boldsymbol{W}\boldsymbol{x}\Vert_2\leq\sigma_1\Vert\boldsymbol{x}\Vert_2$中的$\sigma_1$不过大。
- <strong>保持多样性</strong>：次要奇异方向不受影响，保留更多信息。</p>
<h3 id="4-spectral-normalization">4. 谱归一化（Spectral Normalization）的数学原理<a class="toc-link" href="#4-spectral-normalization" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 定义与动机<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p>谱归一化最早由Miyato et al. (2018)在GAN训练中提出。其核心思想是将权重矩阵归一化为：
$$\boldsymbol{W}_{\text{SN}} = \frac{\boldsymbol{W}}{\sigma_1(\boldsymbol{W})}$$</p>
<p>使得$\Vert\boldsymbol{W}_{\text{SN}}\Vert_2=1$。</p>
<p><strong>目的</strong>：控制神经网络的Lipschitz常数。对于函数$f(\boldsymbol{x})=\sigma(\boldsymbol{W}\boldsymbol{x})$（$\sigma$是激活函数），如果$\sigma$是1-Lipschitz的（如ReLU），则：
$$\Vert f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)\Vert_2 \leq \Vert\boldsymbol{W}\Vert_2\Vert\boldsymbol{x}_1-\boldsymbol{x}_2\Vert_2$$</p>
<p>通过谱归一化，确保每层的Lipschitz常数为1，整个网络的Lipschitz常数为层数的乘积（最多为层数）。</p>
<h4 id="42">4.2 反向传播分析<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p>设损失函数为$\mathcal{L}$，权重为$\boldsymbol{W}<em _text_SN="\text{SN">{\text{SN}}=\boldsymbol{W}/\sigma_1$。反向传播时需要计算：
$$\frac{\partial\mathcal{L}}{\partial\boldsymbol{W}} = \frac{\partial\mathcal{L}}{\partial\boldsymbol{W}</em>$$}}} \frac{\partial\boldsymbol{W}_{\text{SN}}}{\partial\boldsymbol{W}</p>
<p>右侧第二项较为复杂。令$\boldsymbol{W}_{\text{SN}}=g(\boldsymbol{W})$，则：
$$dg = d\left(\frac{\boldsymbol{W}}{\sigma_1}\right) = \frac{d\boldsymbol{W}}{\sigma_1} - \frac{\boldsymbol{W}(d\sigma_1)}{\sigma_1^2}$$</p>
<p>其中$d\sigma_1 = \text{Tr}((\boldsymbol{u}_1\boldsymbol{v}_1^{\top})^{\top}d\boldsymbol{W})$，故：
$$dg = \frac{1}{\sigma_1}\left(\boldsymbol{I} - \frac{\boldsymbol{W}\boldsymbol{u}_1\boldsymbol{v}_1^{\top}}{\sigma_1}\right)d\boldsymbol{W} = \frac{1}{\sigma_1}\left(\boldsymbol{I} - \boldsymbol{u}_1\boldsymbol{u}_1^{\top}\otimes\boldsymbol{v}_1\boldsymbol{v}_1^{\top}\right)d\boldsymbol{W}$$</p>
<p>因此梯度为：
$$\frac{\partial\mathcal{L}}{\partial\boldsymbol{W}} = \frac{1}{\sigma_1}\left(\boldsymbol{I} - \boldsymbol{u}<em _text_SN="\text{SN">1\boldsymbol{u}_1^{\top}\right)\frac{\partial\mathcal{L}}{\partial\boldsymbol{W}</em>\right)$$}}}\left(\boldsymbol{I} - \boldsymbol{v}_1\boldsymbol{v}_1^{\top</p>
<p><strong>解释</strong>：投影到正交于$\boldsymbol{u}_1,\boldsymbol{v}_1$的子空间，避免梯度增大$\sigma_1$（因为归一化会抵消）。</p>
<h4 id="43">4.3 与权重衰减的关系<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>谱归一化是<strong>硬约束</strong>：$\Vert\boldsymbol{W}\Vert_2=1$。</p>
<p>谱范数权重衰减是<strong>软约束</strong>：在损失中添加惩罚项$\lambda\Vert\boldsymbol{W}\Vert_2^2$。</p>
<p>两者的联系可通过拉格朗日乘数法理解。考虑约束优化问题：
$$\min_{\boldsymbol{W}} \mathcal{L}(\boldsymbol{W}) \quad\text{s.t.}\quad \Vert\boldsymbol{W}\Vert_2 \leq C$$</p>
<p>拉格朗日函数为：
$$\mathcal{L}_{\text{aug}}(\boldsymbol{W},\lambda) = \mathcal{L}(\boldsymbol{W}) + \lambda(\Vert\boldsymbol{W}\Vert_2 - C)$$</p>
<p>当约束激活时（$\Vert\boldsymbol{W}\Vert_2=C$），对$\boldsymbol{W}$的梯度为：
$$\nabla_{\boldsymbol{W}}\mathcal{L}<em _boldsymbol_W="\boldsymbol{W">{\text{aug}} = \nabla</em>$$}}\mathcal{L} + \lambda\boldsymbol{u}_1\boldsymbol{v}_1^{\top</p>
<p>这正是谱范数权重衰减的形式（$\lambda$为拉格朗日乘子）。</p>
<h3 id="5">5. 权重衰减的不同形式及其统一框架<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 一般化的权重衰减<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>权重衰减可以统一为如下形式：
$$\boldsymbol{W}<em _boldsymbol_W="\boldsymbol{W">{t+1} = \boldsymbol{W}_t - \eta\nabla</em>)$$}}\mathcal{L} - \eta\lambda\nabla_{\boldsymbol{W}}R(\boldsymbol{W</p>
<p>其中$R(\boldsymbol{W})$是正则项。常见形式：</p>
<table>
<thead>
<tr>
<th>正则项</th>
<th>梯度</th>
<th>名称</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\frac{1}{2}\Vert\boldsymbol{W}\Vert_F^2$</td>
<td>$\boldsymbol{W}$</td>
<td>L2正则（Frobenius）</td>
</tr>
<tr>
<td>$\frac{1}{2}\Vert\boldsymbol{W}\Vert_2^2$</td>
<td>$\sigma_1\boldsymbol{u}_1\boldsymbol{v}_1^{\top}$</td>
<td>谱正则</td>
</tr>
<tr>
<td>$\Vert\boldsymbol{W}\Vert_1$</td>
<td>$\text{sign}(\boldsymbol{W})$</td>
<td>L1正则</td>
</tr>
<tr>
<td>$\Vert\boldsymbol{W}\Vert_*$</td>
<td>$\boldsymbol{U}\boldsymbol{V}^{\top}$</td>
<td>核范数正则</td>
</tr>
</tbody>
</table>
<p>其中核范数（nuclear norm）$\Vert\boldsymbol{W}\Vert_*=\sum_{i}\sigma_i$，促进低秩。</p>
<h4 id="52-l2">5.2 L2正则的退化问题<a class="toc-link" href="#52-l2" title="Permanent link">&para;</a></h4>
<p>L2正则等价于高斯先验$\boldsymbol{W}\sim\mathcal{N}(\mathbf{0},\lambda^{-1}\boldsymbol{I})$。更新规则：
$$\boldsymbol{W}<em _boldsymbol_W="\boldsymbol{W">{t+1} = (1-\eta\lambda)\boldsymbol{W}_t - \eta\nabla</em>$$}}\mathcal{L</p>
<p><strong>问题</strong>：当$\eta\lambda$较大时，$(1-\eta\lambda)$可能很小，导致：
- <strong>梯度消失</strong>：历史信息快速衰减。
- <strong>学习困难</strong>：需要非常大的梯度才能维持权重。</p>
<p><strong>SVD视角</strong>：
$$(1-\eta\lambda)\boldsymbol{W} = \sum_{i}(1-\eta\lambda)\sigma_i\boldsymbol{u}_i\boldsymbol{v}_i^{\top}$$</p>
<p>所有$\sigma_i$被等比例缩放，包括已经很小的奇异值，可能导致信息丢失。</p>
<h4 id="53">5.3 谱正则的优势<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p>谱正则更新：
$$\boldsymbol{W}<em _boldsymbol_W="\boldsymbol{W">{t+1} = \boldsymbol{W}_t - \eta\lambda\sigma_1\boldsymbol{u}_1\boldsymbol{v}_1^{\top} - \eta\nabla</em>$$}}\mathcal{L</p>
<p>SVD视角：
$$\boldsymbol{W}<em i="1">{t+1} = \sum</em>}^{r}\sigma_i^{(t)}\boldsymbol{u<em _boldsymbol_W="\boldsymbol{W">i\boldsymbol{v}_i^{\top} - \eta\lambda\sigma_1^{(t)}\boldsymbol{u}_1\boldsymbol{v}_1^{\top} - \eta\nabla</em>$$}}\mathcal{L</p>
<p>主要作用于$\sigma_1$：
$$\sigma_1^{(t+1)} \approx \sigma_1^{(t)} - \eta\lambda\sigma_1^{(t)} + \mathcal{O}(\eta\nabla\mathcal{L})$$</p>
<p>而其他$\sigma_i(i\geq 2)$主要受$\nabla\mathcal{L}$影响，不会被过度压缩。</p>
<p><strong>适应性</strong>：谱正则的衰减强度$\eta\lambda\sigma_1$与$\sigma_1$成正比，具有<strong>自适应性</strong>——大的奇异值被更强地惩罚。</p>
<h3 id="6">6. 梯度下降动力学的谱分析<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 简化模型：线性回归<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>考虑线性回归问题：$\min_{\boldsymbol{W}} \frac{1}{2}\Vert\boldsymbol{Y}-\boldsymbol{W}\boldsymbol{X}\Vert_F^2$，其中$\boldsymbol{X}\in\mathbb{R}^{m\times N}$，$\boldsymbol{Y}\in\mathbb{R}^{n\times N}$。</p>
<p>梯度为：
$$\nabla_{\boldsymbol{W}}\mathcal{L} = (\boldsymbol{W}\boldsymbol{X}-\boldsymbol{Y})\boldsymbol{X}^{\top}$$</p>
<p><strong>L2权重衰减</strong>下的动力学：
$$\boldsymbol{W}_{t+1} = (1-\eta\lambda)\boldsymbol{W}_t - \eta(\boldsymbol{W}_t\boldsymbol{X}-\boldsymbol{Y})\boldsymbol{X}^{\top}$$</p>
<p>改写为：
$$\boldsymbol{W}_{t+1} = \boldsymbol{W}_t(1-\eta\lambda) - \eta\boldsymbol{W}_t\boldsymbol{X}\boldsymbol{X}^{\top} + \eta\boldsymbol{Y}\boldsymbol{X}^{\top}$$
$$= \boldsymbol{W}_t\left[(1-\eta\lambda)\boldsymbol{I} - \eta\boldsymbol{X}\boldsymbol{X}^{\top}\right] + \eta\boldsymbol{Y}\boldsymbol{X}^{\top}$$</p>
<p>设$\boldsymbol{X}\boldsymbol{X}^{\top}=\sum_{i}\mu_i\boldsymbol{q}<em t_1="t+1">i\boldsymbol{q}_i^{\top}$（特征分解），则：
$$\boldsymbol{W}</em>} = \boldsymbol{W<em i="i">t\sum</em>$$}(1-\eta\lambda-\eta\mu_i)\boldsymbol{q}_i\boldsymbol{q}_i^{\top} + \eta\boldsymbol{Y}\boldsymbol{X}^{\top</p>
<p><strong>收敛条件</strong>：$|1-\eta\lambda-\eta\mu_i|&lt;1$对所有$i$成立，即：
$$0 &lt; \eta &lt; \frac{2}{\lambda+\mu_{\max}}$$</p>
<p>其中$\mu_{\max}$是$\boldsymbol{X}\boldsymbol{X}^{\top}$的最大特征值。</p>
<p><strong>谱偏差</strong>（Spectral Bias）：不同特征方向的收敛速度不同。第$i$个特征方向的衰减率为$(1-\eta\lambda-\eta\mu_i)$，对应的收敛速度与$\mu_i$成正比。大特征值方向收敛快，小特征值方向收敛慢。</p>
<h4 id="62">6.2 谱权重衰减的动力学<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p>谱权重衰减下：
$$\boldsymbol{W}_{t+1} = \boldsymbol{W}_t - \eta\lambda\sigma_1^{(t)}\boldsymbol{u}_1^{(t)}(\boldsymbol{v}_1^{(t)})^{\top} - \eta(\boldsymbol{W}_t\boldsymbol{X}-\boldsymbol{Y})\boldsymbol{X}^{\top}$$</p>
<p>这是一个<strong>非线性动力系统</strong>，因为$\sigma_1,\boldsymbol{u}_1,\boldsymbol{v}_1$都依赖于$\boldsymbol{W}_t$。</p>
<p><strong>局部线性化分析</strong>：在平衡点$\boldsymbol{W}^<em>$附近，设$\boldsymbol{W}_t=\boldsymbol{W}^</em>+\boldsymbol{\Delta}_t$，则：
$$\sigma_1(\boldsymbol{W}_t) \approx \sigma_1(\boldsymbol{W}^<em>) + \text{Tr}((\boldsymbol{u}_1^</em>(\boldsymbol{v}_1^<em>)^{\top})^{\top}\boldsymbol{\Delta}_t)$$
$$\boldsymbol{u}_1(\boldsymbol{W}_t)\boldsymbol{v}_1(\boldsymbol{W}_t)^{\top} \approx \boldsymbol{u}_1^</em>(\boldsymbol{v}_1^*)^{\top} + \mathcal{O}(\Vert\boldsymbol{\Delta}_t\Vert)$$</p>
<p>一阶近似下：
$$\boldsymbol{\Delta}_{t+1} \approx \boldsymbol{\Delta}_t\left[\boldsymbol{I} - \eta\boldsymbol{X}\boldsymbol{X}^{\top}\right] - \eta\lambda\sigma_1^<em>\boldsymbol{u}_1^</em>(\boldsymbol{v}_1^*)^{\top}$$</p>
<p><strong>关键区别</strong>：谱衰减项$\eta\lambda\sigma_1^<em>\boldsymbol{u}_1^</em>(\boldsymbol{v}_1^<em>)^{\top}$是一个</em><em>秩1矩阵</em><em>，只在$\boldsymbol{u}_1^</em>$方向施加衰减。这意味着：
- 与$\boldsymbol{u}_1^<em>$正交的方向</em><em>不受谱衰减影响</em>*，完全由数据项$\boldsymbol{X}\boldsymbol{X}^{\top}$驱动。
- 更精细地控制权重的谱分布。</p>
<h3 id="7-muon">7. Muon优化器中的谱裁剪<a class="toc-link" href="#7-muon" title="Permanent link">&para;</a></h3>
<h4 id="71-muon">7.1 Muon优化器回顾<a class="toc-link" href="#71-muon" title="Permanent link">&para;</a></h4>
<p>Muon优化器的核心更新规则为：
$$\boldsymbol{W}<em _boldsymbol_W="\boldsymbol{W">{t+1} = \boldsymbol{W}_t - \eta\frac{\nabla</em>$$}}\mathcal{L}}{\Vert\nabla_{\boldsymbol{W}}\mathcal{L}\Vert_2</p>
<p>即对梯度进行谱归一化（使其谱范数为1）后更新。</p>
<p><strong>理论解释</strong>：这可以视为在谱范数球约束下的最速下降：
$$\min_{\boldsymbol{\Delta}:\Vert\boldsymbol{\Delta}\Vert_2\leq\eta} \text{Tr}((\nabla\mathcal{L})^{\top}\boldsymbol{\Delta})$$</p>
<p>最优解为$\boldsymbol{\Delta}^*=-\eta\nabla\mathcal{L}/\Vert\nabla\mathcal{L}\Vert_2$。</p>
<h4 id="72">7.2 谱裁剪的数学形式<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p>一般的梯度裁剪可以写为：
$$\boldsymbol{g}_{\text{clip}} = \begin{cases}
\boldsymbol{g} &amp; \text{if }\Vert\boldsymbol{g}\Vert \leq \tau \
\tau\frac{\boldsymbol{g}}{\Vert\boldsymbol{g}\Vert} &amp; \text{otherwise}
\end{cases}$$</p>
<p>Muon使用<strong>谱范数裁剪</strong>：
$$\boldsymbol{g}_{\text{clip}} = \begin{cases}
\boldsymbol{g} &amp; \text{if }\Vert\boldsymbol{g}\Vert_2 \leq \tau \
\tau\frac{\boldsymbol{g}}{\Vert\boldsymbol{g}\Vert_2} &amp; \text{otherwise}
\end{cases}$$</p>
<p>其中$\Vert\boldsymbol{g}\Vert_2=\sigma_1(\boldsymbol{g})$。</p>
<p><strong>SVD视角</strong>：设$\boldsymbol{g}=\sum_{i}\gamma_i\boldsymbol{p}<em i="i">i\boldsymbol{q}_i^{\top}$，则：
$$\frac{\boldsymbol{g}}{\Vert\boldsymbol{g}\Vert_2} = \sum</em>$$}\frac{\gamma_i}{\gamma_1}\boldsymbol{p}_i\boldsymbol{q}_i^{\top</p>
<p>最大奇异值方向被归一化为1，其他方向被相应缩放（$\gamma_i/\gamma_1\leq 1$）。</p>
<h4 id="73">7.3 与自适应学习率的关系<a class="toc-link" href="#73" title="Permanent link">&para;</a></h4>
<p>传统自适应优化器（如Adam）对每个参数分量使用不同的学习率：
$$w_{ij,t+1} = w_{ij,t} - \frac{\eta}{\sqrt{v_{ij,t}}+\epsilon}g_{ij,t}$$</p>
<p>Muon则对整个矩阵使用<strong>全局的谱归一化</strong>：
$$\boldsymbol{W}_{t+1} = \boldsymbol{W}_t - \eta\frac{\boldsymbol{G}_t}{\sigma_1(\boldsymbol{G}_t)}$$</p>
<p><strong>对比</strong>：
- Adam：元素级自适应，可能破坏梯度的<strong>几何结构</strong>。
- Muon：矩阵级归一化，保持梯度的<strong>主要方向</strong>（由$\boldsymbol{p}_1\boldsymbol{q}_1^{\top}$决定）。</p>
<p><strong>理论优势</strong>：对于矩阵参数，其自然几何是矩阵流形，而非欧氏空间。谱归一化尊重这一几何。</p>
<h3 id="8">8. 收敛性分析<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81-l2">8.1 L2权重衰减的收敛性<a class="toc-link" href="#81-l2" title="Permanent link">&para;</a></h4>
<p>考虑强凸函数$\mathcal{L}(\boldsymbol{W})$（满足$\nabla^2\mathcal{L}\succeq\mu\boldsymbol{I}$）和L-光滑（$\Vert\nabla^2\mathcal{L}\Vert_2\leq L$）。</p>
<p><strong>定理（L2衰减的收敛率）</strong>：在L2权重衰减下，选择$\eta\leq\min{1/(L+\lambda),1/\lambda}$，则：
$$\Vert\boldsymbol{W}_t-\boldsymbol{W}^<em>\Vert_F^2 \leq (1-\eta\mu)^t\Vert\boldsymbol{W}_0-\boldsymbol{W}^</em>\Vert_F^2$$</p>
<p>收敛速度为$\mathcal{O}(\exp(-\mu t\eta))$，线性收敛。</p>
<p><strong>证明草图</strong>：定义Lyapunov函数$V(\boldsymbol{W})=\mathcal{L}(\boldsymbol{W})+\frac{\lambda}{2}\Vert\boldsymbol{W}\Vert_F^2$。一步更新后：
$$V(\boldsymbol{W}_{t+1}) \leq V(\boldsymbol{W}_t) - \frac{\eta}{2}(1-\eta L)\Vert\nabla V(\boldsymbol{W}_t)\Vert_F^2$$</p>
<p>利用强凸性$V(\boldsymbol{W})\geq V(\boldsymbol{W}^<em>)+\frac{\mu}{2}\Vert\boldsymbol{W}-\boldsymbol{W}^</em>\Vert_F^2$，得到收敛率。</p>
<h4 id="82">8.2 谱权重衰减的收敛性<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>谱权重衰减的收敛性分析更为复杂，因为更新不再是简单的线性变换。我们给出一个简化的结果。</p>
<p><strong>假设</strong>：$\mathcal{L}$是L-光滑的，且满足<strong>谱强凸性</strong>（spectral strong convexity）：
$$\langle\nabla\mathcal{L}(\boldsymbol{W})-\nabla\mathcal{L}(\boldsymbol{W}'),\boldsymbol{W}-\boldsymbol{W}'\rangle \geq \mu_{\text{spec}}\Vert\boldsymbol{W}-\boldsymbol{W}'\Vert_2^2$$</p>
<p><strong>定理（谱衰减的收敛）</strong>：在谱权重衰减下，选择$\eta\leq 1/(L+\lambda\Vert\boldsymbol{W}_0\Vert_2)$，则对于足够小的$\lambda$：
$$\Vert\boldsymbol{W}_t-\boldsymbol{W}^<em>\Vert_2 \leq \exp\left(-\frac{\mu_{\text{spec}}t\eta}{2}\right)\Vert\boldsymbol{W}_0-\boldsymbol{W}^</em>\Vert_2$$</p>
<p>收敛速度为$\mathcal{O}(\exp(-\mu_{\text{spec}}t\eta))$。</p>
<p><strong>关键洞察</strong>：谱衰减在谱范数意义下具有线性收敛率，而F范数下可能更慢（因为只控制了最大奇异值）。</p>
<h4 id="83">8.3 非凸情况的局部收敛<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p>对于神经网络等非凸问题，全局收敛性难以保证。但可以分析<strong>临界点附近的局部收敛</strong>。</p>
<p><strong>定理（逃离鞍点）</strong>：假设$\nabla\mathcal{L}(\boldsymbol{W}^<em>)=\mathbf{0}$且Hessian $\nabla^2\mathcal{L}(\boldsymbol{W}^</em>)$有负特征值$\lambda_{\min}&lt;0$。在谱权重衰减下，若初始点接近$\boldsymbol{W}^<em>$，则：
- 若$\boldsymbol{u}_1(\boldsymbol{W}_0)$与Hessian的最小特征向量有显著重叠，算法会</em><em>加速逃离</em><em>该鞍点。
- L2衰减会在所有方向上施加衰减，可能</em><em>减慢逃离速度</em>*。</p>
<p><strong>直觉</strong>：谱衰减不会干扰负曲率方向（若$\boldsymbol{u}_1$不对齐），而L2衰减会在所有方向施加阻力。</p>
<h3 id="9">9. 泛化界的改进<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91-rademacher">9.1 Rademacher复杂度视角<a class="toc-link" href="#91-rademacher" title="Permanent link">&para;</a></h4>
<p>神经网络的泛化误差可以用Rademacher复杂度界定：
$$\mathbb{E}[\text{Risk}] - \text{Empirical Risk} \leq \mathcal{R}(\mathcal{F}) + \mathcal{O}\left(\sqrt{\frac{\log(1/\delta)}{N}}\right)$$</p>
<p>其中$\mathcal{R}(\mathcal{F})$是函数类$\mathcal{F}$的Rademacher复杂度。</p>
<p>对于多层神经网络$f(\boldsymbol{x})=\boldsymbol{W}<em L-1="L-1">L\sigma(\boldsymbol{W}</em>)\cdots)$：}\cdots\sigma(\boldsymbol{W}_1\boldsymbol{x</p>
<p><strong>L2约束</strong>下（$\Vert\boldsymbol{W}<em i="1">i\Vert_F\leq B_i$）：
$$\mathcal{R}(\mathcal{F}) \leq \frac{\prod</em>$$}^{L}B_i}{\sqrt{N}}\sum_{i=1}^{L}\frac{1}{B_i</p>
<p><strong>谱约束</strong>下（$\Vert\boldsymbol{W}<em i="1">i\Vert_2\leq B_i$）：
$$\mathcal{R}(\mathcal{F}) \leq \frac{\prod</em>$$}^{L}B_i}{\sqrt{N}</p>
<p><strong>对比</strong>：谱约束的泛化界<strong>更紧</strong>（无求和项），因为谱范数直接控制函数的Lipschitz常数。</p>
<h4 id="92-pac-bayes">9.2 PAC-Bayes界<a class="toc-link" href="#92-pac-bayes" title="Permanent link">&para;</a></h4>
<p>PAC-Bayes理论提供了另一种泛化分析。设参数后验为$Q$，先验为$P$，则：
$$\mathbb{E}<em _boldsymbol_W="\boldsymbol{W">{\boldsymbol{W}\sim Q}[\text{Risk}(\boldsymbol{W})] \leq \mathbb{E}</em>$$}\sim Q}[\text{Empirical Risk}(\boldsymbol{W})] + \sqrt{\frac{\text{KL}(Q\Vert P)+\log(N/\delta)}{2N}</p>
<p><strong>L2先验</strong>：$P=\prod_{i}\mathcal{N}(\boldsymbol{W}<em i="i">i|\mathbf{0},\lambda^{-1}\boldsymbol{I})$，KL散度为：
$$\text{KL}(Q\Vert P) = \frac{\lambda}{2}\sum</em>$$}\mathbb{E}_{Q}[\Vert\boldsymbol{W}_i\Vert_F^2] + \text{const</p>
<p><strong>谱先验</strong>：考虑矩阵Langevin分布（Matrix Langevin distribution），其密度正比于$\exp(-\lambda\Vert\boldsymbol{W}\Vert_2)$。相应的KL散度为：
$$\text{KL}(Q\Vert P) = \lambda\sum_{i}\mathbb{E}_{Q}[\Vert\boldsymbol{W}_i\Vert_2] + \text{const}$$</p>
<p>由于$\Vert\boldsymbol{W}\Vert_2\leq\Vert\boldsymbol{W}\Vert_F$，谱先验对应的KL项<strong>更小</strong>，导致更紧的泛化界。</p>
<h4 id="93">9.3 实证风险与谱复杂度<a class="toc-link" href="#93" title="Permanent link">&para;</a></h4>
<p>Bartlett et al. (2017)提出<strong>谱复杂度</strong>（spectral complexity）：
$$\mathcal{S}(f) = \left(\prod_{i=1}^{L}\Vert\boldsymbol{W}<em i="1">i\Vert_2\right)\left(\sum</em>$$}^{L}\frac{\Vert\boldsymbol{W}_i\Vert_F^{2/3}}{\Vert\boldsymbol{W}_i\Vert_2^{2/3}}\right)^{3/2</p>
<p>相应的泛化界为：
$$\text{Gap} \leq \tilde{\mathcal{O}}\left(\frac{\mathcal{S}(f)\sqrt{L}}{\sqrt{N}}\right)$$</p>
<p><strong>谱权重衰减</strong>通过减小$\Vert\boldsymbol{W}_i\Vert_2$来降低$\mathcal{S}(f)$，同时保持$\Vert\boldsymbol{W}_i\Vert_F$不过分缩小，从而在表达能力和泛化之间取得更好的平衡。</p>
<h3 id="10-gan">10. GAN训练中的应用<a class="toc-link" href="#10-gan" title="Permanent link">&para;</a></h3>
<h4 id="101-wganlipschitz">10.1 WGAN与Lipschitz约束<a class="toc-link" href="#101-wganlipschitz" title="Permanent link">&para;</a></h4>
<p>Wasserstein GAN的判别器（critic）需要满足1-Lipschitz约束：
$$\Vert D(\boldsymbol{x})-D(\boldsymbol{x}')\Vert \leq \Vert\boldsymbol{x}-\boldsymbol{x}'\Vert$$</p>
<p>原始WGAN使用权重裁剪（weight clipping），但这会导致梯度爆炸/消失。</p>
<h4 id="102-gan">10.2 谱归一化在GAN中的应用<a class="toc-link" href="#102-gan" title="Permanent link">&para;</a></h4>
<p>Miyato et al. (2018)提出将判别器的每一层权重$\boldsymbol{W}_i$替换为$\boldsymbol{W}_i/\sigma_1(\boldsymbol{W}_i)$。</p>
<p><strong>理论保证</strong>：若每层激活函数$\sigma$是1-Lipschitz的（如ReLU、LeakyReLU），则判别器$D$的Lipschitz常数为：
$$\text{Lip}(D) \leq \prod_{i=1}^{L}\Vert\boldsymbol{W}<em i="1">i\Vert_2 = \prod</em>1 = 1$$}^{L</p>
<p>从而满足WGAN的约束。</p>
<p><strong>证明</strong>：对于$f(\boldsymbol{x})=\sigma(\boldsymbol{W}\boldsymbol{x})$，
$$\Vert f(\boldsymbol{x})-f(\boldsymbol{x}')\Vert \leq \text{Lip}(\sigma)\Vert\boldsymbol{W}(\boldsymbol{x}-\boldsymbol{x}')\Vert \leq \text{Lip}(\sigma)\Vert\boldsymbol{W}\Vert_2\Vert\boldsymbol{x}-\boldsymbol{x}'\Vert$$</p>
<p>若$\text{Lip}(\sigma)=1$且$\Vert\boldsymbol{W}\Vert_2=1$，则$\text{Lip}(f)=1$。多层复合得到结果。</p>
<h4 id="103">10.3 谱权重衰减作为软约束<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p>与硬归一化（谱归一化）不同，谱权重衰减作为软约束：
$$\min_{D,G} \mathbb{E}<em _text_data="\text{data">{\boldsymbol{x}\sim p</em>}}}[D(\boldsymbol{x})] - \mathbb{E<em i="i">{\boldsymbol{z}\sim p_z}[D(G(\boldsymbol{z}))] + \lambda\sum</em>_i^D\Vert_2^2$$}\Vert\boldsymbol{W</p>
<p><strong>优点</strong>：
- <strong>灵活性</strong>：通过调整$\lambda$控制Lipschitz常数的上界，而非固定为1。
- <strong>更平滑的优化</strong>：避免硬约束导致的梯度不连续。</p>
<p><strong>实验观察</strong>：在某些GAN任务中，谱权重衰减可以获得与谱归一化相近的效果，同时训练更稳定。</p>
<h4 id="104">10.4 与梯度惩罚的结合<a class="toc-link" href="#104" title="Permanent link">&para;</a></h4>
<p>梯度惩罚（Gradient Penalty, GP）是另一种实现Lipschitz约束的方法：
$$\mathcal{L}<em _tilde_boldsymbol_x="\tilde{\boldsymbol{x">{\text{GP}} = \mathbb{E}</em>)\Vert_2-1)^2]$$}}}[(\Vert\nabla_{\tilde{\boldsymbol{x}}}D(\tilde{\boldsymbol{x}</p>
<p>其中$\tilde{\boldsymbol{x}}$是真实样本和生成样本之间的插值。</p>
<p><strong>结合方式</strong>：
$$\mathcal{L}<em _text_WGAN="\text{WGAN">D = \mathcal{L}</em>}} + \lambda_{\text{GP}}\mathcal{L<em _text_spec="\text{spec">{\text{GP}} + \lambda</em>_i\Vert_2^2$$}}\sum_{i}\Vert\boldsymbol{W</p>
<ul>
<li>GP确保沿数据流形的Lipschitz约束。</li>
<li>谱正则确保全局的谱范数不过大。</li>
</ul>
<p><strong>协同效应</strong>：两者互补——GP是局部约束，谱正则是全局约束。</p>
<h3 id="11_1">11. 实现细节与优化<a class="toc-link" href="#11_1" title="Permanent link">&para;</a></h3>
<h4 id="111-boldsymbolv_1">11.1 高效计算$\boldsymbol{v}_1$的策略<a class="toc-link" href="#111-boldsymbolv_1" title="Permanent link">&para;</a></h4>
<p><strong>策略1：幂迭代</strong>（如正文所述）</p>
<div class="highlight"><pre><span></span><code><span class="err">初始化</span><span class="o">:</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ones</span><span class="o">(</span><span class="n">m</span><span class="o">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sqrt</span><span class="o">(</span><span class="n">m</span><span class="o">)</span>
<span class="k">for</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">range</span><span class="o">(</span><span class="n">num_iterations</span><span class="o">):</span>
<span class="w">    </span><span class="n">u</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="err">@</span><span class="w"> </span><span class="n">v</span>
<span class="w">    </span><span class="n">u</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">u</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">norm</span><span class="o">(</span><span class="n">u</span><span class="o">)</span>
<span class="w">    </span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">W</span><span class="o">.</span><span class="na">T</span><span class="w"> </span><span class="err">@</span><span class="w"> </span><span class="n">u</span>
<span class="w">    </span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">norm</span><span class="o">(</span><span class="n">v</span><span class="o">)</span>
</code></pre></div>

<p>复杂度：$\mathcal{O}(t\cdot nm)$，通常$t=1\sim 10$。</p>
<p><strong>策略2：缓存与渐进更新</strong></p>
<p>利用相邻迭代中$\boldsymbol{W}_t$变化不大，$\boldsymbol{v}_1^{(t)}\approx\boldsymbol{v}_1^{(t-1)}$：</p>
<div class="highlight"><pre><span></span><code>v = v_cached  # 使用上一步的v1作为初始化
v = power_iteration(W, v, num_iterations=1)
v_cached = v
</code></pre></div>

<p>这样每步只需1次幂迭代，效果仍然良好。</p>
<p><strong>策略3：随机化方法</strong></p>
<p>对于非常大的矩阵，可以使用随机SVD（Randomized SVD）：</p>
<div class="highlight"><pre><span></span><code>Omega = random_matrix(m, k)  # k &lt;&lt; m
Y = W @ Omega
Q, _ = qr(Y)  # QR分解
B = Q.T @ W
U_B, Sigma, Vt = svd(B)  # 对小矩阵做SVD
u1 = Q @ U_B[:, 0]
v1 = Vt[0, :]
</code></pre></div>

<p>复杂度：$\mathcal{O}(nmk)$，$k\sim 10$通常足够。</p>
<h4 id="112">11.2 数值稳定性<a class="toc-link" href="#112" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：当$\sigma_1(\boldsymbol{W})\to 0$时，$\boldsymbol{u}_1\boldsymbol{v}_1^{\top}$可能不稳定。</p>
<p><strong>解决方案</strong>：
1. <strong>添加正则项</strong>：计算$\boldsymbol{v}<em _min="\min">1$时使用$\boldsymbol{W}^{\top}\boldsymbol{W}+\epsilon\boldsymbol{I}$。
2. <strong>阈值检查</strong>：若$\sigma_1&lt;\epsilon</em>$，回退到L2衰减或不施加衰减。
3. <strong>混合策略</strong>：
   $$\text{Decay} = \alpha\cdot\sigma_1\boldsymbol{u}_1\boldsymbol{v}_1^{\top} + (1-\alpha)\cdot\boldsymbol{W}$$
   其中$\alpha\in[0,1]$根据$\sigma_1/\Vert\boldsymbol{W}\Vert_F$自适应调整。</p>
<h4 id="113">11.3 分布式训练的考虑<a class="toc-link" href="#113" title="Permanent link">&para;</a></h4>
<p>在数据并行的分布式训练中，梯度在各设备间平均。谱权重衰减需要在梯度聚合<strong>之后</strong>计算：</p>
<div class="highlight"><pre><span></span><code><span class="gh">#</span> 错误：各设备独立计算v1，然后平均 ❌
v1_local = compute_v1(W_local)
v1 = all_reduce_mean(v1_local)  # 平均后的v1不正确

<span class="gh">#</span> 正确：聚合梯度后，在主设备上计算v1 ✓
grad = all_reduce_mean(grad_local)
v1 = compute_v1(W)  # W在所有设备上一致
decay = sigma1 * outer(u1, v1)
</code></pre></div>

<h3 id="12_1">12. 总结与展望<a class="toc-link" href="#12_1" title="Permanent link">&para;</a></h3>
<h4 id="121">12.1 理论贡献总结<a class="toc-link" href="#121" title="Permanent link">&para;</a></h4>
<p>本文从多个角度深入分析了谱范数及其在权重衰减中的应用：</p>
<ol>
<li>
<p><strong>矩阵理论视角</strong>：谱范数是矩阵的本质不变量，通过SVD与奇异值紧密联系，梯度为秩1矩阵$\boldsymbol{u}_1\boldsymbol{v}_1^{\top}$。</p>
</li>
<li>
<p><strong>优化理论视角</strong>：谱权重衰减精准控制最大奇异值，避免L2衰减的过度正则化；与谱归一化相关，但作为软约束更灵活。</p>
</li>
<li>
<p><strong>泛化理论视角</strong>：谱约束导致更紧的泛化界（Rademacher复杂度、PAC-Bayes界），理论上优于F范数约束。</p>
</li>
<li>
<p><strong>计算视角</strong>：幂迭代提供高效近似，复杂度$\mathcal{O}(tnm)$可接受；缓存和随机化进一步提升效率。</p>
</li>
</ol>
<h4 id="122">12.2 开放问题<a class="toc-link" href="#122" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>非凸优化的全局性质</strong>：深度网络中谱权重衰减的全局收敛性仍缺乏理论保证。</p>
</li>
<li>
<p><strong>自适应超参数</strong>：$\lambda$的选择目前依赖经验，能否设计自适应机制（如根据训练阶段调整）？</p>
</li>
<li>
<p><strong>与其他技术的结合</strong>：谱权重衰减与Batch Normalization、Layer Normalization等归一化技术的交互作用需要进一步研究。</p>
</li>
<li>
<p><strong>大规模实验验证</strong>：在LLM等大规模模型上的系统性实验仍然不足。</p>
</li>
</ol>
<h4 id="123">12.3 未来方向<a class="toc-link" href="#123" title="Permanent link">&para;</a></h4>
<ol>
<li>
<p><strong>张量正则化</strong>：将谱范数推广到高阶张量（如卷积核），定义基于张量分解的正则项。</p>
</li>
<li>
<p><strong>动态正则化</strong>：根据训练进度动态调整正则化强度，例如早期侧重表达能力（小$\lambda$），后期侧重泛化（大$\lambda$）。</p>
</li>
<li>
<p><strong>理论-实践的桥梁</strong>：设计更多的实验来验证理论预测（如泛化界的改进），建立理论与实践的反馈循环。</p>
</li>
<li>
<p><strong>硬件优化</strong>：针对谱范数计算开发专门的硬件加速器或高效库，降低计算开销。</p>
</li>
</ol>
<p>谱范数作为矩阵的基本属性，在深度学习优化中有着广阔的应用前景。本文的推导为理解和应用谱正则化提供了坚实的数学基础，期待未来有更多的理论突破和实践验证。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="生成扩散模型漫谈二十八分步理解一致性模型.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#302 生成扩散模型漫谈（二十八）：分步理解一致性模型</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="为什么梯度裁剪的默认模长是1.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#304 为什么梯度裁剪的默认模长是1？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">从谱范数梯度到新式权重衰减的思考</a><ul>
<li><a href="#_2">基础回顾</a></li>
<li><a href="#_3">梯度推导</a></li>
<li><a href="#_4">权重衰减</a></li>
<li><a href="#_5">数值计算</a></li>
<li><a href="#_6">迭代证明</a></li>
<li><a href="#_7">相关工作</a></li>
<li><a href="#_8">文章小结</a></li>
<li><a href="#_9">公式推导与注释</a><ul>
<li><a href="#1">1. 谱范数的定义与性质</a></li>
<li><a href="#2">2. 谱范数梯度的完整推导</a></li>
<li><a href="#3-frobenius">3. 与Frobenius范数的深入对比</a></li>
<li><a href="#4-spectral-normalization">4. 谱归一化（Spectral Normalization）的数学原理</a></li>
<li><a href="#5">5. 权重衰减的不同形式及其统一框架</a></li>
<li><a href="#6">6. 梯度下降动力学的谱分析</a></li>
<li><a href="#7-muon">7. Muon优化器中的谱裁剪</a></li>
<li><a href="#8">8. 收敛性分析</a></li>
<li><a href="#9">9. 泛化界的改进</a></li>
<li><a href="#10-gan">10. GAN训练中的应用</a></li>
<li><a href="#11_1">11. 实现细节与优化</a></li>
<li><a href="#12_1">12. 总结与展望</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>