<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>初探MuP：超参数的跨模型尺度迁移规律 | ML & Math Blog Posts</title>
    <meta name="description" content="初探MuP：超参数的跨模型尺度迁移规律
原文链接: https://spaces.ac.cn/archives/10770
发布日期: 

众所周知，完整训练一次大型LLM的成本是昂贵的，这就决定了我们不可能直接在大型LLM上反复测试超参数。一个很自然的想法是希望可以在同结构的小模型上仔细搜索超参数，找到最优组合后直接迁移到大模型上。尽管这个想法很朴素，但要实现它并不平凡，它需要我们了解常见的超参...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">初探MuP：超参数的跨模型尺度迁移规律</h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/10770" target="_blank">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                <span class="tag"><i class="fas fa-tag"></i> 学习率</span>
                <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                <span class="tag"><i class="fas fa-tag"></i> 尺度定律</span>
                <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                
            </div>
            
        </header>

        <!-- Post Body -->
        <div class="post-content">
            <h1 id="mup">初探MuP：超参数的跨模型尺度迁移规律</h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10770">https://spaces.ac.cn/archives/10770</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，完整训练一次大型LLM的成本是昂贵的，这就决定了我们不可能直接在大型LLM上反复测试超参数。一个很自然的想法是希望可以在同结构的小模型上仔细搜索超参数，找到最优组合后直接迁移到大模型上。尽管这个想法很朴素，但要实现它并不平凡，它需要我们了解常见的超参数与模型尺度之间的缩放规律，而MuP正是这个想法的一个实践。</p>
<p>MuP，有时也写$\mu P$，全名是Maximal Update Parametrization，出自论文<a href="https://papers.cool/arxiv/2203.03466">《Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer》</a>，随着LLM训练的普及，它逐渐已经成为了科学炼丹的事实标配之一。</p>
<h2 id="_1">方法大意</h2>
<p>在接入主题之前，必须先吐槽一下MuP原论文写得实在太过晦涩，并且结论的表达也不够清晰，平白增加了不少理解难度，所以接下来笔者尽量以一种（自认为）简明扼要的方式来复现MuP的结论。</p>
<p>先说结论，MuP主要研究超参数跨模型尺度的迁移规律。这里有几个关键词：</p>
<blockquote>
<p>1、超参数，目前主要指<strong>学习率</strong> ；</p>
<p>2、模型尺度，目前主要是模型<strong>宽度</strong> ；</p>
<p>3、这里的核心是“<strong>迁移</strong> ”。</p>
</blockquote>
<p>请注意，MuP不研究什么是最优的超参数，只研究最优超参数 <em>随着模型尺度的变化规律</em> ，所以我们需要在某个小模型上搜索最优的超参数组合，然后迁移到大模型上，这就是MuP的使用场景和使用方法。</p>
<p>推导MuP的原理是让模型的 <em>前向传播、反向传播、损失增量和特征变化</em> 都不随模型尺度的变化而发生明显变化：</p>
<blockquote>
<p>1、具体做法是分析初始化的数量级，然后认为结论可以代表后续优化的规律；</p>
<p>2、说白了就是假设做好初始化，后面就会自动沿着正确的轨迹走（好的开始是成功的一大半？）;</p>
<p>3、当然也可以给这个假设讲<strong>大数定律</strong> 或<strong>中心极限定理</strong> 的故事，但个人认为非必须。</p>
</blockquote>
<h2 id="_2">前向传播</h2>
<p>我们从前向传播开始讨论，因为这是相对简单且成熟的部分。首先，考虑线性层$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{W}$，其中$\boldsymbol{X}\in\mathbb{R}^{b\times d_{in}},\boldsymbol{W}\in\mathbb{R}^{d_{in}\times d_{out}}$。我们用RMS（Root Mean Square）来作为矩阵尺度的指标，例如<br />
\begin{equation}\text{RMS}(\boldsymbol{W}) = \sqrt{\frac{1}{d_{in} d_{out}}\sum_{i=1}^{d_{in}} \sum_{j=1}^{d_{out}} W_{i,j}^2}\end{equation}</p>
<p>我们知道，要让初始化阶段$\boldsymbol{X}$的RMS跟$\boldsymbol{Y}$的RMS大致相等（简称“<strong>稳定</strong> ”），那么$\boldsymbol{W}$要用：</p>
<blockquote>
<p><strong>LeCun初始化</strong> ：“均值为0、方差为$1/d_{in}$”的随机初始化。</p>
</blockquote>
<p>这已经算是深度学习的基础结论之一，所以不再展开推导，还不大了解的读者可以参考以往的<a href="/archives/7180">《从几何视角来理解模型参数的初始化策略》</a>、<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>等博文。</p>
<p>接着，我们考虑非线性层$\boldsymbol{Y}=\phi(\boldsymbol{X}\boldsymbol{W})$，其中$\phi$是Element-wise的激活函数。如果还是要维持$\boldsymbol{X}$的RMS跟$\boldsymbol{Y}$的RMS近似相等，那么结果会稍有不同，比如$\text{relu}$激活时我们得到</p>
<blockquote>
<p><strong>Kaiming初始化</strong> ：“均值为0、方差为$2/d_{in}$”的随机初始化。</p>
</blockquote>
<p>容易看出，<strong>Kaiming初始化</strong> 跟<strong>LeCun初始化</strong> 相比，只是方差相差一个（跟模型尺度无关的）常数2，可以证明其他激活函数的结果也类似。所以我们可以下一个结论：</p>
<blockquote>
<p><strong>fan_in初始化</strong> ：要保证前向传播的稳定性，那么应该要用“均值为0、方差 <em>正比于</em> $1/d_{in}$”的随机初始化。</p>
</blockquote>
<p>这个结论也可以理解为“激活函数的影响是模型尺度无关的”，所以如果我们只想分析模型尺度的效应，那么可以忽略（Element-wise的）激活函数的存在，由LeCun初始化直接得到缩放规律$\propto 1/d_{in}$。</p>
<h2 id="_3">反向传播</h2>
<p>现在我们继续分析反向传播（梯度），注意这里约定变量及其梯度具有相同的shape，那么可以算得<br />
\begin{align}<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}} =&amp;\, \boldsymbol{X}^{\top}\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\otimes \phi'(\boldsymbol{X}\boldsymbol{W})\right) \\[5pt]<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{X}} =&amp;\, \left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\otimes \phi'(\boldsymbol{X}\boldsymbol{W})\right)\boldsymbol{W}^{\top}<br />
\end{align}<br />
第一个公式是当前层内参数的梯度，第二个公式则是该层往前传播的梯度，$\otimes$是Hadamard积，$\phi'$是$\phi$的导函数。</p>
<p>注意到一个事实：我们常用的激活函数，其导数都可以被一个（尺度无关的）常数给Bound住，所以至少在数量级上我们可以写出<br />
\begin{align}<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}} =&amp;\, \boldsymbol{X}^{\top}\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\otimes \phi'(\boldsymbol{X}\boldsymbol{W})\right) \sim \boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}} \label{eq:grad-w}\\[5pt]<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{X}} =&amp;\, \left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\otimes \phi'(\boldsymbol{X}\boldsymbol{W})\right)\boldsymbol{W}^{\top}\sim \frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\boldsymbol{W}^{\top}\label{eq:grad-x}<br />
\end{align}<br />
我们先来看第二个公式，跟$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{W}$相比，它右端乘的矩阵变成了$\boldsymbol{W}^{\top}$，那么按照上一节的结论，如果要保持反向传播的RMS稳定性，那么$\boldsymbol{W}$的初始化就应该是：</p>
<blockquote>
<p><strong>fan_out初始化</strong> ：“均值为0、方差为$1/d_{out}$”的随机初始化。</p>
</blockquote>
<p>当$d_{in}\neq d_{out}$时，前向传播和反向传播的要求就出现冲突，这时候有人提了一个折中策略：</p>
<blockquote>
<p><strong>Xavier初始化</strong> ：“均值为0、方差为$2/(d_{in} + d_{out})$”的随机初始化。</p>
</blockquote>
<p>这也叫“<strong>fan_avg初始化</strong> ”，因为就是将$d_{in}$和$d_{out}$简单代数平均了一下，其他平均方式也可以考虑，参考<a href="/archives/8725">《初始化方法中非方阵的维度平均策略思考》</a>。Xavier初始化看上去同时兼顾了前向和反向，但也可以说两者都没兼顾，更好的办法是设计模型让大部分参数都是方阵，如后面讨论的模型簇$\eqref{eq:model}$。</p>
<h2 id="_4">损失增量</h2>
<p>有了前向传播和反向传播的铺垫，我们就可以尝试分析损失函数的增量了。考虑$\boldsymbol{W}\to \boldsymbol{W} + \Delta\boldsymbol{W}$时损失函数的变化量<br />
\begin{equation}\Delta \mathcal{L} = \mathcal{L}(\boldsymbol{W} + \Delta\boldsymbol{W}) - \mathcal{L}(\boldsymbol{W})\approx \left\langle\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}, \Delta\boldsymbol{W}\right\rangle_F\end{equation}<br />
这里的$\langle\cdot,\cdot\rangle_F$是Frobenius内积，即把矩阵展平成向量后算向量内积。考虑梯度下降$\Delta\boldsymbol{W} = -\eta \frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}$，这里$\eta$自然是学习率，结合式$\eqref{eq:grad-w}$，我们有<br />
$$\begin{equation}\Delta \mathcal{L}\approx -\eta\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right\Vert_F^2\sim -\eta \left\Vert\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\right\Vert_F^2\end{equation}$$<br />
事实上，这个式子已经告诉了我们同一个学习率$\eta$不能跨模型尺度使用的原因：</p>
<blockquote>
<p>1、$\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$是一个$d_{in}\times d_{out}$的矩阵；</p>
<p>2、$\left\Vert\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\right\Vert_F^2$是$d_{in}\times d_{out}$个数的平方和；</p>
<p>3、$\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$正好是前向和反向的乘积；</p>
<p>4、如果前向和反向都稳定，那么$\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$每个元素都是$\mathcal{\Theta}(1)$（$\mathcal{\Theta}$是“<a href="https://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann%E2%80%93Landau_notations">Big Theta Notation</a>”）；</p>
<p>5、所以$\left\Vert\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\right\Vert_F^2$就是$\mathcal{\Theta}(d_{in} d_{out})$。</p>
</blockquote>
<p>第4点可能要多加评述一下。$\boldsymbol{X}^{\top}$是一个$d_{in}\times b$矩阵，$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$是一个$b\times d_{out}$矩阵，两者相乘就是$d_{in} d_{out}$个$b$维向量对做内积，内积是$b$项求和，而损失$\mathcal{L}$通常是对样本求平均（即包含了除以$b$操作），所以如果$\boldsymbol{X}^{\top}$和$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$都是尺度无关的，那么它们乘起来基本也是尺度无关的【即RMS都是$\mathcal{\Theta}(1)$】。</p>
<p>最后的结论表明，如果我们直接将小模型的学习率用于大模型，那么对于足够大的模型，它的每一步损失增量就会随着参数尺度（即$d_{in} d_{out}$）的变大而 <em><strong>爆炸</strong></em> ，这意味着没法复制小模型的收敛过程，甚至可能因为步子迈得太大导致无法收敛。</p>
<p>此时大家可能想到的一个做法是让$\eta\propto 1/(d_{in} d_{out})$来缩放$\Delta\mathcal{L}$，事实上这个想法已经跟上了MuP的思路，但实际场景中由于前面说的前向和反向的不兼容性，导致第4点“如果前向和反向都稳定，那么$\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$每个元素就是$\mathcal{\Theta}(1)$”不能总是成立，所以实际情况更为复杂一些，</p>
<h2 id="_5">模型假设</h2>
<p>现在让我们考虑一个更接近实践的场景。我们的任务是训练一个$\mathbb{R}^{d_{in}}\mapsto \mathbb{R}^{d_{out}}$的模型，其中$d_{in},d_{out}$是数据决定的，不可改变。开头我们就说了，MuP旨在研究超参数随着模型尺度的缩放规律，所以一切固定不变的量，都相当于是常数或者说$\mathcal{\Theta}(1)$，比如初始化方差为$1/d_{in}$，等价于说初始化方差为$\mathcal{\Theta}(1)$。</p>
<p>我们可以改变的是模型的架构、参数量等部分，但MuP主要考虑宽度的规律，所以我们把模型的架构定一下。这里主要考虑的模型簇是：<br />
\begin{equation}\begin{gathered}<br />
\boldsymbol{Y}<em in="in">{in} = \boldsymbol{X} \boldsymbol{W}</em> \\[5pt]<br />
\boldsymbol{Y}<em in="in">{out} = \text{NN}(\boldsymbol{Y}</em>) \\[5pt]},\boldsymbol{\Omega<br />
\boldsymbol{Z} = \boldsymbol{Y}<em out="out">{out} \boldsymbol{W}</em><br />
\end{gathered}\label{eq:model}\end{equation}</p>
<p>其中：</p>
<blockquote>
<p>1、$\boldsymbol{X}\in\mathbb{R}^{b\times d_{in}}$（带上了batch size）；</p>
<p>2、$\boldsymbol{W}<em in="in">{in} \in \mathbb{R}^{d</em>}\times d}, \boldsymbol{W<em out="out">{out} \in \mathbb{R}^{d\times d</em>$；}</p>
<p>3、$\text{NN}$是任意$\mathbb{R}^d\mapsto \mathbb{R}^d$的神经网络；</p>
<p>4、这里$d$其实就是我们常说的hidden size；</p>
<p>5、我们可以随意调大$d$，来提升模型的参数量和潜力；</p>
<p>6、MuP就是想研究超参数关于$d$的变化规律。</p>
</blockquote>
<p>更具体一点，这里我们考虑的$\text{NN}$是$K$层MLP：<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{Y}<em in="in">0 =&amp;\, Y</em> \\[5pt]<br />
\boldsymbol{Y}<em k_1="k+1">{k+1} =&amp;\, \phi(\boldsymbol{Y}_k \boldsymbol{W}</em>) \\[5pt]<br />
\boldsymbol{Y}_{out} =&amp;\, \boldsymbol{Y}_K<br />
\end{aligned}\end{equation}<br />
这里$\boldsymbol{\Omega}=\{\boldsymbol{W}_1,\boldsymbol{W}_2,\cdots,\boldsymbol{W}_K\}$，$\boldsymbol{W}_k\in\mathbb{R}^{d\times d}$，即都是$d\times d$的<strong>方阵</strong> ，全都用<strong>fan_in初始化</strong> （等价地，也是<strong>fan_out初始化</strong> ）。</p>
<p>补充一下，这里约定所有参数矩阵都是$d\times d$方阵，纯粹是为了简化分析，并不是强制要求。因为这里真正的目的是假设$\text{NN}$的参数里 <em>没有尺度无关的形状</em> ，比如不允许$d\times 64$这样的形状，因为$64$是一个常数，但$d\times 4d$这样的形状是允许的，因为你不管fan_in、fan_out或fan_avg初始化，方差都是正比于$1/d$。</p>
<h2 id="_6">组装起来</h2>
<p>确立后具体模型后，我们就可以把前面的结论都组装起来了。要更新的参数分为$\boldsymbol{W}<em out="out">{in},\boldsymbol{\Omega},\boldsymbol{W}</em>$三部分，分别求梯度：<br />
\begin{align}<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">{out}} =&amp;\, \boldsymbol{Y}</em> \\[6pt]}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">k} =&amp;\, \frac{\partial \boldsymbol{Y}</em>}}{\partial \boldsymbol{W<em out="out">k} \cdot\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}</em>}} = \frac{\partial \boldsymbol{Y<em out="out">{out}}{\partial \boldsymbol{W}_k} \cdot\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}\boldsymbol{W}</em>\right) \\[6pt]}^{\top<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em in="in">{in}} =&amp;\, \boldsymbol{X}^{\top} \frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}</em>}} = \boldsymbol{X}^{\top} \left(\frac{\partial\boldsymbol{Y<em in="in">{out}}{\partial \boldsymbol{Y}</em>}}\cdot\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y<em out="out">{out}}\right) = \boldsymbol{X}^{\top} \left(\frac{\partial\boldsymbol{Y}</em>}}{\partial \boldsymbol{Y<em out="out">{in}}\cdot\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}\boldsymbol{W}</em>\right)\right) \\[6pt]}^{\top<br />
\end{align}</p>
<p>这里的$\cdot$运算需要稍微解释一下：$\boldsymbol{Y}<em out="out">{in},\boldsymbol{Y}</em>}$都是一个矩阵，所以$\frac{\partial\boldsymbol{Y<em in="in">{out}}{\partial \boldsymbol{Y}</em>}}$原则上是一个四阶张量，链式法则$\frac{\partial\boldsymbol{Y<em in="in">{out}}{\partial \boldsymbol{Y}</em>$实际是高阶张量的乘法，但这里不打算展开介绍了，所以简单用一个$\cdot$代替，读者只需要知道它是矩阵乘法的一般推广就行。}}\cdot\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}_{out}</p>
<p>现在来观察规律：</p>
<blockquote>
<p>1、三个式子都有$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}$；</p>
<p>2、后两式都有$\boldsymbol{W}_{out}^{\top}$；</p>
<p>3、$\boldsymbol{W}<em out="out">k$里都是方阵，$\frac{\partial\boldsymbol{Y}</em>}}{\partial \boldsymbol{Y<em out="out">{in}}$和$\frac{\partial \boldsymbol{Y}</em>(1)$】；}}{\partial \boldsymbol{W}_k}$都是稳定的【RMS是$\mathcal{\Theta</p>
<p>4、如果$\boldsymbol{W}<em out="out">{in}$也用fan_in初始化，那么$\boldsymbol{Y}</em>$也是稳定的；</p>
<p>5、要想$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}\boldsymbol{W}<em out="out">{out}^{\top}$稳定，那么初始化方差是$1/d</em>$是尺度无关的，相当于常数。}$，但$d_{out</p>
</blockquote>
<p>这样一来：</p>
<blockquote>
<p>1、$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">{out}}$的RMS是$\mathcal{\Theta}(1)$，$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>\propto 1/d$；}}\right\Vert_F^2$是$d\times d_{out}$个数平方和，所以大小是$\mathcal{\Theta}(d\times d_{out})$，别忘了$d_{out}$是常数，所以实际上就是$\mathcal{\Theta}(d)$，于是为了得到$\mathcal{\Theta}(1)$的$\Delta\mathcal{L}$，它的学习率要满足$\eta_{out</p>
<p>2、$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">k}\right\Vert_F^2$是$d^2$个数求和，$\frac{\partial \boldsymbol{Y}</em>}}{\partial \boldsymbol{W<em out="out">k}$和$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}$的RMS都是$\mathcal{\Theta}(1)$，我们直接将$\boldsymbol{W}</em>(1)$，因此学习率不用变化；}$的初始化方差设为$\propto 1/d^2$，那么$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$的RMS就是$\mathcal{\Theta}(1/d)$，平方求和后就正好是$\mathcal{\Theta</p>
<p>3、此时$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em in="in">{in}}$的RMS也是$\mathcal{\Theta}(1/d)$，但$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>\propto d$。}}\right\Vert_F^2$只是$d_{in}\times d$个数平方和，所以结果是$\mathcal{\Theta}(1/d)$的，为了得到$\mathcal{\Theta}(1)$的$\Delta\mathcal{L}$，学习率反而需要放大$d$倍来抵消这个影响，即$\eta_{in</p>
</blockquote>
<h2 id="_7">特征变化</h2>
<p>以上结果是没有问题的，但仔细思考我们会发现推导过程的一个问题：上面的第2、3点，都建立在“我们直接将$\boldsymbol{W}_{out}$的初始化方差设为$\propto 1/d^2$”这个设置上，然而这个设置目前来说并没有直接的依据。如果不对此进一步解释，那么推导过程还是不够完备的。</p>
<p>事实上，单看$\Delta \mathcal{L}=\mathcal{\Theta}(1)$这个要求的话，确实是无法排除其他选择的可能性的，比如$\boldsymbol{W}<em out="out">{out}$的初始化方差设为$\propto 1/d$，此时$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$的RMS是$\mathcal{\Theta}(1/\sqrt{d})$，平方求和后是$\mathcal{\Theta}(d)$，那么只要学习率$\eta\propto 1/d$同样可以实现$\Delta \mathcal{L}=\mathcal{\Theta}(1)$。因此，为了解释“$\boldsymbol{W}</em>$的初始化方差设为$\propto 1/d^2$”的必要性，那么就需要引入新的条件。</p>
<p>损失函数$\mathcal{L}$是模型的一个宏观指标，或者说外部指标，单看它的变化已经不足以解释全部结果了，那么就需要细化到模型内部了。具体来说，我们希望模型每一层的输出（通常也称为特征，有时也称激活值）变化量也具有尺度不变性。比如线性层$\boldsymbol{Y}<em k-1="k-1">k = \boldsymbol{Y}</em>} \boldsymbol{W<em k-1="k-1">k$，参数$\boldsymbol{W}_k\to \boldsymbol{W}_k + \Delta \boldsymbol{W}_k$带来的输出变化是<br />
\begin{equation}\Delta\boldsymbol{Y}_k = \boldsymbol{Y}</em>} (\boldsymbol{W<em k-1="k-1">k + \Delta \boldsymbol{W}_k) - \boldsymbol{Y}</em>} \boldsymbol{W<em k-1="k-1">k = \boldsymbol{Y}</em>} \Delta\boldsymbol{W<em k-1="k-1">k\end{equation}<br />
注意$\boldsymbol{Y}</em>}\in\mathbb{R}^{b\times d},\Delta\boldsymbol{W<em k-1="k-1">k\in\mathbb{R}^{d\times d}$，所以$\boldsymbol{Y}</em>} \Delta\boldsymbol{W<em k-1="k-1">k$就是$b\times d$个$d$维向量对的内积。注意这里$\Delta\boldsymbol{W}_k$是精心设计的更新量，它不大可能跟初始化那样跟$\boldsymbol{Y}</em>_k))$。}$是独立的，所以“$d$维向量对的内积”更有可能是$\mathcal{\Theta}(d)$（$d$维内积共有$d$项求和），因此如果$\Delta\boldsymbol{Y}_{k-1}$的RMS是$\mathcal{\Theta}(1)$，那么可以认为$\Delta\boldsymbol{Y}_k$的RMS将是$\mathcal{\Theta}(d\times \text{RMS}(\Delta \boldsymbol{W</p>
<p>于是，为了让$\Delta\boldsymbol{Y}_k$的RMS是$\mathcal{\Theta}(1)$，我们得到了对$\Delta \boldsymbol{W}_k$的一个额外要求：<br />
\begin{equation}\text{RMS}(\Delta \boldsymbol{W}_k) = \mathcal{\Theta}(1 / d)\label{eq:dw-rms}\end{equation}</p>
<p>结合$\Delta \boldsymbol{W}<em out="out">k = -\eta\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$和$\Delta\mathcal{L}=\mathcal{\Theta}(1)$，我们就可以得到“$\boldsymbol{W}</em>$的初始化方差设为$\propto 1/d^2$”的结果。</p>
<p>（注：这一节依赖于 <a href="/archives/10770/comment-page-1#comment-27212">@Chenyu Zheng</a> 的指点，非常感谢！）</p>
<h2 id="adam">Adam版本</h2>
<p>以上就是SGD的MuP，对于Adam，我们通常用SignSGD近似做数量级分析：</p>
<blockquote>
<p>1、$\Delta \boldsymbol{W} = -\eta \mathop{\text{sign}}\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right)$；</p>
<p>2、$\Delta \mathcal{L} \approx -\eta \left|\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right|_1$；</p>
<p>3、这里的$|\cdot|_1$指每个元素取绝对值然后求和。</p>
</blockquote>
<p>关于SignSGD近似本身，读者还可以参考<a href="/archives/10542">《当Batch Size增大时，学习率该如何随之变化？》</a>、<a href="/archives/10563">《Adam的epsilon如何影响学习率的Scaling Law？》</a>等文章，这里也不展开讨论了。总而言之，SignSGD是分析Adam相关缩放规律时一个常用的近似方式。</p>
<p>现在可以模仿SGD的过程进行分析：</p>
<blockquote>
<p>1、$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">{out}}$的RMS是$\mathcal{\Theta}(1)$，$\left|\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>\right|}<em out="out">1$是$d\times d</em>\propto 1/d$来抵消尺度影响；}$个数求和，大小是$\mathcal{\Theta}(d\times d_{out}) = \mathcal{\Theta}(d)$，所以它的学习率要满足$\eta_{out</p>
<p>2、$\left|\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">k}\right|_1$是$d^2$个数求和，$\frac{\partial \boldsymbol{Y}</em>}}{\partial \boldsymbol{W<em out="out">k}$和$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}$的RMS都是$\mathcal{\Theta}(1)$，我们将$\boldsymbol{W}</em>(d)$，所以学习率按照$\eta_k\propto 1/d$变换来抵消尺度影响；}$的初始方差设为$\propto 1/d^2$，那么$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$的RMS就是$\mathcal{\Theta}(1/d)$，$d^2$个数求和后是$\mathcal{\Theta</p>
<p>3、此时$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em in="in">{in}}$的RMS也是$\mathcal{\Theta}(1/d)$，但$\left|\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>\right|}<em in="in">1$只是$d</em>(1)$，从而学习率不用随尺度改变。}\times d$个数求和，所以它已经是$\mathcal{\Theta</p>
</blockquote>
<p>（注：读者可以自行检查一下式$\eqref{eq:dw-rms}$是满足的。）</p>
<h2 id="muon">Muon版本</h2>
<p>接下来自然少不了Muon的分析。对于Muon本身，我们已经在<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>、<a href="/archives/10739">《Muon续集：为什么我们选择尝试Muon？》</a>做了详细介绍，这里不再重复。跟Adam用SignSGD类似，我们用MSignSGD来近似Muon：</p>
<blockquote>
<p>1、$\Delta \boldsymbol{W} = -\eta \mathop{\text{msign}}\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right)$；</p>
<p>2、$\Delta \mathcal{L} \approx -\eta \left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right\Vert_*$（证明见<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>）；</p>
<p>3、这里的$\Vert\cdot\Vert_*$指<a href="https://en.wikipedia.org/wiki/Nuclear_norm">Nuclear范数</a>，是矩阵的 <em>所有奇异值之和</em> ；</p>
<p>4、Nuclear范数并不好算，但$F$范数好算，它等于矩阵的 <em>所有奇异值的平方和的平方根</em> ；</p>
<p>5、我们用$F$范数作为Nuclear范数近似，因此$\Delta \mathcal{L} \approx -\eta \left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right\Vert_*\approx -\eta \left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right\Vert_F$；</p>
<p>6、$F$范数又等于矩阵的 <em>所有元素的平方和的平方根</em> 。</p>
</blockquote>
<p>那么可以开始分析过程：</p>
<blockquote>
<p>1、$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">{out}}$的RMS是$\mathcal{\Theta}(1)$，所以$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>$；}}\right\Vert_*$大小是$\mathcal{\Theta}(\sqrt{d\times d_{out}}) = \mathcal{\Theta}(\sqrt{d})$，要消除尺度的影响，那么它的学习率要满足$\eta_{out}\propto 1/\sqrt{d</p>
<p>2、$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">k}\right\Vert_F$是$d^2$个数的平方和的平方根，$\frac{\partial \boldsymbol{Y}</em>}}{\partial \boldsymbol{W<em out="out">k}$和$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}$的RMS都是$\mathcal{\Theta}(1)$，我们将$\boldsymbol{W}</em>(1)$，所以学习率不用变；}$的初始方差设为$\propto 1/d^2$，那么$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$的RMS就是$\mathcal{\Theta}(1/d)$，平方和后再平方根，结果是$\mathcal{\Theta</p>
<p>3、此时$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em in="in">{in}}$的RMS也是$\mathcal{\Theta}(1/d)$，但$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>$。}}\right\Vert_F$只是$d_{in}\times d$个数的平方和平方根，所以它是$\mathcal{\Theta}(1/\sqrt{d})$的，学习率反而需要放大$\sqrt{d}$倍来抵消这个影响，即$\eta_{in}\propto \sqrt{d</p>
</blockquote>
<p>（注：这里Muon的结论是对的，但它不满足条件$\eqref{eq:dw-rms}$，因为式$\eqref{eq:dw-rms}$要细说的话还依赖于一个更新量是Element-wise的假设，而Muon不符合这个假设，所以实际上不可用。这里没有仔细展开相关讨论，而是直接沿用了“$\boldsymbol{W}_{out}$的初始化方差设为$\propto 1/d^2$”的结论，回避了式$\eqref{eq:dw-rms}$。）</p>
<h2 id="_8">结论汇总</h2>
<p>将上述结论汇总在一起是：<br />
\begin{array}{c|c|c|c|c|c|c}<br />
\hline<br />
&amp; \boldsymbol{W}<em in="in">{in}\text{方差} &amp; \boldsymbol{W}</em>}\text{学习率} &amp; \boldsymbol{W<em out="out">k\text{方差} &amp; \boldsymbol{W}_k\text{学习率} &amp; \boldsymbol{W}</em>}\text{方差} &amp; \boldsymbol{W<em in="in">{out}\text{学习率} \\<br />
\hline<br />
\text{SGD} &amp; 1/d</em> &amp; d &amp; 1 / d &amp; 1 &amp; 1/d^2 &amp; 1 / d\\<br />
\text{Adam} &amp; 1/d_{in} &amp; 1 &amp; 1 / d &amp; 1 / d &amp; 1/d^2 &amp; 1 / d\\<br />
\text{Muon} &amp; 1/d_{in} &amp; \sqrt{d} &amp; 1 / d &amp; 1 &amp; 1/d^2 &amp; 1 / \sqrt{d} \\<br />
\hline<br />
\end{array}</p>
<p>这里的$\boldsymbol{W}<em in="in">k$指的是除$\boldsymbol{W}</em>},\boldsymbol{W<em in="in">{out}$外的所有参数，还有要强调的是，这里的关系都是“正比于”而不是“等于”。另外实践中可以根据具体需求稍作变化，比如实际我们用Muon时，$\boldsymbol{W}</em>$的优化通常不用Muon而是用Adam，这将导致两个变化：}$和$\boldsymbol{W}_{out</p>
<blockquote>
<p>1、$\eta_{out}\propto 1/d$；</p>
<p>2、$\eta_{in}$不变。</p>
</blockquote>
<p>如果结合我们在<a href="https://papers.cool/arxiv/2502.16982">《Muon is Scalable for LLM Training》</a>所提的Adujst LR的话，那么学习率要多乘一个$\sqrt{\max(n, m)}$，$n\times m$是参数矩阵的形状，我们已经假设了$\text{NN}$部分的参数总等比例缩放，所以$\sqrt{\max(n, m)}\propto \sqrt{d}$。因此，如果要抵消Adujst LR带来的尺度影响，那么就需要</p>
<blockquote>
<p>3、$\eta_k\propto 1/\sqrt{d}$ 。</p>
</blockquote>
<h2 id="_9">文章小结</h2>
<p>本文以尽可能简明清晰的方式介绍了MuP（Maximal Update Parametrization），这是旨在研究超参数跨模型尺度的迁移规律的工作。基于MuP，我们可以在小模型上以相对较小的成本仔细搜索超参数（这里主要是学习率和初始化），然后迁移到大模型上，降低大模型的炼丹成本。</p>
<p>客观来讲，这里的介绍和分析还比较初步，比如没有考虑Bias项、没有评估结论在MLP以外架构的通用性、也没有仔细考虑Normalization和残差的作用等。没有考虑Bias项这个单纯是偷懒，权当留给读者的习题了；至于不同架构下的MuP，一般分析起来比较麻烦，但由于神经网络的相似性，结论大致上是相同的，我们可以不加证明地用着。个人认为比较关键的改进点是Normalization和残差的影响，尤其是Normalization，它使得不依赖特殊的初始化就可以稳定前向传播，带来了更大的自由度和可能性。</p>
<p>当然，这些都留给后续分析了。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10770">https://spaces.ac.cn/archives/10770</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Mar. 13, 2025). 《初探MuP：超参数的跨模型尺度迁移规律 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10770">https://spaces.ac.cn/archives/10770</a></p>
<p>@online{kexuefm-10770,<br />
title={初探MuP：超参数的跨模型尺度迁移规律},<br />
author={苏剑林},<br />
year={2025},<br />
month={Mar},<br />
url={\url{https://spaces.ac.cn/archives/10770}},<br />
} </p>
<hr />
<h2 id="_10">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>

        <!-- Back to Home -->
        <div class="text-center mt-5 mb-4">
            <a href="../index.html" class="btn btn-outline-primary">
                <i class="fas fa-arrow-left"></i> 返回首页
            </a>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>
</body>
</html>
