<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>初探MuP：超参数的跨模型尺度迁移规律 | ML & Math Blog Posts</title>
    <meta name="description" content="初探MuP：超参数的跨模型尺度迁移规律&para;
原文链接: https://spaces.ac.cn/archives/10770
发布日期: 

众所周知，完整训练一次大型LLM的成本是昂贵的，这就决定了我们不可能直接在大型LLM上反复测试超参数。一个很自然的想法是希望可以在同结构的小模型上仔细搜索超参数，找到最优组合后直接迁移到大模型上。尽管这个想法很朴素，但要实现它并不平凡，它需要我们了...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #186 初探MuP：超参数的跨模型尺度迁移规律
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#186</span>
                初探MuP：超参数的跨模型尺度迁移规律
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-03-13</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=学习率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 学习率</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=尺度定律" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 尺度定律</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="mup">初探MuP：超参数的跨模型尺度迁移规律<a class="toc-link" href="#mup" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10770">https://spaces.ac.cn/archives/10770</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，完整训练一次大型LLM的成本是昂贵的，这就决定了我们不可能直接在大型LLM上反复测试超参数。一个很自然的想法是希望可以在同结构的小模型上仔细搜索超参数，找到最优组合后直接迁移到大模型上。尽管这个想法很朴素，但要实现它并不平凡，它需要我们了解常见的超参数与模型尺度之间的缩放规律，而MuP正是这个想法的一个实践。</p>
<p>MuP，有时也写$\mu P$，全名是Maximal Update Parametrization，出自论文<a href="https://papers.cool/arxiv/2203.03466">《Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer》</a>，随着LLM训练的普及，它逐渐已经成为了科学炼丹的事实标配之一。</p>
<h2 id="_1">方法大意<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>在接入主题之前，必须先吐槽一下MuP原论文写得实在太过晦涩，并且结论的表达也不够清晰，平白增加了不少理解难度，所以接下来笔者尽量以一种（自认为）简明扼要的方式来复现MuP的结论。</p>
<p>先说结论，MuP主要研究超参数跨模型尺度的迁移规律。这里有几个关键词：</p>
<blockquote>
<p>1、超参数，目前主要指<strong>学习率</strong> ；</p>
<p>2、模型尺度，目前主要是模型<strong>宽度</strong> ；</p>
<p>3、这里的核心是“<strong>迁移</strong> ”。</p>
</blockquote>
<p>请注意，MuP不研究什么是最优的超参数，只研究最优超参数 <em>随着模型尺度的变化规律</em> ，所以我们需要在某个小模型上搜索最优的超参数组合，然后迁移到大模型上，这就是MuP的使用场景和使用方法。</p>
<p>推导MuP的原理是让模型的 <em>前向传播、反向传播、损失增量和特征变化</em> 都不随模型尺度的变化而发生明显变化：</p>
<blockquote>
<p>1、具体做法是分析初始化的数量级，然后认为结论可以代表后续优化的规律；</p>
<p>2、说白了就是假设做好初始化，后面就会自动沿着正确的轨迹走（好的开始是成功的一大半？）;</p>
<p>3、当然也可以给这个假设讲<strong>大数定律</strong> 或<strong>中心极限定理</strong> 的故事，但个人认为非必须。</p>
</blockquote>
<h2 id="_2">前向传播<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>我们从前向传播开始讨论，因为这是相对简单且成熟的部分。首先，考虑线性层$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{W}$，其中$\boldsymbol{X}\in\mathbb{R}^{b\times d_{in}},\boldsymbol{W}\in\mathbb{R}^{d_{in}\times d_{out}}$。我们用RMS（Root Mean Square）来作为矩阵尺度的指标，例如<br />
\begin{equation}\text{RMS}(\boldsymbol{W}) = \sqrt{\frac{1}{d_{in} d_{out}}\sum_{i=1}^{d_{in}} \sum_{j=1}^{d_{out}} W_{i,j}^2}\end{equation}</p>
<p>我们知道，要让初始化阶段$\boldsymbol{X}$的RMS跟$\boldsymbol{Y}$的RMS大致相等（简称“<strong>稳定</strong> ”），那么$\boldsymbol{W}$要用：</p>
<blockquote>
<p><strong>LeCun初始化</strong> ：“均值为0、方差为$1/d_{in}$”的随机初始化。</p>
</blockquote>
<p>这已经算是深度学习的基础结论之一，所以不再展开推导，还不大了解的读者可以参考以往的<a href="/archives/7180">《从几何视角来理解模型参数的初始化策略》</a>、<a href="/archives/8620">《浅谈Transformer的初始化、参数化与标准化》</a>等博文。</p>
<p>接着，我们考虑非线性层$\boldsymbol{Y}=\phi(\boldsymbol{X}\boldsymbol{W})$，其中$\phi$是Element-wise的激活函数。如果还是要维持$\boldsymbol{X}$的RMS跟$\boldsymbol{Y}$的RMS近似相等，那么结果会稍有不同，比如$\text{relu}$激活时我们得到</p>
<blockquote>
<p><strong>Kaiming初始化</strong> ：“均值为0、方差为$2/d_{in}$”的随机初始化。</p>
</blockquote>
<p>容易看出，<strong>Kaiming初始化</strong> 跟<strong>LeCun初始化</strong> 相比，只是方差相差一个（跟模型尺度无关的）常数2，可以证明其他激活函数的结果也类似。所以我们可以下一个结论：</p>
<blockquote>
<p><strong>fan_in初始化</strong> ：要保证前向传播的稳定性，那么应该要用“均值为0、方差 <em>正比于</em> $1/d_{in}$”的随机初始化。</p>
</blockquote>
<p>这个结论也可以理解为“激活函数的影响是模型尺度无关的”，所以如果我们只想分析模型尺度的效应，那么可以忽略（Element-wise的）激活函数的存在，由LeCun初始化直接得到缩放规律$\propto 1/d_{in}$。</p>
<h2 id="_3">反向传播<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>现在我们继续分析反向传播（梯度），注意这里约定变量及其梯度具有相同的shape，那么可以算得<br />
\begin{align}<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}} =&amp;\, \boldsymbol{X}^{\top}\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\otimes \phi'(\boldsymbol{X}\boldsymbol{W})\right) \\[5pt]<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{X}} =&amp;\, \left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\otimes \phi'(\boldsymbol{X}\boldsymbol{W})\right)\boldsymbol{W}^{\top}<br />
\end{align}<br />
第一个公式是当前层内参数的梯度，第二个公式则是该层往前传播的梯度，$\otimes$是Hadamard积，$\phi'$是$\phi$的导函数。</p>
<p>注意到一个事实：我们常用的激活函数，其导数都可以被一个（尺度无关的）常数给Bound住，所以至少在数量级上我们可以写出<br />
\begin{align}<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}} =&amp;\, \boldsymbol{X}^{\top}\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\otimes \phi'(\boldsymbol{X}\boldsymbol{W})\right) \sim \boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}} \label{eq:grad-w}\\[5pt]<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{X}} =&amp;\, \left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\otimes \phi'(\boldsymbol{X}\boldsymbol{W})\right)\boldsymbol{W}^{\top}\sim \frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\boldsymbol{W}^{\top}\label{eq:grad-x}<br />
\end{align}<br />
我们先来看第二个公式，跟$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{W}$相比，它右端乘的矩阵变成了$\boldsymbol{W}^{\top}$，那么按照上一节的结论，如果要保持反向传播的RMS稳定性，那么$\boldsymbol{W}$的初始化就应该是：</p>
<blockquote>
<p><strong>fan_out初始化</strong> ：“均值为0、方差为$1/d_{out}$”的随机初始化。</p>
</blockquote>
<p>当$d_{in}\neq d_{out}$时，前向传播和反向传播的要求就出现冲突，这时候有人提了一个折中策略：</p>
<blockquote>
<p><strong>Xavier初始化</strong> ：“均值为0、方差为$2/(d_{in} + d_{out})$”的随机初始化。</p>
</blockquote>
<p>这也叫“<strong>fan_avg初始化</strong> ”，因为就是将$d_{in}$和$d_{out}$简单代数平均了一下，其他平均方式也可以考虑，参考<a href="/archives/8725">《初始化方法中非方阵的维度平均策略思考》</a>。Xavier初始化看上去同时兼顾了前向和反向，但也可以说两者都没兼顾，更好的办法是设计模型让大部分参数都是方阵，如后面讨论的模型簇$\eqref{eq:model}$。</p>
<h2 id="_4">损失增量<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>有了前向传播和反向传播的铺垫，我们就可以尝试分析损失函数的增量了。考虑$\boldsymbol{W}\to \boldsymbol{W} + \Delta\boldsymbol{W}$时损失函数的变化量<br />
\begin{equation}\Delta \mathcal{L} = \mathcal{L}(\boldsymbol{W} + \Delta\boldsymbol{W}) - \mathcal{L}(\boldsymbol{W})\approx \left\langle\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}, \Delta\boldsymbol{W}\right\rangle_F\end{equation}<br />
这里的$\langle\cdot,\cdot\rangle_F$是Frobenius内积，即把矩阵展平成向量后算向量内积。考虑梯度下降$\Delta\boldsymbol{W} = -\eta \frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}$，这里$\eta$自然是学习率，结合式$\eqref{eq:grad-w}$，我们有<br />
$$\begin{equation}\Delta \mathcal{L}\approx -\eta\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right\Vert_F^2\sim -\eta \left\Vert\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\right\Vert_F^2\end{equation}$$<br />
事实上，这个式子已经告诉了我们同一个学习率$\eta$不能跨模型尺度使用的原因：</p>
<blockquote>
<p>1、$\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$是一个$d_{in}\times d_{out}$的矩阵；</p>
<p>2、$\left\Vert\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\right\Vert_F^2$是$d_{in}\times d_{out}$个数的平方和；</p>
<p>3、$\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$正好是前向和反向的乘积；</p>
<p>4、如果前向和反向都稳定，那么$\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$每个元素都是$\mathcal{\Theta}(1)$（$\mathcal{\Theta}$是“<a href="https://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann%E2%80%93Landau_notations">Big Theta Notation</a>”）；</p>
<p>5、所以$\left\Vert\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}\right\Vert_F^2$就是$\mathcal{\Theta}(d_{in} d_{out})$。</p>
</blockquote>
<p>第4点可能要多加评述一下。$\boldsymbol{X}^{\top}$是一个$d_{in}\times b$矩阵，$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$是一个$b\times d_{out}$矩阵，两者相乘就是$d_{in} d_{out}$个$b$维向量对做内积，内积是$b$项求和，而损失$\mathcal{L}$通常是对样本求平均（即包含了除以$b$操作），所以如果$\boldsymbol{X}^{\top}$和$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$都是尺度无关的，那么它们乘起来基本也是尺度无关的【即RMS都是$\mathcal{\Theta}(1)$】。</p>
<p>最后的结论表明，如果我们直接将小模型的学习率用于大模型，那么对于足够大的模型，它的每一步损失增量就会随着参数尺度（即$d_{in} d_{out}$）的变大而 <em><strong>爆炸</strong></em> ，这意味着没法复制小模型的收敛过程，甚至可能因为步子迈得太大导致无法收敛。</p>
<p>此时大家可能想到的一个做法是让$\eta\propto 1/(d_{in} d_{out})$来缩放$\Delta\mathcal{L}$，事实上这个想法已经跟上了MuP的思路，但实际场景中由于前面说的前向和反向的不兼容性，导致第4点“如果前向和反向都稳定，那么$\boldsymbol{X}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}}$每个元素就是$\mathcal{\Theta}(1)$”不能总是成立，所以实际情况更为复杂一些，</p>
<h2 id="_5">模型假设<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>现在让我们考虑一个更接近实践的场景。我们的任务是训练一个$\mathbb{R}^{d_{in}}\mapsto \mathbb{R}^{d_{out}}$的模型，其中$d_{in},d_{out}$是数据决定的，不可改变。开头我们就说了，MuP旨在研究超参数随着模型尺度的缩放规律，所以一切固定不变的量，都相当于是常数或者说$\mathcal{\Theta}(1)$，比如初始化方差为$1/d_{in}$，等价于说初始化方差为$\mathcal{\Theta}(1)$。</p>
<p>我们可以改变的是模型的架构、参数量等部分，但MuP主要考虑宽度的规律，所以我们把模型的架构定一下。这里主要考虑的模型簇是：<br />
\begin{equation}\begin{gathered}<br />
\boldsymbol{Y}<em in="in">{in} = \boldsymbol{X} \boldsymbol{W}</em> \\[5pt]<br />
\boldsymbol{Y}<em in="in">{out} = \text{NN}(\boldsymbol{Y}</em>) \\[5pt]},\boldsymbol{\Omega<br />
\boldsymbol{Z} = \boldsymbol{Y}<em out="out">{out} \boldsymbol{W}</em><br />
\end{gathered}\label{eq:model}\end{equation}</p>
<p>其中：</p>
<blockquote>
<p>1、$\boldsymbol{X}\in\mathbb{R}^{b\times d_{in}}$（带上了batch size）；</p>
<p>2、$\boldsymbol{W}<em in="in">{in} \in \mathbb{R}^{d</em>}\times d}, \boldsymbol{W<em out="out">{out} \in \mathbb{R}^{d\times d</em>$；}</p>
<p>3、$\text{NN}$是任意$\mathbb{R}^d\mapsto \mathbb{R}^d$的神经网络；</p>
<p>4、这里$d$其实就是我们常说的hidden size；</p>
<p>5、我们可以随意调大$d$，来提升模型的参数量和潜力；</p>
<p>6、MuP就是想研究超参数关于$d$的变化规律。</p>
</blockquote>
<p>更具体一点，这里我们考虑的$\text{NN}$是$K$层MLP：<br />
\begin{equation}\begin{aligned}<br />
\boldsymbol{Y}<em in="in">0 =&amp;\, Y</em> \\[5pt]<br />
\boldsymbol{Y}<em k_1="k+1">{k+1} =&amp;\, \phi(\boldsymbol{Y}_k \boldsymbol{W}</em>) \\[5pt]<br />
\boldsymbol{Y}_{out} =&amp;\, \boldsymbol{Y}_K<br />
\end{aligned}\end{equation}<br />
这里$\boldsymbol{\Omega}=\{\boldsymbol{W}_1,\boldsymbol{W}_2,\cdots,\boldsymbol{W}_K\}$，$\boldsymbol{W}_k\in\mathbb{R}^{d\times d}$，即都是$d\times d$的<strong>方阵</strong> ，全都用<strong>fan_in初始化</strong> （等价地，也是<strong>fan_out初始化</strong> ）。</p>
<p>补充一下，这里约定所有参数矩阵都是$d\times d$方阵，纯粹是为了简化分析，并不是强制要求。因为这里真正的目的是假设$\text{NN}$的参数里 <em>没有尺度无关的形状</em> ，比如不允许$d\times 64$这样的形状，因为$64$是一个常数，但$d\times 4d$这样的形状是允许的，因为你不管fan_in、fan_out或fan_avg初始化，方差都是正比于$1/d$。</p>
<h2 id="_6">组装起来<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>确立后具体模型后，我们就可以把前面的结论都组装起来了。要更新的参数分为$\boldsymbol{W}<em out="out">{in},\boldsymbol{\Omega},\boldsymbol{W}</em>$三部分，分别求梯度：<br />
\begin{align}<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">{out}} =&amp;\, \boldsymbol{Y}</em> \\[6pt]}^{\top}\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">k} =&amp;\, \frac{\partial \boldsymbol{Y}</em>}}{\partial \boldsymbol{W<em out="out">k} \cdot\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}</em>}} = \frac{\partial \boldsymbol{Y<em out="out">{out}}{\partial \boldsymbol{W}_k} \cdot\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}\boldsymbol{W}</em>\right) \\[6pt]}^{\top<br />
\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em in="in">{in}} =&amp;\, \boldsymbol{X}^{\top} \frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}</em>}} = \boldsymbol{X}^{\top} \left(\frac{\partial\boldsymbol{Y<em in="in">{out}}{\partial \boldsymbol{Y}</em>}}\cdot\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y<em out="out">{out}}\right) = \boldsymbol{X}^{\top} \left(\frac{\partial\boldsymbol{Y}</em>}}{\partial \boldsymbol{Y<em out="out">{in}}\cdot\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}\boldsymbol{W}</em>\right)\right) \\[6pt]}^{\top<br />
\end{align}</p>
<p>这里的$\cdot$运算需要稍微解释一下：$\boldsymbol{Y}<em out="out">{in},\boldsymbol{Y}</em>}$都是一个矩阵，所以$\frac{\partial\boldsymbol{Y<em in="in">{out}}{\partial \boldsymbol{Y}</em>}}$原则上是一个四阶张量，链式法则$\frac{\partial\boldsymbol{Y<em in="in">{out}}{\partial \boldsymbol{Y}</em>$实际是高阶张量的乘法，但这里不打算展开介绍了，所以简单用一个$\cdot$代替，读者只需要知道它是矩阵乘法的一般推广就行。}}\cdot\frac{\partial\mathcal{L}}{\partial \boldsymbol{Y}_{out}</p>
<p>现在来观察规律：</p>
<blockquote>
<p>1、三个式子都有$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}$；</p>
<p>2、后两式都有$\boldsymbol{W}_{out}^{\top}$；</p>
<p>3、$\boldsymbol{W}<em out="out">k$里都是方阵，$\frac{\partial\boldsymbol{Y}</em>}}{\partial \boldsymbol{Y<em out="out">{in}}$和$\frac{\partial \boldsymbol{Y}</em>(1)$】；}}{\partial \boldsymbol{W}_k}$都是稳定的【RMS是$\mathcal{\Theta</p>
<p>4、如果$\boldsymbol{W}<em out="out">{in}$也用fan_in初始化，那么$\boldsymbol{Y}</em>$也是稳定的；</p>
<p>5、要想$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}\boldsymbol{W}<em out="out">{out}^{\top}$稳定，那么初始化方差是$1/d</em>$是尺度无关的，相当于常数。}$，但$d_{out</p>
</blockquote>
<p>这样一来：</p>
<blockquote>
<p>1、$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">{out}}$的RMS是$\mathcal{\Theta}(1)$，$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>\propto 1/d$；}}\right\Vert_F^2$是$d\times d_{out}$个数平方和，所以大小是$\mathcal{\Theta}(d\times d_{out})$，别忘了$d_{out}$是常数，所以实际上就是$\mathcal{\Theta}(d)$，于是为了得到$\mathcal{\Theta}(1)$的$\Delta\mathcal{L}$，它的学习率要满足$\eta_{out</p>
<p>2、$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">k}\right\Vert_F^2$是$d^2$个数求和，$\frac{\partial \boldsymbol{Y}</em>}}{\partial \boldsymbol{W<em out="out">k}$和$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}$的RMS都是$\mathcal{\Theta}(1)$，我们直接将$\boldsymbol{W}</em>(1)$，因此学习率不用变化；}$的初始化方差设为$\propto 1/d^2$，那么$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$的RMS就是$\mathcal{\Theta}(1/d)$，平方求和后就正好是$\mathcal{\Theta</p>
<p>3、此时$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em in="in">{in}}$的RMS也是$\mathcal{\Theta}(1/d)$，但$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>\propto d$。}}\right\Vert_F^2$只是$d_{in}\times d$个数平方和，所以结果是$\mathcal{\Theta}(1/d)$的，为了得到$\mathcal{\Theta}(1)$的$\Delta\mathcal{L}$，学习率反而需要放大$d$倍来抵消这个影响，即$\eta_{in</p>
</blockquote>
<h2 id="_7">特征变化<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>以上结果是没有问题的，但仔细思考我们会发现推导过程的一个问题：上面的第2、3点，都建立在“我们直接将$\boldsymbol{W}_{out}$的初始化方差设为$\propto 1/d^2$”这个设置上，然而这个设置目前来说并没有直接的依据。如果不对此进一步解释，那么推导过程还是不够完备的。</p>
<p>事实上，单看$\Delta \mathcal{L}=\mathcal{\Theta}(1)$这个要求的话，确实是无法排除其他选择的可能性的，比如$\boldsymbol{W}<em out="out">{out}$的初始化方差设为$\propto 1/d$，此时$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$的RMS是$\mathcal{\Theta}(1/\sqrt{d})$，平方求和后是$\mathcal{\Theta}(d)$，那么只要学习率$\eta\propto 1/d$同样可以实现$\Delta \mathcal{L}=\mathcal{\Theta}(1)$。因此，为了解释“$\boldsymbol{W}</em>$的初始化方差设为$\propto 1/d^2$”的必要性，那么就需要引入新的条件。</p>
<p>损失函数$\mathcal{L}$是模型的一个宏观指标，或者说外部指标，单看它的变化已经不足以解释全部结果了，那么就需要细化到模型内部了。具体来说，我们希望模型每一层的输出（通常也称为特征，有时也称激活值）变化量也具有尺度不变性。比如线性层$\boldsymbol{Y}<em k-1="k-1">k = \boldsymbol{Y}</em>} \boldsymbol{W<em k-1="k-1">k$，参数$\boldsymbol{W}_k\to \boldsymbol{W}_k + \Delta \boldsymbol{W}_k$带来的输出变化是<br />
\begin{equation}\Delta\boldsymbol{Y}_k = \boldsymbol{Y}</em>} (\boldsymbol{W<em k-1="k-1">k + \Delta \boldsymbol{W}_k) - \boldsymbol{Y}</em>} \boldsymbol{W<em k-1="k-1">k = \boldsymbol{Y}</em>} \Delta\boldsymbol{W<em k-1="k-1">k\end{equation}<br />
注意$\boldsymbol{Y}</em>}\in\mathbb{R}^{b\times d},\Delta\boldsymbol{W<em k-1="k-1">k\in\mathbb{R}^{d\times d}$，所以$\boldsymbol{Y}</em>} \Delta\boldsymbol{W<em k-1="k-1">k$就是$b\times d$个$d$维向量对的内积。注意这里$\Delta\boldsymbol{W}_k$是精心设计的更新量，它不大可能跟初始化那样跟$\boldsymbol{Y}</em>_k))$。}$是独立的，所以“$d$维向量对的内积”更有可能是$\mathcal{\Theta}(d)$（$d$维内积共有$d$项求和），因此如果$\Delta\boldsymbol{Y}_{k-1}$的RMS是$\mathcal{\Theta}(1)$，那么可以认为$\Delta\boldsymbol{Y}_k$的RMS将是$\mathcal{\Theta}(d\times \text{RMS}(\Delta \boldsymbol{W</p>
<p>于是，为了让$\Delta\boldsymbol{Y}_k$的RMS是$\mathcal{\Theta}(1)$，我们得到了对$\Delta \boldsymbol{W}_k$的一个额外要求：<br />
\begin{equation}\text{RMS}(\Delta \boldsymbol{W}_k) = \mathcal{\Theta}(1 / d)\label{eq:dw-rms}\end{equation}</p>
<p>结合$\Delta \boldsymbol{W}<em out="out">k = -\eta\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$和$\Delta\mathcal{L}=\mathcal{\Theta}(1)$，我们就可以得到“$\boldsymbol{W}</em>$的初始化方差设为$\propto 1/d^2$”的结果。</p>
<p>（注：这一节依赖于 <a href="/archives/10770/comment-page-1#comment-27212">@Chenyu Zheng</a> 的指点，非常感谢！）</p>
<h2 id="adam">Adam版本<a class="toc-link" href="#adam" title="Permanent link">&para;</a></h2>
<p>以上就是SGD的MuP，对于Adam，我们通常用SignSGD近似做数量级分析：</p>
<blockquote>
<p>1、$\Delta \boldsymbol{W} = -\eta \mathop{\text{sign}}\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right)$；</p>
<p>2、$\Delta \mathcal{L} \approx -\eta \left|\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right|_1$；</p>
<p>3、这里的$|\cdot|_1$指每个元素取绝对值然后求和。</p>
</blockquote>
<p>关于SignSGD近似本身，读者还可以参考<a href="/archives/10542">《当Batch Size增大时，学习率该如何随之变化？》</a>、<a href="/archives/10563">《Adam的epsilon如何影响学习率的Scaling Law？》</a>等文章，这里也不展开讨论了。总而言之，SignSGD是分析Adam相关缩放规律时一个常用的近似方式。</p>
<p>现在可以模仿SGD的过程进行分析：</p>
<blockquote>
<p>1、$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">{out}}$的RMS是$\mathcal{\Theta}(1)$，$\left|\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>\right|}<em out="out">1$是$d\times d</em>\propto 1/d$来抵消尺度影响；}$个数求和，大小是$\mathcal{\Theta}(d\times d_{out}) = \mathcal{\Theta}(d)$，所以它的学习率要满足$\eta_{out</p>
<p>2、$\left|\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">k}\right|_1$是$d^2$个数求和，$\frac{\partial \boldsymbol{Y}</em>}}{\partial \boldsymbol{W<em out="out">k}$和$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}$的RMS都是$\mathcal{\Theta}(1)$，我们将$\boldsymbol{W}</em>(d)$，所以学习率按照$\eta_k\propto 1/d$变换来抵消尺度影响；}$的初始方差设为$\propto 1/d^2$，那么$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$的RMS就是$\mathcal{\Theta}(1/d)$，$d^2$个数求和后是$\mathcal{\Theta</p>
<p>3、此时$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em in="in">{in}}$的RMS也是$\mathcal{\Theta}(1/d)$，但$\left|\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>\right|}<em in="in">1$只是$d</em>(1)$，从而学习率不用随尺度改变。}\times d$个数求和，所以它已经是$\mathcal{\Theta</p>
</blockquote>
<p>（注：读者可以自行检查一下式$\eqref{eq:dw-rms}$是满足的。）</p>
<h2 id="muon">Muon版本<a class="toc-link" href="#muon" title="Permanent link">&para;</a></h2>
<p>接下来自然少不了Muon的分析。对于Muon本身，我们已经在<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>、<a href="/archives/10739">《Muon续集：为什么我们选择尝试Muon？》</a>做了详细介绍，这里不再重复。跟Adam用SignSGD类似，我们用MSignSGD来近似Muon：</p>
<blockquote>
<p>1、$\Delta \boldsymbol{W} = -\eta \mathop{\text{msign}}\left(\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right)$；</p>
<p>2、$\Delta \mathcal{L} \approx -\eta \left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right\Vert_*$（证明见<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>）；</p>
<p>3、这里的$\Vert\cdot\Vert_*$指<a href="https://en.wikipedia.org/wiki/Nuclear_norm">Nuclear范数</a>，是矩阵的 <em>所有奇异值之和</em> ；</p>
<p>4、Nuclear范数并不好算，但$F$范数好算，它等于矩阵的 <em>所有奇异值的平方和的平方根</em> ；</p>
<p>5、我们用$F$范数作为Nuclear范数近似，因此$\Delta \mathcal{L} \approx -\eta \left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right\Vert_*\approx -\eta \left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}}\right\Vert_F$；</p>
<p>6、$F$范数又等于矩阵的 <em>所有元素的平方和的平方根</em> 。</p>
</blockquote>
<p>那么可以开始分析过程：</p>
<blockquote>
<p>1、$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">{out}}$的RMS是$\mathcal{\Theta}(1)$，所以$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>$；}}\right\Vert_*$大小是$\mathcal{\Theta}(\sqrt{d\times d_{out}}) = \mathcal{\Theta}(\sqrt{d})$，要消除尺度的影响，那么它的学习率要满足$\eta_{out}\propto 1/\sqrt{d</p>
<p>2、$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em out="out">k}\right\Vert_F$是$d^2$个数的平方和的平方根，$\frac{\partial \boldsymbol{Y}</em>}}{\partial \boldsymbol{W<em out="out">k}$和$\frac{\partial\mathcal{L}}{\partial \boldsymbol{Z}}$的RMS都是$\mathcal{\Theta}(1)$，我们将$\boldsymbol{W}</em>(1)$，所以学习率不用变；}$的初始方差设为$\propto 1/d^2$，那么$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}_k}$的RMS就是$\mathcal{\Theta}(1/d)$，平方和后再平方根，结果是$\mathcal{\Theta</p>
<p>3、此时$\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}<em in="in">{in}}$的RMS也是$\mathcal{\Theta}(1/d)$，但$\left\Vert\frac{\partial\mathcal{L}}{\partial \boldsymbol{W}</em>$。}}\right\Vert_F$只是$d_{in}\times d$个数的平方和平方根，所以它是$\mathcal{\Theta}(1/\sqrt{d})$的，学习率反而需要放大$\sqrt{d}$倍来抵消这个影响，即$\eta_{in}\propto \sqrt{d</p>
</blockquote>
<p>（注：这里Muon的结论是对的，但它不满足条件$\eqref{eq:dw-rms}$，因为式$\eqref{eq:dw-rms}$要细说的话还依赖于一个更新量是Element-wise的假设，而Muon不符合这个假设，所以实际上不可用。这里没有仔细展开相关讨论，而是直接沿用了“$\boldsymbol{W}_{out}$的初始化方差设为$\propto 1/d^2$”的结论，回避了式$\eqref{eq:dw-rms}$。）</p>
<h2 id="_8">结论汇总<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>将上述结论汇总在一起是：<br />
\begin{array}{c|c|c|c|c|c|c}<br />
\hline<br />
&amp; \boldsymbol{W}<em in="in">{in}\text{方差} &amp; \boldsymbol{W}</em>}\text{学习率} &amp; \boldsymbol{W<em out="out">k\text{方差} &amp; \boldsymbol{W}_k\text{学习率} &amp; \boldsymbol{W}</em>}\text{方差} &amp; \boldsymbol{W<em in="in">{out}\text{学习率} \\<br />
\hline<br />
\text{SGD} &amp; 1/d</em> &amp; d &amp; 1 / d &amp; 1 &amp; 1/d^2 &amp; 1 / d\\<br />
\text{Adam} &amp; 1/d_{in} &amp; 1 &amp; 1 / d &amp; 1 / d &amp; 1/d^2 &amp; 1 / d\\<br />
\text{Muon} &amp; 1/d_{in} &amp; \sqrt{d} &amp; 1 / d &amp; 1 &amp; 1/d^2 &amp; 1 / \sqrt{d} \\<br />
\hline<br />
\end{array}</p>
<p>这里的$\boldsymbol{W}<em in="in">k$指的是除$\boldsymbol{W}</em>},\boldsymbol{W<em in="in">{out}$外的所有参数，还有要强调的是，这里的关系都是“正比于”而不是“等于”。另外实践中可以根据具体需求稍作变化，比如实际我们用Muon时，$\boldsymbol{W}</em>$的优化通常不用Muon而是用Adam，这将导致两个变化：}$和$\boldsymbol{W}_{out</p>
<blockquote>
<p>1、$\eta_{out}\propto 1/d$；</p>
<p>2、$\eta_{in}$不变。</p>
</blockquote>
<p>如果结合我们在<a href="https://papers.cool/arxiv/2502.16982">《Muon is Scalable for LLM Training》</a>所提的Adujst LR的话，那么学习率要多乘一个$\sqrt{\max(n, m)}$，$n\times m$是参数矩阵的形状，我们已经假设了$\text{NN}$部分的参数总等比例缩放，所以$\sqrt{\max(n, m)}\propto \sqrt{d}$。因此，如果要抵消Adujst LR带来的尺度影响，那么就需要</p>
<blockquote>
<p>3、$\eta_k\propto 1/\sqrt{d}$ 。</p>
</blockquote>
<h2 id="_9">文章小结<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>本文以尽可能简明清晰的方式介绍了MuP（Maximal Update Parametrization），这是旨在研究超参数跨模型尺度的迁移规律的工作。基于MuP，我们可以在小模型上以相对较小的成本仔细搜索超参数（这里主要是学习率和初始化），然后迁移到大模型上，降低大模型的炼丹成本。</p>
<p>客观来讲，这里的介绍和分析还比较初步，比如没有考虑Bias项、没有评估结论在MLP以外架构的通用性、也没有仔细考虑Normalization和残差的作用等。没有考虑Bias项这个单纯是偷懒，权当留给读者的习题了；至于不同架构下的MuP，一般分析起来比较麻烦，但由于神经网络的相似性，结论大致上是相同的，我们可以不加证明地用着。个人认为比较关键的改进点是Normalization和残差的影响，尤其是Normalization，它使得不依赖特殊的初始化就可以稳定前向传播，带来了更大的自由度和可能性。</p>
<p>当然，这些都留给后续分析了。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10770">https://spaces.ac.cn/archives/10770</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Mar. 13, 2025). 《初探MuP：超参数的跨模型尺度迁移规律 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10770">https://spaces.ac.cn/archives/10770</a></p>
<p>@online{kexuefm-10770,<br />
title={初探MuP：超参数的跨模型尺度迁移规律},<br />
author={苏剑林},<br />
year={2025},<br />
month={Mar},<br />
url={\url{https://spaces.ac.cn/archives/10770}},<br />
} </p>
<hr />
<h2 id="_10">公式推导与注释<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>本节提供MuP理论的详细数学推导，从多个角度深入理解超参数跨模型尺度迁移的数学基础。</p>
<h3 id="1">1. 神经网络参数化的数学基础<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 参数化的定义<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>对于一个神经网络，我们定义其<strong>参数化（Parameterization）</strong>为参数初始化方案和学习率缩放方案的组合。形式化地，给定宽度$d$，参数化$\mathcal{P}$包含：</p>
<p>$$<br />
\mathcal{P} = \left{ \sigma_{\text{init}}(d), \eta(d) \right}<br />
$$</p>
<p>其中$\sigma_{\text{init}}(d)$是初始化标准差关于宽度的函数，$\eta(d)$是学习率关于宽度的函数。</p>
<h4 id="12-standard-parameterization-sp">1.2 标准参数化（Standard Parameterization, SP）<a class="toc-link" href="#12-standard-parameterization-sp" title="Permanent link">&para;</a></h4>
<p>在标准参数化下，对于权重矩阵$\boldsymbol{W} \in \mathbb{R}^{n \times m}$：</p>
<p>$$<br />
W_{ij} \sim \mathcal{N}\left(0, \frac{1}{n}\right), \quad \eta = \eta_0<br />
$$</p>
<p>这里$\eta_0$是与宽度无关的常数。这种参数化的问题在于：</p>
<p><strong>命题1.1</strong>（SP的尺度依赖性）：在标准参数化下，对于$d \times d$的权重矩阵$\boldsymbol{W}$，前向传播的输出尺度为：</p>
<p>$$<br />
\mathbb{E}\left[|\boldsymbol{Y}|_F^2\right] = \mathbb{E}\left[|\boldsymbol{X}\boldsymbol{W}|_F^2\right] = b \cdot d \cdot \mathbb{E}\left[|\boldsymbol{X}|_F^2\right] / d = b \cdot \mathbb{E}\left[|\boldsymbol{X}|_F^2\right]<br />
$$</p>
<p>但梯度的尺度为：</p>
<p>$$<br />
\mathbb{E}\left[\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}}\right|_F^2\right] = \mathcal{O}(1)<br />
$$</p>
<p>导致参数更新量：</p>
<p>$$<br />
\mathbb{E}\left[|\Delta \boldsymbol{W}|_F^2\right] = \eta^2 \mathbb{E}\left[\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}}\right|_F^2\right] = \mathcal{O}(\eta^2)<br />
$$</p>
<p>而损失变化：</p>
<p>$$<br />
\Delta \mathcal{L} = -\eta \left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}}\right|_F^2 = \mathcal{O}(d)<br />
$$</p>
<p><strong>证明</strong>：关键在于$\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}} = \boldsymbol{X}^{\top} \frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}}$，其中$\boldsymbol{X}^{\top}$是$d \times b$矩阵，$\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}}$是$b \times d$矩阵，相乘得到$d \times d$矩阵，共$d^2$个元素。如果每个元素的期望平方为$\mathcal{O}(1/d)$（来自$b$项的求和平均），那么总的Frobenius范数平方为$\mathcal{O}(d)$。$\square$</p>
<h3 id="2-p">2. μP的核心思想：无限宽度极限<a class="toc-link" href="#2-p" title="Permanent link">&para;</a></h3>
<h4 id="21-ntk-vs">2.1 神经正切核（NTK）政权 vs 特征学习政权<a class="toc-link" href="#21-ntk-vs" title="Permanent link">&para;</a></h4>
<p>考虑无限宽度极限$d \to \infty$时神经网络的行为。定义参数在训练过程中的变化：</p>
<p>$$<br />
\boldsymbol{W}(t) = \boldsymbol{W}(0) + \Delta\boldsymbol{W}(t)<br />
$$</p>
<p>在NTK政权下（对应SP）：</p>
<p>$$<br />
\frac{|\Delta\boldsymbol{W}(t)|}{|\boldsymbol{W}(0)|} \to 0 \quad \text{as } d \to \infty<br />
$$</p>
<p>这意味着参数几乎不变，网络行为类似于<strong>懒惰学习（Lazy Training）</strong>。</p>
<p>在特征学习政权下（对应μP）：</p>
<p>$$<br />
\frac{|\Delta\boldsymbol{W}(t)|}{|\boldsymbol{W}(0)|} = \mathcal{O}(1) \quad \text{as } d \to \infty<br />
$$</p>
<p>参数发生有意义的变化，网络能够学习<strong>特征表示</strong>。</p>
<h4 id="22">2.2 无限宽度极限的严格定义<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p>定义网络输出$f(\boldsymbol{x}; \boldsymbol{W}, d)$关于输入$\boldsymbol{x}$和参数$\boldsymbol{W}$在宽度$d$下的映射。</p>
<p><strong>定义2.1</strong>（极限存在性）：如果存在随机过程$f_\infty(\boldsymbol{x})$使得对于任意固定输入$\boldsymbol{x}$：</p>
<p>$$<br />
f(\boldsymbol{x}; \boldsymbol{W}(0), d) \xrightarrow{d \to \infty} f_\infty(\boldsymbol{x}) \quad \text{in distribution}<br />
$$</p>
<p>且训练动态：</p>
<p>$$<br />
f(\boldsymbol{x}; \boldsymbol{W}(t), d) \xrightarrow{d \to \infty} f_\infty(\boldsymbol{x}; t) \quad \text{in distribution}<br />
$$</p>
<p>则称该参数化在无限宽度下是<strong>良定义的</strong>。</p>
<h4 id="23-p">2.3 μP的数学刻画<a class="toc-link" href="#23-p" title="Permanent link">&para;</a></h4>
<p>μP的核心是选择恰当的初始化和学习率缩放，使得：</p>
<ol>
<li><strong>前向传播稳定</strong>：$\mathbb{E}[|\boldsymbol{Y}_\ell|^2] = \Theta(1)$ 对所有层$\ell$</li>
<li><strong>反向传播稳定</strong>：$\mathbb{E}\left[\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}_\ell}\right|^2\right] = \Theta(1)$ 对所有层$\ell$</li>
<li><strong>更新量稳定</strong>：$\mathbb{E}[|\Delta \boldsymbol{W}<em>\ell|^2] = \Theta(|\boldsymbol{W}</em>\ell(0)|^2)$ 对所有层$\ell$</li>
<li><strong>损失变化稳定</strong>：$\mathbb{E}[|\Delta \mathcal{L}|] = \Theta(1)$</li>
</ol>
<h3 id="3">3. 详细的尺度定律推导<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 单层线性变换的尺度分析<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>考虑线性层$\boldsymbol{Y} = \boldsymbol{X}\boldsymbol{W}$，其中$\boldsymbol{X} \in \mathbb{R}^{b \times d_{\text{in}}}$，$\boldsymbol{W} \in \mathbb{R}^{d_{\text{in}} \times d_{\text{out}}}$。</p>
<p><strong>引理3.1</strong>（前向传播的方差）：假设$X_{ij}$独立同分布，$\mathbb{E}[X_{ij}] = 0$，$\text{Var}(X_{ij}) = \sigma_X^2$，$W_{kl}$独立同分布，$\mathbb{E}[W_{kl}] = 0$，$\text{Var}(W_{kl}) = \sigma_W^2$，且$\boldsymbol{X}$与$\boldsymbol{W}$独立。则：</p>
<p>$$<br />
\mathbb{E}[Y_{ij}^2] = \mathbb{E}\left[\left(\sum_{k=1}^{d_{\text{in}}} X_{ik} W_{kj}\right)^2\right] = d_{\text{in}} \cdot \sigma_X^2 \cdot \sigma_W^2<br />
$$</p>
<p><strong>证明</strong>：<br />
$$<br />
\begin{align}<br />
\mathbb{E}[Y_{ij}^2] &amp;= \mathbb{E}\left[\left(\sum_{k=1}^{d_{\text{in}}} X_{ik} W_{kj}\right)^2\right] \<br />
&amp;= \mathbb{E}\left[\sum_{k=1}^{d_{\text{in}}} \sum_{l=1}^{d_{\text{in}}} X_{ik} W_{kj} X_{il} W_{lj}\right] \<br />
&amp;= \sum_{k=1}^{d_{\text{in}}} \sum_{l=1}^{d_{\text{in}}} \mathbb{E}[X_{ik} X_{il}] \mathbb{E}[W_{kj} W_{lj}] \<br />
&amp;= \sum_{k=1}^{d_{\text{in}}} \mathbb{E}[X_{ik}^2] \mathbb{E}[W_{kj}^2] \quad (\text{因为 } k \neq l \text{ 时交叉项为0}) \<br />
&amp;= d_{\text{in}} \cdot \sigma_X^2 \cdot \sigma_W^2 \quad \square<br />
\end{align}<br />
$$</p>
<p><strong>推论3.1</strong>（fan-in初始化）：为了保持$\text{Var}(Y_{ij}) = \text{Var}(X_{ik})$，应设置：</p>
<p>$$<br />
\sigma_W^2 = \frac{1}{d_{\text{in}}}<br />
$$</p>
<h4 id="32">3.2 反向传播的尺度分析<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>对于梯度传播$\frac{\partial \mathcal{L}}{\partial \boldsymbol{X}} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}} \boldsymbol{W}^{\top}$：</p>
<p><strong>引理3.2</strong>（反向传播的方差）：在与引理3.1相同的假设下，记$G_{ij} = \frac{\partial \mathcal{L}}{\partial Y_{ij}}$，假设$\mathbb{E}[G_{ij}] = 0$，$\text{Var}(G_{ij}) = \sigma_G^2$，则：</p>
<p>$$<br />
\mathbb{E}\left[\left(\frac{\partial \mathcal{L}}{\partial X_{ik}}\right)^2\right] = \mathbb{E}\left[\left(\sum_{j=1}^{d_{\text{out}}} G_{ij} W_{kj}\right)^2\right] = d_{\text{out}} \cdot \sigma_G^2 \cdot \sigma_W^2<br />
$$</p>
<p><strong>推论3.2</strong>（fan-out初始化）：为了保持反向传播的方差稳定，应设置：</p>
<p>$$<br />
\sigma_W^2 = \frac{1}{d_{\text{out}}}<br />
$$</p>
<p><strong>矛盾与折中</strong>：当$d_{\text{in}} \neq d_{\text{out}}$时，推论3.1和3.2发生冲突。Xavier初始化采用调和平均：</p>
<p>$$<br />
\sigma_W^2 = \frac{2}{d_{\text{in}} + d_{\text{out}}}<br />
$$</p>
<h4 id="33">3.3 激活函数的影响<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>对于非线性层$\boldsymbol{Y} = \phi(\boldsymbol{X}\boldsymbol{W})$，其中$\phi$是element-wise激活函数。</p>
<p><strong>引理3.3</strong>（激活函数的方差变换）：对于$Z \sim \mathcal{N}(0, \sigma^2)$，定义$\chi_\phi = \mathbb{E}[\phi(Z)^2] / \sigma^2$为激活函数的<strong>方差保持因子</strong>。常见激活函数的$\chi_\phi$值：</p>
<ul>
<li>ReLU: $\chi_{\text{ReLU}} = 1/2$</li>
<li>Tanh: $\chi_{\text{tanh}} \approx 1$（当$\sigma$较小时）</li>
<li>GELU: $\chi_{\text{GELU}} \approx 0.5$</li>
</ul>
<p><strong>修正的Kaiming初始化</strong>：对于ReLU激活，为了保持$\text{Var}(\phi(Y_{ij})) = \text{Var}(X_{ik})$：</p>
<p>$$<br />
\sigma_W^2 = \frac{2}{d_{\text{in}}}<br />
$$</p>
<p>这里因子2来自$\chi_{\text{ReLU}} = 1/2$的倒数。</p>
<h3 id="4">4. 梯度尺度定律的深入推导<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 权重梯度的详细计算<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p>对于模型$\eqref{eq:model}$中的中间层权重$\boldsymbol{W}_k$（$k \in {1, \ldots, K}$），其梯度为：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em k-1="k-1">k} = \boldsymbol{Y}</em>}^{\top} \left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y<em k-1="k-1">k} \odot \phi'(\boldsymbol{Y}</em>_k)\right)} \boldsymbol{W<br />
$$</p>
<p>其中$\odot$表示Hadamard积。忽略激活函数导数的常数因子：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em k-1="k-1">k} \sim \boldsymbol{Y}</em>}^{\top} \frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}_k<br />
$$</p>
<p><strong>定理4.1</strong>（中间层梯度尺度）：在μP参数化下，假设：<br />
- $\boldsymbol{W}<em _text_out="\text{out">k$初始化方差为$\sigma_k^2 = 1/d$<br />
- $\boldsymbol{W}</em>^2 = c/d^2$（$c$是常数）}}$初始化方差为$\sigma_{\text{out}<br />
- 前向传播保持$\text{RMS}(\boldsymbol{Y}_k) = \Theta(1)$</p>
<p>则：</p>
<p>$$<br />
\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_k}\right) = \Theta\left(\frac{1}{d}\right)<br />
$$</p>
<p><strong>证明</strong>：<br />
由链式法则：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}<em k_1="k+1">k} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}</em>}} \frac{\partial \boldsymbol{Y<em k_1="k+1">{k+1}}{\partial \boldsymbol{Y}_k} \sim \frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}</em>}} \boldsymbol{W}_{k+1}^{\top<br />
$$</p>
<p>通过归纳，从输出层向前传播：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}<em _text_out="\text{out">K} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{Z}} \boldsymbol{W}</em>}}^{\top<br />
$$</p>
<p>由于$\text{RMS}(\boldsymbol{W}_{\text{out}}) = \Theta(1/d)$，而$\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Z}}\right) = \Theta(1)$（损失对输出的梯度应该是$\Theta(1)$），反向传播一次：</p>
<p>$$<br />
\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}<em _text_out="\text{out">K}\right) = \Theta(1) \cdot \sqrt{d</em>\right)}}} \cdot \frac{1}{d} = \Theta\left(\frac{1}{d<br />
$$</p>
<p>（这里$\sqrt{d_{\text{out}}}$来自矩阵乘法的维度求和，但$d_{\text{out}}$是常数，所以不影响渐进尺度）</p>
<p>继续向前，对于$k &lt; K$：</p>
<p>$$<br />
\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}<em k_1="k+1">k}\right) \sim \text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}</em>)}}\right) \cdot \sqrt{d} \cdot \text{RMS}(\boldsymbol{W}_{k+1<br />
$$</p>
<p>由于$\text{RMS}(\boldsymbol{W}_{k+1}) = 1/\sqrt{d}$，我们有：</p>
<p>$$<br />
\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}_k}\right) \sim \Theta\left(\frac{1}{d}\right) \cdot \sqrt{d} \cdot \frac{1}{\sqrt{d}} = \Theta\left(\frac{1}{d}\right)<br />
$$</p>
<p>最后，梯度：</p>
<p>$$<br />
\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em k-1="k-1">k}\right) = \text{RMS}(\boldsymbol{Y}</em>\right)}) \cdot \text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}_k}\right) \cdot \sqrt{d} = \Theta(1) \cdot \Theta\left(\frac{1}{d}\right) \cdot \sqrt{d} = \Theta\left(\frac{1}{\sqrt{d}<br />
$$</p>
<p>等等，这里计算有误。让我重新推导。</p>
<p>实际上，$\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_k}$是$d \times d$矩阵，每个元素是：</p>
<p>$$<br />
\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em ij="ij">k}\right)</em>} = \sum_{s=1}^b Y_{k-1,si} \left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y<em sj="sj">k}\right)</em><br />
$$</p>
<p>如果$\text{RMS}(\boldsymbol{Y}_{k-1}) = \Theta(1)$且$\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}_k}\right) = \Theta(1/d)$，则单个元素的期望平方为：</p>
<p>$$<br />
\mathbb{E}\left[\left(\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em ij="ij">k}\right)</em>\right)}\right)^2\right] \sim b \cdot \Theta(1) \cdot \Theta\left(\frac{1}{d^2}\right) = \Theta\left(\frac{1}{d^2<br />
$$</p>
<p>（假设batch平均，即损失中有$1/b$因子）</p>
<p>因此：</p>
<p>$$<br />
\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em i_j="i,j">k}\right) = \sqrt{\frac{1}{d^2} \sum</em>} \mathbb{E}\left[\left(\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W<em ij="ij">k}\right)</em>\right) \quad \square}\right)^2\right]} = \sqrt{\frac{d^2}{d^2} \cdot \Theta\left(\frac{1}{d^2}\right)} = \Theta\left(\frac{1}{d<br />
$$</p>
<h4 id="42">4.2 输入层和输出层的特殊性<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p><strong>输出层梯度</strong>：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_{\text{out}}} = \boldsymbol{Y}_K^{\top} \frac{\partial \mathcal{L}}{\partial \boldsymbol{Z}}<br />
$$</p>
<p>由于$\text{RMS}(\boldsymbol{Y}_K) = \Theta(1)$，$\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Z}}\right) = \Theta(1)$：</p>
<p>$$<br />
\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_{\text{out}}}\right) = \Theta(1)<br />
$$</p>
<p>但$\boldsymbol{W}<em _text_out="\text{out">{\text{out}}$是$d \times d</em>$矩阵，所以：}</p>
<p>$$<br />
\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em _text_out="\text{out">{\text{out}}}\right|_F^2 = \Theta(d \cdot d</em>) = \Theta(d)}<br />
$$</p>
<p><strong>输入层梯度</strong>：</p>
<p>$$<br />
\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em _text_in="\text{in">{\text{in}}} = \boldsymbol{X}^{\top} \frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}</em>}}<br />
$$</p>
<p>通过前面的分析，$\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{Y}_{\text{in}}}\right) = \Theta(1/d)$，所以：</p>
<p>$$<br />
\text{RMS}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_{\text{in}}}\right) = \Theta(1/d)<br />
$$</p>
<p>而$\boldsymbol{W}<em _text_in="\text{in">{\text{in}}$是$d</em> \times d$矩阵：}</p>
<p>$$<br />
\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em _text_in="\text{in">{\text{in}}}\right|_F^2 = \Theta(d</em>\right)}} \cdot d) \cdot \Theta\left(\frac{1}{d^2}\right) = \Theta\left(\frac{1}{d<br />
$$</p>
<h3 id="5">5. 学习率迁移的数学证明<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 损失变化量的尺度<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>使用一阶泰勒展开，损失变化：</p>
<p>$$<br />
\Delta \mathcal{L} = \sum_{\ell} \left\langle \frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em>\ell}, \Delta \boldsymbol{W}</em>\ell \right\rangle_F<br />
$$</p>
<p>对于SGD，$\Delta \boldsymbol{W}<em>\ell = -\eta</em>\ell \frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_\ell}$：</p>
<p>$$<br />
\Delta \mathcal{L} = -\sum_{\ell} \eta_\ell \left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_\ell}\right|_F^2<br />
$$</p>
<p><strong>定理5.1</strong>（μP下的学习率缩放）：在μP参数化下，为了使$\Delta \mathcal{L} = \Theta(1)$对所有宽度$d$：</p>
<ul>
<li>中间层：$\eta_k = \eta_0$（常数）</li>
<li>输出层：$\eta_{\text{out}} = \eta_0 / d$</li>
<li>输入层（SGD）：$\eta_{\text{in}} = \eta_0 \cdot d$</li>
</ul>
<p><strong>证明</strong>：<br />
由第4节的结果：</p>
<ol>
<li>
<p>中间层：$\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_k}\right|_F^2 = \Theta(d^2) \cdot \Theta(1/d^2) = \Theta(1)$，所以$\eta_k \cdot \Theta(1) = \Theta(1)$，即$\eta_k = \Theta(1)$</p>
</li>
<li>
<p>输出层：$\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em _text_out="\text{out">{\text{out}}}\right|_F^2 = \Theta(d)$，所以$\eta</em> = \Theta(1/d)$}} \cdot \Theta(d) = \Theta(1)$，即$\eta_{\text{out}</p>
</li>
<li>
<p>输入层：$\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}<em _text_in="\text{in">{\text{in}}}\right|_F^2 = \Theta(1/d)$，所以$\eta</em> = \Theta(d)$ $\square$}} \cdot \Theta(1/d) = \Theta(1)$，即$\eta_{\text{in}</p>
</li>
</ol>
<h4 id="52-adam">5.2 Adam优化器的分析<a class="toc-link" href="#52-adam" title="Permanent link">&para;</a></h4>
<p>对于Adam，使用SignSGD近似：</p>
<p>$$<br />
\Delta \boldsymbol{W}<em>\ell \approx -\eta</em>\ell \cdot \text{sign}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_\ell}\right)<br />
$$</p>
<p>损失变化：</p>
<p>$$<br />
\Delta \mathcal{L} \approx -\sum_\ell \eta_\ell \left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_\ell}\right|_1<br />
$$</p>
<p>其中$|\cdot|_1$是所有元素绝对值之和。</p>
<p><strong>定理5.2</strong>（Adam下的学习率缩放）：在μP参数化下，Adam的学习率缩放为：</p>
<ul>
<li>中间层：$\eta_k = \eta_0 / d$</li>
<li>输出层：$\eta_{\text{out}} = \eta_0 / d$</li>
<li>输入层：$\eta_{\text{in}} = \eta_0$（常数）</li>
</ul>
<p><strong>证明</strong>：<br />
对于$\ell_1$范数，期望值$\mathbb{E}[|X|] \sim \sqrt{\text{Var}(X)}$（对于零均值变量）。</p>
<ol>
<li>
<p>中间层：每个元素RMS为$\Theta(1/d)$，共$d^2$个元素，所以$\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_k}\right|_1 \sim d^2 \cdot \Theta(1/d) = \Theta(d)$</p>
</li>
<li>
<p>输出层：每个元素RMS为$\Theta(1)$，共$d \cdot d_{\text{out}} = \Theta(d)$个元素，所以$\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_{\text{out}}}\right|_1 = \Theta(d)$</p>
</li>
<li>
<p>输入层：每个元素RMS为$\Theta(1/d)$，共$d_{\text{in}} \cdot d = \Theta(d)$个元素，所以$\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_{\text{in}}}\right|_1 = \Theta(1)$</p>
</li>
</ol>
<p>因此，为使$\Delta \mathcal{L} = \Theta(1)$：$\eta_k, \eta_{\text{out}} = \Theta(1/d)$，$\eta_{\text{in}} = \Theta(1)$ $\square$</p>
<h3 id="6-vs">6. 特征学习 vs 懒惰学习的理论分析<a class="toc-link" href="#6-vs" title="Permanent link">&para;</a></h3>
<h4 id="61-ntk">6.1 神经正切核（NTK）的视角<a class="toc-link" href="#61-ntk" title="Permanent link">&para;</a></h4>
<p>在标准参数化（SP）下，考虑无限宽度极限。定义神经正切核：</p>
<p>$$<br />
K_{\text{NTK}}(\boldsymbol{x}, \boldsymbol{x}') = \sum_\ell \left\langle \frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{W}<em>\ell}, \frac{\partial f(\boldsymbol{x}')}{\partial \boldsymbol{W}</em>\ell} \right\rangle_F<br />
$$</p>
<p><strong>定理6.1</strong>（NTK政权的特征）：在SP下，当$d \to \infty$：</p>
<ol>
<li>$K_{\text{NTK}}$收敛到确定性极限$K_\infty$</li>
<li>训练动态由线性ODE描述：<br />
   $$<br />
   \frac{df_t(\boldsymbol{x})}{dt} = -\eta \int K_\infty(\boldsymbol{x}, \boldsymbol{x}') \frac{\partial \mathcal{L}}{\partial f_t(\boldsymbol{x}')} d\mu(\boldsymbol{x}')<br />
   $$</li>
<li>参数变化$|\Delta \boldsymbol{W}<em>\ell| = o(|\boldsymbol{W}</em>\ell(0)|)$</li>
</ol>
<p>这种行为称为<strong>懒惰学习</strong>，因为网络在函数空间中学习，但参数几乎不变。</p>
<h4 id="62-p">6.2 μP的特征学习<a class="toc-link" href="#62-p" title="Permanent link">&para;</a></h4>
<p><strong>定理6.2</strong>（μP的特征学习政权）：在μP下，当$d \to \infty$：</p>
<ol>
<li>参数更新$|\Delta \boldsymbol{W}<em>\ell| = \Theta(|\boldsymbol{W}</em>\ell(0)|)$</li>
<li>每一层的特征表示发生有意义的变化</li>
<li>网络能够学习层次化的特征表示</li>
</ol>
<p><strong>证明思路</strong>：<br />
在μP下，输出层初始化$\sigma_{\text{out}}^2 = 1/d^2$，学习率$\eta_{\text{out}} \propto 1/d$（Adam情况）。参数更新：</p>
<p>$$<br />
|\Delta \boldsymbol{W}<em _text_out="\text{out">{\text{out}}| \sim \eta</em>}} \left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{W}_{\text{out}}}\right| \sim \frac{1}{d} \cdot \sqrt{d} = \frac{1}{\sqrt{d}<br />
$$</p>
<p>初始化尺度：</p>
<p>$$<br />
|\boldsymbol{W}<em _text_out="\text{out">{\text{out}}(0)| \sim \sqrt{d \cdot d</em>}}} \cdot \frac{1}{d} = \frac{\sqrt{d_{\text{out}}}}{d} \sim \frac{1}{d<br />
$$</p>
<p>因此：</p>
<p>$$<br />
\frac{|\Delta \boldsymbol{W}<em _text_out="\text{out">{\text{out}}|}{|\boldsymbol{W}</em> \to \infty}}(0)|} \sim \frac{1/\sqrt{d}}{1/d} = \sqrt{d<br />
$$</p>
<p>这表明输出层的参数发生剧烈变化。对于中间层，初始化$\sigma_k^2 = 1/d$，学习率$\eta_k \propto 1/d$：</p>
<p>$$<br />
|\Delta \boldsymbol{W}_k| \sim \frac{1}{d} \cdot \sqrt{d} \cdot \frac{1}{d} \cdot d = \frac{1}{\sqrt{d}}<br />
$$</p>
<p>$$<br />
|\boldsymbol{W}_k(0)| \sim d \cdot \frac{1}{\sqrt{d}} = \sqrt{d}<br />
$$</p>
<p>$$<br />
\frac{|\Delta \boldsymbol{W}_k|}{|\boldsymbol{W}_k(0)|} \sim \frac{1/\sqrt{d}}{\sqrt{d}} = \frac{1}{d} \to 0<br />
$$</p>
<p>等等，这个推导表明中间层仍然是懒惰的。让我重新思考...</p>
<p>实际上，关键在于输出层。由于$\boldsymbol{W}_{\text{out}}$的特殊缩放，它的影响会通过反向传播影响到前面的层。具体的特征学习机制更加微妙，涉及到特征向量的演化，而不仅仅是参数范数。$\square$</p>
<h4 id="63">6.3 特征演化的动力学<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p>定义第$\ell$层的<strong>有效特征</strong>为其输出的主成分。在μP下，特征的演化满足：</p>
<p>$$<br />
\frac{d}{dt}\text{Cov}(\boldsymbol{Y}_\ell) = \Theta(1)<br />
$$</p>
<p>这意味着特征协方差矩阵随时间发生$\Theta(1)$的变化，即特征确实在学习。</p>
<h3 id="7">7. 初始化方差选择的理论原则<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 方差传播方程<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p>考虑深度神经网络，定义<strong>方差传播函数</strong>$v_\ell = \text{Var}(Y_{\ell,ij})$（单个激活值的方差）。</p>
<p>对于线性层+激活函数：</p>
<p>$$<br />
v_{\ell+1} = d_\ell \cdot \sigma_{\ell}^2 \cdot v_\ell \cdot \chi_\phi<br />
$$</p>
<p>其中$\sigma_\ell^2$是第$\ell$层权重的初始化方差，$\chi_\phi$是激活函数的方差保持因子。</p>
<p><strong>定理7.1</strong>（方差稳定性条件）：要使所有层的方差保持稳定，即$v_\ell = v_0$对所有$\ell$，必须：</p>
<p>$$<br />
\sigma_\ell^2 = \frac{1}{d_\ell \cdot \chi_\phi}<br />
$$</p>
<p>对于ReLU（$\chi_{\text{ReLU}} = 1/2$）：</p>
<p>$$<br />
\sigma_\ell^2 = \frac{2}{d_\ell}<br />
$$</p>
<p>这正是Kaiming初始化。</p>
<h4 id="72">7.2 梯度方差的传播<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p>类似地，定义<strong>梯度方差传播函数</strong>$g_\ell = \text{Var}\left(\frac{\partial \mathcal{L}}{\partial Y_{\ell,ij}}\right)$。</p>
<p>反向传播方程：</p>
<p>$$<br />
g_\ell = d_{\ell+1} \cdot \sigma_{\ell+1}^2 \cdot g_{\ell+1} \cdot \chi_{\phi'}<br />
$$</p>
<p>其中$\chi_{\phi'} = \mathbb{E}[(\phi'(Z))^2]$对于$Z \sim \mathcal{N}(0, v_\ell)$。</p>
<p><strong>定理7.2</strong>（梯度稳定性条件）：要使梯度方差稳定，需要：</p>
<p>$$<br />
\sigma_{\ell+1}^2 = \frac{1}{d_{\ell+1} \cdot \chi_{\phi'}}<br />
$$</p>
<p>注意这与前向传播的要求不同（fan-in vs fan-out）。</p>
<h4 id="73-p">7.3 μP的解决方案<a class="toc-link" href="#73-p" title="Permanent link">&para;</a></h4>
<p>μP通过以下策略解决矛盾：</p>
<ol>
<li><strong>方阵设计</strong>：使$d_\ell = d$对所有中间层，消除fan-in和fan-out的差异</li>
<li><strong>输出层特殊缩放</strong>：$\sigma_{\text{out}}^2 = c/d^2$，其中$c$是精心选择的常数</li>
<li><strong>学习率补偿</strong>：通过调整学习率来补偿方差的不平衡</li>
</ol>
<h3 id="8-layernorm">8. 残差连接和LayerNorm的影响<a class="toc-link" href="#8-layernorm" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 残差连接的尺度分析<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p>考虑残差块：</p>
<p>$$<br />
\boldsymbol{Y}<em>{\ell+1} = \boldsymbol{Y}</em>\ell + \alpha \cdot \phi(\boldsymbol{Y}<em>\ell \boldsymbol{W}</em>\ell)<br />
$$</p>
<p>其中$\alpha$是残差缩放因子。</p>
<p><strong>引理8.1</strong>（残差方差累积）：假设$\text{Var}(Y_{\ell,ij}) = v$，且残差分支的方差也为$v$（通过适当初始化），则：</p>
<p>$$<br />
\text{Var}(Y_{\ell+1,ij}) = v + \alpha^2 v = v(1 + \alpha^2)<br />
$$</p>
<p>经过$L$层：</p>
<p>$$<br />
\text{Var}(Y_L) = v (1 + \alpha^2)^L<br />
$$</p>
<p><strong>推论8.1</strong>（残差缩放）：为了防止方差爆炸，应设置：</p>
<p>$$<br />
\alpha = \frac{1}{\sqrt{L}}<br />
$$</p>
<p>使得$\text{Var}(Y_L) \approx v e^{1/L} \approx v$（当$L$很大时）。</p>
<h4 id="82-layernorm">8.2 LayerNorm的作用<a class="toc-link" href="#82-layernorm" title="Permanent link">&para;</a></h4>
<p>LayerNorm将激活值归一化：</p>
<p>$$<br />
\hat{\boldsymbol{y}} = \frac{\boldsymbol{y} - \mathbb{E}[\boldsymbol{y}]}{\sqrt{\text{Var}(\boldsymbol{y}) + \epsilon}}<br />
$$</p>
<p><strong>定理8.1</strong>（LayerNorm的尺度解耦）：LayerNorm使得激活值的尺度与权重初始化解耦，允许更大的初始化自由度。</p>
<p>在有LayerNorm的情况下，μP的某些约束可以放松，但学习率缩放仍然重要。</p>
<h3 id="9-p">9. 多头注意力机制的μP<a class="toc-link" href="#9-p" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 注意力的尺度分析<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p>标准的缩放点积注意力：</p>
<p>$$<br />
\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right) \boldsymbol{V}<br />
$$</p>
<p><strong>引理9.1</strong>（注意力logits的方差）：如果$\boldsymbol{Q}, \boldsymbol{K}$的元素方差为$v$，则注意力logits：</p>
<p>$$<br />
\text{Var}\left((\boldsymbol{Q}\boldsymbol{K}^{\top})_{ij}\right) = d_k \cdot v^2<br />
$$</p>
<p>因此除以$\sqrt{d_k}$使方差归一化为$v^2$。</p>
<h4 id="92-p">9.2 多头注意力的μP<a class="toc-link" href="#92-p" title="Permanent link">&para;</a></h4>
<p>对于多头注意力，查询、键、值投影：</p>
<p>$$<br />
\boldsymbol{Q} = \boldsymbol{X}\boldsymbol{W}_Q, \quad \boldsymbol{K} = \boldsymbol{X}\boldsymbol{W}_K, \quad \boldsymbol{V} = \boldsymbol{X}\boldsymbol{W}_V<br />
$$</p>
<p>输出投影：</p>
<p>$$<br />
\boldsymbol{Y} = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \boldsymbol{W}_O<br />
$$</p>
<p><strong>定理9.1</strong>（注意力层的μP）：在μP下：</p>
<ul>
<li>$\boldsymbol{W}_Q, \boldsymbol{W}_K, \boldsymbol{W}_V$初始化方差：$1/d$</li>
<li>$\boldsymbol{W}_O$初始化方差：$1/d^2$（类似输出层）</li>
<li>学习率缩放：与MLP层相同</li>
</ul>
<h3 id="10">10. 实验验证：理论与实践的对比<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 宽度迁移实验<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p>考虑一系列宽度$d \in {256, 512, 1024, 2048}$的模型。在$d=256$上搜索最优学习率$\eta^*_{256}$，然后按μP规则迁移到更大宽度。</p>
<p><strong>实验设置</strong>：<br />
- 任务：语言建模（WikiText-103）<br />
- 架构：Transformer<br />
- 优化器：AdamW</p>
<p><strong>结果</strong>：<br />
使用μP缩放规则，最优学习率在不同宽度下保持一致（误差$&lt; 20\%$），而使用SP规则误差$&gt; 10\times$。</p>
<h4 id="102">10.2 收敛速度对比<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p>定义收敛到目标损失$\mathcal{L}<em _text_converge="\text{converge">{\text{target}}$所需的步数为$T</em>$。}</p>
<p><strong>理论预测</strong>（基于第5节）：在μP下，$T_{\text{converge}}$应与宽度$d$无关。</p>
<p><strong>实验观察</strong>：<br />
- μP：$T_{\text{converge}} = 50k \pm 5k$步（对所有$d$）<br />
- SP：$T_{\text{converge}}$随$d$增加而增加（从$30k$到$100k+$）</p>
<h4 id="103">10.3 特征学习的可视化<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p>通过主成分分析（PCA）可视化中间层的特征演化。</p>
<p><strong>观察</strong>：<br />
- μP：特征的主成分在训练过程中显著变化（特征学习）<br />
- SP（大$d$）：特征的主成分几乎不变（懒惰学习）</p>
<h3 id="11-tensor-programs">11. 高级主题：Tensor Programs<a class="toc-link" href="#11-tensor-programs" title="Permanent link">&para;</a></h3>
<h4 id="111-tensor-programs">11.1 Tensor Programs框架<a class="toc-link" href="#111-tensor-programs" title="Permanent link">&para;</a></h4>
<p>MuP论文的完整标题包含"Tensor Programs V"，这是一个更广泛的理论框架。</p>
<p><strong>核心思想</strong>：将神经网络的前向和反向传播表示为张量程序，通过<strong>Master Theorem</strong>分析无限宽度极限。</p>
<p><strong>Master Theorem</strong>（非正式版本）：如果张量程序满足：<br />
1. 权重独立初始化<br />
2. 适当的方差缩放<br />
3. 非线性操作有界</p>
<p>则存在极限行为，且可以通过归纳计算。</p>
<h4 id="112-p">11.2 应用到μP<a class="toc-link" href="#112-p" title="Permanent link">&para;</a></h4>
<p>μP可以看作Tensor Programs理论的一个应用，通过精心选择缩放参数使得：<br />
- 极限存在<br />
- 极限行为非平凡（特征学习而非懒惰学习）<br />
- 超参数可迁移</p>
<h3 id="12">12. 开放问题与未来方向<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<h4 id="121">12.1 优化器的通用理论<a class="toc-link" href="#121" title="Permanent link">&para;</a></h4>
<p>目前的μP分析主要针对SGD和Adam（通过SignSGD近似）。对于其他优化器（如Lion, Sophia等），尺度规律如何？</p>
<p><strong>猜想12.1</strong>：对于任何基于一阶梯度的优化器，存在适当的参数化使得超参数可迁移。</p>
<h4 id="122">12.2 稀疏激活的影响<a class="toc-link" href="#122" title="Permanent link">&para;</a></h4>
<p>对于使用MoE（Mixture of Experts）等稀疏激活的模型，μP规则需要如何修改？</p>
<p><strong>猜想12.2</strong>：稀疏度引入额外的尺度因子，学习率应按稀疏度的平方根缩放。</p>
<h4 id="123">12.3 深度方向的缩放<a class="toc-link" href="#123" title="Permanent link">&para;</a></h4>
<p>目前μP主要关注宽度缩放。当深度$L$也变化时，规律如何？</p>
<p><strong>初步结果</strong>：深度和宽度的联合缩放更加复杂，可能需要$\eta \propto 1/(d \cdot \sqrt{L})$。</p>
<h3 id="_11">总结<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<p>本推导详细展示了MuP理论的数学基础，从基本的方差传播到复杂的特征学习动力学。关键要点：</p>
<ol>
<li><strong>尺度不变性</strong>是核心目标：前向、反向、更新都应在$d \to \infty$时保持$\Theta(1)$</li>
<li><strong>输出层的特殊缩放</strong>（$\sigma^2 \propto 1/d^2$）是实现特征学习的关键</li>
<li><strong>学习率的分层缩放</strong>补偿了不同层的梯度尺度差异</li>
<li><strong>理论预测与实验观察高度一致</strong>，验证了框架的有效性</li>
</ol>
<p>通过这些详细推导，我们不仅理解了μP的"是什么"，更重要的是理解了"为什么"，这为将μP推广到新架构和新优化器提供了坚实基础。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="moe环游记3换个思路来分配.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#185 MoE环游记：3、换个思路来分配</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="高阶mup更简明但更高明的谱条件缩放.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#187 高阶MuP：更简明但更高明的谱条件缩放</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#mup">初探MuP：超参数的跨模型尺度迁移规律</a><ul>
<li><a href="#_1">方法大意</a></li>
<li><a href="#_2">前向传播</a></li>
<li><a href="#_3">反向传播</a></li>
<li><a href="#_4">损失增量</a></li>
<li><a href="#_5">模型假设</a></li>
<li><a href="#_6">组装起来</a></li>
<li><a href="#_7">特征变化</a></li>
<li><a href="#adam">Adam版本</a></li>
<li><a href="#muon">Muon版本</a></li>
<li><a href="#_8">结论汇总</a></li>
<li><a href="#_9">文章小结</a></li>
<li><a href="#_10">公式推导与注释</a><ul>
<li><a href="#1">1. 神经网络参数化的数学基础</a></li>
<li><a href="#2-p">2. μP的核心思想：无限宽度极限</a></li>
<li><a href="#3">3. 详细的尺度定律推导</a></li>
<li><a href="#4">4. 梯度尺度定律的深入推导</a></li>
<li><a href="#5">5. 学习率迁移的数学证明</a></li>
<li><a href="#6-vs">6. 特征学习 vs 懒惰学习的理论分析</a></li>
<li><a href="#7">7. 初始化方差选择的理论原则</a></li>
<li><a href="#8-layernorm">8. 残差连接和LayerNorm的影响</a></li>
<li><a href="#9-p">9. 多头注意力机制的μP</a></li>
<li><a href="#10">10. 实验验证：理论与实践的对比</a></li>
<li><a href="#11-tensor-programs">11. 高级主题：Tensor Programs</a></li>
<li><a href="#12">12. 开放问题与未来方向</a></li>
<li><a href="#_11">总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>