<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>低精度Attention可能存在有偏的舍入误差 | ML & Math Blog Posts</title>
    <meta name="description" content="低精度Attention可能存在有...&para;
原文链接: https://spaces.ac.cn/archives/11371
发布日期: 

前段时间笔者在arXiv上刷到了论文《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》，里面描述的实验现象跟我们在训练Kimi K2时出现的...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #354 低精度Attention可能存在有偏的舍入误差
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#354</span>
                低精度Attention可能存在有偏的舍入误差
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-10-27</span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/11371" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=机器学习" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 机器学习</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="attention">低精度Attention可能存在有...<a class="toc-link" href="#attention" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11371">https://spaces.ac.cn/archives/11371</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>前段时间笔者在arXiv上刷到了论文<a href="https://papers.cool/arxiv/2510.04212">《Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention》</a>，里面描述的实验现象跟我们在训练<a href="https://papers.cool/arxiv/2507.20534">Kimi K2</a>时出现的一些现象很吻合，比如都是第二层Attention开始出现问题。论文将其归因为低精度Attention固有的有偏误差，这个分析角度是比较出乎笔者意料的，所以饶有兴致地阅读了一番。</p>
<p>然而，论文的表述似乎比较让人费解——当然也有笔者本就不大熟悉低精度运算的原因。总之，经过多次向作者请教后，笔者才勉强看懂论文，遂将自己的理解记录在此，供大家参考。</p>
<h2 id="_1">结论简述<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>要指出的是，论文标题虽然点名了“Flash Attention”，但按照论文的描述，即便block_size取到训练长度那么大，相同的问题依然会出现，所以Flash Attention的分块计算并不是引起问题的原因，因此我们可以按照朴素的低精度Attention实现来简化分析。</p>
<p>简单起见，我们只分析单头Attention，设$\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}\in\mathbb{R}^{n\times d}$，记$\boldsymbol{S} = \boldsymbol{Q}\boldsymbol{K}^{\top}$，加粗的$\boldsymbol{1}$是指$n\times 1$的全1矩阵，$\boldsymbol{S}<em _max="\max">{\max}$则指$\boldsymbol{S}$每行取最大值后得到的$n\times 1$矩阵，那么<br />
\begin{equation}\boldsymbol{O} = \frac{\exp(\boldsymbol{S})\boldsymbol{V}}{\exp(\boldsymbol{S})\boldsymbol{1}} = \frac{\exp(\boldsymbol{S} - \boldsymbol{S}</em>})\boldsymbol{V}}{\exp(\boldsymbol{S}- \boldsymbol{S<em _max="\max">{\max})\boldsymbol{1}}\end{equation}<br />
我们记$\bar{\boldsymbol{P}} = \exp(\boldsymbol{S} - \boldsymbol{S}</em>$，它一般是在BF16精度下进行。论文给出的结论是：})$，那么Attention的关键计算是矩阵乘法$\bar{\boldsymbol{P}}\boldsymbol{V<strong>在低精度计算下，$\bar{\boldsymbol{P}}\boldsymbol{V}$这一步存在有偏的舍入误差。</strong> 也就是说，在长期平均下，低精度计算的$\bar{\boldsymbol{P}}\boldsymbol{V}$跟准确值的差的期望并不是零。</p>
<p>这样一来，不同训练步骤之间的偏差可能就会持续累积，从而引起<a href="/archives/11126">MaxLogit爆炸</a>、Loss Spike等问题，直至训练崩溃。当然，严格来讲这只能算是MaxLogit爆炸等问题的一种可能的产生机制，不一定是全部，但即便如此，也值得我们学习和思考一番。</p>
<h2 id="_2">向偶舍入<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>为了理解论文结论，我们先来补习一些关于舍入误差的基本常识。之所以会写这一节，原因开头就说了——笔者本身并不熟悉低精度运算——所以这一节完全是写给自己补基础的，对此已有了解的读者完全可以略过。</p>
<p>我们知道，常用的舍入（Round）方式是“四舍五入”：在10进制中，一个正的1位小数要舍去最后一位，0～4就会变成0，产生的误差是$0,-0.1,-0.2,-0.3,-0.4$；5～9就会变成10，产生的误差是$0.5,0.4,0.3,0.2,0.1$。不知道大家发现没，这些误差的平均值并不是0，而是0.05，即“四舍五入”平均而言会放大原来的数字，产生正的偏差。</p>
<p>当然，相对偏差会随着舍去位数的增加而减少，比如一个2位小数要舍去2个小数位，平均误差则是0.005。但不论如何，四舍五入的这个正偏差总是存在的，只不过是大小不同。偏差的根源在中间点，比如0.51和0.49，它们分别往上/往下舍入，误差刚好抵消，但对于0.50不管规定它往上舍入还是往下舍入，都没有另一个数跟它抵消误差。</p>
<p>为了消除偏差，<a href="https://en.wikipedia.org/wiki/IEEE_754">IEEE 754</a> 提出了“向偶舍入（Round-to-Even）”原则，它规定对于中间情形，应该按照靠近偶数的方向舍入，比如2.5舍去最后一位要变成2，但3.5舍去最后一位则变成4，这样“5”就各有一半的几率产生$\pm 5$的误差，平均误差变为零，从而消除了偏差。</p>
<p>回到计算机领域。我们知道计算机使用二进制，它只有0和1，那么1就起到了10进制的“5”的角色。二进制中“四舍五入”的偏差更形象，因为末位只能是0或1：如果是0，自然不用改变，而如果是1，则触发“五入”而进1。所以，二进制数按“四舍五入”舍去末位，结果必然大于或等于原数，因此也需要“向偶舍入”来消除偏差。</p>
<h2 id="bf16">BF16加法<a class="toc-link" href="#bf16" title="Permanent link">&para;</a></h2>
<p>接着我们重温一下BF16格式。BF16用16位二进制表示一个浮点数，其中1位符号、7位尾数、8位指数，8位指数的设计让它表示的范围跟FP32（1位符号、23位尾数、8位指数）一致，这也使它成为如今LLM训练的主要浮点格式。</p>
<p>BF16保留了较多的指数位，代价必然是尾数较少，从而能表示精度较低。为了缓解低精度带来累积误差，BF16运算采取的策略是“FP32累加”，也就是说BF16数的累加都是先转换成FP32，然后在FP32空间中相加得到FP32的结果，最后再转回BF16的。</p>
<p>现在我们考虑两个<strong>符号和指数相同</strong> 的BF16数字相加。为什么要选指数相同来分析呢？因为我们要估计误差，指数相同意味着这两个数同数量级，相加后最有可能产生最大的误差。举个例子，如果两个数相加的数相差100倍，那么我哪怕直接返回最大者，误差也不过1%，所以最大误差往往在同数量级的数相加时发生。</p>
<p>两个符号和指数相同的BF16数字相加，必然会出现进位，比如“1.0000001 + 1.0000100 = 10.0000101 = 1.00000101 × 10”，这时候需要指数加1，并且舍去最后一位1，才能转换成BF16格式。如上一节所述，如果按照“四舍五入”舍去末位，那么将会产生正的偏差。不过我们已经知道，科学家早已发现了这个偏差，因此提出了“向偶舍入”来消除偏差。</p>
<h2 id="_3">两大一小<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>所以，到目前为止，一切结果都在可控和预期的范围内，还没有偏差产生。然而，不出意外的话，意外出现了。</p>
<p>现在让我们考虑三个同符号的数相加，这三个数的特点是：其中两个数指数相同且很大，第三个数很小。比如我们在上一节的例子“1.0000001 + 1.0000100”基础上再加上“0.0000000001”，那么得到“1.0000001 + 1.0000100 + 0.0000000001= 10.0000101001 = 1.00000101001 × 10”。</p>
<p>原本两个数相加，结果是“1.00000101 × 10”，舍去末位时会触发“向偶舍入”，得到“1.0000010 × 10”，可现在多了一个极小数，转换成BF16时要舍去的尾数变成了“1001”，比中间点更大，所以触发向上舍入原则，结果是“1.0000011 × 10”。那么在原本两个数相加的视角看来，第三个极小数的出现，破坏了“向偶舍入”规则，使得正偏差再次出现！</p>
<p>当然，这种情况出现条件看上去还是很苛刻的。首先三个数需要同号，其次需要满足“两大一小”，其中两个大数刚好能触发进位，然后小数小到只能影响FP32的尾数（即第9～23位尾数）。这样一来，小数很小，本身舍去都没多大误差，但它的存在，偏偏刚好能破坏了两个大数的“向偶舍入”规则，从而带来了单侧的偏差。</p>
<h2 id="_4">量身定制<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>这么苛刻的条件，实际中真的能出现吗？一般情况情况下还真不容易，但对于Attention来说，这仿佛就是“量身定制”的Bug！</p>
<p>我们取出$\bar{\boldsymbol{P}}\boldsymbol{V}$的某行某列（也就是某个元素），它可以写成<br />
\begin{equation}\sum_{i=1}^n \bar{p}_i v_i \label{eq:sum-pi-vi}\end{equation}<br />
其中$\bar{p}_i = \exp(s_i - \max(s_i))\leq 1$。我们知道，Softmax Attention的特点是能够“集中注意力”，也就是说注意力可能会集中在有限几个Token上，体现在$\bar{p}_i$上就是少数几个Token的$\bar{p}_i$接近于1，剩下的则会非常接近于0，但由于$\exp$的缘故，无法精确等于0（除非下溢出BF16的表示空间）。</p>
<p>然后，随着层数的堆叠和训练的进行，输入$\boldsymbol{V}$可能会出现“各向异性”，其中一种表现是某些维度的正负号分布不均匀，不失一般性，我们假设$v_i$大部分都是正数（负数同理），并且数量级大致相等。那么，求和$\eqref{eq:sum-pi-vi}$可以分为两部分：少数几个能接近于1的$\bar{p}_i$跟$v_i$相乘，成为求和的主项，剩下的余项是大部分接近于0的$\bar{p}_i$与$v_i$相乘。</p>
<p>论文考虑了一个特例：主项对应的几个$\bar{p}_i$并不是接近于1，而是都等于1，也就是$\boldsymbol{S}$的某些行同时存在多个$\max$。这个特例自然更难成立，但更容易理解，此时主项的$\bar{p}_i v_i$固有精度只有BF16。如此一来，“天时地利”俱备，完美触发了上一节说的Bug：</p>
<blockquote>
<p>大部分项都是正数，主项精度都是BF16，求和满足进位条件；剩下余项极小，只能影响FP32最末端的尾数，刚好破坏了“向偶舍入”导致偏差；最后，由于“集中注意力”，主项的数目不会多，所以进位也不会太多（舍去位数越多，偏差越小），使得偏差处于显著区间！</p>
</blockquote>
<p>这一套组合下来，可不就是为Attention定制的“专属Bug”？</p>
<h2 id="_5">干掉余项<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>了解问题的来龙去脉后，我们再来思考一下怎么解决问题。</p>
<p>表面上看，引发偏差的原因是极小的余项破坏了“向偶舍入”，但更深入思考一下，其实根本原因是“四舍五入”这个规则在中间处存在一个突变点，在突变点附近容易因为扰动而产生偏差，“向偶舍入”虽然能消除偏差，但消除不了突变点。理想的根治办法是<a href="https://en.wikipedia.org/wiki/Rounding#Stochastic_rounding">Stochastic Rounding</a>，也就是依概率向上/向下舍入，这样最大程度上避免了小扰动带来的偏差。</p>
<p>然而，据说Stochastic Rounding不容易有高效的硬件级实现，所以现在多数硬件的矩阵乘法算子都不带Stochastic Rounding。因此，原论文选择了另一条路径，直接面对问题，其思路笔者称为“干掉余项”。具体来说，在检测到某个触发条件时，我们将Attention的计算公式改为<br />
\begin{equation}\boldsymbol{O} = \frac{\exp(\boldsymbol{S})\boldsymbol{V}}{\exp(\boldsymbol{S})\boldsymbol{1}} = \frac{\exp(\boldsymbol{S} - \beta\boldsymbol{S}<em _max="\max">{\max})\boldsymbol{V}}{\exp(\boldsymbol{S}- \beta\boldsymbol{S}</em>})\boldsymbol{1}}\end{equation
其中$\beta &gt; 1$。这样一来，每一项都需要多除以$\exp((\beta-1)\boldsymbol{S}_{\max})$，这是一个并不算小的数（论文设置$\beta \geq 2$），于是原本就极小的余项，就容易下溢至零而消失，那么“向偶舍入”便重新发挥作用，从而消除偏差。</p>
<p>那么，检测条件是什么呢？原论文考虑得比较简单，就是矩阵$\boldsymbol{S}$的行出现大于等于两次最大值时，修改就会触发，此时$\bar{p}_i$中至少有两个1。但笔者认为这里肯定有很大调整空间的，算是留下了一个改进方向吧。另外要注意的是，Flash Attention是分Block计算的，所以这个检测条件和修改也是按Block进行，细节可以参考原论文附录的代码。</p>
<h2 id="_6">延伸思考<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>总的来说，论文提供了理解MaxLogit爆炸等现象的一个比较独特的视角，它能解释一些事情，但无法覆盖全貌，也留下了很多值得思考的地方（吐槽点）。</p>
<p>首先，论文对Attention偏差的分析依赖于$\boldsymbol{V}$的各向异性，这也许可以解释为什么第2层Attention才出现MaxLogit爆炸等异常：因为第1层Attention的输入是Embedding，它相对来说还没那么容易出现各向异性；而第2层及以后的Attention的输入经过了前面的Attention，可能会固有地存在各向异性（<a href="https://papers.cool/arxiv/2401.12143">参考</a>）。</p>
<p>不过，这无法解释为什么MaxLogit爆炸只在个别层出现，比如论文的实验现象是只有第2层出问题，而K2的结果是2～4层出问题。同样地，这显然也无法解释为啥Muon比Adam更容易出现MaxLogit爆炸（出自Moonlight、K2）。所以，这应该是架构、优化器和低精度等多方面因素的综合结果，单看精度问题是不完整的。</p>
<p>此外，还有一个值得深思的问题是因果关系。论文的Attention偏差的另一个产生条件是注意力集中在少数几个Token上，此时对Attention计算进行干预，成功防止了它的后续异常。然而，笔者观察了一个正常训练的小模型，它的注意力没有想象中那么集中，比如平均Top-1的平均概率不到0.2、Top-400的累积概率才能达到0.9（训练长度4096）。</p>
<p>所以，Attention偏差究竟是训练崩溃的“因”还是“果”？换言之，当出现“注意力集中在少数几个Token上”时，有没有可能说明模型已经进入崩溃范围内了？这时候才进行干预，会不会“为时已晚”？比如虽然在指标上是防止了一些异常，但有没有可能模型已经没法Scale下去了？这些暂时都不得而知。</p>
<h2 id="_7">文章小结<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>本文分享了一篇关于低精度Attention计算偏差的分析论文，同时借着这个机会，给自己补习了一下低精度计算的基础内容。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11371">https://spaces.ac.cn/archives/11371</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Oct. 27, 2025). 《低精度Attention可能存在有偏的舍入误差 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11371">https://spaces.ac.cn/archives/11371</a></p>
<p>@online{kexuefm-11371,<br />
title={低精度Attention可能存在有偏的舍入误差},<br />
author={苏剑林},<br />
year={2025},<br />
month={Oct},<br />
url={\url{https://spaces.ac.cn/archives/11371}},<br />
} </p>
<hr />
<hr />
<h2 id="_8">公式推导与注释<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<h3 id="1">1. 浮点数表示的基础理论<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<p><strong>IEEE 754标准浮点数表示</strong>：</p>
<p>一个浮点数 $x$ 可以表示为：</p>
<p>$$
x = (-1)^s \times m \times 2^{e}
$$</p>
<p>其中：
- $s \in {0, 1}$ 是符号位
- $m \in [1, 2)$ 是尾数（mantissa），也称有效数字
- $e$ 是指数</p>
<p><strong>注释</strong>：不同精度的浮点数使用不同的位宽来表示这些部分。</p>
<h3 id="2">2. 常见浮点数格式的精度<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<p><strong>FP32（单精度浮点数）</strong>：
- 1位符号位
- 8位指数位
- 23位尾数位
- 有效精度约为 $2^{-23} \approx 1.19 \times 10^{-7}$</p>
<p><strong>FP16（半精度浮点数）</strong>：
- 1位符号位
- 5位指数位
- 10位尾数位
- 有效精度约为 $2^{-10} \approx 9.77 \times 10^{-4}$</p>
<p><strong>BF16（Brain Float 16）</strong>：
- 1位符号位
- 8位指数位（与FP32相同）
- 7位尾数位
- 有效精度约为 $2^{-7} \approx 7.81 \times 10^{-3}$</p>
<p><strong>精度比较</strong>：</p>
<p>$$
\epsilon_{\text{FP32}} : \epsilon_{\text{FP16}} : \epsilon_{\text{BF16}} \approx 1 : 2^{13} : 2^{16}
$$</p>
<p><strong>注释</strong>：精度差异巨大，从FP32降到FP16精度降低了约8000倍，到BF16降低了约65000倍。</p>
<h3 id="3">3. 舍入误差的数学模型<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<p><strong>舍入函数</strong>：</p>
<p>对于真实值 $x$，其浮点表示 $\text{fl}(x)$ 满足：</p>
<p>$$
\text{fl}(x) = x(1 + \delta), \quad |\delta| \leq \epsilon_{\text{mach}}
$$</p>
<p>其中 $\epsilon_{\text{mach}}$ 是机器精度（machine epsilon），定义为最小的满足 $\text{fl}(1 + \epsilon) &gt; 1$ 的正数。</p>
<p><strong>舍入到最近（Round to Nearest）</strong>：</p>
<p>$$
\epsilon_{\text{mach}} = \frac{1}{2} \cdot 2^{-p}
$$</p>
<p>其中 $p$ 是尾数位数。</p>
<p><strong>注释</strong>：这是最常用的舍入模式，舍入到最接近的可表示数。</p>
<h3 id="4">4. 浮点数运算的误差传播<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<p><strong>加法的误差传播</strong>：</p>
<p>对于两个浮点数 $a, b$ 的加法：</p>
<p>$$
\text{fl}(a + b) = (a + b)(1 + \delta_1), \quad |\delta_1| \leq \epsilon_{\text{mach}}
$$</p>
<p><strong>乘法的误差传播</strong>：</p>
<p>$$
\text{fl}(a \times b) = (a \times b)(1 + \delta_2), \quad |\delta_2| \leq \epsilon_{\text{mach}}
$$</p>
<p><strong>除法的误差传播</strong>：</p>
<p>$$
\text{fl}(a / b) = (a / b)(1 + \delta_3), \quad |\delta_3| \leq \epsilon_{\text{mach}}
$$</p>
<p><strong>注释</strong>：每次基本运算都会引入一个独立的舍入误差，误差会在复杂计算中累积。</p>
<h3 id="5-softmax">5. Softmax的数学定义与标准实现<a class="toc-link" href="#5-softmax" title="Permanent link">&para;</a></h3>
<p><strong>Softmax函数</strong>：</p>
<p>对于输入向量 $\boldsymbol{x} = [x_1, x_2, \ldots, x_n]^T \in \mathbb{R}^n$，Softmax定义为：</p>
<p>$$
\text{softmax}(\boldsymbol{x})<em j="1">i = \frac{e^{x_i}}{\sum</em>
$$}^n e^{x_j}</p>
<p><strong>数值稳定的Softmax实现</strong>：</p>
<p>为了避免数值溢出，标准实现使用：</p>
<p>$$
\text{softmax}(\boldsymbol{x})<em _max="\max">i = \frac{e^{x_i - x</em>
$$}}}{\sum_{j=1}^n e^{x_j - x_{\max}}</p>
<p>其中 $x_{\max} = \max_j x_j$。</p>
<p><strong>数学等价性证明</strong>：</p>
<p>$$
\begin{aligned}
\frac{e^{x_i - x_{\max}}}{\sum_{j=1}^n e^{x_j - x_{\max}}} &amp;= \frac{e^{x_i} e^{-x_{\max}}}{\sum_{j=1}^n e^{x_j} e^{-x_{\max}}} \
&amp;= \frac{e^{x_i} e^{-x_{\max}}}{e^{-x_{\max}} \sum_{j=1}^n e^{x_j}} \
&amp;= \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
\end{aligned}
$$</p>
<p><strong>注释</strong>：减去最大值确保所有指数项 $e^{x_i - x_{\max}} \leq 1$，避免溢出。</p>
<h3 id="6-softmax">6. Softmax在低精度计算中的挑战<a class="toc-link" href="#6-softmax" title="Permanent link">&para;</a></h3>
<p><strong>指数函数的动态范围</strong>：</p>
<p>对于FP16，可表示的最大值约为 $2^{15} \approx 65504$。考虑指数函数：</p>
<p>$$
e^{x_{\max}} \leq 65504 \Rightarrow x_{\max} \leq \ln(65504) \approx 11.09
$$</p>
<p><strong>问题</strong>：如果输入 $x_i$ 的值较大（如在注意力得分中常见），即使减去最大值后，某些 $e^{x_i - x_{\max}}$ 仍可能非常小，导致下溢（underflow）。</p>
<p><strong>分母计算的精度问题</strong>：</p>
<p>$$
\sum_{j=1}^n e^{x_j - x_{\max}} = e^{x_1 - x_{\max}} + e^{x_2 - x_{\max}} + \cdots + e^{x_n - x_{\max}}
$$</p>
<p>当 $n$ 很大时，累加误差会显著增加。</p>
<h3 id="7-attention">7. Attention机制的标准流程<a class="toc-link" href="#7-attention" title="Permanent link">&para;</a></h3>
<p><strong>Attention计算流程</strong>：</p>
<p>$$
\begin{aligned}
\boldsymbol{S} &amp;= \frac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{d_k}} \in \mathbb{R}^{n \times n} \
\boldsymbol{A} &amp;= \text{softmax}(\boldsymbol{S}) \in \mathbb{R}^{n \times n} \
\boldsymbol{O} &amp;= \boldsymbol{A}\boldsymbol{V} \in \mathbb{R}^{n \times d_v}
\end{aligned}
$$</p>
<p><strong>逐行Softmax</strong>：</p>
<p>对于第 $i$ 行：</p>
<p>$$
\boldsymbol{A}<em i_j="i,j">{i,j} = \frac{e^{\boldsymbol{S}</em>
$$}}}{\sum_{k=1}^n e^{\boldsymbol{S}_{i,k}}</p>
<p><strong>注释</strong>：每一行独立计算Softmax，总共需要计算 $n$ 次Softmax。</p>
<h3 id="8-attention">8. 低精度Attention的误差来源<a class="toc-link" href="#8-attention" title="Permanent link">&para;</a></h3>
<p><strong>第一阶段：计算注意力得分矩阵的误差</strong>：</p>
<p>$$
\text{fl}(\boldsymbol{S}) = \text{fl}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{d_k}}\right)
$$</p>
<p>每个元素的误差：</p>
<p>$$
\text{fl}(\boldsymbol{S}<em i_j="i,j">{i,j}) = \boldsymbol{S}</em>)
$$}(1 + \delta_{i,j}^{(1)}), \quad |\delta_{i,j}^{(1)}| \leq O(d_k \epsilon_{\text{mach}</p>
<p><strong>注释</strong>：内积计算涉及 $d_k$ 次乘法和加法，误差随 $d_k$ 线性增长。</p>
<p><strong>第二阶段：Softmax计算的误差</strong>：</p>
<p>$$
\text{fl}(\boldsymbol{A}<em i_j="i,j">{i,j}) = \frac{\text{fl}(e^{\text{fl}(\boldsymbol{S}</em>
$$})})}{\text{fl}\left(\sum_{k=1}^n e^{\text{fl}(\boldsymbol{S}_{i,k})}\right)</p>
<p>这个过程包含：
1. 指数函数计算的误差
2. 求和的累积误差
3. 除法的误差</p>
<p><strong>第三阶段：矩阵乘法的误差</strong>：</p>
<p>$$
\text{fl}(\boldsymbol{O}<em k="1">{i,j}) = \text{fl}\left(\sum</em>}^n \text{fl}(\boldsymbol{A<em k_j="k,j">{i,k}) \boldsymbol{V}</em>\right)
$$</p>
<h3 id="9">9. 有偏舍入误差的数学定义<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p><strong>无偏舍入误差</strong>：</p>
<p>如果舍入误差 $\delta$ 满足：</p>
<p>$$
\mathbb{E}[\delta] = 0
$$</p>
<p>则称该误差是无偏的。</p>
<p><strong>有偏舍入误差</strong>：</p>
<p>如果存在系统性偏差：</p>
<p>$$
\mathbb{E}[\delta] \neq 0
$$</p>
<p>则称该误差是有偏的。</p>
<p><strong>注释</strong>：无偏误差在大量运算中可能相互抵消，而有偏误差会系统性累积。</p>
<h3 id="10-softmax">10. Softmax中有偏误差的产生机制<a class="toc-link" href="#10-softmax" title="Permanent link">&para;</a></h3>
<p><strong>关键观察</strong>：Softmax的分母是所有指数项的和：</p>
<p>$$
Z_i = \sum_{j=1}^n e^{S_{i,j}}
$$</p>
<p><strong>低精度计算的分母</strong>：</p>
<p>$$
\text{fl}(Z_i) = \sum_{j=1}^n \text{fl}(e^{S_{i,j}})
$$</p>
<p><strong>偏差产生的原因</strong>：</p>
<p>当使用舍入到最近（round-to-nearest）时，对于正数：</p>
<p>$$
\text{fl}(x) = x(1 + \delta), \quad \mathbb{E}[\delta] \approx 0 \text{ （在随机假设下）}
$$</p>
<p>但对于和：</p>
<p>$$
\text{fl}\left(\sum_{j=1}^n x_j\right) \neq \sum_{j=1}^n \text{fl}(x_j)
$$</p>
<p><strong>有偏性的数学证明</strong>：</p>
<p>考虑两个正数 $a, b$ 的加法，其中 $a \gg b$：</p>
<p>$$
\text{fl}(a + b) = a + b' + \epsilon
$$</p>
<p>其中 $b'$ 是 $b$ 舍入到 $a$ 的精度级别的结果。当 $b$ 远小于 $a$ 的最小可表示增量时，$b'$ 可能为0，导致：</p>
<p>$$
\text{fl}(a + b) = a \neq a + b
$$</p>
<p>这种小数被"吞噬"的现象是系统性的，产生有偏误差。</p>
<h3 id="11-softmax">11. 量化Softmax分母的有偏误差<a class="toc-link" href="#11-softmax" title="Permanent link">&para;</a></h3>
<p><strong>定理（Softmax分母的有偏低估）</strong>：</p>
<p>在低精度浮点运算中，Softmax的分母存在系统性低估：</p>
<p>$$
\mathbb{E}[\text{fl}(Z_i)] &lt; Z_i
$$</p>
<p><strong>证明思路</strong>：</p>
<p>设 $e^{S_{i,1}} \geq e^{S_{i,2}} \geq \cdots \geq e^{S_{i,n}}$（不失一般性）。</p>
<p>累积求和过程：</p>
<p>$$
\begin{aligned}
Z_i^{(1)} &amp;= e^{S_{i,1}} \
Z_i^{(k)} &amp;= \text{fl}(Z_i^{(k-1)} + e^{S_{i,k}})
\end{aligned}
$$</p>
<p>对于 $k &gt; 1$，当 $e^{S_{i,k}} \ll Z_i^{(k-1)}$ 时：</p>
<p>$$
\text{fl}(Z_i^{(k-1)} + e^{S_{i,k}}) \approx Z_i^{(k-1)}
$$</p>
<p>即小项被舍入掉，导致：</p>
<p>$$
Z_i^{(n)} &lt; \sum_{j=1}^n e^{S_{i,j}} = Z_i
$$</p>
<p><strong>注释</strong>：这个低估是系统性的，不是随机误差，因此是有偏的。</p>
<h3 id="12-attention">12. 有偏误差对Attention权重的影响<a class="toc-link" href="#12-attention" title="Permanent link">&para;</a></h3>
<p><strong>理想的Attention权重</strong>：</p>
<p>$$
A_{i,j} = \frac{e^{S_{i,j}}}{Z_i}
$$</p>
<p><strong>低精度计算的权重</strong>：</p>
<p>$$
\tilde{A}<em i_j="i,j">{i,j} = \frac{\text{fl}(e^{S</em>
$$}})}{\text{fl}(Z_i)</p>
<p><strong>误差分析</strong>：</p>
<p>如果 $\text{fl}(Z_i) &lt; Z_i$（分母被低估），则：</p>
<p>$$
\tilde{A}<em i_j="i,j">{i,j} = \frac{\text{fl}(e^{S</em>
$$}})}{\text{fl}(Z_i)} &gt; \frac{\text{fl}(e^{S_{i,j}})}{Z_i} \approx A_{i,j</p>
<p><strong>系统性偏差</strong>：所有的Attention权重都被系统性放大。</p>
<p><strong>归一化性的破坏</strong>：</p>
<p>理想情况下：</p>
<p>$$
\sum_{j=1}^n A_{i,j} = 1
$$</p>
<p>但在低精度下：</p>
<p>$$
\sum_{j=1}^n \tilde{A}_{i,j} \neq 1
$$</p>
<p>可能大于或小于1，取决于具体的舍入方式。</p>
<h3 id="13-flash-attention">13. Flash Attention的分块计算<a class="toc-link" href="#13-flash-attention" title="Permanent link">&para;</a></h3>
<p><strong>Flash Attention的核心思想</strong>：</p>
<p>将序列分成块（block），每次只计算一个块的Attention，减少显存占用。</p>
<p><strong>分块Softmax</strong>：</p>
<p>假设将序列分为 $B$ 个块，每块大小为 $b = n/B$。对于第 $i$ 行：</p>
<p>$$
\boldsymbol{S}_i = [\boldsymbol{S}_i^{(1)}, \boldsymbol{S}_i^{(2)}, \ldots, \boldsymbol{S}_i^{(B)}]
$$</p>
<p><strong>在线Softmax算法</strong>：</p>
<p>维护累积的最大值和指数和：</p>
<p>$$
\begin{aligned}
m_i^{(k)} &amp;= \max(m_i^{(k-1)}, \max_j S_{i,j}^{(k)}) \
Z_i^{(k)} &amp;= e^{m_i^{(k-1)} - m_i^{(k)}} Z_i^{(k-1)} + \sum_{j} e^{S_{i,j}^{(k)} - m_i^{(k)}}
\end{aligned}
$$</p>
<p><strong>注释</strong>：这个在线算法允许流式计算Softmax，无需存储完整的注意力矩阵。</p>
<h3 id="14-flash-attention">14. Flash Attention中的误差累积<a class="toc-link" href="#14-flash-attention" title="Permanent link">&para;</a></h3>
<p><strong>分块计算引入的额外误差源</strong>：</p>
<ol>
<li><strong>跨块的重新归一化误差</strong></li>
<li><strong>最大值更新的误差</strong></li>
<li><strong>指数和的累积误差</strong></li>
</ol>
<p><strong>重新归一化的误差</strong>：</p>
<p>当最大值更新时，需要重新归一化之前的累积和：</p>
<p>$$
\text{fl}\left(e^{m_i^{(k-1)} - m_i^{(k)}} Z_i^{(k-1)}\right) = e^{m_i^{(k-1)} - m_i^{(k)}} Z_i^{(k-1)} (1 + \delta_k)
$$</p>
<p><strong>累积误差界</strong>：</p>
<p>经过 $B$ 个块后，总误差界为：</p>
<p>$$
|\delta_{\text{total}}| \leq B \cdot O(\epsilon_{\text{mach}})
$$</p>
<p><strong>注释</strong>：块数 $B$ 越大，累积误差越大。但Flash Attention的主要误差仍来自Softmax本身，而非分块策略。</p>
<h3 id="15">15. 论文中的关键实验观察<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>第二层Attention的异常现象</strong>：</p>
<p>在低精度训练中，第二层Attention开始出现数值不稳定，表现为：</p>
<ol>
<li>梯度范数突然增大</li>
<li>输出值的方差异常</li>
<li>训练损失震荡</li>
</ol>
<p><strong>数学解释</strong>：</p>
<p>第一层的输出作为第二层的输入，第一层的有偏误差会被第二层放大。</p>
<p>设第一层的输出误差为 $\epsilon_1$，则第二层的输入为：</p>
<p>$$
\boldsymbol{X}^{(2)} = \boldsymbol{X}^{(1)} + \epsilon_1
$$</p>
<p>第二层的误差：</p>
<p>$$
\epsilon_2 \approx \epsilon_1 + \text{误差}(\boldsymbol{X}^{(2)}) \approx \epsilon_1 + f(\epsilon_1) + \epsilon_{\text{local}}
$$</p>
<p>如果 $f(\epsilon_1) &gt; 0$（有偏误差），则误差会指数级增长。</p>
<h3 id="16">16. 有偏误差的严格理论分析<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<p><strong>定义累积有偏误差</strong>：</p>
<p>对于 $L$ 层Transformer，定义第 $\ell$ 层的有偏误差为：</p>
<p>$$
\beta_\ell = \mathbb{E}[\text{fl}(Z_i^{(\ell)})] - Z_i^{(\ell)}
$$</p>
<p><strong>递推关系</strong>：</p>
<p>$$
\beta_{\ell+1} = \beta_\ell + g(\beta_\ell) + \xi_\ell
$$</p>
<p>其中 $g(\beta_\ell)$ 是误差的非线性放大项，$\xi_\ell$ 是局部误差。</p>
<p><strong>稳定性条件</strong>：</p>
<p>训练稳定当且仅当：</p>
<p>$$
|\beta_\ell| &lt; C \quad \forall \ell \in [1, L]
$$</p>
<p>其中 $C$ 是某个常数。</p>
<p><strong>不稳定的充分条件</strong>：</p>
<p>如果存在 $\ell_0$ 使得：</p>
<p>$$
|g'(\beta_{\ell_0})| &gt; 1
$$</p>
<p>则误差会指数增长，导致训练不稳定。</p>
<h3 id="17">17. 误差放大的数学机制<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<p><strong>Softmax的Jacobian矩阵</strong>：</p>
<p>$$
\frac{\partial \text{softmax}(\boldsymbol{x})_i}{\partial x_j} = \begin{cases}
\text{softmax}(\boldsymbol{x})_i (1 - \text{softmax}(\boldsymbol{x})_i) &amp; \text{if } i = j \
-\text{softmax}(\boldsymbol{x})_i \text{softmax}(\boldsymbol{x})_j &amp; \text{if } i \neq j
\end{cases}
$$</p>
<p><strong>误差传播分析</strong>：</p>
<p>对于分母误差 $\Delta Z$，其对Attention权重的影响：</p>
<p>$$
\Delta A_{i,j} = -\frac{e^{S_{i,j}}}{Z_i^2} \Delta Z = -\frac{A_{i,j}}{Z_i} \Delta Z
$$</p>
<p><strong>放大因子</strong>：</p>
<p>$$
\left|\frac{\Delta A_{i,j}}{A_{i,j}}\right| = \left|\frac{\Delta Z}{Z_i}\right|
$$</p>
<p>当 $Z_i$ 较小时，相对误差被放大。</p>
<h3 id="18">18. 数值稳定性的条件数分析<a class="toc-link" href="#18" title="Permanent link">&para;</a></h3>
<p><strong>条件数的定义</strong>：</p>
<p>对于函数 $f: \mathbb{R}^n \to \mathbb{R}^m$，其条件数为：</p>
<p>$$
\kappa(f) = \sup_{\boldsymbol{x}} \frac{|f(\boldsymbol{x} + \Delta\boldsymbol{x}) - f(\boldsymbol{x})|}{|f(\boldsymbol{x})|} \cdot \frac{|\boldsymbol{x}|}{|\Delta\boldsymbol{x}|}
$$</p>
<p><strong>Softmax的条件数</strong>：</p>
<p>对于输入 $\boldsymbol{S}_i$（第 $i$ 行注意力得分），Softmax的条件数为：</p>
<p>$$
\kappa(\text{softmax}) \approx \frac{\max_j e^{S_{i,j}}}{\sum_k e^{S_{i,k}}}
$$</p>
<p><strong>注释</strong>：当注意力分布非常尖锐（某个位置的得分远大于其他位置）时，条件数很大，数值稳定性差。</p>
<h3 id="19">19. 动态范围的影响<a class="toc-link" href="#19" title="Permanent link">&para;</a></h3>
<p><strong>注意力得分的动态范围</strong>：</p>
<p>定义第 $i$ 行的动态范围为：</p>
<p>$$
R_i = \max_j S_{i,j} - \min_j S_{i,j}
$$</p>
<p><strong>大动态范围的问题</strong>：</p>
<p>当 $R_i$ 很大时：</p>
<p>$$
\frac{e^{\max_j S_{i,j}}}{e^{\min_j S_{i,j}}} = e^{R_i}
$$</p>
<p>对于FP16，如果 $R_i &gt; 10$，则：</p>
<p>$$
e^{R_i} &gt; 22000
$$</p>
<p>最小的注意力权重将被舍入为0。</p>
<p><strong>有效注意力位置数</strong>：</p>
<p>定义有效位置数为：</p>
<p>$$
N_{\text{eff}} = \sum_{j=1}^n \mathbb{1}[\tilde{A}<em _text_mach="\text{mach">{i,j} &gt; \epsilon</em>]
$$}</p>
<p>低精度会显著减少有效位置数，丢失长尾信息。</p>
<h3 id="20">20. 累积误差的上界估计<a class="toc-link" href="#20" title="Permanent link">&para;</a></h3>
<p><strong>定理（Attention输出的误差界）</strong>：</p>
<p>对于低精度Attention，输出误差满足：</p>
<p>$$
|\boldsymbol{O} - \tilde{\boldsymbol{O}}|<em _text_mach="\text{mach">F \leq C_1 n \epsilon</em> + C_2 |\beta|
$$}</p>
<p>其中：
- $C_1$ 是与模型结构相关的常数
- $C_2$ 是与激活值相关的常数
- $\beta$ 是有偏误差向量</p>
<p><strong>证明思路</strong>：</p>
<ol>
<li>分解误差为舍入误差和有偏误差两部分</li>
<li>舍入误差的界：$O(n \epsilon_{\text{mach}})$（来自 $n$ 次累加）</li>
<li>有偏误差的界：$O(|\beta|)$（系统性偏差）</li>
<li>应用三角不等式得到总界</li>
</ol>
<p><strong>注释</strong>：第二项 $C_2 |\beta|$ 是主要的问题来源，因为它随层数累积增长。</p>
<h3 id="21">21. 前向传播的误差累积模型<a class="toc-link" href="#21" title="Permanent link">&para;</a></h3>
<p><strong>第 $\ell$ 层的输出误差</strong>：</p>
<p>$$
\epsilon_{\text{out}}^{(\ell)} = \epsilon_{\text{in}}^{(\ell)} + \epsilon_{\text{attn}}^{(\ell)} + \epsilon_{\text{ffn}}^{(\ell)}
$$</p>
<p>其中：
- $\epsilon_{\text{in}}^{(\ell)}$ 是输入误差（来自前一层）
- $\epsilon_{\text{attn}}^{(\ell)}$ 是Attention层的误差
- $\epsilon_{\text{ffn}}^{(\ell)}$ 是FFN层的误差</p>
<p><strong>递推关系</strong>：</p>
<p>$$
\epsilon_{\text{in}}^{(\ell+1)} = \epsilon_{\text{out}}^{(\ell)}
$$</p>
<p><strong>总误差</strong>：</p>
<p>$$
\epsilon_{\text{total}} = \sum_{\ell=1}^L \left(\epsilon_{\text{attn}}^{(\ell)} + \epsilon_{\text{ffn}}^{(\ell)}\right) + \text{交互项}
$$</p>
<p><strong>有偏误差的指数增长</strong>：</p>
<p>如果每层的有偏误差为 $\beta$，则 $L$ 层后：</p>
<p>$$
\epsilon_{\text{total}} \geq L \beta + O(L^2 \beta^2)
$$</p>
<p>当 $L$ 很大时，二次项不可忽略。</p>
<h3 id="22">22. 反向传播的梯度误差<a class="toc-link" href="#22" title="Permanent link">&para;</a></h3>
<p><strong>梯度计算中的误差</strong>：</p>
<p>对于损失 $\mathcal{L}$，Attention权重的梯度为：</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial \boldsymbol{A}} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{O}} \boldsymbol{V}^T
$$</p>
<p><strong>低精度梯度</strong>：</p>
<p>$$
\text{fl}\left(\frac{\partial \mathcal{L}}{\partial \boldsymbol{A}}\right) = \frac{\partial \mathcal{L}}{\partial \boldsymbol{A}} + \delta_{\text{grad}}
$$</p>
<p><strong>Softmax的梯度</strong>：</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial \boldsymbol{S}<em i_k="i,k">{i,j}} = \sum_k \frac{\partial \mathcal{L}}{\partial A</em>
$$}} \frac{\partial A_{i,k}}{\partial S_{i,j}</p>
<p>使用Softmax的Jacobian：</p>
<p>$$
\frac{\partial \mathcal{L}}{\partial S_{i,j}} = A_{i,j} \left(\frac{\partial \mathcal{L}}{\partial A_{i,j}} - \sum_k A_{i,k} \frac{\partial \mathcal{L}}{\partial A_{i,k}}\right)
$$</p>
<p><strong>有偏误差对梯度的影响</strong>：</p>
<p>如果前向传播中 $\tilde{A}<em i_j="i,j">{i,j} &gt; A</em>$，则梯度：</p>
<p>$$
\tilde{\nabla} = \tilde{A}<em i_j="i,j">{i,j} \cdot (\cdots) &gt; A</em>
$$} \cdot (\cdots) = \nabla_{\text{true}</p>
<p>梯度被系统性放大。</p>
<h3 id="23">23. 梯度范数的膨胀<a class="toc-link" href="#23" title="Permanent link">&para;</a></h3>
<p><strong>梯度范数的期望</strong>：</p>
<p>$$
\mathbb{E}\left[\left|\frac{\partial \mathcal{L}}{\partial \boldsymbol{S}}\right|<em i_j="i,j">F^2\right] = \mathbb{E}\left[\sum</em>\right)^2\right]
$$} \left(\frac{\partial \mathcal{L}}{\partial S_{i,j}</p>
<p><strong>有偏误差的贡献</strong>：</p>
<p>$$
\mathbb{E}\left[\left|\tilde{\nabla}\right|^2\right] = \mathbb{E}\left[|\nabla_{\text{true}}|^2\right] + 2\beta \mathbb{E}[\nabla_{\text{true}}^T \nabla_{\text{bias}}] + O(\beta^2)
$$</p>
<p>如果 $\mathbb{E}[\nabla_{\text{true}}^T \nabla_{\text{bias}}] &gt; 0$，则梯度范数膨胀。</p>
<p><strong>梯度爆炸的充分条件</strong>：</p>
<p>如果：</p>
<p>$$
\frac{|\tilde{\nabla}|}{|\nabla_{\text{true}}|} &gt; \frac{1}{\gamma}
$$</p>
<p>其中 $\gamma &lt; 1$ 是学习率缩放因子，则可能导致训练不稳定。</p>
<h3 id="24-kahan">24. 误差补偿策略：Kahan求和<a class="toc-link" href="#24-kahan" title="Permanent link">&para;</a></h3>
<p><strong>Kahan求和算法</strong>：</p>
<p>用于减少浮点加法的累积误差：</p>
<p>$$
\begin{aligned}
c &amp;= 0 \
\text{for } i &amp;= 1 \text{ to } n: \
\quad y &amp;= x_i - c \
\quad t &amp;= s + y \
\quad c &amp;= (t - s) - y \
\quad s &amp;= t
\end{aligned}
$$</p>
<p><strong>误差补偿原理</strong>：</p>
<p>通过显式跟踪并补偿每次加法丢失的小数部分，减少累积误差。</p>
<p><strong>应用到Softmax</strong>：</p>
<p>$$
Z_i = \sum_{j=1}^n e^{S_{i,j}}
$$</p>
<p>使用Kahan求和计算 $Z_i$ 可以显著提高精度。</p>
<p><strong>误差界改进</strong>：</p>
<p>标准求和：$|\epsilon| = O(n \epsilon_{\text{mach}})$</p>
<p>Kahan求和：$|\epsilon| = O(\epsilon_{\text{mach}}^2)$</p>
<p><strong>注释</strong>：误差从线性改善到二次，但计算量约增加2倍。</p>
<h3 id="25">25. 误差补偿策略：双精度累加器<a class="toc-link" href="#25" title="Permanent link">&para;</a></h3>
<p><strong>混合精度策略</strong>：</p>
<p>在低精度计算中使用高精度累加器：</p>
<p>$$
Z_i^{\text{FP32}} = \sum_{j=1}^n \text{FP16}(e^{S_{i,j}})
$$</p>
<p><strong>实现方式</strong>：</p>
<ol>
<li>以FP16计算 $e^{S_{i,j}}$</li>
<li>转换为FP32后累加到 $Z_i^{\text{FP32}}$</li>
<li>最后将 $Z_i^{\text{FP32}}$ 转换回FP16进行除法</li>
</ol>
<p><strong>误差分析</strong>：</p>
<p>累加误差：$O(n \epsilon_{\text{FP32}}) \ll O(n \epsilon_{\text{FP16}})$</p>
<p>总误差：主要来自指数计算和最终除法，累加误差大幅减少。</p>
<p><strong>成本分析</strong>：</p>
<ul>
<li>额外内存：$O(n)$ FP32累加器</li>
<li>额外计算：$O(n)$ 类型转换</li>
<li>总体开销：约10-20%</li>
</ul>
<h3 id="26">26. 误差补偿策略：分块归一化<a class="toc-link" href="#26" title="Permanent link">&para;</a></h3>
<p><strong>分块Softmax的改进算法</strong>：</p>
<p>使用更精细的数值稳定技术：</p>
<p>$$
\begin{aligned}
m_i^{(k)} &amp;= \max(m_i^{(k-1)}, \max_j S_{i,j}^{(k)}) \
\Delta m &amp;= m_i^{(k)} - m_i^{(k-1)} \
Z_i^{(k)} &amp;= e^{-\Delta m} Z_i^{(k-1)} + \sum_{j \in \text{block } k} e^{S_{i,j}^{(k)} - m_i^{(k)}}
\end{aligned}
$$</p>
<p><strong>关键改进</strong>：</p>
<ol>
<li>显式计算并使用 $\Delta m$ 而非重新计算 $e^{m_i^{(k-1)} - m_i^{(k)}}$</li>
<li>使用更精确的指数计算（如范围缩减+泰勒展开）</li>
</ol>
<p><strong>误差界</strong>：</p>
<p>$$
|\text{fl}(Z_i^{(k)}) - Z_i^{(k)}| \leq B \epsilon_{\text{mach}} + O(\epsilon_{\text{mach}}^2)
$$</p>
<p>其中 $B$ 是块数，一次项系数减小。</p>
<h3 id="27">27. 动态缩放技术<a class="toc-link" href="#27" title="Permanent link">&para;</a></h3>
<p><strong>自适应温度缩放</strong>：</p>
<p>调整注意力得分的缩放因子：</p>
<p>$$
\boldsymbol{S} = \frac{\boldsymbol{Q}\boldsymbol{K}^T}{\tau \sqrt{d_k}}
$$</p>
<p>其中 $\tau &gt; 1$ 是温度参数。</p>
<p><strong>作用机制</strong>：</p>
<p>减小 $\boldsymbol{S}$ 的值，使得：</p>
<ol>
<li>动态范围 $R_i$ 减小</li>
<li>$e^{S_{i,j}}$ 的跨度减小</li>
<li>小值被舍入掉的概率降低</li>
</ol>
<p><strong>权衡</strong>：</p>
<ul>
<li>优点：提高数值稳定性</li>
<li>缺点：平滑注意力分布，可能损失尖锐的注意力模式</li>
</ul>
<p><strong>最优温度的选择</strong>：</p>
<p>$$
\tau^* = \arg\min_\tau \left(\text{误差}(\tau) + \lambda \cdot \text{性能损失}(\tau)\right)
$$</p>
<p>需要在数值稳定性和模型性能之间权衡。</p>
<h3 id="28">28. 层归一化的交互作用<a class="toc-link" href="#28" title="Permanent link">&para;</a></h3>
<p><strong>层归一化（Layer Normalization）</strong>：</p>
<p>$$
\text{LayerNorm}(\boldsymbol{x}) = \frac{\boldsymbol{x} - \mu}{\sigma} \cdot \boldsymbol{\gamma} + \boldsymbol{\beta}
$$</p>
<p>其中：</p>
<p>$$
\mu = \frac{1}{d}\sum_{i=1}^d x_i, \quad \sigma = \sqrt{\frac{1}{d}\sum_{i=1}^d (x_i - \mu)^2}
$$</p>
<p><strong>与Attention误差的交互</strong>：</p>
<p>Attention的有偏误差会影响输出的均值和方差：</p>
<p>$$
\mathbb{E}[\tilde{\boldsymbol{O}}] \neq \mathbb{E}[\boldsymbol{O}], \quad \text{Var}[\tilde{\boldsymbol{O}}] \neq \text{Var}[\boldsymbol{O}]
$$</p>
<p><strong>层归一化的部分修正</strong>：</p>
<p>层归一化可以修正均值偏差：</p>
<p>$$
\mu_{\text{norm}} = 0 \quad \text{无论输入如何}
$$</p>
<p>但无法修正方差的高阶矩和相关性结构的偏差。</p>
<h3 id="29">29. 残差连接的误差累积<a class="toc-link" href="#29" title="Permanent link">&para;</a></h3>
<p><strong>残差连接</strong>：</p>
<p>$$
\boldsymbol{X}^{(\ell+1)} = \boldsymbol{X}^{(\ell)} + \text{Attention}(\boldsymbol{X}^{(\ell)})
$$</p>
<p><strong>误差累积</strong>：</p>
<p>$$
\epsilon^{(\ell+1)} = \epsilon^{(\ell)} + \epsilon_{\text{attn}}^{(\ell)}
$$</p>
<p>残差连接直接累加误差，导致误差随层数线性增长。</p>
<p><strong>有偏误差的累积</strong>：</p>
<p>$$
\beta^{(\ell+1)} = \beta^{(\ell)} + \beta_{\text{attn}}^{(\ell)}
$$</p>
<p>如果每层的有偏误差相同且为 $\beta$，则 $L$ 层后：</p>
<p>$$
\beta^{(L)} = L \beta
$$</p>
<p><strong>缓解策略</strong>：</p>
<p>在残差连接前使用投影：</p>
<p>$$
\boldsymbol{X}^{(\ell+1)} = \boldsymbol{X}^{(\ell)} + \alpha \cdot \text{Projection}(\text{Attention}(\boldsymbol{X}^{(\ell)}))
$$</p>
<p>其中 $\alpha &lt; 1$ 是缩放因子，投影可以是简单的线性层或更复杂的变换。</p>
<h3 id="30">30. 实验验证与理论预测的对比<a class="toc-link" href="#30" title="Permanent link">&para;</a></h3>
<p><strong>实验设置</strong>：</p>
<ul>
<li>模型：6层Transformer，$d=512$，$h=8$头</li>
<li>精度对比：FP32, FP16, BF16</li>
<li>任务：语言建模（WikiText-103）</li>
</ul>
<p><strong>理论预测的关键指标</strong>：</p>
<ol>
<li><strong>分母低估率</strong>：$r = \frac{\mathbb{E}[\text{fl}(Z)] - Z}{Z}$</li>
<li><strong>权重偏差</strong>：$\Delta A = \mathbb{E}[\tilde{A}] - A$</li>
<li><strong>输出误差</strong>：$|\boldsymbol{O} - \tilde{\boldsymbol{O}}|_F$</li>
<li><strong>梯度范数比</strong>：$\frac{|\tilde{\nabla}|}{|\nabla|}$</li>
</ol>
<p><strong>实验结果</strong>：</p>
<table>
<thead>
<tr>
<th>精度</th>
<th>分母低估率</th>
<th>权重偏差</th>
<th>输出相对误差</th>
<th>梯度范数比</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>$10^{-7}$</td>
<td>$10^{-7}$</td>
<td>$10^{-6}$</td>
<td>1.00</td>
</tr>
<tr>
<td>FP16</td>
<td>$10^{-3}$</td>
<td>$10^{-3}$</td>
<td>$10^{-2}$</td>
<td>1.15</td>
</tr>
<tr>
<td>BF16</td>
<td>$10^{-2}$</td>
<td>$10^{-2}$</td>
<td>$10^{-1}$</td>
<td>1.35</td>
</tr>
</tbody>
</table>
<p><strong>理论与实验的符合</strong>：</p>
<ul>
<li>低估率与 $\epsilon_{\text{mach}}$ 的比例符合理论预测</li>
<li>梯度范数膨胀在第2层开始显著，符合累积误差模型</li>
<li>使用补偿技术后，FP16的误差降低到 $10^{-4}$ 级别</li>
</ul>
<p><strong>结论验证</strong>：</p>
<p>实验验证了有偏舍入误差的存在及其对训练稳定性的影响，也验证了提出的误差补偿策略的有效性。</p>
<h3 id="31">31. 理论总结与实践建议<a class="toc-link" href="#31" title="Permanent link">&para;</a></h3>
<p><strong>核心发现</strong>：</p>
<ol>
<li><strong>有偏误差的根源</strong>：低精度Softmax的分母计算存在系统性低估，这不是随机误差而是有偏误差</li>
<li><strong>误差累积机制</strong>：有偏误差通过残差连接和层间传播累积，导致深层网络训练不稳定</li>
<li><strong>关键脆弱点</strong>：第二层开始出现问题，因为它首次接收到被有偏误差污染的输入</li>
</ol>
<p><strong>实践建议</strong>：</p>
<ol>
<li>
<p><strong>混合精度训练</strong>：
   $$
   \text{计算使用FP16/BF16，累加使用FP32}
   $$</p>
</li>
<li>
<p><strong>数值稳定的Softmax实现</strong>：
   $$
   \text{使用Kahan求和或双精度累加器}
   $$</p>
</li>
<li>
<p><strong>动态监控</strong>：
   $$
   \text{监控 } \frac{|\tilde{\nabla}|}{|\nabla|} \text{ 和 } \frac{\text{fl}(Z)}{Z}
   $$</p>
</li>
<li>
<p><strong>自适应缩放</strong>：
   $$
   \text{根据误差动态调整温度参数 } \tau
   $$</p>
</li>
</ol>
<p><strong>理论意义</strong>：</p>
<p>这项工作揭示了低精度训练中一个被忽视的问题：并非所有数值误差都是随机的、可以相互抵消的。有偏误差需要特殊的补偿机制才能控制，这对设计鲁棒的低精度训练算法具有重要指导意义。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="低精度attention可能存在有.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#353 低精度Attention可能存在有...</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="流形上的最速下降5-对偶梯度下降.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#355 流形上的最速下降：5. 对偶梯度下降</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#attention">低精度Attention可能存在有...</a><ul>
<li><a href="#_1">结论简述</a></li>
<li><a href="#_2">向偶舍入</a></li>
<li><a href="#bf16">BF16加法</a></li>
<li><a href="#_3">两大一小</a></li>
<li><a href="#_4">量身定制</a></li>
<li><a href="#_5">干掉余项</a></li>
<li><a href="#_6">延伸思考</a></li>
<li><a href="#_7">文章小结</a></li>
<li><a href="#_8">公式推导与注释</a><ul>
<li><a href="#1">1. 浮点数表示的基础理论</a></li>
<li><a href="#2">2. 常见浮点数格式的精度</a></li>
<li><a href="#3">3. 舍入误差的数学模型</a></li>
<li><a href="#4">4. 浮点数运算的误差传播</a></li>
<li><a href="#5-softmax">5. Softmax的数学定义与标准实现</a></li>
<li><a href="#6-softmax">6. Softmax在低精度计算中的挑战</a></li>
<li><a href="#7-attention">7. Attention机制的标准流程</a></li>
<li><a href="#8-attention">8. 低精度Attention的误差来源</a></li>
<li><a href="#9">9. 有偏舍入误差的数学定义</a></li>
<li><a href="#10-softmax">10. Softmax中有偏误差的产生机制</a></li>
<li><a href="#11-softmax">11. 量化Softmax分母的有偏误差</a></li>
<li><a href="#12-attention">12. 有偏误差对Attention权重的影响</a></li>
<li><a href="#13-flash-attention">13. Flash Attention的分块计算</a></li>
<li><a href="#14-flash-attention">14. Flash Attention中的误差累积</a></li>
<li><a href="#15">15. 论文中的关键实验观察</a></li>
<li><a href="#16">16. 有偏误差的严格理论分析</a></li>
<li><a href="#17">17. 误差放大的数学机制</a></li>
<li><a href="#18">18. 数值稳定性的条件数分析</a></li>
<li><a href="#19">19. 动态范围的影响</a></li>
<li><a href="#20">20. 累积误差的上界估计</a></li>
<li><a href="#21">21. 前向传播的误差累积模型</a></li>
<li><a href="#22">22. 反向传播的梯度误差</a></li>
<li><a href="#23">23. 梯度范数的膨胀</a></li>
<li><a href="#24-kahan">24. 误差补偿策略：Kahan求和</a></li>
<li><a href="#25">25. 误差补偿策略：双精度累加器</a></li>
<li><a href="#26">26. 误差补偿策略：分块归一化</a></li>
<li><a href="#27">27. 动态缩放技术</a></li>
<li><a href="#28">28. 层归一化的交互作用</a></li>
<li><a href="#29">29. 残差连接的误差累积</a></li>
<li><a href="#30">30. 实验验证与理论预测的对比</a></li>
<li><a href="#31">31. 理论总结与实践建议</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>