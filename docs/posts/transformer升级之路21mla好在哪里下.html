<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer升级之路：21、MLA好在哪里?（下） | ML & Math Blog Posts</title>
    <meta name="description" content="Transformer升级之路：21、MLA好在哪里?（下）&para;
原文链接: https://spaces.ac.cn/archives/11111
发布日期: 

在文章《Transformer升级之路：20、MLA好在哪里?（上）》中，我们对MLA相比常见MHA、GQA、MQA的一些变化分别做了消融实验，其中的变化包括“增大head_dims”、“Partial RoPE”和“KV共享...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=优化">优化</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #207 Transformer升级之路：21、MLA好在哪里?（下）
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#207</span>
                Transformer升级之路：21、MLA好在哪里?（下）
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-07-10</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="transformer21mla">Transformer升级之路：21、MLA好在哪里?（下）<a class="toc-link" href="#transformer21mla" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11111">https://spaces.ac.cn/archives/11111</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在文章<a href="/archives/10907">《Transformer升级之路：20、MLA好在哪里?（上）》</a>中，我们对<a href="/archives/10091">MLA</a>相比常见MHA、GQA、MQA的一些变化分别做了消融实验，其中的变化包括“增大head_dims”、“Partial RoPE”和“KV共享”，实验的初步结果是这三个变化很可能都是MLA效果优异的原因。</p>
<p>本文我们将从一个更加偏理论的角度出发，来理解MLA的成功之处。</p>
<h2 id="_1">部分旋转<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>首先，我们把最终的断言放在前面：</p>
<blockquote>
<p>在相同训练成本和推理成本下，MLA可能是效果最好的Full Attention变体。</p>
</blockquote>
<p>很明显，这个判断把MLA摆在了非常高的位置。这是在比较理想和简化的假设下，根据上一篇文章的实验结果以及本文接下来的理论分析所得的结论。由于实际的训练和推理存在诸多复杂的因素，所以该结论大概率会有所偏差，但我们至少可以得出，MLA应该是走在了正确的改进方向上。</p>
<p>MLA之所以能够表现出色，有一个非常大的前提，那就是部分旋转的<a href="/archives/10122">Partial RoPE</a>效果不逊色于甚至可能优于完全体的RoPE。这里的Partial RoPE可以有两种含义：一是我们对Attention的$\boldsymbol{Q}$、$\boldsymbol{K}$加RoPE时，可以只对小部份维度加，剩下的维度保持不变；二是我们可以考虑层间RoPE与NoPE交替出现，并且NoPE的层可以占多数。</p>
<p>说白了，RoPE可以只加“一点点”，但不能不加，完全不加的话效果不行。如果需要理论，笔者比较认同<a href="/archives/10122">《Transformer升级之路：18、RoPE的底数选择原则》</a>的解释，大致意思是Partial RoPE使得检索结果更兼顾位置与语义。此外，像<a href="https://papers.cool/arxiv/2503.02130">FoX</a>、<a href="https://papers.cool/arxiv/2410.17980">SBA</a>等新工作也体现出一定潜力，但对于MLA来说，这些变体就相当于NoPE，因此不改变结论。</p>
<p>“Partial RoPE效果不差”的结论，允许我们把Attention的主要计算复杂度放到NoPE部分上，这提供了更大的腾挪空间，MLA便是得益于此。</p>
<h2 id="_2">键值共享<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>Full Attention的变化大致上是从<a href="/archives/4765">MHA</a>、<a href="https://papers.cool/arxiv/1911.02150">MQA</a>、<a href="https://papers.cool/arxiv/2305.13245">GQA</a>然后到MLA，虽然MQA可以看作是GQA的特例，但按时间顺序来说确实是GQA在后。在MLA之后，还出现了<a href="https://papers.cool/arxiv/2412.19255">MFA</a>、<a href="https://papers.cool/arxiv/2501.06425">TPA</a>两个变体。这些变体本质上都是在尽量保持效果的前提下，尽可能压榨KV Cache以提高生成速度。</p>
<p>简单来说，Attention模型的复杂度可以分 <em>训练、Prefill和Decoding</em> 三部分，其中训练和Prefill是相似的，所以本质上是Prefill和Decoding两部分。Prefill是指模型处理输入、直至吐出第一个token的阶段，这部分我们下节再谈；Decoding是指Token by Token的生成阶段，它可以通过KV Cache机制来加速，但同时也导致了KV Cache大小几乎是Decoding速度的唯一瓶颈。</p>
<p>所以，压缩KV Cache就是提高Decoding速度。现在问大家一个问题：<strong>在NoPE背景下，给定KV Cache大小后，效果最好的Attention是什么呢？</strong> 如果不考虑参数量差异，只在单层MHA/GQA/MQA内讨论（TPA和MFA我们后面再补充讨论），那么答案将会是：</p>
<blockquote>
<p><strong>一个head_dims等于KV Cache大小、K和V共享的MQA。</strong></p>
</blockquote>
<p>看上去是不是让人意外？其实不难理解。因为MHA、MQA都可以看成是GQA的一个特例，所以我们只需要分析GQA，我们在<a href="/archives/10091">《缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA》</a>已经给出了，GQA可以重新表示成一个K、V拼接起来的模型：<br />
\begin{equation}\underbrace{\left[\boldsymbol{k}<em _boldsymbol_c="\boldsymbol{c">i^{(1)},\cdots,\boldsymbol{k}_i^{(g)},\boldsymbol{v}_i^{(1)},\cdots,\boldsymbol{v}_i^{(g)}\right]}</em><em _boldsymbol_W="\boldsymbol{W">i\in\mathbb{R}^{g(d_k+d_v)}} = \boldsymbol{x}_i \underbrace{\left[\boldsymbol{W}_k^{(1)},\cdots,\boldsymbol{W}_k^{(g)},\boldsymbol{W}_v^{(1)},\cdots,\boldsymbol{W}_v^{(g)}\right]}</em>}_c\in\mathbb{R}^{d\times g(d_k+d_v)}}\end{equation<br />
这里$g(d_k+d_v)$正是单个Token的KV Cache总大小。接着我们算Attention的时候，$\boldsymbol{c}$到$\boldsymbol{k},\boldsymbol{v}$的变换分别吸收到$\boldsymbol{W}_q$和$\boldsymbol{W}_o$那边去，那么就得到了一个K、V都是$\boldsymbol{c}$的MQA。所以说，“head_dims等于KV Cache大小、K和V共享的MQA”，实际上是给定KV Cache大小后MHA/GQA/MQA的“超集”，那么它自然是理论上效果最好的选择。</p>
<h2 id="_3">双重投影<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>综上所述，如果我们想要在相同Decoding速度下效果最优，那么应该训练一个指定head_dims的、KV共享的MQA，比如约定KV Cache不超过512，那么head_dims=512的、KV共享的MQA就是最佳选择。事实上，MLA在Decoding阶段正是KV共享的MQA（NoPE部分），这就是它走在正确方向上的体现之一。</p>
<p>然而，将head_dims升到512，Decoding是没问题，但训练和Prefill都很难接受，因为它们俩的瓶颈是计算，而影响计算速度的主要因素是num_heads和head_dims。为了保证效果，num_heads变动的空间不大，因此head_dims大小可以说是计算量的唯一指标，head_dims升到512意味着计算量要增加到原来的4倍（相比head_dims=128）。</p>
<p>现在再来问大家一个问题：<strong>同样在NoPE背景下，给定num_heads和head_dims后，效果最好的Attention是什么呢？</strong> 这个问题的答案我相信大家都能接受，那就是<strong>MHA</strong> ，因为它限制最少。所以，单从训练和Prefill成本来看，我们希望的是训练一个head_dims=128的MHA。</p>
<p>怎么调和Prefill与Decoding这两个不同的期望呢？这就是MLA的“大招”了，它通过两步投影得到K、V：先将输入投影到单个512维的向量，然后将该向量投影到多个128维的向量，然后利用“Attention + NoPE”固有的恒等变换性质，可以让模型在MHA-128和MQA-512间自由切换。</p>
<p>$$\require{cancel}\begin{array}{c|c}<br />
\text{训练/Prefill} &amp; \text{Decoding} \\<br />
\\<br />
\begin{gathered}<br />
\boldsymbol{o}<em i_leq="i\leq" t="t">t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\right] \\[10pt]<br />
\boldsymbol{o}_t^{(s)} = \frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)\boldsymbol{v}_i^{(s)}}{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{(s)}{}^{\top}\right)} \\[15pt]<br />
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(s)}\in\mathbb{R}^{d\times d_k}\\<br />
\boldsymbol{k}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(s)}\in\mathbb{R}^{d_c\times d_k} \\<br />
\boldsymbol{v}_i^{(s)} = \boldsymbol{c}_i\boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(s)}\in\mathbb{R}^{d_c\times d_v} \\[10pt]<br />
\boldsymbol{c}_i = \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c},\quad \boldsymbol{W}_c\in\mathbb{R}^{d\times d_c}<br />
\end{gathered}<br />
&amp;<br />
\begin{gathered}<br />
\boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}\boldsymbol{W}_v^{(1)}, \boldsymbol{o}_t^{(2)}\boldsymbol{W}_v^{(2)}, \cdots, \boldsymbol{o}_t^{(h)}\boldsymbol{W}_v^{(h)}\right] \\[10pt]<br />
\boldsymbol{o}_t^{(s)} = \frac{\sum</em>}\exp\left(\boldsymbol{q<em i_leq="i\leq" t="t">t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)\boldsymbol{v}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} }{\sum</em> \\[15pt]}\exp\left(\boldsymbol{q}_t^{(s)} \boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}}{}^{\top}\right)<br />
\boldsymbol{q}_i^{(s)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(s)}\boldsymbol{W}_k^{(s)}{}^{\top}\in\mathbb{R}^{d_c}\\<br />
\boldsymbol{k}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \boldsymbol{v}_i^{\color{#ccc}{\smash{\bcancel{(s)}}}} = \boldsymbol{c}_i= \boldsymbol{x}_i \boldsymbol{W}_c\in\mathbb{R}^{d_c}<br />
\end{gathered}<br />
\end{array}$$</p>
<h2 id="_4">总而言之<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>我们将前面的推理逻辑做个总结：</p>
<blockquote>
<p>1、大前提：Partial RoPE的效果不差于甚至可能优于RoPE，这使得我们可以把主要精力放在NoPE上；</p>
<p>2、Decoding主要瓶颈是KV Cache，理论效果最优的模型是head_dims=KV Cache、KV共享的MQA；</p>
<p>3、训练和Prefill的主要瓶颈都是head_dims，理论效果最优的模型是head_dims为期望值的MHA；</p>
<p>4、在NoPE前提下，Attention具有恒等变换性质，可以通过LoRA来尽可能地兼顾两个理想方向，这正好是MLA所做的。</p>
</blockquote>
<p>剩下的，就是给K拼接一个共享的低维RoPE，以最小的成本给MLA补充上位置信息，同时还“一箭双雕”：拼接RoPE的做法暗合了“Partial RoPE”，同时也增加了head_dims，这跟上一篇文章的结论相符。换句话说，有意或者无意之中使用了Partial RoPE和增加了head_dims，是MLA在极致压缩之下还能媲美MHA的主要原因。</p>
<p>从MQA的角度看，MLA是给Q加了rank=128的LoRA；从MHA的角度看，MLA是给K、V加了rank=512的LoRA。可以说，MLA是一场NoPE结合LoRA、MHA结合MQA的极致“魔术秀”，成功实现了Prefill和Decoding的“双向奔赴”。</p>
<p>当然，上述思考过程肯定有一些过于简化的地方。比如，实际的训练和推理还有诸多细节因素，笼统地归结为head_dims和KV Cache是不完全准确的，例如MQA在Decoding阶段无法TP（张量并行），这可能会带来新的效率问题；还有，分析过程中我们也没有特别注重参数量的对齐，比如在head_dims=128时我们也可以考虑增加Q、K、V的投影复杂度来提高性能，而不一定要增大head_dims；等等。</p>
<p>总之，上下两篇文章旨在提供一些实验和思考，来论证MLA在一定范围内的最优性。当然，MLA是DeepSeek首先提出的，第三方使用MLA总会给人一种复制DeepSeek的感觉，但在更好的变体出现之前，或者在发现严重的缺陷之前，MLA始终是一个相当有竞争力的选择，如果单纯是为了显示自己不“追随”DeepSeek而不用MLA，那是一个相当不明智的选择。</p>
<p>举个例子，现在Linear Attention和Softmax Attention的混合模型也体现出极大的竞争力，但如果我们将Linear Attention跟LLAMA使用的GQA8-128按3:1混合，那么KV Cache大致上降低到GQA8-128的1/4，然而MLA本身就能将KV Cache降低到GQA8-128的1/4了。</p>
<h2 id="_5">补充讨论<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>前面我们都在围绕MHA、GQA、MQA和MLA讨论，这一节我们来简单聊聊两个比较少谈及的Attention变体：TPA和MFA。</p>
<p>TPA全称是Tensor Product Attention，作者给它安了个Tensor Product的名字，显得比较“唬人”，实际上它是一个介乎GQA和MLA的中间产物。我们以目标KV Cache=512为例，TPA先投影得到一个512维向量，然后reshape为(4, 128)，然后分成两个(2,128)分别代表K Cache和V Cache。到目前为止，TPA的做法都跟GQA2-128一致。</p>
<p>接下来，TPA借鉴了MLA的思想，将(2,128)的K/V重新投影成Multi-Head，但它不是像MLA那样整个向量投影，而是沿着“2”所在的维度投影，说白了就是将2个128维向量做head_dims次不同的线性组合。显然，这样TPA的上限是不如MLA直接从整个512维向量出发来投影的。为了缓解这个问题，TPA又引入了data-dependent的组合系数来增强K、V的表达能力，即便如此，笔者还是认为它上限不如MLA。</p>
<p>为什么TPA要这样设计呢？大体上是为了兼容RoPE，这也是它相比MLA的最大“优点”。然而，这里的“优点”是要加个双引号的，因为在Partial RoPE不逊色甚至还可能更优的背景下，兼容RoPE就有点啼笑皆非的感觉了。还有，TPA这样设计，堵死了它升head_dims的空间，比如head_dims想要升到256，那么K Cache、V Cache就只是(1,256)形状了，单个向量没有线性组合的自由度。</p>
<p>再来看MFA，它全称是“Multi-matrix Factorization Attention”，这个名字看上去也有点“唬人”，它实际上就是一个带有Q-LoRA的、head_dims=256的MQA。看到这个配置，是不是有点熟悉？因为这配置跟我们上一篇文章的结论完全吻合——增大head_dims到256来提升MQA的效果，并且KV Cache跟MLA接近，同时通过Q-LoRA来控制参数量。</p>
<p>所以，MFA能“打”MLA，笔者并不意外，上一篇文章我们也实验过差不多的做法了。此外，上一篇文章我们还提出另外两个提升MQA效果的方向，一个是本文已经多次提及的Partial RoPE，另一个是通过<a href="/archives/10862">QKVO-RoPE</a>的方式实现完全的KV共享，让MQA变成GQA2-256，这两点叠加上去，MFA应该还能再涨一点。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文在上一篇文章的实验结果基础上，给出一个偏理论的思考过程，以论证MLA在一定范围内的最优性。总的来说，在Partial RoPE的背景下，MLA似乎是一个非常难以超越的Attention变体。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11111">https://spaces.ac.cn/archives/11111</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jul. 10, 2025). 《Transformer升级之路：21、MLA好在哪里?（下） 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11111">https://spaces.ac.cn/archives/11111</a></p>
<p>@online{kexuefm-11111,<br />
title={Transformer升级之路：21、MLA好在哪里?（下）},<br />
author={苏剑林},<br />
year={2025},<br />
month={Jul},<br />
url={\url{https://spaces.ac.cn/archives/11111}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="对角低秩三角阵的高效求逆方法.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#206 “对角+低秩”三角阵的高效求逆方法</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="qk-clip让muon在scaleup之路上更进一步.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#208 QK-Clip：让Muon在Scaleup之路上更进一步</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#transformer21mla">Transformer升级之路：21、MLA好在哪里?（下）</a><ul>
<li><a href="#_1">部分旋转</a></li>
<li><a href="#_2">键值共享</a></li>
<li><a href="#_3">双重投影</a></li>
<li><a href="#_4">总而言之</a></li>
<li><a href="#_5">补充讨论</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>