<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decoder-only的LLM为什么需要位置编码？ | ML & Math Blog Posts</title>
    <meta name="description" content="Decoder-only的LLM为什么需要位置编码？
原文链接: https://spaces.ac.cn/archives/10347
发布日期: 

众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在《为什么现在的LLM都是Decoder-only的架构？》也有过相关讨论），而对于Causal Attention，已经有不少工作表明它...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">Decoder-only的LLM为什么需要位置编码？</h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/10347" target="_blank">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                <span class="tag"><i class="fas fa-tag"></i> attention</span>
                <span class="tag"><i class="fas fa-tag"></i> 位置编码</span>
                <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                <span class="tag"><i class="fas fa-tag"></i> attention</span>
                
            </div>
            
        </header>

        <!-- Post Body -->
        <div class="post-content">
            <h1 id="decoder-onlyllm">Decoder-only的LLM为什么需要位置编码？</h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10347">https://spaces.ac.cn/archives/10347</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在<a href="/archives/9529">《为什么现在的LLM都是Decoder-only的架构？》</a>也有过相关讨论），而对于Causal Attention，已经有不少工作表明它不需要额外的位置编码（简称NoPE）就可以取得非平凡的结果。然而，事实是主流的Decoder-only LLM都还是加上了额外的位置编码，比如RoPE、ALIBI等。</p>
<p>那么问题就来了：明明说了不加位置编码也可以，为什么主流的LLM反而都加上了呢？不是说“多一事不如少一事”吗？这篇文章我们从三个角度给出笔者的看法：</p>
<blockquote>
<p>1、位置编码对于Attention的作用是什么？</p>
<p>2、NoPE的Causal Attention是怎么实现位置编码的？</p>
<p>3、NoPE实现的位置编码有什么不足？</p>
</blockquote>
<h2 id="_1">位置编码</h2>
<p>在这一节中，我们先思考第一个问题：位置编码对于Attention机制的意义。</p>
<p>在BERT盛行的年代，有不少位置编码工作被提了出来，笔者在<a href="/archives/8130">《让研究人员绞尽脑汁的Transformer位置编码》</a>也总结过一些。后来，我们在<a href="/archives/8231">《Transformer升级之路：1、Sinusoidal位置编码追根溯源》</a>中，试图从更贴近原理的视角来理解位置编码，并得到了最早的Sinusoidal位置编码的一种理论解释，这也直接启发了后面的<a href="/archives/8265">RoPE</a>。</p>
<p>简单来说，位置编码最根本的作用是 <em><strong>打破Attention的置换不变性</strong></em> 。什么是置换不变性呢？在BERT时代，我们主要用的是双向Attention，它的基本形式为：<br />
\begin{equation}\boldsymbol{y}<em m="1">n = \boldsymbol{f}(\boldsymbol{q}_n;\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_L) = \frac{\sum</em>}^L e^{\boldsymbol{q<em m="1">n\cdot \boldsymbol{k}_m}\boldsymbol{v}_m}{\sum</em>}^L e^{\boldsymbol{q<em k_v="k/v">n\cdot \boldsymbol{k}_m}},\quad \boldsymbol{k}_n / \boldsymbol{v}_n= \boldsymbol{x}_n\boldsymbol{W}</em>} + \boldsymbol{b<em _sigma_1="\sigma_1">{k/v}\label{eq:bi-att}\end{equation}<br />
假设$\sigma_1,\sigma_2,\cdots,\sigma_L$是$\{1,2,\cdots,L\}$的任意排列，那么置换不变性是指<br />
\begin{equation}\boldsymbol{y}_n = \boldsymbol{f}(\boldsymbol{q}_n;\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_L) = \boldsymbol{f}(\boldsymbol{q}_n;\boldsymbol{x}</em>},\boldsymbol{x<em _sigma_L="\sigma_L">{\sigma_2},\cdots,\boldsymbol{x}</em>})\end{equation<br />
说白了，就是$\boldsymbol{y}_n$跟key-value的序无关，这跟自然语言的特性不符，所以我们要想办法打破这种不变性。用数据库来类比，没有位置编码的Attention就像是没有时间标签的数据库，检索结果只跟query有关，而位置编码就相当于给数据库的item按顺序打上时间标签，使得检索结果还可以跟item顺序有关。</p>
<h2 id="_2">先验认知</h2>
<p>位置编码的另一个作用，是加入对Attention的先验认知，或者赋予Attention学习到这些先验认知性质的能力。</p>
<p>比如刚才提到的Sinusoidal位置编码，它是直接由三角函数生成的绝对位置编码，并且相邻的两个位置向量相似度更高，这隐含了相近的token应该具有相近的Embedding的先验；BERT所用的位置编码同样绝对位置编码，但它是随机初始化然后作为参数来学习的，也就是说它没有作出相近的假设，但允许模型学到这个性质（如果模型认为有必要的话）。</p>
<p>更流行的是相对位置编码，它的先验假设是“相对位置比绝对位置更重要”，早期的相对位置编码通常还会做一个截断（大于某个数值后的相对位置直接取同一个值），这里边的假设是“远距离的相对位置可以不用那么准确”，T5的位置编码则更进一步，它将相对位置按对数形式分桶处理，实现了“越远的相对位置越模糊”的效果。此外，有些相对位置编码会直接给Token的重要性加上先验，比如ALIBI就隐含了越远的Token平均而言越不重要的假设（远程衰减）。</p>
<p>诸如RNN、CNN之类的模型，本质上就是把“越近的Token越重要”的先验融入到了架构中，使其可以不用位置编码并且将复杂度降低到线性。然而，先验都是人为的、有偏的，说直接点就是不够准确的，而目前看来LLM的目标是碾压人类而不是模仿人类，这也就可以解释为什么主流架构都用Attention了，因为架构先验更少，即人为的偏见和误区更少，从而天花板更高。</p>
<h2 id="_3">单向注意</h2>
<p>了解完位置编码的作用后，我们再来思考一下NoPE是如何工作的，或者说它多大程度上能实现上面说的这些位置编码的作用。</p>
<p>前两节我们已经说了，双向Attention具有置换不变性，所以需要位置编码来打破它，所以NoPE不适用于双向Attention，它的前提是单向Attention，或者说Causal Attention：<br />
\begin{equation}\boldsymbol{y}<em m="1">n = \boldsymbol{f}(\boldsymbol{q}_n;\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_L) = \frac{\sum</em>}^n e^{\boldsymbol{q<em m="1">n\cdot \boldsymbol{k}_m}\boldsymbol{v}_m}{\sum</em>}^n e^{\boldsymbol{q<em k_v="k/v">n\cdot \boldsymbol{k}_m}},\quad \boldsymbol{k}_n / \boldsymbol{v}_n= \boldsymbol{x}_n\boldsymbol{W}</em>} + \boldsymbol{b}_{k/v}\label{eq:uni-att}\end{equation<br />
它跟式$\eqref{eq:bi-att}$的双向Attention的区别，只是求和符号的上限从$L$改为了$n$，由此可见它类似于$\text{cumsum}$，结果依赖于$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_L$的顺序。换句话说，它本身就不具有置换不变性。因此，“Causal + NoPE”的组合原则上不需要位置编码，也能取得非平凡的效果（非平凡是指效果跟有位置编码的在同一级别）。</p>
<p>首先指出该结论的论文应该是<a href="https://papers.cool/arxiv/2203.16634">《Transformer Language Models without Positional Encodings Still Learn Positional Information》</a>，当然，这主要是说作者第一次以“实验+论文”这种比较规范的方式来宣告该结论，事实上根据笔者的了解，在这篇论文之前该结论已经被不少人所默认。此外，后来的<a href="https://papers.cool/arxiv/2305.19466">《The Impact of Positional Encoding on Length Generalization in Transformers》</a>和<a href="https://papers.cool/arxiv/2404.12224">《Length Generalization of Causal Transformers without Position Encoding》</a>还探讨了NoPE的长度泛化能力。</p>
<h2 id="_4">方差辨位</h2>
<p>进一步地，“Causal + NoPE”是通过什么机制来识别位置信息的呢？我们可以通过一个极简的例子来悟一下。</p>
<p>直观来看，式$\eqref{eq:uni-att}$所定义的$\boldsymbol{y}<em n_1="n+1">n$就是$n$个$\boldsymbol{v}$的（加权）平均，$\boldsymbol{y}</em>$的（加权）平均，依此类推，所以我们可以先尝试最简单的情形——均匀分布，也就是考虑如下的Attention矩阵：}$就是$n+1$个$\boldsymbol{v<br />
\begin{equation}A = \begin{pmatrix}1 &amp; \\<br />
\frac{1}{2} &amp; \frac{1}{2} &amp; \\<br />
\frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; \\<br />
\vdots &amp; \vdots &amp; \vdots &amp; \ddots \\<br />
\frac{1}{n} &amp; \frac{1}{n} &amp; \cdots &amp; \cdots &amp; \frac{1}{n}\\<br />
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots \\<br />
\end{pmatrix}\end{equation}<br />
在这个假设下，我们有<br />
\begin{equation}\boldsymbol{y}<em m="1">n = \frac{1}{n}\sum</em>}^n \boldsymbol{v<em i="1">m\end{equation}<br />
然后，我们假设每个$\boldsymbol{v}$的每个分量，都是从同一个“均值为0、方差为$\sigma^2$”的分布中独立重复采样出来的。在此假设之下，我们可以$\boldsymbol{y}_n$的均值和方差：<br />
\begin{align}\frac{1}{d}\sum</em>}^d \boldsymbol{y<em n_i="n,i">{n,i} \approx&amp;\, \mathbb{E}[\boldsymbol{y}</em>}] = \mathbb{E}\left[\frac{1}{n}\sum_{m=1}^n \boldsymbol{v<em m="1">{n,i}\right] = \frac{1}{n}\sum</em>}^n \mathbb{E}\left[\boldsymbol{v<em i="1">{n,i}\right] = 0 \\[5pt]<br />
\frac{1}{d}\sum</em>}^d \boldsymbol{y<em n_i="n,i">{n,i}^2 \approx&amp;\, \mathbb{E}[\boldsymbol{y}</em>}^2] = \mathbb{E}\left[\left(\frac{1}{n}\sum_{m=1}^n \boldsymbol{v<em m="1">{n,i}\right)^2\right] = \frac{1}{n^2}\sum</em> \\}^n \mathbb{E}\left[\boldsymbol{v}_{n,i}^2\right] = \frac{\sigma^2}{n<br />
\end{align}<br />
第二个等式其实就是RMS Norm中的“MS（Mean Square）”，可以看到它跟位置$n$有关，由于均值为零，所以MS也等价于方差。由此我们得出，“Causal + NoPE”实际上是将位置信息隐藏在了$\boldsymbol{y}$的分量方差之中，或者等价地，隐藏在$\boldsymbol{y}$的$\mathcal{l}_2$范数中。当然，读者可能会质疑这个结论的假设。确实，这两个假设顶多适用于初始化的模型，但用来“悟”一下NoPE识别位置的原理其实足够了：<strong>各$\boldsymbol{y}_n$的直观区别就是求平均的$\boldsymbol{v}_m$的个数，而不同数量的平均导致的最直接的变化量就是方差。</strong></p>
<p>同样的结论也出现在论文<a href="https://papers.cool/arxiv/2305.13571">《Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings》</a>之中，并且作者在预训练过的NoPE模型上做了进一步的验证，肯定了该结论的普适性。</p>
<h2 id="_5">不足之处</h2>
<p>让我们来汇总一下到目前为止的结果：首先，头两节我们总结了位置编码的两个作用——主要作用是打破Attention的置换不变性，其次是为Attention注入一些先验；然后我们表明了Causal Attention本身不具备置换不变性，所以它原则上不需要位置编码（NoPE）；最后，我们发现NoPE主要是通过hidden state向量的方差来表达位置信息的。</p>
<p>现在回到标题的问题上来：为什么基于Causal Attention的Decoder-only模型通常都还会加上位置编码呢？答案其实我们刚才就说了——Causal Attention“原则上”不需要位置编码——“原则上”通常要表达的意思是“能凑合用，但不够好”，说白了就是NoPE虽然还行，但加上位置编码更好。</p>
<p>为什么这样说呢？这还得从“NoPE通过向量的方差来表达位置信息”说起，它相当于说$\boldsymbol{y}_n$是由某个不带位置信息的向量$\boldsymbol{z}_n$乘上某个跟位置$n$相关的标量函数$p(n)$得到，这又意味着：</p>
<blockquote>
<p>一、NoPE实现的是类似于乘性的绝对位置编码，并且它只是将位置信息压缩到单个标量中，所以这是一种非常弱的位置编码；</p>
<p>二、单个标量能表示的信息有限，当输入长度增加时，位置编码会越来越紧凑以至于难以区分，比如极简例子有$p(n)\sim \frac{1}{\sqrt{n}}$，当$n$足够大时$\frac{1}{\sqrt{n}}$与$\frac{1}{\sqrt{n+1}}$几乎不可分辨，也就是没法区分位置$n$与$n+1$；</p>
<p>三、主流的观点认为相对位置编码更适合自然语言，既然NoPE实现的是绝对位置编码，所以效率上自然不如再给模型额外补充上相对位置编码；</p>
<p>四、NoPE既没有给模型添加诸如远程衰减之类的先验，看上去也没有赋予模型学习到这种先验的能力，当输入长度足够大可能就会出现注意力不集中的问题。</p>
</blockquote>
<p>综上所述，NoPE对于长文本可能会存在位置分辨率不足、效率较低、注意力弥散等问题，所以即便是Decoder-only模型，我们仍需要给它补充上额外的位置编码（特别是相对位置编码），以完善上述种种不足之处。</p>
<p>当然，这些分析主要还是针对Single-Head Attention的，事实上哪怕每个Head的位置信息只有一个标量，但在Multi-Head和Multi-Layer的加持下，总的位置信息也是一个比较可观的大向量了，所以实际上NoPE没有那么糟糕，只是加上位置编码后会更好一些，因为这可以让LLM本身更聚焦于整体的推理能力，而不是还要花心思去复现一些位置编码就可以实现的能力。</p>
<h2 id="_6">文章小结</h2>
<p>尽管已经有一些工作表明，Deocder-only模型不加位置编码似乎也能取得不错的结果，但主流的LLM仍然额外加上了额外的位置编码，本文试图对这个现象给出自己的理解。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10347">https://spaces.ac.cn/archives/10347</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 01, 2024). 《Decoder-only的LLM为什么需要位置编码？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10347">https://spaces.ac.cn/archives/10347</a></p>
<p>@online{kexuefm-10347,<br />
title={Decoder-only的LLM为什么需要位置编码？},<br />
author={苏剑林},<br />
year={2024},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/10347}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释</h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
        </div>

        <!-- Back to Home -->
        <div class="text-center mt-5 mb-4">
            <a href="../index.html" class="btn btn-outline-primary">
                <i class="fas fa-arrow-left"></i> 返回首页
            </a>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>
</body>
</html>
