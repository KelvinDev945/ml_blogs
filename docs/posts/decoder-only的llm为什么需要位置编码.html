<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decoder-only的LLM为什么需要位置编码？ | ML & Math Blog Posts</title>
    <meta name="description" content="Decoder-only的LLM为什么需要位置编码？&para;
原文链接: https://spaces.ac.cn/archives/10347
发布日期: 

众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在《为什么现在的LLM都是Decoder-only的架构？》也有过相关讨论），而对于Causal Attention，已经有不...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #287 Decoder-only的LLM为什么需要位置编码？
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#287</span>
                Decoder-only的LLM为什么需要位置编码？
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-09-01</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=位置编码" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 位置编码</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=Transformer" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> Transformer</span>
                </a>
                
                <a href="../index.html?tags=RoPE" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> RoPE</span>
                </a>
                
                <a href="../index.html?tags=ALiBi" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> ALiBi</span>
                </a>
                
                <a href="../index.html?tags=NoPE" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> NoPE</span>
                </a>
                
                <a href="../index.html?tags=Causal Attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> Causal Attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="decoder-onlyllm">Decoder-only的LLM为什么需要位置编码？<a class="toc-link" href="#decoder-onlyllm" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10347">https://spaces.ac.cn/archives/10347</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在<a href="/archives/9529">《为什么现在的LLM都是Decoder-only的架构？》</a>也有过相关讨论），而对于Causal Attention，已经有不少工作表明它不需要额外的位置编码（简称NoPE）就可以取得非平凡的结果。然而，事实是主流的Decoder-only LLM都还是加上了额外的位置编码，比如RoPE、ALIBI等。</p>
<p>那么问题就来了：明明说了不加位置编码也可以，为什么主流的LLM反而都加上了呢？不是说“多一事不如少一事”吗？这篇文章我们从三个角度给出笔者的看法：</p>
<blockquote>
<p>1、位置编码对于Attention的作用是什么？</p>
<p>2、NoPE的Causal Attention是怎么实现位置编码的？</p>
<p>3、NoPE实现的位置编码有什么不足？</p>
</blockquote>
<h2 id="_1">位置编码<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>在这一节中，我们先思考第一个问题：位置编码对于Attention机制的意义。</p>
<p>在BERT盛行的年代，有不少位置编码工作被提了出来，笔者在<a href="/archives/8130">《让研究人员绞尽脑汁的Transformer位置编码》</a>也总结过一些。后来，我们在<a href="/archives/8231">《Transformer升级之路：1、Sinusoidal位置编码追根溯源》</a>中，试图从更贴近原理的视角来理解位置编码，并得到了最早的Sinusoidal位置编码的一种理论解释，这也直接启发了后面的<a href="/archives/8265">RoPE</a>。</p>
<p>简单来说，位置编码最根本的作用是 <em><strong>打破Attention的置换不变性</strong></em> 。什么是置换不变性呢？在BERT时代，我们主要用的是双向Attention，它的基本形式为：<br />
\begin{equation}\boldsymbol{y}<em m="1">n = \boldsymbol{f}(\boldsymbol{q}_n;\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_L) = \frac{\sum</em>}^L e^{\boldsymbol{q<em m="1">n\cdot \boldsymbol{k}_m}\boldsymbol{v}_m}{\sum</em>}^L e^{\boldsymbol{q<em k_v="k/v">n\cdot \boldsymbol{k}_m}},\quad \boldsymbol{k}_n / \boldsymbol{v}_n= \boldsymbol{x}_n\boldsymbol{W}</em>} + \boldsymbol{b<em _sigma_1="\sigma_1">{k/v}\label{eq:bi-att}\end{equation}<br />
假设$\sigma_1,\sigma_2,\cdots,\sigma_L$是$\{1,2,\cdots,L\}$的任意排列，那么置换不变性是指<br />
\begin{equation}\boldsymbol{y}_n = \boldsymbol{f}(\boldsymbol{q}_n;\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_L) = \boldsymbol{f}(\boldsymbol{q}_n;\boldsymbol{x}</em>},\boldsymbol{x<em _sigma_L="\sigma_L">{\sigma_2},\cdots,\boldsymbol{x}</em>})\end{equation
说白了，就是$\boldsymbol{y}_n$跟key-value的序无关，这跟自然语言的特性不符，所以我们要想办法打破这种不变性。用数据库来类比，没有位置编码的Attention就像是没有时间标签的数据库，检索结果只跟query有关，而位置编码就相当于给数据库的item按顺序打上时间标签，使得检索结果还可以跟item顺序有关。</p>
<h2 id="_2">先验认知<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>位置编码的另一个作用，是加入对Attention的先验认知，或者赋予Attention学习到这些先验认知性质的能力。</p>
<p>比如刚才提到的Sinusoidal位置编码，它是直接由三角函数生成的绝对位置编码，并且相邻的两个位置向量相似度更高，这隐含了相近的token应该具有相近的Embedding的先验；BERT所用的位置编码同样绝对位置编码，但它是随机初始化然后作为参数来学习的，也就是说它没有作出相近的假设，但允许模型学到这个性质（如果模型认为有必要的话）。</p>
<p>更流行的是相对位置编码，它的先验假设是“相对位置比绝对位置更重要”，早期的相对位置编码通常还会做一个截断（大于某个数值后的相对位置直接取同一个值），这里边的假设是“远距离的相对位置可以不用那么准确”，T5的位置编码则更进一步，它将相对位置按对数形式分桶处理，实现了“越远的相对位置越模糊”的效果。此外，有些相对位置编码会直接给Token的重要性加上先验，比如ALIBI就隐含了越远的Token平均而言越不重要的假设（远程衰减）。</p>
<p>诸如RNN、CNN之类的模型，本质上就是把“越近的Token越重要”的先验融入到了架构中，使其可以不用位置编码并且将复杂度降低到线性。然而，先验都是人为的、有偏的，说直接点就是不够准确的，而目前看来LLM的目标是碾压人类而不是模仿人类，这也就可以解释为什么主流架构都用Attention了，因为架构先验更少，即人为的偏见和误区更少，从而天花板更高。</p>
<h2 id="_3">单向注意<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>了解完位置编码的作用后，我们再来思考一下NoPE是如何工作的，或者说它多大程度上能实现上面说的这些位置编码的作用。</p>
<p>前两节我们已经说了，双向Attention具有置换不变性，所以需要位置编码来打破它，所以NoPE不适用于双向Attention，它的前提是单向Attention，或者说Causal Attention：<br />
\begin{equation}\boldsymbol{y}<em m="1">n = \boldsymbol{f}(\boldsymbol{q}_n;\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_L) = \frac{\sum</em>}^n e^{\boldsymbol{q<em m="1">n\cdot \boldsymbol{k}_m}\boldsymbol{v}_m}{\sum</em>}^n e^{\boldsymbol{q<em k_v="k/v">n\cdot \boldsymbol{k}_m}},\quad \boldsymbol{k}_n / \boldsymbol{v}_n= \boldsymbol{x}_n\boldsymbol{W}</em>} + \boldsymbol{b}_{k/v}\label{eq:uni-att}\end{equation
它跟式$\eqref{eq:bi-att}$的双向Attention的区别，只是求和符号的上限从$L$改为了$n$，由此可见它类似于$\text{cumsum}$，结果依赖于$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_L$的顺序。换句话说，它本身就不具有置换不变性。因此，“Causal + NoPE”的组合原则上不需要位置编码，也能取得非平凡的效果（非平凡是指效果跟有位置编码的在同一级别）。</p>
<p>首先指出该结论的论文应该是<a href="https://papers.cool/arxiv/2203.16634">《Transformer Language Models without Positional Encodings Still Learn Positional Information》</a>，当然，这主要是说作者第一次以“实验+论文”这种比较规范的方式来宣告该结论，事实上根据笔者的了解，在这篇论文之前该结论已经被不少人所默认。此外，后来的<a href="https://papers.cool/arxiv/2305.19466">《The Impact of Positional Encoding on Length Generalization in Transformers》</a>和<a href="https://papers.cool/arxiv/2404.12224">《Length Generalization of Causal Transformers without Position Encoding》</a>还探讨了NoPE的长度泛化能力。</p>
<h2 id="_4">方差辨位<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>进一步地，“Causal + NoPE”是通过什么机制来识别位置信息的呢？我们可以通过一个极简的例子来悟一下。</p>
<p>直观来看，式$\eqref{eq:uni-att}$所定义的$\boldsymbol{y}<em n_1="n+1">n$就是$n$个$\boldsymbol{v}$的（加权）平均，$\boldsymbol{y}</em>$的（加权）平均，依此类推，所以我们可以先尝试最简单的情形——均匀分布，也就是考虑如下的Attention矩阵：}$就是$n+1$个$\boldsymbol{v<br />
\begin{equation}A = \begin{pmatrix}1 &amp; \\
\frac{1}{2} &amp; \frac{1}{2} &amp; \\
\frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots \\
\frac{1}{n} &amp; \frac{1}{n} &amp; \cdots &amp; \cdots &amp; \frac{1}{n}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots \\
\end{pmatrix}\end{equation}<br />
在这个假设下，我们有<br />
\begin{equation}\boldsymbol{y}<em m="1">n = \frac{1}{n}\sum</em>}^n \boldsymbol{v<em i="1">m\end{equation}<br />
然后，我们假设每个$\boldsymbol{v}$的每个分量，都是从同一个“均值为0、方差为$\sigma^2$”的分布中独立重复采样出来的。在此假设之下，我们可以$\boldsymbol{y}_n$的均值和方差：<br />
\begin{align}\frac{1}{d}\sum</em>}^d \boldsymbol{y<em n_i="n,i">{n,i} \approx&amp;\, \mathbb{E}[\boldsymbol{y}</em>}] = \mathbb{E}\left[\frac{1}{n}\sum_{m=1}^n \boldsymbol{v<em m="1">{n,i}\right] = \frac{1}{n}\sum</em>}^n \mathbb{E}\left[\boldsymbol{v<em i="1">{n,i}\right] = 0 \\[5pt]
\frac{1}{d}\sum</em>}^d \boldsymbol{y<em n_i="n,i">{n,i}^2 \approx&amp;\, \mathbb{E}[\boldsymbol{y}</em>}^2] = \mathbb{E}\left[\left(\frac{1}{n}\sum_{m=1}^n \boldsymbol{v<em m="1">{n,i}\right)^2\right] = \frac{1}{n^2}\sum</em> \\}^n \mathbb{E}\left[\boldsymbol{v}_{n,i}^2\right] = \frac{\sigma^2}{n
\end{align}<br />
第二个等式其实就是RMS Norm中的“MS（Mean Square）”，可以看到它跟位置$n$有关，由于均值为零，所以MS也等价于方差。由此我们得出，“Causal + NoPE”实际上是将位置信息隐藏在了$\boldsymbol{y}$的分量方差之中，或者等价地，隐藏在$\boldsymbol{y}$的$\mathcal{l}_2$范数中。当然，读者可能会质疑这个结论的假设。确实，这两个假设顶多适用于初始化的模型，但用来“悟”一下NoPE识别位置的原理其实足够了：<strong>各$\boldsymbol{y}_n$的直观区别就是求平均的$\boldsymbol{v}_m$的个数，而不同数量的平均导致的最直接的变化量就是方差。</strong></p>
<p>同样的结论也出现在论文<a href="https://papers.cool/arxiv/2305.13571">《Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings》</a>之中，并且作者在预训练过的NoPE模型上做了进一步的验证，肯定了该结论的普适性。</p>
<h2 id="_5">不足之处<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>让我们来汇总一下到目前为止的结果：首先，头两节我们总结了位置编码的两个作用——主要作用是打破Attention的置换不变性，其次是为Attention注入一些先验；然后我们表明了Causal Attention本身不具备置换不变性，所以它原则上不需要位置编码（NoPE）；最后，我们发现NoPE主要是通过hidden state向量的方差来表达位置信息的。</p>
<p>现在回到标题的问题上来：为什么基于Causal Attention的Decoder-only模型通常都还会加上位置编码呢？答案其实我们刚才就说了——Causal Attention“原则上”不需要位置编码——“原则上”通常要表达的意思是“能凑合用，但不够好”，说白了就是NoPE虽然还行，但加上位置编码更好。</p>
<p>为什么这样说呢？这还得从“NoPE通过向量的方差来表达位置信息”说起，它相当于说$\boldsymbol{y}_n$是由某个不带位置信息的向量$\boldsymbol{z}_n$乘上某个跟位置$n$相关的标量函数$p(n)$得到，这又意味着：</p>
<blockquote>
<p>一、NoPE实现的是类似于乘性的绝对位置编码，并且它只是将位置信息压缩到单个标量中，所以这是一种非常弱的位置编码；</p>
<p>二、单个标量能表示的信息有限，当输入长度增加时，位置编码会越来越紧凑以至于难以区分，比如极简例子有$p(n)\sim \frac{1}{\sqrt{n}}$，当$n$足够大时$\frac{1}{\sqrt{n}}$与$\frac{1}{\sqrt{n+1}}$几乎不可分辨，也就是没法区分位置$n$与$n+1$；</p>
<p>三、主流的观点认为相对位置编码更适合自然语言，既然NoPE实现的是绝对位置编码，所以效率上自然不如再给模型额外补充上相对位置编码；</p>
<p>四、NoPE既没有给模型添加诸如远程衰减之类的先验，看上去也没有赋予模型学习到这种先验的能力，当输入长度足够大可能就会出现注意力不集中的问题。</p>
</blockquote>
<p>综上所述，NoPE对于长文本可能会存在位置分辨率不足、效率较低、注意力弥散等问题，所以即便是Decoder-only模型，我们仍需要给它补充上额外的位置编码（特别是相对位置编码），以完善上述种种不足之处。</p>
<p>当然，这些分析主要还是针对Single-Head Attention的，事实上哪怕每个Head的位置信息只有一个标量，但在Multi-Head和Multi-Layer的加持下，总的位置信息也是一个比较可观的大向量了，所以实际上NoPE没有那么糟糕，只是加上位置编码后会更好一些，因为这可以让LLM本身更聚焦于整体的推理能力，而不是还要花心思去复现一些位置编码就可以实现的能力。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>尽管已经有一些工作表明，Deocder-only模型不加位置编码似乎也能取得不错的结果，但主流的LLM仍然额外加上了额外的位置编码，本文试图对这个现象给出自己的理解。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10347">https://spaces.ac.cn/archives/10347</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 01, 2024). 《Decoder-only的LLM为什么需要位置编码？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10347">https://spaces.ac.cn/archives/10347</a></p>
<p>@online{kexuefm-10347,<br />
title={Decoder-only的LLM为什么需要位置编码？},<br />
author={苏剑林},<br />
year={2024},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/10347}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<h3 id="1">第1部分：核心理论、公理与历史基础<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 理论起源与历史发展<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p><strong>位置编码的历史演进</strong></p>
<p>位置编码（Positional Encoding）是Transformer架构中解决序列顺序问题的关键技术，其发展经历了多个阶段：</p>
<div class="theorem-box">

**多阶段演进**：
- **RNN/LSTM时代（1980s-2010s）**：位置信息隐式地通过循环结构编码，天然具有顺序性
- **CNN时代（1990s-2010s）**：通过卷积核的局部性捕获位置关系
- **Attention萌芽（2014-2016）**：Bahdanau Attention、Luong Attention等早期工作开始探索注意力机制
- **Transformer革命（2017）**：Vaswani et al. 提出Sinusoidal位置编码，使得纯Attention架构成为可能
- **相对位置时代（2018-2020）**：T5、DeBERTa等提出相对位置编码
- **RoPE时代（2021-至今）**：RoPE、ALiBi等成为LLM主流选择

</div>

<p><strong>关键里程碑</strong>：</p>
<ol>
<li>
<p><strong>2017 - Transformer（Vaswani et al.）</strong>：
   - 首次系统性地引入Sinusoidal位置编码
   - 发现纯Attention需要显式位置信息
   - 公式：$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$</p>
</li>
<li>
<p><strong>2019 - Transformer-XL（Dai et al.）</strong>：
   - 提出相对位置编码
   - 引入循环机制处理长文本</p>
</li>
<li>
<p><strong>2020 - DeBERTa（He et al.）</strong>：
   - 解耦内容和位置注意力
   - 相对位置偏置（Relative Position Bias）</p>
</li>
<li>
<p><strong>2021 - RoFormer（Su et al.）</strong>：
   - 提出RoPE（Rotary Position Embedding）
   - 几何视角：旋转位置编码
   - 成为GPT-3、LLaMA等主流架构的选择</p>
</li>
<li>
<p><strong>2021 - Train Short, Test Long（Press et al.）</strong>：
   - 提出ALiBi（Attention with Linear Biases）
   - 直接在注意力矩阵上加入线性衰减</p>
</li>
<li>
<p><strong>2022 - NoPE探索（Haviv et al.）</strong>：
   - 首次系统研究Causal Attention无需显式位置编码
   - 发现位置信息隐藏在方差中</p>
</li>
</ol>
<h4 id="12">1.2 数学公理与基础假设<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<div class="theorem-box">

### 公理1：Attention的置换不变性（Permutation Invariance）

**定义**：对于标准的双向Attention，输出对输入序列的排列不变：

$$\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{Attention}(\boldsymbol{Q}, \boldsymbol{K}\boldsymbol{P}, \boldsymbol{V}\boldsymbol{P})$$

其中$\boldsymbol{P}$是任意排列矩阵（$\boldsymbol{P}^T\boldsymbol{P} = \boldsymbol{I}$）。

**证明**（直觉）：
$$\boldsymbol{y}_i = \sum_{j=1}^{L} \frac{\exp(\boldsymbol{q}_i^T \boldsymbol{k}_j)}{\sum_{m=1}^{L} \exp(\boldsymbol{q}_i^T \boldsymbol{k}_m)} \boldsymbol{v}_j$$

求和操作的顺序不影响结果（加法交换律）。

</div>

<div class="theorem-box">

### 公理2：Causal Attention的顺序依赖性（Order Dependence）

**定义**：Causal Attention（单向注意力）的输出依赖于输入序列的顺序：

$$\boldsymbol{y}_n = f(\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n)$$

是一个序列函数，满足：
- **因果性**（Causality）：$\boldsymbol{y}_n$ 只依赖于 $\boldsymbol{x}_1, \ldots, \boldsymbol{x}_n$，不依赖于未来
- **非交换性**（Non-commutativity）：$(x_1, x_2) \neq (x_2, x_1)$ 一般导致不同的输出

**关键区别**：Causal Mask打破了置换不变性！

</div>

<div class="theorem-box">

### 公理3：位置信息的必要性（Necessity of Position Information）

**基本假设**：自然语言具有强烈的位置依赖性。

**例子**：
- "狗咬人" vs "人咬狗"（词序改变语义）
- "我没说他偷了钱" vs "我说他没偷钱"（否定词位置）

**数学表述**：设$\mathcal{D}$为自然语言分布，则：

$$P_{\mathcal{D}}(\boldsymbol{y} | \boldsymbol{x}_1, \ldots, \boldsymbol{x}_L) \neq P_{\mathcal{D}}(\boldsymbol{y} | \boldsymbol{x}_{\sigma(1)}, \ldots, \boldsymbol{x}_{\sigma(L)})$$

对于大多数非平凡排列$\sigma$。

</div>

<h4 id="13">1.3 设计哲学<a class="toc-link" href="#13" title="Permanent link">&para;</a></h4>
<p><strong>位置编码的核心设计哲学体现为三个原则</strong>：</p>
<p><strong>1. 打破对称性（Symmetry Breaking）</strong>：
- Attention机制天然具有置换不变性（双向）或弱位置依赖性（单向）
- 位置编码的首要任务是引入位置信息，使得不同位置可区分
- 类比：如同给每个座位编号，否则观众无法找到自己的座位</p>
<p><strong>2. 注入先验知识（Prior Injection）</strong>：
- 相近位置的Token通常相关性更高（局部性先验）
- 远距离Token的重要性通常衰减（远程衰减先验）
- 相对位置比绝对位置更重要（相对性先验）</p>
<p><strong>3. 平衡表达力与泛化性（Expressiveness vs. Generalization）</strong>：
- 过强的位置编码：模型过拟合到训练长度，难以外推
- 过弱的位置编码：模型难以区分不同位置，性能受限
- 目标：在训练长度内精确，在更长长度上平滑外推</p>
<hr />
<h3 id="2">第2部分：严谨的核心数学推导<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21-attention">2.1 双向Attention的置换不变性证明<a class="toc-link" href="#21-attention" title="Permanent link">&para;</a></h4>
<div class="derivation-box">

### 推导目标：证明标准Attention对Key-Value的排列不变

**步骤1：定义标准Attention**

给定查询$\boldsymbol{q} \in \mathbb{R}^d$，键值对$\\{(\boldsymbol{k}_i, \boldsymbol{v}_i)\\}_{i=1}^L$，标准Attention定义为：

$$\text{Attention}(\boldsymbol{q}, \\{\boldsymbol{k}_i, \boldsymbol{v}_i\\}) = \sum_{i=1}^{L} \alpha_i \boldsymbol{v}_i$$

其中注意力权重为：

$$\alpha_i = \frac{\exp(\boldsymbol{q}^T \boldsymbol{k}_i / \sqrt{d})}{\sum_{j=1}^{L} \exp(\boldsymbol{q}^T \boldsymbol{k}_j / \sqrt{d})}$$

**步骤2：引入排列**

设$\sigma: \\{1, \ldots, L\\} \to \\{1, \ldots, L\\}$是一个排列（双射），定义排列后的序列：

$$\\{(\boldsymbol{k}'_i, \boldsymbol{v}'_i)\\}_{i=1}^L = \\{(\boldsymbol{k}_{\sigma(i)}, \boldsymbol{v}_{\sigma(i)})\\}_{i=1}^L$$

**步骤3：计算排列后的Attention权重**

$$\alpha'_i = \frac{\exp(\boldsymbol{q}^T \boldsymbol{k}'_i / \sqrt{d})}{\sum_{j=1}^{L} \exp(\boldsymbol{q}^T \boldsymbol{k}'_j / \sqrt{d})} = \frac{\exp(\boldsymbol{q}^T \boldsymbol{k}_{\sigma(i)} / \sqrt{d})}{\sum_{j=1}^{L} \exp(\boldsymbol{q}^T \boldsymbol{k}_{\sigma(j)} / \sqrt{d})}$$

**步骤4：使用求和的置换不变性**

注意到分母：

$$\sum_{j=1}^{L} \exp(\boldsymbol{q}^T \boldsymbol{k}_{\sigma(j)} / \sqrt{d})$$

由于$\sigma$是双射，当$j$遍历$1, \ldots, L$时，$\sigma(j)$也遍历$1, \ldots, L$（只是顺序不同）。因此：

$$\sum_{j=1}^{L} \exp(\boldsymbol{q}^T \boldsymbol{k}_{\sigma(j)} / \sqrt{d}) = \sum_{m=1}^{L} \exp(\boldsymbol{q}^T \boldsymbol{k}_m / \sqrt{d})$$

（令$m = \sigma(j)$，利用加法交换律）

**步骤5：得出权重关系**

$$\alpha'_i = \frac{\exp(\boldsymbol{q}^T \boldsymbol{k}_{\sigma(i)} / \sqrt{d})}{\sum_{m=1}^{L} \exp(\boldsymbol{q}^T \boldsymbol{k}_m / \sqrt{d})} = \alpha_{\sigma(i)}$$

**步骤6：计算输出**

$$\text{Attention}(\boldsymbol{q}, \\{\boldsymbol{k}'_i, \boldsymbol{v}'_i\\}) = \sum_{i=1}^{L} \alpha'_i \boldsymbol{v}'_i = \sum_{i=1}^{L} \alpha_{\sigma(i)} \boldsymbol{v}_{\sigma(i)}$$

令$m = \sigma(i)$，则$i = \sigma^{-1}(m)$，当$i$遍历$1, \ldots, L$时，$m$也遍历$1, \ldots, L$：

$$\sum_{i=1}^{L} \alpha_{\sigma(i)} \boldsymbol{v}_{\sigma(i)} = \sum_{m=1}^{L} \alpha_m \boldsymbol{v}_m = \text{Attention}(\boldsymbol{q}, \\{\boldsymbol{k}_i, \boldsymbol{v}_i\\})$$

**结论**：

$$\boxed{\text{Attention}(\boldsymbol{q}, \\{\boldsymbol{k}'_i, \boldsymbol{v}'_i\\}) = \text{Attention}(\boldsymbol{q}, \\{\boldsymbol{k}_i, \boldsymbol{v}_i\\})}$$

双向Attention输出与Key-Value的排列无关！

</div>

<h4 id="22-causal-attention">2.2 Causal Attention打破置换不变性<a class="toc-link" href="#22-causal-attention" title="Permanent link">&para;</a></h4>
<div class="derivation-box">

### 推导目标：证明Causal Attention依赖序列顺序

**步骤1：定义Causal Attention**

Causal Attention在第$n$个位置的输出为：

$$\boldsymbol{y}_n = \sum_{m=1}^{n} \alpha_{nm} \boldsymbol{v}_m$$

其中：

$$\alpha_{nm} = \frac{\exp(\boldsymbol{q}_n^T \boldsymbol{k}_m / \sqrt{d})}{\sum_{j=1}^{n} \exp(\boldsymbol{q}_n^T \boldsymbol{k}_j / \sqrt{d})}, \quad m \leq n$$

**关键**：求和上限是$n$（因果约束），而非$L$。

**步骤2：构造反例**

考虑最简单的情况：$L = 2$，两个Token $\boldsymbol{x}_1, \boldsymbol{x}_2$。

**原始顺序**：
- $\boldsymbol{y}_1 = \boldsymbol{v}_1$（只能看到自己）
- $\boldsymbol{y}_2 = \alpha_{21} \boldsymbol{v}_1 + \alpha_{22} \boldsymbol{v}_2$（能看到$\boldsymbol{x}_1$和$\boldsymbol{x}_2$）

**交换顺序**：$\boldsymbol{x}'_1 = \boldsymbol{x}_2, \boldsymbol{x}'_2 = \boldsymbol{x}_1$
- $\boldsymbol{y}'_1 = \boldsymbol{v}_2$（现在第一个位置是$\boldsymbol{x}_2$）
- $\boldsymbol{y}'_2 = \alpha'_{21} \boldsymbol{v}_2 + \alpha'_{22} \boldsymbol{v}_1$

**步骤3：对比输出**

显然 $\boldsymbol{y}_1 = \boldsymbol{v}_1 \neq \boldsymbol{v}_2 = \boldsymbol{y}'_1$（除非$\boldsymbol{v}_1 = \boldsymbol{v}_2$，这是平凡情况）。

**结论**：

$$\boxed{\text{Causal Attention}(\boldsymbol{x}_1, \boldsymbol{x}_2) \neq \text{Causal Attention}(\boldsymbol{x}_2, \boldsymbol{x}_1)}$$

Causal Attention对序列顺序敏感！

</div>

<h4 id="23-nope">2.3 NoPE的方差辨位机制（详细推导）<a class="toc-link" href="#23-nope" title="Permanent link">&para;</a></h4>
<div class="derivation-box">

### 推导目标：证明NoPE通过方差编码位置信息

**问题设定**：
- 考虑Causal Attention without Position Encoding
- Attention矩阵为均匀分布（最简化假设）
- 每个$\boldsymbol{v}_i$的分量独立同分布，均值0，方差$\sigma^2$

**步骤1：定义均匀Causal Attention**

假设Attention权重为均匀分布：

$$\alpha_{nm} = \frac{1}{n}, \quad m \leq n$$

则第$n$个位置的输出为：

$$\boldsymbol{y}_n = \frac{1}{n} \sum_{m=1}^{n} \boldsymbol{v}_m$$

**步骤2：计算期望**

$$\mathbb{E}[\boldsymbol{y}_{n,i}] = \mathbb{E}\left[\frac{1}{n} \sum_{m=1}^{n} \boldsymbol{v}_{m,i}\right] = \frac{1}{n} \sum_{m=1}^{n} \mathbb{E}[\boldsymbol{v}_{m,i}] = \frac{1}{n} \cdot n \cdot 0 = 0$$

期望为0（由于$\boldsymbol{v}$的均值为0）。

**步骤3：计算二阶矩（Mean Square）**

$$\mathbb{E}[\boldsymbol{y}_{n,i}^2] = \mathbb{E}\left[\left(\frac{1}{n} \sum_{m=1}^{n} \boldsymbol{v}_{m,i}\right)^2\right]$$

展开平方：

$$= \mathbb{E}\left[\frac{1}{n^2} \left(\sum_{m=1}^{n} \boldsymbol{v}_{m,i}\right)^2\right] = \frac{1}{n^2} \mathbb{E}\left[\sum_{m=1}^{n} \sum_{m'=1}^{n} \boldsymbol{v}_{m,i} \boldsymbol{v}_{m',i}\right]$$

**步骤4：利用独立性**

由于不同$m, m'$的$\boldsymbol{v}_{m,i}, \boldsymbol{v}_{m',i}$独立：

$$\mathbb{E}[\boldsymbol{v}_{m,i} \boldsymbol{v}_{m',i}] = \begin{cases} \mathbb{E}[\boldsymbol{v}_{m,i}^2] = \sigma^2, & m = m' \\ \mathbb{E}[\boldsymbol{v}_{m,i}] \mathbb{E}[\boldsymbol{v}_{m',i}] = 0, & m \neq m' \end{cases}$$

因此：

$$\mathbb{E}\left[\sum_{m=1}^{n} \sum_{m'=1}^{n} \boldsymbol{v}_{m,i} \boldsymbol{v}_{m',i}\right] = \sum_{m=1}^{n} \mathbb{E}[\boldsymbol{v}_{m,i}^2] = n \sigma^2$$

**步骤5：得出方差公式**

$$\mathbb{E}[\boldsymbol{y}_{n,i}^2] = \frac{1}{n^2} \cdot n \sigma^2 = \frac{\sigma^2}{n}$$

由于期望为0，方差$\text{Var}(\boldsymbol{y}_{n,i}) = \mathbb{E}[\boldsymbol{y}_{n,i}^2] = \frac{\sigma^2}{n}$。

**步骤6：RMS Norm视角**

RMS（Root Mean Square）定义为：

$$\text{RMS}(\boldsymbol{y}_n) = \sqrt{\frac{1}{d} \sum_{i=1}^{d} \boldsymbol{y}_{n,i}^2} \approx \sqrt{\mathbb{E}[\boldsymbol{y}_{n,i}^2]} = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}$$

**结论**：

$$\boxed{\text{RMS}(\boldsymbol{y}_n) \propto \frac{1}{\sqrt{n}}}$$

位置$n$的信息编码在向量的RMS（或等价地，方差）中！

</div>

<div class="formula-explanation">

<div class="formula-step">
<parameter name="step-label">关键洞察：为什么是$1/\sqrt{n}$？
这是统计学中的经典结果：$n$个独立同分布变量的平均值，其标准差正比于$1/\sqrt{n}$（中心极限定理的推论）。

</div>

</div>

<hr />
<h3 id="3">第3部分：数学直觉、多角度解释与类比<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 生活化类比<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<div class="intuition-box">

### 🧠 直觉理解1：累积平均的方差衰减

**场景**：你每天测量体重，计算从第1天到第$n$天的平均体重。

**NoPE的类比**：
- 第1天：只有1次测量，平均值方差最大（波动大）
- 第10天：10次测量的平均，方差变为原来的1/10
- 第100天：100次测量的平均，方差变为原来的1/100

**关键洞察**：
- 累积次数越多，平均值越"稳定"（方差越小）
- 方差的大小直接反映了"已经累积了多少个数据点"
- NoPE正是利用这一点来编码位置$n$！

**问题**：当$n$很大时，$1/\sqrt{n}$和$1/\sqrt{n+1}$几乎无法区分
- $n=10000$: $1/\sqrt{10000} = 0.01$
- $n=10001$: $1/\sqrt{10001} \approx 0.00999950$
- 相对差异：仅$0.00005$！

</div>

<div class="intuition-box">

### 🧠 直觉理解2：数据库查询的时间戳

**没有位置编码的双向Attention** = 没有时间戳的数据库：
- 查询返回所有匹配项，但无法知道哪个是最新的
- "给我所有关于Python的文档" → 无法按时间排序

**Causal Attention + NoPE** = 隐式时间戳（通过方差）：
- 每条记录的"新鲜度"隐藏在某个统计量中
- 早期记录：方差大（因为平均次数少）
- 晚期记录：方差小（因为平均次数多）

**显式位置编码（RoPE, ALiBi）** = 明确的时间戳：
- 每条记录都有清晰的"2024-01-15 10:30"标签
- 查询时可以精确按时间筛选和排序

</div>

<h4 id="32">3.2 几何意义<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p><strong>几何视角1：向量长度编码位置</strong></p>
<div class="intuition-box">

在$d$维空间中，NoPE将位置$n$映射为：

$$\boldsymbol{y}_n = \text{content}_n \times \underbrace{\frac{1}{\sqrt{n}}}_{\text{position}}$$

其中$\text{content}_n$是单位向量（方向），$1/\sqrt{n}$是标量（长度）。

**可视化**（2D情况）：
- 位置1：长度为$1$的向量
- 位置2：长度为$1/\sqrt{2} \approx 0.707$的向量
- 位置4：长度为$1/2 = 0.5$的向量
- 位置100：长度为$1/10 = 0.1$的向量

**问题**：向量长度只能编码1维信息（位置是1维的，但用$d$维向量只利用了模长这1个自由度）

</div>

<h4 id="33">3.3 多角度理解<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p><strong>📊 信息论视角</strong></p>
<div class="intuition-box">

**NoPE的信息瓶颈**：
- 位置信息压缩到单个标量（RMS或方差）
- 信息容量：$I(position; \text{RMS}) \leq \log_2(\text{可区分的RMS值数量})$
- 当序列长度$L \to \infty$时，相邻位置的RMS差异 → 0

**显式位置编码的优势**：
- RoPE：用完整的$d$维向量编码位置（信息容量$\approx d$倍）
- ALiBi：显式的位置偏置，无信息损失

</div>

<p><strong>🎯 优化视角</strong></p>
<div class="intuition-box">

**NoPE的学习难度**：
模型需要同时学习：
1. 内容表示（$\boldsymbol{z}_n$）
2. 位置表示（通过控制方差）

这是一个耦合优化问题，增加了训练难度。

**显式位置编码的简化**：
- 位置信息由外部提供（RoPE、ALiBi）
- 模型只需学习内容表示
- 解耦 → 更容易优化

</div>

<hr />
<h3 id="4">第4部分：方法论变体、批判性比较与优化<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 主流位置编码方案对比表<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方法</th>
<th>核心思想</th>
<th>优点</th>
<th><strong>缺陷</strong></th>
<th><strong>优化方向</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>NoPE</strong></td>
<td>Causal自然顺序性，通过方差编码位置</td>
<td>✅ 无额外参数<br>✅ 架构简洁</td>
<td>❌ <strong>位置分辨率低</strong>（单标量）<br>❌ 长序列难区分<br>❌ 缺乏相对位置先验</td>
<td>✅ 补充显式位置编码<br>✅ Multi-Head分散位置信息<br>✅ 增加模型深度</td>
</tr>
<tr>
<td><strong>Sinusoidal</strong></td>
<td>三角函数生成固定编码</td>
<td>✅ 无需学习<br>✅ 理论优雅</td>
<td>❌ <strong>绝对位置</strong>，不适合长文本<br>❌ 难以外推</td>
<td>✅ 改为相对位置<br>✅ 学习式位置编码</td>
</tr>
<tr>
<td><strong>Learned Absolute</strong></td>
<td>可学习的绝对位置Embedding</td>
<td>✅ 灵活性高<br>✅ 数据驱动</td>
<td>❌ <strong>训练长度固定</strong><br>❌ 无法外推到更长序列</td>
<td>✅ 使用插值技巧<br>✅ 改为相对位置</td>
</tr>
<tr>
<td><strong>RoPE</strong></td>
<td>旋转矩阵编码相对位置</td>
<td>✅ 相对位置<br>✅ 长度外推性好<br>✅ 几何直观</td>
<td>❌ <strong>计算稍复杂</strong><br>❌ 超长外推仍有困难</td>
<td>✅ YaRN等改进外推<br>✅ 动态NTK缩放</td>
</tr>
<tr>
<td><strong>ALiBi</strong></td>
<td>线性偏置衰减</td>
<td>✅ 极简（无额外参数）<br>✅ 外推性极强</td>
<td>❌ <strong>远程衰减过强</strong><br>❌ 无法学习非单调模式</td>
<td>✅ 可学习衰减率<br>✅ 非线性衰减函数</td>
</tr>
</tbody>
</table>
<h4 id="42-nope-">4.2 NoPE - 批判性分析<a class="toc-link" href="#42-nope-" title="Permanent link">&para;</a></h4>
<div class="analysis-box">

### **核心缺陷**

**缺陷1：位置分辨率不足（Position Resolution Bottleneck）**

**问题描述**：
- 位置信息压缩到单个标量（RMS或$\mathcal{l}_2$范数）
- $d$维向量只利用了1个自由度

**根本原因**：
$$\text{RMS}(\boldsymbol{y}_n) \propto \frac{1}{\sqrt{n}} \implies \text{相邻位置差异} = \left|\frac{1}{\sqrt{n}} - \frac{1}{\sqrt{n+1}}\right| \approx \frac{1}{2n^{3/2}}$$

当$n$大时，差异极小。

**定量影响**：
- $n=100$时，相邻位置RMS差异 ≈ 0.0005
- $n=1000$时，差异 ≈ 0.000016
- 论文实验（Haviv et al. 2022）：NoPE在512长度表现尚可，但1024+长度性能显著下降

**数值示例**：

| 位置$n$ | RMS值（归一化） | 与下一位置的差异 | 可区分性 |
|---------|---------------|----------------|---------|
| 10 | 0.316 | 0.011 | ✅ 容易 |
| 100 | 0.100 | 0.0005 | ⚠️ 困难 |
| 1000 | 0.032 | 0.000016 | ❌ 几乎不可能 |

---

**缺陷2：缺乏相对位置先验（No Relative Position Bias）**

**问题描述**：
- NoPE只提供绝对位置信息（通过$n$）
- 无法直接表达"Token A 在 Token B 之前5个位置"

**根本原因**：
方差编码的是绝对位置$n$，而非相对位置$n - m$。

**定量影响**：
- 相对位置任务（如"找到前3个出现的名词"）性能下降
- 论文实验：NoPE在需要相对位置的任务上比RoPE差5-15%

---

**缺陷3：无远程衰减机制（No Distance Decay）**

**问题描述**：
- NoPE没有引入"远处Token权重衰减"的先验
- 长距离依赖可能导致注意力弥散

**根本原因**：
Causal Attention对所有历史Token同等对待（除非Attention自己学会衰减）。

**实验证据**：
- ALiBi论文显示：加入线性衰减后，长文本性能提升10-20%
- RoPE的隐式衰减（远距离Query-Key内积衰减）也有类似效果

</div>

<h4 id="43">4.3 优化方向<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p><strong>优化1：显式补充位置编码（Explicit Position Encoding）</strong></p>
<p><strong>策略</strong>：在NoPE基础上，额外添加RoPE或ALiBi。</p>
<p><strong>公式</strong>：
$$\text{Attention}<em _text_显式位置="\text{显式位置">{nm} = \text{softmax}\left(\frac{\boldsymbol{q}_n^T \boldsymbol{k}_m}{\sqrt{d}} + \underbrace{\text{pos_bias}(n, m)}</em>)$$}}\right) \quad (\text{ALiBi</p>
<p>或：
$$\boldsymbol{q}_n' = \boldsymbol{R}(n) \boldsymbol{q}_n, \quad \boldsymbol{k}_m' = \boldsymbol{R}(m) \boldsymbol{k}_m \quad (\text{RoPE})$$</p>
<p><strong>效果</strong>：
- LLaMA、GPT-3等主流模型均采用此策略
- 性能提升：5-15%（取决于任务和序列长度）</p>
<hr />
<p><strong>优化2：Multi-Head分散位置信息（Multi-Head Decomposition）</strong></p>
<p><strong>策略</strong>：不同Head学习不同"尺度"的位置信息。</p>
<p><strong>直觉</strong>：
- Head 1：关注局部位置（相邻Token）
- Head 2：关注中程位置（±10 Tokens）
- Head 3：关注远程位置（±100 Tokens）</p>
<p>每个Head虽然只有1个标量位置信息，但$H$个Head总共有$H$个标量。</p>
<p><strong>实验证据</strong>：
- 论文（Haviv et al. 2022）显示：8 Heads的NoPE比单Head好30%+</p>
<hr />
<p><strong>优化3：增加模型深度（Depth Amplification）</strong></p>
<p><strong>策略</strong>：通过更多层，逐步细化位置表示。</p>
<p><strong>数学分析</strong>：
设第$l$层的位置信息为$p_n^{(l)}$，则：
$$p_n^{(l+1)} = f(p_n^{(l)}, \boldsymbol{x}_n)$$</p>
<p>多层堆叠后，位置信息从1维标量变为$L$维向量（每层贡献1维）。</p>
<p><strong>效果</strong>：
- 实验表明：24层NoPE模型接近12层RoPE模型性能
- 代价：计算量翻倍</p>
<hr />
<h3 id="5">第5部分：学习路线图与未来展望<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 学习路线图<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p><strong>必备前置知识</strong>：
- 线性代数：向量范数、排列、期望与方差
- 概率论：独立性、中心极限定理
- Transformer基础：Attention机制、Causal Mask</p>
<p><strong>推荐学习顺序</strong>：
1. 理解双向Attention的置换不变性（本文2.1节）
2. 理解Causal Attention打破置换不变性（本文2.2节）
3. 理解NoPE的方差辨位（本文2.3节）
4. 学习RoPE、ALiBi等显式位置编码
5. 阅读论文：Haviv et al. (2022)、Press et al. (2021)</p>
<p><strong>核心论文列表</strong>：
1. Vaswani et al. (2017) - "Attention Is All You Need" ⭐
2. Su et al. (2021) - "RoFormer: Enhanced Transformer with Rotary Position Embedding" ⭐
3. Press et al. (2021) - "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation" ⭐
4. Haviv et al. (2022) - "Transformer Language Models without Positional Encodings Still Learn Positional Information" ⭐
5. Kazemnejad et al. (2023) - "The Impact of Positional Encoding on Length Generalization in Transformers"</p>
<hr />
<h4 id="52">5.2 研究空白与未来方向<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<h4 id="1-nope"><strong>方向1：理论层面 - NoPE的表达能力界</strong><a class="toc-link" href="#1-nope" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- NoPE能表达什么样的位置函数？
- 单标量位置编码的理论上限是什么？
- 多少层/多少Head才能达到显式位置编码的效果？</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：NoPE的VC维或Rademacher复杂度是多少？
   - <strong>挑战</strong>：NoPE的位置编码是隐式的，难以分析
   - <strong>潜在方法</strong>：将NoPE视为特殊的函数类，推导其复杂度上界
   - <strong>潜在意义</strong>：指导何时使用NoPE，何时必须用显式编码</p>
</li>
<li>
<p><strong>问题</strong>：$H$个Head、$L$层的NoPE等价于多少维的显式位置编码？
   - <strong>已知</strong>：单层单Head ≈ 1维
   - <strong>未知</strong>：$H$和$L$如何组合？是否线性？
   - <strong>探索方向</strong>：设计合成任务，测试不同$(H, L)$配置的位置分辨能力</p>
</li>
<li>
<p><strong>问题</strong>：能否设计"更好的"方差编码策略？
   - <strong>现状</strong>：$1/\sqrt{n}$是均匀平均的自然结果
   - <strong>优化</strong>：能否通过特殊的Attention权重设计，使得位置信息更明显？
   - <strong>例子</strong>：指数衰减权重$\alpha_m \propto e^{-(n-m)}$？</p>
</li>
</ol>
<p><strong>量化目标</strong>：
- 推导形如"$L$层$H$-Head NoPE的位置分辨率 ≥ $f(L, H)$"的理论界
- 证明存在某些位置函数，NoPE无法有效表达
- 设计新的隐式位置编码，性能接近RoPE但无额外参数</p>
<hr />
<h4 id="2-"><strong>方向2：效率层面 - 混合策略</strong><a class="toc-link" href="#2-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- 能否动态决定何时使用NoPE，何时使用显式编码？
- 短序列用NoPE（节省参数），长序列自动切换到显式编码？</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：如何设计自适应位置编码？
   - <strong>需求</strong>：序列长度$L \leq 512$用NoPE，$L &gt; 512$用RoPE
   - <strong>挑战</strong>：训练时如何平滑过渡？
   - <strong>潜在方法</strong>：</p>
<ul>
<li>软切换：$\text{pos} = \lambda(L) \cdot \text{NoPE} + (1 - \lambda(L)) \cdot \text{RoPE}$</li>
<li>门控机制：学习一个Gate决定使用哪种编码</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：能否压缩显式位置编码？
   - <strong>观察</strong>：RoPE每个位置需要$d$维旋转矩阵
   - <strong>优化方向</strong>：</p>
<ul>
<li>低秩近似RoPE旋转矩阵</li>
<li>只在关键层使用RoPE，其他层用NoPE</li>
<li><strong>量化目标</strong>：参数量减少50%，性能降低&lt;3%</li>
</ul>
</li>
</ol>
<p><strong>量化目标</strong>：
- 开发自适应方案，在多种序列长度上性能均衡
- 混合NoPE+RoPE，参数量仅为纯RoPE的30%
- 推理速度提升10-15%（减少位置计算）</p>
<hr />
<h4 id="3-"><strong>方向3：应用层面 - 超长上下文</strong><a class="toc-link" href="#3-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- 百万Token上下文中，位置编码如何设计？
- NoPE在超长序列的极限行为？</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：百万Token时，$1/\sqrt{n}$完全饱和怎么办？
   - <strong>现状</strong>：$n=10^6$时，$1/\sqrt{n} \approx 0.001$，相邻位置完全无法区分
   - <strong>优化方向</strong>：</p>
<ul>
<li>分段编码：每10000 Token重置位置计数</li>
<li>对数编码：使用$1/\log n$代替$1/\sqrt{n}$</li>
<li>多尺度编码：粗粒度+细粒度两级位置</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：流式处理中的位置编码？
   - <strong>场景</strong>：无限长的输入流（如实时聊天）
   - <strong>挑战</strong>：绝对位置会无限增长
   - <strong>解决方案</strong>：</p>
<ul>
<li>相对位置窗口（只关心最近1000 Token）</li>
<li>滑动窗口 + 局部位置编码</li>
<li>周期性位置编码（$\sin(n / T)$，$T$是周期）</li>
</ul>
</li>
</ol>
<p><strong>量化目标</strong>：
- 支持百万Token上下文，位置分辨率保持在1%水平
- 流式处理中，内存占用固定（不随序列长度增长）
- 超长文档检索任务，Recall@10提升至90%+</p>
<hr />
<h4 id="4_1"><strong>方向4：跨模态位置编码</strong><a class="toc-link" href="#4_1" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- 图像+文本的位置编码如何统一？
- 视频的时空位置编码？
- 3D点云的位置表示？</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：2D图像的位置编码与1D文本如何对齐？
   - <strong>挑战</strong>：图像是$(H, W)$的2D网格，文本是1D序列
   - <strong>优化方向</strong>：</p>
<ul>
<li>2D RoPE（行列分别旋转）</li>
<li>学习图像→文本的位置映射</li>
<li>统一到高维潜在空间</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：视频的帧内+帧间位置编码？
   - <strong>需求</strong>：既要编码空间位置$(x, y)$，又要编码时间位置$t$
   - <strong>优化</strong>：3D RoPE，或分解为空间编码+时间编码</p>
</li>
</ol>
<p><strong>量化目标</strong>：
- 多模态模型在图文匹配任务上，性能提升5-10%
- 视频理解任务，时空位置编码准确率&gt;95%</p>
<hr />
<h3 id="_8">总结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<p>本文深入探讨了"Decoder-only LLM为何需要位置编码"这一问题：</p>
<p><strong>核心发现</strong>：
1. <strong>NoPE可行但不完美</strong>：Causal Attention通过方差$\propto 1/\sqrt{n}$隐式编码位置
2. <strong>根本局限</strong>：单标量编码，长序列分辨率不足，缺乏相对位置先验
3. <strong>实践选择</strong>：主流LLM均补充显式位置编码（RoPE、ALiBi）</p>
<p><strong>关键洞察</strong>：
- NoPE是"够用"的baseline，但不是最优解
- 显式位置编码提供了更强的先验和更好的外推性
- 未来方向：自适应混合、超长上下文、跨模态统一</p>
<p><strong>实用建议</strong>：
- 短序列（&lt;512）：NoPE可尝试，节省参数
- 中长序列（512-4K）：推荐RoPE
- 超长序列（&gt;4K）：ALiBi或改进RoPE（YaRN）
- 多模态：需要专门设计的位置编码方案</p>
<hr />
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="通向最优分布之路概率空间的最小化.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#286 通向最优分布之路：概率空间的最小化</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="闭门造车之多模态思路浅谈三位置编码.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#288 “闭门造车”之多模态思路浅谈（三）：位置编码</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#decoder-onlyllm">Decoder-only的LLM为什么需要位置编码？</a><ul>
<li><a href="#_1">位置编码</a></li>
<li><a href="#_2">先验认知</a></li>
<li><a href="#_3">单向注意</a></li>
<li><a href="#_4">方差辨位</a></li>
<li><a href="#_5">不足之处</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a><ul>
<li><a href="#1">第1部分：核心理论、公理与历史基础</a></li>
<li><a href="#2">第2部分：严谨的核心数学推导</a></li>
<li><a href="#3">第3部分：数学直觉、多角度解释与类比</a></li>
<li><a href="#4">第4部分：方法论变体、批判性比较与优化</a></li>
<li><a href="#5">第5部分：学习路线图与未来展望</a></li>
<li><a href="#_8">总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>