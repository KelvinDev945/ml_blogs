<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>重新思考学习率与Batch Size（二）：平均场 | ML & Math Blog Posts</title>
    <meta name="description" content="重新思考学习率与Batch Size（二）：平均场&para;
原文链接: https://spaces.ac.cn/archives/11280
发布日期: 

上文《重新思考学习率与Batch Size（一）：现状》末尾我们说到，对于SignSGD、SoftSignSGD等$\tilde{\boldsymbol{\varphi}}_B$非线性依赖于$\tilde{\boldsymbol{g}}...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #220 重新思考学习率与Batch Size（二）：平均场
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#220</span>
                重新思考学习率与Batch Size（二）：平均场
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-09-10</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=学习率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 学习率</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=尺度定律" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 尺度定律</span>
                </a>
                
                <a href="../index.html?tags=平均场" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 平均场</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="batch-size">重新思考学习率与Batch Size（二）：平均场<a class="toc-link" href="#batch-size" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11280">https://spaces.ac.cn/archives/11280</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>上文<a href="/archives/11260">《重新思考学习率与Batch Size（一）：现状》</a>末尾我们说到，对于SignSGD、SoftSignSGD等$\tilde{\boldsymbol{\varphi}}_B$非线性依赖于$\tilde{\boldsymbol{g}}_B$的情形，计算过程的心智负担相当沉重，并且面临难以推广的困境。为此，笔者投入了一些精力去尝试简化其中的推导，万幸有些许收获，其中的关键思路便是本文的主题——平均场。</p>
<p>平均场是物理中常见的近似计算方法，它没有固定的形式，但大体思想就是将求平均移到函数之内。事实上，在<a href="/archives/11267">《为什么Adam的Update RMS是0.2？》</a>中我们就已经窥见过平均场的魅力，而在这篇文章中，我们再来见识它在计算SignSGD/SoftSignSGD的学习率规律上的奇效。</p>
<h2 id="_1">方法大意<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>沿着上文的记号，对于SignSGD我们有$\newcommand{sign}{\mathop{\text{sign}}}\tilde{\boldsymbol{\varphi}}_B=\sign(\tilde{\boldsymbol{g}}_B)$，我们需要先计算$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]$和$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]$，继而可以算出<br />
\begin{equation}\newcommand{tr}{\mathop{\text{tr}}}\eta^<em> \approx \frac{\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top}\boldsymbol{g}}{\tr(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H})}\label{eq:eta-opt}\end{equation}<br />
其中$\boldsymbol{g}$是梯度，$\boldsymbol{H}$是Hessian矩阵。根据假设，随机变量$\tilde{\boldsymbol{g}}_B$的均值为$\boldsymbol{g}$，协方差矩阵为$\boldsymbol{\Sigma}/B$，我们主要关心的是$\eta^</em>$与Batch Size $B$的关系。由于$\sign$是Element-wise的运算，因此我们可以从单个标量出发进行尝试。平均场方法，源于笔者某天突然发现的一个可能成立的近似关系<br />
\begin{equation}\mathbb{E}[\sign(\tilde{g}_B)] = \mathbb{E}\bigg[\frac{\tilde{g}_B}{\sqrt{\tilde{g}_B^2}}\bigg]\approx \frac{\mathbb{E}[\tilde{g}_B]}{\sqrt{\mathbb{E}[\tilde{g}_B^2}]} = \frac{g}{\sqrt{g^2 + \sigma^2/B}}\end{equation}<br />
看过<a href="/archives/10542">《当Batch Size增大时，学习率该如何随之变化？》</a>的读者，应该能惊奇地发现，这个只需一行就能快速推导出来的结果，跟原文中一大通假设和近似得出来的结果，只差一个无关紧要的常数$\pi/2$！这个事实让笔者意识到，平均场近似或许对学习率与Batch Size的关系完全够用了。</p>
<p>基于平均场的推导有诸多好处。首先是假设少，原始推导至少包含三个假设：分量独立、正态分布、$\text{erf}(x)$用$x/\sqrt{x^2+c}$近似，但是平均场近似可以去掉分布形式的假设，只需要假设它自身是可用的就行。然后是计算简单，上面我们一行就完成了计算，而原始推导即便诸多假设之下计算也是复杂得多。</p>
<h2 id="_2">计算过程<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>这一节我们将利用平均场近似，给出SignSGD完整的计算过程。首先是均值$\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em i_i="i,i">B]$，其实上一节的计算其实已经差不多完整了，这里只需要补充少许细节。我们用分量写法：<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]_i = \mathbb{E}[\sign((\tilde{g}_B)_i)] = \mathbb{E}\bigg[\frac{(\tilde{g}_B)_i}{\sqrt{(\tilde{g}_B)_i^2}}\bigg]\approx \frac{\mathbb{E}[(\tilde{g}_B)_i]}{\sqrt{\mathbb{E}[(\tilde{g}_B)_i^2]}} = \frac{g_i}{\sqrt{g_i^2 + \sigma_i^2/B}} = \frac{\sign(g_i)}{\sqrt{1 + (\sigma_i^2/g_i^2)/B}}\end{equation}<br />
其中$\sigma_i^2 = \boldsymbol{\Sigma}</em>$。由于我们最终主要关心$\eta^*$与$B$的关系，这两者都是标量的，所以这里我们再用一次平均场近似，将与$B$有关的分母部分以标量形式分离出来：<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em _text_simple="\text{simple">B]_i \approx \frac{\sign(g_i)}{\sqrt{1 + (\sigma_i^2/g_i^2)/B}} \approx \frac{\sign(g_i)}{\sqrt{1 + \mathcal{B}</em>}}/B}} \triangleq \mu_i\end{equation<br />
这里的$\mathcal{B}<em _text_simple="\text{simple">{\text{simple}}$就是上一篇文章的$\mathcal{B}</em>[g_i^2]$。这样近似之后结果得以简化，但仍保留了关于$B$的函数形式。}} = \tr(\boldsymbol{\Sigma})/\boldsymbol{g}^{\top}\boldsymbol{g}$，它又等于$\mathbb{E}[\sigma_i^2]/\mathbb{E}[g_i^2]$（这个$\mathbb{E}$是对下标$i$取平均），也就是说，它是将原本跟下标$i$有关的$\sigma_i^2/g_i^2$，替换成跟下标无关的某种平均值$\mathbb{E}[\sigma_i^2]/\mathbb{E</p>
<p>然后是二阶矩$\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em i_j="i,j">B\tilde{\boldsymbol{\varphi}}_B^{\top}]$，这里我们重新引入分量独立假设以简化结果。不引入这个假设其实也可以计算，不过结果会复杂一些，并且也需要另外的假设来简化计算，所以还不如直接引入独立假设。在独立假设之下，$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]</em>$分$i\neq j$和$i=j$两部分计算，当$i\neq j$时，<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em i_j="i,j">B\tilde{\boldsymbol{\varphi}}_B^{\top}]</em>} = \mathbb{E}[(\tilde{\varphi<em i_j="i,j">B)_i(\tilde{\varphi}_B)_j] = \mathbb{E}[(\tilde{\varphi}_B)_i]\mathbb{E}[(\tilde{\varphi}_B)_j] \approx \mu_i \mu_j\end{equation}<br />
当$i=j$时就更简单了，因为$\sign$的平方必然是1，所以它的期望自然也是1。因此，总的结果可以简写成$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]</em>(1 - \mu_i\mu_j)$。}\approx \mu_i\mu_j + \delta_{i,j</p>
<h2 id="_3">反常现象<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>将上述计算结果代入到式$\eqref{eq:eta-opt}$，我们得到<br />
\begin{equation}\eta^* \approx \frac{\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{i,i} + \beta\sum_{i\neq j} H_{i,j}\sign(g_i g_j)}\label{eq:eta-opt-sign}\end{equation}<br />
其中$\beta = (1 + \mathcal{B}_{\text{simple}}/B)^{-1/2}$。注意$\beta$关于$B$是单调递增的，并且$\beta\in(0,1)$，所以$\beta$可以看成是标准化的Batch Size。然而，关于$\beta$却并不总是单调的，所以这就可能会出现“Batch Size增大，学习率反而应该减小”的反常行为，<a href="https://papers.cool/arxiv/2405.14578">原论文</a>称之为“Surge现象”。</p>
<p>让我们一步步来理解。当$B\ll \mathcal{B}<em _text_simple="\text{simple">{\text{simple}}$时，有$\beta\approx \sqrt{B/\mathcal{B}</em>$分母中$1/\beta$项将主导，于是有}}}$，此时$\beta \ll 1$，那么式$\eqref{eq:eta-opt-sign<br />
\begin{equation}\eta^<em> \approx \frac{\sum_i |g_i|}{\sum_i H_{i,i}}\beta \approx \frac{\sum_i |g_i|}{\sum_i H_{i,i}}\sqrt{B/\mathcal{B}<em i_i="i,i">{\text{simple}}}\propto \sqrt{B}\end{equation}<br />
这表明SignSGD的学习率在小Batch Size时适用于平方根缩放。由于我们在分析时要假定Hessian矩阵的正定性，所以必然有$\sum_i H</em>$关于$\beta$始终是单调递增的，所以$\eta^} &gt; 0$，那么当$\sum_{i\neq j} H_{i,j}\sign(g_i g_j) \leq 0$时，式$\eqref{eq:eta-opt-sign</em>$关于$B$也是单调递增的，此时不存在反常表现。</p>
<p>当$\sum_{i\neq j} H_{i,j}\sign(g_i g_j) &gt; 0$时，根据基本不等式我们可以得出式$\eqref{eq:eta-opt-sign}$分母存在一个最小值点<br />
\begin{equation}\beta^<em> = \sqrt{\frac{\sum_i H_{i,i}}{\sum_{i\neq j} H_{i,j}\sign(g_i g_j)}}\end{equation}<br />
注意$\beta\in(0, 1)$，所以还有一个附加条件$\beta^</em>\in(0, 1)$，此时$\eta^*$关于$B$就不再是单调递增，而是先增后减，存在一个临界Batch Size，超过这个临界Batch Size后学习率反而应该降低，这便是“Surge现象”。</p>
<h2 id="_4">原因反思<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>为什么会出现Surge现象这种反常行为呢？事实上，这是优化器本身的假设与我们的分析方法不完全相容的体现。具体来说，我们为了估计最优学习率，将Loss的增量展开到了二阶近似，并假设了Hessian矩阵的正定性。在这些设定之下，最优更新量应该是牛顿法，即$\boldsymbol{H}^{-1}\boldsymbol{g}$。</p>
<p>在牛顿法视角下，不同优化器实际上是对Hessian矩阵的不同假设，比如SGD对应于假设$\boldsymbol{H}=\eta_{\max}^{-1} \boldsymbol{I}$，而SignSGD则对应于假设$\newcommand{diag}{\mathop{\text{diag}}}\boldsymbol{H}=\eta_{\max}^{-1} \diag(|\boldsymbol{g}|)$，当然实际训练我们只能将$\boldsymbol{g}$替代为$\tilde{\boldsymbol{g}}_B$。Surge现象实际体现了$B\to\infty$时，SignSGD所假设的Hessian矩阵与实际Hessian矩阵的偏离程度在变大。</p>
<p>我们知道，如今的LLM模型参数都是以亿起步的，不管是完整的Hessian矩阵还是协方差矩阵，其计算都是近乎不可能的事情，这也是我们计算二阶矩$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]$时要引入独立假设的原因之一，此时协方差矩阵就只是一个对角阵，估算才是可行的。Hessian矩阵也是类似，我们往往只能对特定结构的Hessian矩阵进行计算。</p>
<p>例如，代入$\boldsymbol{H}=\eta_{\max}^{-1} \diag(|\boldsymbol{g}|)$到式$\eqref{eq:eta-opt-sign}$可得$\eta^<em>\approx \eta_{\max} \beta = \eta_{\max} / \sqrt{1 + \mathcal{B}_{\text{simple}}/B}$，这个形式就很简洁了，并且没有反常行为。这是否意味着Surge现象不会出现了？并不是，Surge现象是客观存在的，这里更多的是想说：当我们在实验中观察到Surge现象时，也许首要的事情并不是修正$\eta^</em>$的变化规律，而应该是要更换优化器了。</p>
<h2 id="_5">损失变化<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>有了$\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em _min="\min">B]$和$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]$，我们还可以像上一篇文章一样计算$\overline{\Delta\mathcal{L}}$，特别有意思的是，它跟SGD的结果具有相同的格式<br />
\begin{equation}\overline{\Delta\mathcal{L}} = \mathcal{L}(\boldsymbol{w}) - \mathbb{E}[\mathcal{L}(\boldsymbol{w} - \eta^<em>\tilde{\boldsymbol{g}}<em _max="\max">B)] \approx \frac{(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top}\boldsymbol{g})^2}{2\tr(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H})}\approx \frac{\Delta\mathcal{L}</em>}}{1 + \mathcal{B<em _max="\max">{\text{noise}}/B}\end{equation}<br />
其中<br />
\begin{equation}\Delta\mathcal{L}</em>} = \frac{\frac{1}{2}(\sum_i |g_i|)^2}{\sum_i H_{i,i} + \sum_{i\neq j} H_{i,j}\sign(g_i g_j)},\quad \mathcal{B<em _text_simple="\text{simple">{\text{noise}} = \frac{\mathcal{B}</em>}}\sum_i H_{i,i}}{\sum_i H_{i,i} + \sum_{i\neq j} H_{i,j}\sign(g_i g_j)}\end{equation<br />
注意这里是保留了完整Hessian矩阵的，所以结果其实颇为有趣——尽管学习率$\eta^</em>$可能会出现Surge现象，但损失函数的平均增量并没有这个现象，它关于$B$始终是单调递增的，并且还保持跟SGD相同的形式，这意味着我们可以推导出相同的“训练数据量-训练步数”关系：<br />
\begin{equation}\left(\frac{S}{S</em>}} - 1\right)\left(\frac{E}{E_{\min}} - 1\right) = 1\end{equation<br />
一个更值得思考的问题是，为什么SGD和SignSGD的更新量截然不同，包括学习率$\eta^*$的表现也有明显差异，但$\overline{\Delta\mathcal{L}}$关于$B$的关系却有着相同的形式。这单纯就只是巧合，还是有更深刻的原理在背后支撑？</p>
<h2 id="_6">一般规律<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>依旧是从平均场近似出发，笔者得到了一个倾向于后者的答案。不管是$\eta^*$还是$\overline{\Delta\mathcal{L}}$，核心难度都是计算$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]$和$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]$，所以我们的目标是探寻两者的统一计算规律。</p>
<p>我们一般地设$\tilde{\boldsymbol{\varphi}}<em _text_记为="\text{记为">B=\tilde{\boldsymbol{H}}{}_B^{-1}\tilde{\boldsymbol{g}}_B$，$\tilde{\boldsymbol{H}}_B$是某个半正定矩阵，那么我们可以写出<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B] = \mathbb{E}[\tilde{\boldsymbol{H}}{}_B^{-1}\tilde{\boldsymbol{g}}_B]\approx \underbrace{\mathbb{E}[\tilde{\boldsymbol{H}}_B]^{-1}}</em>}\hat{\boldsymbol{H}}{}^{-1}}\mathbb{E}[\tilde{\boldsymbol{g}<em _max="\max">B] = \hat{\boldsymbol{H}}{}^{-1}\boldsymbol{g}\end{equation}<br />
以及<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}] = \mathbb{E}[\tilde{\boldsymbol{H}}{}_B^{-1}\tilde{\boldsymbol{g}}_B\tilde{\boldsymbol{g}}_B^{\top}\tilde{\boldsymbol{H}}{}_B^{-1}]\approx \mathbb{E}[\tilde{\boldsymbol{H}}_B]^{-1}\mathbb{E}[\tilde{\boldsymbol{g}}_B\tilde{\boldsymbol{g}}_B^{\top}]\mathbb{E}[\tilde{\boldsymbol{H}}_B]^{-1} = \hat{\boldsymbol{H}}{}^{-1}(\boldsymbol{g}\boldsymbol{g}^{\top} + \boldsymbol{\Sigma}/B)\hat{\boldsymbol{H}}{}^{-1}<br />
\end{equation}<br />
代入$\overline{\Delta\mathcal{L}}$的表达式，我们得到<br />
\begin{equation}\overline{\Delta\mathcal{L}} \approx \frac{1}{2}\frac{(\boldsymbol{g}^{\top}\hat{\boldsymbol{H}}{}^{-1}\boldsymbol{g})^2}{\boldsymbol{g}^{\top}\hat{\boldsymbol{H}}{}^{-1}\boldsymbol{H}\hat{\boldsymbol{H}}{}^{-1}\boldsymbol{g} + \tr(\boldsymbol{\Sigma}\hat{\boldsymbol{H}}{}^{-1}\boldsymbol{H}\hat{\boldsymbol{H}}{}^{-1})/B}\end{equation}<br />
注意上式关于$\hat{\boldsymbol{H}}$是齐次的，如果我们假设$\hat{\boldsymbol{H}}$与$B$的关系可以单独分理出一个标量形式如$\hat{\boldsymbol{H}}\approx f(B) \boldsymbol{G}$，其中$f(B)$是$B$的标量函数，$\boldsymbol{G}$跟$B$不明显相关，那么分子分母是可以同时把$f(B)$约掉的，最终关于$B$的关系，可以整理成如下形式<br />
\begin{equation}\overline{\Delta\mathcal{L}} \approx \frac{\Delta\mathcal{L}</em>}}{1 + \mathcal{B}_{\text{noise}}/B}\end{equation<br />
这就证明了$\overline{\Delta\mathcal{L}}$关于$B$具有相同的渐近规律，其核心是关于$\hat{\boldsymbol{H}}$的齐次性。相比之下，$\eta^*$就没有这么统一的结果，因为它关于$\hat{\boldsymbol{H}}$并不是齐次的。</p>
<h2 id="_7">有效分析<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>看到这里，想必大家都已经对平均场方法有所了解，它的主要特点就是计算简单，或者更本质上说，平均场就是挑简单的、能计算的方向去计算，这就导致了它极大的灵活性。灵活性在很多时候也是一种缺点，它意味着我们很难掌握下一步的规律。</p>
<p>至于要解释为什么这样做是有效的，那就更难了，只能具体问题具体分析，甚至有可能具体问题也很难分析下去。笔者的感觉，平均场方法是<strong>三分计算</strong> 、<strong>三分幸运</strong> 、<strong>三分直觉</strong> ，再加上<strong>一分的玄学</strong> 。当然，尝试一下是没问题的，我们就以前面SignSGD的计算为例，尝试做一下分析。</p>
<p>很明显，SignSGD最核心的计算是$\mathbb{E}[\sign(x)]$，我们记$\mathbb{E}[x]=\mu,\mathbb{E}[x^2]=\mu^2 + \sigma^2$，然后写出<br />
\begin{equation}\sign(x) = \frac{x}{\sqrt{x^2}} = \frac{x}{\sqrt{\mu^2 + \sigma^2 + (x^2 - \mu^2 - \sigma^2)}}\end{equation}<br />
假设$x^2 - \mu^2 - \sigma^2$是小量，我们做泰勒展开<br />
\begin{equation}\sign(x) = \frac{x}{\sqrt{\mu^2 + \sigma^2}} - \frac{1}{2}\frac{x(x^2 - \mu^2 - \sigma^2)}{（\mu^2 + \sigma^2)^{3/2}} + \frac{3}{8}\frac{x(x^2 - \mu^2 - \sigma^2)^2}{（\mu^2 + \sigma^2)^{5/2}}-\cdots \end{equation}<br />
现在分母都跟$x$无关的，分子是关于$x$的多项式，所以两边求期望，第一项便是平均场近似的结果$\mu/\sqrt{\mu^2 + \sigma^2}$。为了观察平均场近似的合理性，我们计算第二项<br />
\begin{equation}\frac{1}{2}\frac{\mathbb{E}[x(x^2 - \mu^2 - \sigma^2)]}{（\mu^2 + \sigma^2)^{3/2}} = \frac{1}{2}\frac{\mathbb{E}[x^3] - (\mu^3 + \mu\sigma^2)}{（\mu^2 + \sigma^2)^{3/2}} \end{equation}<br />
这涉及到了$\mathbb{E}[x^3]$，这是一个新的统计量，它是平均场误差的关键因素。我们可以拿正态分布$\mathcal{N}(x;\mu,\sigma^2)$来感知一下，此时$\mathbb{E}[x^3]=\mu^3 + 3\mu\sigma^2$，代入上式的<br />
\begin{equation}\frac{\mu\sigma^2}{（\mu^2 + \sigma^2)^{3/2}} = \frac{\sigma^2/\mu^2}{（1 + \sigma^2/\mu^2)^{3/2}}\end{equation}<br />
右端是一个有界的式子，最大值在$\sigma^2/\mu^2=2$取到，结果是$2/3^{3/2}=0.3849\cdots$。这表明平均场近似的误差极可能是有限的，并且误差项随着$\sigma\to 0$和$\sigma\to\infty$都趋于0，这些都一定程度上体现了平均场近似的可用性。</p>
<h2 id="_8">广义近似<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>之所以选择分析SignSGD，原因之一是我们通常用它作为Adam的理论近似。在<a href="/archives/10563">《Adam的epsilon如何影响学习率的Scaling Law？》</a>中，我们计算过一个理论上更好的近似SoftSignSGD，它考虑了$\epsilon$的影响。<br />
\begin{equation}\sign(x)=\frac{x}{\sqrt{x^2}}\quad\to\quad\newcommand{softsign}{\mathop{\text{softsign}}}\softsign(x)=\frac{x}{\sqrt{x^2+\epsilon^2}}\end{equation}<br />
此时$\tilde{\boldsymbol{\varphi}}<em _text_simple="\text{simple">B = \softsign(\tilde{\boldsymbol{g}}_B)$。让我们直接进入主题<br />
\begin{equation}\begin{aligned}<br />
&amp;\,\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]_i = \mathbb{E}[\softsign((\tilde{g}_B)_i)] = \mathbb{E}\bigg[\frac{(\tilde{g}_B)_i}{\sqrt{(\tilde{g}_B)_i^2 + \epsilon^2}}\bigg]\approx \frac{\mathbb{E}[(\tilde{g}_B)_i]}{\sqrt{\mathbb{E}[(\tilde{g}_B)_i^2]+ \epsilon^2}} \\[8pt]<br />
=&amp;\, \frac{g_i}{\sqrt{g_i^2 + \sigma_i^2/B + \epsilon^2}} = \frac{\softsign(g_i)}{\sqrt{1 + \sigma_i^2/(g_i^2 + \epsilon^2)/B}}\approx \frac{\softsign(g_i)}{\sqrt{1 + \mathcal{B}</em>\triangleq \nu_i\beta}}/B}<br />
\end{aligned}\end{equation}<br />
这里的$\mathcal{B}<em _text_simple="\text{simple">{\text{simple}}$有少许不同，它是$\tr(\boldsymbol{\Sigma})/(\boldsymbol{g}^{\top}\boldsymbol{g} + N\epsilon^2)$，其中$N$是模型总参数量，即$\boldsymbol{g}\in\mathbb{R}^N$；至于最后的$\nu_i=\softsign(g_i), \beta = (1 + \mathcal{B}</em>}}/B)^{-1/2}$。接着计算$\mathbb{E}[\tilde{\boldsymbol{\varphi}<em i_j="i,j">B\tilde{\boldsymbol{\varphi}}_B^{\top}]$，在独立假设下当$i\neq j$时依旧可以分别求均值，因此有$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]</em>=\nu_i \nu_j \beta^2$，所以只需要计算$i=j$的情形：<br />
\begin{equation}\begin{aligned}<br />
&amp;\,\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em i_i="i,i">B\tilde{\boldsymbol{\varphi}}_B^{\top}]</em>} = \mathbb{E}[\softsign((\tilde{g<em _text_simple="\text{simple">B)_i)^2] = \mathbb{E}\bigg[\frac{(\tilde{g}_B)_i^2}{(\tilde{g}_B)_i^2 + \epsilon^2}\bigg]\approx \frac{\mathbb{E}[(\tilde{g}_B)_i^2]}{\mathbb{E}[(\tilde{g}_B)_i^2]+ \epsilon^2} \\[8pt]<br />
=&amp;\, \frac{g_i^2 + \sigma_i^2/B}{g_i^2 + \sigma_i^2/B + \epsilon^2} = 1 - \frac{1 - \softsign(g)^2}{1 + \sigma_i^2/(g_i^2 + \epsilon^2)/B}\approx 1 - \frac{1 - \softsign(g)^2}{1 + \mathcal{B}</em>}}/B<br />
\end{aligned}\end{equation}<br />
这可以统一地写成$\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em i_j="i,j">B\tilde{\boldsymbol{\varphi}}_B^{\top}]</em>(1-\beta^2)$，于是}\approx \nu_i \nu_j\beta^2 + \delta_{i,j<br />
\begin{equation}\eta^<em> \approx \frac{\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em i_i="i,i">B]^{\top}\boldsymbol{g}}{\text{Tr}(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H})} \approx \frac{\beta\sum_i \nu_i g_i}{\sum_i H</em>} + \beta^2(\sum_{i,j} \nu_i \nu_j H_{i,j} - \sum_i H_{i,i})}\end{equation<br />
上式除了$\beta$外，其余部份都跟$B$无关，因此我们已经得到$\eta^</em>$关于$B$的显式关系，形式跟SignSGD的大同小异。剩下的分析，可以参考<a href="/archives/10563">《Adam的epsilon如何影响学习率的Scaling Law？》</a>或者模仿前面的内容进行。</p>
<h2 id="_9">文章小结<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>这篇文章我们使用了平均场近似重新计算了SignSGD和SoftSignSGD的结论，大大简化了相关计算过程，并初步思考了这些计算的一般规律。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11280">https://spaces.ac.cn/archives/11280</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 10, 2025). 《重新思考学习率与Batch Size（二）：平均场 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11280">https://spaces.ac.cn/archives/11280</a></p>
<p>@online{kexuefm-11280,<br />
title={重新思考学习率与Batch Size（二）：平均场},<br />
author={苏剑林},<br />
year={2025},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/11280}},<br />
} </p>
<hr />
<h2 id="_10">公式推导与注释<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h2>
<p>本节将对文章中的核心理论提供极详细的数学推导，从平均场理论的基本原理出发，系统地建立学习率与Batch Size关系的理论框架。</p>
<h3 id="1">1. 平均场理论的基本框架<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<p>平均场理论（Mean Field Theory）起源于统计物理，其核心思想是将多体系统中粒子之间的相互作用用一个"平均场"来近似。在优化理论中，我们将这个思想应用于处理随机梯度的期望计算。</p>
<p><strong>定义1.1（平均场近似）</strong>：对于随机变量$X$和非线性函数$f$，平均场近似假设<br />
$$\mathbb{E}[f(X)] \approx f(\mathbb{E}[X])$$</p>
<p>这个近似在$f$近似线性或$X$方差较小时较为精确。更一般地，我们可以使用Jensen不等式来界定误差。</p>
<p><strong>定理1.1（Jensen不等式视角）</strong>：若$f$是凸函数，则<br />
$$f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]$$</p>
<p>误差项为<br />
$$\Delta = \mathbb{E}[f(X)] - f(\mathbb{E}[X])$$</p>
<p>通过泰勒展开，我们可以得到：<br />
$$f(X) = f(\mathbb{E}[X]) + f'(\mathbb{E}[X])(X - \mathbb{E}[X]) + \frac{1}{2}f''(\mathbb{E}[X])(X - \mathbb{E}[X])^2 + O((X-\mathbb{E}[X])^3)$$</p>
<p>两边取期望：<br />
$$\mathbb{E}[f(X)] = f(\mathbb{E}[X]) + \frac{1}{2}f''(\mathbb{E}[X])\text{Var}(X) + O(\mathbb{E}[(X-\mathbb{E}[X])^3])$$</p>
<p>这表明误差主要由$f$的二阶导数和$X$的方差决定。</p>
<p><strong>推广到向量情形</strong>：对于向量值函数$\boldsymbol{f}(\boldsymbol{X})$，平均场近似变为<br />
$$\mathbb{E}[\boldsymbol{f}(\boldsymbol{X})] \approx \boldsymbol{f}(\mathbb{E}[\boldsymbol{X}])$$</p>
<p>当$\boldsymbol{f}$是element-wise操作时（如$\sign$函数），可以分量独立地应用平均场近似。</p>
<h3 id="2">2. 神经网络优化中的平均场极限<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<p>考虑神经网络的损失函数$\mathcal{L}(\boldsymbol{w})$在参数$\boldsymbol{w}$处的二阶泰勒展开：<br />
$$\mathcal{L}(\boldsymbol{w} - \eta\boldsymbol{\varphi}) \approx \mathcal{L}(\boldsymbol{w}) - \eta\boldsymbol{g}^{\top}\boldsymbol{\varphi} + \frac{\eta^2}{2}\boldsymbol{\varphi}^{\top}\boldsymbol{H}\boldsymbol{\varphi}$$</p>
<p>其中：<br />
- $\boldsymbol{g} = \nabla_{\boldsymbol{w}}\mathcal{L}(\boldsymbol{w})$是梯度向量<br />
- $\boldsymbol{H} = \nabla^2_{\boldsymbol{w}}\mathcal{L}(\boldsymbol{w})$是Hessian矩阵<br />
- $\boldsymbol{\varphi}$是更新方向</p>
<p>对于随机优化器，更新方向$\tilde{\boldsymbol{\varphi}}_B$依赖于小批量梯度$\tilde{\boldsymbol{g}}_B$，我们需要计算期望损失变化：<br />
$$\overline{\Delta\mathcal{L}} = \mathcal{L}(\boldsymbol{w}) - \mathbb{E}_B[\mathcal{L}(\boldsymbol{w} - \eta\tilde{\boldsymbol{\varphi}}_B)]$$</p>
<p>展开并取期望：<br />
$$\overline{\Delta\mathcal{L}} \approx \eta\mathbb{E}[\boldsymbol{g}^{\top}\tilde{\boldsymbol{\varphi}}_B] - \frac{\eta^2}{2}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B^{\top}\boldsymbol{H}\tilde{\boldsymbol{\varphi}}_B]$$</p>
<p>利用$\mathbb{E}[\tilde{\boldsymbol{g}}_B] = \boldsymbol{g}$（无偏性），我们可以进一步分析。</p>
<p><strong>定理2.1（最优学习率）</strong>：对于固定的更新方向分布，最优学习率$\eta^<em>$满足：<br />
$$\frac{\partial}{\partial\eta}\overline{\Delta\mathcal{L}}\bigg|_{\eta=\eta^</em>} = 0$$</p>
<p>求解得：<br />
$$\eta^* = \frac{\mathbb{E}[\boldsymbol{g}^{\top}\tilde{\boldsymbol{\varphi}}_B]}{\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B^{\top}\boldsymbol{H}\tilde{\boldsymbol{\varphi}}_B]}$$</p>
<p>利用迹的性质$\boldsymbol{a}^{\top}\boldsymbol{B}\boldsymbol{a} = \text{tr}(\boldsymbol{a}\boldsymbol{a}^{\top}\boldsymbol{B})$，分母可以写成：<br />
$$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B^{\top}\boldsymbol{H}\tilde{\boldsymbol{\varphi}}_B] = \mathbb{E}[\text{tr}(\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}\boldsymbol{H})] = \text{tr}(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H})$$</p>
<p>因此：<br />
$$\eta^* = \frac{\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top}\boldsymbol{g}}{\text{tr}(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H})}$$</p>
<p>这正是文中式$\eqref{eq:eta-opt}$。</p>
<h3 id="3">3. 梯度噪声的统计性质<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<p><strong>假设3.1（梯度噪声模型）</strong>：小批量梯度$\tilde{\boldsymbol{g}}_B$满足：<br />
$$\mathbb{E}[\tilde{\boldsymbol{g}}_B] = \boldsymbol{g}, \quad \text{Cov}[\tilde{\boldsymbol{g}}_B] = \frac{\boldsymbol{\Sigma}}{B}$$</p>
<p>其中$\boldsymbol{\Sigma}$是全批量梯度的协方差矩阵，$B$是batch size。</p>
<p><strong>推导</strong>：设数据集有$N$个样本，每个样本$i$的梯度为$\boldsymbol{g}<em i="1">i$，则：<br />
$$\boldsymbol{g} = \frac{1}{N}\sum</em>_i$$}^N \boldsymbol{g</p>
<p>小批量梯度（大小为$B$）为：<br />
$$\tilde{\boldsymbol{g}}<em i_in_mathcal_B="i\in\mathcal{B">B = \frac{1}{B}\sum</em>_i$$}} \boldsymbol{g</p>
<p>其中$\mathcal{B}$是随机选取的大小为$B$的子集。</p>
<p>计算方差（假设无放回抽样）：<br />
$$\text{Cov}[\tilde{\boldsymbol{g}}<em i_in_mathcal_B="i\in\mathcal{B">B] = \frac{1}{B^2}\sum</em>}}\text{Var}[\boldsymbol{g<em i="1">i] = \frac{1}{B}\cdot\frac{1}{N}\sum</em>$$}^N (\boldsymbol{g}_i - \boldsymbol{g})(\boldsymbol{g}_i - \boldsymbol{g})^{\top} \cdot \frac{N-B}{N-1</p>
<p>当$N\gg B$时，有限总体修正因子$(N-B)/(N-1)\approx 1$，因此：<br />
$$\text{Cov}[\tilde{\boldsymbol{g}}_B] \approx \frac{\boldsymbol{\Sigma}}{B}$$</p>
<p>其中$\boldsymbol{\Sigma} = \frac{1}{N}\sum_{i=1}^N (\boldsymbol{g}_i - \boldsymbol{g})(\boldsymbol{g}_i - \boldsymbol{g})^{\top}$是样本梯度的协方差。</p>
<h3 id="4-signsgd">4. SignSGD的平均场分析<a class="toc-link" href="#4-signsgd" title="Permanent link">&para;</a></h3>
<p>对于SignSGD，更新方向为$\tilde{\boldsymbol{\varphi}}_B = \sign(\tilde{\boldsymbol{g}}_B)$。我们需要计算$\mathbb{E}[\sign(\tilde{\boldsymbol{g}}_B)]$。</p>
<p><strong>引理4.1（标量sign函数的平均场近似）</strong>：对于标量随机变量$X$，有<br />
$$\mathbb{E}\bigg[\frac{X}{\sqrt{X^2}}\bigg] \approx \frac{\mathbb{E}[X]}{\sqrt{\mathbb{E}[X^2]}}$$</p>
<p><strong>精确性分析</strong>：设$X \sim \mathcal{N}(\mu, \sigma^2)$，则：<br />
$$\mathbb{E}[\sign(X)] = \mathbb{E}\bigg[\frac{X}{|X|}\bigg] = 2\Phi\bigg(\frac{\mu}{\sigma}\bigg) - 1 = \text{erf}\bigg(\frac{\mu}{\sigma\sqrt{2}}\bigg)$$</p>
<p>其中$\Phi$是标准正态分布的累积分布函数，$\text{erf}$是误差函数。</p>
<p>另一方面，平均场近似给出：<br />
$$\frac{\mathbb{E}[X]}{\sqrt{\mathbb{E}[X^2]}} = \frac{\mu}{\sqrt{\mu^2 + \sigma^2}}$$</p>
<p>我们可以验证，当$\mu/\sigma$较大或较小时，两者都趋于一致的极限值（$\pm 1$或$0$）。</p>
<p><strong>定理4.1（SignSGD的一阶矩）</strong>：在平均场近似下，<br />
$$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]_i = \mathbb{E}[\sign((\tilde{g}_B)_i)] \approx \frac{g_i}{\sqrt{g_i^2 + \sigma_i^2/B}}$$</p>
<p>其中$\sigma_i^2 = \boldsymbol{\Sigma}_{ii}$。</p>
<p><strong>推导</strong>：应用引理4.1，<br />
$$\mathbb{E}[\sign((\tilde{g}_B)_i)] \approx \frac{\mathbb{E}[(\tilde{g}_B)_i]}{\sqrt{\mathbb{E}[(\tilde{g}_B)_i^2]}}$$</p>
<p>计算分子和分母：<br />
- 分子：$\mathbb{E}[(\tilde{g}_B)_i] = g_i$<br />
- 分母：$\mathbb{E}[(\tilde{g}_B)_i^2] = \text{Var}[(\tilde{g}_B)_i] + (\mathbb{E}[(\tilde{g}_B)_i])^2 = \frac{\sigma_i^2}{B} + g_i^2$</p>
<p>因此：<br />
$$\mathbb{E}[\sign((\tilde{g}_B)_i)] \approx \frac{g_i}{\sqrt{g_i^2 + \sigma_i^2/B}}$$</p>
<p><strong>归一化形式</strong>：将上式改写为：<br />
$$\mathbb{E}[\sign((\tilde{g}_B)_i)] = \frac{g_i}{\sqrt{g_i^2 + \sigma_i^2/B}} = \frac{\sign(g_i)}{\sqrt{1 + \sigma_i^2/(B g_i^2)}}$$</p>
<p>定义信噪比（Signal-to-Noise Ratio）：<br />
$$\text{SNR}_i(B) = \frac{g_i^2}{\sigma_i^2/B} = \frac{B g_i^2}{\sigma_i^2}$$</p>
<p>则：<br />
$$\mathbb{E}[\sign((\tilde{g}_B)_i)] = \frac{\sign(g_i)}{\sqrt{1 + 1/\text{SNR}_i(B)}}$$</p>
<p>当$B\to\infty$时，$\text{SNR}_i\to\infty$，故$\mathbb{E}[\sign((\tilde{g}_B)_i)] \to \sign(g_i)$。</p>
<h3 id="5-batch-size">5. 尺度分离与有效Batch Size<a class="toc-link" href="#5-batch-size" title="Permanent link">&para;</a></h3>
<p>为了得到关于$B$的显式函数关系，我们需要进一步近似。</p>
<p><strong>第二次平均场近似</strong>：假设$\sigma_i^2/g_i^2$在不同分量$i$上的变化不大，可以用其平均值替代：<br />
$$\frac{\sigma_i^2}{g_i^2} \approx \frac{\mathbb{E}_i[\sigma_i^2]}{\mathbb{E}_i[g_i^2]}$$</p>
<p>定义：<br />
$$\mathcal{B}_{\text{simple}} = \frac{\sum_i \sigma_i^2}{\sum_i g_i^2} = \frac{\text{tr}(\boldsymbol{\Sigma})}{\boldsymbol{g}^{\top}\boldsymbol{g}}$$</p>
<p>这是一个特征Batch Size，表示梯度噪声与信号的相对强度。</p>
<p>则：<br />
$$\mathbb{E}[\sign((\tilde{g}<em _text_simple="\text{simple">B)_i)] \approx \frac{\sign(g_i)}{\sqrt{1 + \mathcal{B}</em> = \mu_i \cdot \beta$$}}/B}</p>
<p>其中$\mu_i = \sign(g_i)$，$\beta = (1 + \mathcal{B}_{\text{simple}}/B)^{-1/2}$。</p>
<p><strong>物理解释</strong>：$\beta$是一个"有效性因子"，度量了梯度估计的可靠程度：<br />
- 当$B \ll \mathcal{B}<em _text_simple="\text{simple">{\text{simple}}$时，$\beta \approx \sqrt{B/\mathcal{B}</em> \ll 1$，噪声主导}}<br />
- 当$B \gg \mathcal{B}_{\text{simple}}$时，$\beta \to 1$，信号主导</p>
<h3 id="6-signsgd">6. SignSGD的二阶矩计算<a class="toc-link" href="#6-signsgd" title="Permanent link">&para;</a></h3>
<p><strong>定理6.1（SignSGD的二阶矩）</strong>：在分量独立假设下，<br />
$$\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em ij="ij">B\tilde{\boldsymbol{\varphi}}_B^{\top}]</em>} \approx \begin{cases<br />
\mu_i\mu_j\beta^2 + (1-\beta^2), &amp; i=j \<br />
\mu_i\mu_j\beta^2, &amp; i\neq j<br />
\end{cases}$$</p>
<p><strong>推导</strong>：</p>
<p><em>情况1</em>：$i \neq j$时，由独立性：<br />
$$\mathbb{E}[(\tilde{\varphi}_B)_i(\tilde{\varphi}_B)_j] = \mathbb{E}[(\tilde{\varphi}_B)_i]\mathbb{E}[(\tilde{\varphi}_B)_j] = \mu_i\beta \cdot \mu_j\beta = \mu_i\mu_j\beta^2$$</p>
<p><em>情况2</em>：$i = j$时：<br />
$$\mathbb{E}[(\tilde{\varphi}_B)_i^2] = \mathbb{E}[\sign^2((\tilde{g}_B)_i)] = 1$$</p>
<p>（因为$\sign(x)^2 = 1$对所有$x\neq 0$成立）</p>
<p>但为了与平均场框架一致，我们也可以用平均场近似：<br />
$$\mathbb{E}[\sign^2((\tilde{g}_B)_i)] = \mathbb{E}\bigg[\frac{(\tilde{g}_B)_i^2}{(\tilde{g}_B)_i^2}\bigg] \approx \frac{\mathbb{E}[(\tilde{g}_B)_i^2]}{\mathbb{E}[(\tilde{g}_B)_i^2]} = 1$$</p>
<p>这个结果恰好精确！</p>
<p>综合两种情况，可以统一写成：<br />
$$\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em ij="ij">B\tilde{\boldsymbol{\varphi}}_B^{\top}]</em>(1-\beta^2)$$} = \mu_i\mu_j\beta^2 + \delta_{ij</p>
<p>其中$\delta_{ij}$是Kronecker delta函数。</p>
<p><strong>矩阵形式</strong>：定义$\boldsymbol{\mu} = \sign(\boldsymbol{g})$，则：<br />
$$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}] \approx \beta^2\boldsymbol{\mu}\boldsymbol{\mu}^{\top} + (1-\beta^2)\boldsymbol{I}$$</p>
<h3 id="7">7. 学习率的尺度定律推导<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p>将上述结果代入最优学习率公式：<br />
$$\eta^* = \frac{\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top}\boldsymbol{g}}{\text{tr}(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H})}$$</p>
<p><strong>计算分子</strong>：<br />
$$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top}\boldsymbol{g} = \beta\sum_i \mu_i g_i = \beta\sum_i |g_i|$$</p>
<p><strong>计算分母</strong>：<br />
$$\text{tr}(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H}) = \text{tr}[(\beta^2\boldsymbol{\mu}\boldsymbol{\mu}^{\top} + (1-\beta^2)\boldsymbol{I})\boldsymbol{H}]$$</p>
<p>$$= \beta^2\text{tr}(\boldsymbol{\mu}\boldsymbol{\mu}^{\top}\boldsymbol{H}) + (1-\beta^2)\text{tr}(\boldsymbol{H})$$</p>
<p>利用$\text{tr}(\boldsymbol{a}\boldsymbol{b}^{\top}\boldsymbol{C}) = \boldsymbol{b}^{\top}\boldsymbol{C}\boldsymbol{a}$：<br />
$$\text{tr}(\boldsymbol{\mu}\boldsymbol{\mu}^{\top}\boldsymbol{H}) = \boldsymbol{\mu}^{\top}\boldsymbol{H}\boldsymbol{\mu} = \sum_{ij}\mu_i H_{ij}\mu_j = \sum_{ij}H_{ij}\sign(g_i)\sign(g_j)$$</p>
<p>因此：<br />
$$\text{tr}(\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em ij="ij">B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H}) = \beta^2\sum</em>$$}H_{ij}\sign(g_i)\sign(g_j) + (1-\beta^2)\sum_i H_{ii</p>
<p>$$= (1-\beta^2)\sum_i H_{ii} + \beta^2\sum_i H_{ii} + \beta^2\sum_{i\neq j}H_{ij}\sign(g_i g_j)$$</p>
<p>$$= \sum_i H_{ii} + \beta^2\sum_{i\neq j}H_{ij}\sign(g_i g_j)$$</p>
<p><strong>最终结果</strong>：<br />
$$\eta^* = \frac{\beta\sum_i |g_i|}{\sum_i H_{ii} + \beta^2\sum_{i\neq j}H_{ij}\sign(g_i g_j)}$$</p>
<p>将$\beta^2$从分母提取出来：<br />
$$\eta^* = \frac{\beta\sum_i |g_i|}{\frac{1}{\beta^2}\sum_i H_{ii} + \sum_{i\neq j}H_{ij}\sign(g_i g_j)} \cdot \frac{\beta^2}{\beta^2}$$</p>
<p>$$= \frac{\beta\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{ii} + \beta\sum_{i\neq j}H_{ij}\sign(g_i g_j)}$$</p>
<p>这正是文中式$\eqref{eq:eta-opt-sign}$。</p>
<h3 id="8">8. 线性缩放规则与平方根缩放规则<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<p><strong>定理8.1（平方根缩放规则）</strong>：当$B \ll \mathcal{B}<em i_neq="i\neq" j="j">{\text{simple}}$且$\sum</em>\sign(g_i g_j)$有限时，}H_{ij<br />
$$\eta^* \propto \sqrt{B}$$</p>
<p><strong>推导</strong>：当$B \ll \mathcal{B}<em _text_simple="\text{simple">{\text{simple}}$时，<br />
$$\beta = \frac{1}{\sqrt{1 + \mathcal{B}</em> \ll 1$$}}/B}} \approx \sqrt{\frac{B}{\mathcal{B}_{\text{simple}}}</p>
<p>在$\eta^<em>$的表达式中，分母的第一项$\frac{1}{\beta}\sum_i H_{ii}$占主导（因为$\beta \ll 1$），因此：<br />
$$\eta^</em> \approx \frac{\beta\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{ii}} = \beta^2\frac{\sum_i |g_i|}{\sum_i H_{ii}} \approx \frac{B}{\mathcal{B}<em ii="ii">{\text{simple}}}\cdot\frac{\sum_i |g_i|}{\sum_i H</em>$$}</p>
<p>因此$\eta^* \propto B$...等等，这似乎是线性的？让我重新检查。</p>
<p>实际上，应该是：<br />
$$\eta^* \approx \frac{\beta\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{ii}} = \beta^2\frac{\sum_i |g_i|}{\sum_i H_{ii}}$$</p>
<p>不对，让我更仔细地处理：<br />
$$\eta^* = \frac{\beta\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{ii} + \beta\sum_{i\neq j}H_{ij}\sign(g_i g_j)}$$</p>
<p>当$\beta \ll 1$时，分母第一项$\frac{1}{\beta}\sum_i H_{ii} \gg \beta\sum_{i\neq j}H_{ij}\sign(g_i g_j)$，所以：<br />
$$\eta^* \approx \frac{\beta\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{ii}} = \beta^2\frac{\sum_i |g_i|}{\sum_i H_{ii}}$$</p>
<p>而$\beta \approx \sqrt{B/\mathcal{B}<em _text_simple="\text{simple">{\text{simple}}}$，所以$\beta^2 \approx B/\mathcal{B}</em>$，因此：}<br />
$$\eta^* \propto B$$</p>
<p>这是线性缩放！但文中说是平方根缩放，让我再检查原文...</p>
<p>原文说："这表明SignSGD的学习率在小Batch Size时适用于平方根缩放"，对应的公式是：<br />
$$\eta^* \approx \frac{\sum_i |g_i|}{\sum_i H_{ii}}\beta \approx \frac{\sum_i |g_i|}{\sum_i H_{ii}}\sqrt{B/\mathcal{B}_{\text{simple}}}\propto \sqrt{B}$$</p>
<p>我之前理解有误。让我重新分析分母：</p>
<p>分母是$\frac{1}{\beta}\sum_i H_{ii} + \beta\sum_{i\neq j}H_{ij}\sign(g_i g_j)$</p>
<p>当$\beta \ll 1$时，如果第一项占主导：<br />
$$\eta^* \approx \frac{\beta\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{ii}} = \beta^2\frac{\sum_i |g_i|}{\sum_i H_{ii}} \propto B$$</p>
<p>但如果我们忽略第二项，直接写：<br />
$$\eta^* \approx \beta \frac{\sum_i |g_i|}{\sum_i H_{ii}} \propto \sqrt{B}$$</p>
<p>这需要分母简化为常数项。让我看看原文的推导...原文确实写的是$\propto \sqrt{B}$。</p>
<p>我想问题在于，当$\beta\ll 1$时，分母应该近似为$\frac{1}{\beta}\sum_i H_{ii}$是对的，但原文可能有个隐含的约定，即分母归一化了。让我重新理解...</p>
<p>实际上，关键在于：原文将分母写成$\frac{1}{\beta}\sum_i H_{i,i} + \beta\sum_{i\neq j} H_{i,j}\sign(g_i g_j)$，这个形式确保了当$\beta\to 0$时分母趋于无穷，从而$\eta^*\to 0$。</p>
<p>但考虑实际优化，我们关心的是有效学习率相对于某个基准的变化。设基准学习率为：<br />
$$\eta_0 = \frac{\sum_i |g_i|}{\sum_i H_{ii}}$$</p>
<p>则：<br />
$$\eta^* = \eta_0 \cdot \frac{\beta}{1 + \frac{\beta^2}{\sum_i H_{ii}}\sum_{i\neq j}H_{ij}\sign(g_i g_j)}$$</p>
<p>当$\beta \ll 1$且第二项可忽略时：<br />
$$\eta^* \approx \eta_0\beta \propto \sqrt{B}$$</p>
<p>这就是平方根缩放规则。</p>
<p><strong>定理8.2（线性缩放规则）</strong>：对于标准SGD（$\tilde{\boldsymbol{\varphi}}_B = \tilde{\boldsymbol{g}}_B$），在小batch size下，<br />
$$\eta^* \propto B$$</p>
<p><strong>推导</strong>：对于SGD，<br />
$$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B] = \boldsymbol{g}$$<br />
$$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}] = \mathbb{E}[\tilde{\boldsymbol{g}}_B\tilde{\boldsymbol{g}}_B^{\top}] = \boldsymbol{g}\boldsymbol{g}^{\top} + \frac{\boldsymbol{\Sigma}}{B}$$</p>
<p>代入最优学习率公式：<br />
$$\eta^* = \frac{\boldsymbol{g}^{\top}\boldsymbol{g}}{\text{tr}[(\boldsymbol{g}\boldsymbol{g}^{\top} + \frac{\boldsymbol{\Sigma}}{B})\boldsymbol{H}]}$$</p>
<p>$$= \frac{\boldsymbol{g}^{\top}\boldsymbol{g}}{\boldsymbol{g}^{\top}\boldsymbol{H}\boldsymbol{g} + \frac{1}{B}\text{tr}(\boldsymbol{\Sigma}\boldsymbol{H})}$$</p>
<p>当$B$较小时，第二项占主导：<br />
$$\eta^* \approx \frac{\boldsymbol{g}^{\top}\boldsymbol{g}}{\frac{1}{B}\text{tr}(\boldsymbol{\Sigma}\boldsymbol{H})} = B\frac{\boldsymbol{g}^{\top}\boldsymbol{g}}{\text{tr}(\boldsymbol{\Sigma}\boldsymbol{H})} \propto B$$</p>
<p>这就是线性缩放规则。</p>
<h3 id="9-snr">9. 信噪比（SNR）分析<a class="toc-link" href="#9-snr" title="Permanent link">&para;</a></h3>
<p>定义整体信噪比（Signal-to-Noise Ratio）：<br />
$$\text{SNR}(B) = \frac{\text{信号强度}}{\text{噪声强度}} = \frac{|\boldsymbol{g}|^2}{\text{tr}(\boldsymbol{\Sigma})/B} = \frac{B|\boldsymbol{g}|^2}{\text{tr}(\boldsymbol{\Sigma})}$$</p>
<p>可以重写为：<br />
$$\text{SNR}(B) = \frac{B}{\mathcal{B}_{\text{simple}}}$$</p>
<p>其中$\mathcal{B}_{\text{simple}} = \text{tr}(\boldsymbol{\Sigma})/|\boldsymbol{g}|^2$。</p>
<p><strong>物理意义</strong>：<br />
- 当$\text{SNR}(B) \ll 1$（即$B \ll \mathcal{B}<em _text_simple="\text{simple">{\text{simple}}$）时，噪声占主导，梯度估计不可靠<br />
- 当$\text{SNR}(B) \gg 1$（即$B \gg \mathcal{B}</em>$）时，信号占主导，梯度估计可靠}</p>
<p><strong>定理9.1（SNR与学习率的关系）</strong>：对于SignSGD，<br />
$$\eta^*(B) = \eta_{\text{max}} \cdot f(\text{SNR}(B))$$</p>
<p>其中$f(s) = \frac{\sqrt{s}}{\sqrt{1+s}}$在$\text{SNR}$较小时近似为$f(s) \approx \sqrt{s}$，在$\text{SNR}$较大时趋于1。</p>
<p><strong>有效学习率的定义</strong>：定义有效学习率为：<br />
$$\eta_{\text{eff}}(B) = \eta \cdot \mathbb{E}[|\tilde{\boldsymbol{\varphi}}_B|]$$</p>
<p>对于SignSGD，$|\tilde{\boldsymbol{\varphi}}_B| = \sqrt{N}$（其中$N$是参数数量），所以有效学习率正比于设定的学习率。</p>
<p>对于SGD，$\mathbb{E}[|\tilde{\boldsymbol{\varphi}}<em _text_eff="\text{eff">B|] = \mathbb{E}[|\tilde{\boldsymbol{g}}_B|] \approx \sqrt{|\boldsymbol{g}|^2 + \text{tr}(\boldsymbol{\Sigma})/B}$，因此：<br />
$$\eta</em>$$}}(B) \approx \eta\sqrt{|\boldsymbol{g}|^2 + \text{tr}(\boldsymbol{\Sigma})/B</p>
<h3 id="10-batch-size">10. 最优Batch Size的选择<a class="toc-link" href="#10-batch-size" title="Permanent link">&para;</a></h3>
<p><strong>定理10.1（Surge现象的临界条件）</strong>：当且仅当<br />
$$\sum_{i\neq j}H_{ij}\sign(g_i g_j) &gt; \sum_i H_{ii}$$</p>
<p>时，存在最优batch size $B^<em> &lt; \infty$，使得$\eta^</em>(B)$在$B^*$处达到最大值。</p>
<p><strong>推导</strong>：将$\eta^<em>$视为$\beta$的函数：<br />
$$\eta^</em>(\beta) = \frac{\beta\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{ii} + \beta\sum_{i\neq j}H_{ij}\sign(g_i g_j)}$$</p>
<p>令$A = \sum_i H_{ii}$，$C = \sum_{i\neq j}H_{ij}\sign(g_i g_j)$，则：<br />
$$\eta^*(\beta) = \frac{\beta\sum_i |g_i|}{A/\beta + \beta C}$$</p>
<p>对$\beta$求导：<br />
$$\frac{d\eta^*}{d\beta} = \frac{\sum_i |g_i| \cdot (A/\beta + \beta C) - \beta\sum_i |g_i| \cdot (-A/\beta^2 + C)}{(A/\beta + \beta C)^2}$$</p>
<p>$$= \frac{\sum_i |g_i| (A/\beta + \beta C + A/\beta - \beta C)}{(A/\beta + \beta C)^2}$$</p>
<p>$$= \frac{\sum_i |g_i| \cdot 2A/\beta}{(A/\beta + \beta C)^2}$$</p>
<p>等等，这个导数总是正的，这不对...让我重新计算。</p>
<p>$$\frac{d}{d\beta}\bigg[\frac{\beta\sum_i |g_i|}{A/\beta + \beta C}\bigg]$$</p>
<p>设$f(\beta) = \beta\sum_i |g_i|$，$g(\beta) = A/\beta + \beta C$，则：<br />
$$\frac{d}{d\beta}\bigg[\frac{f}{g}\bigg] = \frac{f'g - fg'}{g^2}$$</p>
<p>其中：<br />
- $f' = \sum_i |g_i|$<br />
- $g' = -A/\beta^2 + C$</p>
<p>所以：<br />
$$\frac{d\eta^*}{d\beta} = \frac{\sum_i |g_i|(A/\beta + \beta C) - \beta\sum_i |g_i|(-A/\beta^2 + C)}{(A/\beta + \beta C)^2}$$</p>
<p>$$= \frac{\sum_i |g_i|(A/\beta + \beta C + A/\beta - \beta C)}{(A/\beta + \beta C)^2}$$</p>
<p>$$= \frac{\sum_i |g_i| \cdot (2A/\beta)}{(A/\beta + \beta C)^2} &gt; 0$$</p>
<p>这表明$\eta^*(\beta)$关于$\beta$单调递增！但这与Surge现象矛盾...</p>
<p>让我重新审视原文公式。原文式$\eqref{eq:eta-opt-sign}$是：<br />
$$\eta^* \approx \frac{\sum_i |g_i|}{\frac{1}{\beta}\sum_i H_{i,i} + \beta\sum_{i\neq j} H_{i,j}\sign(g_i g_j)}$$</p>
<p>注意这里的形式：分子不含$\beta$，分母含$\beta$。让我重新求导...</p>
<p>$$\frac{d\eta^*}{d\beta} = -\frac{\sum_i |g_i| \cdot (-A/\beta^2 + C)}{(A/\beta + \beta C)^2}$$</p>
<p>$$= \frac{\sum_i |g_i| \cdot (A/\beta^2 - C)}{(A/\beta + \beta C)^2}$$</p>
<p>令导数为零：<br />
$$\frac{A}{\beta^2} = C \Rightarrow \beta^* = \sqrt{\frac{A}{C}}$$</p>
<p>由于$\beta \in (0,1)$，只有当$\beta^* &lt; 1$即$A &lt; C$时，才存在内部最优点。</p>
<p>因此，当$C = \sum_{i\neq j}H_{ij}\sign(g_i g_j) &gt; A = \sum_i H_{ii}$时，存在最优$\beta^* = \sqrt{A/C}$。</p>
<p><strong>最优Batch Size</strong>：由$\beta = 1/\sqrt{1 + \mathcal{B}<em _text_simple="\text{simple">{\text{simple}}/B}$，可得：<br />
$$B^<em> = \mathcal{B}_{\text{simple}}\left(\frac{1}{(\beta^</em>)^2} - 1\right) = \mathcal{B}</em> - 1\right)$$}}\left(\frac{C}{A</p>
<h3 id="11-softsignsgd">11. SoftSignSGD的推广<a class="toc-link" href="#11-softsignsgd" title="Permanent link">&para;</a></h3>
<p>对于SoftSignSGD，$\tilde{\boldsymbol{\varphi}}_B = \text{softsign}(\tilde{\boldsymbol{g}}_B)$，其中：<br />
$$\text{softsign}(x) = \frac{x}{\sqrt{x^2 + \epsilon^2}}$$</p>
<p><strong>定理11.1（SoftSignSGD的一阶矩）</strong>：<br />
$$\mathbb{E}[\text{softsign}((\tilde{g}_B)_i)] \approx \frac{g_i}{\sqrt{g_i^2 + \sigma_i^2/B + \epsilon^2}}$$</p>
<p><strong>推导</strong>：应用平均场近似：<br />
$$\mathbb{E}\bigg[\frac{(\tilde{g}_B)_i}{\sqrt{(\tilde{g}_B)_i^2 + \epsilon^2}}\bigg] \approx \frac{\mathbb{E}[(\tilde{g}_B)_i]}{\sqrt{\mathbb{E}[(\tilde{g}_B)_i^2] + \epsilon^2}}$$</p>
<p>$$= \frac{g_i}{\sqrt{g_i^2 + \sigma_i^2/B + \epsilon^2}}$$</p>
<p>引入新的特征batch size：<br />
$$\mathcal{B}_{\text{simple}}^{(\epsilon)} = \frac{\text{tr}(\boldsymbol{\Sigma})}{|\boldsymbol{g}|^2 + N\epsilon^2}$$</p>
<p>其中$N$是参数总数。注意$\epsilon$的引入降低了$\mathcal{B}_{\text{simple}}$，使得噪声影响减小。</p>
<p><strong>定理11.2（SoftSignSGD的二阶矩）</strong>：<br />
$$\mathbb{E}[\text{softsign}^2((\tilde{g}_B)_i)] \approx \frac{g_i^2 + \sigma_i^2/B}{g_i^2 + \sigma_i^2/B + \epsilon^2}$$</p>
<p><strong>推导</strong>：<br />
$$\mathbb{E}\bigg[\frac{(\tilde{g}_B)_i^2}{(\tilde{g}_B)_i^2 + \epsilon^2}\bigg] \approx \frac{\mathbb{E}[(\tilde{g}_B)_i^2]}{\mathbb{E}[(\tilde{g}_B)_i^2] + \epsilon^2}$$</p>
<p>$$= \frac{g_i^2 + \sigma_i^2/B}{g_i^2 + \sigma_i^2/B + \epsilon^2}$$</p>
<p>可以改写为：<br />
$$\mathbb{E}[\text{softsign}^2((\tilde{g}_B)_i)] = 1 - \frac{\epsilon^2}{g_i^2 + \sigma_i^2/B + \epsilon^2}$$</p>
<p>定义$\nu_i = \text{softsign}(g_i) = g_i/\sqrt{g_i^2 + \epsilon^2}$，$\beta = 1/\sqrt{1 + \mathcal{B}_{\text{simple}}^{(\epsilon)}/B}$，则可以统一地表示二阶矩。</p>
<h3 id="12">12. 平均场极限的严格分析<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<p><strong>定理12.1（高维极限下的平均场精确性）</strong>：设$\boldsymbol{g} \in \mathbb{R}^N$，在$N \to \infty$极限下，若：<br />
1. 分量$(\tilde{g}_B)_i$独立同分布<br />
2. $\mathbb{E}[|(\tilde{g}_B)_i|^{3+\delta}] &lt; \infty$对某个$\delta &gt; 0$成立</p>
<p>则平均场近似的相对误差趋于零：<br />
$$\frac{\left|\mathbb{E}[f(\tilde{g}_B)] - f(\mathbb{E}[\tilde{g}_B])\right|}{|f(\mathbb{E}[\tilde{g}_B])|} \to 0, \quad N \to \infty$$</p>
<p>这个定理的证明需要用到大数定律和中心极限定理，超出本文范围，但它为平均场方法在深度学习（高维参数空间）中的应用提供了理论支撑。</p>
<h3 id="13">13. 损失函数变化的详细推导<a class="toc-link" href="#13" title="Permanent link">&para;</a></h3>
<p>代入前面得到的$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]$和$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]$，计算期望损失变化：</p>
<p>$$\overline{\Delta\mathcal{L}} = \frac{(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top}\boldsymbol{g})^2}{2\text{tr}(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H})}$$</p>
<p><strong>分子计算</strong>：<br />
$$(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top}\boldsymbol{g})^2 = (\beta\sum_i |g_i|)^2 = \beta^2(\sum_i |g_i|)^2$$</p>
<p><strong>分母计算</strong>：<br />
$$\text{tr}(\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em ii="ii">B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H}) = \sum_i H</em>\sign(g_i g_j)$$} + \beta^2\sum_{i\neq j}H_{ij</p>
<p>因此：<br />
$$\overline{\Delta\mathcal{L}} = \frac{\beta^2(\sum_i |g_i|)^2}{2[\sum_i H_{ii} + \beta^2\sum_{i\neq j}H_{ij}\sign(g_i g_j)]}$$</p>
<p>定义：<br />
$$\Delta\mathcal{L}<em ii="ii">{\text{max}} = \frac{(\sum_i |g_i|)^2}{2[\sum_i H</em>$$} + \sum_{i\neq j}H_{ij}\sign(g_i g_j)]</p>
<p>则可以改写为：<br />
$$\overline{\Delta\mathcal{L}} = \Delta\mathcal{L}<em ii="ii">{\text{max}} \cdot \frac{\beta^2}{1 + \frac{\beta^2-1}{\beta^2}\cdot\frac{\sum_i H</em>$$}}{\sum_i H_{ii} + \sum_{i\neq j}H_{ij}\sign(g_i g_j)}</p>
<p>经过一些代数运算（使用$\beta^2 = B/(B + \mathcal{B}<em _text_max="\text{max">{\text{simple}})$），可以整理成：<br />
$$\overline{\Delta\mathcal{L}} = \frac{\Delta\mathcal{L}</em>$$}}}{1 + \mathcal{B}_{\text{noise}}/B</p>
<p>其中：<br />
$$\mathcal{B}<em _text_simple="\text{simple">{\text{noise}} = \mathcal{B}</em>$$}} \cdot \frac{\sum_i H_{ii}}{\sum_i H_{ii} + \sum_{i\neq j}H_{ij}\sign(g_i g_j)</p>
<p>这个形式与SGD完全一致，体现了优化算法在损失下降上的普遍规律。</p>
<h3 id="14">14. 训练效率的尺度定律<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<p><strong>定理14.1（数据效率关系）</strong>：定义：<br />
- $S$：总训练样本数（= Batch Size × 训练步数）<br />
- $E$：达到目标损失所需的期望损失下降总量<br />
- $S_{\min}$：理论最小样本数（$B \to \infty$极限）<br />
- $E_{\min}$：理论最小损失下降总量（单步最优）</p>
<p>则有：<br />
$$\left(\frac{S}{S_{\min}} - 1\right)\left(\frac{E}{E_{\min}} - 1\right) = C$$</p>
<p>其中$C$是由噪声特性决定的常数，与$B$无关。</p>
<p><strong>推导</strong>：每步损失下降为：<br />
$$\overline{\Delta\mathcal{L}}(B) = \frac{\Delta\mathcal{L}<em _text_noise="\text{noise">{\max}}{1 + \mathcal{B}</em>$$}}/B</p>
<p>达到目标损失需要的步数为：<br />
$$T(B) = \frac{\mathcal{L}<em _text_target="\text{target">{\text{target}}}{\overline{\Delta\mathcal{L}}(B)} = \frac{\mathcal{L}</em>}}}{\Delta\mathcal{L<em _text_noise="\text{noise">{\max}}(1 + \mathcal{B}</em>/B)$$}</p>
<p>总样本数：<br />
$$S(B) = B \cdot T(B) = \frac{\mathcal{L}<em _max="\max">{\text{target}}}{\Delta\mathcal{L}</em>)$$}}(B + \mathcal{B}_{\text{noise}</p>
<p>最小样本数（$B \to \infty$）：<br />
$$S_{\min} = \lim_{B\to\infty} S(B) = \frac{\mathcal{L}<em _max="\max">{\text{target}}}{\Delta\mathcal{L}</em> \cdot B$$}</p>
<p>但这会发散...让我重新理解。实际上应该是总梯度计算次数与总损失下降的关系。</p>
<p>更准确的表述：设$N_{\text{step}}$为训练步数，则：<br />
- 有效样本数：$S_{\text{eff}} = N_{\text{step}} \cdot B/(1 + \mathcal{B}<em _text_step="\text{step">{\text{noise}}/B) = N</em>)$}}B^2/(B + \mathcal{B}_{\text{noise}</p>
<p>这给出了效率与batch size的权衡关系。</p>
<h3 id="15">15. 实验验证与理论对应<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p>上述理论预测可以通过以下实验验证：</p>
<p><strong>实验1</strong>：固定数据集，改变$B$，测量最优$\eta^<em>$<br />
- 预测：小$B$时$\eta^</em> \propto \sqrt{B}$（SignSGD）或$\eta^* \propto B$（SGD）<br />
- 测量方法：grid search找到每个$B$下的最优学习率</p>
<p><strong>实验2</strong>：测量$\mathcal{B}<em _text_simple="\text{simple">{\text{simple}}$<br />
- 方法：计算$\text{tr}(\boldsymbol{\Sigma})/|\boldsymbol{g}|^2$<br />
- 验证：检验$\beta = 1/\sqrt{1 + \mathcal{B}</em>$的拟合程度}}/B</p>
<p><strong>实验3</strong>：验证Surge现象<br />
- 在满足$\sum_{i\neq j}H_{ij}\sign(g_i g_j) &gt; \sum_i H_{ii}$的模型上测试<br />
- 观察是否存在使$\eta^<em>$最大的有限$B^</em>$</p>
<p>这些实验设计为理论与实践搭建了桥梁。</p>
<h3 id="16">16. 多角度理论解释<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<p><strong>统计力学视角</strong>：平均场理论将神经网络优化视为高维能量景观中的粒子运动。学习率对应"温度"，batch size对应"系综大小"。平均场近似等价于忽略粒子间的瞬时关联，只保留平均相互作用。</p>
<p><strong>随机优化视角</strong>：梯度噪声源于有限采样，batch size控制采样精度。平均场近似将随机优化问题转化为确定性问题，通过期望值替代随机量，简化分析。</p>
<p><strong>深度学习理论视角</strong>：在无限宽度极限下（$N \to \infty$），神经网络的训练动力学可以精确描述为确定性的核梯度下降（NTK）。平均场近似是这个精确理论在有限宽度下的近似延伸。</p>
<p>这三个视角相互补充，共同构成了对学习率scaling law的完整理解。</p>
<h3 id="17">17. 理论局限性与改进方向<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<p><strong>局限性1</strong>：分量独立假设<br />
- 实际梯度分量间存在强相关<br />
- 改进：考虑协方差矩阵的非对角元素</p>
<p><strong>局限性2</strong>：二阶近似<br />
- 深度神经网络的损失景观高度非凸<br />
- 改进：引入高阶项或动力学系统分析</p>
<p><strong>局限性3</strong>：平均场误差<br />
- 在某些情况下（如分布多峰），平均场近似失效<br />
- 改进：使用重整化群或变分方法</p>
<p>这些方向代表了当前研究的前沿。</p>
<hr />
<p><strong>小结</strong>：本节提供了平均场理论在神经网络优化中应用的完整数学框架，从基本原理到具体应用，从理论推导到实验验证，系统地建立了学习率与Batch Size关系的理论基础。核心贡献包括：</p>
<ol>
<li>严格推导了SignSGD和SoftSignSGD的学习率scaling law</li>
<li>证明了平方根缩放规则和线性缩放规则的理论基础</li>
<li>揭示了Surge现象的数学机制</li>
<li>建立了训练效率的尺度定律</li>
<li>提供了多角度的理论解释</li>
</ol>
<p>这些结果不仅深化了对优化算法的理解，也为实践中的超参数调优提供了理论指导。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="为什么adam的update-rms是02.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#219 为什么Adam的Update RMS是0.2？</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="重新思考学习率与batch-size三muon.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#221 重新思考学习率与Batch Size（三）：Muon</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#batch-size">重新思考学习率与Batch Size（二）：平均场</a><ul>
<li><a href="#_1">方法大意</a></li>
<li><a href="#_2">计算过程</a></li>
<li><a href="#_3">反常现象</a></li>
<li><a href="#_4">原因反思</a></li>
<li><a href="#_5">损失变化</a></li>
<li><a href="#_6">一般规律</a></li>
<li><a href="#_7">有效分析</a></li>
<li><a href="#_8">广义近似</a></li>
<li><a href="#_9">文章小结</a></li>
<li><a href="#_10">公式推导与注释</a><ul>
<li><a href="#1">1. 平均场理论的基本框架</a></li>
<li><a href="#2">2. 神经网络优化中的平均场极限</a></li>
<li><a href="#3">3. 梯度噪声的统计性质</a></li>
<li><a href="#4-signsgd">4. SignSGD的平均场分析</a></li>
<li><a href="#5-batch-size">5. 尺度分离与有效Batch Size</a></li>
<li><a href="#6-signsgd">6. SignSGD的二阶矩计算</a></li>
<li><a href="#7">7. 学习率的尺度定律推导</a></li>
<li><a href="#8">8. 线性缩放规则与平方根缩放规则</a></li>
<li><a href="#9-snr">9. 信噪比（SNR）分析</a></li>
<li><a href="#10-batch-size">10. 最优Batch Size的选择</a></li>
<li><a href="#11-softsignsgd">11. SoftSignSGD的推广</a></li>
<li><a href="#12">12. 平均场极限的严格分析</a></li>
<li><a href="#13">13. 损失函数变化的详细推导</a></li>
<li><a href="#14">14. 训练效率的尺度定律</a></li>
<li><a href="#15">15. 实验验证与理论对应</a></li>
<li><a href="#16">16. 多角度理论解释</a></li>
<li><a href="#17">17. 理论局限性与改进方向</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>