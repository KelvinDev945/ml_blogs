<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Muon续集：为什么我们选择尝试Muon？ | ML & Math Blog Posts</title>
    <meta name="description" content="Muon续集：为什么我们选择尝试Muon？&para;
原文链接: https://spaces.ac.cn/archives/10739
发布日期: 

本文解读一下我们最新的技术报告《Muon is Scalable for LLM Training》，里边分享了我们之前在《Muon优化器赏析：从向量到矩阵的本质跨越》介绍过的Muon优化器的一次较大规模的实践，并开源了相应的模型（我们称之为“...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=矩阵">矩阵</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #184 Muon续集：为什么我们选择尝试Muon？
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#184</span>
                Muon续集：为什么我们选择尝试Muon？
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-02-27</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=矩阵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 矩阵</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=谱范数" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 谱范数</span>
                </a>
                
                <a href="../index.html?tags=muon" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> muon</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="muonmuon">Muon续集：为什么我们选择尝试Muon？<a class="toc-link" href="#muonmuon" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10739">https://spaces.ac.cn/archives/10739</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>本文解读一下我们最新的技术报告<a href="https://papers.cool/arxiv/2502.16982">《Muon is Scalable for LLM Training》</a>，里边分享了我们之前在<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>介绍过的Muon优化器的一次较大规模的实践，并开源了相应的模型（我们称之为“<a href="https://github.com/MoonshotAI/Moonlight">Moonlight</a>”，目前是一个3B/16B的MoE模型）。我们发现了一个比较惊人的结论：在我们的实验设置下，Muon相比Adam能够达到将近2倍的训练效率。</p>
<p><a href="/usr/uploads/2025/02/1300601661.png" title="点击查看原图"><img alt="Muon的Scaling Law及Moonlight的MMLU表现" src="/usr/uploads/2025/02/1300601661.png" /></a></p>
<p>Muon的Scaling Law及Moonlight的MMLU表现</p>
<p>优化器的工作说多不多，但说少也不少，为什么我们会选择Muon来作为新的尝试方向呢？已经调好超参的Adam优化器，怎么快速切换到Muon上进行尝试呢？模型Scale上去之后，Muon与Adam的性能效果差异如何？接下来将分享我们的思考过程。</p>
<h2 id="_1">优化原理<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>关于优化器，其实笔者之前在<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>时就有过浅评，多数优化器改进实际上都只是一些小补丁，不能说毫无价值，但终究没能给人一种深刻和惊艳的感觉。</p>
<p>我们需要从更贴近本质的原理出发，来思考什么才是好的优化器。直观来想，理想的优化器应当有两个特性：<strong>稳</strong> 和<strong>快</strong> 。具体来说，理想的优化器每一步的更新应该满足两点：1、对模型扰动尽可能小；2、对Loss贡献尽可能大。说更直接点，就是我们不希望大改模型（稳），但希望大降Loss（快），典型的“既要...又要...”。</p>
<p>怎么将这两个特性转化为数学语言呢？<strong>稳</strong> 我们可以理解为对更新量的一个约束，而<strong>快</strong> 则可以理解为寻找让损失函数下降最快的更新量，所以这可以转化为一个约束优化问题。用回前文的记号，对于矩阵参数$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，其梯度为$\boldsymbol{G}\in\mathbb{R}^{n\times m}$，当参数由$\boldsymbol{W}$变成$\boldsymbol{W}+\Delta\boldsymbol{W}$时，损失函数的变化量为<br />
\begin{equation}\text{Tr}(\boldsymbol{G}^{\top}\Delta\boldsymbol{W})\end{equation}<br />
那么在<strong>稳</strong> 的前提下寻找<strong>快</strong> 的更新量，那么就可以表示为<br />
\begin{equation}\mathop{\text{argmin}}_{\Delta\boldsymbol{W}}\text{Tr}(\boldsymbol{G}^{\top}\Delta\boldsymbol{W})\quad\text{s.t.}\quad \rho(\Delta\boldsymbol{W})\leq \eta\label{eq:least-action}\end{equation}<br />
这里$\rho(\Delta\boldsymbol{W})\geq 0$是<strong>稳</strong> 的某个指标，越小表示越稳，而$\eta$是某个小于1的常数，表示我们对<strong>稳</strong> 的要求，后面我们会看到它实际上就是优化器的学习率。如果读者不介意，我们可以模仿理论物理的概念，称上述原理为优化器的“<strong>最小作用量原理（Least Action Principle）</strong> ”。</p>
<h2 id="_2">矩阵范数<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>式$\eqref{eq:least-action}$唯一不确定的就是<strong>稳</strong> 的度量$\rho(\Delta\boldsymbol{W})$，选定$\rho(\Delta\boldsymbol{W})$后就可以把$\Delta\boldsymbol{W}$明确求解出来（至少理论上没问题）。某种程度上来说，我们可以认为不同优化器的本质差异就是它们对<strong>稳</strong> 的定义不一样。</p>
<p>很多读者在刚学习SGD时想必都看到过类似“梯度反方向是函数值局部下降最快的方向”的说法，放到这里的框架来看，它其实就是把<strong>稳</strong> 的度量选择为矩阵的$F$范数$\Vert\Delta\boldsymbol{W}\Vert_F$，也就是说，“下降最快的方向”并不是一成不变的，选定度量后才能把它确定下来，换一个范数就不一定是梯度反方向了。</p>
<p>接下来的问题自然是什么范数才能最恰当地度量<strong>稳</strong> ？如果我们加了强约束，那么稳则稳矣，但优化器举步维艰，只能收敛到次优解；相反如果减弱约束，那么优化器放飞自我，那么训练进程将会极度不可控。所以，最理想的情况就能找到<strong>稳</strong> 的最精准指标。考虑到神经网络以矩阵乘法为主，我们以$\boldsymbol{y}=\boldsymbol{x}\boldsymbol{W}$为例，有<br />
\begin{equation}\Vert\Delta \boldsymbol{y}\Vert = \Vert\boldsymbol{x}(\boldsymbol{W} + \Delta\boldsymbol{W}) - \boldsymbol{x}\boldsymbol{W}\Vert = \Vert\boldsymbol{x} \Delta\boldsymbol{W}\Vert\leq \rho(\Delta\boldsymbol{W}) \Vert\boldsymbol{x}\Vert\end{equation}<br />
上式的意思是，当参数由$\boldsymbol{W}$变成$\boldsymbol{W}+\Delta\boldsymbol{W}$时，模型输出变化量为$\Delta\boldsymbol{y}$，我们寄望于这个变化量的模长能够被$\Vert\boldsymbol{x}\Vert$以及$\Delta\boldsymbol{W}$相关的一个函数$\rho(\Delta\boldsymbol{W})$控制，我们就用这个函数作为<strong>稳</strong> 的指标。从线性代数我们知道，$\rho(\Delta\boldsymbol{W})$的最准确值就是$\Delta\boldsymbol{W}$的谱范数$\Vert\Delta\boldsymbol{W}\Vert_2$，代入式$\eqref{eq:least-action}$得到<br />
\begin{equation}\mathop{\text{argmin}}<em _:_:r_="[:,:r]">{\Delta\boldsymbol{W}}\text{Tr}(\boldsymbol{G}^{\top}\Delta\boldsymbol{W})\quad\text{s.t.}\quad \Vert\Delta\boldsymbol{W}\Vert_2\leq \eta\end{equation}<br />
这个优化问题求解出来就是$\beta=0$的Muon：<br />
\begin{equation}\Delta\boldsymbol{W} = -\eta\, \text{msign}(\boldsymbol{G}) = -\eta\,\boldsymbol{U}</em>}\boldsymbol{V}_{[:,:r]}^{\top}, \quad \boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}^{\top} = \mathop{\text{SVD}}(\boldsymbol{G})\end{equation<br />
当$\beta &gt; 0$时，$\boldsymbol{G}$换成动量$\boldsymbol{M}$，$\boldsymbol{M}$可以看作是对梯度更平滑的估计，所以依然可以理解为上式的结论，因此我们可以得出“Muon就是谱范数下的最速下降”的说法，至于Newton-schulz迭代之类的，则是计算上的近似，这里就不细说。详细推导我们在<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>已经给出，也不再重复。</p>
<h2 id="_3">权重衰减<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>至此，我们可以回答第一个问题：为什么选择尝试Muon？因为跟SGD一样，Muon给出的同样是下降最快的方向，但它的谱范数约束比SGD的$F$范数更为精准，所以有更佳的潜力。另一方面，从“为不同的参数选择最恰当的约束”角度来改进优化器，看起来也比各种补丁式修改更为本质。</p>
<p>当然，潜力不意味着实力，在更大尺寸的模型上验证Muon存在一些“陷阱”。首先登场的是Weight Decay问题，尽管我们在<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>介绍Muon的时候带上了Weight Decay，但实际上作者提出Muon时是没有的，而我们一开始也按照官方版本来实现，结果发现Muon前期收敛是快，但很快就被Adam追上了，而且各种“内科”还有崩溃的苗头。</p>
<p>我们很快意识到这可能是Weight Decay的问题，于是补上Weight Decay：<br />
\begin{equation}\Delta\boldsymbol{W} = -\eta\, [\text{msign}(\boldsymbol{M})+ \lambda \boldsymbol{W}]\end{equation}<br />
继续实验，果不其然，这时候Muon一直保持领先于Adam，如论文Figure 2所示：  </p>
<p><a href="/usr/uploads/2025/02/3675708776.png" title="点击查看原图"><img alt="有无Weight Decay的效果比较" src="/usr/uploads/2025/02/3675708776.png" /></a></p>
<p>有无Weight Decay的效果比较</p>
<p>Weight Decay起到什么作用呢？事后分析来看，可能比较关键的地方是能够让参数范数保持有界：<br />
\begin{equation}\begin{aligned}<br />
\Vert\boldsymbol{W}<em t-1="t-1">t\Vert =&amp;\, \Vert\boldsymbol{W}</em>} - \eta_t (\boldsymbol{\Phi<em t-1="t-1">t + \lambda \boldsymbol{W}</em>)\Vert \\[5pt]<br />
=&amp;\, \Vert(1 - \eta_t \lambda)\boldsymbol{W}<em t-1="t-1">{t-1} - \eta_t \lambda (\boldsymbol{\Phi}_t/\lambda)\Vert \\[5pt]<br />
\leq &amp;\,(1 - \eta_t \lambda)\Vert\boldsymbol{W}</em>}\Vert + \eta_t \lambda \Vert\boldsymbol{\Phi<em t-1="t-1">t/\lambda\Vert \\[5pt]<br />
\leq &amp;\,\max(\Vert\boldsymbol{W}</em>}\Vert,\Vert\boldsymbol{\Phi<em t-1="t-1">t/\lambda\Vert) \\[5pt]<br />
\end{aligned}\end{equation}<br />
这里的$\Vert\cdot\Vert$是任意一种矩阵范数，即上述不等式对于任意矩阵范数都是成立的，$\boldsymbol{\Phi}_t$是优化器给出的更新向量，对Muon来说是$\text{msign}(\boldsymbol{M})$，当我们取谱范数时，有$\Vert\text{msign}(\boldsymbol{M})\Vert_2 = 1$，所以对于Muon来说有<br />
\begin{equation}<br />
\Vert\boldsymbol{W}_t\Vert_2 \leq \max(\Vert\boldsymbol{W}</em>}\Vert_2,1/\lambda)\leq\cdots \leq \max(\Vert\boldsymbol{W}_0\Vert_2,1/\lambda)\end{equation<br />
这保证了模型“内科”的健康，因为$\Vert\boldsymbol{x}\boldsymbol{W}\Vert\leq \Vert\boldsymbol{x}\Vert\Vert\boldsymbol{W}\Vert_2$，$\Vert\boldsymbol{W}\Vert_2$被控制住了，意味着$\Vert\boldsymbol{x}\boldsymbol{W}\Vert$也被控制住了，就不会有爆炸的风险，这对于Attention Logits爆炸等问题也尤其重要。当然这个上界在多数情况下还是相当宽松的，实际中参数的谱范数多数会明显小于这个上界，这个不等关系只是简单显示Weight Decay有控制范数的性质存在。</p>
<h2 id="rms">RMS对齐<a class="toc-link" href="#rms" title="Permanent link">&para;</a></h2>
<p>当我们决定去尝试新的优化器时，一个比较头疼的问题是如何快速找到接近最优的超参数，比如Muon至少有学习率$\eta_t$和衰减率$\lambda$两个超参数。网格搜索自然是可以，但比较费时费力，这里我们提出Update RMS对齐的超参迁移思路，可以将Adam调好的超参数用到其他优化器上。</p>
<p>首先，对于一个矩阵$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，它的RMS（Root Mean Square）定义为<br />
\begin{equation}\text{RMS}(\boldsymbol{W}) = \frac{\Vert \boldsymbol{W}\Vert_F}{\sqrt{nm}} = \sqrt{\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^m W_{i,j}^2}\end{equation}<br />
简单来说，RMS度量了矩阵每个元素的平均大小。我们观察到Adam更新量的RMS是比较稳定的，通常在0.2～0.4之间，这也是为什么<a href="/archives/10542">理论分析</a>常用SignSGD作为Adam近似。基于此，我们建议通过RMS Norm将新优化器的Update RMS对齐到0.2：<br />
\begin{gather}<br />
\boldsymbol{W}<em t-1="t-1">t =\boldsymbol{W}</em>} - \eta_t (\boldsymbol{\Phi<em t-1="t-1">t + \lambda \boldsymbol{W}</em>) \\[6pt]<br />
\downarrow \notag\\[6pt]<br />
\boldsymbol{W}<em t-1="t-1">t = \boldsymbol{W}</em>} - \eta_t (0.2\, \boldsymbol{\Phi<em t-1="t-1">t/\text{RMS}(\boldsymbol{\Phi}_t) + \lambda \boldsymbol{W}</em>)<br />
\end{gather}<br />
这样一来，我们就可以复用Adam的$\eta_t$和$\lambda$，以达到每步对参数的更新幅度大致相同的效果。实践表明，通过这个简单策略从Adam迁移到Muon，就能训出明显优于Adam的效果，接近进一步对Muon超参进行精搜索的结果。特别地，Muon的$\text{RMS}(\boldsymbol{\Phi}<em _:_:r_="[:,:r]">t)=\text{RMS}(\boldsymbol{U}</em>}\boldsymbol{V<em i="1">{[:,:r]}^{\top})$还可以解析地算出来：<br />
\begin{equation}nm\,\text{RMS}(\boldsymbol{\Phi}_t)^2 = \sum</em>}^n\sum_{j=1}^m \sum_{k=1}^r U_{i,k}^2V_{k,j}^2 = \sum_{k=1}^r\left(\sum_{i=1}^n U_{i,k}^2\right)\left(\sum_{j=1}^m V_{k,j}^2\right) = \sum_{k=1}^r 1 = r\end{equation<br />
即$\text{RMS}(\boldsymbol{\Phi}<em t-1="t-1">t) = \sqrt{r/nm}$，实践中一个矩阵严格低秩的概率比较小，因此可以认为$r = \min(n,m)$，从而有$\text{RMS}(\boldsymbol{\Phi}_t) = \sqrt{1/\max(n,m)}$。所以我们最终没有用RMS Norm而是用等价的解析版本：<br />
\begin{equation}\boldsymbol{W}_t = \boldsymbol{W}</em>} - \eta_t (0.2\, \boldsymbol{\Phi<em t-1="t-1">t\,\sqrt{\max(n,m)} + \lambda \boldsymbol{W}</em>})\end{equation<br />
最后的这个式子，表明了在Muon中不适宜所有参数使用同一个学习率。比如Moonlight是一个MoE模型，有不少矩阵参数的形状都偏离方阵，$\max(n,m)$跨度比较大，如果用单一学习率，必然会导致某些参数学习过快/过慢的不同步问题，从而影响最终效果。</p>
<h2 id="_4">实验分析<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>我们在2.4B/16B这个尺寸的MoE上，对Adam和Muon做了比较充分的对比，发现Muon在收敛速度和最终效果上都有明显的优势。详细的比较结果建议大家去看原论文，这里仅截取部分分享。</p>
<blockquote>
<p><strong>Github:<a href="https://github.com/MoonshotAI/Moonlight">https://github.com/MoonshotAI/Moonlight</a></strong></p>
</blockquote>
<p>首先是一个相对客观的对照表格，包括我们自己控制变量训练的Muon和Adam的对比，以及与外界（DeepSeek）用Adam训练的同样架构的模型对比（为了便于对比，Moonlight的架构跟DSV3-Small完全一致），显示出Muon的独特优势：  </p>
<p><a href="/usr/uploads/2025/02/2528279021.png" title="点击查看原图"><img alt="Muon（Moonlight） vs Adam（Moonlight-A 和DSV3-small）的比较" src="/usr/uploads/2025/02/2528279021.png" /></a></p>
<p>Muon（Moonlight） vs Adam（Moonlight-A 和DSV3-small）的比较</p>
<p>Muon训练出来的模型有什么不同呢？既然前面我们说Muon是谱范数下的最速下降，谱范数是最大的奇异值，所以我们想到了监控和分析奇异值。果然，我们发现了一些有趣的信号，Muon训出来的参数，奇异值分布相对更均匀一些，我们使用奇异值熵来定量描述这个现象：<br />
\begin{equation}H(\boldsymbol{\sigma}) = -\frac{1}{\log n}\sum_{i=1}^n \frac{\sigma_i^2}{\sum_{j=1}^n\sigma_j^2}\log \frac{\sigma_i^2}{\sum_{j=1}^n\sigma_j^2}\end{equation}<br />
这里$\boldsymbol{\sigma}=(\sigma_1,\sigma_2,\cdots,\sigma_n)$是某个参数的全体奇异值。Muon训出来的参数熵更大，即奇异值分布更均匀，意味着这个参数越不容易压缩，这说明Muon更充分发挥了参数的潜能：  </p>
<p><a href="/usr/uploads/2025/02/3782823216.png" title="点击查看原图"><img alt="Muon训练出来的权重奇异值熵更高" src="/usr/uploads/2025/02/3782823216.png" /></a></p>
<p>Muon训练出来的权重奇异值熵更高</p>
<p>还有一个有趣的发现是当我们将Muon用于微调（SFT）时，可能会因为预训练没用Muon而得到次优解。具体来说，如果预训练和微调都用Muon，那么表现是最好的，但如果是另外三种组合（Adam+Muon、Muon+Adam、Adam+Adam），其效果优劣没有呈现于明显的规律。  </p>
<p><a href="/usr/uploads/2025/02/2697758072.png" title="点击查看原图"><img alt="预训练/微调分别用Muon/Adam的组合测试" src="/usr/uploads/2025/02/2697758072.png" /></a></p>
<p>预训练/微调分别用Muon/Adam的组合测试</p>
<p><a href="/usr/uploads/2025/02/3565677851.png" title="点击查看原图"><img alt="在开源模型上用Muon/Adam微调的尝试" src="/usr/uploads/2025/02/3565677851.png" /></a></p>
<p>在开源模型上用Muon/Adam微调的尝试</p>
<p>这个现象表明存在一些特殊的初始化对Muon不利，当然反过来也可能存在一些初始化对Muon更有利，更底层的原理我们还在探索中。</p>
<h2 id="_5">拓展思考<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>总的来说，在我们的实验里，Muon的表现跟Adam相比显得非常有竞争力。作为一个形式上跟Adam差异较大的新优化器，Muon的这个表现其实不单单是“可圈可点”了，还表明它可能捕捉到了一些本质的特性。</p>
<p>此前，社区流传着一个观点：Adam之所以表现好，是因为主流的模型架构改进都在“过拟合”Adam。这个观点最早应该出自<a href="https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/">《Neural Networks (Maybe) Evolved to Make Adam The Best Optimizer》</a>，看上去有点荒谬，但实际上意蕴深长。试想一下，当我们尝试改进模型后，就会拿Adam训一遍看效果，效果好就保留，否则放弃。可这个效果好，究竟是因为它本质更佳，还是因为它更匹配Adam了呢？</p>
<p>这就有点耐人寻味了。当然不说全部，肯定至少有一部份工作，是因为它跟Adam更配而表现出更好的效果，所以久而久之，模型架构就会朝着一个有利于Adam的方向演进。在这个背景下，一个跟Adam显著不同的优化器还能“出圈”，就尤其值得关注和思考了。注意笔者和所在公司都不属Muon提出者，所以这番言论纯属“肺腑之言”，并不存在自卖自夸的意思。</p>
<p>接下来Muon还有什么工作可做呢？其实应该还有不少。比如上面提到的“Adam预训练+Muon微调”效果不佳问题，进一步分析还是有必要和有价值的，毕竟现在大家开源的模型权重基本都是Adam训的，如果Muon微调不行，必然也影响它的普及。当然了，我们还可以借着这个契机进一步深化对Muon理解（面向Bug学习）。</p>
<p>还有一个推广的思考，就是Muon基于谱范数，谱范数是最大奇异值，事实上基于奇异值我们还可以构造一系列范数，如<a href="https://en.wikipedia.org/wiki/Schatten_norm">Schatten范数</a>，将它推广到这种更广义的范数然后进行调参，理论上还有机会取得更好的效果。此外，Moonlight发布后，还有一些读者问到Muon下<a href="https://papers.cool/arxiv/2203.03466">µP（maximal update parametrization）</a>如何设计，这也是一个亟待解决的问题。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文介绍了我们在Muon优化器上的一次较大规模实践（Moonlight），并分享了我们对Muon优化器的最新思考。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10739">https://spaces.ac.cn/archives/10739</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Feb. 27, 2025). 《Muon续集：为什么我们选择尝试Muon？ 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10739">https://spaces.ac.cn/archives/10739</a></p>
<p>@online{kexuefm-10739,<br />
title={Muon续集：为什么我们选择尝试Muon？},<br />
author={苏剑林},<br />
year={2025},<br />
month={Feb},<br />
url={\url{https://spaces.ac.cn/archives/10739}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="moe环游记2不患寡而患不均.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#183 MoE环游记：2、不患寡而患不均</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="moe环游记3换个思路来分配.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#185 MoE环游记：3、换个思路来分配</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#muonmuon">Muon续集：为什么我们选择尝试Muon？</a><ul>
<li><a href="#_1">优化原理</a></li>
<li><a href="#_2">矩阵范数</a></li>
<li><a href="#_3">权重衰减</a></li>
<li><a href="#rms">RMS对齐</a></li>
<li><a href="#_4">实验分析</a></li>
<li><a href="#_5">拓展思考</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>