<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>熵不变性Softmax的一个快速推导 | ML & Math Blog Posts</title>
    <meta name="description" content="熵不变性Softmax的一个快速推导&para;
原文链接: https://spaces.ac.cn/archives/9034
发布日期: 

在文章《从熵不变性看Attention的Scale操作》中，我们推导了一版具有熵不变性质的注意力机制：
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\kappa \log n}{d}QK^...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=近似">近似</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #177 熵不变性Softmax的一个快速推导
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#177</span>
                熵不变性Softmax的一个快速推导
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-04-11</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=近似" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 近似</span>
                </a>
                
                <a href="../index.html?tags=熵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 熵</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="softmax">熵不变性Softmax的一个快速推导<a class="toc-link" href="#softmax" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9034">https://spaces.ac.cn/archives/9034</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>在文章<a href="/archives/8823">《从熵不变性看Attention的Scale操作》</a>中，我们推导了一版具有熵不变性质的注意力机制：<br />
\begin{equation}Attention(Q,K,V) = softmax\left(\frac{\kappa \log n}{d}QK^{\top}\right)V\label{eq:a}\end{equation}<br />
可以观察到，它主要是往Softmax里边引入了长度相关的缩放因子$\log n$来实现的。原来的推导比较繁琐，并且做了较多的假设，不利于直观理解，本文为其补充一个相对简明快速的推导。</p>
<h2 id="_1">推导过程<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>我们可以抛开注意力机制的背景，直接设有$s_1,s_2,\cdots,s_n\in\mathbb{R}$，定义<br />
$$p_i = \frac{e^{\lambda s_i}}{\sum\limits_{i=1}^n e^{\lambda s_i}}$$<br />
显然这就是$s_1,s_2,\cdots,s_n$同时乘上缩放因子$\lambda$后做Softmax的结果。现在我们算它的熵<br />
\begin{equation}\begin{aligned}H =&amp;\, -\sum_{i=1}^n p_i \log p_i = \log\sum_{i=1}^n e^{\lambda s_i} - \lambda\sum_{i=1}^n p_i s_i \\
=&amp;\, \log n + \log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} - \lambda\sum_{i=1}^n p_i s_i
\end{aligned}\end{equation}<br />
第一项的$\log$里边是“先指数后平均”，我们用“先平均后指数”（平均场）来近似它：<br />
\begin{equation}
\log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i}\approx \log\exp\left(\frac{1}{n}\sum_{i=1}^n \lambda s_i\right) = \lambda \bar{s}
\end{equation}<br />
然后我们知道Softmax是会侧重于$\max$的那个（参考<a href="/archives/6620#softmax">《函数光滑化杂谈：不可导函数的可导逼近》</a>），所以有近似<br />
\begin{equation}\lambda\sum_{i=1}^n p_i s_i \approx \lambda s_{\max}\end{equation}<br />
所以<br />
\begin{equation}H\approx \log n - \lambda(s_{\max} - \bar{s})\end{equation}<br />
所谓熵不变性，就是希望尽可能地消除长度$n$的影响，所以根据上式我们需要有$\lambda\propto \log n$。如果放到注意力机制中，那么$s$的形式为$\langle \boldsymbol{q}, \boldsymbol{k}\rangle\propto d$（$d$是向量维度），所以需要有$\lambda\propto \frac{1}{d}$，综合起来就是<br />
\begin{equation}\lambda\propto \frac{\log n}{d}\end{equation}<br />
这就是文章开头式$\eqref{eq:a}$的结果。</p>
<h2 id="_2">文章小结<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>为之前提出的“熵不变性Softmax”构思了一个简单明快的推导。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9034">https://spaces.ac.cn/archives/9034</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 11, 2022). 《熵不变性Softmax的一个快速推导 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9034">https://spaces.ac.cn/archives/9034</a></p>
<p>@online{kexuefm-9034,<br />
title={熵不变性Softmax的一个快速推导},<br />
author={苏剑林},<br />
year={2022},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/9034}},<br />
} </p>
<hr />
<h2 id="_3">详细数学推导与理论分析<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<h3 id="1">1. 问题的背景与动机<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<p><strong>标准Scaled Dot-Product Attention</strong>：</p>
<p>在Transformer中，注意力机制定义为：
\begin{equation}
\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\boldsymbol{Q}\boldsymbol{K}^\top}{\sqrt{d}}\right) \boldsymbol{V} \tag{1}
\end{equation}</p>
<p>其中$\sqrt{d}$是缩放因子，$d$是向量维度。</p>
<p><strong>问题</strong>：为什么是$\sqrt{d}$而不是其他值？</p>
<p>标准解释：保持方差不变（见Vaswani et al. 2017）。</p>
<p><strong>本文的视角</strong>：从<strong>熵不变性</strong>的角度理解缩放因子，并推导出与序列长度$n$相关的缩放。</p>
<h3 id="2">2. 熵不变性的定义<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<p><strong>直觉</strong>：我们希望注意力分布的熵不随序列长度$n$的变化而剧烈变化。</p>
<p><strong>定义</strong>：对于注意力权重$\boldsymbol{\alpha} = [\alpha_1, \ldots, \alpha_n]$，其熵为：
\begin{equation}
H(\boldsymbol{\alpha}) = -\sum_{i=1}^n \alpha_i \log \alpha_i \tag{2}
\end{equation}</p>
<p><strong>熵不变性条件</strong>：希望存在缩放因子$\lambda(n, d)$，使得：
\begin{equation}
H\left(\text{softmax}(\lambda \boldsymbol{s})\right) \approx C \tag{3}
\end{equation}</p>
<p>其中$C$是与$n$无关的常数，$\boldsymbol{s} = [s_1, \ldots, s_n]$是注意力分数。</p>
<h3 id="3">3. 熵的展开式<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<p><strong>Softmax定义</strong>：
\begin{equation}
\alpha_i = \frac{e^{\lambda s_i}}{\sum_{j=1}^n e^{\lambda s_j}} \tag{4}
\end{equation}</p>
<p><strong>熵的计算</strong>：
\begin{equation}
\begin{aligned}
H &amp;= -\sum_{i=1}^n \alpha_i \log \alpha_i \
&amp;= -\sum_{i=1}^n \alpha_i \left(\lambda s_i - \log\sum_{j=1}^n e^{\lambda s_j}\right) \
&amp;= -\lambda \sum_{i=1}^n \alpha_i s_i + \sum_{i=1}^n \alpha_i \log\sum_{j=1}^n e^{\lambda s_j} \
&amp;= \log\sum_{j=1}^n e^{\lambda s_j} - \lambda \sum_{i=1}^n \alpha_i s_i
\end{aligned} \tag{5}
\end{equation}</p>
<p>记<strong>配分函数</strong>（partition function）：
\begin{equation}
Z = \sum_{j=1}^n e^{\lambda s_j} \tag{6}
\end{equation}</p>
<p>则：
\begin{equation}
H = \log Z - \lambda \mathbb{E}_\alpha[s] \tag{7}
\end{equation}</p>
<p>其中$\mathbb{E}_\alpha[s] = \sum_i \alpha_i s_i$是加权平均分数。</p>
<h3 id="4">4. 平均场近似<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<p><strong>关键步骤</strong>：对配分函数进行近似。</p>
<p><strong>Jensen不等式的反向应用</strong>：</p>
<p>对于凸函数$\exp$，有：
\begin{equation}
\exp\left(\frac{1}{n}\sum_{i=1}^n \lambda s_i\right) \leq \frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} \tag{8}
\end{equation}</p>
<p>但我们需要的是对$\log Z$的近似，而不是$Z$本身。</p>
<p><strong>平均场近似</strong>（"先平均后指数"近似"先指数后平均"）：</p>
<p>\begin{equation}
\log Z = \log\sum_{i=1}^n e^{\lambda s_i} = \log\left(n \cdot \frac{1}{n}\sum_{i=1}^n e^{\lambda s_i}\right) = \log n + \log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} \tag{9}
\end{equation}</p>
<p><strong>关键近似</strong>：
\begin{equation}
\log\frac{1}{n}\sum_{i=1}^n e^{\lambda s_i} \approx \log\exp\left(\frac{1}{n}\sum_{i=1}^n \lambda s_i\right) = \lambda \bar{s} \tag{10}
\end{equation}</p>
<p>其中$\bar{s} = \frac{1}{n}\sum_i s_i$是算术平均。</p>
<p><strong>代入式(9)</strong>：
\begin{equation}
\log Z \approx \log n + \lambda \bar{s} \tag{11}
\end{equation}</p>
<h3 id="5-softmaxmax">5. Softmax侧重max的性质<a class="toc-link" href="#5-softmaxmax" title="Permanent link">&para;</a></h3>
<p><strong>Softmax作为max的平滑近似</strong>：</p>
<p>回顾不等式：
\begin{equation}
\max_i s_i \leq \log\sum_i e^{s_i} \leq \max_i s_i + \log n \tag{12}
\end{equation}</p>
<p><strong>加权平均的近似</strong>：</p>
<p>Softmax会侧重于较大的$s_i$，因此：
\begin{equation}
\mathbb{E}<em _max="\max">\alpha[s] = \sum_i \alpha_i s_i \approx s</em>
\end{equation}} \tag{13</p>
<p>其中$s_{\max} = \max_i s_i$。</p>
<p><strong>理由</strong>：当$\lambda$适中时，最大的$s_i$对应的$\alpha_i$占主导。</p>
<h3 id="6">6. 熵的最终近似<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<p>结合式(7)、(11)和(13)：
\begin{equation}
H \approx \log n + \lambda \bar{s} - \lambda s_{\max} = \log n - \lambda (s_{\max} - \bar{s}) \tag{14}
\end{equation}</p>
<p><strong>定义波动幅度</strong>：
\begin{equation}
\Delta s = s_{\max} - \bar{s} \tag{15}
\end{equation}</p>
<p>则：
\begin{equation}
H \approx \log n - \lambda \Delta s \tag{16}
\end{equation}</p>
<h3 id="7">7. 熵不变性条件<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p><strong>目标</strong>：使$H$不依赖于$n$。</p>
<p>从式(16)，如果我们希望$H \approx C$（常数），则需要：
\begin{equation}
\log n - \lambda \Delta s \approx C \tag{17}
\end{equation}</p>
<p>移项：
\begin{equation}
\lambda \Delta s \approx \log n - C \tag{18}
\end{equation}</p>
<p><strong>假设$\Delta s$与$n$无关</strong>（合理假设：分数的波动主要由数据和模型决定），则：
\begin{equation}
\lambda \propto \log n \tag{19}
\end{equation}</p>
<h3 id="8-d">8. 与维度$d$的关系<a class="toc-link" href="#8-d" title="Permanent link">&para;</a></h3>
<p><strong>分数的形式</strong>：</p>
<p>在注意力机制中，$s_i = \langle \boldsymbol{q}, \boldsymbol{k}_i \rangle$，其中$\boldsymbol{q}, \boldsymbol{k}_i \in \mathbb{R}^d$。</p>
<p><strong>方差分析</strong>：</p>
<p>假设$\boldsymbol{q}$和$\boldsymbol{k}_i$的元素独立同分布，$\mathbb{E}[q_j] = 0$，$\text{Var}(q_j) = \sigma^2$。</p>
<p>则：
\begin{equation}
s_i = \sum_{j=1}^d q_j k_{i,j} \tag{20}
\end{equation}</p>
<p>期望：$\mathbb{E}[s_i] = 0$</p>
<p>方差：
\begin{equation}
\text{Var}(s_i) = d \cdot \mathbb{E}[q_j^2] \mathbb{E}[k_{i,j}^2] = d\sigma^4 \tag{21}
\end{equation}</p>
<p><strong>标准化</strong>：</p>
<p>为了使方差不随$d$变化，我们除以$\sqrt{d}$：
\begin{equation}
s_i' = \frac{s_i}{\sqrt{d}} \Rightarrow \text{Var}(s_i') = \sigma^4 \tag{22}
\end{equation}</p>
<p><strong>结合两者</strong>：</p>
<p>总的缩放因子应该是：
\begin{equation}
\lambda \propto \frac{\log n}{d} \tag{23}
\end{equation}</p>
<p>更精确地：
\begin{equation}
\lambda = \frac{\kappa \log n}{d} \tag{24}
\end{equation}</p>
<p>其中$\kappa$是一个与$n$和$d$无关的常数（通常取$\kappa = 1$）。</p>
<h3 id="9">9. 最终的熵不变性注意力<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p><strong>提出的公式</strong>：
\begin{equation}
\boxed{\text{Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\kappa \log n}{d} \boldsymbol{Q}\boldsymbol{K}^\top\right) \boldsymbol{V}} \tag{25}
\end{equation}</p>
<p><strong>与标准注意力的对比</strong>：</p>
<p>标准注意力使用$\frac{1}{\sqrt{d}}$，只考虑了维度$d$。</p>
<p>熵不变性注意力使用$\frac{\kappa \log n}{d}$，同时考虑了维度$d$和序列长度$n$。</p>
<h3 id="10-laplace">10. 严格的数学推导（Laplace近似）<a class="toc-link" href="#10-laplace" title="Permanent link">&para;</a></h3>
<p>上述推导使用了较粗糙的近似。现在我们用Laplace近似进行更严格的分析。</p>
<p><strong>Laplace近似的思想</strong>：</p>
<p>对于积分或求和：
\begin{equation}
I = \int e^{nf(x)} dx \approx e^{nf(x^<em>)} \sqrt{\frac{2\pi}{n|f''(x^</em>)|}} \tag{26}
\end{equation}</p>
<p>其中$x^* = \arg\max_x f(x)$。</p>
<p><strong>应用到配分函数</strong>：</p>
<p>\begin{equation}
Z = \sum_{i=1}^n e^{\lambda s_i} \tag{27}
\end{equation}</p>
<p>改写为连续形式（假设$s_i$是从某个分布采样的）：
\begin{equation}
Z \approx n \int e^{\lambda s} p(s) ds \tag{28}
\end{equation}</p>
<p>其中$p(s)$是$s$的分布。</p>
<p><strong>对数配分函数</strong>：</p>
<p>\begin{equation}
\log Z = \log n + \log\int e^{\lambda s} p(s) ds \tag{29}
\end{equation}</p>
<p><strong>Laplace近似</strong>（当$\lambda$适中时）：</p>
<p>\begin{equation}
\log\int e^{\lambda s} p(s) ds \approx \lambda s^* - \frac{1}{2}\log(2\pi) + \frac{1}{2}\log\lambda \tag{30}
\end{equation}</p>
<p>其中$s^*$是使$\lambda s + \log p(s)$最大的点。</p>
<p>对于均匀分布$p(s)$，$s^* = s_{\max}$。</p>
<p><strong>简化</strong>（忽略常数项）：
\begin{equation}
\log Z \approx \log n + \lambda s_{\max} \tag{31}
\end{equation}</p>
<p><strong>熵的计算</strong>（使用$\mathbb{E}<em _max="\max">\alpha[s] \approx s</em>$）：
\begin{equation}
H = \log Z - \lambda \mathbb{E}<em _max="\max">\alpha[s] \approx \log n + \lambda s</em>
\end{equation}} - \lambda s_{\max} = \log n \tag{32</p>
<p>这给出了$H \approx \log n$，恰好是均匀分布的熵！</p>
<p><strong>修正</strong>：实际上$\mathbb{E}<em _max="\max">\alpha[s] \neq s</em>$，所以需要更精细的分析。</p>
<h3 id="11">11. 二阶修正与波动<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>更精确的$\mathbb{E}_\alpha[s]$</strong>：</p>
<p>考虑softmax的二阶展开：
\begin{equation}
\alpha_i = \frac{e^{\lambda s_i}}{\sum_j e^{\lambda s_j}} \approx \frac{e^{\lambda (s_i - s_{\max})}}{\sum_j e^{\lambda (s_j - s_{\max})}} \tag{33}
\end{equation}</p>
<p>记$\Delta_i = s_i - s_{\max} \leq 0$，则：
\begin{equation}
\alpha_i \approx \frac{e^{\lambda \Delta_i}}{1 + \sum_{j \neq i^*} e^{\lambda \Delta_j}} \tag{34}
\end{equation}</p>
<p>其中$i^* = \arg\max_i s_i$。</p>
<p><strong>当$\lambda$适中时</strong>：</p>
<p>\begin{equation}
\alpha_{i^<em>} \approx \frac{1}{1 + \sum_{j \neq i^</em>} e^{\lambda \Delta_j}} \tag{35}
\end{equation}</p>
<p><strong>平均分数</strong>：
\begin{equation}
\mathbb{E}<em>\alpha[s] \approx \alpha</em>{i^<em>} s_{\max} + \sum_{j \neq i^</em>} \alpha_j s_j \tag{36}
\end{equation}</p>
<p><strong>简化</strong>（假设其他$\alpha_j$较小）：
\begin{equation}
\mathbb{E}<em _max="\max">\alpha[s] \approx s</em>
\end{equation}} - \Delta s \tag{37</p>
<p>其中$\Delta s = s_{\max} - \bar{s}$的具体值依赖于分布。</p>
<h3 id="12">12. 缩放因子的渐近分析<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<p><strong>大$n$极限</strong>：</p>
<p>假设$s_i \sim \mathcal{N}(\mu, \sigma^2)$独立同分布。</p>
<p><strong>极值分布</strong>：</p>
<p>当$n \to \infty$时，$s_{\max} = \max_i s_i$的分布趋向于Gumbel分布。</p>
<p>具体地：
\begin{equation}
s_{\max} \approx \mu + \sigma\sqrt{2\log n} \tag{38}
\end{equation}</p>
<p><strong>平均值</strong>：
\begin{equation}
\bar{s} = \mathbb{E}[s] = \mu \tag{39}
\end{equation}</p>
<p><strong>波动</strong>：
\begin{equation}
\Delta s = s_{\max} - \bar{s} \approx \sigma\sqrt{2\log n} \tag{40}
\end{equation}</p>
<p><strong>代入式(16)</strong>：
\begin{equation}
H \approx \log n - \lambda \sigma\sqrt{2\log n} \tag{41}
\end{equation}</p>
<p><strong>熵不变性条件</strong>：</p>
<p>希望$H$不依赖于$n$，但从式(41)看，这似乎不可能！</p>
<p><strong>分辨率</strong>：实际上，式(41)中的$\log n$项和$\lambda\sqrt{2\log n}$项都与$n$有关，我们需要它们相互抵消。</p>
<p>但$\log n$和$\sqrt{\log n}$的增长速度不同，无法完全抵消。</p>
<p><strong>修正理解</strong>：</p>
<p>"熵不变性"不是指$H$完全不变，而是指$H$的主导项$\log n$被合理控制。</p>
<p>具体地，我们希望：
\begin{equation}
H \approx C \cdot \log n \tag{42}
\end{equation}</p>
<p>其中$C$是一个接近1的常数。</p>
<h3 id="13-johnson-lindenstrauss">13. 与Johnson-Lindenstrauss引理的联系<a class="toc-link" href="#13-johnson-lindenstrauss" title="Permanent link">&para;</a></h3>
<p><strong>Johnson-Lindenstrauss (JL) 引理</strong>：</p>
<p>对于$n$个点在高维空间$\mathbb{R}^d$中，可以将它们投影到$O(\log n / \epsilon^2)$维空间，保持距离在$(1-\epsilon, 1+\epsilon)$范围内。</p>
<p><strong>与注意力的类比</strong>：</p>
<p>注意力机制可以理解为一种<strong>软投影</strong>：从$n$个值向量${\boldsymbol{v}_i}$中提取信息到单个输出$\boldsymbol{o}$。</p>
<p><strong>熵与维度</strong>：</p>
<p>JL引理告诉我们，保持$n$个点的信息需要$O(\log n)$维。</p>
<p>类比地，注意力的熵（度量信息量）也应该是$O(\log n)$。</p>
<p><strong>缩放因子的连接</strong>：</p>
<p>我们的缩放因子$\lambda \propto \log n / d$恰好体现了这个关系：
- 当$d$固定时，$\lambda \propto \log n$
- 当$n$固定时，$\lambda \propto 1/d$</p>
<h3 id="14">14. 信息论视角<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<p><strong>熵的信息论意义</strong>：</p>
<p>熵$H$度量了注意力分布的<strong>不确定性</strong>，也是编码注意力分布所需的<strong>平均比特数</strong>。</p>
<p><strong>熵不变性的意义</strong>：</p>
<p>如果$H \propto \log n$，那么：
- 编码注意力分布需要$O(\log n)$比特
- 这与用$\lceil \log_2 n \rceil$比特编码$n$个位置的任意一个一致</p>
<p><strong>香农熵界</strong>：</p>
<p>对于$n$个等概率的选择，最小编码长度是$\log_2 n$比特。</p>
<p>注意力分布通常不是均匀的（$H &lt; \log n$），所以实际编码可以更短。</p>
<h3 id="15">15. 实验验证<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>设置</strong>：</p>
<p>生成随机的注意力分数$s_i \sim \mathcal{N}(0, 1)$，变化$n$和缩放因子$\lambda$。</p>
<p><strong>观察</strong>：</p>
<ol>
<li>
<p><strong>标准缩放</strong>（$\lambda = 1/\sqrt{d}$）：
   - $H$随$n$增加而增加
   - 大约$H \approx \log n - C$</p>
</li>
<li>
<p><strong>熵不变性缩放</strong>（$\lambda = \log n / d$）：
   - $H$在不同$n$下更加稳定
   - 接近$H \approx \log n$（均匀分布）</p>
</li>
</ol>
<p><strong>问题</strong>：如果$H \approx \log n$，那不就是均匀分布吗？这样有用吗？</p>
<p><strong>回答</strong>：
- $H \approx \log n$是理论上界（最大熵）
- 实际中，由于数据的结构，$H &lt; \log n$
- 关键是$H$的<strong>范围</strong>足够大，可学习的信息量充足</p>
<h3 id="16-layernormrmsnorm">16. 与LayerNorm、RMSNorm的联系<a class="toc-link" href="#16-layernormrmsnorm" title="Permanent link">&para;</a></h3>
<p><strong>LayerNorm</strong>：</p>
<p>在Transformer中，通常在attention之前应用LayerNorm：
\begin{equation}
\boldsymbol{q} = \text{LayerNorm}(\boldsymbol{x}) \boldsymbol{W}_Q \tag{43}
\end{equation}</p>
<p>LayerNorm归一化了均值和方差：
\begin{equation}
\text{LayerNorm}(\boldsymbol{x}) = \frac{\boldsymbol{x} - \mu}{\sigma} \cdot \gamma + \beta \tag{44}
\end{equation}</p>
<p><strong>效果</strong>：</p>
<p>LayerNorm使得$s_i = \langle \boldsymbol{q}, \boldsymbol{k}_i \rangle$的分布更加稳定，减少了对初始化的敏感性。</p>
<p><strong>与熵不变性的关系</strong>：</p>
<p>LayerNorm部分实现了熵不变性的目标，但没有显式地考虑$n$的影响。</p>
<p><strong>RMSNorm</strong>：</p>
<p>RMSNorm只归一化均方根，不中心化：
\begin{equation}
\text{RMSNorm}(\boldsymbol{x}) = \frac{\boldsymbol{x}}{\text{RMS}(\boldsymbol{x})} \cdot \gamma \tag{45}
\end{equation}</p>
<p>其中$\text{RMS}(\boldsymbol{x}) = \sqrt{\frac{1}{d}\sum_i x_i^2}$。</p>
<p><strong>优势</strong>：</p>
<p>RMSNorm更简单，计算更快，同时保持了类似的效果。</p>
<h3 id="17">17. 动态缩放因子<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<p><strong>问题</strong>：在实际应用中，序列长度$n$可能是变化的（如不同长度的句子）。</p>
<p><strong>解决方案</strong>：使用<strong>动态缩放因子</strong>，根据当前的$n$调整$\lambda$。</p>
<p><strong>实现</strong>：
\begin{equation}
\lambda(n) = \frac{\kappa \log n}{d} \tag{46}
\end{equation}</p>
<p>在每个attention层中，根据当前的$n$计算$\lambda$。</p>
<p><strong>挑战</strong>：</p>
<ul>
<li>不同长度的序列使用不同的$\lambda$，可能影响模型的一致性</li>
<li>需要额外的计算开销（虽然很小）</li>
</ul>
<p><strong>折衷方案</strong>：</p>
<p>使用$\lambda = \frac{\kappa \log n_{\max}}{d}$，其中$n_{\max}$是训练中见过的最大序列长度。</p>
<h3 id="18">18. 位置编码的影响<a class="toc-link" href="#18" title="Permanent link">&para;</a></h3>
<p><strong>绝对位置编码</strong>：</p>
<p>添加位置编码$\boldsymbol{p}<em ij="ij">i$后：
\begin{equation}
s</em>
\end{equation}} = \langle \boldsymbol{q}_i + \boldsymbol{p}_i, \boldsymbol{k}_j + \boldsymbol{p}_j \rangle \tag{47</p>
<p><strong>对熵的影响</strong>：</p>
<p>位置编码引入了额外的结构（如局部性偏好），会影响$s_{ij}$的分布，从而影响熵。</p>
<p><strong>分析</strong>：</p>
<p>如果位置编码是Sinusoidal的：
\begin{equation}
p_{i,2k} = \sin(i / 10000^{2k/d}), \quad p_{i,2k+1} = \cos(i / 10000^{2k/d}) \tag{48}
\end{equation}</p>
<p>相邻位置的内积较大，远距离位置的内积较小。</p>
<p><strong>结果</strong>：</p>
<p>注意力倾向于关注附近的token，$H$可能下降。</p>
<p><strong>修正</strong>：</p>
<p>在使用位置编码时，可能需要调整$\kappa$以补偿熵的变化。</p>
<h3 id="19">19. 稀疏注意力的熵<a class="toc-link" href="#19" title="Permanent link">&para;</a></h3>
<p><strong>稀疏注意力</strong>：</p>
<p>只计算部分$(i, j)$对的attention score：
\begin{equation}
s_{ij} = \begin{cases}
\langle \boldsymbol{q}_i, \boldsymbol{k}_j \rangle, &amp; (i,j) \in \mathcal{S} \
-\infty, &amp; \text{otherwise}
\end{cases} \tag{49}
\end{equation}</p>
<p>其中$\mathcal{S}$是允许的位置对集合（如局部窗口、跨步模式等）。</p>
<p><strong>有效序列长度</strong>：</p>
<p>对于位置$i$，有效的$j$数量为$|\mathcal{S}_i|$，通常$|\mathcal{S}_i| \ll n$。</p>
<p><strong>熵的调整</strong>：</p>
<p>最大熵变为：
\begin{equation}
H_{\max} = \log |\mathcal{S}_i| \tag{50}
\end{equation}</p>
<p><strong>缩放因子</strong>：</p>
<p>相应地，应该使用：
\begin{equation}
\lambda = \frac{\kappa \log |\mathcal{S}_i|}{d} \tag{51}
\end{equation}</p>
<h3 id="20">20. 交叉注意力与自注意力<a class="toc-link" href="#20" title="Permanent link">&para;</a></h3>
<p><strong>自注意力</strong>：</p>
<p>$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}$都来自同一序列，长度为$n$。</p>
<p><strong>交叉注意力</strong>（如encoder-decoder attention）：</p>
<p>$\boldsymbol{Q}$来自decoder（长度$n_q$），$\boldsymbol{K}, \boldsymbol{V}$来自encoder（长度$n_k$）。</p>
<p><strong>熵的分析</strong>：</p>
<p>对于decoder的每个位置$i$，熵为：
\begin{equation}
H_i = -\sum_{j=1}^{n_k} \alpha_{ij} \log \alpha_{ij} \tag{52}
\end{equation}</p>
<p><strong>缩放因子</strong>：</p>
<p>应该使用$n_k$（key/value的长度）：
\begin{equation}
\lambda = \frac{\kappa \log n_k}{d} \tag{53}
\end{equation}</p>
<h3 id="21">21. 多头注意力的熵<a class="toc-link" href="#21" title="Permanent link">&para;</a></h3>
<p><strong>多头注意力</strong>：</p>
<p>\begin{equation}
\text{MultiHead}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \boldsymbol{W}^O \tag{54}
\end{equation}</p>
<p>其中：
\begin{equation}
\text{head}_i = \text{Attention}(\boldsymbol{Q}\boldsymbol{W}_i^Q, \boldsymbol{K}\boldsymbol{W}_i^K, \boldsymbol{V}\boldsymbol{W}_i^V) \tag{55}
\end{equation}</p>
<p><strong>每个头的维度</strong>：</p>
<p>通常$d_{\text{head}} = d / h$。</p>
<p><strong>缩放因子</strong>：</p>
<p>对于每个头：
\begin{equation}
\lambda_i = \frac{\kappa \log n}{d_{\text{head}}} = \frac{\kappa h \log n}{d} \tag{56}
\end{equation}</p>
<p><strong>观察</strong>：</p>
<p>多头注意力相当于使用了更大的缩放因子（乘以$h$）。</p>
<h3 id="22-flash-attention">22. Flash Attention与熵不变性<a class="toc-link" href="#22-flash-attention" title="Permanent link">&para;</a></h3>
<p><strong>Flash Attention</strong>：</p>
<p>Flash Attention是一种高效的attention计算方法，通过分块（tiling）和重计算（recomputation）减少内存访问。</p>
<p><strong>与熵不变性的关系</strong>：</p>
<p>Flash Attention本身不改变attention的数学定义，所以熵不变性缩放可以直接应用。</p>
<p><strong>实现</strong>：</p>
<p>在Flash Attention的kernel中，将缩放因子从$1/\sqrt{d}$改为$\log n / d$即可。</p>
<h3 id="23">23. 训练稳定性<a class="toc-link" href="#23" title="Permanent link">&para;</a></h3>
<p><strong>问题</strong>：使用$\lambda = \log n / d$时，当$n$很大时，$\lambda$可能很大，导致softmax过于peaked。</p>
<p><strong>分析</strong>：</p>
<p>假设$n = 1024$，$d = 64$，$\kappa = 1$：
\begin{equation}
\lambda = \frac{\log 1024}{64} = \frac{10 \log 2}{64} \approx 0.108 \tag{57}
\end{equation}</p>
<p>标准缩放：
\begin{equation}
\lambda_{\text{std}} = \frac{1}{\sqrt{64}} = 0.125 \tag{58}
\end{equation}</p>
<p>两者接近！</p>
<p><strong>一般规律</strong>：</p>
<p>对于常见的$n$和$d$值：
\begin{equation}
\frac{\log n}{d} \approx \frac{1}{\sqrt{d}} \tag{59}
\end{equation}</p>
<p>例如，当$n \approx e^{\sqrt{d}}$时，两者相等。</p>
<p><strong>实践中的选择</strong>：</p>
<p>可以使用混合策略：
\begin{equation}
\lambda = \frac{1}{\sqrt{d}} + \beta \frac{\log n}{d} \tag{60}
\end{equation}</p>
<p>其中$\beta$是一个小的权重（如$\beta = 0.1$）。</p>
<h3 id="24">24. 长序列建模<a class="toc-link" href="#24" title="Permanent link">&para;</a></h3>
<p><strong>问题</strong>：对于非常长的序列（如$n = 10^6$），$\log n \approx 14$，缩放因子可能显著不同。</p>
<p><strong>解决方案1</strong>：分段处理</p>
<p>将长序列分成多个段，每段长度为$n_{\text{seg}}$，分别计算attention。</p>
<p><strong>解决方案2</strong>：层次化attention</p>
<p>使用层次化的attention结构，先在局部窗口内计算attention，再在全局level计算。</p>
<p><strong>解决方案3</strong>：自适应缩放</p>
<p>根据熵的实际值动态调整$\lambda$：
\begin{equation}
\lambda \leftarrow \lambda \cdot \frac{H_{\text{target}}}{H_{\text{current}}} \tag{61}
\end{equation}</p>
<h3 id="25">25. 与温度参数的统一<a class="toc-link" href="#25" title="Permanent link">&para;</a></h3>
<p><strong>温度参数</strong>：</p>
<p>在很多场景（如知识蒸馏）中，使用温度参数$T$：
\begin{equation}
\alpha_i = \frac{e^{s_i / T}}{\sum_j e^{s_j / T}} \tag{62}
\end{equation}</p>
<p><strong>与缩放因子的关系</strong>：</p>
<p>$\lambda = 1/T$是缩放因子的倒数。</p>
<p><strong>统一框架</strong>：</p>
<p>我们可以将熵不变性缩放看作一种<strong>自适应温度</strong>：
\begin{equation}
T(n, d) = \frac{d}{\kappa \log n} \tag{63}
\end{equation}</p>
<p><strong>解释</strong>：</p>
<ul>
<li>当$n$增大时，$T$减小（温度降低），使softmax更加peaked</li>
<li>这补偿了序列长度增加带来的熵自然增长</li>
</ul>
<h3 id="26">26. 理论局限性<a class="toc-link" href="#26" title="Permanent link">&para;</a></h3>
<p><strong>假设的局限</strong>：</p>
<ol>
<li>
<p><strong>平均场近似</strong>：式(10)假设$\log \mathbb{E}[e^{\lambda s}] \approx \mathbb{E}[\lambda s]$，这在$\lambda$或$s$的方差很大时不准确。</p>
</li>
<li>
<p><strong>独立性假设</strong>：假设$s_i$独立同分布，但实际上它们可能有相关性（如相邻token的相似性）。</p>
</li>
<li>
<p><strong>$\mathbb{E}<em _max="\max">\alpha[s] \approx s</em>$</strong>：这个近似在softmax非常peaked时才准确。</p>
</li>
</ol>
<p><strong>适用范围</strong>：</p>
<p>熵不变性缩放在以下情况下最有效：
- 序列长度$n$变化范围大（如从10到10000）
- 维度$d$适中（如64到512）
- 数据分布相对均匀（没有极端的outliers）</p>
<h3 id="27">27. 实验设计建议<a class="toc-link" href="#27" title="Permanent link">&para;</a></h3>
<p><strong>消融实验</strong>：</p>
<p>对比以下缩放策略：
1. 标准缩放：$\lambda = 1/\sqrt{d}$
2. 熵不变性缩放：$\lambda = \log n / d$
3. 混合缩放：$\lambda = \alpha/\sqrt{d} + \beta \log n / d$</p>
<p><strong>评估指标</strong>：</p>
<ol>
<li><strong>任务性能</strong>：困惑度（perplexity）、准确率等</li>
<li><strong>熵的稳定性</strong>：在不同$n$下，熵的方差</li>
<li><strong>收敛速度</strong>：达到目标性能所需的训练步数</li>
</ol>
<p><strong>数据集</strong>：</p>
<p>选择序列长度变化大的数据集，如：
- 文本：不同长度的句子/段落
- 图像：不同分辨率的patch序列</p>
<h3 id="28">28. 开放问题<a class="toc-link" href="#28" title="Permanent link">&para;</a></h3>
<p><strong>问题1</strong>：最优的$\kappa$值</p>
<p>理论上$\kappa = 1$，但实验中可能需要调整。如何自动确定最优$\kappa$？</p>
<p><strong>问题2</strong>：非均匀分布</p>
<p>如果$s_i$不是独立同分布，而是有明显的聚类结构，熵不变性条件如何修正？</p>
<p><strong>问题3</strong>：与其他正则化的结合</p>
<p>熵不变性缩放能否与dropout、weight decay等正则化技术结合？</p>
<p><strong>问题4</strong>：因果注意力</p>
<p>在decoder的因果注意力（causal attention）中，有效$n$随位置变化，如何处理？</p>
<h3 id="29">29. 代码实现<a class="toc-link" href="#29" title="Permanent link">&para;</a></h3>
<p><strong>PyTorch实现</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">EntropyInvariantAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">=</span> <span class="n">kappa</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># 线性变换</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># 计算attention scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># (batch, heads, n, n)</span>

        <span class="c1"># 熵不变性缩放</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kappa</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">*</span> <span class="n">scale</span>

        <span class="c1"># 应用mask</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

        <span class="c1"># Softmax</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 加权求和</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># (batch, heads, n, d_k)</span>

        <span class="c1"># 合并多头</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span>
</code></pre></div>

<p><strong>使用示例</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 初始化</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">EntropyInvariantAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># 输入</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="c1"># 前向传播</span>
<span class="n">output</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output shape: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (32, 100, 512)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention weights shape: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (32, 8, 100, 100)</span>
</code></pre></div>

<h3 id="30">30. 总结与展望<a class="toc-link" href="#30" title="Permanent link">&para;</a></h3>
<p><strong>核心贡献</strong>：</p>
<ol>
<li>从熵不变性角度推导了缩放因子$\lambda \propto \log n / d$</li>
<li>连接了序列长度、维度和注意力熵之间的关系</li>
<li>提供了比标准$1/\sqrt{d}$更精细的缩放策略</li>
</ol>
<p><strong>理论意义</strong>：</p>
<ul>
<li>揭示了Softmax注意力的信息论性质</li>
<li>为理解Transformer的缩放规律提供了新视角</li>
<li>连接了离散优化（max）和连续优化（softmax）</li>
</ul>
<p><strong>实践价值</strong>：</p>
<ul>
<li>在序列长度变化大的任务中可能提高性能</li>
<li>为超长序列建模提供了理论指导</li>
<li>有助于理解和调试attention机制</li>
</ul>
<p><strong>未来方向</strong>：</p>
<ol>
<li>在大规模语言模型中验证熵不变性缩放</li>
<li>探索自适应$\kappa$的学习方法</li>
<li>推广到其他类型的attention（如linear attention）</li>
<li>研究熵与泛化性能的关系</li>
</ol>
<hr />
<h2 id="_4">参考文献<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<ol>
<li>Vaswani et al., "Attention Is All You Need", NeurIPS 2017</li>
<li>Johnson &amp; Lindenstrauss, "Extensions of Lipschitz mappings into a Hilbert space", 1984</li>
<li>Dao et al., "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", NeurIPS 2022</li>
<li>Zhang et al., "Root Mean Square Layer Normalization", NeurIPS 2019</li>
<li>Su et al., "RoFormer: Enhanced Transformer with Rotary Position Embedding", 2021</li>
</ol>
<h2 id="_5">最终总结<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文从熵不变性角度重新推导了Softmax注意力的缩放因子，得到：</p>
<p>\begin{equation}
\lambda = \frac{\kappa \log n}{d}
\end{equation}</p>
<p><strong>关键洞察</strong>：
1. <strong>平均场近似</strong>：$\log \mathbb{E}[e^{\lambda s}] \approx \mathbb{E}[\lambda s]$连接了指数和对数
2. <strong>Max近似</strong>：Softmax侧重最大值，$\mathbb{E}<em _max="\max">\alpha[s] \approx s</em>$
3. <strong>熵的渐近</strong>：$H \approx \log n - \lambda(s_{\max} - \bar{s})$给出了熵与$n$的关系
4. <strong>方差控制</strong>：除以$d$保持分数方差稳定</p>
<p><strong>实用建议</strong>：
- 对于固定长度序列，使用标准$1/\sqrt{d}$缩放即可
- 对于变长序列（如$n \in [10, 1000]$），考虑熵不变性缩放
- 超长序列建模中，$\log n$因子可能带来显著改善</p>
<p>熵不变性提供了理解和设计attention机制的新视角，值得进一步探索！</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="听说attention与softmax更配哦.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#176 听说Attention与Softmax更配哦～</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="globalpointer下的kl散度应该是怎样的.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#178 GlobalPointer下的“KL散度”应该是怎样的？</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#softmax">熵不变性Softmax的一个快速推导</a><ul>
<li><a href="#_1">推导过程</a></li>
<li><a href="#_2">文章小结</a></li>
<li><a href="#_3">详细数学推导与理论分析</a><ul>
<li><a href="#1">1. 问题的背景与动机</a></li>
<li><a href="#2">2. 熵不变性的定义</a></li>
<li><a href="#3">3. 熵的展开式</a></li>
<li><a href="#4">4. 平均场近似</a></li>
<li><a href="#5-softmaxmax">5. Softmax侧重max的性质</a></li>
<li><a href="#6">6. 熵的最终近似</a></li>
<li><a href="#7">7. 熵不变性条件</a></li>
<li><a href="#8-d">8. 与维度$d$的关系</a></li>
<li><a href="#9">9. 最终的熵不变性注意力</a></li>
<li><a href="#10-laplace">10. 严格的数学推导（Laplace近似）</a></li>
<li><a href="#11">11. 二阶修正与波动</a></li>
<li><a href="#12">12. 缩放因子的渐近分析</a></li>
<li><a href="#13-johnson-lindenstrauss">13. 与Johnson-Lindenstrauss引理的联系</a></li>
<li><a href="#14">14. 信息论视角</a></li>
<li><a href="#15">15. 实验验证</a></li>
<li><a href="#16-layernormrmsnorm">16. 与LayerNorm、RMSNorm的联系</a></li>
<li><a href="#17">17. 动态缩放因子</a></li>
<li><a href="#18">18. 位置编码的影响</a></li>
<li><a href="#19">19. 稀疏注意力的熵</a></li>
<li><a href="#20">20. 交叉注意力与自注意力</a></li>
<li><a href="#21">21. 多头注意力的熵</a></li>
<li><a href="#22-flash-attention">22. Flash Attention与熵不变性</a></li>
<li><a href="#23">23. 训练稳定性</a></li>
<li><a href="#24">24. 长序列建模</a></li>
<li><a href="#25">25. 与温度参数的统一</a></li>
<li><a href="#26">26. 理论局限性</a></li>
<li><a href="#27">27. 实验设计建议</a></li>
<li><a href="#28">28. 开放问题</a></li>
<li><a href="#29">29. 代码实现</a></li>
<li><a href="#30">30. 总结与展望</a></li>
</ul>
</li>
<li><a href="#_4">参考文献</a></li>
<li><a href="#_5">最终总结</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>