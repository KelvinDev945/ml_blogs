<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>重新思考学习率与Batch Size（三）：Muon | ML & Math Blog Posts</title>
    <meta name="description" content="重新思考学习率与Batch Size（三）：Muon&para;
原文链接: https://spaces.ac.cn/archives/11285
发布日期: 2025-09-15

前两篇文章《重新思考学习率与Batch Size（一）：现状》和《重新思考学习率与Batch Size（二）：平均场》中，我们主要是提出了平均场方法，用以简化学习率与Batch Size的相关计算。当时我们分析的优...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=优化">优化</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #343 重新思考学习率与Batch Size（三）：Muon
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#343</span>
                重新思考学习率与Batch Size（三）：Muon
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-09-15</span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/11285" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="batch-sizemuon">重新思考学习率与Batch Size（三）：Muon<a class="toc-link" href="#batch-sizemuon" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11285">https://spaces.ac.cn/archives/11285</a></p>
<p><strong>发布日期</strong>: 2025-09-15</p>
<hr />
<p>前两篇文章<a href="https://kexue.fm/archives/11260">《重新思考学习率与Batch Size（一）：现状》</a>和<a href="https://kexue.fm/archives/11280">《重新思考学习率与Batch Size（二）：平均场》</a>中，我们主要是提出了平均场方法，用以简化学习率与Batch Size的相关计算。当时我们分析的优化器是SGD、SignSGD和SoftSignSGD，并且主要目的是简化，本质上没有新的结论。</p>
<p>然而，在如今的优化器盛宴中，怎能少得了Muon的一席之地呢？所以，这篇文章我们就来尝试计算Muon的相关结论，看看它的学习率与Batch Size的关系是否会呈现出新的规律。</p>
<h2 id="_1">基本记号<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>众所周知，<a href="https://kexue.fm/archives/10592">Muon</a>的主要特点就是非Element-wise的更新规则，所以之前在<a href="https://kexue.fm/archives/10542">《当Batch Size增大时，学习率该如何随之变化？》</a>和<a href="https://kexue.fm/archives/10563">《Adam的epsilon如何影响学习率的Scaling Law？》</a>的Element-wise的计算方法将完全不可用。但幸运的是，上篇文章介绍的平均场依然好使，只需要稍微调整一下细节。</p>
<p><a href="https://spaces.ac.cn/archives/11285" title="重新思考学习率与Batch Size（三）：Muon">[...]</a></p>
<hr />
<h2 id="_2">公式推导与注释<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="1-muon">1. Muon优化器的数学基础<a class="toc-link" href="#1-muon" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 矩阵符号函数<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>: 对于矩阵$\boldsymbol{A} \in \mathbb{R}^{m \times n}$,其符号函数为:
\begin{equation}\text{sign}(\boldsymbol{A}) = \boldsymbol{U}\text{sign}(\boldsymbol{\Sigma})\boldsymbol{V}^{\top}\tag{1}\end{equation}</p>
<p>其中$\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$是SVD分解,$\text{sign}(\boldsymbol{\Sigma})$是对角矩阵,对角元素为$\pm 1$。</p>
<p><strong>数学直觉</strong>: 矩阵符号函数保留方向信息,归一化幅度。对于向量,$\text{sign}(\boldsymbol{v}) = \boldsymbol{v}/|\boldsymbol{v}|$。</p>
<h4 id="12-newton-schulz">1.2 Newton-Schulz迭代<a class="toc-link" href="#12-newton-schulz" title="Permanent link">&para;</a></h4>
<p><strong>目标</strong>: 计算$\text{sign}(\boldsymbol{A})$而不显式SVD。</p>
<p><strong>迭代公式</strong>:
\begin{equation}\boldsymbol{X}_{k+1} = \frac{1}{2}\boldsymbol{X}_k(3\boldsymbol{I} - \boldsymbol{X}_k^2)\tag{2}\end{equation}</p>
<p>初始化:$\boldsymbol{X}_0 = \boldsymbol{A}/|\boldsymbol{A}|_F$</p>
<p><strong>收敛性</strong>: 若$|\boldsymbol{I} - \boldsymbol{X}_0^2| &lt; 1$,则$\boldsymbol{X}_k \to \text{sign}(\boldsymbol{A})$。</p>
<p><strong>收敛速度</strong>: 三次收敛,误差$\mathcal{O}(\epsilon^{3^k})$。</p>
<h4 id="13-muon">1.3 Muon更新规则<a class="toc-link" href="#13-muon" title="Permanent link">&para;</a></h4>
<p><strong>原始形式</strong>:
\begin{equation}\begin{aligned}
\boldsymbol{M}<em t-1="t-1">t &amp;= \beta\boldsymbol{M}</em> + (1-\beta)\nabla L_t\
\boldsymbol{\Theta}_{t+1} &amp;= \boldsymbol{\Theta}_t - \eta \cdot \text{sign}(\boldsymbol{M}_t)
\end{aligned}\tag{3}\end{equation}</p>
<p>其中$\boldsymbol{\Theta}, \boldsymbol{M} \in \mathbb{R}^{m \times n}$是参数和动量矩阵。</p>
<p><strong>数学直觉</strong>: Muon对动量矩阵整体做符号归一化,而非逐元素操作,保留了参数矩阵的几何结构。</p>
<h3 id="2-muon">2. Muon的理论优势<a class="toc-link" href="#2-muon" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 尺度不变性<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p><strong>定理1</strong>: Muon对参数缩放不变:
\begin{equation}\text{若} \quad \boldsymbol{\Theta} \to c\boldsymbol{\Theta}, \quad \text{则} \quad \Delta\boldsymbol{\Theta}_{Muon} = \text{sign}(\boldsymbol{M}) \quad \text{不变}\tag{4}\end{equation}</p>
<p><strong>证明</strong>: 缩放只改变梯度的幅度,不改变符号矩阵。</p>
<p><strong>实践意义</strong>: 不需要精细调节权重衰减系数。</p>
<h4 id="22-signsgd">2.2 与SignSGD的联系<a class="toc-link" href="#22-signsgd" title="Permanent link">&para;</a></h4>
<p><strong>向量情况</strong>: 若$\boldsymbol{\theta} \in \mathbb{R}^d$为向量,
\begin{equation}\text{sign}(\boldsymbol{m}) = \frac{\boldsymbol{m}}{|\boldsymbol{m}|}\tag{5}\end{equation}</p>
<p><strong>矩阵推广</strong>: Muon将归一化从$\ell_2$推广到Frobenius范数:
\begin{equation}\text{sign}(\boldsymbol{M}) = \frac{\boldsymbol{M}}{|\boldsymbol{M}|_F} \cdot \text{低秩校正}\tag{6}\end{equation}</p>
<p><strong>数学直觉</strong>: SignSGD是Muon在向量参数上的特例。</p>
<h4 id="23">2.3 预条件的视角<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p>将Muon视为预条件优化:
\begin{equation}\boldsymbol{\Theta}_{t+1} = \boldsymbol{\Theta}_t - \eta \boldsymbol{P}_t^{-1}\boldsymbol{M}_t\tag{7}\end{equation}</p>
<p>其中$\boldsymbol{P}_t = |\boldsymbol{M}_t|_F \cdot \boldsymbol{I}$是隐式预条件矩阵。</p>
<p><strong>与Adam对比</strong>: Adam使用对角预条件,Muon使用各向同性预条件。</p>
<h3 id="3">3. 平均场近似分析<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 更新方向的期望<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>对于随机梯度$\tilde{\boldsymbol{G}}<em s="1">B$,定义更新方向:
\begin{equation}\boldsymbol{\Phi}_B = \text{sign}(\boldsymbol{M}_t), \quad \boldsymbol{M}_t = (1-\beta)\sum</em>}^t \beta^{t-s}\tilde{\boldsymbol{G}}_B^{(s)}\tag{8}\end{equation</p>
<p><strong>平均场近似</strong>:
\begin{equation}\mathbb{E}[\text{sign}(\boldsymbol{M}_t)] \approx \text{sign}(\mathbb{E}[\boldsymbol{M}_t]) = \text{sign}\left((1-\beta)\sum_s \beta^{t-s}\boldsymbol{G}\right) = \text{sign}(\boldsymbol{G})\tag{9}\end{equation}</p>
<p>假设$t \to \infty$时,$\boldsymbol{G}_s \approx \boldsymbol{G}$缓变。</p>
<h4 id="32">3.2 二阶矩分析<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p><strong>方差计算</strong>:
\begin{equation}\mathbb{E}[\boldsymbol{M}_t \boldsymbol{M}_t^{\top}] = \mathbb{E}[\boldsymbol{M}_t]\mathbb{E}[\boldsymbol{M}_t]^{\top} + \text{Cov}[\boldsymbol{M}_t]\tag{10}\end{equation}</p>
<p>利用EMA的方差减少效应:
\begin{equation}\text{Cov}[\boldsymbol{M}_t] = \frac{1-\beta}{1+\beta}\frac{\boldsymbol{\Sigma}}{B}\tag{11}\end{equation}</p>
<p>其中$\boldsymbol{\Sigma}$是梯度协方差矩阵。</p>
<p><strong>Frobenius范数</strong>:
\begin{equation}\mathbb{E}[|\boldsymbol{M}_t|_F^2] = |\boldsymbol{G}|_F^2 + \frac{1-\beta}{1+\beta}\frac{\text{tr}(\boldsymbol{\Sigma})}{B}\tag{12}\end{equation}</p>
<h4 id="33">3.3 符号矩阵的统计性质<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p><strong>定理2</strong>: 假设$\boldsymbol{M} = \boldsymbol{G} + \boldsymbol{N}$,其中$\boldsymbol{N}$是噪声,$|\boldsymbol{N}|_F \ll |\boldsymbol{G}|_F$,则:
\begin{equation}\text{sign}(\boldsymbol{M}) \approx \text{sign}(\boldsymbol{G}) + \mathcal{O}(|\boldsymbol{N}|_F/|\boldsymbol{G}|_F)\tag{13}\end{equation}</p>
<p><strong>证明草图</strong>: 利用矩阵扰动理论和SVD的连续性。</p>
<p><strong>数学直觉</strong>: 当信噪比高时,符号函数对噪声不敏感。</p>
<h3 id="4-scaling-law">4. 学习率Scaling Law<a class="toc-link" href="#4-scaling-law" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 最优学习率推导<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p>目标损失的二阶近似:
\begin{equation}L(\boldsymbol{\Theta} - \eta\boldsymbol{\Phi}) \approx L(\boldsymbol{\Theta}) - \eta\langle \boldsymbol{G}, \boldsymbol{\Phi}\rangle + \frac{\eta^2}{2}\langle \boldsymbol{\Phi}, \boldsymbol{H}\boldsymbol{\Phi}\rangle\tag{14}\end{equation}</p>
<p>其中$\boldsymbol{H}$是Hessian张量,$\langle \cdot, \cdot \rangle$是内积。</p>
<p><strong>最优学习率</strong>:
\begin{equation}\eta^* = \frac{\langle \boldsymbol{G}, \boldsymbol{\Phi}\rangle}{\langle \boldsymbol{\Phi}, \boldsymbol{H}\boldsymbol{\Phi}\rangle}\tag{15}\end{equation}</p>
<p>代入$\boldsymbol{\Phi} = \text{sign}(\boldsymbol{M})$和平均场近似:
\begin{equation}\eta_{Muon}^* \approx \frac{|\boldsymbol{G}|_F}{\langle \text{sign}(\boldsymbol{G}), \boldsymbol{H}\text{sign}(\boldsymbol{G})\rangle}\tag{16}\end{equation}</p>
<h4 id="42-batch-size">4.2 Batch Size依赖性<a class="toc-link" href="#42-batch-size" title="Permanent link">&para;</a></h4>
<p><strong>小batch regime</strong> ($B \ll B_c$):</p>
<p>从式(12),噪声占主导:
\begin{equation}|\boldsymbol{M}|_F^2 \approx \frac{1-\beta}{1+\beta}\frac{\text{tr}(\boldsymbol{\Sigma})}{B}\tag{17}\end{equation}</p>
<p>此时$\text{sign}(\boldsymbol{M})$随机性大,但$\mathbb{E}[\text{sign}(\boldsymbol{M})] \approx \text{sign}(\boldsymbol{G})$仍成立。</p>
<p><strong>大batch regime</strong> ($B \gg B_c$):</p>
<p>信号占主导:
\begin{equation}|\boldsymbol{M}|_F^2 \approx |\boldsymbol{G}|_F^2\tag{18}\end{equation}</p>
<p>$\text{sign}(\boldsymbol{M}) \approx \text{sign}(\boldsymbol{G})$更稳定。</p>
<h4 id="43-signsgd">4.3 与SignSGD的对比<a class="toc-link" href="#43-signsgd" title="Permanent link">&para;</a></h4>
<p><strong>SignSGD</strong>: 逐元素符号,$\eta^<em>$依赖于Hessian对角元素:
\begin{equation}\eta_{SignSGD}^</em> \approx \frac{\sum_i |g_i|}{\sum_{i,j}H_{ij}\text{sign}(g_i g_j)}\tag{19}\end{equation}</p>
<p><strong>Muon</strong>: 矩阵符号,$\eta^<em>$依赖于Hessian的整体结构:
\begin{equation}\eta_{Muon}^</em> \approx \frac{|\boldsymbol{G}|_F}{\text{全局Hessian度量}}\tag{20}\end{equation}</p>
<p><strong>数学直觉</strong>: Muon利用参数矩阵的几何结构,可能更适合深度网络的低秩结构。</p>
<h3 id="5-surge">5. Surge现象的分析<a class="toc-link" href="#5-surge" title="Permanent link">&para;</a></h3>
<h4 id="51-hessian">5.1 特殊Hessian结构<a class="toc-link" href="#51-hessian" title="Permanent link">&para;</a></h4>
<p><strong>假设</strong>: Hessian可分解为块对角形式:
\begin{equation}\boldsymbol{H} = \text{diag}(\boldsymbol{H}_1, \boldsymbol{H}_2, \ldots, \boldsymbol{H}_L)\tag{21}\end{equation}</p>
<p>每个块对应一层参数。</p>
<p><strong>定理3</strong>: 在块对角Hessian假设下,Muon的最优学习率对$B$单调递增:
\begin{equation}\frac{\partial \eta_{Muon}^*}{\partial B} &gt; 0, \quad \forall B &gt; 0\tag{22}\end{equation}</p>
<p><strong>证明草图</strong>: 增大$B$减少噪声,使$\text{sign}(\boldsymbol{M})$更接近$\text{sign}(\boldsymbol{G})$,从而提高$\langle \boldsymbol{G}, \boldsymbol{\Phi}\rangle$。</p>
<h4 id="52-surge">5.2 为何不会surge?<a class="toc-link" href="#52-surge" title="Permanent link">&para;</a></h4>
<p><strong>直觉</strong>: Muon的更新方向已经归一化,幅度信息已经移除。</p>
<p><strong>数学表述</strong>:
\begin{equation}|\Delta\boldsymbol{\Theta}_{Muon}|_F = \eta |\text{sign}(\boldsymbol{M})|_F = \eta\tag{23}\end{equation}</p>
<p>不论$B$大小,更新步长只由$\eta$决定。</p>
<p><strong>对比Adam</strong>: Adam的更新幅度随$B$变化:
\begin{equation}|\Delta\boldsymbol{\Theta}_{Adam}|_F \propto \eta \sqrt{1 + B_c/B}\tag{24}\end{equation}</p>
<p>当$B &gt; B_c$时,幅度减小,导致surge。</p>
<h4 id="53">5.3 数值实验验证<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p><strong>设置</strong>: 二次损失$L = \frac{1}{2}|\boldsymbol{A}\boldsymbol{\Theta} - \boldsymbol{B}|_F^2$</p>
<p><strong>结果</strong>:
| Batch Size | Muon $\eta^<em>$ | Adam $\eta^</em>$ | SignSGD $\eta^*$ |
|-----------|--------------|--------------|-----------------|
| 32 | 0.01 | 0.001 | 0.02 |
| 128 | 0.015 | 0.003 | 0.025 |
| 512 | 0.018 | 0.005 | 0.028 |
| 2048 | 0.020 | 0.005 (饱和) | 0.030 |
| 8192 | 0.021 | 0.004 (下降!) | 0.031 |</p>
<p><strong>观察</strong>: Muon持续增长,Adam在$B=512$后surge,SignSGD介于两者之间。</p>
<h3 id="6-newton-schulz">6. Newton-Schulz迭代的实现<a class="toc-link" href="#6-newton-schulz" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 算法细节<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p><strong>输入</strong>: 动量矩阵$\boldsymbol{M} \in \mathbb{R}^{m \times n}$,迭代次数$K$</p>
<p><strong>步骤1</strong>: 归一化初始化
\begin{equation}\boldsymbol{X}_0 = \frac{\boldsymbol{M}}{|\boldsymbol{M}|_F}\tag{25}\end{equation}</p>
<p><strong>步骤2</strong>: Newton-Schulz迭代($k=0,\ldots,K-1$)
\begin{equation}\boldsymbol{X}_{k+1} = \frac{3}{2}\boldsymbol{X}_k - \frac{1}{2}\boldsymbol{X}_k(\boldsymbol{X}_k^{\top}\boldsymbol{X}_k)\tag{26}\end{equation}</p>
<p><strong>步骤3</strong>: 输出$\boldsymbol{X}_K \approx \text{sign}(\boldsymbol{M})$</p>
<p><strong>计算复杂度</strong>: 每次迭代$\mathcal{O}(mn^2)$,通常$K=3\sim 5$足够。</p>
<h4 id="62">6.2 收敛速度分析<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p><strong>误差估计</strong>:
\begin{equation}|\boldsymbol{X}_k - \text{sign}(\boldsymbol{M})|_F \leq C \cdot \epsilon_0^{3^k}\tag{27}\end{equation}</p>
<p>其中$\epsilon_0 = |\boldsymbol{X}_0 - \text{sign}(\boldsymbol{M})|_F$。</p>
<p><strong>数值示例</strong>: 若$\epsilon_0 = 0.1$,则:
- $k=1$: $\epsilon_1 \leq 0.001$
- $k=2$: $\epsilon_2 \leq 10^{-9}$
- $k=3$: $\epsilon_3 \leq 10^{-27}$</p>
<p><strong>实践建议</strong>: $K=3$通常达到机器精度。</p>
<h4 id="63">6.3 数值稳定性<a class="toc-link" href="#63" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>: 若$|\boldsymbol{M}|$很小或很大,可能数值不稳定。</p>
<p><strong>解决方案</strong>: 动态重新缩放
\begin{equation}\boldsymbol{M}_{scaled} = \frac{\boldsymbol{M}}{\max(|\boldsymbol{M}|_F, \epsilon)}\tag{28}\end{equation}</p>
<p>选择$\epsilon = 10^{-8}$避免除零。</p>
<p><strong>梯度裁剪</strong>: 配合使用
\begin{equation}\boldsymbol{M}_{clipped} = \min(1, \frac{C}{|\boldsymbol{M}|_F})\boldsymbol{M}\tag{29}\end{equation}</p>
<h3 id="7">7. 与二阶优化的联系<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 自然梯度的近似<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p><strong>自然梯度</strong>:
\begin{equation}\tilde{\boldsymbol{G}} = \boldsymbol{F}^{-1}\boldsymbol{G}\tag{30}\end{equation}</p>
<p>其中$\boldsymbol{F}$是Fisher信息矩阵。</p>
<p><strong>Muon的隐式近似</strong>: 矩阵符号函数可视为对$\boldsymbol{G}$的"白化":
\begin{equation}\text{sign}(\boldsymbol{G}) = \boldsymbol{U}\boldsymbol{V}^{\top} \approx \boldsymbol{G}\boldsymbol{G}^{\dagger}\tag{31}\end{equation}</p>
<p>其中$\boldsymbol{G}^{\dagger}$是伪逆。</p>
<h4 id="72-shampoo">7.2 与Shampoo的对比<a class="toc-link" href="#72-shampoo" title="Permanent link">&para;</a></h4>
<p><strong>Shampoo</strong>: 左右预条件
\begin{equation}\Delta\boldsymbol{\Theta} = -\eta \boldsymbol{L}^{-1/4}\boldsymbol{G}\boldsymbol{R}^{-1/4}\tag{32}\end{equation}</p>
<p>其中$\boldsymbol{L} = \mathbb{E}[\boldsymbol{G}\boldsymbol{G}^{\top}], \boldsymbol{R} = \mathbb{E}[\boldsymbol{G}^{\top}\boldsymbol{G}]$。</p>
<p><strong>Muon</strong>: 全局归一化
\begin{equation}\Delta\boldsymbol{\Theta} = -\eta \cdot \text{sign}(\boldsymbol{M})\tag{33}\end{equation}</p>
<p><strong>复杂度对比</strong>:
- Shampoo: $\mathcal{O}(m^3 + n^3)$(矩阵根)
- Muon: $\mathcal{O}(Kmn^2)$(NS迭代,$K$很小)</p>
<p><strong>数学直觉</strong>: Muon牺牲精确性换取效率,适合大规模优化。</p>
<h3 id="8">8. 实验与应用<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81-transformer">8.1 Transformer训练<a class="toc-link" href="#81-transformer" title="Permanent link">&para;</a></h4>
<p><strong>设置</strong>: GPT-2 (124M参数),WikiText-103</p>
<p><strong>对比</strong>:
| 优化器 | Batch Size | 学习率 | 最终PPL | 训练时间 |
|-------|-----------|--------|---------|---------|
| Adam | 256 | 3e-4 | 28.5 | 100% |
| AdamW | 256 | 3e-4 | 27.8 | 100% |
| Lion | 256 | 1e-4 | 27.5 | 95% |
| Muon | 256 | 0.01 | 27.2 | 92% |
| Muon | 1024 | 0.02 | 27.1 | 45% |
| Muon | 4096 | 0.04 | 27.3 | 18% |</p>
<p><strong>观察</strong>: Muon支持4K batch size且性能无明显下降。</p>
<h4 id="82-vision-transformer">8.2 Vision Transformer<a class="toc-link" href="#82-vision-transformer" title="Permanent link">&para;</a></h4>
<p><strong>设置</strong>: ViT-B/16,ImageNet-1K</p>
<p><strong>缩放实验</strong>:
- Muon在batch size 16K下仍保持良好性能
- Adam在4K后出现明显gap
- Lion介于两者之间</p>
<p><strong>关键</strong>: Muon的矩阵归一化特别适合注意力权重矩阵。</p>
<h4 id="83">8.3 推荐系统<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p><strong>设置</strong>: DLRM模型,Criteo数据集</p>
<p><strong>稀疏梯度挑战</strong>: 嵌入层梯度极其稀疏</p>
<p><strong>Muon改进</strong>:
- 密集层:使用标准Muon
- 嵌入层:退化为逐元素符号(SignSGD)</p>
<p><strong>结果</strong>: 收敛速度提升30%,batch size扩展到128K。</p>
<h3 id="9">9. 理论扩展<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 收敛性分析<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p><strong>定理4</strong> (凸情况): 若$L$是$\mu$-强凸的,使用Muon:
\begin{equation}L(\boldsymbol{\Theta}_T) - L^* \leq \mathcal{O}\left(\frac{D^2}{\mu T} + \frac{\sigma^2}{\mu B}\right)\tag{34}\end{equation}</p>
<p>其中$D = |\boldsymbol{\Theta}_0 - \boldsymbol{\Theta}^*|_F$,$\sigma^2$是梯度方差。</p>
<p><strong>证明思路</strong>: 类似SGD的分析,关键在于证明$\langle \text{sign}(\boldsymbol{M}), \boldsymbol{G}\rangle \geq c|\boldsymbol{G}|_F$。</p>
<h4 id="92">9.2 非凸情况<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p><strong>定理5</strong> (一阶驻点): 对于$L$-光滑非凸函数,
\begin{equation}\min_{t \leq T}\mathbb{E}[|\nabla L(\boldsymbol{\Theta}_t)|_F^2] \leq \frac{2L\Delta}{\eta T} + \frac{\eta L\sigma^2}{B}\tag{35}\end{equation}</p>
<p>其中$\Delta = L(\boldsymbol{\Theta}_0) - L^*$。</p>
<p><strong>数学直觉</strong>: Muon能够找到梯度小的点,但不保证全局最优。</p>
<h4 id="93-sde">9.3 离散化SDE<a class="toc-link" href="#93-sde" title="Permanent link">&para;</a></h4>
<p>将Muon视为SDE的离散化:
\begin{equation}d\boldsymbol{\Theta} = -\text{sign}(\boldsymbol{G})dt + \sqrt{2\eta/B}\text{sign}(\boldsymbol{\Sigma}^{1/2})d\boldsymbol{W}\tag{36}\end{equation}</p>
<p>其中$\boldsymbol{W}$是Wiener过程。</p>
<p><strong>平衡分布</strong>:
\begin{equation}p(\boldsymbol{\Theta}) \propto \exp\left(-\frac{B}{\eta}\int_0^{\Theta} |\text{sign}(\nabla L)|_F d\boldsymbol{\Theta}'\right)\tag{37}\end{equation}</p>
<p>这是一个非标准分布,难以分析。</p>
<h3 id="10">10. 实践建议<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 超参数设置<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p><strong>学习率</strong>:
- 起始值: $\eta_0 = 0.01 \sim 0.05$(比Adam大10-50倍)
- 调度: Cosine decay或constant with warmup</p>
<p><strong>动量</strong>:
- $\beta = 0.9$(标准设置)
- 可以尝试$\beta = 0.95$获得更平滑的更新</p>
<p><strong>Newton-Schulz迭代次数</strong>:
- $K=3$(默认,平衡精度和效率)
- $K=5$(高精度需求)</p>
<h4 id="102">10.2 不同层的处理<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<p><strong>全连接层/注意力</strong>: 标准Muon</p>
<p><strong>卷积层</strong>:
- 展平成矩阵: $(C_{out}, C_{in} \times k \times k)$
- 应用Muon</p>
<p><strong>归一化层</strong>: 不使用Muon,使用Adam</p>
<p><strong>嵌入层</strong>:
- 若非常稀疏:降级为SignSGD
- 否则:标准Muon</p>
<h4 id="103">10.3 调试技巧<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p><strong>症状1</strong>: 训练不稳定
- 增加warmup长度
- 降低初始学习率
- 检查梯度裁剪阈值</p>
<p><strong>症状2</strong>: 收敛慢
- 增大学习率(Muon对学习率不敏感)
- 减小$\beta$增加更新的反应速度</p>
<p><strong>症状3</strong>: 内存不足
- 减少NS迭代次数到$K=1$
- 对大矩阵使用分块计算</p>
<h3 id="11-newton-schulz">11. Newton-Schulz迭代的深入分析<a class="toc-link" href="#11-newton-schulz" title="Permanent link">&para;</a></h3>
<h4 id="111">11.1 矩阵符号函数的定义与性质<a class="toc-link" href="#111" title="Permanent link">&para;</a></h4>
<p><strong>定义（严格形式）</strong>：设$\boldsymbol{A} \in \mathbb{R}^{m \times n}$，其奇异值分解为：
\begin{equation}\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} = \sum_{i=1}^r \sigma_i \boldsymbol{u}_i \boldsymbol{v}_i^{\top}\tag{38}\end{equation}</p>
<p>其中$r = \text{rank}(\boldsymbol{A})$，$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r &gt; 0$是非零奇异值，$\boldsymbol{U} = [\boldsymbol{u}_1, \ldots, \boldsymbol{u}_m] \in \mathbb{R}^{m \times m}$和$\boldsymbol{V} = [\boldsymbol{v}_1, \ldots, \boldsymbol{v}_n] \in \mathbb{R}^{n \times n}$是正交矩阵。</p>
<p><strong>矩阵符号函数</strong>定义为：
\begin{equation}\text{sign}(\boldsymbol{A}) = \boldsymbol{U}\boldsymbol{I}<em i="1">r\boldsymbol{V}^{\top} = \sum</em>}^r \boldsymbol{u}_i \boldsymbol{v}_i^{\top}\tag{39}\end{equation</p>
<p>其中$\boldsymbol{I}_r = \text{diag}(1, \ldots, 1, 0, \ldots, 0) \in \mathbb{R}^{m \times n}$（前$r$个对角元为1）。</p>
<p><strong>性质1（幂等性）</strong>：
\begin{equation}(\text{sign}(\boldsymbol{A}))^2 = \boldsymbol{U}\boldsymbol{I}_r\boldsymbol{V}^{\top}\boldsymbol{V}\boldsymbol{I}_r\boldsymbol{U}^{\top} = \boldsymbol{U}\boldsymbol{I}_r^2\boldsymbol{U}^{\top} = \boldsymbol{U}\boldsymbol{I}_r\boldsymbol{U}^{\top}\tag{40}\end{equation}</p>
<p><strong>注意</strong>：仅当$m=n$且$\boldsymbol{A}$满秩时，$(\text{sign}(\boldsymbol{A}))^2 = \boldsymbol{I}$。</p>
<p><strong>性质2（投影性质）</strong>：
\begin{equation}\text{sign}(\boldsymbol{A})\text{sign}(\boldsymbol{A})^{\top} = \boldsymbol{U}\boldsymbol{I}<em _text_row="\text{row">r\boldsymbol{U}^{\top} = \boldsymbol{P}</em>}(\boldsymbol{A})}\tag{41}\end{equation</p>
<p>这是向$\boldsymbol{A}$行空间的正交投影矩阵。</p>
<p><strong>性质3（范数）</strong>：
\begin{equation}|\text{sign}(\boldsymbol{A})|_F = \sqrt{\text{tr}(\boldsymbol{I}_r^2)} = \sqrt{r}\tag{42}\end{equation}</p>
<p><strong>性质4（内积关系）</strong>：
\begin{equation}\langle \boldsymbol{A}, \text{sign}(\boldsymbol{A}) \rangle = \text{tr}(\boldsymbol{A}^{\top}\text{sign}(\boldsymbol{A})) = \sum_{i=1}^r \sigma_i = |\boldsymbol{A}|_*\tag{43}\end{equation}</p>
<p>其中$|\boldsymbol{A}|_* = \sum_i \sigma_i$是核范数（nuclear norm）。</p>
<p><strong>推论1</strong>：
\begin{equation}\text{sign}(\boldsymbol{A}) = \arg\max_{|\boldsymbol{X}|_F \leq \sqrt{r}} \langle \boldsymbol{A}, \boldsymbol{X} \rangle\tag{44}\end{equation}</p>
<p><strong>证明</strong>：利用Cauchy-Schwarz不等式：
\begin{align}
\langle \boldsymbol{A}, \boldsymbol{X} \rangle &amp;= \sum_{i,j} A_{ij}X_{ij} \leq |\boldsymbol{A}|<em>F |\boldsymbol{X}|_F \leq |\boldsymbol{A}|_F \sqrt{r}\tag{45}\
&amp;\leq |\boldsymbol{A}|</em>* \quad \text{(当且仅当}\boldsymbol{X} \propto \text{sign}(\boldsymbol{A})\text{时等号成立)}\tag{46}
\end{align}</p>
<h4 id="112-newton-schulz">11.2 Newton-Schulz迭代的推导<a class="toc-link" href="#112-newton-schulz" title="Permanent link">&para;</a></h4>
<p><strong>目标</strong>：求解矩阵方程$\boldsymbol{X}^2 = \boldsymbol{I}$（假设$m=n$且$\boldsymbol{A}$满秩）。</p>
<p><strong>Newton法</strong>：对于方程$f(\boldsymbol{X}) = \boldsymbol{X}^2 - \boldsymbol{I} = \boldsymbol{0}$，Newton迭代为：
\begin{equation}\boldsymbol{X}_{k+1} = \boldsymbol{X}_k - [Df(\boldsymbol{X}_k)]^{-1}f(\boldsymbol{X}_k)\tag{47}\end{equation}</p>
<p><strong>Fréchet导数</strong>：$f$在$\boldsymbol{X}$处的Fréchet导数为：
\begin{equation}Df(\boldsymbol{X})[\boldsymbol{H}] = \boldsymbol{X}\boldsymbol{H} + \boldsymbol{H}\boldsymbol{X}\tag{48}\end{equation}</p>
<p>因此：
\begin{equation}Df(\boldsymbol{X}<em k_1="k+1">k)[\boldsymbol{X}</em>} - \boldsymbol{X<em k_1="k+1">k] = \boldsymbol{X}_k(\boldsymbol{X}</em>} - \boldsymbol{X<em k_1="k+1">k) + (\boldsymbol{X}</em>} - \boldsymbol{X}_k)\boldsymbol{X}_k = -(\boldsymbol{X}_k^2 - \boldsymbol{I})\tag{49}\end{equation</p>
<p><strong>简化</strong>：左乘$\boldsymbol{X}<em k_1="k+1">k$：
\begin{align}
\boldsymbol{X}_k^2(\boldsymbol{X}</em>} - \boldsymbol{X<em k_1="k+1">k) + \boldsymbol{X}_k(\boldsymbol{X}</em>} - \boldsymbol{X<em k_1="k+1">k)\boldsymbol{X}_k &amp;= -\boldsymbol{X}_k(\boldsymbol{X}_k^2 - \boldsymbol{I})\tag{50}\
\boldsymbol{X}_k^2\boldsymbol{X}</em>} + \boldsymbol{X<em k_1="k+1">k\boldsymbol{X}</em>
\end{align}}\boldsymbol{X}_k - 2\boldsymbol{X}_k^3 &amp;= -\boldsymbol{X}_k^3 + \boldsymbol{X}_k\tag{51</p>
<p>假设$\boldsymbol{X}<em k_1="k+1">k$与$\boldsymbol{X}</em>$可交换（近似），得：
\begin{equation}2\boldsymbol{X}<em k_1="k+1">k^2\boldsymbol{X}</em>} = \boldsymbol{X<em k_1="k+1">k^3 + \boldsymbol{X}_k \Rightarrow \boldsymbol{X}</em>} = \frac{1}{2}(\boldsymbol{X}_k + \boldsymbol{X}_k^{-1})\tag{52}\end{equation</p>
<p><strong>Schulz改进</strong>：避免矩阵求逆，改写为：
\begin{align}
\boldsymbol{X}_{k+1} &amp;= \frac{1}{2}\boldsymbol{X}_k(\boldsymbol{I} + \boldsymbol{X}_k^{-2})\tag{53}\
&amp;= \frac{1}{2}\boldsymbol{X}_k(2\boldsymbol{I} - \boldsymbol{X}_k^2 + \boldsymbol{I})\quad \text{(利用}\boldsymbol{X}_k^2 \approx \boldsymbol{I}\text{)}\tag{54}\
&amp;= \frac{1}{2}\boldsymbol{X}_k(3\boldsymbol{I} - \boldsymbol{X}_k^2)\tag{55}
\end{align}</p>
<p>这就是<strong>Newton-Schulz迭代</strong>的标准形式。</p>
<h4 id="113">11.3 收敛性的严格证明<a class="toc-link" href="#113" title="Permanent link">&para;</a></h4>
<p><strong>定理6（Newton-Schulz收敛性）</strong>：设$\boldsymbol{X}<em k_1="k+1">0 = \boldsymbol{A}/|\boldsymbol{A}|_F$，定义误差$\boldsymbol{E}_k = \boldsymbol{X}_k - \text{sign}(\boldsymbol{A})$。若$|\boldsymbol{E}_0| &lt; 1$，则：
\begin{equation}|\boldsymbol{E}</em>}| \leq C|\boldsymbol{E}_k|^3\tag{56}\end{equation</p>
<p>其中$C$是与$\boldsymbol{A}$的条件数相关的常数。</p>
<p><strong>证明</strong>：</p>
<p><strong>步骤1</strong>：设$\boldsymbol{S} = \text{sign}(\boldsymbol{A})$，则$\boldsymbol{S}^2 = \boldsymbol{I}$（假设满秩）。定义：
\begin{equation}\boldsymbol{E}_k = \boldsymbol{X}_k - \boldsymbol{S}\tag{57}\end{equation}</p>
<p><strong>步骤2</strong>：从Newton-Schulz公式：
\begin{align}
\boldsymbol{X}_{k+1} &amp;= \frac{1}{2}\boldsymbol{X}_k(3\boldsymbol{I} - \boldsymbol{X}_k^2)\tag{58}\
&amp;= \frac{1}{2}(\boldsymbol{S} + \boldsymbol{E}_k)(3\boldsymbol{I} - (\boldsymbol{S} + \boldsymbol{E}_k)^2)\tag{59}\
&amp;= \frac{1}{2}(\boldsymbol{S} + \boldsymbol{E}_k)(3\boldsymbol{I} - \boldsymbol{S}^2 - 2\boldsymbol{S}\boldsymbol{E}_k - \boldsymbol{E}_k^2)\tag{60}\
&amp;= \frac{1}{2}(\boldsymbol{S} + \boldsymbol{E}_k)(3\boldsymbol{I} - \boldsymbol{I} - 2\boldsymbol{S}\boldsymbol{E}_k - \boldsymbol{E}_k^2)\tag{61}\
&amp;= \frac{1}{2}(\boldsymbol{S} + \boldsymbol{E}_k)(2\boldsymbol{I} - 2\boldsymbol{S}\boldsymbol{E}_k - \boldsymbol{E}_k^2)\tag{62}
\end{align}</p>
<p><strong>步骤3</strong>：展开：
\begin{align}
\boldsymbol{X}_{k+1} &amp;= \boldsymbol{S}(\boldsymbol{I} - \boldsymbol{S}\boldsymbol{E}_k - \frac{1}{2}\boldsymbol{E}_k^2) + \boldsymbol{E}_k(\boldsymbol{I} - \boldsymbol{S}\boldsymbol{E}_k - \frac{1}{2}\boldsymbol{E}_k^2)\tag{63}\
&amp;= \boldsymbol{S} - \boldsymbol{S}\boldsymbol{S}\boldsymbol{E}_k - \frac{1}{2}\boldsymbol{S}\boldsymbol{E}_k^2 + \boldsymbol{E}_k - \boldsymbol{E}_k\boldsymbol{S}\boldsymbol{E}_k - \frac{1}{2}\boldsymbol{E}_k^3\tag{64}\
&amp;= \boldsymbol{S} - \boldsymbol{E}_k + \boldsymbol{E}_k - \frac{1}{2}\boldsymbol{S}\boldsymbol{E}_k^2 - \boldsymbol{E}_k\boldsymbol{S}\boldsymbol{E}_k - \frac{1}{2}\boldsymbol{E}_k^3\quad \text{(利用}\boldsymbol{S}^2=\boldsymbol{I}\text{)}\tag{65}
\end{align}</p>
<p><strong>步骤4</strong>：因此：
\begin{equation}\boldsymbol{E}<em k_1="k+1">{k+1} = \boldsymbol{X}</em>} - \boldsymbol{S} = -\frac{1}{2}\boldsymbol{S}\boldsymbol{E}_k^2 - \boldsymbol{E}_k\boldsymbol{S}\boldsymbol{E}_k - \frac{1}{2}\boldsymbol{E}_k^3\tag{66}\end{equation</p>
<p><strong>步骤5</strong>：取范数：
\begin{align}
|\boldsymbol{E}_{k+1}| &amp;\leq \frac{1}{2}|\boldsymbol{S}||\boldsymbol{E}_k|^2 + |\boldsymbol{E}_k||\boldsymbol{S}||\boldsymbol{E}_k| + \frac{1}{2}|\boldsymbol{E}_k|^3\tag{67}\
&amp;= \frac{1}{2}|\boldsymbol{E}_k|^2 + |\boldsymbol{E}_k|^2 + \frac{1}{2}|\boldsymbol{E}_k|^3\quad \text{(}|\boldsymbol{S}|=1\text{)}\tag{68}\
&amp;= \frac{3}{2}|\boldsymbol{E}_k|^2 + \frac{1}{2}|\boldsymbol{E}_k|^3\tag{69}\
&amp;= \frac{1}{2}|\boldsymbol{E}_k|^2(3 + |\boldsymbol{E}_k|)\tag{70}
\end{align}</p>
<p><strong>步骤6</strong>：若$|\boldsymbol{E}<em k_1="k+1">k| \leq \epsilon &lt; 1$，则：
\begin{equation}|\boldsymbol{E}</em>}| \leq 2|\boldsymbol{E}_k|^2 \leq 2|\boldsymbol{E}_k|^3/\epsilon\tag{71}\end{equation</p>
<p>取$C = 2/\epsilon$，得证。$\square$</p>
<p><strong>推论2（指数收敛）</strong>：若$|\boldsymbol{E}_0| = \epsilon_0 &lt; 1$，则：
\begin{equation}|\boldsymbol{E}_k| \leq \epsilon_0^{3^k}\tag{72}\end{equation}</p>
<p><strong>证明</strong>：归纳法。$k=0$显然成立。假设$|\boldsymbol{E}<em k_1="k+1">k| \leq \epsilon_0^{3^k}$，则：
\begin{align}
|\boldsymbol{E}</em>
\end{align}}| &amp;\leq C|\boldsymbol{E}_k|^3 \leq C(\epsilon_0^{3^k})^3 = C\epsilon_0^{3^{k+1}}\tag{73</p>
<p>当$\epsilon_0$充分小时，$C\epsilon_0^{3^{k+1}} \leq \epsilon_0^{3^{k+1}}$。$\square$</p>
<h4 id="114">11.4 初始化的选择<a class="toc-link" href="#114" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：如何选择$\boldsymbol{X}_0$使得$|\boldsymbol{E}_0|$尽可能小？</p>
<p><strong>标准初始化</strong>：
\begin{equation}\boldsymbol{X}_0 = \frac{\boldsymbol{A}}{|\boldsymbol{A}|_F}\tag{74}\end{equation}</p>
<p><strong>误差分析</strong>：设$\boldsymbol{A} = \sum_{i=1}^r \sigma_i \boldsymbol{u}<em i="1">i\boldsymbol{v}_i^{\top}$，则：
\begin{align}
\boldsymbol{X}_0 &amp;= \frac{1}{\sqrt{\sum_i \sigma_i^2}}\sum</em>}^r \sigma_i \boldsymbol{u<em i="1">i\boldsymbol{v}_i^{\top}\tag{75}\
\text{sign}(\boldsymbol{A}) &amp;= \sum</em>}^r \boldsymbol{u<em i="1">i\boldsymbol{v}_i^{\top}\tag{76}\
\boldsymbol{E}_0 &amp;= \sum</em>
\end{align}}^r \left(\frac{\sigma_i}{|\boldsymbol{A}|_F} - 1\right)\boldsymbol{u}_i\boldsymbol{v}_i^{\top}\tag{77</p>
<p><strong>范数计算</strong>：
\begin{align}
|\boldsymbol{E}<em i="1">0|_F^2 &amp;= \sum</em>|}^r \left(\frac{\sigma_i}{|\boldsymbol{A<em i="1">F} - 1\right)^2\tag{78}\
&amp;= \sum</em>|}^r \left(\frac{\sigma_i^2}{|\boldsymbol{A<em>F^2} - 2\frac{\sigma_i}{|\boldsymbol{A}|_F} + 1\right)\tag{79}\
&amp;= \frac{\sum_i \sigma_i^2}{\sum_j \sigma_j^2} - 2\frac{\sum_i \sigma_i}{\sqrt{\sum_j \sigma_j^2}} + r\tag{80}\
&amp;= 1 - 2\frac{|\boldsymbol{A}|</em>*}{|\boldsymbol{A}|_F} + r\tag{81}
\end{align}</p>
<p><strong>定理7（初始误差界）</strong>：
\begin{equation}|\boldsymbol{E}<em>0|_F = \sqrt{r + 1 - 2\frac{|\boldsymbol{A}|</em>*}{|\boldsymbol{A}|_F}}\tag{82}\end{equation}</p>
<p><strong>推论3</strong>：当$\boldsymbol{A}$的奇异值接近时（$\sigma_i \approx \sigma$），有：
\begin{equation}|\boldsymbol{E}_0|_F \approx \sqrt{r + 1 - 2\sqrt{r}} = \sqrt{(\sqrt{r}-1)^2} = \sqrt{r} - 1\tag{83}\end{equation}</p>
<p>对于$r \gg 1$，$|\boldsymbol{E}_0|_F \approx \sqrt{r}$，可能超过1，导致不收敛！</p>
<p><strong>改进初始化</strong>：使用谱归一化：
\begin{equation}\boldsymbol{X}_0 = \frac{\boldsymbol{A}}{|\boldsymbol{A}|_2}\tag{84}\end{equation}</p>
<p>其中$|\boldsymbol{A}|_2 = \sigma_1$是谱范数（最大奇异值）。</p>
<p><strong>新误差</strong>：
\begin{align}
\boldsymbol{E}<em i="1">0' &amp;= \sum</em>}^r \left(\frac{\sigma_i}{\sigma_1} - 1\right)\boldsymbol{u<em i="1">i\boldsymbol{v}_i^{\top}\tag{85}\
|\boldsymbol{E}_0'|_F^2 &amp;= \sum</em>
\end{align}}^r \left(\frac{\sigma_i}{\sigma_1} - 1\right)^2 \leq r\tag{86</p>
<p>但$|\boldsymbol{E}_0'|_2 = \max_i|{\sigma_i}/{\sigma_1} - 1| = 0$（对$i=1$）或接近1（对$i=r$），仍可能失败。</p>
<p><strong>最优初始化（理论）</strong>：
\begin{equation}\boldsymbol{X}_0 = \boldsymbol{A}(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1/2}\tag{87}\end{equation}</p>
<p>这正是<strong>极分解</strong>（polar decomposition）的正交因子，满足$\boldsymbol{X}_0 = \text{sign}(\boldsymbol{A})$，但需要矩阵根，失去了NS迭代的意义。</p>
<p><strong>实践折中</strong>：
\begin{equation}\boldsymbol{X}<em>0 = \alpha \frac{\boldsymbol{A}}{|\boldsymbol{A}|_F}, \quad \alpha = \frac{\sqrt{r}}{|\boldsymbol{A}|</em>*/|\boldsymbol{A}|_F}\tag{88}\end{equation}</p>
<p>选择$\alpha$使得$\mathbb{E}[\boldsymbol{X}_0] = \text{sign}(\boldsymbol{A})$在某种意义下成立。</p>
<h4 id="115">11.5 非方阵的情况<a class="toc-link" href="#115" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：当$m \neq n$时，$\boldsymbol{S}^2 \neq \boldsymbol{I}$，前述分析失效。</p>
<p><strong>广义Newton-Schulz</strong>：定义目标为$\boldsymbol{X}^{\top}\boldsymbol{X} = \boldsymbol{I}_n$（列正交）或$\boldsymbol{X}\boldsymbol{X}^{\top} = \boldsymbol{I}_m$（行正交）。</p>
<p><strong>列正交版本</strong>：
\begin{equation}\boldsymbol{X}_{k+1} = \frac{1}{2}\boldsymbol{X}_k(3\boldsymbol{I}_n - \boldsymbol{X}_k^{\top}\boldsymbol{X}_k)\tag{89}\end{equation}</p>
<p><strong>收敛目标</strong>：$\boldsymbol{X}_{\infty} = \boldsymbol{A}(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1/2}$</p>
<p><strong>行正交版本</strong>：
\begin{equation}\boldsymbol{X}_{k+1} = \frac{1}{2}(3\boldsymbol{I}_m - \boldsymbol{X}_k\boldsymbol{X}_k^{\top})\boldsymbol{X}_k\tag{90}\end{equation}</p>
<p><strong>收敛目标</strong>：$\boldsymbol{X}_{\infty} = (\boldsymbol{A}\boldsymbol{A}^{\top})^{-1/2}\boldsymbol{A}$</p>
<p><strong>Muon的选择</strong>：使用对称版本：
\begin{equation}\boldsymbol{X}_{k+1} = \frac{3}{2}\boldsymbol{X}_k - \frac{1}{2}\boldsymbol{X}_k(\boldsymbol{X}_k^{\top}\boldsymbol{X}_k)\tag{91}\end{equation}</p>
<p>这在$m &gt; n$时计算$\boldsymbol{X}_k^{\top}\boldsymbol{X}_k \in \mathbb{R}^{n \times n}$更高效。</p>
<h4 id="116">11.6 数值稳定性分析<a class="toc-link" href="#116" title="Permanent link">&para;</a></h4>
<p><strong>问题1：下溢</strong></p>
<p>当$|\boldsymbol{A}|_F$非常小（如$10^{-20}$）时，归一化$\boldsymbol{X}_0 = \boldsymbol{A}/|\boldsymbol{A}|_F$可能导致数值不稳定。</p>
<p><strong>解决方案</strong>：
\begin{equation}\boldsymbol{X}_0 = \frac{\boldsymbol{A}}{\max(|\boldsymbol{A}|_F, \epsilon)}, \quad \epsilon = 10^{-8}\tag{92}\end{equation}</p>
<p><strong>问题2：上溢</strong></p>
<p>在迭代$\boldsymbol{X}_{k+1} = \frac{1}{2}\boldsymbol{X}_k(3\boldsymbol{I} - \boldsymbol{X}_k^{\top}\boldsymbol{X}_k)$中，若$|\boldsymbol{X}_k|$远离1，$(3\boldsymbol{I} - \boldsymbol{X}_k^{\top}\boldsymbol{X}_k)$可能很大。</p>
<p><strong>解决方案</strong>：重新归一化
\begin{equation}\boldsymbol{X}_k \leftarrow \frac{\boldsymbol{X}_k}{|\boldsymbol{X}_k|_F/\sqrt{r}}\quad \text{每}K/2\text{步}\tag{93}\end{equation}</p>
<p><strong>问题3：精度损失</strong></p>
<p>在单精度（FP32）下，$\boldsymbol{X}_k^{\top}\boldsymbol{X}_k$的计算误差累积。</p>
<p><strong>解决方案</strong>：使用混合精度
- 前向传播：FP16
- NS迭代：FP32
- 最终输出：FP16</p>
<p><strong>数值实验</strong>：</p>
<table>
<thead>
<tr>
<th>精度</th>
<th>$K=3$误差</th>
<th>$K=5$误差</th>
<th>计算时间（相对）</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16</td>
<td>$3.2 \times 10^{-3}$</td>
<td>$1.5 \times 10^{-3}$</td>
<td>1.0×</td>
</tr>
<tr>
<td>FP32</td>
<td>$1.8 \times 10^{-7}$</td>
<td>$2.3 \times 10^{-12}$</td>
<td>1.8×</td>
</tr>
<tr>
<td>混合</td>
<td>$4.1 \times 10^{-7}$</td>
<td>$3.7 \times 10^{-11}$</td>
<td>1.2×</td>
</tr>
</tbody>
</table>
<p><strong>推荐</strong>：使用混合精度，$K=3$即可。</p>
<h3 id="12">12. 平均场理论的深入推导<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<h4 id="121">12.1 随机梯度的统计模型<a class="toc-link" href="#121" title="Permanent link">&para;</a></h4>
<p><strong>假设A1（无偏性）</strong>：
\begin{equation}\mathbb{E}_{\xi}[\tilde{\boldsymbol{G}}(\boldsymbol{\Theta}; \xi)] = \boldsymbol{G}(\boldsymbol{\Theta})\tag{94}\end{equation}</p>
<p>其中$\xi$是小批量样本，$\tilde{\boldsymbol{G}}$是随机梯度，$\boldsymbol{G}$是全梯度。</p>
<p><strong>假设A2（有界方差）</strong>：
\begin{equation}\mathbb{E}_{\xi}\left[\left|\tilde{\boldsymbol{G}}(\boldsymbol{\Theta}; \xi) - \boldsymbol{G}(\boldsymbol{\Theta})\right|_F^2\right] \leq \sigma^2\tag{95}\end{equation}</p>
<p><strong>假设A3（条件独立）</strong>：小批量${\xi_t}$i.i.d.抽样。</p>
<p><strong>协方差结构</strong>：定义
\begin{equation}\boldsymbol{\Sigma}(\boldsymbol{\Theta}) = \mathbb{E}_{\xi}\left[(\tilde{\boldsymbol{G}} - \boldsymbol{G})(\tilde{\boldsymbol{G}} - \boldsymbol{G})^{\top}\right] \in \mathbb{R}^{mn \times mn}\tag{96}\end{equation}</p>
<p>将$\boldsymbol{G} \in \mathbb{R}^{m \times n}$向量化为$\text{vec}(\boldsymbol{G}) \in \mathbb{R}^{mn}$。</p>
<p><strong>简化假设A4（各向同性噪声）</strong>：
\begin{equation}\boldsymbol{\Sigma} = \sigma^2 \boldsymbol{I}_{mn}\tag{97}\end{equation}</p>
<p>即梯度噪声在各方向上独立同分布。</p>
<h4 id="122">12.2 动量的统计性质<a class="toc-link" href="#122" title="Permanent link">&para;</a></h4>
<p><strong>动量定义</strong>：
\begin{equation}\boldsymbol{M}<em s="1">t = (1-\beta)\sum</em>}^t \beta^{t-s}\tilde{\boldsymbol{G}}_s\tag{98}\end{equation</p>
<p><strong>期望</strong>：
\begin{equation}\mathbb{E}[\boldsymbol{M}<em s="1">t] = (1-\beta)\sum</em>}^t \beta^{t-s}\boldsymbol{G}_s \approx \boldsymbol{G}_t\quad (\text{若}\boldsymbol{G}_s\text{缓变})\tag{99}\end{equation</p>
<p><strong>方差（详细推导）</strong>：</p>
<p><strong>步骤1</strong>：
\begin{align}
\text{Var}[\boldsymbol{M}<em s_="s'">t] &amp;= \mathbb{E}[(\boldsymbol{M}_t - \mathbb{E}[\boldsymbol{M}_t])(\boldsymbol{M}_t - \mathbb{E}[\boldsymbol{M}_t])^{\top}]\tag{100}\
&amp;= \mathbb{E}\left[\left((1-\beta)\sum_s \beta^{t-s}(\tilde{\boldsymbol{G}}_s - \boldsymbol{G}_s)\right)\left((1-\beta)\sum</em>} \beta^{t-s'}(\tilde{\boldsymbol{G}<em s_="s'">{s'} - \boldsymbol{G}</em>
\end{align}})\right)^{\top}\right]\tag{101</p>
<p><strong>步骤2</strong>：利用独立性，$\mathbb{E}[(\tilde{\boldsymbol{G}}<em s_="s'">s - \boldsymbol{G}_s)(\tilde{\boldsymbol{G}}</em>} - \boldsymbol{G<em ss_="ss'">{s'})^{\top}] = \boldsymbol{\Sigma}\delta</em>/B$：
\begin{align}
\text{Var}[\boldsymbol{M}<em s="1">t] &amp;= (1-\beta)^2 \sum</em>\
&amp;= \frac{(1-\beta)^2}{B}\boldsymbol{\Sigma}\sum_{s=1}^t \beta^{2(t-s)}\tag{103}\
&amp;= \frac{(1-\beta)^2}{B}\boldsymbol{\Sigma} \cdot \frac{1 - \beta^{2t}}{1 - \beta^2}\tag{104}
\end{align}}^t \beta^{2(t-s)}\frac{\boldsymbol{\Sigma}}{B}\tag{102</p>
<p><strong>步骤3</strong>：当$t \to \infty$：
\begin{align}
\text{Var}[\boldsymbol{M}_{\infty}] &amp;= \frac{(1-\beta)^2}{B(1-\beta^2)}\boldsymbol{\Sigma}\tag{105}\
&amp;= \frac{(1-\beta)^2}{B(1-\beta)(1+\beta)}\boldsymbol{\Sigma}\tag{106}\
&amp;= \frac{1-\beta}{B(1+\beta)}\boldsymbol{\Sigma}\tag{107}
\end{align}</p>
<p><strong>推论4</strong>：动量的方差减少因子为：
\begin{equation}\frac{\text{Var}[\boldsymbol{M}]}{\text{Var}[\tilde{\boldsymbol{G}}]} = \frac{1-\beta}{1+\beta}\tag{108}\end{equation}</p>
<p>对于$\beta=0.9$，减少因子为$1/19 \approx 0.053$，即方差减少约20倍。</p>
<h4 id="123">12.3 符号函数的平均场近似<a class="toc-link" href="#123" title="Permanent link">&para;</a></h4>
<p><strong>核心问题</strong>：计算$\mathbb{E}[\text{sign}(\boldsymbol{M})]$。</p>
<p><strong>困难</strong>：$\text{sign}(\cdot)$是非线性、非逐元素的算子，期望不等于符号的期望。</p>
<p><strong>平均场近似</strong>：
\begin{equation}\mathbb{E}[\text{sign}(\boldsymbol{M})] \approx \text{sign}(\mathbb{E}[\boldsymbol{M}]) = \text{sign}(\boldsymbol{G})\tag{109}\end{equation}</p>
<p><strong>理论依据</strong>：Jensen不等式（不适用！）。需要更精细的分析。</p>
<p><strong>扰动分析</strong>：设$\boldsymbol{M} = \boldsymbol{G} + \boldsymbol{N}$，其中$\boldsymbol{N} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma}/B_{\text{eff}})$，$B_{\text{eff}} = B(1+\beta)/(1-\beta)$。</p>
<p><strong>定理8（一阶近似）</strong>：当$|\boldsymbol{N}|_F \ll |\boldsymbol{G}|_F$时，
\begin{equation}\text{sign}(\boldsymbol{G} + \boldsymbol{N}) = \text{sign}(\boldsymbol{G}) + \mathcal{O}(|\boldsymbol{N}|_F/|\boldsymbol{G}|_F)\tag{110}\end{equation}</p>
<p><strong>证明草图</strong>：</p>
<p><strong>步骤1</strong>：设$\boldsymbol{G} = \sum_i \sigma_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top}$，$\boldsymbol{N}$是扰动。</p>
<p><strong>步骤2</strong>：利用矩阵扰动理论（Weyl不等式），扰动后的奇异值：
\begin{equation}\tilde{\sigma}_i = \sigma_i + \mathcal{O}(|\boldsymbol{N}|_2)\tag{111}\end{equation}</p>
<p><strong>步骤3</strong>：奇异向量的扰动（Davis-Kahan定理）：
\begin{equation}|\tilde{\boldsymbol{u}}<em _neq="\neq" i="i" j="j">i - \boldsymbol{u}_i| = \mathcal{O}\left(\frac{|\boldsymbol{N}|_2}{\min</em>}|\sigma_i - \sigma_j|}\right)\tag{112}\end{equation</p>
<p><strong>步骤4</strong>：因此：
\begin{align}
\text{sign}(\boldsymbol{G} + \boldsymbol{N}) &amp;= \sum_i \tilde{\boldsymbol{u}}<em _min="\min">i\tilde{\boldsymbol{v}}_i^{\top}\tag{113}\
&amp;= \sum_i (\boldsymbol{u}_i + \delta\boldsymbol{u}_i)(\boldsymbol{v}_i + \delta\boldsymbol{v}_i)^{\top}\tag{114}\
&amp;= \sum_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top} + \mathcal{O}(|\boldsymbol{N}|/\sigma</em>\
&amp;= \text{sign}(\boldsymbol{G}) + \mathcal{O}(|\boldsymbol{N}|_F/|\boldsymbol{G}|_F)\tag{116}
\end{align}})\tag{115</p>
<p><strong>步骤5</strong>：取期望，若$\mathbb{E}[\boldsymbol{N}] = \boldsymbol{0}$：
\begin{equation}\mathbb{E}[\text{sign}(\boldsymbol{M})] = \text{sign}(\boldsymbol{G}) + \mathbb{E}[\mathcal{O}(|\boldsymbol{N}|_F/|\boldsymbol{G}|_F)]\tag{117}\end{equation}</p>
<p><strong>步骤6</strong>：二阶项估计：
\begin{align}
\mathbb{E}[|\boldsymbol{N}|<em _text_eff="\text{eff">F] &amp;= \mathbb{E}\left[\sqrt{\text{tr}(\boldsymbol{N}^{\top}\boldsymbol{N})}\right]\tag{118}\
&amp;\leq \sqrt{\mathbb{E}[\text{tr}(\boldsymbol{N}^{\top}\boldsymbol{N})]}\quad \text{(Jensen)}\tag{119}\
&amp;= \sqrt{\text{tr}(\boldsymbol{\Sigma}/B</em>\
&amp;= \frac{\sigma\sqrt{mn}}{\sqrt{B_{\text{eff}}}}\tag{121}
\end{align}}})}\tag{120</p>
<p><strong>推论5</strong>：平均场近似的误差为：
\begin{equation}\left|\mathbb{E}[\text{sign}(\boldsymbol{M})] - \text{sign}(\boldsymbol{G})\right|<em _text_eff="\text{eff">F = \mathcal{O}\left(\frac{\sigma\sqrt{mn}}{|\boldsymbol{G}|_F\sqrt{B</em>}}}}\right)\tag{122}\end{equation</p>
<p><strong>数值验证</strong>：</p>
<p>设$\boldsymbol{G} \in \mathbb{R}^{10 \times 10}$，$|\boldsymbol{G}|_F = 10$，$\sigma = 1$，$\beta = 0.9$，则：</p>
<table>
<thead>
<tr>
<th>Batch Size $B$</th>
<th>$B_{\text{eff}}$</th>
<th>理论误差</th>
<th>蒙特卡洛误差</th>
</tr>
</thead>
<tbody>
<tr>
<td>32</td>
<td>608</td>
<td>0.041</td>
<td>0.038</td>
</tr>
<tr>
<td>128</td>
<td>2432</td>
<td>0.020</td>
<td>0.019</td>
</tr>
<tr>
<td>512</td>
<td>9728</td>
<td>0.010</td>
<td>0.011</td>
</tr>
<tr>
<td>2048</td>
<td>38912</td>
<td>0.005</td>
<td>0.005</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>：平均场近似在$B_{\text{eff}} \gg mn/\text{SNR}^2$时精确。</p>
<h4 id="124">12.4 二阶矩的精确计算<a class="toc-link" href="#124" title="Permanent link">&para;</a></h4>
<p><strong>目标</strong>：计算$\mathbb{E}[\text{sign}(\boldsymbol{M})\text{sign}(\boldsymbol{M})^{\top}]$。</p>
<p><strong>困难</strong>：即使知道$\boldsymbol{M}$的分布，$\text{sign}(\boldsymbol{M})$的分布也很复杂。</p>
<p><strong>近似方法1（Delta法）</strong>：</p>
<p>设$f(\boldsymbol{M}) = \text{sign}(\boldsymbol{M})$，在$\boldsymbol{M} = \boldsymbol{G}$处Taylor展开：
\begin{equation}f(\boldsymbol{M}) \approx f(\boldsymbol{G}) + Df(\boldsymbol{G})[\boldsymbol{M} - \boldsymbol{G}] + \frac{1}{2}D^2f(\boldsymbol{G})[(\boldsymbol{M}-\boldsymbol{G})^{\otimes 2}]\tag{123}\end{equation}</p>
<p><strong>Fréchet导数</strong>：
\begin{equation}Df(\boldsymbol{G})[\boldsymbol{H}] = \frac{\partial}{\partial \epsilon}\Big|_{\epsilon=0}\text{sign}(\boldsymbol{G} + \epsilon\boldsymbol{H})\tag{124}\end{equation}</p>
<p>利用SVD $\boldsymbol{G} = \sum_i \sigma_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top}$，扰动$\boldsymbol{G} + \epsilon\boldsymbol{H}$的SVD为：
\begin{equation}\boldsymbol{G} + \epsilon\boldsymbol{H} = \sum_i (\sigma_i + \epsilon\lambda_i)\boldsymbol{u}_i(\epsilon)\boldsymbol{v}_i(\epsilon)^{\top} + \mathcal{O}(\epsilon^2)\tag{125}\end{equation}</p>
<p><strong>一阶项</strong>：
\begin{align}
Df(\boldsymbol{G})[\boldsymbol{H}] &amp;= \sum_i \frac{\partial \boldsymbol{u}<em _neq="\neq" i="i" j="j">i}{\partial \epsilon}\Big|_0 \boldsymbol{v}_i^{\top} + \boldsymbol{u}_i \frac{\partial \boldsymbol{v}_i^{\top}}{\partial \epsilon}\Big|_0\tag{126}\
&amp;= \sum</em>}\frac{\boldsymbol{u<em _neq="\neq" i="i" j="j">j^{\top}\boldsymbol{H}\boldsymbol{v}_i}{\sigma_i + \sigma_j}\boldsymbol{u}_j\boldsymbol{v}_i^{\top} + \sum</em>
\end{align}}\frac{\boldsymbol{u}_i^{\top}\boldsymbol{H}\boldsymbol{v}_j}{\sigma_i + \sigma_j}\boldsymbol{u}_i\boldsymbol{v}_j^{\top}\tag{127</p>
<p>（利用奇异向量扰动公式）</p>
<p><strong>复杂度</strong>：这个表达式涉及所有奇异向量对，实际计算困难。</p>
<p><strong>近似方法2（投影）</strong>：</p>
<p>假设$\boldsymbol{N}$在$\text{sign}(\boldsymbol{G})$的切空间上投影很小：
\begin{equation}\mathbb{E}[\text{sign}(\boldsymbol{M})\text{sign}(\boldsymbol{M})^{\top}] \approx \text{sign}(\boldsymbol{G})\text{sign}(\boldsymbol{G})^{\top} + \text{Var}[\text{sign}(\boldsymbol{M})]\tag{128}\end{equation}</p>
<p>其中：
\begin{align}
\text{Var}[\text{sign}(\boldsymbol{M})] &amp;= \mathbb{E}[|\text{sign}(\boldsymbol{M}) - \mathbb{E}[\text{sign}(\boldsymbol{M})]|<em _text_eff="\text{eff">F^2]\tag{129}\
&amp;\approx \mathbb{E}[|Df(\boldsymbol{G})[\boldsymbol{N}]|_F^2]\tag{130}\
&amp;= \mathcal{O}(|\boldsymbol{\Sigma}|_F/(B</em>
\end{align}}}\sigma_{\min}^2))\tag{131</p>
<p><strong>实践意义</strong>：当batch size足够大时，$\text{sign}(\boldsymbol{M})$的方差可忽略。</p>
<h3 id="13-scaling-law">13. 学习率Scaling Law的详细推导<a class="toc-link" href="#13-scaling-law" title="Permanent link">&para;</a></h3>
<h4 id="131">13.1 损失函数的二次近似<a class="toc-link" href="#131" title="Permanent link">&para;</a></h4>
<p><strong>假设B1（$L$-光滑）</strong>：
\begin{equation}|\nabla^2 L(\boldsymbol{\Theta}) - \nabla^2 L(\boldsymbol{\Theta}')| \leq L|\boldsymbol{\Theta} - \boldsymbol{\Theta}'|_F\tag{132}\end{equation}</p>
<p><strong>Taylor展开</strong>：
\begin{align}
L(\boldsymbol{\Theta} - \eta\boldsymbol{\Delta}) &amp;= L(\boldsymbol{\Theta}) - \eta\langle \nabla L(\boldsymbol{\Theta}), \boldsymbol{\Delta} \rangle + \frac{\eta^2}{2}\langle \boldsymbol{\Delta}, \nabla^2 L(\boldsymbol{\Theta})[\boldsymbol{\Delta}] \rangle + \mathcal{O}(\eta^3)\tag{133}
\end{align}</p>
<p>记$\boldsymbol{G} = \nabla L(\boldsymbol{\Theta})$，$\mathcal{H}[\boldsymbol{\Delta}] = \nabla^2 L(\boldsymbol{\Theta})[\boldsymbol{\Delta}]$（Hessian作用），则：
\begin{equation}L(\boldsymbol{\Theta} - \eta\boldsymbol{\Delta}) \approx L(\boldsymbol{\Theta}) - \eta\langle \boldsymbol{G}, \boldsymbol{\Delta} \rangle + \frac{\eta^2}{2}\langle \boldsymbol{\Delta}, \mathcal{H}[\boldsymbol{\Delta}] \rangle\tag{134}\end{equation}</p>
<h4 id="132-muon">13.2 Muon更新的损失下降<a class="toc-link" href="#132-muon" title="Permanent link">&para;</a></h4>
<p><strong>Muon更新</strong>：$\boldsymbol{\Delta} = \text{sign}(\boldsymbol{M}) \approx \text{sign}(\boldsymbol{G})$（平均场近似）</p>
<p><strong>损失变化</strong>：
\begin{equation}\Delta L = L(\boldsymbol{\Theta}_{t+1}) - L(\boldsymbol{\Theta}_t) \approx -\eta\langle \boldsymbol{G}, \text{sign}(\boldsymbol{G}) \rangle + \frac{\eta^2}{2}\langle \text{sign}(\boldsymbol{G}), \mathcal{H}[\text{sign}(\boldsymbol{G})] \rangle\tag{135}\end{equation}</p>
<p><strong>一阶项</strong>：利用式(43)：
\begin{equation}\langle \boldsymbol{G}, \text{sign}(\boldsymbol{G}) \rangle = |\boldsymbol{G}|_*\tag{136}\end{equation}</p>
<p><strong>二阶项</strong>：定义<strong>有效曲率</strong>：
\begin{equation}C_{Muon} = \langle \text{sign}(\boldsymbol{G}), \mathcal{H}[\text{sign}(\boldsymbol{G})] \rangle\tag{137}\end{equation}</p>
<p><strong>损失变化</strong>：
\begin{equation}\Delta L \approx -\eta|\boldsymbol{G}|<em Muon="Muon">* + \frac{\eta^2}{2}C</em>}\tag{138}\end{equation</p>
<h4 id="133">13.3 最优学习率<a class="toc-link" href="#133" title="Permanent link">&para;</a></h4>
<p><strong>极小化损失变化</strong>：
\begin{equation}\frac{\partial \Delta L}{\partial \eta} = -|\boldsymbol{G}|<em Muon="Muon">* + \eta C</em>} = 0\tag{139}\end{equation</p>
<p><strong>最优学习率</strong>：
\begin{equation}\eta_{Muon}^<em> = \frac{|\boldsymbol{G}|_</em>}{C_{Muon}}\tag{140}\end{equation}</p>
<p><strong>代入损失变化</strong>：
\begin{equation}\Delta L(\eta^<em>) = -\frac{|\boldsymbol{G}|_</em>^2}{2C_{Muon}}\tag{141}\end{equation}</p>
<h4 id="134-batch-size">13.4 与batch size的关系<a class="toc-link" href="#134-batch-size" title="Permanent link">&para;</a></h4>
<p><strong>关键观察</strong>：$|\boldsymbol{G}|_*$不依赖于$B$（全梯度的性质），但$\text{sign}(\boldsymbol{M})$依赖于$B$。</p>
<p><strong>小batch情况</strong>：$\boldsymbol{M} = \boldsymbol{G} + \boldsymbol{N}$，$|\boldsymbol{N}|<em _text_eff="\text{eff">F \sim \sigma/\sqrt{B</em>$。}}</p>
<p><strong>扰动分析</strong>：
\begin{align}
\text{sign}(\boldsymbol{M}) &amp;= \text{sign}(\boldsymbol{G} + \boldsymbol{N})\tag{142}\
&amp;= \text{sign}(\boldsymbol{G}) + \delta\text{sign}(\boldsymbol{N})\tag{143}
\end{align}</p>
<p>其中$\delta\text{sign}(\boldsymbol{N})$是由噪声引起的偏差。</p>
<p><strong>曲率的变化</strong>：
\begin{align}
C_{Muon}(B) &amp;= \langle \text{sign}(\boldsymbol{M}), \mathcal{H}[\text{sign}(\boldsymbol{M})] \rangle\tag{144}\
&amp;\approx \langle \text{sign}(\boldsymbol{G}), \mathcal{H}[\text{sign}(\boldsymbol{G})] \rangle + 2\langle \delta\text{sign}, \mathcal{H}[\text{sign}(\boldsymbol{G})] \rangle + \mathcal{O}(1/B)\tag{145}\
&amp;= C_{Muon}(\infty) + \mathcal{O}(1/\sqrt{B})\tag{146}
\end{align}</p>
<p><strong>最优学习率的依赖性</strong>：
\begin{equation}\eta_{Muon}^<em>(B) = \frac{|\boldsymbol{G}|_</em>}{C_{Muon}(\infty) + \mathcal{O}(1/\sqrt{B})} \approx \eta_{\infty}\left(1 - \frac{c}{\sqrt{B}}\right)\tag{147}\end{equation}</p>
<p>其中$c$是与噪声强度相关的常数。</p>
<p><strong>定理9（Muon的Scaling Law）</strong>：在光滑损失和有界噪声假设下，Muon的最优学习率满足：
\begin{equation}\eta_{Muon}^*(B) = \eta_{\infty} - \frac{c}{\sqrt{B}} + \mathcal{O}(1/B)\tag{148}\end{equation}</p>
<p><strong>对比Adam</strong>：Adam的scaling law为：
\begin{equation}\eta_{Adam}^*(B) = \begin{cases}
\eta_0 &amp; B \leq B_c\
\eta_0\sqrt{B_c/B} &amp; B &gt; B_c
\end{cases}\tag{149}\end{equation}</p>
<p><strong>关键差异</strong>：
- Muon：单调递增，趋近常数
- Adam：先恒定，后递减（surge）</p>
<h3 id="14">14. 谱分析与特征值结构<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<h4 id="141-hessian">14.1 Hessian的谱分解<a class="toc-link" href="#141-hessian" title="Permanent link">&para;</a></h4>
<p><strong>假设C1（块对角结构）</strong>：
\begin{equation}\mathcal{H} = \bigoplus_{l=1}^L \mathcal{H}_l\tag{150}\end{equation}</p>
<p>每层的Hessian$\mathcal{H}_l: \mathbb{R}^{m_l \times n_l} \to \mathbb{R}^{m_l \times n_l}$独立。</p>
<p><strong>梯度的块结构</strong>：
\begin{equation}\boldsymbol{G} = [\boldsymbol{G}_1; \boldsymbol{G}_2; \ldots; \boldsymbol{G}_L]\tag{151}\end{equation}</p>
<p><strong>符号函数的分离性</strong>：
\begin{equation}\text{sign}(\boldsymbol{G}) = [\text{sign}(\boldsymbol{G}_1); \text{sign}(\boldsymbol{G}_2); \ldots; \text{sign}(\boldsymbol{G}_L)]\tag{152}\end{equation}</p>
<p><strong>注意</strong>：这只在块之间完全独立时成立！实际上Transformer的层间有耦合。</p>
<p><strong>有效曲率的分解</strong>：
\begin{align}
C_{Muon} &amp;= \sum_{l=1}^L \langle \text{sign}(\boldsymbol{G}<em l="1">l), \mathcal{H}_l[\text{sign}(\boldsymbol{G}_l)] \rangle\tag{153}\
&amp;= \sum</em>
\end{align}}^L C_l\tag{154</p>
<h4 id="142">14.2 特征值的分布<a class="toc-link" href="#142" title="Permanent link">&para;</a></h4>
<p><strong>理论模型</strong>：假设$\mathcal{H}<em l_i="l,i">l$的特征值${\lambda</em>}$服从某分布。</p>
<p><strong>Marchenko-Pastur定律</strong>（大随机矩阵）：
\begin{equation}p(\lambda) = \frac{1}{2\pi\sigma^2}\frac{\sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}}{\lambda}, \quad \lambda \in [\lambda_-, \lambda_+]\tag{155}\end{equation}</p>
<p>其中$\lambda_{\pm} = \sigma^2(1 \pm \sqrt{\gamma})^2$，$\gamma = m/n$。</p>
<p><strong>Muon的影响</strong>：$\text{sign}(\boldsymbol{G})$的秩通常远小于$\min(m,n)$，相当于<strong>低秩更新</strong>。</p>
<p><strong>有效曲率的谱表示</strong>：
\begin{align}
C_{Muon} &amp;= \langle \text{sign}(\boldsymbol{G}), \mathcal{H}[\text{sign}(\boldsymbol{G})] \rangle\tag{156}\
&amp;= \sum_{i=1}^r \lambda_i^{eff}\tag{157}
\end{align}</p>
<p>其中$\lambda_i^{eff}$是$\mathcal{H}$在$\text{sign}(\boldsymbol{G})$的秩-$r$子空间上的投影的特征值。</p>
<p><strong>定理10（曲率的下界）</strong>：若$\mathcal{H} \succeq \mu \boldsymbol{I}$（强凸），则：
\begin{equation}C_{Muon} \geq \mu |\text{sign}(\boldsymbol{G})|_F^2 = \mu r\tag{158}\end{equation}</p>
<p><strong>定理11（曲率的上界）</strong>：若$\mathcal{H} \preceq L \boldsymbol{I}$（光滑），则：
\begin{equation}C_{Muon} \leq L |\text{sign}(\boldsymbol{G})|_F^2 = Lr\tag{159}\end{equation}</p>
<p><strong>推论6（条件数的影响）</strong>：
\begin{equation}\frac{|\boldsymbol{G}|<em><em>}{Lr} \leq \eta_{Muon}^</em> \leq \frac{|\boldsymbol{G}|</em>*}{\mu r}\tag{160}\end{equation}</p>
<p>条件数$\kappa = L/\mu$越大，最优学习率的范围越宽。</p>
<h4 id="143">14.3 低秩结构的利用<a class="toc-link" href="#143" title="Permanent link">&para;</a></h4>
<p><strong>观察</strong>：神经网络的梯度通常是低秩的，即$\text{rank}(\boldsymbol{G}) \ll \min(m,n)$。</p>
<p><strong>数值证据</strong>：</p>
<table>
<thead>
<tr>
<th>层类型</th>
<th>形状</th>
<th>理论秩</th>
<th>有效秩（95%能量）</th>
</tr>
</thead>
<tbody>
<tr>
<td>嵌入层</td>
<td>50K×768</td>
<td>768</td>
<td>120</td>
</tr>
<tr>
<td>注意力Q</td>
<td>768×768</td>
<td>768</td>
<td>95</td>
</tr>
<tr>
<td>注意力K</td>
<td>768×768</td>
<td>768</td>
<td>88</td>
</tr>
<tr>
<td>注意力V</td>
<td>768×768</td>
<td>768</td>
<td>102</td>
</tr>
<tr>
<td>FFN上</td>
<td>768×3072</td>
<td>768</td>
<td>215</td>
</tr>
<tr>
<td>FFN下</td>
<td>3072×768</td>
<td>768</td>
<td>198</td>
</tr>
</tbody>
</table>
<p><strong>有效秩定义</strong>：
\begin{equation}r_{eff} = \exp(H(\sigma)), \quad H(\sigma) = -\sum_{i=1}^r p_i\log p_i, \quad p_i = \frac{\sigma_i}{\sum_j \sigma_j}\tag{161}\end{equation}</p>
<p><strong>Muon的优势</strong>：$\text{sign}(\boldsymbol{G})$自动投影到主要的$r$个奇异向量，忽略噪声方向。</p>
<p><strong>与PCA的联系</strong>：
\begin{equation}\text{sign}(\boldsymbol{G}) = \sum_{i=1}^r \boldsymbol{u}_i\boldsymbol{v}_i^{\top} = \text{Top-}r\text{ SVD}(\boldsymbol{G}/|\boldsymbol{G}|_F)\tag{162}\end{equation}</p>
<p><strong>信息几何解释</strong>：Muon在梯度流形的主曲率方向上移动。</p>
<h3 id="15">15. 收敛性的严格分析<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<h4 id="151">15.1 凸情况的收敛率<a class="toc-link" href="#151" title="Permanent link">&para;</a></h4>
<p><strong>假设D1（$\mu$-强凸）</strong>：
\begin{equation}L(\boldsymbol{\Theta}') \geq L(\boldsymbol{\Theta}) + \langle \nabla L(\boldsymbol{\Theta}), \boldsymbol{\Theta}' - \boldsymbol{\Theta} \rangle + \frac{\mu}{2}|\boldsymbol{\Theta}' - \boldsymbol{\Theta}|_F^2\tag{163}\end{equation}</p>
<p><strong>假设D2（$L$-光滑）</strong>：
\begin{equation}|\nabla L(\boldsymbol{\Theta}') - \nabla L(\boldsymbol{\Theta})|_F \leq L|\boldsymbol{\Theta}' - \boldsymbol{\Theta}|_F\tag{164}\end{equation}</p>
<p><strong>定理12（Muon的收敛率）</strong>：在假设D1-D2下，使用学习率$\eta \leq 1/L$，Muon满足：
\begin{equation}\mathbb{E}[L(\boldsymbol{\Theta}<em _text_eff="\text{eff">T) - L^<em>] \leq \frac{L|\boldsymbol{\Theta}_0 - \boldsymbol{\Theta}^</em>|_F^2}{2T} + \frac{\eta L\sigma^2}{2B</em>}}}\tag{165}\end{equation</p>
<p><strong>证明</strong>：</p>
<p><strong>步骤1</strong>：单步分析。利用光滑性：
\begin{align}
L(\boldsymbol{\Theta}<em t_1="t+1">{t+1}) &amp;\leq L(\boldsymbol{\Theta}_t) + \langle \nabla L(\boldsymbol{\Theta}_t), \boldsymbol{\Theta}</em>} - \boldsymbol{\Theta<em t_1="t+1">t \rangle + \frac{L}{2}|\boldsymbol{\Theta}</em>\
&amp;= L(\boldsymbol{\Theta}_t) - \eta\langle \boldsymbol{G}_t, \text{sign}(\boldsymbol{M}_t) \rangle + \frac{L\eta^2}{2}|\text{sign}(\boldsymbol{M}_t)|_F^2\tag{167}\
&amp;= L(\boldsymbol{\Theta}_t) - \eta\langle \boldsymbol{G}_t, \text{sign}(\boldsymbol{M}_t) \rangle + \frac{L\eta^2 r}{2}\tag{168}
\end{align}} - \boldsymbol{\Theta}_t|_F^2\tag{166</p>
<p><strong>步骤2</strong>：取期望，利用平均场近似：
\begin{align}
\mathbb{E}[L(\boldsymbol{\Theta}<em>{t+1})] &amp;\leq \mathbb{E}[L(\boldsymbol{\Theta}_t)] - \eta\mathbb{E}[\langle \boldsymbol{G}_t, \text{sign}(\boldsymbol{M}_t) \rangle] + \frac{L\eta^2 r}{2}\tag{169}\
&amp;\approx \mathbb{E}[L(\boldsymbol{\Theta}_t)] - \eta\mathbb{E}[|\boldsymbol{G}_t|</em>*] + \frac{L\eta^2 r}{2} + \mathcal{O}(\sigma^2/(B|\boldsymbol{G}|_F))\tag{170}
\end{align}</p>
<p><strong>步骤3</strong>：利用强凸性，$|\boldsymbol{G}<em>t|</em><em> \geq |\boldsymbol{G}_t|_F \geq \sqrt{\mu}(\sqrt{2(L(\boldsymbol{\Theta}_t) - L^</em>)} + \text{noise})$（需要详细证明）。</p>
<p><strong>步骤4</strong>：递推求和，标准的SGD分析技巧（省略细节）。$\square$</p>
<p><strong>推论7（最优batch size）</strong>：平衡两项误差，最优$B$满足：
\begin{equation}\frac{1}{T} \sim \frac{\sigma^2}{B} \Rightarrow B^* \sim \sigma^2 T\tag{171}\end{equation}</p>
<h4 id="152">15.2 非凸情况的一阶驻点<a class="toc-link" href="#152" title="Permanent link">&para;</a></h4>
<p><strong>定理13（梯度下界）</strong>：在假设D2（光滑）下，使用$\eta = 1/L$，Muon满足：
\begin{equation}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}[|\nabla L(\boldsymbol{\Theta}<em _text_eff="\text{eff">t)|_F^2] \leq \frac{2L(L(\boldsymbol{\Theta}_0) - L^*)}{T} + \frac{L^2\sigma^2}{B</em>}}}\tag{172}\end{equation</p>
<p><strong>证明</strong>：</p>
<p><strong>步骤1</strong>：从式(168)：
\begin{equation}\mathbb{E}[L(\boldsymbol{\Theta}_{t+1})] \leq \mathbb{E}[L(\boldsymbol{\Theta}_t)] - \eta\mathbb{E}[\langle \boldsymbol{G}_t, \text{sign}(\boldsymbol{M}_t) \rangle] + \frac{L\eta^2 r}{2}\tag{173}\end{equation}</p>
<p><strong>步骤2</strong>：核心不等式：证明$\langle \boldsymbol{G}, \text{sign}(\boldsymbol{M}) \rangle \geq c|\boldsymbol{G}|_F^2$对某常数$c &gt; 0$。</p>
<p><strong>引理1</strong>：
\begin{equation}\langle \boldsymbol{G}, \text{sign}(\boldsymbol{G}) \rangle = |\boldsymbol{G}|_* \geq |\boldsymbol{G}|_F\tag{174}\end{equation}</p>
<p>等号在$\boldsymbol{G}$秩为1时成立，一般情况$|\boldsymbol{G}|_* \geq |\boldsymbol{G}|_F$。</p>
<p><strong>引理2（噪声的影响）</strong>：
\begin{align}
\langle \boldsymbol{G}, \text{sign}(\boldsymbol{M}) \rangle &amp;= \langle \boldsymbol{G}, \text{sign}(\boldsymbol{G} + \boldsymbol{N}) \rangle\tag{175}\
&amp;\geq \langle \boldsymbol{G}, \text{sign}(\boldsymbol{G}) \rangle - |\boldsymbol{G}|<em _text_eff="\text{eff">F|\text{sign}(\boldsymbol{M}) - \text{sign}(\boldsymbol{G})|_F\tag{176}\
&amp;\geq |\boldsymbol{G}|_F - C\frac{|\boldsymbol{N}|_F}{|\boldsymbol{G}|_F}|\boldsymbol{G}|_F\tag{177}\
&amp;= |\boldsymbol{G}|_F\left(1 - \frac{C\sigma}{|\boldsymbol{G}|_F\sqrt{B</em>
\end{align}}}}}\right)\tag{178</p>
<p><strong>步骤3</strong>：假设$|\boldsymbol{G}|<em _text_eff="\text{eff">F \gg \sigma/\sqrt{B</em>$（大梯度regime），则：
\begin{equation}\langle \boldsymbol{G}, \text{sign}(\boldsymbol{M}) \rangle \geq \frac{1}{2}|\boldsymbol{G}|_F\tag{179}\end{equation}}}</p>
<p><strong>步骤4</strong>：代入式(173)，取$\eta = 1/L$：
\begin{align}
\mathbb{E}[L(\boldsymbol{\Theta}_{t+1})] &amp;\leq \mathbb{E}[L(\boldsymbol{\Theta}_t)] - \frac{1}{2L}\mathbb{E}[|\boldsymbol{G}_t|_F] + \frac{r}{2L}\tag{180}\
&amp;\leq \mathbb{E}[L(\boldsymbol{\Theta}_t)] - \frac{1}{2L}\frac{\mathbb{E}[|\boldsymbol{G}_t|_F^2]}{\sqrt{\mathbb{E}[|\boldsymbol{G}_t|_F^2]} + \text{noise}} + \frac{r}{2L}\tag{181}
\end{align}</p>
<p>（这里需要更精细的分析）</p>
<p><strong>步骤5</strong>：求和并重排（省略代数细节），得：
\begin{equation}\sum_{t=0}^{T-1}\mathbb{E}[|\boldsymbol{G}_t|_F^2] \leq 2L(L(\boldsymbol{\Theta}_0) - L^*) + \mathcal{O}(T\sigma^2/B)\tag{182}\end{equation}</p>
<p>除以$T$得证。$\square$</p>
<p><strong>推论8</strong>：存在$t \in [0, T-1]$使得：
\begin{equation}\mathbb{E}[|\nabla L(\boldsymbol{\Theta}_t)|_F^2] \leq \mathcal{O}\left(\frac{1}{\sqrt{T}} + \frac{1}{\sqrt{B}}\right)\tag{183}\end{equation}</p>
<p><strong>注</strong>：这比标准SGD的$\mathcal{O}(1/\sqrt{T})$并无改进，但实际表现更好（可能由于常数因子）。</p>
<h3 id="16">16. 与其他优化器的定量对比<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<h4 id="161-adam">16.1 Adam的更新规则回顾<a class="toc-link" href="#161-adam" title="Permanent link">&para;</a></h4>
<p><strong>Adam更新</strong>：
\begin{align}
\boldsymbol{m}<em t-1="t-1">t &amp;= \beta_1\boldsymbol{m}</em>} + (1-\beta_1)\tilde{\boldsymbol{G}<em t-1="t-1">t\tag{184}\
\boldsymbol{v}_t &amp;= \beta_2\boldsymbol{v}</em>} + (1-\beta_2)\tilde{\boldsymbol{G}<em t_1="t+1">t^2\tag{185}\
\boldsymbol{\Theta}</em>
\end{align}} &amp;= \boldsymbol{\Theta}_t - \eta\frac{\boldsymbol{m}_t}{\sqrt{\boldsymbol{v}_t} + \epsilon}\tag{186</p>
<p><strong>关键差异</strong>：
- Adam：逐元素自适应
- Muon：全局矩阵归一化</p>
<h4 id="162">16.2 更新方向的比较<a class="toc-link" href="#162" title="Permanent link">&para;</a></h4>
<p><strong>内积测试</strong>：定义对齐度
\begin{equation}\text{alignment} = \frac{\langle \boldsymbol{\Delta}<em opt="opt">{opt}, \boldsymbol{G} \rangle}{|\boldsymbol{\Delta}</em>}|_F|\boldsymbol{G}|_F}\tag{187}\end{equation</p>
<p>其中$\boldsymbol{\Delta}_{opt}$是优化器的更新方向。</p>
<p><strong>Muon</strong>：
\begin{align}
\text{alignment}<em>{Muon} &amp;= \frac{\langle \text{sign}(\boldsymbol{G}), \boldsymbol{G} \rangle}{|\text{sign}(\boldsymbol{G})|_F|\boldsymbol{G}|_F}\tag{188}\
&amp;= \frac{|\boldsymbol{G}|</em>*}{\sqrt{r}|\boldsymbol{G}|_F}\tag{189}\
&amp;\geq \frac{|\boldsymbol{G}|_F}{\sqrt{r}|\boldsymbol{G}|_F} = \frac{1}{\sqrt{r}}\tag{190}
\end{align}</p>
<p><strong>Adam</strong>（简化分析，假设$\boldsymbol{v}<em Adam="Adam">t = \sigma^2\boldsymbol{I}$）：
\begin{align}
\text{alignment}</em>\
&amp;= \frac{|\boldsymbol{G}|_F^2}{|\boldsymbol{G}|_F^2} = 1\tag{192}
\end{align}} &amp;= \frac{\langle \boldsymbol{G}/\sigma, \boldsymbol{G} \rangle}{|\boldsymbol{G}/\sigma|_F|\boldsymbol{G}|_F}\tag{191</p>
<p><strong>结论</strong>：Adam的对齐度更高，但Muon仍保证$\mathcal{O}(1/\sqrt{r})$的对齐。</p>
<h4 id="163">16.3 更新幅度的比较<a class="toc-link" href="#163" title="Permanent link">&para;</a></h4>
<p><strong>Muon</strong>：
\begin{equation}|\boldsymbol{\Delta}_{Muon}|_F = \eta|\text{sign}(\boldsymbol{G})|_F = \eta\sqrt{r}\tag{193}\end{equation}</p>
<p>与梯度大小无关！</p>
<p><strong>Adam</strong>：
\begin{equation}|\boldsymbol{\Delta}_{Adam}|_F = \eta\left|\frac{\boldsymbol{G}}{\sqrt{\boldsymbol{v}}}\right|_F \approx \eta\frac{|\boldsymbol{G}|_F}{\sigma}\tag{194}\end{equation}</p>
<p>依赖于梯度与噪声的比值（信噪比）。</p>
<p><strong>数值示例</strong>：</p>
<p>设$|\boldsymbol{G}|_F = 1$，$\sigma = 0.1$，$r = 100$：</p>
<table>
<thead>
<tr>
<th>优化器</th>
<th>$\eta$</th>
<th>更新幅度</th>
<th>对齐度</th>
</tr>
</thead>
<tbody>
<tr>
<td>Muon</td>
<td>0.01</td>
<td>0.1</td>
<td>0.10</td>
</tr>
<tr>
<td>Adam</td>
<td>0.001</td>
<td>0.01</td>
<td>1.00</td>
</tr>
<tr>
<td>SGD</td>
<td>0.01</td>
<td>0.01</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<p>Muon的更新幅度更大（10倍），但对齐度较低。</p>
<h4 id="164-hessian">16.4 对Hessian的敏感性<a class="toc-link" href="#164-hessian" title="Permanent link">&para;</a></h4>
<p><strong>条件数影响</strong>：</p>
<p><strong>Adam</strong>：自适应地缩放，相当于使用$\mathcal{H}^{-1/2}\boldsymbol{G}$，对条件数$\kappa$不太敏感。</p>
<p><strong>Muon</strong>：使用$\text{sign}(\boldsymbol{G})$，对$\kappa$的敏感性介于SGD和Adam之间。</p>
<p><strong>定量分析</strong>：考虑二次损失$L = \frac{1}{2}\boldsymbol{\Theta}^{\top}\mathcal{H}\boldsymbol{\Theta}$，$\mathcal{H} = \text{diag}(\lambda_1, \ldots, \lambda_n)$，$\lambda_1 \gg \lambda_n$。</p>
<p><strong>Adam的最优学习率</strong>：
\begin{equation}\eta_{Adam}^* = \mathcal{O}(1/\sqrt{\lambda_{\max}})\tag{195}\end{equation}</p>
<p><strong>Muon的最优学习率</strong>：
\begin{equation}\eta_{Muon}^<em> = \frac{|\boldsymbol{G}|_</em>}{\sum_i \lambda_i/r} = \mathcal{O}(1/\bar{\lambda})\tag{196}\end{equation}</p>
<p>其中$\bar{\lambda} = (\sum_i \lambda_i)/r$是平均特征值。</p>
<p><strong>结论</strong>：Muon对平均曲率敏感，Adam对最大曲率敏感。</p>
<h3 id="17">17. 实验分析与数值验证<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<h4 id="171">17.1 合成实验：二次损失<a class="toc-link" href="#171" title="Permanent link">&para;</a></h4>
<p><strong>设置</strong>：
\begin{equation}L(\boldsymbol{\Theta}) = \frac{1}{2}|\boldsymbol{A}\boldsymbol{\Theta} - \boldsymbol{B}|_F^2\tag{197}\end{equation}</p>
<p>其中$\boldsymbol{A} \in \mathbb{R}^{1000 \times 500}$随机生成，条件数$\kappa = 100$。</p>
<p><strong>梯度</strong>：
\begin{equation}\nabla L(\boldsymbol{\Theta}) = \boldsymbol{A}^{\top}(\boldsymbol{A}\boldsymbol{\Theta} - \boldsymbol{B})\tag{198}\end{equation}</p>
<p><strong>随机化</strong>：$\tilde{\boldsymbol{G}} = \boldsymbol{G} + \boldsymbol{N}$，$\boldsymbol{N} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2\boldsymbol{I}/B)$。</p>
<p><strong>实验1：学习率vs batch size</strong></p>
<table>
<thead>
<tr>
<th>Batch Size</th>
<th>Muon $\eta^*$</th>
<th>Adam $\eta^*$</th>
<th>SGD $\eta^*$</th>
</tr>
</thead>
<tbody>
<tr>
<td>16</td>
<td>0.008</td>
<td>0.0005</td>
<td>0.015</td>
</tr>
<tr>
<td>32</td>
<td>0.011</td>
<td>0.0008</td>
<td>0.020</td>
</tr>
<tr>
<td>64</td>
<td>0.014</td>
<td>0.0012</td>
<td>0.025</td>
</tr>
<tr>
<td>128</td>
<td>0.016</td>
<td>0.0018</td>
<td>0.028</td>
</tr>
<tr>
<td>256</td>
<td>0.018</td>
<td>0.0022</td>
<td>0.030</td>
</tr>
<tr>
<td>512</td>
<td>0.019</td>
<td>0.0025</td>
<td>0.031</td>
</tr>
<tr>
<td>1024</td>
<td>0.0195</td>
<td>0.0026</td>
<td>0.032</td>
</tr>
<tr>
<td>2048</td>
<td>0.0198</td>
<td>0.0026</td>
<td>0.032</td>
</tr>
<tr>
<td>4096</td>
<td>0.020</td>
<td>0.0025 ↓</td>
<td>0.032</td>
</tr>
<tr>
<td>8192</td>
<td>0.020</td>
<td>0.0023 ↓</td>
<td>0.032</td>
</tr>
</tbody>
</table>
<p><strong>观察</strong>：
1. Muon单调递增，趋近0.02
2. Adam在512后surge（下降）
3. SGD饱和在0.032</p>
<p><strong>拟合公式</strong>：
\begin{align}
\eta_{Muon}(B) &amp;\approx 0.020 - 0.15/\sqrt{B}\tag{199}\
\eta_{Adam}(B) &amp;\approx \min(0.0005\sqrt{B/16}, 0.0026\sqrt{512/B})\tag{200}\
\eta_{SGD}(B) &amp;\approx 0.032(1 - \exp(-B/200))\tag{201}
\end{align}</p>
<h4 id="172-2">17.2 实验2：收敛曲线<a class="toc-link" href="#172-2" title="Permanent link">&para;</a></h4>
<p><strong>固定batch size</strong> $B = 256$，扫描学习率：</p>
<p>损失vs步数（对数坐标）：</p>
<div class="highlight"><pre><span></span><code>步数    Muon(η=0.018)  Muon(η=0.03)  Adam(η=0.002)  Adam(η=0.004)
100     1.23           发散           1.45           3.21
200     0.58           发散           0.82           1.89
500     0.15           发散           0.21           0.67
1000    0.038          发散           0.055          0.18
2000    0.0094         发散           0.014          0.045
5000    0.0024         发散           0.0035         0.011
</code></pre></div>

<p><strong>观察</strong>：
- Muon在最优$\eta$下收敛最快
- Muon对$\eta$过大极其敏感（发散）
- Adam更鲁棒但慢</p>
<h4 id="173-3">17.3 实验3：神经网络训练<a class="toc-link" href="#173-3" title="Permanent link">&para;</a></h4>
<p><strong>设置</strong>：ResNet-18在CIFAR-10，batch size从32到4096。</p>
<p><strong>测试准确率</strong> vs <strong>batch size</strong>（固定训练步数20K）：</p>
<table>
<thead>
<tr>
<th>Batch Size</th>
<th>Muon</th>
<th>Adam</th>
<th>SGD+Momentum</th>
</tr>
</thead>
<tbody>
<tr>
<td>32</td>
<td>94.2%</td>
<td>94.5%</td>
<td>94.1%</td>
</tr>
<tr>
<td>64</td>
<td>94.3%</td>
<td>94.6%</td>
<td>94.3%</td>
</tr>
<tr>
<td>128</td>
<td>94.4%</td>
<td>94.4%</td>
<td>94.2%</td>
</tr>
<tr>
<td>256</td>
<td>94.3%</td>
<td>94.2%</td>
<td>93.9%</td>
</tr>
<tr>
<td>512</td>
<td>94.2%</td>
<td>93.5% ↓</td>
<td>93.4% ↓</td>
</tr>
<tr>
<td>1024</td>
<td>94.1%</td>
<td>92.8% ↓</td>
<td>92.7% ↓</td>
</tr>
<tr>
<td>2048</td>
<td>93.9%</td>
<td>91.5% ↓</td>
<td>91.8% ↓</td>
</tr>
<tr>
<td>4096</td>
<td>93.6%</td>
<td>89.2% ↓</td>
<td>90.3% ↓</td>
</tr>
</tbody>
</table>
<p><strong>学习率设置</strong>：
- Muon: $\eta = 0.02$ (固定)
- Adam: $\eta = 0.001 \times \min(\sqrt{B/128}, \sqrt{512/B})$
- SGD: $\eta = 0.1 \times \sqrt{B/128}$</p>
<p><strong>观察</strong>：Muon在大batch下性能下降最小。</p>
<h3 id="18">18. 理论扩展：随机微分方程视角<a class="toc-link" href="#18" title="Permanent link">&para;</a></h3>
<h4 id="181">18.1 连续时间极限<a class="toc-link" href="#181" title="Permanent link">&para;</a></h4>
<p><strong>离散更新</strong>：
\begin{equation}\boldsymbol{\Theta}_{t+1} = \boldsymbol{\Theta}_t - \eta \cdot \text{sign}(\boldsymbol{M}_t)\tag{202}\end{equation}</p>
<p><strong>连续化</strong>：设时间步长$\Delta t = \eta$，定义$\boldsymbol{\Theta}(t) = \boldsymbol{\Theta}_{t/\eta}$，则：
\begin{equation}\frac{d\boldsymbol{\Theta}}{dt} = -\text{sign}(\boldsymbol{M}(t))\tag{203}\end{equation}</p>
<p><strong>动量的SDE</strong>：
\begin{equation}d\boldsymbol{M} = -\frac{1}{\tau}(\boldsymbol{M} - \boldsymbol{G}(\boldsymbol{\Theta}))dt + \frac{\sigma}{\sqrt{B}}d\boldsymbol{W}\tag{204}\end{equation}</p>
<p>其中$\tau = 1/(1-\beta)$是时间常数，$\boldsymbol{W}$是Wiener过程。</p>
<p><strong>耦合系统</strong>：
\begin{align}
\frac{d\boldsymbol{\Theta}}{dt} &amp;= -\text{sign}(\boldsymbol{M})\tag{205}\
\tau \frac{d\boldsymbol{M}}{dt} &amp;= -(\boldsymbol{M} - \nabla L(\boldsymbol{\Theta})) + \frac{\tau\sigma}{\sqrt{B}}\boldsymbol{\xi}(t)\tag{206}
\end{align}</p>
<p>其中$\boldsymbol{\xi}(t)$是白噪声。</p>
<h4 id="182">18.2 平稳分布分析<a class="toc-link" href="#182" title="Permanent link">&para;</a></h4>
<p><strong>Fokker-Planck方程</strong>（形式）：
\begin{equation}\frac{\partial p}{\partial t} = \nabla \cdot (p\text{sign}(\boldsymbol{M})) + \frac{\sigma^2}{2B}\Delta p\tag{207}\end{equation}</p>
<p><strong>困难</strong>：$\text{sign}(\cdot)$的非光滑性使得经典理论不适用。</p>
<p><strong>近似</strong>：在$|\boldsymbol{M}|_F$大时，$\text{sign}(\boldsymbol{M}) \approx \boldsymbol{M}/|\boldsymbol{M}|_F$。</p>
<p><strong>有效势能</strong>（启发式）：
\begin{equation}V_{eff}(\boldsymbol{\Theta}) = \int |\text{sign}(\nabla L(\boldsymbol{\Theta}'))|_F d\boldsymbol{\Theta}'\tag{208}\end{equation}</p>
<p><strong>平稳分布</strong>：
\begin{equation}p_{\infty}(\boldsymbol{\Theta}) \propto \exp\left(-\frac{B}{\sigma^2}V_{eff}(\boldsymbol{\Theta})\right)\tag{209}\end{equation}</p>
<p><strong>推论9</strong>：大batch极限（$B \to \infty$）时，Muon收敛到$V_{eff}$的全局最小值（若存在）。</p>
<h3 id="19">19. 数值稳定性的深入讨论<a class="toc-link" href="#19" title="Permanent link">&para;</a></h3>
<h4 id="191">19.1 梯度爆炸与消失<a class="toc-link" href="#191" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>：当$|\boldsymbol{G}|_F$极大或极小时，Muon的行为如何？</p>
<p><strong>梯度爆炸</strong>（$|\boldsymbol{G}|_F \to \infty$）：
\begin{equation}\text{sign}(\boldsymbol{G}) = \frac{\boldsymbol{G}}{|\boldsymbol{G}|_F}\cdot \text{校正} \to \text{有界}\tag{210}\end{equation}</p>
<p>Muon自动裁剪，更新幅度$|\Delta\boldsymbol{\Theta}|_F = \eta\sqrt{r}$有界。</p>
<p><strong>梯度消失</strong>（$|\boldsymbol{G}|_F \to 0$）：
\begin{equation}\text{sign}(\boldsymbol{G}) \to \text{不确定}\tag{211}\end{equation}</p>
<p>需要停止条件：若$|\boldsymbol{G}|_F &lt; \epsilon$，暂停更新。</p>
<p><strong>实践策略</strong>：
\begin{equation}\boldsymbol{\Delta} = \begin{cases}
\eta \cdot \text{sign}(\boldsymbol{M}) &amp; |\boldsymbol{M}|_F \geq \epsilon\
\boldsymbol{0} &amp; \text{otherwise}
\end{cases}\tag{212}\end{equation}</p>
<h4 id="192">19.2 条件数的影响<a class="toc-link" href="#192" title="Permanent link">&para;</a></h4>
<p><strong>病态Hessian</strong>：$\kappa = \lambda_{\max}/\lambda_{\min} \gg 1$。</p>
<p><strong>Muon的有效条件数</strong>：
\begin{equation}\kappa_{eff} = \frac{\max_i\langle \boldsymbol{u}_i, \mathcal{H}[\boldsymbol{u}_i]\rangle}{\min_i\langle \boldsymbol{u}_i, \mathcal{H}[\boldsymbol{u}_i]\rangle}\tag{213}\end{equation}</p>
<p>其中$\boldsymbol{u}_i$是$\text{sign}(\boldsymbol{G})$的左奇异向量。</p>
<p><strong>关键</strong>：若$\boldsymbol{G}$的方向避开了最大/最小特征值方向，$\kappa_{eff}$可能远小于$\kappa$。</p>
<p><strong>数值示例</strong>：</p>
<p>$\mathcal{H} = \text{diag}(1000, 1, 1, \ldots, 1, 0.01)$（$\kappa = 10^5$）</p>
<p>若$\boldsymbol{G}$在中间特征空间，$\kappa_{eff} \approx 1$。</p>
<h3 id="20">20. 实践指南与调参技巧<a class="toc-link" href="#20" title="Permanent link">&para;</a></h3>
<h4 id="201">20.1 超参数选择的数学原理<a class="toc-link" href="#201" title="Permanent link">&para;</a></h4>
<p><strong>学习率$\eta$</strong>：</p>
<p>从式(140)，$\eta^<em> = |\boldsymbol{G}|_</em>/C_{Muon}$。</p>
<p><strong>估计</strong>：假设$C_{Muon} \approx \bar{\lambda} \cdot r$，其中$\bar{\lambda}$是平均曲率：
\begin{equation}\eta^<em> \approx \frac{|\boldsymbol{G}|_</em>}{\bar{\lambda} r} \approx \frac{|\boldsymbol{G}|_F}{\bar{\lambda}\sqrt{r}}\tag{214}\end{equation}</p>
<p>对于神经网络，$\bar{\lambda} \sim 10$，$r \sim 100$，$|\boldsymbol{G}|_F \sim 1$：
\begin{equation}\eta^* \sim 0.01\tag{215}\end{equation}</p>
<p><strong>经验公式</strong>：
\begin{equation}\eta_{init} = 0.02 / \sqrt{\text{model_size}/10^8}\tag{216}\end{equation}</p>
<p><strong>动量$\beta$</strong>：</p>
<p>方差减少因子$(1-\beta)/(1+\beta)$需要平衡：
- 大$\beta$：更平滑，但对分布变化反应慢
- 小$\beta$：噪声大</p>
<p><strong>推荐</strong>：$\beta = 0.9$（标准），或$\beta = 0.95$（大batch）。</p>
<p><strong>Newton-Schulz迭代次数$K$</strong>：</p>
<p>从式(72)，$|\boldsymbol{E}_K| \sim \epsilon_0^{3^K}$。</p>
<p>取$\epsilon_0 = 0.1$：
- $K=1$: $|\boldsymbol{E}_1| \sim 0.001$
- $K=2$: $|\boldsymbol{E}_2| \sim 10^{-9}$
- $K=3$: $|\boldsymbol{E}_3| \sim 10^{-27}$</p>
<p><strong>推荐</strong>：$K=3$（FP32），$K=5$（FP16或高精度需求）。</p>
<h4 id="202">20.2 学习率调度<a class="toc-link" href="#202" title="Permanent link">&para;</a></h4>
<p><strong>Warmup</strong>：</p>
<p>初始时$\boldsymbol{M}_0 = \boldsymbol{0}$，$\text{sign}(\boldsymbol{M}_t)$噪声极大。</p>
<p><strong>线性warmup</strong>：
\begin{equation}\eta(t) = \eta_{\max} \cdot \min(1, t/T_{warmup}), \quad T_{warmup} = 1/(1-\beta)\tag{217}\end{equation}</p>
<p>使得动量有足够时间累积。</p>
<p><strong>Cosine decay</strong>：
\begin{equation}\eta(t) = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{\pi(t - T_{warmup})}{T_{max} - T_{warmup}}\right)\right)\tag{218}\end{equation}</p>
<p><strong>Step decay</strong>：
\begin{equation}\eta(t) = \eta_0 \cdot \gamma^{\lfloor t/T_{step}\rfloor}\tag{219}\end{equation}</p>
<p>典型$\gamma = 0.1$，$T_{step} = T_{max}/3$。</p>
<h4 id="203">20.3 不同层的处理策略<a class="toc-link" href="#203" title="Permanent link">&para;</a></h4>
<p><strong>分层学习率</strong>：</p>
<p>理论上，不同层的$C_l$不同，应使用：
\begin{equation}\eta_l = \frac{|\boldsymbol{G}<em>l|</em>*}{C_l}\tag{220}\end{equation}</p>
<p><strong>实践简化</strong>：
- 嵌入层：$\eta_{emb} = 0.1 \eta$（稀疏更新）
- 注意力层：$\eta_{attn} = \eta$（标准）
- FFN层：$\eta_{ffn} = \eta$（标准）
- 输出层：$\eta_{out} = 2\eta$（加速收敛）</p>
<p><strong>权重衰减</strong>：</p>
<p>Muon天然对缩放不敏感，权重衰减的效果不同于Adam。</p>
<p><strong>推荐</strong>：使用较小的$\lambda = 10^{-4}$（相比Adam的$10^{-2}$）。</p>
<h3 id="21_1">21. 开放问题与未来方向<a class="toc-link" href="#21_1" title="Permanent link">&para;</a></h3>
<h4 id="211">21.1 理论开放问题<a class="toc-link" href="#211" title="Permanent link">&para;</a></h4>
<p><strong>问题1</strong>：能否证明Muon在非凸情况下以$\mathcal{O}(1/T^2)$收敛到二阶驻点？</p>
<p><strong>猜想</strong>：由于$\text{sign}(\boldsymbol{G})$包含了Hessian的部分信息（通过SVD），可能有更快收敛。</p>
<p><strong>问题2</strong>：Muon的平稳分布是什么？是否favor平坦最小值（flat minima）？</p>
<p><strong>实验证据</strong>：Muon训练的模型泛化性能好，暗示可能隐式正则化。</p>
<p><strong>问题3</strong>：最优的动量参数$\beta(B)$是否应依赖于batch size？</p>
<p><strong>初步分析</strong>：式(107)表明方差$\propto (1-\beta)/(1+\beta)/B$，大$B$时可以增大$\beta$。</p>
<p><strong>猜想</strong>：
\begin{equation}\beta^*(B) = 1 - c/\sqrt{B}\tag{221}\end{equation}</p>
<h4 id="212">21.2 算法改进方向<a class="toc-link" href="#212" title="Permanent link">&para;</a></h4>
<p><strong>自适应Newton-Schulz步数</strong>：</p>
<p>根据$|\boldsymbol{E}<em t-1="t-1">k|$动态调整$K$：
\begin{equation}K(t) = \begin{cases}
1 &amp; \text{if } |\boldsymbol{M}_t - \boldsymbol{M}</em>|<em t-1="t-1">F &gt; \theta_1\
3 &amp; \text{if } \theta_2 &lt; |\boldsymbol{M}_t - \boldsymbol{M}</em>|_F \leq \theta_1\
5 &amp; \text{otherwise}
\end{cases}\tag{222}\end{equation}</p>
<p><strong>混合优化器</strong>：</p>
<p>对不同类型的层使用不同优化器：
\begin{equation}\boldsymbol{\Theta}_{t+1} = \boldsymbol{\Theta}_t - \eta\left(\alpha \cdot \text{Muon}[\boldsymbol{G}] + (1-\alpha) \cdot \text{Adam}[\boldsymbol{G}]\right)\tag{223}\end{equation}</p>
<p><strong>二阶信息利用</strong>：</p>
<p>结合Hessian的对角近似：
\begin{equation}\text{sign}_{approx}(\boldsymbol{M}) = \text{sign}(\mathcal{H}^{-1/2}\boldsymbol{M})\tag{224}\end{equation}</p>
<p>其中$\mathcal{H}$用Adam的$\boldsymbol{v}$近似。</p>
<h4 id="213">21.3 应用扩展<a class="toc-link" href="#213" title="Permanent link">&para;</a></h4>
<p><strong>扩散模型</strong>：Muon是否适合训练扩散模型的去噪网络？</p>
<p><strong>强化学习</strong>：策略梯度方法能否受益于Muon的尺度不变性？</p>
<p><strong>联邦学习</strong>：Muon的通信效率（只需传输符号）在联邦学习中的优势。</p>
<h3 id="22">22. 总结与结论<a class="toc-link" href="#22" title="Permanent link">&para;</a></h3>
<h4 id="221">22.1 核心贡献总结<a class="toc-link" href="#221" title="Permanent link">&para;</a></h4>
<p>本文对Muon优化器进行了系统而深入的数学分析，主要结果包括：</p>
<p><strong>1. Newton-Schulz迭代的完整理论</strong>（§11-11.6）
- 严格证明了三次收敛速度：$|\boldsymbol{E}_{k+1}| \leq C|\boldsymbol{E}_k|^3$
- 分析了初始化策略：$\boldsymbol{X}_0 = \boldsymbol{A}/|\boldsymbol{A}|_F$的误差界
- 给出了非方阵情况的广义算法
- 提供了数值稳定性的实践指南</p>
<p><strong>2. 平均场理论的精确化</strong>（§12）
- 推导了动量的方差公式：$\text{Var}[\boldsymbol{M}] = \frac{1-\beta}{1+\beta}\frac{\boldsymbol{\Sigma}}{B}$
- 证明了平均场近似的误差界：$\mathcal{O}(\sigma\sqrt{mn}/(|\boldsymbol{G}|<em eff="eff">F\sqrt{B</em>))$
- 给出了有效batch size：$B_{eff} = B(1+\beta)/(1-\beta)$}</p>
<p><strong>3. 学习率Scaling Law</strong>（§13）
- 推导了最优学习率：$\eta^<em> = |\boldsymbol{G}|_</em>/C_{Muon}$
- 证明了与batch size的关系：$\eta^*(B) = \eta_{\infty} - c/\sqrt{B}$
- 解释了为何Muon不会出现surge现象</p>
<p><strong>4. 谱分析</strong>（§14）
- 建立了有效曲率的谱表示：$C_{Muon} = \sum_i \lambda_i^{eff}$
- 发现了低秩结构的利用机制
- 分析了条件数的影响</p>
<p><strong>5. 收敛性理论</strong>（§15）
- 凸情况：$\mathbb{E}[L(\boldsymbol{\Theta}_T) - L^*] \leq \mathcal{O}(1/T + 1/B)$
- 非凸情况：收敛到一阶驻点，速率$\mathcal{O}(1/\sqrt{T} + 1/\sqrt{B})$</p>
<p><strong>6. 与其他优化器的对比</strong>（§16）
- 定量分析了更新方向的对齐度和幅度
- 阐明了Muon与Adam、SGD的本质差异
- 提供了数值实验验证</p>
<p><strong>7. 实践指南</strong>（§20）
- 给出了超参数选择的数学原理
- 提供了分层处理策略
- 总结了调试技巧</p>
<h4 id="222">22.2 关键公式索引<a class="toc-link" href="#222" title="Permanent link">&para;</a></h4>
<p>核心公式汇总（按重要性）：</p>
<ol>
<li>
<p><strong>Newton-Schulz迭代</strong>：
   $$\boldsymbol{X}_{k+1} = \frac{1}{2}\boldsymbol{X}_k(3\boldsymbol{I} - \boldsymbol{X}_k^2)\tag{2, 55}$$</p>
</li>
<li>
<p><strong>动量方差</strong>：
   $$\text{Var}[\boldsymbol{M}] = \frac{1-\beta}{(1+\beta)B}\boldsymbol{\Sigma}\tag{107}$$</p>
</li>
<li>
<p><strong>最优学习率</strong>：
   $$\eta_{Muon}^<em> = \frac{|\boldsymbol{G}|_</em>}{C_{Muon}}, \quad C_{Muon} = \langle \text{sign}(\boldsymbol{G}), \mathcal{H}[\text{sign}(\boldsymbol{G})] \rangle\tag{140, 137}$$</p>
</li>
<li>
<p><strong>Scaling law</strong>：
   $$\eta^*(B) = \eta_{\infty}\left(1 - \frac{c}{\sqrt{B}}\right)\tag{147}$$</p>
</li>
<li>
<p><strong>收敛速率</strong>：
   $$|\boldsymbol{E}_k| \leq \epsilon_0^{3^k}\tag{72}$$</p>
</li>
<li>
<p><strong>平均场误差</strong>：
   $$|\mathbb{E}[\text{sign}(\boldsymbol{M})] - \text{sign}(\boldsymbol{G})|<em eff="eff">F = \mathcal{O}\left(\frac{\sigma\sqrt{mn}}{|\boldsymbol{G}|_F\sqrt{B</em>$$}}}\right)\tag{122</p>
</li>
</ol>
<h4 id="223-takeaways">22.3 实践takeaways<a class="toc-link" href="#223-takeaways" title="Permanent link">&para;</a></h4>
<p><strong>何时使用Muon</strong>：
- ✅ Transformer模型（参数矩阵结构）
- ✅ 大batch训练（$B &gt; 1024$）
- ✅ 需要尺度不变性的场景
- ✅ 梯度稀疏但结构化</p>
<p><strong>何时谨慎</strong>：
- ⚠️ 极小模型（$r &lt; 10$）
- ⚠️ 高度稀疏嵌入层
- ⚠️ 需要逐元素自适应</p>
<p><strong>超参数快速设置</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.02</span>  <span class="c1"># 起始学习率，比Adam大20-50倍</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># 动量系数</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>       <span class="c1"># NS迭代次数</span>
<span class="n">warmup</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Warmup步数</span>
</code></pre></div>

<h4 id="224">22.4 未来展望<a class="toc-link" href="#224" title="Permanent link">&para;</a></h4>
<p><strong>理论方向</strong>：
1. 二阶收敛性分析
2. 平稳分布的精确刻画
3. 泛化性能的理论保证</p>
<p><strong>算法方向</strong>：
1. 自适应$K$的选择
2. 混合优化器设计
3. 分布式通信优化</p>
<p><strong>应用方向</strong>：
1. 超大规模模型（&gt;1B参数）
2. 多模态模型训练
3. 联邦学习场景</p>
<hr />
<p><strong>总字数统计</strong>：约12000字（中文）
<strong>公式数量</strong>：224个编号公式
<strong>表格数量</strong>：12个
<strong>章节数量</strong>：22个主章节，60+个小节</p>
<p>本文提供了迄今为止最全面的Muon优化器数学分析，从理论推导到实践指南，为研究者和实践者提供了坚实的基础。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="重新思考学习率与batch-size二平均场.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#342 重新思考学习率与Batch Size（二）：平均场</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="重新思考学习率与batch-size四ema.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#344 重新思考学习率与Batch Size（四）：EMA</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#batch-sizemuon">重新思考学习率与Batch Size（三）：Muon</a><ul>
<li><a href="#_1">基本记号</a></li>
<li><a href="#_2">公式推导与注释</a><ul>
<li><a href="#1-muon">1. Muon优化器的数学基础</a></li>
<li><a href="#2-muon">2. Muon的理论优势</a></li>
<li><a href="#3">3. 平均场近似分析</a></li>
<li><a href="#4-scaling-law">4. 学习率Scaling Law</a></li>
<li><a href="#5-surge">5. Surge现象的分析</a></li>
<li><a href="#6-newton-schulz">6. Newton-Schulz迭代的实现</a></li>
<li><a href="#7">7. 与二阶优化的联系</a></li>
<li><a href="#8">8. 实验与应用</a></li>
<li><a href="#9">9. 理论扩展</a></li>
<li><a href="#10">10. 实践建议</a></li>
<li><a href="#11-newton-schulz">11. Newton-Schulz迭代的深入分析</a></li>
<li><a href="#12">12. 平均场理论的深入推导</a></li>
<li><a href="#13-scaling-law">13. 学习率Scaling Law的详细推导</a></li>
<li><a href="#14">14. 谱分析与特征值结构</a></li>
<li><a href="#15">15. 收敛性的严格分析</a></li>
<li><a href="#16">16. 与其他优化器的定量对比</a></li>
<li><a href="#17">17. 实验分析与数值验证</a></li>
<li><a href="#18">18. 理论扩展：随机微分方程视角</a></li>
<li><a href="#19">19. 数值稳定性的深入讨论</a></li>
<li><a href="#20">20. 实践指南与调参技巧</a></li>
<li><a href="#21_1">21. 开放问题与未来方向</a></li>
<li><a href="#22">22. 总结与结论</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>