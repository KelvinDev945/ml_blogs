<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EAE：自编码器 + BN + 最大熵  = 生成模型 | ML & Math Blog Posts</title>
    <meta name="description" content="EAE：自编码器 + BN + 最大熵  = 生成模型&para;
原文链接: https://spaces.ac.cn/archives/7343
发布日期: 

生成模型一直是笔者比较关注的主题，不管是NLP和CV的生成模型都是如此。这篇文章里，我们介绍一个新颖的生成模型，来自论文《Batch norm with entropic regularization turns determinis...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=最大熵">最大熵</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #92 EAE：自编码器 + BN + 最大熵  = 生成模型
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#92</span>
                EAE：自编码器 + BN + 最大熵  = 生成模型
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/7343" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=最大熵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 最大熵</span>
                </a>
                
                <a href="../index.html?tags=熵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 熵</span>
                </a>
                
                <a href="../index.html?tags=无监督" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 无监督</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="eae-bn">EAE：自编码器 + BN + 最大熵  = 生成模型<a class="toc-link" href="#eae-bn" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/7343">https://spaces.ac.cn/archives/7343</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>生成模型一直是笔者比较关注的主题，不管是NLP和CV的生成模型都是如此。这篇文章里，我们介绍一个新颖的生成模型，来自论文<a href="https://papers.cool/arxiv/2002.10631">《Batch norm with entropic regularization turns deterministic autoencoders into generative models》</a>，论文中称之为<strong>EAE</strong> （Entropic AutoEncoder）。它要做的事情给变分自编码器（VAE）基本一致，最终效果其实也差不多（略优），说它新颖并不是它生成效果有多好，而是思路上的新奇，颇有别致感。此外，借着这个机会，我们还将学习一种统计量的估计方法——$k$邻近方法，这是一种很有用的非参数估计方法。</p>
<h2 id="vs">自编码器vs生成模型<a class="toc-link" href="#vs" title="Permanent link">&para;</a></h2>
<p>普通的自编码器是一个“编码-解码”的重构过程，如下图所示：  </p>
<p><a href="/usr/uploads/2020/04/2128638498.png" title="点击查看原图"><img alt="典型自编码器示意图" src="/usr/uploads/2020/04/2128638498.png" /></a></p>
<p>典型自编码器示意图</p>
<p>其loss一般为<br />
\begin{equation}L_{AE} = \mathbb{E}<em _tilde_p="\tilde{p" x_sim="x\sim">{x\sim \tilde{p}(x)}\left[\left\Vert x - \hat{x}\right\Vert^2\right] = \mathbb{E}</em>}(x)}\left[\left\Vert x - D(E(x))\right\Vert^2\right]\end{equation</p>
<p>当训练完成后，我们自然可以针对每一幅图像$x$，得到它的编码$z=E(x)$以及重构图$\hat{x}=D(z)$，而当$x$与$\hat{x}$足够接近时，我们就可以认为$z$是$x$的有效表征，它已经充分包含了$x$的信息。</p>
<p>那么，生成模型又是什么情况呢？“生成”指的是随机生成，也就是说允许我们能随机构建一幅图像来，对于自编码器的解码器$D(z)$，并不是每一个$z$解码出来的$D(z)$都是一幅有意义的图像，因此普通的自编码器并不能视为生成模型。如果我们能够事先知道所有的$x$编码出来的$z=E(x)$所构成的分布，并且这个分布是一个易于采样分布，那么就可以实现随机采样生成了。</p>
<p>所以，从自编码器到生成模型，缺的那一步就是确定隐变量$z$的分布，更准确来说，是迫使隐变量$z$服从一个易于采样的简单分布，比如标准正态分布。VAE通过引入KL散度项来达到这一点，那么EAE又是怎么实现的呢？</p>
<h2 id="_1">正态分布与最大熵<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>我们知道，最大熵原理是一个相当普适的原理，它代表着我们对未知事件的最客观认知。最大熵原理的一个结论是：</p>
<blockquote>
<p>在所有均值为0、方差为1的分布中，标准正态分布的熵最大。</p>
</blockquote>
<p>如果读者还不了解最大熵的相关内容，可以参考旧作<a href="/archives/3552">《“熵”不起：从熵、最大熵原理到最大熵模型（二）》</a>。上述结论告诉我们，如果我们能有某种手段保证隐变量的均值为0和方差为1，那么我们只需要同时最大化隐变量的熵，就可以得到“隐变量服从标准正态分布”这个目的了，即<br />
\begin{equation}\begin{aligned}&amp;L_{EAE} = \mathbb{E}<em p_z_="p(z)" z_sim="z\sim">{x\sim \tilde{p}(x)}\left[\left\Vert x - D(E(x))\right\Vert^2
\right] - \lambda H(Z)\\
&amp;\text{s.t.}\,\,\text{avg}(E(x))=0,\,\text{std}(E(x))=1
\end{aligned}\end{equation}<br />
其中$\lambda &gt; 0$是超参数，而<br />
\begin{equation}H(Z)=\mathbb{E}</em>}[-\log p(z)]\end{equation
是隐变量$z=E(x)$对应的熵，最小化$- \lambda H(Z)$意味着最大化$\lambda H(Z)$，即最大熵。</p>
<p>问题是如何保证这两个约束呢？如果计算隐变量的熵呢？</p>
<h2 id="bn">均值方差约束与BN<a class="toc-link" href="#bn" title="Permanent link">&para;</a></h2>
<p>先来解决第一个问题：如何达到——至少近似地达到——“隐变量的均值为0、方差为1”这个约束？因为只有满足这个约束的前提下，最大熵的分布才是标准正态的。解决这个问题的办法是我们熟悉的批归一化，也就是BN（Batch Normalization）。</p>
<p>在BN的训练阶段，我们会直接对每个变量减去其batch内的均值并且除以batch内标准差，这保证了训练阶段每个batch的变量均值确实为0，方差确实为1；然后，它会将每个batch内的均值方差滑动平均并缓存下来，用于推断阶段的预测。总而言之，就是将BN应用于隐变量，就可以使得隐变量（近似地）满足相应的均值方差约束。对了，要说明的是，本文说的BN层，不包括$\beta,\gamma$这两个可训练参数，如果在Keras中，则是要在初始化BN层时传入参数<code>scale=False, center=False</code>。</p>
<p>此时，我们就得到
\begin{equation}L_{EAE} = \mathbb{E}_{x\sim \tilde{p}(x)}\left[\left\Vert x - D(\mathcal{N}(E(x)))\right\Vert^2\right] - \lambda H(Z)\label{eq:eae}\end{equation}<br />
这里的$\mathcal{N}(\cdot)$代表BN层。</p>
<h2 id="_2">熵的采样邻近估计<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>现在，来到了整个EAE模型的最后一部分、同时也是最硬核的一部分了，也就是如何估计熵$H(Z)$。理论上来说，为了算$H(Z)$我们需要知道$p(z)$，但我们现在只有样本$z_1, z_2, \dots, z_n$而不知道$p(z)$的表达式，在这种前提下对$H(Z)$做的估计叫做非参数估计。</p>
<p>先给结论：</p>
<blockquote>
<p><strong>熵的最邻近估计</strong> 设$z_1,z_2,\dots,z_n\in\mathbb{R}^d$是从$p(z)$采样出来的$n$个样本，记$\varepsilon(i)$为$z_i$到它最邻近的样本的距离，即$\varepsilon(i) = \min\limits_{j\neq i} \Vert z_i - z_j \Vert$，$B_d$是$d$维单位球的体积，$\gamma=0.5772\dots$是<a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant">欧拉常数</a>，则 \begin{equation}H(Z)\approx \frac{d}{n}\sum_{i=1}^n \log \varepsilon(i) + \log B_d + \log (n - 1) + \gamma \label{eq:1nn}\end{equation}</p>
</blockquote>
<p>抛开跟优化不相关的常数，上述结论实际上就是说$H(Z)\sim \sum\limits_{i=1}^n \log \varepsilon(i)$，这就是我们需要添加到loss的项。</p>
<blockquote>
<p>这个看上去很奇怪、实际上确实也不容易理解的结果是怎么得来的呢？事实上，它是一种重要的估计方法——$k$邻近方法——的经典例子。下面将会给出它的推导过程，该过程参考自论文<a href="https://papers.cool/arxiv/1506.06501">《A non-parametric k-nearest neighbour entropy estimator》</a>。</p>
<p>让我们考虑特定的样本$z_i$，设$z_{i(k)}$是它的第$k$个最邻近的样本，即将所有的$z_j (j\neq i)$按照$\Vert z_j - z_i\Vert$从小到大排列，第$k$个就是$z_{i(k)}$，记$\varepsilon_k(i) = \left\Vert z_i - z_{i(k)}\right\Vert$，我们现在考虑$\varepsilon_k(i)$的概率分布。</p>
<p>假设$\varepsilon \leq \varepsilon_k(i) \leq \varepsilon + d\varepsilon$，那么就意味着剩下的$n-1$个样本之中，有$k-1$个落在了“以$z_i$为球心、以$\varepsilon$为半径”的球内，有$n-k-1$个落在了“以$z_i$为球心、以$\varepsilon+d\varepsilon$为半径”的球外，剩下一个夹在两球之间，不难得到这种情况发生的概率是<br />
 \begin{equation}\binom{n-1}{1}\binom{n-2}{k-1}P_i(\varepsilon)^{k-1}(1 - P_i(\varepsilon + d\varepsilon))^{n-k-1}(P_i(\varepsilon + d\varepsilon) - P_i(\varepsilon))\label{eq:dp-1}\end{equation}<br />
 其中$\binom{n-1}{1}$代表着从$n-1$个样本中挑出$1$个样本夹在两球之间的组合数，而$\binom{n-2}{k-1}$则是从剩下的$n-2$个样本中挑出$k-1$个样本放到球内的组合数（剩下的$n-k-1$个自动就在球外了）；$P_i(\varepsilon)$是单个样本位于球内的概率，即<br />
 \begin{equation}P_i(\varepsilon) = \int_{\Vert z - z_i\Vert \leq \varepsilon} p(z)dz\end{equation}<br />
 所以$P_i(\varepsilon)^{k-1}$是挑出来的$k-1$个样本都在球内的概率，$(1 - P_i(\varepsilon + d\varepsilon))^{n-k-1}$是$n-k-1$个样本都在球外的概率，$P_i(\varepsilon + d\varepsilon) - P_i(\varepsilon)$则是一个样本在球间的概率，所有项乘起来就是式$\eqref{eq:dp-1}$，而展开并只保留一阶项得到近似式：<br />
 \begin{equation}\binom{n-1}{1}\binom{n-2}{k-1}P_i(\varepsilon)^{k-1}(1 - P_i(\varepsilon))^{n-k-1}dP_i(\varepsilon)\label{eq:dp-2}\end{equation}<br />
 注意上式描述了一个合理的概率分布，因此它的积分必然为1。</p>
<p>现在我们可以做个<strong>近似假设</strong> ，值得注意的是，这是整个推导过程的唯一假设，而最终结果的可靠程度也取决于这个假设的成立程度：<br />
 \begin{equation}p(z_i) \approx \frac{1}{B_d \varepsilon_k(i)^d}\int_{\Vert z - z_i\Vert \leq \varepsilon} p(z)dz = \frac{P_i(\varepsilon)}{B_d \varepsilon_k(i)^d}\end{equation}<br />
 其中$B_d \varepsilon_k(i)^d$就是半径为$\varepsilon_k(i)$的球的体积。根据这个近似我们有$p(z_i) B_d \varepsilon_k(i)^d \approx P_i(\varepsilon)$，看上去不合理，因为左端相当于一个常数，右端则是关于$\varepsilon$的函数，两者怎么能一直保持近似？事实上，当$n$足够大的时候，采样出来的样本足够稠密，这时候$\varepsilon$会集中在一个比较小的范围内，而$\varepsilon_k(i)$是$\varepsilon$某次采样的值，所以我们可以认为$\varepsilon$会集中在$\varepsilon_k(i)$附近。虽然$P_i(\varepsilon)$关于$\varepsilon$是变化的，但下面我们还要对$\varepsilon$进行积分，所以我们只需要在$\varepsilon_k(i)$附近对$P_i(\varepsilon)$做好近似进行，不需要整体都可以很好地近似（即$\varepsilon \gg \varepsilon_k(i)$时，$\eqref{eq:dp-2}$几乎为0）。而在$\varepsilon_k(i)$附近我们可以认为概率变化是平缓的，所以$p(z_i) B_d \varepsilon_k(i)^d \approx P_i(\varepsilon)$。</p>
<p>现在我们可以写出<br />
 \begin{equation}\log p(z_i) \approx \log P_i(\varepsilon) - \log B_d - d \log \varepsilon_k(i) \end{equation}<br />
 用$\eqref{eq:dp-2}$乘以上式两端，并对$\varepsilon$积分（积分区间为$[0,+\infty)$，或者等价于对$P_i$在$[0,1]$积分）。除$\log P_i(\varepsilon)$外，其余几项都是跟$\varepsilon$无关，所以积分后依然等于自身，而<br />
 \begin{equation}\begin{aligned}&amp;\int_0^1 \binom{n-1}{1}\binom{n-2}{k-1}P_i(\varepsilon)^{k-1}(1 - P_i(\varepsilon))^{n-k-1} \log P_i(\varepsilon) d P_i(\varepsilon) \\
 =&amp;\psi(k)-\psi(n)\end{aligned}\end{equation}<br />
 其中$\psi$代表着<a href="https://en.wikipedia.org/wiki/Digamma_function">双伽马函数</a>。（别问我这些积分是怎么算出来的，我也不知道，但我知道用Mathematica软件能把它都算出来～）</p>
<p>于是我们得到近似<br />
 \begin{equation}\log p(z_i) \approx \psi(k)-\psi(n) - \log B_d - d \log \varepsilon_k(i) \end{equation}<br />
 所以最终熵的近似为：<br />
 \begin{equation}\begin{aligned}H(Z)=&amp;\, \mathbb{E}<em i="1">{z\sim p(z)}[-\log p(z)]\\
 \approx&amp;\, -\frac{1}{n}\sum</em>^n \log p(z_i)\\
 \approx&amp;\, \frac{d}{n}\sum_{i=1}^n \log \varepsilon_k(i) + \log B_d + \psi(n)-\psi(k)
 \end{aligned}\label{eq:knn}\end{equation}<br />
 这是比式$\eqref{eq:1nn}$更一般的结果。事实上式$\eqref{eq:1nn}$是上式$k=1$时的特例，因为$\psi(1)=-\gamma$，而$\psi(n)= \sum\limits_{m=1}^{n-1}\frac{1}{m}-\gamma\approx \log(n-1)$，这些变换公式都可以在维基百科上找到。</p>
<p>开头就已经提到过，$k$邻近方法是一种很有用的非参数估计方法，它还跟笔者之前介绍过的<a href="/archives/6394">IMLE模型</a>有关。但笔者本身也不熟悉$k$邻近方法，还需要多多学习，目前找到的资料是<a href="https://link.springer.com/book/10.1007/978-3-319-25388-6">《Lectures on the Nearest Neighbor Method》</a>。此外，关于熵的估计，还可以参考斯坦福的资料<a href="https://web.stanford.edu/~yjhan/diff_entropy.pdf">《Theory and Practice of Differential Entropy Estimation》</a>。</p>
</blockquote>
<h2 id="_3">进一步思考与分析<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>有了$\eqref{eq:1nn}$或$\eqref{eq:knn}$，式$\eqref{eq:eae}$所描述的EAE的loss就完成了，所以EAE模型也就介绍完毕了。剩下的是实验结果，就不详细介绍了，反正就是感觉生成的图像跟VAE差不多，但指标上更优一些。</p>
<p><a href="/usr/uploads/2020/04/3907052608.jpg" title="点击查看原图"><img alt="来自EAE论文的实验对比" src="/usr/uploads/2020/04/3907052608.jpg" /></a></p>
<p>来自EAE论文的实验对比</p>
<p><a href="/usr/uploads/2020/04/2546475316.jpg" title="点击查看原图"><img alt="来自EAE论文的效果图示" src="/usr/uploads/2020/04/2546475316.jpg" /></a></p>
<p>来自EAE论文的效果图示</p>
<p>那EAE相比VAE的好处在哪呢？在VAE中，比较关键的一步是重参数（可以参考笔者的<a href="/archives/5253">《变分自编码器（一）：原来是这么一回事》</a>），就是这一步降低了模型训练的方差（相比REINFORCE方差更小，可以参考笔者的<a href="/archives/6705">《漫谈重参数：从正态分布到Gumbel Softmax》</a>），从而使得VAE可以有效地训练下去。然而，虽然重参数降低了方差，但事实上方差依然不小，简单来说就是重参数这一步带来较大的噪声（尤其是训练早期），导致decoder无法很好地利用encoder的信息，典型的例子就是将VAE用在NLP时的“KL散度消失”现象。</p>
<p>但是EAE基本上不存在这个问题，因为EAE基本上就是普通的自编码器，多加的BN不会对自编码性能有什么影响，而多加的熵正则项原则上也只是增加隐变量的多样性，不会给编码信息的利用与重构带来明显困难。笔者认为，这就是EAE相对于VAE的优势所在。当然，笔者目前还没有对EAE进行太多实验，上述分析多为主观推断，请读者自行甄别。如果笔者有进一步的实验结论，到时会继续在博客与大家分享。</p>
<h2 id="_4">最后补上一个小结<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>本文介绍了一个称之为EAE的模型，主要是把BN层和最大熵塞进了普通的自编码器中，使得它具有生成模型的能力。原论文做的不少实验显示EAE比VAE效果更好，所以应该是一个值得学习和试用的模型。此外，EAE的关键部分是通过$k$邻近方法来估计熵，这部分比较硬核，但事实上也很有价值，值得对统计估计感兴趣的读者细细阅读。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/7343">https://spaces.ac.cn/archives/7343</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Apr. 20, 2020). 《EAE：自编码器 + BN + 最大熵 = 生成模型 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/7343">https://spaces.ac.cn/archives/7343</a></p>
<p>@online{kexuefm-7343,<br />
title={EAE：自编码器 + BN + 最大熵 = 生成模型},<br />
author={苏剑林},<br />
year={2020},<br />
month={Apr},<br />
url={\url{https://spaces.ac.cn/archives/7343}},<br />
} </p>
<hr />
<h2 id="_5">公式推导与注释<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="智能家居之小爱同学控制极米投影仪的简单方案.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#91 智能家居之小爱同学控制极米投影仪的简单方案</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="designing-gans又一个gan生产车间.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#93 Designing GANs：又一个GAN生产车间</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#eae-bn">EAE：自编码器 + BN + 最大熵 = 生成模型</a><ul>
<li><a href="#vs">自编码器vs生成模型</a></li>
<li><a href="#_1">正态分布与最大熵</a></li>
<li><a href="#bn">均值方差约束与BN</a></li>
<li><a href="#_2">熵的采样邻近估计</a></li>
<li><a href="#_3">进一步思考与分析</a></li>
<li><a href="#_4">最后补上一个小结</a></li>
<li><a href="#_5">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>