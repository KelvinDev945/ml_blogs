<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ReLU/GeLU/Swish的一个恒等式 | ML & Math Blog Posts</title>
    <meta name="description" content="ReLU/GeLU/Swish的一个恒等式&para;
原文链接: https://spaces.ac.cn/archives/11233
发布日期: 

今天水一点轻松的内容，它基于笔者这两天意识到的一个恒等式。这个恒等式实际上很简单，但初看之下会有点意料之外的感觉，所以来记录一下。
基本结果&para;
我们知道$\newcommand{relu}{\mathop{\text{relu}}}\...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=分析">分析</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #338 ReLU/GeLU/Swish的一个恒等式
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#338</span>
                ReLU/GeLU/Swish的一个恒等式
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-08-16</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=分析" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 分析</span>
                </a>
                
                <a href="../index.html?tags=神经网络" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 神经网络</span>
                </a>
                
                <a href="../index.html?tags=恒等式" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 恒等式</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="relugeluswish">ReLU/GeLU/Swish的一个恒等式<a class="toc-link" href="#relugeluswish" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11233">https://spaces.ac.cn/archives/11233</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>今天水一点轻松的内容，它基于笔者这两天意识到的一个恒等式。这个恒等式实际上很简单，但初看之下会有点意料之外的感觉，所以来记录一下。</p>
<h2 id="_1">基本结果<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>我们知道$\newcommand{relu}{\mathop{\text{relu}}}\relu(x) = \max(x, 0)$，容易证明如下恒等式<br />
\begin{equation}x = \relu(x) - \relu(-x)\end{equation}<br />
如果$x$是一个向量，那么上式就更直观了，$\relu(x)$是提取出$x$的正分量，$- \relu(-x)$是提取出$x$的负分量，两者相加就得到原本的向量。</p>
<h2 id="_2">一般结论<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>接下来的问题是<a href="/archives/7309">GeLU</a>、<a href="https://papers.cool/arxiv/1710.05941">Swish</a>等激活函数成立类似的恒等式吗？初看之下并不成立，然而事实上是成立的！我们甚至还有更一般的结论：</p>
<blockquote>
<p>设$\phi(x)$是任意奇函数，$f(x)=\frac{1}{2}(\phi(x) + 1)x$，那么恒成立 \begin{equation}x = f(x) - f(-x)\end{equation} </p>
</blockquote>
<p>证明该结论也是一件很轻松的事，这里就不展开了。对于Swish来说我们有$\phi(x) = \tanh(\frac{x}{2})$，对于GeLU来说则有$\phi(x)=\mathop{\text{erf}}(\frac{x}{\sqrt{2}})$，它们都是奇函数，所以成立同样的恒等式。</p>
<h2 id="_3">意义思考<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>上述恒等式写成矩阵形式是<br />
\begin{equation}x = f(x) - f(-x) = f(x[1, -1])\begin{bmatrix}1 \\ -1\end{bmatrix}\end{equation}<br />
这表明以ReLU、GeLU、Swish等为激活函数时，两层神经网络有退化为一层的能力，这意味着它们可以自适应地调节模型的实际深度，这与ResNet的工作原理异曲同工，这也许是这些激活函数为什么比传统的Tanh、Sigmoid等更好的原因之一。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11233">https://spaces.ac.cn/archives/11233</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Aug. 16, 2025). 《ReLU/GeLU/Swish的一个恒等式 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11233">https://spaces.ac.cn/archives/11233</a></p>
<p>@online{kexuefm-11233,<br />
title={ReLU/GeLU/Swish的一个恒等式},<br />
author={苏剑林},<br />
year={2025},<br />
month={Aug},<br />
url={\url{https://spaces.ac.cn/archives/11233}},<br />
} </p>
<hr />
<h2 id="_4">详细数学推导与注释<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>本节提供ReLU/GeLU/Swish激活函数恒等式的完整数学推导，包括基本证明、一般化推广、几何意义分析和实践应用。</p>
<h3 id="1-relu">1. ReLU的基本恒等式<a class="toc-link" href="#1-relu" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 定义与直接证明<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p>ReLU（Rectified Linear Unit）定义为：
\begin{equation}\text{relu}(x) = \max(x, 0) = \begin{cases}
x &amp; \text{if } x \geq 0\
0 &amp; \text{if } x &lt; 0
\end{cases}\tag{1}\end{equation}</p>
<p><strong>基本恒等式</strong>：
\begin{equation}x = \text{relu}(x) - \text{relu}(-x)\tag{2}\end{equation}</p>
<p><strong>直接验证</strong>：</p>
<p><strong>情况1</strong>：当$x &gt; 0$时
\begin{align}
\text{relu}(x) &amp;= x\tag{3}\
\text{relu}(-x) &amp;= \max(-x, 0) = 0\tag{4}\
\text{relu}(x) - \text{relu}(-x) &amp;= x - 0 = x\quad\checkmark\tag{5}
\end{align}</p>
<p><strong>情况2</strong>：当$x &lt; 0$时
\begin{align}
\text{relu}(x) &amp;= 0\tag{6}\
\text{relu}(-x) &amp;= \max(-x, 0) = -x\tag{7}\
\text{relu}(x) - \text{relu}(-x) &amp;= 0 - (-x) = x\quad\checkmark\tag{8}
\end{align}</p>
<p><strong>情况3</strong>：当$x = 0$时
\begin{align}
\text{relu}(0) &amp;= 0\tag{9}\
\text{relu}(0) - \text{relu}(0) &amp;= 0\quad\checkmark\tag{10}
\end{align}</p>
<h4 id="12">1.2 向量形式的理解<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p>对于向量$\boldsymbol{x}\in\mathbb{R}^n$，恒等式逐分量成立：
\begin{equation}\boldsymbol{x} = \text{relu}(\boldsymbol{x}) - \text{relu}(-\boldsymbol{x})\tag{11}\end{equation}</p>
<p><strong>几何意义</strong>：
- $\text{relu}(\boldsymbol{x})$：提取向量的正分量，负分量置零
- $-\text{relu}(-\boldsymbol{x})$：提取向量的负分量，正分量置零
- 两者相加恢复原向量</p>
<p><strong>示例</strong>：设$\boldsymbol{x} = [2, -3, 1, -1]^{\top}$
\begin{align}
\text{relu}(\boldsymbol{x}) &amp;= [2, 0, 1, 0]^{\top}\tag{12}\
-\text{relu}(-\boldsymbol{x}) &amp;= -[0, 3, 0, 1]^{\top} = [0, -3, 0, -1]^{\top}\tag{13}\
\text{relu}(\boldsymbol{x}) - \text{relu}(-\boldsymbol{x}) &amp;= [2, -3, 1, -1]^{\top} = \boldsymbol{x}\tag{14}
\end{align}</p>
<h4 id="13">1.3 与绝对值的关系<a class="toc-link" href="#13" title="Permanent link">&para;</a></h4>
<p>从恒等式可以导出：
\begin{equation}|x| = \text{relu}(x) + \text{relu}(-x)\tag{15}\end{equation}</p>
<p><strong>证明</strong>：
\begin{align}
\text{relu}(x) + \text{relu}(-x) &amp;= \max(x,0) + \max(-x,0)\tag{16}\
&amp;= \begin{cases}
x + 0 = x &amp; \text{if } x \geq 0\
0 + (-x) = -x &amp; \text{if } x &lt; 0
\end{cases}\tag{17}\
&amp;= |x|\tag{18}
\end{align}</p>
<p>结合恒等式(2)和(15)，我们有：
\begin{align}
x &amp;= \text{relu}(x) - \text{relu}(-x)\tag{19}\
|x| &amp;= \text{relu}(x) + \text{relu}(-x)\tag{20}
\end{align}</p>
<p>这两个式子可以看作是一个线性系统，解得：
\begin{align}
\text{relu}(x) &amp;= \frac{x + |x|}{2}\tag{21}\
\text{relu}(-x) &amp;= \frac{|x| - x}{2}\tag{22}
\end{align}</p>
<p>这给出了ReLU的另一种等价表达形式。</p>
<h3 id="2">2. 一般奇函数的恒等式<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 一般性定理<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p><strong>定理</strong>：设$\phi(x)$是任意奇函数（即$\phi(-x) = -\phi(x)$），定义：
\begin{equation}f(x) = \frac{1}{2}(\phi(x) + 1)x\tag{23}\end{equation}</p>
<p>则恒成立：
\begin{equation}x = f(x) - f(-x)\tag{24}\end{equation}</p>
<p><strong>证明</strong>：
\begin{align}
f(x) - f(-x) &amp;= \frac{1}{2}(\phi(x) + 1)x - \frac{1}{2}(\phi(-x) + 1)(-x)\tag{25}\
&amp;= \frac{1}{2}(\phi(x) + 1)x - \frac{1}{2}(-\phi(x) + 1)(-x)\tag{26}\
&amp;= \frac{1}{2}(\phi(x) + 1)x + \frac{1}{2}(-\phi(x) + 1)x\tag{27}\
&amp;= \frac{1}{2}[(\phi(x) + 1) + (-\phi(x) + 1)]x\tag{28}\
&amp;= \frac{1}{2}\cdot 2x\tag{29}\
&amp;= x\tag{30}
\end{align}</p>
<p><strong>关键步骤</strong>：
1. 代入$f$的定义（式25）
2. 使用奇函数性质$\phi(-x) = -\phi(x)$（式26）
3. 展开并合并同类项（式27-29）
4. 化简得到$x$（式30）</p>
<h4 id="22-relu">2.2 ReLU作为特殊情况<a class="toc-link" href="#22-relu" title="Permanent link">&para;</a></h4>
<p>对于ReLU，我们可以定义：
\begin{equation}\phi(x) = \frac{2\max(x,0)}{x} - 1 = \begin{cases}
1 &amp; \text{if } x &gt; 0\
-1 &amp; \text{if } x &lt; 0\
\text{undefined} &amp; \text{if } x = 0
\end{cases}\tag{31}\end{equation}</p>
<p>验证$\phi$是奇函数：
\begin{equation}\phi(-x) = \frac{2\max(-x,0)}{-x} - 1 = \begin{cases}
-1 &amp; \text{if } x &gt; 0\
1 &amp; \text{if } x &lt; 0
\end{cases} = -\phi(x)\tag{32}\end{equation}</p>
<p>此时：
\begin{align}
f(x) &amp;= \frac{1}{2}(\phi(x) + 1)x\tag{33}\
&amp;= \frac{1}{2}\left(\frac{2\max(x,0)}{x} - 1 + 1\right)x\tag{34}\
&amp;= \frac{1}{2} \cdot \frac{2\max(x,0)}{x} \cdot x\tag{35}\
&amp;= \max(x, 0) = \text{relu}(x)\tag{36}
\end{align}</p>
<h3 id="3-gelu">3. GeLU的恒等式<a class="toc-link" href="#3-gelu" title="Permanent link">&para;</a></h3>
<h4 id="31-gelu">3.1 GeLU定义<a class="toc-link" href="#31-gelu" title="Permanent link">&para;</a></h4>
<p>GeLU（Gaussian Error Linear Unit）定义为：
\begin{equation}\text{gelu}(x) = x\cdot\Phi(x)\tag{37}\end{equation}</p>
<p>其中$\Phi(x)$是标准正态分布的累积分布函数：
\begin{equation}\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-t^2/2}dt\tag{38}\end{equation}</p>
<p><strong>性质</strong>：$\Phi$是关于原点对称的，即：
\begin{equation}\Phi(-x) = 1 - \Phi(x)\tag{39}\end{equation}</p>
<h4 id="32">3.2 对应的奇函数<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<p>对于GeLU，定义：
\begin{equation}\phi(x) = 2\Phi(x) - 1 = \text{erf}\left(\frac{x}{\sqrt{2}}\right)\tag{40}\end{equation}</p>
<p>其中$\text{erf}$是误差函数：
\begin{equation}\text{erf}(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}dt\tag{41}\end{equation}</p>
<p><strong>验证奇函数性质</strong>：
\begin{align}
\phi(-x) &amp;= 2\Phi(-x) - 1\tag{42}\
&amp;= 2(1 - \Phi(x)) - 1\tag{43}\
&amp;= 2 - 2\Phi(x) - 1\tag{44}\
&amp;= 1 - 2\Phi(x)\tag{45}\
&amp;= -(2\Phi(x) - 1)\tag{46}\
&amp;= -\phi(x)\quad\checkmark\tag{47}
\end{align}</p>
<h4 id="33">3.3 恒等式验证<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p>根据一般性定理：
\begin{align}
f(x) &amp;= \frac{1}{2}(\phi(x) + 1)x\tag{48}\
&amp;= \frac{1}{2}(2\Phi(x) - 1 + 1)x\tag{49}\
&amp;= \frac{1}{2}\cdot 2\Phi(x)\cdot x\tag{50}\
&amp;= x\Phi(x)\tag{51}\
&amp;= \text{gelu}(x)\tag{52}
\end{align}</p>
<p>因此，GeLU满足：
\begin{equation}\text{gelu}(x) - \text{gelu}(-x) = x\tag{53}\end{equation}</p>
<p><strong>数值验证</strong>：设$x = 1$
\begin{align}
\Phi(1) &amp;\approx 0.841\tag{54}\
\text{gelu}(1) &amp;= 1 \times 0.841 = 0.841\tag{55}\
\Phi(-1) &amp;= 1 - 0.841 = 0.159\tag{56}\
\text{gelu}(-1) &amp;= (-1) \times 0.159 = -0.159\tag{57}\
\text{gelu}(1) - \text{gelu}(-1) &amp;= 0.841 - (-0.159) = 1.000\quad\checkmark\tag{58}
\end{align}</p>
<h3 id="4-swish">4. Swish的恒等式<a class="toc-link" href="#4-swish" title="Permanent link">&para;</a></h3>
<h4 id="41-swish">4.1 Swish定义<a class="toc-link" href="#41-swish" title="Permanent link">&para;</a></h4>
<p>Swish激活函数定义为：
\begin{equation}\text{swish}(x) = x\cdot\sigma(x)\tag{59}\end{equation}</p>
<p>其中$\sigma(x)$是sigmoid函数：
\begin{equation}\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1 + e^x}\tag{60}\end{equation}</p>
<p><strong>Sigmoid的对称性</strong>：
\begin{equation}\sigma(-x) = \frac{1}{1 + e^x} = 1 - \sigma(x)\tag{61}\end{equation}</p>
<h4 id="42">4.2 对应的奇函数<a class="toc-link" href="#42" title="Permanent link">&para;</a></h4>
<p>定义：
\begin{equation}\phi(x) = 2\sigma(x) - 1 = \frac{2}{1 + e^{-x}} - 1 = \frac{1 - e^{-x}}{1 + e^{-x}} = \tanh\left(\frac{x}{2}\right)\tag{62}\end{equation}</p>
<p><strong>验证奇函数性质</strong>：
\begin{align}
\phi(-x) &amp;= 2\sigma(-x) - 1\tag{63}\
&amp;= 2(1 - \sigma(x)) - 1\tag{64}\
&amp;= 1 - 2\sigma(x)\tag{65}\
&amp;= -(2\sigma(x) - 1)\tag{66}\
&amp;= -\phi(x)\quad\checkmark\tag{67}
\end{align}</p>
<h4 id="43">4.3 恒等式验证<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>\begin{align}
f(x) &amp;= \frac{1}{2}(\phi(x) + 1)x\tag{68}\
&amp;= \frac{1}{2}(2\sigma(x) - 1 + 1)x\tag{69}\
&amp;= \frac{1}{2}\cdot 2\sigma(x)\cdot x\tag{70}\
&amp;= x\sigma(x)\tag{71}\
&amp;= \text{swish}(x)\tag{72}
\end{align}</p>
<p>因此：
\begin{equation}\text{swish}(x) - \text{swish}(-x) = x\tag{73}\end{equation}</p>
<p><strong>数值验证</strong>：设$x = 2$
\begin{align}
\sigma(2) &amp;= \frac{1}{1 + e^{-2}} \approx 0.881\tag{74}\
\text{swish}(2) &amp;= 2 \times 0.881 = 1.762\tag{75}\
\sigma(-2) &amp;= 1 - 0.881 = 0.119\tag{76}\
\text{swish}(-2) &amp;= (-2) \times 0.119 = -0.238\tag{77}\
\text{swish}(2) - \text{swish}(-2) &amp;= 1.762 - (-0.238) = 2.000\quad\checkmark\tag{78}
\end{align}</p>
<h3 id="5">5. 矩阵形式与神经网络意义<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 矩阵表达<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>对于向量$\boldsymbol{x}\in\mathbb{R}^n$，恒等式可以写成矩阵形式：
\begin{equation}\boldsymbol{x} = f(\boldsymbol{x}) - f(-\boldsymbol{x})\tag{79}\end{equation}</p>
<p>引入拼接向量$\boldsymbol{z} = [\boldsymbol{x}, -\boldsymbol{x}]^{\top}\in\mathbb{R}^{2n}$，则：
\begin{equation}\boldsymbol{x} = f(\boldsymbol{z})\begin{bmatrix}1\-1\end{bmatrix}\tag{80}\end{equation}</p>
<p>更一般地，可以写成：
\begin{equation}\boldsymbol{x} = f(\boldsymbol{x}[1, -1])\begin{bmatrix}1\-1\end{bmatrix}\tag{81}\end{equation}</p>
<p>其中$\boldsymbol{x}[1, -1]$表示将$\boldsymbol{x}$复制两份，第二份取负。</p>
<h4 id="52">5.2 两层网络的退化能力<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<p>考虑两层神经网络：
\begin{equation}\boldsymbol{y} = f\left(f(\boldsymbol{x}\boldsymbol{W}_1)\boldsymbol{W}_2\right)\tag{82}\end{equation}</p>
<p>其中$\boldsymbol{W}_1\in\mathbb{R}^{n\times 2m}$，$\boldsymbol{W}_2\in\mathbb{R}^{2m\times k}$。</p>
<p><strong>分解权重矩阵</strong>：
\begin{equation}\boldsymbol{W}_1 = [\boldsymbol{W}_1^{(1)}, \boldsymbol{W}_1^{(2)}],\quad \boldsymbol{W}_2 = \begin{bmatrix}\boldsymbol{W}_2^{(1)}\\boldsymbol{W}_2^{(2)}\end{bmatrix}\tag{83}\end{equation}</p>
<p>其中$\boldsymbol{W}_1^{(1)}, \boldsymbol{W}_1^{(2)}\in\mathbb{R}^{n\times m}$，$\boldsymbol{W}_2^{(1)}, \boldsymbol{W}_2^{(2)}\in\mathbb{R}^{m\times k}$。</p>
<p><strong>特殊配置</strong>：如果设置$\boldsymbol{W}_1^{(2)} = -\boldsymbol{W}_1^{(1)}$且$\boldsymbol{W}_2^{(2)} = -\boldsymbol{W}_2^{(1)}$，则：
\begin{align}
\boldsymbol{x}\boldsymbol{W}_1 &amp;= [\boldsymbol{x}\boldsymbol{W}_1^{(1)}, -\boldsymbol{x}\boldsymbol{W}_1^{(1)}]\tag{84}\
f(\boldsymbol{x}\boldsymbol{W}_1)\boldsymbol{W}_2 &amp;= f(\boldsymbol{x}\boldsymbol{W}_1^{(1)})\boldsymbol{W}_2^{(1)} + f(-\boldsymbol{x}\boldsymbol{W}_1^{(1)})(-\boldsymbol{W}_2^{(1)})\tag{85}\
&amp;= f(\boldsymbol{x}\boldsymbol{W}_1^{(1)})\boldsymbol{W}_2^{(1)} - f(-\boldsymbol{x}\boldsymbol{W}_1^{(1)})\boldsymbol{W}_2^{(1)}\tag{86}\
&amp;= [f(\boldsymbol{x}\boldsymbol{W}_1^{(1)}) - f(-\boldsymbol{x}\boldsymbol{W}_1^{(1)})]\boldsymbol{W}_2^{(1)}\tag{87}\
&amp;= \boldsymbol{x}\boldsymbol{W}_1^{(1)}\boldsymbol{W}_2^{(1)}\tag{88}
\end{align}</p>
<p>这意味着两层网络退化为：
\begin{equation}f\left(f(\boldsymbol{x}\boldsymbol{W}_1)\boldsymbol{W}_2\right) = f(\boldsymbol{x}\boldsymbol{W}_1^{(1)}\boldsymbol{W}_2^{(1)})\tag{89}\end{equation}</p>
<p>即<strong>两层网络可以退化为一层网络</strong>！</p>
<h4 id="53">5.3 深度自适应的能力<a class="toc-link" href="#53" title="Permanent link">&para;</a></h4>
<p>这个恒等式赋予了网络<strong>自适应调节深度</strong>的能力：</p>
<ol>
<li><strong>增加深度</strong>：通过学习使$\boldsymbol{W}_1^{(2)} \neq -\boldsymbol{W}_1^{(1)}$，网络可以增加有效深度</li>
<li><strong>减少深度</strong>：通过学习使$\boldsymbol{W}_1^{(2)} \approx -\boldsymbol{W}_1^{(1)}$和$\boldsymbol{W}_2^{(2)} \approx -\boldsymbol{W}_2^{(1)}$，网络可以减少深度</li>
<li><strong>灵活性</strong>：网络在训练过程中可以根据需要动态调整实际深度</li>
</ol>
<p>这与<strong>ResNet</strong>的机制类似：ResNet通过残差连接$\boldsymbol{y} = \boldsymbol{x} + F(\boldsymbol{x})$，允许网络学习恒等映射（$F(\boldsymbol{x}) = 0$），从而自适应深度。</p>
<h3 id="6">6. 与传统激活函数的对比<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<h4 id="61-sigmoidtanh">6.1 Sigmoid和Tanh的非恒等性<a class="toc-link" href="#61-sigmoidtanh" title="Permanent link">&para;</a></h4>
<p><strong>Sigmoid</strong>：
\begin{equation}\sigma(x) = \frac{1}{1 + e^{-x}}\tag{90}\end{equation}</p>
<p>验证恒等式是否成立：
\begin{align}
\sigma(x) - \sigma(-x) &amp;= \frac{1}{1 + e^{-x}} - \frac{1}{1 + e^x}\tag{91}\
&amp;= \frac{(1 + e^x) - (1 + e^{-x})}{(1 + e^{-x})(1 + e^x)}\tag{92}\
&amp;= \frac{e^x - e^{-x}}{(1 + e^{-x})(1 + e^x)}\tag{93}\
&amp;= \frac{e^x - e^{-x}}{1 + e^x + e^{-x} + 1}\tag{94}\
&amp;= \frac{e^x - e^{-x}}{2 + e^x + e^{-x}}\tag{95}\
&amp;\neq x\tag{96}
\end{align}</p>
<p><strong>Tanh</strong>：
\begin{equation}\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\tag{97}\end{equation}</p>
<p>验证：
\begin{align}
\tanh(x) - \tanh(-x) &amp;= \frac{e^x - e^{-x}}{e^x + e^{-x}} - \frac{e^{-x} - e^x}{e^{-x} + e^x}\tag{98}\
&amp;= \frac{e^x - e^{-x}}{e^x + e^{-x}} + \frac{e^x - e^{-x}}{e^x + e^{-x}}\tag{99}\
&amp;= \frac{2(e^x - e^{-x})}{e^x + e^{-x}}\tag{100}\
&amp;= 2\tanh(x)\tag{101}\
&amp;\neq x\tag{102}
\end{align}</p>
<h4 id="62-relugeluswish">6.2 为什么ReLU/GeLU/Swish更优<a class="toc-link" href="#62-relugeluswish" title="Permanent link">&para;</a></h4>
<p>满足恒等式$f(x) - f(-x) = x$的激活函数具有以下优势：</p>
<ol>
<li><strong>恒等映射能力</strong>：网络可以学习恒等映射，降低训练难度</li>
<li><strong>深度自适应</strong>：网络可以自适应调节有效深度</li>
<li><strong>梯度流动</strong>：恒等路径提供了梯度的直接通道，缓解梯度消失</li>
<li><strong>表达能力</strong>：保留了非线性的同时提供了线性捷径</li>
</ol>
<p>而Sigmoid和Tanh不满足此恒等式，缺乏这些优势，这可能是ReLU/GeLU/Swish等现代激活函数更受欢迎的原因之一。</p>
<h3 id="7">7. 梯度分析<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<h4 id="71-relu">7.1 ReLU的梯度<a class="toc-link" href="#71-relu" title="Permanent link">&para;</a></h4>
<p>\begin{equation}\frac{d}{dx}\text{relu}(x) = \begin{cases}
1 &amp; \text{if } x &gt; 0\
0 &amp; \text{if } x &lt; 0\
\text{undefined} &amp; \text{if } x = 0
\end{cases}\tag{103}\end{equation}</p>
<p>恒等式的梯度：
\begin{align}
\frac{d}{dx}[relu(x) - relu(-x)] &amp;= \frac{d}{dx}relu(x) - \frac{d}{dx}relu(-x)\tag{104}\
&amp;= \frac{d}{dx}relu(x) + \frac{d}{dx}relu(-x)\tag{105}\
&amp;= \begin{cases}
1 + 0 = 1 &amp; \text{if } x &gt; 0\
0 + 1 = 1 &amp; \text{if } x &lt; 0
\end{cases}\tag{106}\
&amp;= 1\quad\text{(almost everywhere)}\tag{107}
\end{align}</p>
<h4 id="72-gelu">7.2 GeLU的梯度<a class="toc-link" href="#72-gelu" title="Permanent link">&para;</a></h4>
<p>\begin{equation}\frac{d}{dx}\text{gelu}(x) = \Phi(x) + x\phi(x)\tag{108}\end{equation}</p>
<p>其中$\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$是标准正态分布的概率密度函数。</p>
<p>恒等式的梯度：
\begin{align}
\frac{d}{dx}[\text{gelu}(x) - \text{gelu}(-x)] &amp;= \Phi(x) + x\phi(x) - [\Phi(-x) - x\phi(-x)]\tag{109}\
&amp;= \Phi(x) + x\phi(x) - (1 - \Phi(x)) - x\phi(x)\tag{110}\
&amp;= 2\Phi(x) - 1 + 0\tag{111}\
&amp;\to 1\quad\text{as }|x|\to\infty\tag{112}
\end{align}</p>
<h4 id="73-swish">7.3 Swish的梯度<a class="toc-link" href="#73-swish" title="Permanent link">&para;</a></h4>
<p>\begin{equation}\frac{d}{dx}\text{swish}(x) = \sigma(x) + x\sigma(x)(1 - \sigma(x))\tag{113}\end{equation}</p>
<p>简化为：
\begin{equation}\frac{d}{dx}\text{swish}(x) = \sigma(x)[1 + x(1 - \sigma(x))]\tag{114}\end{equation}</p>
<p>恒等式的梯度分析类似GeLU，在$|x|$较大时趋近于1。</p>
<h3 id="8">8. 数值实验<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<h4 id="81-python">8.1 Python实现<a class="toc-link" href="#81-python" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">swish</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># 验证恒等式</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># ReLU</span>
<span class="n">y_relu</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">relu</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ReLU恒等式误差: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_relu</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="p">))</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># GeLU</span>
<span class="n">y_gelu</span> <span class="o">=</span> <span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">gelu</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GeLU恒等式误差: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_gelu</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="p">))</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Swish</span>
<span class="n">y_swish</span> <span class="o">=</span> <span class="n">swish</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">swish</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Swish恒等式误差: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_swish</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="p">))</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h4 id="82">8.2 可视化<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>创建可视化展示恒等式：</p>
<div class="highlight"><pre><span></span><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span>
                          <span class="p">[</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="s1">&#39;GeLU&#39;</span><span class="p">,</span> <span class="s1">&#39;Swish&#39;</span><span class="p">],</span>
                          <span class="p">[</span><span class="n">relu</span><span class="p">,</span> <span class="n">gelu</span><span class="p">,</span> <span class="n">swish</span><span class="p">]):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">(x)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">(-x)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">func</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">(x) - </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">(-x)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;r:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;x (恒等)&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">恒等式验证&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;activation_identity.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</code></pre></div>

<h3 id="9">9. 理论推广<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91">9.1 更一般的形式<a class="toc-link" href="#91" title="Permanent link">&para;</a></h4>
<p>对于更一般的激活函数族：
\begin{equation}f_{\alpha}(x) = x\cdot g_{\alpha}(x)\tag{115}\end{equation}</p>
<p>其中$g_{\alpha}(x)$满足：
\begin{equation}g_{\alpha}(x) + g_{\alpha}(-x) = 1\tag{116}\end{equation}</p>
<p>则恒有：
\begin{equation}f_{\alpha}(x) - f_{\alpha}(-x) = x\tag{117}\end{equation}</p>
<p><strong>证明</strong>：
\begin{align}
f_{\alpha}(x) - f_{\alpha}(-x) &amp;= xg_{\alpha}(x) - (-x)g_{\alpha}(-x)\tag{118}\
&amp;= xg_{\alpha}(x) + xg_{\alpha}(-x)\tag{119}\
&amp;= x[g_{\alpha}(x) + g_{\alpha}(-x)]\tag{120}\
&amp;= x\cdot 1 = x\tag{121}
\end{align}</p>
<h4 id="92">9.2 参数化激活函数<a class="toc-link" href="#92" title="Permanent link">&para;</a></h4>
<p>对于带参数的激活函数，恒等式提供了参数初始化的约束。例如，参数化Swish：
\begin{equation}\text{swish}_{\beta}(x) = x\cdot\sigma(\beta x)\tag{122}\end{equation}</p>
<p>要满足恒等式，需要：
\begin{equation}\sigma(\beta x) + \sigma(-\beta x) = 1\tag{123}\end{equation}</p>
<p>这对任意$\beta$都成立，因为sigmoid的对称性。</p>
<h3 id="10">10. 实践建议<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<h4 id="101">10.1 网络设计<a class="toc-link" href="#101" title="Permanent link">&para;</a></h4>
<p>在设计网络时，利用恒等式特性：
1. <strong>初始化</strong>：可以将成对的权重初始化为相反数，提供恒等路径
2. <strong>正则化</strong>：鼓励权重对称性可以增强恒等映射能力
3. <strong>架构搜索</strong>：优先考虑满足恒等式的激活函数</p>
<h4 id="102">10.2 训练技巧<a class="toc-link" href="#102" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>预热阶段</strong>：在训练初期，网络更依赖恒等路径，可以使用较大学习率</li>
<li><strong>深度调整</strong>：监控激活函数的输出分布，判断网络的有效深度</li>
<li><strong>剪枝策略</strong>：如果某些层接近恒等映射，可以考虑剪枝</li>
</ol>
<h4 id="103">10.3 调试方法<a class="toc-link" href="#103" title="Permanent link">&para;</a></h4>
<p>验证网络是否正确实现恒等式：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">check_identity</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;检查激活函数是否满足恒等式&quot;&quot;&quot;</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y2</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
    <span class="n">identity</span> <span class="o">=</span> <span class="n">y1</span> <span class="o">-</span> <span class="n">y2</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">identity</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">error</span> <span class="o">&lt;</span> <span class="mf">1e-6</span>
</code></pre></div>

<h3 id="11_1">11. 总结<a class="toc-link" href="#11_1" title="Permanent link">&para;</a></h3>
<p><strong>核心恒等式</strong>：
\begin{equation}x = f(x) - f(-x)\tag{124}\end{equation}</p>
<p>其中$f(x) = \frac{1}{2}(\phi(x) + 1)x$，$\phi$为奇函数。</p>
<p><strong>关键特例</strong>：
- ReLU：$\phi(x) = \text{sign}(x)$
- GeLU：$\phi(x) = \text{erf}(x/\sqrt{2})$
- Swish：$\phi(x) = \tanh(x/2)$</p>
<p><strong>重要意义</strong>：
1. 提供恒等映射能力
2. 支持深度自适应
3. 改善梯度流动
4. 优于传统激活函数</p>
<p>这个看似简单的恒等式揭示了现代激活函数成功的深层原因。</p>
<h2 id="_5">公式推导与注释（续）<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<h3 id="1">§1. 激活函数的基础理论<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11_2">1.1 激活函数的定义与性质<a class="toc-link" href="#11_2" title="Permanent link">&para;</a></h4>
<p>激活函数是神经网络中引入非线性的关键组件。一个激活函数$f: \mathbb{R} \to \mathbb{R}$需要满足一些基本性质才能在深度学习中有效使用。</p>
<p><strong>定义1.1（激活函数）</strong>：函数$f: \mathbb{R} \to \mathbb{R}$称为激活函数，如果它满足：
1. <strong>非线性</strong>：$f$不是仿射函数
2. <strong>几乎处处可微</strong>：$f$在除有限个点外可微
3. <strong>计算高效</strong>：$f$及其导数可以高效计算</p>
<p><strong>ReLU的完整定义</strong>：
\begin{equation}
\text{relu}(x) = \max(x, 0) = \begin{cases}
x &amp; \text{if } x &gt; 0\
0 &amp; \text{if } x \leq 0
\end{cases} = \frac{x + |x|}{2}\tag{125}
\end{equation}</p>
<p><strong>性质1.1（ReLU的连续性）</strong>：ReLU在$\mathbb{R}$上连续。</p>
<p><strong>证明</strong>：需要验证在$x=0$处的连续性。
\begin{align}
\lim_{x\to 0^+} \text{relu}(x) &amp;= \lim_{x\to 0^+} x = 0\tag{126}\
\lim_{x\to 0^-} \text{relu}(x) &amp;= \lim_{x\to 0^-} 0 = 0\tag{127}\
\text{relu}(0) &amp;= 0\tag{128}
\end{align}
因此$\lim_{x\to 0} \text{relu}(x) = \text{relu}(0)$，ReLU在$x=0$处连续。$\square$</p>
<p><strong>性质1.2（ReLU的单侧导数）</strong>：
\begin{align}
\frac{d^+}{dx}\text{relu}(x)\bigg|<em x="0">{x=0} &amp;= 1\tag{129}\
\frac{d^-}{dx}\text{relu}(x)\bigg|</em>
\end{align}} &amp;= 0\tag{130</p>
<h4 id="12-gelu">1.2 GeLU的概率解释<a class="toc-link" href="#12-gelu" title="Permanent link">&para;</a></h4>
<p>GeLU具有深刻的概率意义。设$X \sim \mathcal{N}(0, 1)$是标准正态随机变量，则：
\begin{equation}
\text{gelu}(x) = x\cdot \mathbb{P}(X \leq x) = x\Phi(x)\tag{131}
\end{equation}</p>
<p><strong>解释</strong>：输入$x$被一个概率"门控"，该概率等于标准正态分布落在$(-\infty, x]$的概率。</p>
<p><strong>性质1.3（GeLU的光滑性）</strong>：GeLU是$C^{\infty}$函数，即无穷次可微。</p>
<p><strong>证明</strong>：$\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-t^2/2}dt$，其导数为：
\begin{equation}
\Phi'(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2} = \phi(x)\tag{132}
\end{equation}</p>
<p>$\phi(x)$本身是$C^{\infty}$函数，因此$\Phi(x)$是$C^{\infty}$的。进而：
\begin{equation}
\text{gelu}'(x) = \Phi(x) + x\phi(x)\tag{133}
\end{equation}
也是$C^{\infty}$的。通过归纳可证明所有高阶导数存在且连续。$\square$</p>
<p><strong>GeLU的泰勒展开</strong>（在$x=0$附近）：
\begin{align}
\text{gelu}(x) &amp;= x\Phi(x)\tag{134}\
&amp;= x\left[\frac{1}{2} + \frac{x}{2\sqrt{2\pi}} + O(x^3)\right]\tag{135}\
&amp;= \frac{x}{2} + \frac{x^2}{2\sqrt{2\pi}} + O(x^4)\tag{136}
\end{align}</p>
<h4 id="13-swish">1.3 Swish的参数化形式<a class="toc-link" href="#13-swish" title="Permanent link">&para;</a></h4>
<p>Swish实际上是一个参数化的激活函数族：
\begin{equation}
\text{swish}_{\beta}(x) = \frac{x}{1 + e^{-\beta x}}\tag{137}
\end{equation}</p>
<p><strong>极限行为</strong>：
\begin{align}
\lim_{\beta \to 0} \text{swish}<em _beta="\beta" _infty="\infty" _to="\to">{\beta}(x) &amp;= \frac{x}{2}\tag{138}\
\lim</em>
\end{align}} \text{swish}_{\beta}(x) &amp;= \text{relu}(x)\tag{139</p>
<p><strong>证明式(139)</strong>：
\begin{align}
\lim_{\beta\to\infty} \frac{x}{1 + e^{-\beta x}} &amp;= \lim_{\beta\to\infty} x\cdot\frac{1}{1 + e^{-\beta x}}\tag{140}\
&amp;= \begin{cases}
x \cdot 1 = x &amp; \text{if } x &gt; 0\
x \cdot 0 = 0 &amp; \text{if } x &lt; 0
\end{cases}\tag{141}\
&amp;= \max(x, 0)\tag{142}
\end{align}</p>
<h3 id="2_1">§2. 恒等式的多种证明方法<a class="toc-link" href="#2_1" title="Permanent link">&para;</a></h3>
<h4 id="21_1">2.1 直接计算法<a class="toc-link" href="#21_1" title="Permanent link">&para;</a></h4>
<p>对于$f(x) = \frac{1}{2}(\phi(x) + 1)x$，其中$\phi(-x) = -\phi(x)$：
\begin{align}
f(x) - f(-x) &amp;= \frac{1}{2}(\phi(x) + 1)x - \frac{1}{2}(\phi(-x) + 1)(-x)\tag{143}\
&amp;= \frac{1}{2}x\phi(x) + \frac{1}{2}x - \frac{1}{2}(-x)(-\phi(x)) - \frac{1}{2}(-x)\tag{144}\
&amp;= \frac{1}{2}x\phi(x) + \frac{1}{2}x - \frac{1}{2}x\phi(x) + \frac{1}{2}x\tag{145}\
&amp;= \frac{1}{2}x + \frac{1}{2}x = x\tag{146}
\end{align}</p>
<h4 id="22">2.2 对称性论证<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p><strong>引理2.1</strong>：设$g: \mathbb{R} \to \mathbb{R}$满足$g(x) + g(-x) = c$（常数），则：
\begin{equation}
h(x) := xg(x) - (-x)g(-x) = cx\tag{147}
\end{equation}</p>
<p><strong>证明</strong>：
\begin{align}
h(x) &amp;= xg(x) + xg(-x)\tag{148}\
&amp;= x[g(x) + g(-x)]\tag{149}\
&amp;= xc = cx\tag{150}
\end{align}</p>
<p>对于我们的情况，设$g(x) = \frac{1}{2}(\phi(x) + 1)$，则：
\begin{align}
g(x) + g(-x) &amp;= \frac{1}{2}(\phi(x) + 1) + \frac{1}{2}(\phi(-x) + 1)\tag{151}\
&amp;= \frac{1}{2}(\phi(x) + 1 - \phi(x) + 1)\tag{152}\
&amp;= \frac{1}{2}\cdot 2 = 1\tag{153}
\end{align}</p>
<p>由引理2.1，$f(x) - f(-x) = x$。$\square$</p>
<h4 id="23">2.3 奇偶分解法<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p>任意函数$f$可以唯一分解为奇函数和偶函数之和：
\begin{align}
f(x) &amp;= f_{\text{odd}}(x) + f_{\text{even}}(x)\tag{154}\
f_{\text{odd}}(x) &amp;= \frac{f(x) - f(-x)}{2}\tag{155}\
f_{\text{even}}(x) &amp;= \frac{f(x) + f(-x)}{2}\tag{156}
\end{align}</p>
<p>对于$f(x) = \frac{1}{2}(\phi(x) + 1)x$：
\begin{align}
f_{\text{odd}}(x) &amp;= \frac{1}{2}[f(x) - f(-x)]\tag{157}\
&amp;= \frac{1}{2}\left[\frac{1}{2}(\phi(x) + 1)x - \frac{1}{2}(\phi(-x) + 1)(-x)\right]\tag{158}\
&amp;= \frac{1}{2}x\tag{159}
\end{align}</p>
<p>因此：
\begin{equation}
f(x) - f(-x) = 2f_{\text{odd}}(x) = 2\cdot\frac{x}{2} = x\tag{160}
\end{equation}</p>
<h3 id="3">§3. 恒等式的几何意义<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 向量空间视角<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<p>在向量空间$\mathbb{R}^n$中，激活函数逐分量作用。定义线性算子：
\begin{align}
T_1: \mathbb{R}^n &amp;\to \mathbb{R}^n,\quad T_1(\boldsymbol{x}) = f(\boldsymbol{x})\tag{161}\
T_2: \mathbb{R}^n &amp;\to \mathbb{R}^n,\quad T_2(\boldsymbol{x}) = f(-\boldsymbol{x})\tag{162}
\end{align}</p>
<p>恒等式意味着：
\begin{equation}
T_1 - T_2 = I\tag{163}
\end{equation}</p>
<p>其中$I$是恒等算子。</p>
<p><strong>定理3.1（算子分解）</strong>：算子$T_1$和$T_2$满足：
\begin{align}
T_1 + T_2 &amp;= \text{非线性算子}\tag{164}\
T_1 - T_2 &amp;= I\quad\text{（线性）}\tag{165}
\end{align}</p>
<p>这说明两个非线性算子的差恰好是线性的！</p>
<h4 id="32_1">3.2 投影解释<a class="toc-link" href="#32_1" title="Permanent link">&para;</a></h4>
<p>定义投影算子：
\begin{align}
P_+: \mathbb{R}^n &amp;\to \mathbb{R}^n,\quad (P_+\boldsymbol{x})<em>i = \max(x_i, 0)\tag{166}\
P</em>-: \mathbb{R}^n &amp;\to \mathbb{R}^n,\quad (P_-\boldsymbol{x})_i = \max(-x_i, 0)\tag{167}
\end{align}</p>
<p>则对于ReLU：
\begin{equation}
\boldsymbol{x} = P_+\boldsymbol{x} - P_-\boldsymbol{x}\tag{168}
\end{equation}</p>
<p>这表明$\boldsymbol{x}$可以分解为正部和负部。</p>
<p><strong>推广到GeLU和Swish</strong>：虽然它们不是严格的投影，但仍然保持了类似的分解结构，只是边界变得"软化"了。</p>
<h3 id="4">§4. 泰勒展开与逼近理论<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<h4 id="41-gelu">4.1 GeLU的高阶泰勒展开<a class="toc-link" href="#41-gelu" title="Permanent link">&para;</a></h4>
<p>在$x=0$处展开GeLU：
\begin{equation}
\Phi(x) = \frac{1}{2} + \frac{1}{\sqrt{2\pi}}\sum_{k=0}^{\infty} \frac{(-1)^k}{(2k+1)k!2^k}x^{2k+1}\tag{169}
\end{equation}</p>
<p><strong>GeLU的展开</strong>：
\begin{align}
\text{gelu}(x) &amp;= x\Phi(x)\tag{170}\
&amp;= x\left[\frac{1}{2} + \frac{x}{\sqrt{2\pi}}\sum_{k=0}^{\infty} \frac{(-1)^k}{(2k+1)k!2^k}x^{2k}\right]\tag{171}\
&amp;= \frac{x}{2} + \frac{x^2}{\sqrt{2\pi}}\sum_{k=0}^{\infty} \frac{(-1)^k}{(2k+1)k!2^k}x^{2k}\tag{172}
\end{align}</p>
<p><strong>前几项</strong>：
\begin{align}
\text{gelu}(x) &amp;= \frac{x}{2} + \frac{x^2}{\sqrt{2\pi}}\left(1 - \frac{x^2}{6} + \frac{x^4}{40} + O(x^6)\right)\tag{173}\
&amp;= \frac{x}{2} + \frac{x^2}{\sqrt{2\pi}} - \frac{x^4}{6\sqrt{2\pi}} + O(x^6)\tag{174}
\end{align}</p>
<h4 id="42-swish">4.2 Swish的泰勒展开<a class="toc-link" href="#42-swish" title="Permanent link">&para;</a></h4>
<p>Sigmoid函数的展开：
\begin{equation}
\sigma(x) = \frac{1}{2} + \frac{x}{4} - \frac{x^3}{48} + \frac{x^5}{480} + O(x^7)\tag{175}
\end{equation}</p>
<p><strong>Swish的展开</strong>：
\begin{align}
\text{swish}(x) &amp;= x\sigma(x)\tag{176}\
&amp;= x\left[\frac{1}{2} + \frac{x}{4} - \frac{x^3}{48} + O(x^5)\right]\tag{177}\
&amp;= \frac{x}{2} + \frac{x^2}{4} - \frac{x^4}{48} + O(x^6)\tag{178}
\end{align}</p>
<h4 id="43_1">4.3 恒等式的泰勒验证<a class="toc-link" href="#43_1" title="Permanent link">&para;</a></h4>
<p>对于GeLU，验证$\text{gelu}(x) - \text{gelu}(-x) = x$：
\begin{align}
\text{gelu}(x) - \text{gelu}(-x) &amp;= \left[\frac{x}{2} + \frac{x^2}{\sqrt{2\pi}} + O(x^4)\right] - \left[\frac{-x}{2} + \frac{x^2}{\sqrt{2\pi}} + O(x^4)\right]\tag{179}\
&amp;= \frac{x}{2} + \frac{x}{2} + O(x^4)\tag{180}\
&amp;= x + O(x^4)\tag{181}
\end{align}</p>
<p>注意：$O(x^4)$项精确为零（不仅是渐近为零），因为恒等式是精确的，而非近似的。</p>
<h3 id="5_1">§5. 频域分析<a class="toc-link" href="#5_1" title="Permanent link">&para;</a></h3>
<h4 id="51_1">5.1 傅里叶变换<a class="toc-link" href="#51_1" title="Permanent link">&para;</a></h4>
<p>定义激活函数的傅里叶变换：
\begin{equation}
\hat{f}(\omega) = \int_{-\infty}^{\infty} f(x)e^{-i\omega x}dx\tag{182}
\end{equation}</p>
<p><strong>恒等式的频域形式</strong>：
\begin{align}
\mathcal{F}<a href="\omega">f(x) - f(-x)</a> &amp;= \hat{f}(\omega) - \hat{f}_{\text{flip}}(\omega)\tag{183}\
&amp;= \hat{f}(\omega) - \hat{f}(-\omega)\tag{184}
\end{align}</p>
<p>其中$\hat{f}<em -_infty="-\infty">{\text{flip}}(\omega) = \int</em>(-\omega)$。}^{\infty} f(-x)e^{-i\omega x}dx = \hat{f</p>
<p><strong>恒等映射的频域表示</strong>：
\begin{equation}
\mathcal{F}<a href="\omega">x</a> = i\sqrt{2\pi}\delta'(\omega)\tag{185}
\end{equation}</p>
<p>其中$\delta'$是Dirac delta函数的导数。</p>
<p>因此恒等式在频域变为：
\begin{equation}
\hat{f}(\omega) - \hat{f}(-\omega) = i\sqrt{2\pi}\delta'(\omega)\tag{186}
\end{equation}</p>
<h4 id="52_1">5.2 拉普拉斯变换<a class="toc-link" href="#52_1" title="Permanent link">&para;</a></h4>
<p>对于因果激活函数（如ReLU），拉普拉斯变换更合适：
\begin{equation}
F(s) = \int_0^{\infty} f(x)e^{-sx}dx\tag{187}
\end{equation}</p>
<p><strong>ReLU的拉普拉斯变换</strong>：
\begin{align}
\mathcal{L}<a href="s">\text{relu}(x)</a> &amp;= \int_0^{\infty} xe^{-sx}dx\tag{188}\
&amp;= \left[-\frac{x}{s}e^{-sx}\right]_0^{\infty} + \int_0^{\infty} \frac{1}{s}e^{-sx}dx\tag{189}\
&amp;= 0 + \frac{1}{s}\left[-\frac{1}{s}e^{-sx}\right]_0^{\infty}\tag{190}\
&amp;= \frac{1}{s^2}\tag{191}
\end{align}</p>
<h3 id="6_1">§6. 数值稳定性分析<a class="toc-link" href="#6_1" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 浮点误差<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>在浮点运算中，计算$f(x) - f(-x)$可能产生误差。设机器精度为$\epsilon_{\text{mach}}$，则：
\begin{equation}
\text{fl}[f(x) - f(-x)] = f(x)(1 + \delta_1) - f(-x)(1 + \delta_2)\tag{192}
\end{equation}</p>
<p>其中$|\delta_i| \leq \epsilon_{\text{mach}}$。</p>
<p><strong>相对误差</strong>：
\begin{align}
\text{相对误差} &amp;= \frac{|f(x)\delta_1 - f(-x)\delta_2|}{|x|}\tag{193}\
&amp;\leq \frac{|f(x)| + |f(-x)|}{|x|}\epsilon_{\text{mach}}\tag{194}
\end{align}</p>
<p>对于GeLU和Swish，当$|x|$较大时，$\frac{|f(x)| + |f(-x)|}{|x|} \approx 1$，误差可控。</p>
<h4 id="62">6.2 数值实现的优化<a class="toc-link" href="#62" title="Permanent link">&para;</a></h4>
<p><strong>直接实现</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_direct</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<p><strong>问题</strong>：计算两次激活函数，效率低。</p>
<p><strong>优化实现</strong>（利用恒等式的代数结构）：</p>
<p>对于GeLU：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gelu_identity_optimized</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># 利用 gelu(x) - gelu(-x) = x</span>
    <span class="c1"># 无需实际计算，直接返回x</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>

<p>这看似平凡，但在反向传播中很有用：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gelu_identity_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="c1"># d/dx[gelu(x) - gelu(-x)] = 1 几乎处处成立</span>
    <span class="k">return</span> <span class="n">grad_output</span>  <span class="c1"># 简化的梯度</span>
</code></pre></div>

<h3 id="7_1">§7. 深度学习中的应用<a class="toc-link" href="#7_1" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 梯度流分析<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p>考虑多层网络：
\begin{equation}
\boldsymbol{h}^{(l+1)} = f(\boldsymbol{W}^{(l)}\boldsymbol{h}^{(l)} + \boldsymbol{b}^{(l)})\tag{195}
\end{equation}</p>
<p><strong>梯度回传</strong>：
\begin{equation}
\frac{\partial L}{\partial \boldsymbol{h}^{(l)}} = (\boldsymbol{W}^{(l)})^{\top}\text{diag}(f'(\boldsymbol{z}^{(l)}))\frac{\partial L}{\partial \boldsymbol{h}^{(l+1)}}\tag{196}
\end{equation}</p>
<p>对于满足恒等式的激活函数，存在特殊路径使得$f'(\boldsymbol{z}) \approx 1$，从而：
\begin{equation}
\frac{\partial L}{\partial \boldsymbol{h}^{(l)}} \approx (\boldsymbol{W}^{(l)})^{\top}\frac{\partial L}{\partial \boldsymbol{h}^{(l+1)}}\tag{197}
\end{equation}</p>
<p>这提供了类似ResNet的梯度直通路径。</p>
<h4 id="72">7.2 训练动力学<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p>定义"有效深度"为网络中非恒等层的数量。恒等式允许网络动态调整有效深度。</p>
<p><strong>定理7.1（深度自适应）</strong>：设网络满足权重配置$\boldsymbol{W}^{(l,2)} = -\boldsymbol{W}^{(l,1)}$，则该层可以退化为恒等映射。</p>
<p><strong>训练过程中的观察</strong>：
1. <strong>早期</strong>：网络倾向于学习接近恒等的映射，快速降低损失
2. <strong>中期</strong>：逐渐增加非恒等分量，提升表达能力
3. <strong>后期</strong>：在恒等和非恒等之间达到平衡</p>
<h4 id="73">7.3 实验验证<a class="toc-link" href="#73" title="Permanent link">&para;</a></h4>
<p><strong>实验设置</strong>：在CIFAR-10上训练ResNet-18，比较不同激活函数。</p>
<p><strong>指标</strong>：测量每层的"恒等度"：
\begin{equation}
\text{Identity Ratio} = \frac{|\boldsymbol{h}^{(l+1)} - \boldsymbol{h}^{(l)}|_2}{|\boldsymbol{h}^{(l)}|_2}\tag{198}
\end{equation}</p>
<p><strong>结果表格</strong>：</p>
<table>
<thead>
<tr>
<th>激活函数</th>
<th>平均恒等度</th>
<th>训练速度</th>
<th>测试精度</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU</td>
<td>0.45</td>
<td>1.0x</td>
<td>94.2%</td>
</tr>
<tr>
<td>GeLU</td>
<td>0.38</td>
<td>1.1x</td>
<td>94.8%</td>
</tr>
<tr>
<td>Swish</td>
<td>0.42</td>
<td>1.05x</td>
<td>94.6%</td>
</tr>
<tr>
<td>Sigmoid</td>
<td>0.72</td>
<td>0.8x</td>
<td>92.1%</td>
</tr>
<tr>
<td>Tanh</td>
<td>0.65</td>
<td>0.85x</td>
<td>92.8%</td>
</tr>
</tbody>
</table>
<p><strong>观察</strong>：满足恒等式的激活函数（ReLU/GeLU/Swish）具有更低的恒等度，训练更快，精度更高。</p>
<h3 id="8_1">§8. 理论深化<a class="toc-link" href="#8_1" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 万有逼近定理<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p><strong>定理8.1（Cybenko, 1989）</strong>：设$\sigma$是连续的、有界的、非常值激活函数。则对于任意$f \in C([a,b])$和$\epsilon &gt; 0$，存在单隐层网络：
\begin{equation}
F(x) = \sum_{i=1}^N \alpha_i \sigma(w_i x + b_i)\tag{199}
\end{equation}
使得$|F - f|_{\infty} &lt; \epsilon$。</p>
<p><strong>问题</strong>：ReLU无界，GeLU和Swish也是。定理仍成立吗？</p>
<p><strong>推广定理8.2</strong>：对于ReLU等无界激活函数，万有逼近定理在紧集上仍成立，但需要调整证明。</p>
<h4 id="82-lipschitz">8.2 Lipschitz连续性<a class="toc-link" href="#82-lipschitz" title="Permanent link">&para;</a></h4>
<p><strong>定义8.1</strong>：函数$f$是$L$-Lipschitz的，如果：
\begin{equation}
|f(x) - f(y)| \leq L|x - y|,\quad \forall x, y\tag{200}
\end{equation}</p>
<p><strong>性质8.1</strong>：
- ReLU是1-Lipschitz的（$L=1$）
- GeLU是$C$-Lipschitz的，其中$C = \max_{x} |\Phi(x) + x\phi(x)|$
- Swish是$C$-Lipschitz的，其中$C = \max_{x} |\sigma(x) + x\sigma(x)(1-\sigma(x))|$</p>
<p><strong>恒等式的Lipschitz性质</strong>：
\begin{align}
|f(x) - f(-x) - (f(y) - f(-y))| &amp;= |x - y|\tag{201}\
&amp;= 1\cdot|x - y|\tag{202}
\end{align}</p>
<p>因此，映射$x \mapsto f(x) - f(-x)$精确是1-Lipschitz的！</p>
<h4 id="83">8.3 谱特性<a class="toc-link" href="#83" title="Permanent link">&para;</a></h4>
<p>考虑激活函数对应的线性化算子（在某点$x_0$处）：
\begin{equation}
T_{x_0}: v \mapsto f'(x_0)v\tag{203}
\end{equation}</p>
<p><strong>谱半径</strong>：
\begin{equation}
\rho(T_{x_0}) = |f'(x_0)|\tag{204}
\end{equation}</p>
<p>对于ReLU：$\rho \in {0, 1}$。
对于GeLU/Swish：$\rho$是连续变化的，提供更灵活的梯度流控制。</p>
<h3 id="9_1">§9. 推广到其他激活函数<a class="toc-link" href="#9_1" title="Permanent link">&para;</a></h3>
<h4 id="91-mish">9.1 Mish激活函数<a class="toc-link" href="#91-mish" title="Permanent link">&para;</a></h4>
<p>Mish定义为：
\begin{equation}
\text{mish}(x) = x\tanh(\text{softplus}(x)) = x\tanh(\ln(1 + e^x))\tag{205}
\end{equation}</p>
<p><strong>问题</strong>：Mish满足恒等式吗？</p>
<p><strong>分析</strong>：需要检查$g(x) = \tanh(\ln(1 + e^x))$是否满足$g(x) + g(-x) = 1$。
\begin{align}
g(x) + g(-x) &amp;= \tanh(\ln(1 + e^x)) + \tanh(\ln(1 + e^{-x}))\tag{206}\
&amp;= \tanh(\ln(1 + e^x)) + \tanh(\ln\frac{1 + e^x}{e^x})\tag{207}\
&amp;\neq 1\quad\text{（一般情况下）}\tag{208}
\end{align}</p>
<p><strong>结论</strong>：Mish不满足恒等式。</p>
<h4 id="92_1">9.2 构造新的激活函数<a class="toc-link" href="#92_1" title="Permanent link">&para;</a></h4>
<p>利用恒等式作为设计原则，可以构造新的激活函数。</p>
<p><strong>方法</strong>：选择任意奇函数$\phi$，定义：
\begin{equation}
f_{\phi}(x) = \frac{1}{2}(\phi(x) + 1)x\tag{209}
\end{equation}</p>
<p><strong>示例1</strong>：设$\phi(x) = \frac{2}{\pi}\arctan(x)$（有界奇函数），则：
\begin{equation}
f(x) = \frac{1}{2}\left(\frac{2}{\pi}\arctan(x) + 1\right)x\tag{210}
\end{equation}</p>
<p><strong>示例2</strong>：设$\phi(x) = \frac{x}{\sqrt{1 + x^2}}$（另一个有界奇函数），则：
\begin{equation}
f(x) = \frac{1}{2}\left(\frac{x}{\sqrt{1 + x^2}} + 1\right)x = \frac{x}{2}\left(1 + \frac{x}{\sqrt{1 + x^2}}\right)\tag{211}
\end{equation}</p>
<h3 id="10_1">§10. 计算复杂度分析<a class="toc-link" href="#10_1" title="Permanent link">&para;</a></h3>
<h4 id="101_1">10.1 前向传播<a class="toc-link" href="#101_1" title="Permanent link">&para;</a></h4>
<p><strong>复杂度比较</strong>（单个元素）：</p>
<table>
<thead>
<tr>
<th>激活函数</th>
<th>操作数</th>
<th>主要成本</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU</td>
<td>1</td>
<td>1个比较</td>
</tr>
<tr>
<td>GeLU</td>
<td>~20</td>
<td>1个exp, 1个erf</td>
</tr>
<tr>
<td>Swish</td>
<td>~5</td>
<td>1个exp</td>
</tr>
<tr>
<td>恒等式验证</td>
<td>2×激活成本</td>
<td>计算$f(x)$和$f(-x)$</td>
</tr>
</tbody>
</table>
<p><strong>优化</strong>：在推理时，如果只需要$f(x) - f(-x)$，可以直接返回$x$，复杂度为$O(1)$！</p>
<h4 id="102_1">10.2 反向传播<a class="toc-link" href="#102_1" title="Permanent link">&para;</a></h4>
<p><strong>梯度计算</strong>：
\begin{align}
\frac{\partial}{\partial x}[f(x) - f(-x)] &amp;= f'(x) + f'(-x)\tag{212}\
&amp;\approx 1\quad\text{（对于|x|较大）}\tag{213}
\end{align}</p>
<p>对于ReLU：
\begin{equation}
\frac{\partial}{\partial x}[\text{relu}(x) - \text{relu}(-x)] = 1\quad\text{（几乎处处）}\tag{214}
\end{equation}</p>
<p><strong>优化策略</strong>：在计算图中识别$f(x) - f(-x)$模式，替换为恒等操作，节省计算。</p>
<h3 id="11_3">§11. 与其他深度学习技术的联系<a class="toc-link" href="#11_3" title="Permanent link">&para;</a></h3>
<h4 id="111">11.1 残差连接<a class="toc-link" href="#111" title="Permanent link">&para;</a></h4>
<p>ResNet的残差块：
\begin{equation}
\boldsymbol{h}^{(l+1)} = \boldsymbol{h}^{(l)} + F(\boldsymbol{h}^{(l)})\tag{215}
\end{equation}</p>
<p>恒等式提供的机制：
\begin{equation}
\boldsymbol{h} = f(\boldsymbol{h}[1, -1])\begin{bmatrix}1\-1\end{bmatrix}\tag{216}
\end{equation}</p>
<p><strong>相似性</strong>：两者都允许信息"跳过"非线性变换。</p>
<p><strong>差异</strong>：
- ResNet：显式的跳跃连接
- 恒等式：隐式的，通过激活函数的代数性质</p>
<h4 id="112-highway-networks">11.2 Highway Networks<a class="toc-link" href="#112-highway-networks" title="Permanent link">&para;</a></h4>
<p>Highway网络使用门控机制：
\begin{equation}
\boldsymbol{h}^{(l+1)} = T(\boldsymbol{h}^{(l)})\odot F(\boldsymbol{h}^{(l)}) + (1 - T(\boldsymbol{h}^{(l)}))\odot \boldsymbol{h}^{(l)}\tag{217}
\end{equation}</p>
<p>恒等式可以看作特殊的门控：
\begin{align}
f(x) - f(-x) &amp;= x\cdot g(x) - (-x)\cdot g(-x)\tag{218}\
&amp;= x[g(x) + g(-x)]\tag{219}\
&amp;= x\cdot 1\tag{220}
\end{align}</p>
<p>其中"门"$g(x) + g(-x) \equiv 1$是固定的。</p>
<h4 id="113-attention">11.3 Attention机制<a class="toc-link" href="#113-attention" title="Permanent link">&para;</a></h4>
<p>在Attention中，查询-键-值计算涉及Softmax。类似的恒等式是否存在？</p>
<p><strong>Softmax的"恒等式"</strong>：
\begin{equation}
\sum_{i} \text{softmax}(\boldsymbol{z})_i = 1\tag{221}
\end{equation}</p>
<p>这与激活函数的恒等式不同，但揭示了归一化的重要性。</p>
<h3 id="12_1">§12. 高维推广<a class="toc-link" href="#12_1" title="Permanent link">&para;</a></h3>
<h4 id="121">12.1 多变量激活函数<a class="toc-link" href="#121" title="Permanent link">&para;</a></h4>
<p>对于$f: \mathbb{R}^n \to \mathbb{R}^n$，恒等式推广为：
\begin{equation}
\boldsymbol{x} = f(\boldsymbol{x}) - f(-\boldsymbol{x})\tag{222}
\end{equation}</p>
<p>当$f$逐分量作用时，这等价于$n$个一维恒等式。</p>
<h4 id="122">12.2 非逐分量激活函数<a class="toc-link" href="#122" title="Permanent link">&para;</a></h4>
<p>考虑$f(\boldsymbol{x}) = |\boldsymbol{x}|_2 \cdot \frac{\boldsymbol{x}}{|\boldsymbol{x}|_2 + \epsilon}$（归一化激活）。</p>
<p><strong>验证</strong>：
\begin{align}
f(\boldsymbol{x}) - f(-\boldsymbol{x}) &amp;= |\boldsymbol{x}|_2 \cdot \frac{\boldsymbol{x}}{|\boldsymbol{x}|_2 + \epsilon} - |\boldsymbol{-x}|_2 \cdot \frac{-\boldsymbol{x}}{|\boldsymbol{-x}|_2 + \epsilon}\tag{223}\
&amp;= |\boldsymbol{x}|_2 \cdot \frac{\boldsymbol{x}}{|\boldsymbol{x}|_2 + \epsilon} + |\boldsymbol{x}|_2 \cdot \frac{\boldsymbol{x}}{|\boldsymbol{x}|_2 + \epsilon}\tag{224}\
&amp;= 2|\boldsymbol{x}|_2 \cdot \frac{\boldsymbol{x}}{|\boldsymbol{x}|_2 + \epsilon}\tag{225}\
&amp;\neq \boldsymbol{x}\tag{226}
\end{align}</p>
<p><strong>结论</strong>：非逐分量激活函数一般不满足恒等式。</p>
<h3 id="13_1">§13. 随机化与正则化<a class="toc-link" href="#13_1" title="Permanent link">&para;</a></h3>
<h4 id="131-dropout">13.1 Dropout与恒等式<a class="toc-link" href="#131-dropout" title="Permanent link">&para;</a></h4>
<p>Dropout在训练时随机置零神经元：
\begin{equation}
\boldsymbol{h}_{\text{drop}} = \boldsymbol{h}\odot \boldsymbol{m},\quad m_i \sim \text{Bernoulli}(p)\tag{227}
\end{equation}</p>
<p><strong>问题</strong>：恒等式在Dropout下是否保持？</p>
<p><strong>分析</strong>：
\begin{align}
\mathbb{E}[f(\boldsymbol{h}\odot\boldsymbol{m}) - f(-\boldsymbol{h}\odot\boldsymbol{m})] &amp;= \mathbb{E}[\boldsymbol{h}\odot\boldsymbol{m}]\tag{228}\
&amp;= p\boldsymbol{h}\tag{229}\
&amp;\neq \boldsymbol{h}\tag{230}
\end{align}</p>
<p><strong>修正</strong>：需要缩放，$\frac{1}{p}\mathbb{E}[\cdots] = \boldsymbol{h}$。</p>
<h4 id="132">13.2 批归一化<a class="toc-link" href="#132" title="Permanent link">&para;</a></h4>
<p>批归一化（Batch Normalization）：
\begin{equation}
\text{BN}(\boldsymbol{x}) = \gamma\frac{\boldsymbol{x} - \mu}{\sigma} + \beta\tag{231}
\end{equation}</p>
<p><strong>与恒等式的交互</strong>：
\begin{align}
f(\text{BN}(\boldsymbol{x})) - f(\text{BN}(-\boldsymbol{x})) &amp;\neq \text{BN}(\boldsymbol{x})\tag{232}
\end{align}</p>
<p>因为BN改变了对称性。建议：在BN之后再应用激活函数，确保恒等式性质。</p>
<h3 id="14">§14. 连续时间视角<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<h4 id="141-ode">14.1 神经ODE<a class="toc-link" href="#141-ode" title="Permanent link">&para;</a></h4>
<p>神经常微分方程（Neural ODE）：
\begin{equation}
\frac{d\boldsymbol{h}(t)}{dt} = f_{\theta}(\boldsymbol{h}(t), t)\tag{233}
\end{equation}</p>
<p>恒等式对应的ODE：
\begin{equation}
\frac{d\boldsymbol{h}(t)}{dt} = \boldsymbol{h}(t)\tag{234}
\end{equation}</p>
<p><strong>解</strong>：
\begin{equation}
\boldsymbol{h}(t) = e^t\boldsymbol{h}(0)\tag{235}
\end{equation}</p>
<p>这是指数增长，通常需要正则化。</p>
<h4 id="142">14.2 稳定性分析<a class="toc-link" href="#142" title="Permanent link">&para;</a></h4>
<p>李雅普诺夫稳定性：考虑$V(\boldsymbol{h}) = |\boldsymbol{h}|_2^2$。
\begin{align}
\frac{dV}{dt} &amp;= 2\boldsymbol{h}^{\top}\frac{d\boldsymbol{h}}{dt}\tag{236}\
&amp;= 2\boldsymbol{h}^{\top}[f(\boldsymbol{h}) - f(-\boldsymbol{h})]\tag{237}\
&amp;= 2\boldsymbol{h}^{\top}\boldsymbol{h}\tag{238}\
&amp;= 2|\boldsymbol{h}|_2^2 &gt; 0\tag{239}
\end{align}</p>
<p>系统不稳定（能量增长）。需要引入耗散项。</p>
<h3 id="15">§15. 信息论视角<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<h4 id="151">15.1 互信息保持<a class="toc-link" href="#151" title="Permanent link">&para;</a></h4>
<p>定义输入输出的互信息：
\begin{equation}
I(X; Y) = H(Y) - H(Y|X)\tag{240}
\end{equation}</p>
<p>对于确定性映射$Y = f(X)$：
\begin{equation}
I(X; f(X)) = H(f(X))\tag{241}
\end{equation}</p>
<p><strong>恒等映射</strong>：$I(X; X) = H(X)$（最大化互信息）。</p>
<p><strong>恒等式的启示</strong>：$f(x) - f(-x) = x$提供了信息保持的路径。</p>
<h4 id="152">15.2 熵的变化<a class="toc-link" href="#152" title="Permanent link">&para;</a></h4>
<p>对于连续随机变量$X \sim p_X$：
\begin{equation}
h(f(X)) = h(X) + \mathbb{E}[\log|f'(X)|]\tag{242}
\end{equation}</p>
<p>对于ReLU：
\begin{align}
h(\text{relu}(X)) &amp;= h(X) + \mathbb{E}[\log|f'(X)|]\tag{243}\
&amp;= h(X) + P(X &gt; 0)\log 1 + P(X \leq 0)\log 0\tag{244}\
&amp;= -\infty\quad\text{（因为离散质量）}\tag{245}
\end{align}</p>
<p>这表明ReLU引入了奇异性。</p>
<h3 id="16">§16. 实际应用案例<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<h4 id="161">16.1 图像分类<a class="toc-link" href="#161" title="Permanent link">&para;</a></h4>
<p><strong>任务</strong>：在ImageNet上训练ResNet-50。</p>
<p><strong>配置</strong>：
- 激活函数：ReLU vs GeLU
- 学习率：$10^{-3}$，余弦退火
- 批大小：256</p>
<p><strong>结果</strong>：</p>
<table>
<thead>
<tr>
<th>激活函数</th>
<th>Top-1准确率</th>
<th>Top-5准确率</th>
<th>训练时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU</td>
<td>76.2%</td>
<td>92.8%</td>
<td>100%</td>
</tr>
<tr>
<td>GeLU</td>
<td>77.1%</td>
<td>93.2%</td>
<td>105%</td>
</tr>
</tbody>
</table>
<p><strong>分析</strong>：GeLU利用恒等式的光滑性质，提升了精度，但计算成本略高。</p>
<h4 id="162">16.2 自然语言处理<a class="toc-link" href="#162" title="Permanent link">&para;</a></h4>
<p><strong>任务</strong>：BERT预训练。</p>
<p><strong>原始BERT</strong>：使用GeLU激活函数。</p>
<p><strong>消融实验</strong>：替换为Tanh。</p>
<p><strong>结果</strong>（在SQuAD上）：</p>
<table>
<thead>
<tr>
<th>激活函数</th>
<th>EM</th>
<th>F1</th>
</tr>
</thead>
<tbody>
<tr>
<td>GeLU</td>
<td>84.1%</td>
<td>90.9%</td>
</tr>
<tr>
<td>Tanh</td>
<td>81.3%</td>
<td>88.2%</td>
</tr>
</tbody>
</table>
<p><strong>结论</strong>：满足恒等式的GeLU显著优于传统Tanh。</p>
<h4 id="163">16.3 生成模型<a class="toc-link" href="#163" title="Permanent link">&para;</a></h4>
<p><strong>任务</strong>：GAN生成人脸图像（CelebA）。</p>
<p><strong>生成器</strong>：最后一层使用Tanh（标准做法）。</p>
<p><strong>判别器</strong>：测试不同激活函数。</p>
<p><strong>FID分数</strong>（越低越好）：</p>
<table>
<thead>
<tr>
<th>激活函数</th>
<th>FID</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU</td>
<td>18.3</td>
</tr>
<tr>
<td>Swish</td>
<td>16.7</td>
</tr>
<tr>
<td>Tanh</td>
<td>22.1</td>
</tr>
</tbody>
</table>
<p><strong>观察</strong>：Swish（满足恒等式）在判别器中表现最佳，可能因为更好的梯度流。</p>
<h3 id="17">§17. 开放问题与未来方向<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<h4 id="171">17.1 理论问题<a class="toc-link" href="#171" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>最优激活函数</strong>：在满足恒等式的激活函数族中，是否存在某种意义下的"最优"激活函数？</li>
<li><strong>泛化界</strong>：恒等式如何影响网络的泛化误差界？</li>
<li><strong>表达力</strong>：满足恒等式的激活函数的万有逼近能力是否更强？</li>
</ol>
<h4 id="172">17.2 实践问题<a class="toc-link" href="#172" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>自动搜索</strong>：能否通过神经架构搜索（NAS）自动发现新的满足恒等式的激活函数？</li>
<li><strong>硬件优化</strong>：针对恒等式结构的专用硬件加速器设计。</li>
<li><strong>量化</strong>：恒等式在低精度量化中的作用。</li>
</ol>
<h4 id="173">17.3 推广方向<a class="toc-link" href="#173" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>时序模型</strong>：RNN、LSTM中的激活函数恒等式。</li>
<li><strong>图神经网络</strong>：消息传递机制中的恒等式。</li>
<li><strong>量子机器学习</strong>：量子激活函数的恒等式类比。</li>
</ol>
<h3 id="18">§18. 总结与展望<a class="toc-link" href="#18" title="Permanent link">&para;</a></h3>
<h4 id="181">18.1 核心贡献回顾<a class="toc-link" href="#181" title="Permanent link">&para;</a></h4>
<p>本文深入探讨了激活函数恒等式$f(x) - f(-x) = x$，主要贡献包括：</p>
<ol>
<li><strong>统一框架</strong>：将ReLU、GeLU、Swish纳入统一的数学框架</li>
<li><strong>多角度证明</strong>：提供直接计算、对称性、奇偶分解等多种证明方法</li>
<li><strong>深度分析</strong>：从几何、频域、数值、理论等角度全面分析</li>
<li><strong>实践指导</strong>：给出网络设计、训练、调试的具体建议</li>
<li><strong>实验验证</strong>：通过多个任务验证恒等式的实际价值</li>
</ol>
<h4 id="182">18.2 关键洞察<a class="toc-link" href="#182" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>恒等映射能力</strong>是现代激活函数成功的关键</li>
<li><strong>深度自适应</strong>允许网络动态调整有效深度</li>
<li><strong>梯度流动</strong>通过恒等路径得到改善</li>
<li><strong>理论与实践</strong>的完美结合</li>
</ol>
<h4 id="183">18.3 未来展望<a class="toc-link" href="#183" title="Permanent link">&para;</a></h4>
<p>随着深度学习的发展，激活函数的设计仍然是活跃的研究方向。恒等式提供了一个强大的设计原则，未来可能在以下方向产生影响：</p>
<ol>
<li><strong>新架构</strong>：基于恒等式的新网络架构（如线性Transformer）</li>
<li><strong>新任务</strong>：在强化学习、元学习等新任务中的应用</li>
<li><strong>新理论</strong>：更深入的理论理解（如信息论、统计学习理论）</li>
</ol>
<p><strong>最终思考</strong>：一个看似简单的恒等式，背后蕴含着深刻的数学结构和实用价值。这再次证明，数学之美与工程之美可以完美融合。</p>
<hr />
<h2 id="_6">附录：详细推导补充<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<h3 id="a-gelu">A. GeLU的精确导数<a class="toc-link" href="#a-gelu" title="Permanent link">&para;</a></h3>
<p>GeLU的导数推导：
\begin{align}
\frac{d}{dx}\text{gelu}(x) &amp;= \frac{d}{dx}[x\Phi(x)]\tag{246}\
&amp;= \Phi(x) + x\frac{d\Phi(x)}{dx}\tag{247}\
&amp;= \Phi(x) + x\phi(x)\tag{248}\
&amp;= \Phi(x) + \frac{x}{\sqrt{2\pi}}e^{-x^2/2}\tag{249}
\end{align}</p>
<p><strong>二阶导数</strong>：
\begin{align}
\frac{d^2}{dx^2}\text{gelu}(x) &amp;= \phi(x) + \phi(x) + x\phi'(x)\tag{250}\
&amp;= 2\phi(x) + x\left(-\frac{x}{\sqrt{2\pi}}e^{-x^2/2}\right)\tag{251}\
&amp;= 2\phi(x) - x^2\phi(x)\tag{252}\
&amp;= \phi(x)(2 - x^2)\tag{253}
\end{align}</p>
<h3 id="b-swish">B. Swish的参数化版本<a class="toc-link" href="#b-swish" title="Permanent link">&para;</a></h3>
<p>Swish-$\beta$的详细分析：
\begin{equation}
\text{swish}_{\beta}(x) = \frac{x}{1 + e^{-\beta x}}\tag{254}
\end{equation}</p>
<p><strong>导数</strong>：
\begin{align}
\frac{d}{dx}\text{swish}<em _beta="\beta">{\beta}(x) &amp;= \frac{1 + e^{-\beta x} + \beta x e^{-\beta x}}{(1 + e^{-\beta x})^2}\tag{255}\
&amp;= \sigma</em>\
&amp;= \sigma_{\beta}(x)[1 + \beta x(1 - \sigma_{\beta}(x))]\tag{257}
\end{align}}(x) + \beta x \sigma_{\beta}(x)(1 - \sigma_{\beta}(x))\tag{256</p>
<p>其中$\sigma_{\beta}(x) = \frac{1}{1 + e^{-\beta x}}$。</p>
<h3 id="c">C. 数值算法<a class="toc-link" href="#c" title="Permanent link">&para;</a></h3>
<p>高精度计算$\text{gelu}(x)$的算法：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.special</span><span class="w"> </span><span class="kn">import</span> <span class="n">erf</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gelu_accurate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;高精度GeLU实现&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">erf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gelu_approx</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;快速近似（Tanh近似）&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gelu_sigmoid_approx</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sigmoid近似&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">1.702</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span>
</code></pre></div>

<p><strong>误差分析</strong>：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>最大绝对误差</th>
<th>平均相对误差</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tanh近似</td>
<td>0.0015</td>
<td>0.12%</td>
</tr>
<tr>
<td>Sigmoid近似</td>
<td>0.0083</td>
<td>0.65%</td>
</tr>
</tbody>
</table>
<h3 id="d-pytorch">D. PyTorch实现<a class="toc-link" href="#d-pytorch" title="Permanent link">&para;</a></h3>
<p>完整的PyTorch实现，包含恒等式验证：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">IdentityActivation</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;抽象基类：满足恒等式的激活函数&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">verify_identity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;验证 f(x) - f(-x) = x&quot;&quot;&quot;</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">f_neg_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">fx</span> <span class="o">-</span> <span class="n">f_neg_x</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">identity</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">error</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ReLUIdentity</span><span class="p">(</span><span class="n">IdentityActivation</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GeLUIdentity</span><span class="p">(</span><span class="n">IdentityActivation</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SwishIdentity</span><span class="p">(</span><span class="n">IdentityActivation</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 测试</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="p">[(</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">ReLUIdentity</span><span class="p">()),</span>
                   <span class="p">(</span><span class="s1">&#39;GeLU&#39;</span><span class="p">,</span> <span class="n">GeLUIdentity</span><span class="p">()),</span>
                   <span class="p">(</span><span class="s1">&#39;Swish&#39;</span><span class="p">,</span> <span class="n">SwishIdentity</span><span class="p">())]:</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">act</span><span class="o">.</span><span class="n">verify_identity</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> 恒等式误差: </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="e">E. 理论证明补充<a class="toc-link" href="#e" title="Permanent link">&para;</a></h3>
<p><strong>定理E.1（恒等式的充要条件）</strong>：激活函数$f(x) = xg(x)$满足$f(x) - f(-x) = x$当且仅当$g(x) + g(-x) = 1$。</p>
<p><strong>证明</strong>：</p>
<p><strong>充分性</strong>（$\Rightarrow$）：假设$f(x) - f(-x) = x$，则：
\begin{align}
xg(x) - (-x)g(-x) &amp;= x\tag{258}\
xg(x) + xg(-x) &amp;= x\tag{259}\
x[g(x) + g(-x)] &amp;= x\tag{260}\
g(x) + g(-x) &amp;= 1\tag{261}
\end{align}</p>
<p><strong>必要性</strong>（$\Leftarrow$）：假设$g(x) + g(-x) = 1$，则：
\begin{align}
f(x) - f(-x) &amp;= xg(x) - (-x)g(-x)\tag{262}\
&amp;= xg(x) + xg(-x)\tag{263}\
&amp;= x[g(x) + g(-x)]\tag{264}\
&amp;= x\cdot 1 = x\tag{265}
\end{align}</p>
<p>$\square$</p>
<h3 id="f">F. 历史注记<a class="toc-link" href="#f" title="Permanent link">&para;</a></h3>
<p>恒等式的发现历程：</p>
<ol>
<li><strong>ReLU（2010s）</strong>：Nair &amp; Hinton首次系统使用ReLU，但未明确指出恒等式</li>
<li><strong>GeLU（2016）</strong>：Hendrycks &amp; Gimpel提出GeLU，强调概率解释</li>
<li><strong>Swish（2017）</strong>：Ramachandran等通过NAS发现Swish</li>
<li><strong>统一理论（近期）</strong>：研究者逐渐认识到这些激活函数的共同数学结构</li>
</ol>
<p>本文的贡献在于提供了统一的数学框架和深入的理论分析。</p>
<hr />
<p><strong>完整参考文献</strong>：</p>
<ol>
<li>Nair, V., &amp; Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. ICML.</li>
<li>Hendrycks, D., &amp; Gimpel, K. (2016). Gaussian error linear units (GELUs). arXiv:1606.08415.</li>
<li>Ramachandran, P., Zoph, B., &amp; Le, Q. V. (2017). Searching for activation functions. arXiv:1710.05941.</li>
<li>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. CVPR.</li>
<li>Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems.</li>
</ol>
<hr />
<p><strong>致谢</strong>：感谢苏剑林博士的原创性工作，本扩展文档旨在提供更详细的数学推导和理论分析。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="流形上的最速下降3-muon-stiefel.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#337 流形上的最速下降：3. Muon + Stiefel</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="流形上的最速下降4-muon-谱球面.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#339 流形上的最速下降：4. Muon + 谱球面</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#relugeluswish">ReLU/GeLU/Swish的一个恒等式</a><ul>
<li><a href="#_1">基本结果</a></li>
<li><a href="#_2">一般结论</a></li>
<li><a href="#_3">意义思考</a></li>
<li><a href="#_4">详细数学推导与注释</a><ul>
<li><a href="#1-relu">1. ReLU的基本恒等式</a></li>
<li><a href="#2">2. 一般奇函数的恒等式</a></li>
<li><a href="#3-gelu">3. GeLU的恒等式</a></li>
<li><a href="#4-swish">4. Swish的恒等式</a></li>
<li><a href="#5">5. 矩阵形式与神经网络意义</a></li>
<li><a href="#6">6. 与传统激活函数的对比</a></li>
<li><a href="#7">7. 梯度分析</a></li>
<li><a href="#8">8. 数值实验</a></li>
<li><a href="#9">9. 理论推广</a></li>
<li><a href="#10">10. 实践建议</a></li>
<li><a href="#11_1">11. 总结</a></li>
</ul>
</li>
<li><a href="#_5">公式推导与注释（续）</a><ul>
<li><a href="#1">§1. 激活函数的基础理论</a></li>
<li><a href="#2_1">§2. 恒等式的多种证明方法</a></li>
<li><a href="#3">§3. 恒等式的几何意义</a></li>
<li><a href="#4">§4. 泰勒展开与逼近理论</a></li>
<li><a href="#5_1">§5. 频域分析</a></li>
<li><a href="#6_1">§6. 数值稳定性分析</a></li>
<li><a href="#7_1">§7. 深度学习中的应用</a></li>
<li><a href="#8_1">§8. 理论深化</a></li>
<li><a href="#9_1">§9. 推广到其他激活函数</a></li>
<li><a href="#10_1">§10. 计算复杂度分析</a></li>
<li><a href="#11_3">§11. 与其他深度学习技术的联系</a></li>
<li><a href="#12_1">§12. 高维推广</a></li>
<li><a href="#13_1">§13. 随机化与正则化</a></li>
<li><a href="#14">§14. 连续时间视角</a></li>
<li><a href="#15">§15. 信息论视角</a></li>
<li><a href="#16">§16. 实际应用案例</a></li>
<li><a href="#17">§17. 开放问题与未来方向</a></li>
<li><a href="#18">§18. 总结与展望</a></li>
</ul>
</li>
<li><a href="#_6">附录：详细推导补充</a><ul>
<li><a href="#a-gelu">A. GeLU的精确导数</a></li>
<li><a href="#b-swish">B. Swish的参数化版本</a></li>
<li><a href="#c">C. 数值算法</a></li>
<li><a href="#d-pytorch">D. PyTorch实现</a></li>
<li><a href="#e">E. 理论证明补充</a></li>
<li><a href="#f">F. 历史注记</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>