<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下） | ML & Math Blog Posts</title>
    <meta name="description" content="生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）&para;
原文链接: https://spaces.ac.cn/archives/10567
发布日期: 

继续回到我们的扩散系列。在《生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）》中，我们介绍了SiD（Score identity Distillation），这是一种不需要真实数据、也不需要从教师模型采样的扩散模型蒸馏方案，其形式类似...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=生成模型">生成模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #298 生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#298</span>
                生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-11-22</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=扩散" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 扩散</span>
                </a>
                
                <a href="../index.html?tags=去噪" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 去噪</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10567">https://spaces.ac.cn/archives/10567</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>继续回到我们的扩散系列。在<a href="/archives/10085">《生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上）》</a>中，我们介绍了SiD（Score identity Distillation），这是一种不需要真实数据、也不需要从教师模型采样的扩散模型蒸馏方案，其形式类似GAN，但有着比GAN更好的训练稳定性。</p>
<p>SiD的核心是通过恒等变换来为学生模型构建更好的损失函数，这一点是开创性的，同时也遗留了一些问题。比如，SiD对损失函数的恒等变换是不完全的，如果完全变换会如何？如何从理论上解释SiD引入的$\lambda$的必要性？上个月放出的<a href="https://papers.cool/arxiv/2410.19310">《Flow Generator Matching》</a>（简称FGM）成功从更本质的梯度角度解释了$\lambda=0.5$的选择，而受到FGM启发，笔者则进一步发现了$\lambda = 1$的一种解释。</p>
<p>接下来我们将详细介绍SiD的上述理论进展。</p>
<h2 id="_2">思想回顾<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>根据上一篇文章的介绍，我们知道SiD实现蒸馏的思想是“相近的分布，它们训练出来的去噪模型也是相近的”，用公式表示就是<br />
\begin{align}
&amp;\text{教师扩散模型:}\quad\boldsymbol{\varphi}^<em> = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\varphi}} \mathbb{E}</em><em _boldsymbol_varphi="\boldsymbol{\varphi">0\sim \tilde{p}(\boldsymbol{x}_0),\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\Vert\boldsymbol{\epsilon}</em> \\[8pt]}}(\boldsymbol{x}_t,t) - \boldsymbol{\varepsilon}\Vert^2\right]\label{eq:tloss
&amp;\text{学生扩散模型:}\quad\boldsymbol{\psi}^</em> = \mathop{\text{argmin}}<em _boldsymbol_z="\boldsymbol{z">{\boldsymbol{\psi}} \mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\Vert\boldsymbol{\epsilon<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{\psi}}(\boldsymbol{x}_t^{(g)},t) - \boldsymbol{\varepsilon}\Vert^2\right]\label{eq:dloss}\\[8pt]
&amp;\text{学生生成模型:}\quad\boldsymbol{\theta}^<em> = \mathop{\text{argmin}}<em _boldsymbol_z="\boldsymbol{z">{\boldsymbol{\theta}} \underbrace{\mathbb{E}</em>^},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\Vert\boldsymbol{\epsilon}_{\boldsymbol{\varphi</em>}(\boldsymbol{x}_t^{(g)},t) - \boldsymbol{\epsilon}</em>}^*}(\boldsymbol{x<em _mathcal_L="\mathcal{L">t^{(g)},t)\Vert^2\right]}</em><em _boldsymbol_theta="\boldsymbol{\theta">1}\label{eq:gloss-1}
\end{align}<br />
这里记号比较多，我们逐一解释。第一个损失函数就是我们要蒸馏的扩散模型的训练目标，其中$\boldsymbol{x}_t = \bar{\alpha}_t\boldsymbol{x}_0 + \bar{\beta}_t\boldsymbol{\varepsilon}$代表加噪样本，$\bar{\alpha}_t,\bar{\beta}_t$是noise schedule，$\boldsymbol{x}_0$是训练样本；第二个损失函数则是用学生模型生成的数据来训练的扩散模型，其中$\boldsymbol{x}_t^{(g)}=\bar{\alpha}_t\boldsymbol{g}</em>}}(\boldsymbol{z}) + \bar{\beta<em _boldsymbol_theta="\boldsymbol{\theta">t\boldsymbol{\varepsilon}$，这里的$\boldsymbol{g}</em>$；第三个损失函数，则是试图通过拉近真实数据和学生数据所训练的扩散模型的差距，来训练学生生成模型（生成器）。}}(\boldsymbol{z})$代表学生模型的生成样本，也记为$\boldsymbol{x}_0^{(g)</p>
<p>这里的教师模型是可以提前训练好的，而两个学生模型的训练只需要教师模型本身，并不需要用到训练教师模型的数据，所以作为一种蒸馏方式来看SiD是data-free的；两个学生模型则是类似GAN那样的交替训练，逐步提高生成器的生成质量。就笔者所阅读过的文献来看，这种训练思想最早出自论文<a href="https://papers.cool/arxiv/2001.02728">《Learning Generative Models using Denoising Density Estimators》</a>，我们在<a href="/archives/7038">《从去噪自编码器到生成模型》</a>也有过相关介绍。</p>
<p>然而，尽管看上去没什么毛病，但实际情况是式$\eqref{eq:dloss}$和式$\eqref{eq:gloss-1}$的交替训练非常容易崩溃，以至于几乎不能出效果。这是因为理论和实践上的两个gap：</p>
<blockquote>
<p>1、理论上要求先求出式$\eqref{eq:dloss}$的最优解，然后才去优化式$\eqref{eq:gloss-1}$，但实际上从训练成本考虑，我们并没有将它训练到最优就去优化式$\eqref{eq:gloss-1}$了；</p>
<p>2、理论上$\boldsymbol{\psi}^<em>$随$\boldsymbol{\theta}$而变，即应该写成$\boldsymbol{\psi}^</em>(\boldsymbol{\theta})$，从而在优化式$\eqref{eq:gloss-1}$时应该多出一项$\boldsymbol{\psi}^<em>(\boldsymbol{\theta})$对$\boldsymbol{\theta}$的梯度，但实际上在优化式$\eqref{eq:gloss-1}$时我们都只当$\boldsymbol{\psi}^</em>$是常数。</p>
</blockquote>
<p>第1个问题其实还好，因为随着训练的推进$\boldsymbol{\psi}$总能慢慢逼近理论最优的$\boldsymbol{\psi}^*$，但第2个问题非常困难且本质，可以说GAN的训练不稳定性同样也有这个问题的“功劳”。而SiD和FGM的核心贡献，正是试图解决第2个问题。</p>
<h2 id="_3">恒等变换<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>SiD的想法是通过恒等变换来减少生成器损失函数$\eqref{eq:gloss-1}$对$\boldsymbol{\psi}^*$的依赖，从而弱化第2个问题。这一想法确实是开创性的，后面已经有不少工作围绕着SiD展开，包括下面要介绍的FGM也算是其中之一。</p>
<p>恒等变换的核心，是如下恒等式：<br />
\begin{equation}\mathbb{E}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{x}_0\sim \tilde{p}(\boldsymbol{x}_0),\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t,t)\right\rangle\right] = \mathbb{E}</em><em _boldsymbol_varphi="\boldsymbol{\varphi">0\sim \tilde{p}(\boldsymbol{x}_0),\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\varepsilon}\right\rangle\right]\label{eq:id}\end{equation}<br />
简单来说就是$\boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_varphi="\boldsymbol{\varphi">t,t)$可以替换成$\boldsymbol{\varepsilon}$。这里的$\boldsymbol{\epsilon}</em>}^*}(\boldsymbol{x<em>t,t)$是式$\eqref{eq:tloss}$的理论最优解，而$\boldsymbol{f}(\boldsymbol{x}_t,t)$是任意 _只依赖于$\boldsymbol{x}_t$和$t$</em> 的向量函数。注意“只依赖于$\boldsymbol{x}_t$和$t$”是恒等式成立的必要条件，一旦$\boldsymbol{f}$掺杂了独立的$\boldsymbol{x}_0$或$\boldsymbol{\varepsilon}$，那么恒等式就未必成立了，所以应用该恒等式之前需要仔细检查这一点。</p>
<p>上一篇文章我们已经给出了该恒等式的证明，不过现在看来那个证明显得有点迂回，这里给出一个更直接点的证明：</p>
<blockquote>
<p><strong>证明：</strong> 将目标$\eqref{eq:tloss}$等价地改写成<br />
 \begin{equation}\boldsymbol{\varphi}^<em> = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\varphi}} \mathbb{E}</em><em _boldsymbol_varepsilon="\boldsymbol{\varepsilon">t\sim p(\boldsymbol{x}_t)}\Big[\mathbb{E}</em>}\sim p(\boldsymbol{\varepsilon}|\boldsymbol{x<em _boldsymbol_varphi="\boldsymbol{\varphi">t)}\left[\Vert\boldsymbol{\epsilon}</em>}}(\boldsymbol{x<em _boldsymbol_mu="\boldsymbol{\mu">t,t) - \boldsymbol{\varepsilon}\Vert^2\right]\Big]\end{equation}<br />
 根据$\mathbb{E}[\boldsymbol{x}] = \mathop{\text{argmin}}\limits</em>}}\mathbb{E<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{x}}\left[\Vert \boldsymbol{\mu} - \boldsymbol{x}\Vert^2\right]$（不熟悉可以求导证一下），我们可以得出上式的理论最优解是<br />
 \begin{equation}\boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_varepsilon="\boldsymbol{\varepsilon">t,t) = \mathbb{E}</em>}\sim p(\boldsymbol{\varepsilon}|\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t)}[\boldsymbol{\varepsilon}]\end{equation}<br />
 所以<br />
 \begin{equation}\begin{aligned}
 \mathbb{E}</em><em _boldsymbol_varphi="\boldsymbol{\varphi">0\sim \tilde{p}(\boldsymbol{x}_0),\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t,t)\right\rangle\right]=&amp;\, \mathbb{E}</em><em _boldsymbol_varphi="\boldsymbol{\varphi">t\sim p(\boldsymbol{x}_t)}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t,t)\right\rangle\right] \\
 =&amp;\, \mathbb{E}</em><em _boldsymbol_varepsilon="\boldsymbol{\varepsilon">t\sim p(\boldsymbol{x}_t)}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \mathbb{E}</em>}\sim p(\boldsymbol{\varepsilon}|\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t)}[\boldsymbol{\varepsilon}]\right\rangle\right] \\
 =&amp;\, \mathbb{E}</em><em _boldsymbol_x="\boldsymbol{x">t\sim p(\boldsymbol{x}_t),\boldsymbol{\varepsilon}\sim p(\boldsymbol{\varepsilon}|\boldsymbol{x}_t)}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\varepsilon}\right\rangle\right] \\
 =&amp;\, \mathbb{E}</em>\right\rangle\right]}_0\sim \tilde{p}(\boldsymbol{x}_0),\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\varepsilon
 \end{aligned}\end{equation}<br />
 证毕。证明过程的“必经之路”是第一个等号，这需要用到“$\boldsymbol{f}(\boldsymbol{x}_t,t)$只依赖于$\boldsymbol{x}_t$和$t$”这个条件。</p>
</blockquote>
<p>恒等式$\eqref{eq:id}$的关键是$\boldsymbol{\epsilon}<em _boldsymbol_z="\boldsymbol{z">{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t,t)$的最优性，而目标$\eqref{eq:tloss}$和$\eqref{eq:dloss}$形式是一样的，所以同样的结论也适用于$\boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t,t)$，利用它我们就可以将$\eqref{eq:gloss-1}$变换成<br />
\begin{equation}\begin{aligned}
&amp;\,\mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\Vert\boldsymbol{\epsilon<em _boldsymbol_z="\boldsymbol{z">{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t) - \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t)\Vert^2\right] \\[8pt]
=&amp;\,\mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\bigg[\Big\langle\boldsymbol{\epsilon<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t) - \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t),\boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t) - \underbrace{\boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}<em _text_可以替换为="\text{可以替换为">t^{(g)},t)}</em>\Big\rangle\bigg] \\[5pt]}\boldsymbol{\varepsilon}
=&amp;\,\mathbb{E}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\left\langle\boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t) - \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_varphi="\boldsymbol{\varphi">t^{(g)},t),\boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}_t^{(g)},t) - \boldsymbol{\varepsilon}\right\rangle\right]\triangleq \mathcal{L}_2
\end{aligned}\label{eq:gloss-2}\end{equation}<br />
最后的形式就是SiD所提的生成器损失函数$\mathcal{L}_2$，它是SiD成功训练的关键，我们可以理解为它通过恒等变换提前预估了$\boldsymbol{\psi}^</em>$的值，同时弱化了对$\boldsymbol{\psi}^*$的依赖，从而以它为损失函数训练生成器比$\mathcal{L}_1$有着更好的效果。</p>
<p>SiD的遗留问题是：</p>
<blockquote>
<p>1、$\mathcal{L}<em _boldsymbol_z="\boldsymbol{z">2$的恒等变换并不彻底，将$\mathcal{L}_2$展开会发现里边还有一项$\mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle\boldsymbol{\epsilon<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t),\boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t)\rangle]$，这一项的$\boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t)$同样可以替换为$\boldsymbol{\varepsilon}$，那么问题就是完整的变换即下式会是一个比$\mathcal{L}_2$更好的选择吗？<br />
 \begin{equation}\mathcal{L}_3 = \mathbb{E}</em>^},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\Vert\boldsymbol{\epsilon}_{\boldsymbol{\varphi</em>}\Vert^2 - 2\langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t),\boldsymbol{\varepsilon}\rangle + \langle \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t)\boldsymbol{\varepsilon}\rangle\right]\label{eq:gloss-3}\end{equation}</p>
<p>2、实际上SiD最终用的损失不是$\mathcal{L}_2$也不是$\mathcal{L}_1$，而是$\mathcal{L}_2 - \lambda\mathcal{L}_1$，其中$\lambda &gt; 0$，并且实验发现$\lambda$的最优值在$1$附近，某些任务甚至在$\lambda=1.2$表现最好，这是非常让人困惑的，因为$\mathcal{L}_1,\mathcal{L}_2$是理论上相等的，所以$\lambda &gt; 1$似乎在反向优化$\mathcal{L}_1$？这不就跟出发点相反了？显然这迫切需要一个理论解释。</p>
</blockquote>
<h2 id="_4">直面梯度<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>再来回顾一下，我们面临的根本困难是：理论上$\boldsymbol{\psi}^<em>$是$\boldsymbol{\theta}$的函数，所以我们在求$\nabla_{\boldsymbol{\theta}} \mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">1$或$\nabla</em>}} \mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">2$时，需要想办法求$\nabla</em>^}}\boldsymbol{\psi</em>$，但实践中我们至多可以得到$\mathcal{L}<em _boldsymbol_psi="\boldsymbol{\psi">i^{\color{skyblue}{(\text{sg})}} \triangleq \mathcal{L}_i|</em>^<em> \to \color{skyblue}{\text{sg}[}\boldsymbol{\psi}^</em>\color{skyblue}{]}}$，其中$\color{skyblue}{\text{sg}}$是stop gradient的意思，即无法获取$\boldsymbol{\psi}^*$关于$\boldsymbol{\theta}$的梯度，所以不论$\mathcal{L}_1,\mathcal{L}_2,\mathcal{L}_3$，它们在实践中的梯度都是有偏的。</p>
<p>这时候就轮到FGM登场了，它的想法更贴近本质：损失$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">1,\mathcal{L}_2,\mathcal{L}_3$都只关注到了损失层面的相等性，但对于优化器来说我们需要的是梯度层面的相等，所以我们需要想办法找一个新的损失函数$\mathcal{L}_4$，使得它满足<br />
\begin{equation}\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">4(\boldsymbol{\theta}, \color{skyblue}{\text{sg}[}\boldsymbol{\psi}^<em>\color{skyblue}{]})= \nabla_{\boldsymbol{\theta}}\mathcal{L}_{1/2/3}(\boldsymbol{\theta}, \boldsymbol{\psi}^</em>)\end{equation}<br />
即$\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">4^{\color{skyblue}{(\text{sg})}} = \nabla</em>_4$为损失函数时，就可以实现无偏的优化效果了。}}\mathcal{L}_{1/2/3}$，那么以$\mathcal{L</p>
<p>FGM的推导同样基于恒等式$\eqref{eq:id}$，不过它的原始推导有点繁琐，对于本文来说可以直接从$\mathcal{L}<em _boldsymbol_z="\boldsymbol{z">3$即式$\eqref{eq:gloss-3}$出发，它跟$\boldsymbol{\psi}^<em>$相关的项就只剩下$\mathbb{E}<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t),\boldsymbol{\varepsilon}\rangle]$，我们直接把它的梯度算出来，方法将“先恒等变换后求梯度”和“先求梯度后恒等变换”分别应用于$\mathbb{E}</em>,t)\Vert^2]$操作一遍，对比它们的结果。},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\Vert\boldsymbol{\epsilon}_{\boldsymbol{\psi}^*}(\boldsymbol{x}_t^{(g)</p>
<p>先恒等变换后求梯度：<br />
\begin{equation}\begin{aligned}
&amp;\,\nabla_{\boldsymbol{\theta}}\mathbb{E}<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\Vert\boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)},t)\Vert^2] \\[5pt]
=&amp;\, \nabla</em>}}\mathbb{E<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t),\boldsymbol{\varepsilon}\rangle] = \mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon<em _boldsymbol_z="\boldsymbol{z">{\boldsymbol{\psi}^<em>}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t),\boldsymbol{\varepsilon}\rangle] \\[5pt]
=&amp;\, \mathbb{E}</em>^},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\color{skyblue}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}_t^{(g)},t),\boldsymbol{\varepsilon}\rangle] + \mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon<em _boldsymbol_z="\boldsymbol{z">{\boldsymbol{\psi}^<em>}(\color{skyblue}{\text{sg}[}\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}\color{skyblue}{]},t),\boldsymbol{\varepsilon}\rangle]
\end{aligned}\label{eq:g-grad-1}\end{equation}<br />
先求梯度后恒等变换：<br />
\begin{equation}\begin{aligned}
&amp;\,\nabla</em>}}\mathbb{E<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\Vert\boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t)\Vert^2] \\[8pt]
=&amp;\, \mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\nabla_{\boldsymbol{\theta}}\Vert\boldsymbol{\epsilon<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{\psi}^<em>}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t)\Vert^2] = 2\mathbb{E}</em>^},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\boldsymbol{\psi</em>}(\boldsymbol{x}_t^{(g)},t), \boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t)\rangle] \\[8pt]
=&amp;\, 2\mathbb{E}</em>^},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\color{skyblue}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t), \boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t)\rangle] + \underbrace{2\mathbb{E}</em>^},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\boldsymbol{\psi</em>}(\color{skyblue}{\text{sg}[}\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}\color{skyblue}{]},t), \boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _text_可以应用式="\text{可以应用式">t^{(g)},t)\rangle]}</em> \\[5pt]}\eqref{eq:id}
=&amp;\, 2\mathbb{E}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle\nabla</em>^}}\boldsymbol{\epsilon}_{\color{skyblue}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t), \boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t)\rangle] + 2\mathbb{E}</em>^},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\boldsymbol{\psi</em>}(\color{skyblue}{\text{sg}[}\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}\color{skyblue}{]},t), \boldsymbol{\varepsilon}\rangle]
\end{aligned}\label{eq:g-grad-2}\end{equation}<br />
这里要注意第三个等号，只有$\boldsymbol{\epsilon}</em>^<em>}(\color{skyblue}{\text{sg}[}\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}\color{skyblue}{]},t)$这一项才可以应用恒等式$\eqref{eq:id}$，因为$\nabla</em>^}}\boldsymbol{\epsilon}_{\color{skyblue}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}_t^{(g)},t)$的$\boldsymbol{x}_t^{(g)}$要对$\boldsymbol{\theta}$求梯度，求完梯度后就不一定是$\boldsymbol{x}_t^{(g)}$的函数了，所以不满足应用式$\eqref{eq:id}$的条件。</p>
<p>现在对于$\nabla_{\boldsymbol{\theta}}\mathbb{E}<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\Vert\boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)},t)\Vert^2]$我们有两个结果，将式$\eqref{eq:g-grad-1}$乘以2然后减去式$\eqref{eq:g-grad-2}$得到<br />
\begin{equation}\begin{aligned}
&amp;\,\nabla</em>}}\mathbb{E<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)},t),\boldsymbol{\varepsilon}\rangle] = \nabla</em>}}\mathbb{E<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\Vert\boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t)\Vert^2] = \eqref{eq:g-grad-1}\times 2 - \eqref{eq:g-grad-2} \\[5pt]
=&amp;\,2 \mathbb{E}</em>^},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\color{skyblue}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t),\boldsymbol{\varepsilon}\rangle] - 2\mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon<em _boldsymbol_theta="\boldsymbol{\theta">{\color{skyblue}{\text{sg}[}\boldsymbol{\psi}^<em>\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t), \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t)\rangle] \\[5pt]
=&amp;\,2 \nabla</em>}}\mathbb{E<em _color_skyblue="\color{skyblue">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \boldsymbol{\epsilon}</em>^}{\text{sg}[}\boldsymbol{\psi<em>\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)},t),\boldsymbol{\varepsilon}\rangle] - \nabla</em>}}\mathbb{E<em _color_skyblue="\color{skyblue">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\Vert\boldsymbol{\epsilon}</em>^}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)},t)\Vert^2] \\[5pt]
=&amp;\,\nabla</em>}}\mathbb{E<em _color_skyblue="\color{skyblue">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[2\langle \boldsymbol{\epsilon}</em>^}{\text{sg}[}\boldsymbol{\psi<em>\color{skyblue}{]}}(\boldsymbol{x}<em _color_skyblue="\color{skyblue">t^{(g)},t),\boldsymbol{\varepsilon}\rangle - \Vert\boldsymbol{\epsilon}</em>^}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_z="\boldsymbol{z">t^{(g)},t)\Vert^2]
\end{aligned}\end{equation}<br />
留意最后被求梯度的式子，它所有的$\boldsymbol{\psi}^<em>$都被加上了$\color{skyblue}{\text{sg}}$，说明我们不用设法求它关于$\boldsymbol{\theta}$的梯度了，但它的梯度等于$\mathbb{E}<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}[\langle \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t),\boldsymbol{\varepsilon}\rangle]$的准确梯度，所以用它来替换掉$\mathcal{L}_3$的对应项，我们就得到了$\mathcal{L}_4$：<br />
\begin{equation}\mathcal{L}_4^{\color{skyblue}{(\text{sg})}} = \mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})}\left[\Vert\boldsymbol{\epsilon<em _color_skyblue="\color{skyblue">{\boldsymbol{\varphi}^<em>}\Vert^2 - 2\langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>}(\boldsymbol{x}_t^{(g)},t),\boldsymbol{\varepsilon}\rangle + 2\langle \boldsymbol{\epsilon}</em>^}{\text{sg}[}\boldsymbol{\psi<em>\color{skyblue}{]}}(\boldsymbol{x}<em _color_skyblue="\color{skyblue">t^{(g)},t),\boldsymbol{\varepsilon}\rangle - \Vert\boldsymbol{\epsilon}</em>^}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)},t)\Vert^2\right]\end{equation}<br />
这就是FGM的最终结果，它只依赖于$\color{skyblue}{\text{sg}[}\boldsymbol{\psi}^*\color{skyblue}{]}$，但成立$\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">4^{\color{skyblue}{(\text{sg})}}=\nabla</em>)$，所以FGM相当于从梯度角度肯定了SiD的$\lambda=0.5$的选择。}}\mathcal{L}_{1/2/3}$。再仔细观察一下，就会发现成立$\mathcal{L}_4^{\color{skyblue}{(\text{sg})}}=2\mathcal{L}_2^{\color{skyblue}{(\text{sg})}}-\mathcal{L}_1^{\color{skyblue}{(\text{sg})}}=2(\mathcal{L}_2^{\color{skyblue}{(\text{sg})}}-0.5\times \mathcal{L}_1^{\color{skyblue}{(\text{sg})}</p>
<p>顺便说一下，FGM原论文的描述是在ODE式扩散框架（flow matching）内进行的，但正如笔者在上一篇文章所说，不管是SiD还是FGM，它实际并没有用到扩散模型的迭代生成过程，而是只用到了扩散模型所训练的去噪模型，所以不管是ODE、SDE还是DDPM框架都只是表象，它的去噪模型才是本质，所以本文可以接着上一篇SiD的记号来介绍FGM。</p>
<h2 id="_5">广义散度<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>FGM已经成功地求出了最本质的梯度，但这只能解释SiD的$\lambda=0.5$，这意味着如果我们需要解释其他$\lambda$值的可行性，就必须修改出发点了。为此，我们回到原点，反思一下生成器的目标$\eqref{eq:gloss-1}$。</p>
<p>熟悉扩散模型的读者应该都知道，式$\eqref{eq:tloss}$的理论最优解还可以写成$\boldsymbol{\epsilon}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t,t)=-\bar{\beta}_t\nabla</em><em _boldsymbol_psi="\boldsymbol{\psi">t}\log p(\boldsymbol{x}_t)$，同理式$\eqref{eq:dloss}$的最优解则是$\boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t)=-\bar{\beta}_t\nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)})$，这里的$p(\boldsymbol{x}_t)$、$p</em>)$分别是真实数据、生成器数据加噪的分布，如果不了解这个结果，可以参考}}(\boldsymbol{x}_t^{(g)<a href="/archives/9209">《生成扩散模型漫谈（五）：一般框架之SDE篇》</a>、<a href="/archives/9509">《生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配》</a>等介绍。</p>
<p>将这两个理论最优解代回式$\eqref{eq:gloss-1}$，我们会发现生成器实际上在试图最小化Fisher散度：<br />
\begin{equation}\begin{aligned}
\mathcal{F}(p, p_{\boldsymbol{\theta}}) =&amp;\, \mathbb{E}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})} \left[\Vert \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p</em>}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t^{(g)}) - \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p(\boldsymbol{x}_t^{(g)})\Vert^2\right] \\
=&amp;\, \int p</em>}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t^{(g)}) \left\Vert \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p</em>}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t^{(g)}) - \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p(\boldsymbol{x}_t^{(g)})\right\Vert^2 d\boldsymbol{x}_t^{(g)}
\end{aligned}\end{equation}<br />
我们要反思的事情，就是Fisher散度的合理性和改进点。可以看到，Fisher散度里边$p</em>$出现了两次，现在我们来请读者思考一个问题：}<strong>这两处$p_{\boldsymbol{\theta}}$中哪一处更重要呢？</strong></p>
<p>答案是<strong>第二处</strong> 。为了理解这个事实，我们不妨考虑两种情况： <em _boldsymbol_theta="\boldsymbol{\theta">1、固定第一处$p</em>$。}}$，只优化第二处$p_{\boldsymbol{\theta}}$；2、固定第二处$p_{\boldsymbol{\theta}}$，只优化第一处$p_{\boldsymbol{\theta}<em _boldsymbol_theta="\boldsymbol{\theta"> 它们的结果有什么区别呢？第一种情况大概率不会有什么变化，即依然能学到$p</em>=p$，事实上由于Fisher散度带有$\Vert\Vert^2$，所以下面更一般的结论几乎是显然成立的：}</p>
<blockquote>
<p>只要$r(\boldsymbol{x})$是一个处处不为零的分布，那么$p(\boldsymbol{x})=q(\boldsymbol{x})$依然是如下广义Fisher散度的理论最优解： \begin{equation}\mathcal{F}(p,q|r) = \int r(\boldsymbol{x}) \Vert \nabla_{\boldsymbol{x}} p(\boldsymbol{x}) - \nabla_{\boldsymbol{x}} q(\boldsymbol{x})\Vert^2 d\boldsymbol{x}\end{equation}</p>
</blockquote>
<p>说简单点，就是第一处$p_{\boldsymbol{\theta}}$根本不重要，换成其他分布都行，单靠$\Vert\Vert^2$就能保证两个分布相等。但第二种情况就不一样了，固定第二处$p_{\boldsymbol{\theta}}$只优化第一处$p_{\boldsymbol{\theta}}$的理论最优解是<br />
\begin{equation}p_{\boldsymbol{\theta}}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t^{(g)}) = \delta(\boldsymbol{x}_t^{(g)} - \boldsymbol{x}_t^<em>),\quad \boldsymbol{x}_t^</em> = \mathop{\text{argmin}}</em><em _boldsymbol_x="\boldsymbol{x">t^{(g)}} \,\left\Vert \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p</em>}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t^{(g)}) - \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p(\boldsymbol{x}_t^{(g)})\right\Vert^2\end{equation}<br />
其中$\delta$是狄拉克delta分布，即模型只需要生成让$\Vert\Vert^2$最小的那个样本，就可以让损失最小，这说白了就是模式坍缩（Mode Collapse）！所以，Fisher散度中的第一处$p</em>$的作用不单单是次要的，甚至还可能是负面的。}</p>
<p>这启发我们，当我们使用基于梯度的优化器来训练模型时，第一处$p_{\boldsymbol{\theta}}$的梯度干脆不要还会更好，即下述形式的Fisher散度是一个更好的选择<br />
\begin{equation}\begin{aligned}
\mathcal{F}^+(p, p_{\boldsymbol{\theta}}) =&amp;\, \int p_{\color{skyblue}{\text{sg}[}\boldsymbol{\theta}\color{skyblue}{]}}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t^{(g)}) \left\Vert \nabla</em><em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p</em>}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t^{(g)}) - \nabla</em><em _boldsymbol_z="\boldsymbol{z">t^{(g)}}\log p(\boldsymbol{x}_t^{(g)})\right\Vert^2 d\boldsymbol{x}_t^{(g)} \\[5pt]
=&amp;\, \mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})} \left[\Vert \nabla_{\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}}\log p</em>}}(\color{skyblue}{\text{sg}[}\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t^{(g)}\color{skyblue}{]}) - \nabla</em><em _boldsymbol_z="\boldsymbol{z">t^{(g)}}\log p(\color{skyblue}{\text{sg}[}\boldsymbol{x}_t^{(g)}\color{skyblue}{]})\Vert^2\right] \\[5pt]
\propto&amp;\, \underbrace{\mathbb{E}</em>},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})} \left[\Vert \boldsymbol{\epsilon<em _mathcal_L="\mathcal{L">{\boldsymbol{\varphi}^<em>}(\color{skyblue}{\text{sg}[}\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}\color{skyblue}{]},t) - \boldsymbol{\epsilon}</em>^</em>}(\color{skyblue}{\text{sg}[}\boldsymbol{x}_t^{(g)}\color{skyblue}{]},t)\Vert^2\right]}</em><em _boldsymbol_theta="\boldsymbol{\theta">5}
\end{aligned}\end{equation}<br />
也就是说，这里的$\mathcal{L}_5$极有可能会是一个比$\mathcal{L}_1$更好的出发点，它数值上跟$\mathcal{L}_1$是相等的，但少了一部分梯度：<br />
\begin{equation}\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">5 = \nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">1 - \nabla</em>}}\underbrace{\mathbb{E<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{z},\boldsymbol{\varepsilon}\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})} \left[\Vert \boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _color_skyblue="\color{skyblue">t^{(g)},t) - \boldsymbol{\epsilon}</em>^}{\text{sg}[}\boldsymbol{\psi</em>\color{skyblue}{]}}(\boldsymbol{x}<em _text_刚好是="\text{刚好是">t^{(g)},t)\Vert^2\right]}</em>}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">1^{\color{skyblue}{(\text{sg})}}}\end{equation}<br />
其中$\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">1$已经由FGM算出来了，它等于$\nabla</em>$作为额外的惩罚项，进一步降低模式坍缩的风险，当然这里真就是单纯的惩罚项，所以权重就不能太大了，根据SiD的实验结果，$\lambda=1.5$的时候已经开始训崩了。}}(2\mathcal{L}_2^{\color{skyblue}{(\text{sg})}}-\mathcal{L}_1^{\color{skyblue}{(\text{sg})}})$，因此以$\mathcal{L}_5$为出发点，我们实践中的损失函数是$2\mathcal{L}_2^{\color{skyblue}{(\text{sg})}}-\mathcal{L}_1^{\color{skyblue}{(\text{sg})}}-\mathcal{L}_1^{\color{skyblue}{(\text{sg})}}=2(\mathcal{L}_2^{\color{skyblue}{(\text{sg})}}-\mathcal{L}_1^{\color{skyblue}{(\text{sg})}})$，这就解释了$\lambda=1$的选择。至于$\lambda$稍大于1的选择，则更为极端一些，它相当于在$\mathcal{L}_5$的基础上将$-\mathcal{L}_1^{\color{skyblue}{(\text{sg})}</p>
<p>顺便说一下，FGM之前作者还有个作品<a href="https://papers.cool/arxiv/2410.16794">《One-Step Diffusion Distillation through Score Implicit Matching》</a>，里边也提出了类似的对第一处$p_{\boldsymbol{\theta}}$改为$p_{\color{skyblue}{\text{sg}[}\boldsymbol{\theta}\color{skyblue}{]}}$的做法，但没有明确地从Fisher散度的原始形式出发讨论该操作的合理性，稍欠完整。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文介绍了SiD（Score identity Distillation）的后续理论进展，主要内容是从梯度视角解释了SiD中的$\lambda$参数设置，核心部分是由FGM（Flow Generator Matching）发现的准确估计SiD梯度的巧妙思路，这肯定了$\lambda=0.5$的选择，在此基础上，笔者拓展了Fisher散度的概念，从而解释了$\lambda=1$的取值。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10567">https://spaces.ac.cn/archives/10567</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Nov. 22, 2024). 《生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下） 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10567">https://spaces.ac.cn/archives/10567</a></p>
<p>@online{kexuefm-10567,<br />
title={生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）},<br />
author={苏剑林},<br />
year={2024},<br />
month={Nov},<br />
url={\url{https://spaces.ac.cn/archives/10567}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>本节对SiD和FGM的核心数学原理进行详细推导,深入理解基于恒等式的扩散模型蒸馏方法。</p>
<h3 id="sid">一、SiD基础框架回顾<a class="toc-link" href="#sid" title="Permanent link">&para;</a></h3>
<h4 id="11">1.1 三个核心损失函数<a class="toc-link" href="#11" title="Permanent link">&para;</a></h4>
<p><strong>教师扩散模型</strong>的训练目标:
\begin{equation}
\boldsymbol{\varphi}^* = \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\varphi}} \mathbb{E}</em><em _boldsymbol_varphi="\boldsymbol{\varphi">0\sim \tilde{p},\boldsymbol{\varepsilon}\sim\mathcal{N}}\left[\Vert\boldsymbol{\epsilon}</em>
\end{equation}}}(\boldsymbol{x}_t,t) - \boldsymbol{\varepsilon}\Vert^2\right] \tag{1.1</p>
<p>其中$\boldsymbol{x}_t = \bar{\alpha}_t\boldsymbol{x}_0 + \bar{\beta}_t\boldsymbol{\varepsilon}$。</p>
<p><strong>学生扩散模型</strong>的训练目标:
\begin{equation}
\boldsymbol{\psi}^* = \mathop{\text{argmin}}<em _boldsymbol_z="\boldsymbol{z">{\boldsymbol{\psi}} \mathbb{E}</em>
\end{equation}},\boldsymbol{\varepsilon}}\left[\Vert\boldsymbol{\epsilon}_{\boldsymbol{\psi}}(\boldsymbol{x}_t^{(g)},t) - \boldsymbol{\varepsilon}\Vert^2\right] \tag{1.2</p>
<p>其中$\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)} = \bar{\alpha}_t\boldsymbol{g}</em>$。}}(\boldsymbol{z}) + \bar{\beta}_t\boldsymbol{\varepsilon</p>
<p><strong>朴素生成器损失</strong> $\mathcal{L}<em _boldsymbol_z="\boldsymbol{z">1$:
\begin{equation}
\mathcal{L}_1 = \mathbb{E}</em>^},\boldsymbol{\varepsilon}}\left[\Vert\boldsymbol{\epsilon}_{\boldsymbol{\varphi<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)},t) - \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)},t)\Vert^2\right] \tag{1.3}
\end{equation}</p>
<h4 id="12">1.2 训练不稳定性的根源<a class="toc-link" href="#12" title="Permanent link">&para;</a></h4>
<p><strong>问题1 - 未达最优</strong>:
实践中$\boldsymbol{\psi}$并未达到理论最优$\boldsymbol{\psi}^*$即开始优化生成器。</p>
<p><strong>问题2 - 梯度缺失</strong>:
理论上$\boldsymbol{\psi}^<em>$依赖于$\boldsymbol{\theta}$,应记为$\boldsymbol{\psi}^</em>(\boldsymbol{\theta})$。优化$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">1$时应包含:
\begin{equation}
\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">1 = \nabla</em>}}\mathcal{L<em _boldsymbol_psi="\boldsymbol{\psi">1\big|</em>^<em>\text{固定}} + \underbrace{\frac{\partial\mathcal{L}_1}{\partial\boldsymbol{\psi}^</em>}\frac{\partial\boldsymbol{\psi}^*}{\partial\boldsymbol{\theta}}}_{\text{实践中缺失}} \tag{1.4}
\end{equation}</p>
<h3 id="_8">二、恒等式的严格证明<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<h4 id="21">2.1 核心恒等式陈述<a class="toc-link" href="#21" title="Permanent link">&para;</a></h4>
<p><strong>定理</strong>: 设$\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^*}$是公式(1.1)的最优解,$\boldsymbol{f}(\boldsymbol{x}_t,t)$是仅依赖于$(\boldsymbol{x}_t,t)$的任意向量函数,则:</p>
<p>\begin{equation}
\mathbb{E}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{x}_0,\boldsymbol{\varepsilon}}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\epsilon}</em>}^*}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t,t)\right\rangle\right] = \mathbb{E}</em>
\end{equation}}_0,\boldsymbol{\varepsilon}}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\varepsilon}\right\rangle\right] \tag{2.1</p>
<h4 id="22">2.2 详细证明<a class="toc-link" href="#22" title="Permanent link">&para;</a></h4>
<p><strong>证明</strong>:</p>
<p><strong>步骤1</strong> - 改写优化目标:
\begin{align}
\boldsymbol{\varphi}^* &amp;= \mathop{\text{argmin}}<em _boldsymbol_x="\boldsymbol{x">{\boldsymbol{\varphi}} \mathbb{E}</em><em _boldsymbol_varphi="\boldsymbol{\varphi">0,\boldsymbol{\varepsilon}}\left[\Vert\boldsymbol{\epsilon}</em>}}(\boldsymbol{x<em _boldsymbol_varphi="\boldsymbol{\varphi">t,t) - \boldsymbol{\varepsilon}\Vert^2\right] \tag{2.2}\
&amp;= \mathop{\text{argmin}}</em>}} \mathbb{E<em _boldsymbol_varepsilon="\boldsymbol{\varepsilon">{\boldsymbol{x}_t}\Big[\mathbb{E}</em>}|\boldsymbol{x<em _boldsymbol_varphi="\boldsymbol{\varphi">t}\left[\Vert\boldsymbol{\epsilon}</em>
\end{align}}}(\boldsymbol{x}_t,t) - \boldsymbol{\varepsilon}\Vert^2\right]\Big] \tag{2.3</p>
<p><strong>步骤2</strong> - 应用MSE最优性:</p>
<p>对固定的$\boldsymbol{x}<em _boldsymbol_varepsilon="\boldsymbol{\varepsilon">t$,最小化$\mathbb{E}</em>}|\boldsymbol{x<em _boldsymbol_varphi="\boldsymbol{\varphi">t}[\Vert\boldsymbol{\epsilon}</em>\Vert^2]$的最优解是:
\begin{equation}
\boldsymbol{\epsilon}}} - \boldsymbol{\varepsilon<em _boldsymbol_varepsilon="\boldsymbol{\varepsilon">{\boldsymbol{\varphi}^*}(\boldsymbol{x}_t,t) = \mathbb{E}</em>
\end{equation}}|\boldsymbol{x}_t}[\boldsymbol{\varepsilon}] \tag{2.4</p>
<p>这是因为对任意随机变量$X$和常数$c$:
\begin{equation}
\mathop{\text{argmin}}_c \mathbb{E}[(X-c)^2] = \mathbb{E}[X] \tag{2.5}
\end{equation}</p>
<p><strong>步骤3</strong> - 应用条件期望性质:</p>
<p>利用公式(2.4):
\begin{align}
&amp;\mathbb{E}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{x}_0,\boldsymbol{\varepsilon}}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t,t)\right\rangle\right] \tag{2.6}\
=&amp;\, \mathbb{E}</em><em _boldsymbol_varphi="\boldsymbol{\varphi">t}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t,t)\right\rangle\right] \tag{2.7}\
=&amp;\, \mathbb{E}</em><em _boldsymbol_varepsilon="\boldsymbol{\varepsilon">t}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \mathbb{E}</em>}|\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t}[\boldsymbol{\varepsilon}]\right\rangle\right] \tag{2.8}\
=&amp;\, \mathbb{E}</em><em _boldsymbol_varepsilon="\boldsymbol{\varepsilon">t}\left[\mathbb{E}</em>}|\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\varepsilon}\right\rangle\right]\right] \tag{2.9}\
=&amp;\, \mathbb{E}</em><em _boldsymbol_x="\boldsymbol{x">t,\boldsymbol{\varepsilon}|\boldsymbol{x}_t}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\varepsilon}\right\rangle\right] \tag{2.10}\
=&amp;\, \mathbb{E}</em>
\end{align}}_0,\boldsymbol{\varepsilon}}\left[\left\langle\boldsymbol{f}(\boldsymbol{x}_t,t), \boldsymbol{\varepsilon}\right\rangle\right] \tag{2.11</p>
<p><strong>关键</strong>: 从(2.8)到(2.9)利用了$\boldsymbol{f}$只依赖$\boldsymbol{x}_t$的假设,可以移出条件期望。$\square$</p>
<h4 id="23">2.3 恒等式的适用条件<a class="toc-link" href="#23" title="Permanent link">&para;</a></h4>
<p>恒等式成立的<strong>必要条件</strong>:
1. $\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^<em>}$必须是最优解(或足够接近)
2. $\boldsymbol{f}(\boldsymbol{x}_t,t)$</em><em>不能</em>*显式依赖$\boldsymbol{x}_0$或$\boldsymbol{\varepsilon}$</p>
<p><strong>反例</strong>: 如果$\boldsymbol{f} = \boldsymbol{x}<em _boldsymbol_varphi="\boldsymbol{\varphi">0$,则:
\begin{align}
\text{左边} &amp;= \mathbb{E}[\langle\boldsymbol{x}_0, \boldsymbol{\epsilon}</em>
\end{align}}^*}(\boldsymbol{x}_t)\rangle] \neq \mathbb{E}[\langle\boldsymbol{x}_0, \boldsymbol{\varepsilon}\rangle] = 0 = \text{右边} \tag{2.12</p>
<h3 id="sid_1">三、SiD的恒等变换推导<a class="toc-link" href="#sid_1" title="Permanent link">&para;</a></h3>
<h4 id="31-mathcall_1mathcall_2">3.1 从$\mathcal{L}_1$到$\mathcal{L}_2$<a class="toc-link" href="#31-mathcall_1mathcall_2" title="Permanent link">&para;</a></h4>
<p><strong>原始损失</strong>:
\begin{equation}
\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">1 = \mathbb{E}\left[\Vert\boldsymbol{\epsilon}</em>^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}) - \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)})\Vert^2\right] \tag{3.1}
\end{equation}</p>
<p><strong>步骤1</strong> - 展开平方:
\begin{align}
\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">1 &amp;= \mathbb{E}\Big[\langle\boldsymbol{\epsilon}</em>^<em>} - \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}, \boldsymbol{\epsilon}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{\varphi}^<em>} - \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\rangle\Big] \tag{3.2}\
&amp;= \mathbb{E}\Big[\Vert\boldsymbol{\epsilon}</em>^<em>}\Vert^2 - 2\langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>}, \boldsymbol{\epsilon}_{\boldsymbol{\psi}^<em>}\rangle + \Vert\boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\Vert^2\Big] \tag{3.3}
\end{align}</p>
<p><strong>步骤2</strong> - 检查恒等式适用性:</p>
<p>在第二项中:
- $\boldsymbol{f} = \boldsymbol{\epsilon}<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}) - \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)})$
- 该函数仅依赖$\boldsymbol{x}_t^{(g)}$,满足条件!
- 可对$\boldsymbol{\epsilon}</em>$应用恒等式}^*</p>
<p><strong>步骤3</strong> - 应用恒等式:
\begin{align}
&amp;\mathbb{E}\left[\langle\boldsymbol{\epsilon}<em _boldsymbol_psi="\boldsymbol{\psi">{\boldsymbol{\varphi}^<em>} - \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}, \boldsymbol{\epsilon}</em>^<em>}\rangle\right] \tag{3.4}\
=&amp;\, \mathbb{E}\left[\langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>} - \boldsymbol{\epsilon}_{\boldsymbol{\psi}^*}, \boldsymbol{\varepsilon}\rangle\right] \tag{3.5}
\end{align}</p>
<p><strong>最终结果</strong> $\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">2$:
\begin{align}
\mathcal{L}_2 &amp;= \mathbb{E}\Big[\Vert\boldsymbol{\epsilon}</em>^<em>}\Vert^2 - 2\langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>} - \boldsymbol{\epsilon}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{\psi}^<em>}, \boldsymbol{\varepsilon}\rangle + \Vert\boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\Vert^2\Big] \tag{3.6}\
&amp;= \mathbb{E}\Big[\langle\boldsymbol{\epsilon}</em>^<em>} - \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}, \boldsymbol{\epsilon}_{\boldsymbol{\varphi}^*} - \boldsymbol{\varepsilon}\rangle\Big] \tag{3.7}
\end{align}</p>
<h4 id="32-mathcall_2">3.2 为什么$\mathcal{L}_2$更好?<a class="toc-link" href="#32-mathcall_2" title="Permanent link">&para;</a></h4>
<p><strong>直觉理解</strong>:</p>
<p>$\mathcal{L}_2$将对$\boldsymbol{\psi}^<em>$的依赖从$\boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}$替换为$\boldsymbol{\varepsilon}$:
- $\boldsymbol{\varepsilon}$是已知的随机噪声
- 不需要计算$\boldsymbol{\psi}^*$关于$\boldsymbol{\theta}$的梯度
- 部分解决了问题2</p>
<p><strong>数学分析</strong>:</p>
<p>展开$\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">2$:
\begin{align}
\mathcal{L}_2 &amp;= \mathbb{E}\Big[\langle\boldsymbol{\epsilon}</em>^<em>}, \boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>} - \boldsymbol{\varepsilon}\rangle - \langle\boldsymbol{\epsilon}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\boldsymbol{\psi}^<em>}, \boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>} - \boldsymbol{\varepsilon}\rangle\Big] \tag{3.8}\
&amp;= \underbrace{\mathbb{E}[\Vert\boldsymbol{\epsilon}</em>^<em>}\Vert^2 - \langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>}, \boldsymbol{\varepsilon}\rangle]}<em _boldsymbol_varphi="\boldsymbol{\varphi">{\text{与}\boldsymbol{\psi}^<em>\text{无关}} - \mathbb{E}[\langle\boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}, \boldsymbol{\epsilon}</em>
\end{align}}^*} - \boldsymbol{\varepsilon}\rangle] \tag{3.9</p>
<h3 id="fgm">四、FGM的梯度精确估计<a class="toc-link" href="#fgm" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 问题的精确陈述<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<p><strong>目标</strong>: 找到损失$\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">4^{(\text{sg})}$使得:
\begin{equation}
\nabla</em>^}}\mathcal{L}_4(\boldsymbol{\theta}, \text{sg}[\boldsymbol{\psi<em>]) = \nabla_{\boldsymbol{\theta}}\mathcal{L}_{1/2/3}(\boldsymbol{\theta}, \boldsymbol{\psi}^</em>) \tag{4.1}
\end{equation}</p>
<p>其中$\text{sg}[\cdot]$表示停止梯度(stop gradient)。</p>
<h4 id="42-mathcall_3">4.2 完整变换的$\mathcal{L}_3$<a class="toc-link" href="#42-mathcall_3" title="Permanent link">&para;</a></h4>
<p>首先完成$\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">2$的剩余变换:
\begin{equation}
\mathcal{L}_3 = \mathbb{E}\left[\Vert\boldsymbol{\epsilon}</em>^<em>}\Vert^2 - 2\langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>}, \boldsymbol{\varepsilon}\rangle + \langle\boldsymbol{\epsilon}_{\boldsymbol{\psi}^*}, \boldsymbol{\varepsilon}\rangle\right] \tag{4.2}
\end{equation}</p>
<p>这里将$\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">2$中的$\langle\boldsymbol{\epsilon}</em>^<em>}, \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\rangle$也替换为$\langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^*}, \boldsymbol{\varepsilon}\rangle$。</p>
<h4 id="43">4.3 先恒等变换后求梯度<a class="toc-link" href="#43" title="Permanent link">&para;</a></h4>
<p>对$\mathbb{E}[\Vert\boldsymbol{\epsilon}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\psi}^<em>}\Vert^2]$应用恒等式后求导:
\begin{align}
&amp;\nabla_{\boldsymbol{\theta}}\mathbb{E}[\Vert\boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\Vert^2] \tag{4.3}\
=&amp;\, \nabla</em>^}}\mathbb{E}[\langle\boldsymbol{\epsilon}_{\boldsymbol{\psi<em>}, \boldsymbol{\varepsilon}\rangle] \quad\text{(恒等变换)} \tag{4.4}\
=&amp;\, \mathbb{E}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}, \boldsymbol{\varepsilon}\rangle] \tag{4.5}
\end{align}</p>
<p><strong>分解梯度</strong>:
\begin{align}
\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}<em _text_对="\text{对">{\boldsymbol{\psi}^<em>}(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}) &amp;= \underbrace{\nabla</em>^}}\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi</em>]}(\boldsymbol{x}_t^{(g)})}</em>}\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t^{(g)}\text{求导}} + \underbrace{\nabla</em>^}}\boldsymbol{\epsilon}_{\boldsymbol{\psi<em>}(\text{sg}[\boldsymbol{x}<em _text_对="\text{对">t^{(g)}])}</em>^}\boldsymbol{\psi</em>\text{求导}} \tag{4.6}
\end{align}</p>
<p>因此:
\begin{equation}
\text{(4.5)} = \mathbb{E}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi}^<em>]}, \boldsymbol{\varepsilon}\rangle] + \mathbb{E}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}(\text{sg}[\boldsymbol{x}_t^{(g)}]), \boldsymbol{\varepsilon}\rangle] \tag{4.7}
\end{equation}</p>
<h4 id="44">4.4 先求梯度后恒等变换<a class="toc-link" href="#44" title="Permanent link">&para;</a></h4>
<p>直接对$\mathbb{E}[\Vert\boldsymbol{\epsilon}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\psi}^<em>}\Vert^2]$求导:
\begin{align}
&amp;\nabla_{\boldsymbol{\theta}}\mathbb{E}[\Vert\boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\Vert^2] \tag{4.8}\
=&amp;\, \mathbb{E}[2\langle\nabla</em>}}\boldsymbol{\epsilon<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\psi}^<em>}, \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\rangle] \tag{4.9}\
=&amp;\, 2\mathbb{E}[\langle\nabla</em>}}\boldsymbol{\epsilon<em _boldsymbol_theta="\boldsymbol{\theta">{\text{sg}[\boldsymbol{\psi}^<em>]}, \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\rangle] + 2\mathbb{E}[\langle\nabla</em>^}}\boldsymbol{\epsilon}_{\boldsymbol{\psi<em>}(\text{sg}[\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}]), \boldsymbol{\epsilon}</em>^</em>}\rangle] \tag{4.10}
\end{align}</p>
<p><strong>关键观察</strong>: 只有第二项可应用恒等式!
\begin{equation}
\text{(4.10)} = 2\mathbb{E}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{sg}[\boldsymbol{\psi}^<em>]}, \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\rangle] + 2\mathbb{E}[\langle\nabla</em>
\end{equation}}}\boldsymbol{\epsilon}_{\boldsymbol{\psi}^*}(\text{sg}[\boldsymbol{x}_t^{(g)}]), \boldsymbol{\varepsilon}\rangle] \tag{4.11</p>
<h4 id="45-mathcall_4">4.5 推导$\mathcal{L}_4$<a class="toc-link" href="#45-mathcall_4" title="Permanent link">&para;</a></h4>
<p>联立(4.7)和(4.11),计算$2\times$(4.7)$-$(4.11):
\begin{align}
&amp;\nabla_{\boldsymbol{\theta}}\mathbb{E}[\langle\boldsymbol{\epsilon}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\psi}^<em>}, \boldsymbol{\varepsilon}\rangle] \tag{4.12}\
=&amp;\, 2\mathbb{E}[\langle\nabla_{\boldsymbol{\theta}}\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi}^</em>]}, \boldsymbol{\varepsilon}\rangle] - 2\mathbb{E}[\langle\nabla</em>}}\boldsymbol{\epsilon<em _boldsymbol_theta="\boldsymbol{\theta">{\text{sg}[\boldsymbol{\psi}^<em>]}, \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\rangle] \tag{4.13}\
=&amp;\, 2\nabla</em>}}\mathbb{E}[\langle\boldsymbol{\epsilon<em _boldsymbol_theta="\boldsymbol{\theta">{\text{sg}[\boldsymbol{\psi}^<em>]}, \boldsymbol{\varepsilon}\rangle] - \nabla_{\boldsymbol{\theta}}\mathbb{E}[\Vert\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi}^</em>]}\Vert^2] \tag{4.14}\
=&amp;\, \nabla</em>^}}\mathbb{E}[2\langle\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi<em>]}, \boldsymbol{\varepsilon}\rangle - \Vert\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi}^</em>]}\Vert^2] \tag{4.15}
\end{align}</p>
<p><strong>最终的$\mathcal{L}_4$</strong>:</p>
<p>将(4.15)代入$\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">3$:
\begin{equation}
\mathcal{L}_4^{(\text{sg})} = \mathbb{E}\left[\Vert\boldsymbol{\epsilon}</em>^<em>}\Vert^2 - 2\langle\boldsymbol{\epsilon}_{\boldsymbol{\varphi}^</em>}, \boldsymbol{\varepsilon}\rangle + 2\langle\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi}^<em>]}, \boldsymbol{\varepsilon}\rangle - \Vert\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi}^</em>]}\Vert^2\right] \tag{4.16}
\end{equation}</p>
<h4 id="46-sid">4.6 与SiD的关系<a class="toc-link" href="#46-sid" title="Permanent link">&para;</a></h4>
<p>整理$\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">4$:
\begin{align}
\mathcal{L}_4^{(\text{sg})} &amp;= \mathbb{E}\Big[\langle\boldsymbol{\epsilon}</em>^<em>} - \boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi}^</em>]}, \boldsymbol{\epsilon}<em _text_sg="\text{sg">{\boldsymbol{\varphi}^<em>} - \boldsymbol{\varepsilon}\rangle - \langle\boldsymbol{\epsilon}_{\text{sg}[\boldsymbol{\psi}^</em>]}, \boldsymbol{\epsilon}</em>\
&amp;= 2\mathcal{L}_2^{(\text{sg})} - \mathcal{L}_1^{(\text{sg})} \tag{4.18}\
&amp;= 2(\mathcal{L}_2^{(\text{sg})} - 0.5\mathcal{L}_1^{(\text{sg})}) \tag{4.19}
\end{equation}}[\boldsymbol{\psi}^*]} - \boldsymbol{\varepsilon}\rangle\Big] \tag{4.17</p>
<p><strong>结论</strong>: FGM证明了SiD的$\lambda=0.5$选择!</p>
<h3 id="fisher">五、广义Fisher散度理论<a class="toc-link" href="#fisher" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 回顾得分匹配视角<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p>利用$\boldsymbol{\epsilon}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\varphi}^<em>} = -\bar{\beta}<em _boldsymbol_psi="\boldsymbol{\psi">t\nabla\log p$和$\boldsymbol{\epsilon}</em>^</em>} = -\bar{\beta}_t\nabla\log p</em>$:}</p>
<p>\begin{align}
\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">1 &amp;= \mathbb{E}\left[\Vert\boldsymbol{\epsilon}</em>^<em>} - \boldsymbol{\epsilon}_{\boldsymbol{\psi}^</em>}\Vert^2\right] \tag{5.1}\
&amp;= \bar{\beta}<em _boldsymbol_theta="\boldsymbol{\theta">t^2\mathbb{E}\left[\Vert\nabla\log p</em>\
&amp;= \bar{\beta}}} - \nabla\log p\Vert^2\right] \tag{5.2<em _boldsymbol_theta="\boldsymbol{\theta">t^2\int p</em>\
&amp;= \bar{\beta}}}(\boldsymbol{x})\left\Vert\nabla\log p_{\boldsymbol{\theta}} - \nabla\log p\right\Vert^2 d\boldsymbol{x} \tag{5.3<em _boldsymbol_theta="\boldsymbol{\theta">t^2 \mathcal{F}(p, p</em>
\end{align}}}) \tag{5.4</p>
<p>这就是<strong>Fisher散度</strong>。</p>
<h4 id="52-fisherp_boldsymboltheta">5.2 分析Fisher散度的两处$p_{\boldsymbol{\theta}}$<a class="toc-link" href="#52-fisherp_boldsymboltheta" title="Permanent link">&para;</a></h4>
<p>公式(5.3)中$p_{\boldsymbol{\theta}}$出现两次:
1. <strong>积分测度</strong>: $p_{\boldsymbol{\theta}}(\boldsymbol{x})d\boldsymbol{x}$
2. <strong>散度内部</strong>: $\nabla\log p_{\boldsymbol{\theta}}$</p>
<p><strong>实验</strong>: 固定其中一处,优化另一处</p>
<p><strong>实验1</strong> - 固定测度,优化散度:
\begin{equation}
\min_{p_{\boldsymbol{\theta}}} \int \underbrace{p_0(\boldsymbol{x})}<em _boldsymbol_theta="\boldsymbol{\theta">{\text{固定}} \left\Vert\nabla\log p</em>
\end{equation}}}(\boldsymbol{x}) - \nabla\log p(\boldsymbol{x})\right\Vert^2 d\boldsymbol{x} \tag{5.5</p>
<p>由于$\Vert\cdot\Vert^2$的强凸性,最优解仍是$p_{\boldsymbol{\theta}}=p$。</p>
<p><strong>实验2</strong> - 固定散度,优化测度:
\begin{equation}
\min_{p_{\boldsymbol{\theta}}} \int p_{\boldsymbol{\theta}}(\boldsymbol{x}) \underbrace{\left\Vert\nabla\log p_0(\boldsymbol{x}) - \nabla\log p(\boldsymbol{x})\right\Vert^2}_{\text{固定}=:c(\boldsymbol{x})} d\boldsymbol{x} \tag{5.6}
\end{equation}</p>
<p>最优解是:
\begin{equation}
p_{\boldsymbol{\theta}}^<em>(\boldsymbol{x}) = \delta(\boldsymbol{x} - \boldsymbol{x}^</em>), \quad \boldsymbol{x}^* = \mathop{\text{argmin}}_{\boldsymbol{x}} c(\boldsymbol{x}) \tag{5.7}
\end{equation}</p>
<p>这是<strong>模式坍缩</strong>(Mode Collapse)!</p>
<h4 id="53-fisher">5.3 广义Fisher散度<a class="toc-link" href="#53-fisher" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>: 对任意分布$r(\boldsymbol{x})$:
\begin{equation}
\mathcal{F}(p,q|r) := \int r(\boldsymbol{x}) \left\Vert\nabla\log q(\boldsymbol{x}) - \nabla\log p(\boldsymbol{x})\right\Vert^2 d\boldsymbol{x} \tag{5.8}
\end{equation}</p>
<p><strong>性质</strong>: 若$r(\boldsymbol{x}) &gt; 0$处处成立,则$\mathcal{F}(p,q|r) \geq 0$且等号成立当且仅当$p=q$。</p>
<p><strong>证明</strong>:
\begin{align}
\mathcal{F}(p,q|r) &amp;= \int r(\boldsymbol{x}) \Vert\nabla\log q - \nabla\log p\Vert^2 d\boldsymbol{x} \tag{5.9}\
&amp;= 0 \iff \nabla\log q = \nabla\log p \quad r\text{-a.e.} \tag{5.10}\
&amp;\iff q(\boldsymbol{x}) = C\cdot p(\boldsymbol{x}) \tag{5.11}
\end{align}</p>
<p>由于$\int q = \int p = 1$,得$C=1$,即$q=p$。$\square$</p>
<h4 id="54-fishermathcalf">5.4 修正的Fisher散度$\mathcal{F}^+$<a class="toc-link" href="#54-fishermathcalf" title="Permanent link">&para;</a></h4>
<p><strong>关键思想</strong>: 在第一处$p_{\boldsymbol{\theta}}$上停止梯度:
\begin{align}
\mathcal{F}^+(p, p_{\boldsymbol{\theta}}) &amp;= \int p_{\text{sg}[\boldsymbol{\theta}]}(\boldsymbol{x}) \left\Vert\nabla\log p_{\boldsymbol{\theta}}(\boldsymbol{x}) - \nabla\log p(\boldsymbol{x})\right\Vert^2 d\boldsymbol{x} \tag{5.12}\
&amp;= \mathbb{E}<em _text_sg="\text{sg">{\boldsymbol{x}\sim p</em>
\end{align}}[\boldsymbol{\theta}]}}\left[\left\Vert\nabla\log p_{\boldsymbol{\theta}}(\text{sg}[\boldsymbol{x}]) - \nabla\log p(\text{sg}[\boldsymbol{x}])\right\Vert^2\right] \tag{5.13</p>
<p>转换回噪声预测:
\begin{equation}
\mathcal{L}<em _boldsymbol_varphi="\boldsymbol{\varphi">5 = \mathbb{E}\left[\Vert\boldsymbol{\epsilon}</em>^<em>}(\text{sg}[\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}]) - \boldsymbol{\epsilon}</em>^</em>}(\text{sg}[\boldsymbol{x}_t^{(g)}])\Vert^2\right] \tag{5.14}
\end{equation}</p>
<h4 id="55-mathcall_5">5.5 $\mathcal{L}_5$的梯度分解<a class="toc-link" href="#55-mathcall_5" title="Permanent link">&para;</a></h4>
<p>计算$\nabla_{\boldsymbol{\theta}}\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">5$和$\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">1$的关系:
\begin{align}
\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">5 &amp;= \nabla</em>}}\mathbb{E}\left[\Vert\boldsymbol{\epsilon<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\varphi}^<em>}(\text{sg}[\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}]) - \boldsymbol{\epsilon}</em>^</em>}(\text{sg}[\boldsymbol{x}_t^{(g)}])\Vert^2\right] \tag{5.15}\
&amp;= \nabla</em>}}\mathbb{E}\left[\Vert\boldsymbol{\epsilon<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _boldsymbol_psi="\boldsymbol{\psi">t^{(g)}) - \boldsymbol{\epsilon}</em>^</em>}(\boldsymbol{x}_t^{(g)})\Vert^2\right] - \nabla</em>}}\underbrace{\mathbb{E}\left[\Vert\boldsymbol{\epsilon<em _mathcal_L="\mathcal{L">{\boldsymbol{\varphi}^<em>}(\boldsymbol{x}<em _text_sg="\text{sg">t^{(g)}) - \boldsymbol{\epsilon}</em>^}[\boldsymbol{\psi</em>]}(\boldsymbol{x}_t^{(g)})\Vert^2\right]}</em><em _boldsymbol_theta="\boldsymbol{\theta">1^{(\text{sg})}} \tag{5.16}\
&amp;= \nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">1 - \nabla</em>
\end{align}}}\mathcal{L}_1^{(\text{sg})} \tag{5.17</p>
<p>结合FGM的结果:
\begin{align}
\nabla_{\boldsymbol{\theta}}\mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">5 &amp;= \nabla</em>}}(2\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">2^{(\text{sg})} - \mathcal{L}_1^{(\text{sg})}) - \nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">1^{(\text{sg})} \tag{5.18}\
&amp;= 2\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">2^{(\text{sg})} - 2\nabla</em>}}\mathcal{L<em _boldsymbol_theta="\boldsymbol{\theta">1^{(\text{sg})} \tag{5.19}\
&amp;= 2\nabla</em>
\end{align}}}(\mathcal{L}_2^{(\text{sg})} - \mathcal{L}_1^{(\text{sg})}) \tag{5.20</p>
<p><strong>结论</strong>: 这对应SiD的$\lambda=1$!</p>
<h3 id="_9">六、理论对比与统一<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<h4 id="61">6.1 三种损失的关系总结<a class="toc-link" href="#61" title="Permanent link">&para;</a></h4>
<p>\begin{align}
\mathcal{L}_1 &amp;: \text{朴素损失,理论正确但实践梯度有偏} \tag{6.1}\
\mathcal{L}_2 &amp;: \text{SiD提出,部分恒等变换} \tag{6.2}\
\mathcal{L}_3 &amp;: \text{完全恒等变换} \tag{6.3}\
\mathcal{L}_4 &amp;: \text{FGM提出,梯度精确匹配} \tag{6.4}\
\mathcal{L}_5 &amp;: \text{修正Fisher散度,对应}\lambda=1 \tag{6.5}
\end{align}</p>
<p><strong>梯度关系</strong>:
\begin{align}
\nabla\mathcal{L}_1 &amp;= \nabla\mathcal{L}_2 = \nabla\mathcal{L}_3 \quad\text{(理论上)} \tag{6.6}\
\nabla\mathcal{L}_4^{(\text{sg})} &amp;= \nabla\mathcal{L}_1 = 2\nabla(\mathcal{L}_2^{(\text{sg})} - 0.5\mathcal{L}_1^{(\text{sg})}) \tag{6.7}\
\nabla\mathcal{L}_5 &amp;= \nabla\mathcal{L}_1 - \nabla\mathcal{L}_1^{(\text{sg})} = 2\nabla(\mathcal{L}_2^{(\text{sg})} - \mathcal{L}_1^{(\text{sg})}) \tag{6.8}
\end{align}</p>
<h4 id="62-lambda">6.2 $\lambda$参数的理论解释<a class="toc-link" href="#62-lambda" title="Permanent link">&para;</a></h4>
<p>SiD实际使用:
\begin{equation}
\mathcal{L}_{\text{SiD}} = \mathcal{L}_2^{(\text{sg})} - \lambda\mathcal{L}_1^{(\text{sg})} \tag{6.9}
\end{equation}</p>
<p><strong>三种选择</strong>:</p>
<ol>
<li><strong>$\lambda=0$</strong>: 只用$\mathcal{L}_2$,忽略原始目标</li>
<li><strong>$\lambda=0.5$</strong>: FGM证明的梯度精确匹配</li>
<li><strong>$\lambda=1$</strong>: 修正Fisher散度,避免模式坍缩</li>
</ol>
<p><strong>实验观察</strong>: $\lambda\in[0.5, 1.2]$效果好,验证理论预测。</p>
<h3 id="_10">七、实现细节与技巧<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<h4 id="71">7.1 停止梯度的正确实现<a class="toc-link" href="#71" title="Permanent link">&para;</a></h4>
<p><strong>PyTorch示例</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 计算L2</span>
<span class="n">eps_teacher</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="p">(</span><span class="n">x_t_gen</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># 教师模型</span>
<span class="n">eps_student</span> <span class="o">=</span> <span class="n">student_model</span><span class="p">(</span><span class="n">x_t_gen</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># 学生模型</span>

<span class="c1"># SiD (lambda=0.5)</span>
<span class="n">L2</span> <span class="o">=</span> <span class="p">((</span><span class="n">eps_teacher</span> <span class="o">-</span> <span class="n">eps_student</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">eps_teacher</span> <span class="o">-</span> <span class="n">eps</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">L1_sg</span> <span class="o">=</span> <span class="p">((</span><span class="n">eps_teacher</span> <span class="o">-</span> <span class="n">eps_student</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">loss_gen</span> <span class="o">=</span> <span class="n">L2</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">L1_sg</span>

<span class="c1"># 修正Fisher (lambda=1)</span>
<span class="n">loss_gen_fisher</span> <span class="o">=</span> <span class="n">L2</span> <span class="o">-</span> <span class="n">L1_sg</span>
</code></pre></div>

<h4 id="72">7.2 数值稳定性<a class="toc-link" href="#72" title="Permanent link">&para;</a></h4>
<p><strong>问题</strong>: $\Vert\nabla D\Vert^2$项可能数值不稳定</p>
<p><strong>解决</strong>:
1. 梯度裁剪
2. 归一化
3. 使用EMA(指数移动平均)</p>
<h3 id="_11">八、扩展应用<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<h4 id="81">8.1 多步蒸馏<a class="toc-link" href="#81" title="Permanent link">&para;</a></h4>
<p>扩展到$k$步蒸馏:
\begin{equation}
\boldsymbol{x}<em _text_student="\text{student">0^{(k)} = \text{Solve-ODE-k-steps}(\boldsymbol{x}_T, \boldsymbol{\epsilon}</em>
\end{equation}}}) \tag{8.1</p>
<p>损失变为:
\begin{equation}
\mathcal{L}<em i="0">{\text{multi}} = \sum</em>
\end{equation}}^{k-1} w_i\left[\mathcal{L}_2^{(i)} - \lambda\mathcal{L}_1^{(i)}\right] \tag{8.2</p>
<h4 id="82">8.2 条件生成<a class="toc-link" href="#82" title="Permanent link">&para;</a></h4>
<p>对于条件扩散模型$\boldsymbol{\epsilon}(\boldsymbol{x}<em _text_cond="\text{cond">t, t, \boldsymbol{c})$:
\begin{equation}
\mathcal{L}</em>}} = \mathbb{E<em _text_T="\text{T">{\boldsymbol{c},\boldsymbol{x}_0,\boldsymbol{\varepsilon}}\left[\langle\boldsymbol{\epsilon}</em>}} - \boldsymbol{\epsilon<em _text_T="\text{T">{\text{S}}, \boldsymbol{\epsilon}</em>
\end{equation}}} - \boldsymbol{\varepsilon}\rangle\right] \tag{8.3</p>
<p>恒等式仍然成立!</p>
<h3 id="_12">九、核心洞察与总结<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<p><strong>洞察1 - 恒等式的力量</strong>:
恒等式$\mathbb{E}[\langle\boldsymbol{f}, \boldsymbol{\epsilon}^<em>\rangle] = \mathbb{E}[\langle\boldsymbol{f}, \boldsymbol{\varepsilon}\rangle]$是SiD/FGM的理论基石,它允许我们用已知的$\boldsymbol{\varepsilon}$替换未知的$\boldsymbol{\epsilon}^</em>$。</p>
<p><strong>洞察2 - 梯度匹配的精妙</strong>:
FGM通过"先变换后求导"vs"先求导后变换"的对比,巧妙地找到了梯度精确匹配的损失函数。</p>
<p><strong>洞察3 - Fisher散度的双重角色</strong>:
Fisher散度中的两处$p_{\boldsymbol{\theta}}$有不同作用:
- 积分测度: 可能导致模式坍缩
- 散度内部: 才是真正的优化目标</p>
<p><strong>洞察4 - $\lambda$的理论依据</strong>:
- $\lambda=0.5$: 梯度精确匹配(FGM)
- $\lambda=1.0$: 避免模式坍缩(修正Fisher)
- $\lambda&gt;1.0$: 额外正则化</p>
<p><strong>未来方向</strong>:
1. 更高阶的恒等变换
2. 自适应$\lambda$选择
3. 扩展到其他生成模型</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="adam的epsilon如何影响学习率的scaling-law.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#297 Adam的epsilon如何影响学习率的Scaling Law？</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="从hessian近似看自适应学习率优化器.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#299 从Hessian近似看自适应学习率优化器</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下）</a><ul>
<li><a href="#_2">思想回顾</a></li>
<li><a href="#_3">恒等变换</a></li>
<li><a href="#_4">直面梯度</a></li>
<li><a href="#_5">广义散度</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a><ul>
<li><a href="#sid">一、SiD基础框架回顾</a></li>
<li><a href="#_8">二、恒等式的严格证明</a></li>
<li><a href="#sid_1">三、SiD的恒等变换推导</a></li>
<li><a href="#fgm">四、FGM的梯度精确估计</a></li>
<li><a href="#fisher">五、广义Fisher散度理论</a></li>
<li><a href="#_9">六、理论对比与统一</a></li>
<li><a href="#_10">七、实现细节与技巧</a></li>
<li><a href="#_11">八、扩展应用</a></li>
<li><a href="#_12">九、核心洞察与总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>