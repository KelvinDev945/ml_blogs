<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>你可能不需要BERT-flow：一个线性变换媲美BERT-flow | ML & Math Blog Posts</title>
    <meta name="description" content="你可能不需要BERT-flow：一个线性变换媲美BERT-flow&para;
原文链接: https://spaces.ac.cn/archives/8069
发布日期: 

BERT-flow来自论文《On the Sentence Embeddings from Pre-trained Language Models》，中了EMNLP 2020，主要是用flow模型校正了BERT出来的句向量...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语言模型">语言模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #44 你可能不需要BERT-flow：一个线性变换媲美BERT-flow
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#44</span>
                你可能不需要BERT-flow：一个线性变换媲美BERT-flow
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/8069" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语言模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语言模型</span>
                </a>
                
                <a href="../index.html?tags=语义" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语义</span>
                </a>
                
                <a href="../index.html?tags=flow" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> flow</span>
                </a>
                
                <a href="../index.html?tags=语义相似度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语义相似度</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="bert-flowbert-flow">你可能不需要BERT-flow：一个线性变换媲美BERT-flow<a class="toc-link" href="#bert-flowbert-flow" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8069">https://spaces.ac.cn/archives/8069</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>BERT-flow来自论文<a href="https://papers.cool/arxiv/2011.05864">《On the Sentence Embeddings from Pre-trained Language Models》</a>，中了EMNLP 2020，主要是用flow模型校正了BERT出来的句向量的分布，从而使得计算出来的cos相似度更为合理一些。由于笔者定时刷Arixv的习惯，早在它放到Arxiv时笔者就看到了它，但并没有什么兴趣，想不到前段时间小火了一把，短时间内公众号、知乎等地出现了不少的解读，相信读者们多多少少都被它刷屏了一下。</p>
<p>从实验结果来看，BERT-flow确实是达到了一个新SOTA，但对于这一结果，笔者的第一感觉是：不大对劲！当然，不是说结果有问题，而是根据笔者的理解，flow模型不大可能发挥关键作用。带着这个直觉，笔者做了一些分析，果不其然，笔者发现尽管BERT-flow的思路没有问题，但只要一个线性变换就可以达到相近的效果，flow模型并不是十分关键。</p>
<h2 id="_1">余弦相似度的假设<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>一般来说，我们语义相似度比较或检索，都是给每个句子算出一个句向量来，然后算它们的夹角余弦来比较或者排序。那么，我们有没有思考过这样的一个问题：余弦相似度对所输入的向量提出了什么假设呢？或者说，满足什么条件的向量用余弦相似度做比较效果会更好呢？</p>
<p>我们知道，两个向量$\boldsymbol{x},\boldsymbol{y}$的内积的几何意义就是“各自的模长乘以它们的夹角余弦”，所以余弦相似度就是两个向量的内积并除以各自的模长，对应的坐标计算公式是<br />
\begin{equation}\cos(\boldsymbol{x},\boldsymbol{y}) = \frac{\sum\limits_{i=1}^d x_i y_i}{\sqrt{\sum\limits_{i=1}^d x_i^2} \sqrt{\sum\limits_{i=1}^d y_i^2}}\label{eq:cos}\end{equation}<br />
然而，别忘了一件事情，上述等号只在“标准正交基”下成立。换句话说，向量的“夹角余弦”本身是具有鲜明的几何意义的，但上式右端只是坐标的运算，坐标依赖于所选取的坐标基，基底不同，内积对应的坐标公式就不一样，从而余弦值的坐标公式也不一样。</p>
<p>因此，假定BERT句向量已经包含了足够的语义（比如可以重构出原句子），那么如果它用公式$\eqref{eq:cos}$算余弦值来比较句子相似度时表现不好，那么原因可能就是此时的句向量所属的坐标系并非标准正交基。那么，我们怎么知道它具体用了哪种基底呢？原则上没法知道，但是我们可以去猜。猜测的依据是我们在给向量集合选择基底时，会尽量地平均地用好每一个基向量，从统计学的角度看，这就体现为每个分量的使用都是独立的、均匀的，如果这组基是标准正交基，那么对应的向量集应该表现出“各向同性”来。</p>
<p>当然，这不算是什么推导，只是一个启发式引导，它告诉我们如果一个向量的集合满足各向同性，那么我们可以认为它源于标准正交基，此时可以考虑用式$\eqref{eq:cos}$算相似度；反之，如果它并不满足各向同性，那么可以想办法让它变得更加各向同性一些，然后再用式$\eqref{eq:cos}$算相似度，而BERT-flow正是想到了“flow模型”这个办法。</p>
<h2 id="flow">flow模型的碎碎念<a class="toc-link" href="#flow" title="Permanent link">&para;</a></h2>
<p>依笔者来看，flow模型真的是一种让人觉得一言难尽的模型了，关于它的碎碎念又可以写上几页纸，这里尽量长话短说。2018年中，OpenAI发布了Glow模型，效果看起来很不错，这吸引了笔者进一步去学习flow模型，甚至还去复现了一把Glow模型，相关工作记录在<a href="/archives/5776">《细水长flow之NICE：流模型的基本概念与实现》</a>和<a href="/archives/5807">《细水长flow之RealNVP与Glow：流模型的传承与升华》</a>中，如果还不了解flow模型的，欢迎去看看这两篇博客。简单来说，flow模型是一个向量变换模型，它可以将输入数据的分布转化为标准正态分布，而显然标准正态分布是各向同性的，所以BERT-flow就选择了flow模型。</p>
<p>那么flow模型有什么毛病吗？其实之前在文章<a href="/archives/6482">《细水长flow之可逆ResNet：极致的暴力美学》</a>就已经吐槽过了，这里重复一下：</p>
<blockquote>
<p>（flow模型）通过比较巧妙的设计，使得模型每一层的逆变换比较简单，而且雅可比矩阵是一个三角阵，从而雅可比行列式很容易计算。这样的模型在理论上很优雅漂亮，但是有一个很严重的问题：由于必须保证逆变换简单和雅可比行列式容易计算，那么每一层的非线性变换能力都很弱。事实上像Glow这样的模型，每一层只有一半的变量被变换，所以为了保证充分的拟合能力，模型就必须堆得非常深（比如256的人脸生成，Glow模型堆了大概600个卷积层，两亿参数量），计算量非常大。</p>
</blockquote>
<p>看到这里，读者就能理解为什么笔者开头说看到BERT-flow的第一感觉就是“不对劲”了。上述吐槽告诉我们，flow模型其实是很弱的；然后BERT-flow里边所用的flow模型是多大呢？是一个level=2、depth=3的Glow模型，这两个参数大家可能没什么概念，反正就是很小，以至于整个模型并没有增加什么计算量。所以，笔者的“不对劲”直觉就是：</p>
<blockquote>
<p>flow模型本身很弱，BERT-flow里边使用的flow模型更弱，所以flow模型不大可能在BERT-flow中发挥至关重要的作用。反过来想，那就是也许我们可以找到更简单直接的方法达到BERT-flow的效果。</p>
</blockquote>
<h2 id="_2">标准化协方差矩阵<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>经过探索，笔者还真找到了这样的方法，正如本文标题所说，它只是一个线性变换。</p>
<p>其实思想很简单，我们知道标准正态分布的均值为0、协方差矩阵为单位阵，那么我们不妨将句向量的均值变换为0、协方差矩阵变换为单位阵试试看？假设（行）向量集合为$\{\boldsymbol{x}<em i="1">i\}</em>^N$，我们执行变换<br />
\begin{equation}\tilde{\boldsymbol{x}}<em i="1">i = (\boldsymbol{x}_i - \boldsymbol{\mu})\boldsymbol{W}
\end{equation}<br />
使得$\{\tilde{\boldsymbol{x}}_i\}</em>^N$的均值为0、协方差矩阵为单位阵。了解传统数据挖掘的读者可能知道，这实际上就相当于传统数据挖掘中的白化操作（Whitening），所以该方法笔者称之为BERT-whitening。</p>
<p>均值为0很简单，让$\boldsymbol{\mu}=\frac{1}{N}\sum\limits_{i=1}^N \boldsymbol{x}<em i="1">i$即可，有点难度的是$\boldsymbol{W}$矩阵的求解。将原始数据的协方差矩阵记为<br />
\begin{equation}\boldsymbol{\Sigma}=\frac{1}{N}\sum\limits</em>}^N (\boldsymbol{x<em i="1">i - \boldsymbol{\mu})^{\top}(\boldsymbol{x}_i - \boldsymbol{\mu})=\left(\frac{1}{N}\sum\limits</em>}^N \boldsymbol{x}_i^{\top}\boldsymbol{x}_i\right) - \boldsymbol{\mu}^{\top}\boldsymbol{\mu}\end{equation
那么不难得到变换后的数据协方差矩阵为$\tilde{\boldsymbol{\Sigma}}=\boldsymbol{W}^{\top}\boldsymbol{\Sigma}\boldsymbol{W}$，所以我们实际上要解方程<br />
\begin{equation}\boldsymbol{W}^{\top}\boldsymbol{\Sigma}\boldsymbol{W}=\boldsymbol{I}\quad\Rightarrow \quad \boldsymbol{\Sigma} = \left(\boldsymbol{W}^{\top}\right)^{-1}\boldsymbol{W}^{-1} = \left(\boldsymbol{W}^{-1}\right)^{\top}\boldsymbol{W}^{-1}\end{equation}<br />
我们知道协方差矩阵$\boldsymbol{\Sigma}$是一个半正定对称矩阵，且数据够多时它通常都是正定的，具有如下形式的SVD分解<br />
\begin{equation}\boldsymbol{\Sigma} = \boldsymbol{U}\boldsymbol{\Lambda}\boldsymbol{U}^{\top}\end{equation}<br />
其中$\boldsymbol{U}$是一个正交矩阵，而$\boldsymbol{\Lambda}$是一个对角阵，并且对角线元素都是正的，因此直接让$\boldsymbol{W}^{-1}=\sqrt{\boldsymbol{\Lambda}}\boldsymbol{U}^{\top}$就可以完成求解：<br />
\begin{equation}\boldsymbol{W} = \boldsymbol{U}\sqrt{\boldsymbol{\Lambda}^{-1}}\end{equation}</p>
<p>Numpy的参考代码为：</p>
<div class="highlight"><pre><span></span><code>def compute_kernel_bias(vecs):
    &quot;&quot;&quot;计算kernel和bias
    vecs.shape = [num_samples, embedding_size]，
    最后的变换：y = (x + bias).dot(kernel)
    &quot;&quot;&quot;
    mu = vecs.mean(axis=0, keepdims=True)
    cov = np.cov(vecs.T)
    u, s, vh = np.linalg.svd(cov)
    W = np.dot(u, np.diag(1 / np.sqrt(s)))
    return W, -mu
</code></pre></div>

<p>可能会有人问答大语料怎么办的问题。首先，上述算法只需要知道全体句向量的均值向量$\boldsymbol{\mu}\in\mathbb{R}^{d}$和协方差矩阵$\boldsymbol{\Sigma}\in\mathbb{R}^{d\times d}$（$d$是词向量维度），$\boldsymbol{\mu}$是全体句向量$\boldsymbol{x}<em n_1="n+1">i$的均值，均值是可以递归计算的：<br />
\begin{equation}\boldsymbol{\mu}</em>} = \frac{n}{n+1}\boldsymbol{\mu<em n_1="n+1">{n} + \frac{1}{n+1}\boldsymbol{x}</em>}\end{equation
同理，协方差矩阵$\boldsymbol{\Sigma}$也只不过是全体$\boldsymbol{x}<em n_1="n+1">i^{\top}\boldsymbol{x}_i$的均值再减去$\boldsymbol{\mu}^{\top}\boldsymbol{\mu}$，自然也是可以递归计算的：<br />
\begin{equation}\boldsymbol{\Sigma}</em>} = \frac{n}{n+1}\left(\boldsymbol{\Sigma<em n="n">{n}+\boldsymbol{\mu}</em>}^{\top}\boldsymbol{\mu<em n_1="n+1">{n}\right) + \frac{1}{n+1}\boldsymbol{x}</em>}^{\top}\boldsymbol{x<em n_1="n+1">{n+1}-\boldsymbol{\mu}</em>}^{\top}\boldsymbol{\mu}_{n+1}\end{equation
既然可以递归，那么就意味着我们是可以在有限内存下计算$\boldsymbol{\mu},\boldsymbol{\Sigma}$的，因此对于大语料来说BERT-whitening也不成问题的。</p>
<h2 id="bert-flow">相比于BERT-flow<a class="toc-link" href="#bert-flow" title="Permanent link">&para;</a></h2>
<p>现在，我们就可以测试一下上述BERT-whitening的效果了。为了跟BERT-flow对比，笔者用bert4keras在STS-B任务上进行了测试，参考脚本在：</p>
<blockquote>
<p><strong>Github链接：<a href="https://github.com/bojone/BERT-whitening">https://github.com/bojone/BERT-whitening</a></strong></p>
</blockquote>
<p>效果比较如下：<br />
\begin{array}{l|c}
\hline
&amp; \,\,\text{STS-B}\,\, \\
\hline
\text{BERT}<em _text_base="\text{base">{\text{base}}\text{-last2avg}\,(\text{论文结果}) &amp; 59.04 \\
\text{BERT}</em>) &amp; 70.72 \\}}\text{-flow}\,(\text{target, 论文结果
\text{BERT}<em _text_base="\text{base">{\text{base}}\text{-last2avg}\,(\text{个人复现}) &amp; 59.04 \\
\text{BERT}</em>) &amp; 71.20 \\}}\text{-whitening}\,(\text{target, 个人实现
\hline
\text{BERT}<em _text_large="\text{large">{\text{large}}\text{-last2avg}\,(\text{论文结果}) &amp; 59.56 \\
\text{BERT}</em>) &amp; 72.26 \\}}\text{-flow}\,(\text{target, 论文结果
\text{BERT}<em _text_large="\text{large">{\text{large}}\text{-last2avg}\,(\text{个人复现}) &amp; 59.59 \\
\text{BERT}</em>) &amp; 71.98 \\}}\text{-whitening}\,(\text{target, 个人实现
\hline
\end{array}</p>
<p>可以看到，简单的BERT-whitening确实能取得跟BERT-flow媲美的结果。除了STS-B之外，笔者的同事在中文业务数据内做了类似的比较，结果都表明BERT-flow带来的提升跟BERT-whitening是相近的，这表明，flow模型的引入可能没那么必要了，因为flow模型的层并非常见的层，它需要专门的实现，并且训练起来也有一定的工作量，而BERT-whitening的实现很简单，就一个线性变换，可以轻松套到任意的句向量模型中。（当然，非要辩的话，也可以说whitening是用线性变换实现的flow模型...）</p>
<blockquote>
<p>注：这里顺便补充一句，BERT-flow论文里边说的last2avg，本来含义是最后两层输出的平均向量，但它的代码实际上是“第一层+最后一层”输出的平均向量，相关讨论参考<a href="https://github.com/bohanli/BERT-flow/issues/11">ISSUE</a>。</p>
</blockquote>
<h2 id="_3">降维效果还能更好<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>现在我们知道BERT-whitening的变换矩阵$\boldsymbol{W} = \boldsymbol{U}\sqrt{\boldsymbol{\Lambda}^{-1}}$可以将数据的协方差矩阵变换成单位阵，如果我们不考虑$\sqrt{\boldsymbol{\Lambda}^{-1}}$，直接用$\boldsymbol{U}$来变换，结果如何呢？不难得出，如果只用$\boldsymbol{U}$来变换，那么数据的协方差矩阵就变成了$\boldsymbol{\Lambda}$，它是个对角阵。</p>
<p>前面说了，$\boldsymbol{U}$是一个正交矩阵，它相当于只是旋转了一下整体数据，不改变样本之间的相对位置，换句话说它是完全“保真”的变换。而$\boldsymbol{\Lambda}$的每个对角线元素，则衡量了它所在的那一维数据的变化幅度。如果它的值很小，说明这一维特征的变化很小，接近一个常数，那么就意味着原来句向量所在可能只是一个更低维的空间，我们就可以去掉这一维特征，在降维的同时还可以使得余弦相似度的结果更为合理。</p>
<p>事实上，SVD出来的对角矩阵$\boldsymbol{\Lambda}$已经从大到小排好序了，所以我们只需要保留前面若干维，就可以到达这个降维效果。熟悉线性代数的读者应该清楚，这个操作其实就是PCA！而代码只需要修改一行：</p>
<div class="highlight"><pre><span></span><code>def compute_kernel_bias(vecs, n_components=256):
    &quot;&quot;&quot;计算kernel和bias
    vecs.shape = [num_samples, embedding_size]，
    最后的变换：y = (x + bias).dot(kernel)
    &quot;&quot;&quot;
    mu = vecs.mean(axis=0, keepdims=True)
    cov = np.cov(vecs.T)
    u, s, vh = np.linalg.svd(cov)
    W = np.dot(u, np.diag(1 / np.sqrt(s)))
    return W[:, :n_components], -mu
</code></pre></div>

<p>效果如下：<br />
\begin{array}{l|c}
\hline
&amp; \,\,\text{STS-B}\,\, \\
\hline
\text{BERT}<em _text_base="\text{base">{\text{base}}\text{-last2avg}\,(\text{论文结果}) &amp; 59.04 \\
\text{BERT}</em>) &amp; 70.72 \\}}\text{-flow}\,(\text{target, 论文结果
\text{BERT}<em _text_base="\text{base">{\text{base}}\text{-last2avg}\,(\text{个人复现}) &amp; 59.04 \\
\text{BERT}</em>) &amp; 71.20 \\}}\text{-whitening}\,(\text{target, 个人实现
\text{BERT}<em _text_large="\text{large">{\text{base}}\text{-whitening-256}\,(\text{target, 个人实现}) &amp; 71.42 \\
\hline
\text{BERT}</em>) &amp; 59.56 \\}}\text{-last2avg}\,(\text{论文结果
\text{BERT}<em _text_large="\text{large">{\text{large}}\text{-flow}\,(\text{target, 论文结果}) &amp; 72.26 \\
\text{BERT}</em>) &amp; 59.59 \\}}\text{-last2avg}\,(\text{个人复现
\text{BERT}<em _text_large="\text{large">{\text{large}}\text{-whitening}\,(\text{target, 个人实现}) &amp; 71.98 \\
\text{BERT}</em>) &amp; 72.66 \\}}\text{-whitening-384}\,(\text{target, 个人实现
\hline
\end{array}</p>
<p>从上表可以看出，我们将base版本的768维只保留前256维，那么效果还有所提升，并且由于降维了，向量检索速度肯定也能大大加快；类似地，将large版的1024维只保留前384维，那么降维的同时也提升了效果。这个结果表明，无监督训练出来的句向量其实是“通用型”的，对于特定领域内的应用，里边有很多特征是冗余的，剔除这些冗余特征，往往能达到提速又提效的效果。</p>
<p>相比之下，flow模型是可逆的、不降维的，这在某些场景下是好处，但在不少场景下也是缺点，因为它无法剔除冗余维度，限制了性能，比如GAN的研究表明，通过一个256维的高斯向量就可以随机生成$1024\times 1024$的人脸图，这表明这些人脸图其实只是构成了一个相当低维的流形，但是如果用flow模型来做，因为要保证可逆性，就得强行用$1024\times 1024\times 3$那么多维的高斯向量来随机生成，计算成本大大增加，而且效果还上不去。</p>
<p>（注：后续实验结果，请看<a href="/archives/8321">《无监督语义相似度哪家强？我们做了个比较全面的评测》</a>。）</p>
<h2 id="_4">所以最终结论就是<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>所以，目前的结果就是：笔者的若干实验表明，通过简单的线性变换（BERT-whitening）操作，效果基本上能媲美BERT-flow模型，这表明往句向量模型里边引入flow模型可能并非那么关键，它对分布的校正可能仅仅是浅层的，而通过线性变换直接校正句向量的协方差矩阵就能达到相近的效果。同时，BERT-whitening还支持降维操作，能达到提速又提效的效果。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8069">https://spaces.ac.cn/archives/8069</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jan. 11, 2021). 《你可能不需要BERT-flow：一个线性变换媲美BERT-flow 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8069">https://spaces.ac.cn/archives/8069</a></p>
<p>@online{kexuefm-8069,<br />
title={你可能不需要BERT-flow：一个线性变换媲美BERT-flow},<br />
author={苏剑林},<br />
year={2021},<br />
month={Jan},<br />
url={\url{https://spaces.ac.cn/archives/8069}},<br />
} </p>
<hr />
<h2 id="_5">公式推导与注释<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="突破瓶颈打造更强大的transformer.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#43 突破瓶颈，打造更强大的Transformer</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="更便捷的cool-papers打开方式chrome重定向扩展.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#45 更便捷的Cool Papers打开方式：Chrome重定向扩展</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#bert-flowbert-flow">你可能不需要BERT-flow：一个线性变换媲美BERT-flow</a><ul>
<li><a href="#_1">余弦相似度的假设</a></li>
<li><a href="#flow">flow模型的碎碎念</a></li>
<li><a href="#_2">标准化协方差矩阵</a></li>
<li><a href="#bert-flow">相比于BERT-flow</a></li>
<li><a href="#_3">降维效果还能更好</a></li>
<li><a href="#_4">所以最终结论就是</a></li>
<li><a href="#_5">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>