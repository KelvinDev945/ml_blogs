<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>对齐全量微调！这是我看过最精彩的LoRA改进（一） | ML & Math Blog Posts</title>
    <meta name="description" content="对齐全量微调！这是我看过最精彩的LoRA改进（一）&para;
原文链接: https://spaces.ac.cn/archives/10226
发布日期: 

众所周知，LoRA是一种常见的参数高效的微调方法，我们在《梯度视角下的LoRA：简介、分析、猜测及推广》做过简单介绍。LoRA利用低秩分解来降低微调参数量，节省微调显存，同时训练好的权重可以合并到原始权重上，推理架构不需要作出改变，是一...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #283 对齐全量微调！这是我看过最精彩的LoRA改进（一）
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#283</span>
                对齐全量微调！这是我看过最精彩的LoRA改进（一）
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-07-12</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=低秩" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 低秩</span>
                </a>
                
                <a href="../index.html?tags=lora" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> lora</span>
                </a>
                
                <a href="../index.html?tags=参数高效微调" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 参数高效微调</span>
                </a>
                
                <a href="../index.html?tags=PEFT" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> PEFT</span>
                </a>
                
                <a href="../index.html?tags=低秩分解" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 低秩分解</span>
                </a>
                
                <a href="../index.html?tags=SVD" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> SVD</span>
                </a>
                
                <a href="../index.html?tags=矩阵逼近" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 矩阵逼近</span>
                </a>
                
                <a href="../index.html?tags=Adam优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> Adam优化器</span>
                </a>
                
                <a href="../index.html?tags=全量微调对齐" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 全量微调对齐</span>
                </a>
                
                <a href="../index.html?tags=投影算子" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 投影算子</span>
                </a>
                
                <a href="../index.html?tags=Frobenius范数" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> Frobenius范数</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="lora">对齐全量微调！这是我看过最精彩的LoRA改进（一）<a class="toc-link" href="#lora" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10226">https://spaces.ac.cn/archives/10226</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>众所周知，LoRA是一种常见的参数高效的微调方法，我们在<a href="/archives/9590">《梯度视角下的LoRA：简介、分析、猜测及推广》</a>做过简单介绍。LoRA利用低秩分解来降低微调参数量，节省微调显存，同时训练好的权重可以合并到原始权重上，推理架构不需要作出改变，是一种训练和推理都比较友好的微调方案。此外，我们在<a href="/archives/10001">《配置不同的学习率，LoRA还能再涨一点？》</a>还讨论过LoRA的不对称性，指出给$A,B$设置不同的学习率能取得更好的效果，该结论被称为“LoRA+”。</p>
<p>为了进一步提升效果，研究人员还提出了不少其他LoRA变体，如<a href="https://papers.cool/arxiv/2303.10512">AdaLoRA</a>、<a href="https://papers.cool/arxiv/2312.03732">rsLoRA</a>、<a href="https://papers.cool/arxiv/2402.09353">DoRA</a>、<a href="https://papers.cool/arxiv/2404.02948">PiSSA</a>等，这些改动都有一定道理，但没有特别让人深刻的地方觉。然而，前两天的<a href="https://papers.cool/arxiv/2407.05000">《LoRA-GA: Low-Rank Adaptation with Gradient Approximation》</a>，却让笔者眼前一亮，仅扫了摘要就有种必然有效的感觉，仔细阅读后更觉得它是至今最精彩的LoRA改进。</p>
<p>究竟怎么个精彩法？LoRA-GA的实际含金量如何？我们一起来学习一下。</p>
<hr />
<h2 id="1">第1部分：核心理论、公理与历史基础<a class="toc-link" href="#1" title="Permanent link">&para;</a></h2>
<h3 id="11">1.1 理论起源与历史发展<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>参数高效微调的理论根源</strong>可追溯到多个数学和机器学习领域的交汇：</p>
<div class="theorem-box">

#### 理论根基

**1. 低秩矩阵理论**（20世纪初，线性代数）
- **Eckart-Young-Mirsky定理**（1936）：任意矩阵的最优低秩逼近由其SVD截断给出
- **应用**：LoRA的数学基础，证明低秩分解可以有效逼近全秩矩阵

**2. 流形假说**（2000s，机器学习）
- **核心观点**：高维数据往往分布在低维流形上
- **引申**：参数更新可能主要发生在低维子空间，而非整个参数空间
- **代表工作**：Intrinsic Dimensionality（Li et al., 2018）证明许多任务的优化可以在低维子空间完成

**3. 迁移学习理论**（2010s，深度学习）
- **预训练-微调范式**：预训练模型已经学习了通用知识，微调只需少量调整
- **关键洞察**：微调时的参数变化量往往远小于预训练参数的规模
- **LoRA的动机**：既然变化量小，是否可以用低秩矩阵表示？

</div>

<p><strong>关键里程碑</strong>：</p>
<ol>
<li>
<p><strong>2018 - Intrinsic Dimension</strong>（Li et al.）
   - 证明在随机低维子空间中优化也能取得接近全维度优化的效果
   - 首次量化了深度学习优化的"内在维度"</p>
</li>
<li>
<p><strong>2019 - Adapter Layers</strong>（Houlsby et al.）
   - 在预训练模型中插入小型适配器模块
   - 冻结主干网络，只训练适配器
   - 缺点：改变模型架构，增加推理延迟</p>
</li>
<li>
<p><strong>2021 - LoRA</strong>（Hu et al., Microsoft）
   - 首次提出低秩分解的微调方法
   - 核心创新：$\Delta W = AB$，$r \ll \min(n,m)$
   - 优势：训练时节省显存，推理时可合并权重，无额外开销</p>
</li>
<li>
<p><strong>2022-2024 - LoRA变体涌现</strong>
   - AdaLoRA（2023）：自适应调整每层的秩
   - DoRA（2024）：将权重分解为幅度和方向
   - PiSSA（2024）：对预训练权重SVD初始化
   - <strong>LoRA-GA（2024）</strong>：对梯度SVD初始化，对齐全量微调</p>
</li>
</ol>
<h3 id="12">1.2 设计哲学与核心思想<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<div class="intuition-box">

#### 🧠 LoRA的设计哲学

**核心思想**：**"以小搏大"** —— 用少量参数实现接近全量微调的效果

**三大支柱**：

1. **低秩假设** 🎯
   - **假设**：微调时的权重更新矩阵 $\Delta W$ 具有低秩结构
   - **依据**：预训练模型已经学习了大部分知识，微调只需在少数关键方向上调整
   - **类比**：就像修正一幅画，不需要重画整张画布，只需在关键位置补几笔

2. **参数效率** 💾
   - **目标**：用 $r(n+m)$ 个参数代替 $nm$ 个参数
   - **效果**：当 $n=m=4096, r=8$ 时，参数量减少到 $1/256$
   - **类比**：就像压缩文件，保留最重要的信息，丢弃冗余

3. **可合并性** ⚡
   - **优势**：训练完成后可以将 $AB$ 合并到 $W_0$，推理无额外开销
   - **对比**：Adapter需要额外的前向计算，LoRA完全无性能损失
   - **类比**：就像补丁可以直接融入原程序，而不是每次运行都加载补丁

</div>

<p><strong>LoRA-GA的创新哲学</strong>：</p>
<p>传统LoRA的一个根本问题是：<strong>我们知道目标（对齐全量微调），但不知道如何初始化来达到目标</strong>。</p>
<div class="theorem-box">

### 核心洞察：逼近的层次性

**零阶逼近**：保证初始权重相等
$$W_0^{\text{LoRA}} = W_0^{\text{full}} \quad \checkmark$$

**一阶逼近**（LoRA-GA的贡献）：保证第一步更新后尽可能接近
$$W_1^{\text{LoRA}} \approx W_1^{\text{full}}$$

**高阶逼近**（理想状态）：保证整个优化轨迹接近
$$W_t^{\text{LoRA}} \approx W_t^{\text{full}}, \quad \forall t$$

</div>

<p>LoRA-GA通过<strong>对齐第一步梯度</strong>来实现一阶逼近，这是比零阶逼近更强的保证。</p>
<h3 id="13">1.3 数学公理与基础假设<a class="toc-link" href="#13" title="Permanent link">&para;</a></h3>
<div class="theorem-box">

### 公理1：低秩分解的存在性

对于任意矩阵 $M \in \mathbb{R}^{n \times m}$，存在分解：
$$M = \sum_{i=1}^{\min(n,m)} \sigma_i u_i v_i^{\top}$$
其中 $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$ 是奇异值，$u_i, v_i$ 是奇异向量。

**秩-$r$ 截断**的最优性（Eckart-Young定理）：
$$M_r = \sum_{i=1}^{r} \sigma_i u_i v_i^{\top} = \arg\min_{\text{rank}(X) \leq r} \|M - X\|_F$$

</div>

<div class="theorem-box">

### 公理2：梯度下降的一阶近似

对于损失函数 $\mathcal{L}(W)$，梯度下降的第一步更新为：
$$W_1 = W_0 - \eta \nabla_W \mathcal{L}(W_0) + \mathcal{O}(\eta^2)$$

对于LoRA，$W = W_0 + AB$，梯度为：
$$\nabla_A \mathcal{L} = \nabla_W \mathcal{L} \cdot B^{\top}, \quad \nabla_B \mathcal{L} = A^{\top} \cdot \nabla_W \mathcal{L}$$

</div>

<div class="theorem-box">

### 公理3：Frobenius范数的不变性

对于正交矩阵 $U, V$，Frobenius范数满足：
$$\|U M V^{\top}\|_F = \|M\|_F$$

**证明**：
$$\|U M V^{\top}\|_F^2 = \text{Tr}[(U M V^{\top})^{\top} (U M V^{\top})] = \text{Tr}[V M^{\top} U^{\top} U M V^{\top}]$$
$$= \text{Tr}[V M^{\top} M V^{\top}] = \text{Tr}[M^{\top} M V^{\top} V] = \text{Tr}[M^{\top} M] = \|M\|_F^2$$

这个性质是LoRA-GA推导中利用SVD简化问题的关键。

</div>

<h3 id="14">1.4 与其他方法的本质区别<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>核心理念</th>
<th>初始化策略</th>
<th>理论支撑</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>标准LoRA</strong></td>
<td>低秩约束</td>
<td>$A \sim \mathcal{N}(0, \sigma^2)$, $B = 0$</td>
<td>经验性质</td>
</tr>
<tr>
<td><strong>PiSSA</strong></td>
<td>权重逼近</td>
<td>对 $W_0$ SVD</td>
<td>保持初始权重接近</td>
</tr>
<tr>
<td><strong>LoRA-GA</strong></td>
<td>梯度对齐</td>
<td>对 $\nabla_W \mathcal{L}$ SVD</td>
<td><strong>一阶逼近理论</strong></td>
</tr>
<tr>
<td><strong>AdaLoRA</strong></td>
<td>动态调整</td>
<td>随机 + 剪枝</td>
<td>重要性采样</td>
</tr>
</tbody>
</table>
<p><strong>LoRA-GA的独特性</strong>：</p>
<ul>
<li><strong>理论驱动</strong>：明确的优化目标（最小化与全量微调的差异）</li>
<li><strong>一次性成本</strong>：初始化后无额外计算开销</li>
<li><strong>通用性</strong>：适用于任何使用LoRA的场景</li>
</ul>
<hr />
<h2 id="2">第2部分：严谨的核心数学推导<a class="toc-link" href="#2" title="Permanent link">&para;</a></h2>
<h3 id="21-lora">2.1 LoRA的数学框架<a class="toc-link" href="#21-lora" title="Permanent link">&para;</a></h3>
<p>首先我们再来温习一下LoRA。假设预训练参数为$W_0 \in \mathbb{R}^{n\times m}$，那么全量微调时的更新量自然也是一个$n\times m$矩阵，LoRA将更新量约束为低秩矩阵来降低训练时的参数量，即设$W=W_0 + AB$，其中$A\in\mathbb{R}^{n\times r},B\in\mathbb{R}^{r\times m}$以及$r\ll \min(n,m)$，用新的$W$替换模型原参数，并固定$W_0$不变，只训练$A,B$，如下图所示：
$$\style{display: inline-block; width: 24ex; padding: 10ex 0; border: 1px solid #6C8EBF; background-color: #DAE8FC}{W_0\in\mathbb{R}^{n\times m}} \quad + \quad \style{display: inline-block; width: 8ex; padding: 10ex 0; border: 1px solid #D79B00; background-color: #FFE6CC}{A\in\mathbb{R}^{n\times r}}\quad\times\quad \style{display: inline-block; width: 24ex; padding: 3ex 0; border: 1px solid #D79B00; background-color: #FFE6CC}{B\in\mathbb{R}^{r\times m}}$$</p>
<div class="derivation-box">

### 推导2.1：LoRA参数量与压缩比

**步骤1**：计算全量微调的参数量

全量微调需要存储和更新的参数总数为：
$$\text{Params}_{\text{full}} = nm$$

对于优化器状态（如Adam需要存储一阶和二阶动量）：
$$\text{Memory}_{\text{full}} = nm \times (1 + 2) = 3nm$$
第一项是参数本身，后两项是Adam的 $m_t$ 和 $v_t$ 状态。

**步骤2**：计算LoRA的参数量

LoRA需要存储和更新的参数总数为：
$$\text{Params}_{\text{LoRA}} = nr + rm = r(n+m)$$

**步骤3**：计算压缩比

$$\text{Compression Ratio} = \frac{\text{Params}_{\text{full}}}{\text{Params}_{\text{LoRA}}} = \frac{nm}{r(n+m)}$$

当 $n = m$ 时：
$$\text{Compression Ratio} = \frac{n^2}{2rn} = \frac{n}{2r}$$

**结论**：对于 $n=4096, r=8$，压缩比为 $4096/(2 \times 8) = 256$，即参数量减少到原来的 1/256。

对于一个7B参数的大模型，使用FP32，全量微调需要约 $7 \times 10^9 \times 3 \times 4 = 84$ GB 的显存，而LoRA只需约 $84/256 \approx 0.33$ GB。

</div>

<div class="derivation-box">

### 推导2.2：LoRA的梯度计算

**步骤1**：定义前向传播

设输入为 $x \in \mathbb{R}^m$，则：
$$y = Wx = (W_0 + AB)x$$

**步骤2**：应用链式法则计算 $\frac{\partial \mathcal{L}}{\partial A}$

$$\frac{\partial \mathcal{L}}{\partial A} = \frac{\partial \mathcal{L}}{\partial W} \frac{\partial W}{\partial A}$$

由于 $W = W_0 + AB$，我们有：
$$\frac{\partial W}{\partial A_{ij}} = e_i \otimes (Bx)_j^{\top}$$

其中 $e_i$ 是第$i$个标准基向量，$\otimes$ 是外积。

更简洁地，使用矩阵微分：
$$\frac{\partial \mathcal{L}}{\partial A} = \frac{\partial \mathcal{L}}{\partial W} \cdot B^{\top}$$

**步骤3**：计算 $\frac{\partial \mathcal{L}}{\partial B}$

类似地：
$$\frac{\partial \mathcal{L}}{\partial B} = A^{\top} \cdot \frac{\partial \mathcal{L}}{\partial W}$$

**结论**：LoRA的梯度计算只需要全量梯度 $\frac{\partial \mathcal{L}}{\partial W}$ 与 $A, B$ 的矩阵乘法，计算复杂度为 $\mathcal{O}(nmr)$。

</div>

<p>为了使得LoRA的初始状态跟预训练模型一致，我们通常会将$A,B$之一全零初始化，这样可以得到$A_0 B_0=0$，那么初始的$W$就是$W_0$。但这并不是必须的，如果$A,B$都是非全零初始化，那么我们只需要将$W$设置为<br />
\begin{equation}W = (W_0 - A_0 B_0) + AB\end{equation}<br />
也就是说将固定不变的权重从$W_0$换为$W_0 - A_0 B_0$，同样可以满足 <em><strong>初始$W$等于$W_0$</strong></em> 这一条件。</p>
<p>需要指出的是，LoRA往往只是显存不足的无奈之选，因为一般情况下全量微调的效果都会优于LoRA，所以如果算力足够并且要追求效果最佳时，请优先选择全量微调。这也是LoRA-GA的假设之一，因为它的改进方向就是向全量微调对齐。使用LoRA的另一个场景是有大量的微型定制化需求，我们要存下非常多的微调结果，此时使用LoRA能减少储存成本。</p>
<h3 id="23-lora-ga">2.3 LoRA-GA的核心思想：对齐全量微调<a class="toc-link" href="#23-lora-ga" title="Permanent link">&para;</a></h3>
<p>LoRA-GA提出了一个非常深刻的优化点：通过$W=(W_0 - A_0 B_0) + AB$我们可以保证$W$的初始值等于$W_0$，即初始状态的LoRA与全量微调是等价的，那么我们是否还可以调整$A_0$和$B_0$，使得LoRA和全量微调在后续训练中也尽可能近似？比如最简单地，让经过第一步优化后的$W_1$尽可能相等？</p>
<p>越仔细回味，我们会越发现这个优化点是如此“直击本质”——LoRA的目标不就是“以小搏大”，希望能接近全量微调的效果吗？既然如此，尽可能对齐全量微调的后续更新结果，不就是最正确的改进方向？从逼近的角度来看，“$W$的初始值等于$W_0$”相当于全量微调的零阶近似，保持后面的$W_1,W_2,\cdots$接近，则相当于是更高阶的近似，是合情合理的选择，所以笔者看完摘要后就有种“就是它了”的强烈感觉。</p>
<div class="derivation-box">

### 推导2.3：第一步更新的对比分析

**步骤1**：全量微调的第一步更新

假设我们的优化器是SGD，那么对于全量微调，我们有：
$$W_1^{\text{full}} = W_0 - \eta \frac{\partial \mathcal{L}}{\partial W_0}$$
其中$\mathcal{L}$是损失函数，$\eta$是学习率。

**步骤2**：LoRA的第一步更新

对于LoRA，$A$ 和 $B$ 的更新为：
$$A_1 = A_0 - \eta \frac{\partial \mathcal{L}}{\partial A_0} = A_0 - \eta \frac{\partial \mathcal{L}}{\partial W_0} B_0^{\top}$$
$$B_1 = B_0 - \eta \frac{\partial \mathcal{L}}{\partial B_0} = B_0 - \eta A_0^{\top}\frac{\partial \mathcal{L}}{\partial W_0}$$

**步骤3**：展开LoRA更新后的权重

$$W_1^{\text{LoRA}} = W_0 - A_0 B_0 + A_1 B_1$$

代入 $A_1, B_1$：
$$W_1^{\text{LoRA}} = W_0 - A_0 B_0 + (A_0 - \eta \frac{\partial \mathcal{L}}{\partial W_0} B_0^{\top})(B_0 - \eta A_0^{\top}\frac{\partial \mathcal{L}}{\partial W_0})$$

**步骤4**：展开并忽略二阶项

$$W_1^{\text{LoRA}} = W_0 - A_0 B_0 + A_0 B_0 - \eta A_0 A_0^{\top}\frac{\partial \mathcal{L}}{\partial W_0} - \eta \frac{\partial \mathcal{L}}{\partial W_0}B_0^{\top} B_0 + \mathcal{O}(\eta^2)$$

忽略 $\eta^2$ 的二阶小量（因为学习率 $\eta$ 通常很小，如 $10^{-3}$ 或 $10^{-4}$）：
$$W_1^{\text{LoRA}} \approx W_0 - \eta\left(A_0 A_0^{\top}\frac{\partial \mathcal{L}}{\partial W_0} + \frac{\partial \mathcal{L}}{\partial W_0}B_0^{\top} B_0\right)$$

**步骤5**：建立优化目标

为了让 $W_1^{\text{LoRA}}$ 尽可能接近 $W_1^{\text{full}}$，我们希望：
$$A_0 A_0^{\top}\frac{\partial \mathcal{L}}{\partial W_0} + \frac{\partial \mathcal{L}}{\partial W_0}B_0^{\top} B_0 \approx \frac{\partial \mathcal{L}}{\partial W_0}$$

因此，优化目标为：
$$\mathop{\text{argmin}}_{A_0,B_0}\left\Vert A_0 A_0^{\top}\frac{\partial \mathcal{L}}{\partial W_0} + \frac{\partial \mathcal{L}}{\partial W_0}B_0^{\top} B_0 - \frac{\partial \mathcal{L}}{\partial W_0}\right\Vert_F^2 \label{eq:loss-0}$$

其中$\Vert\cdot\Vert_F^2$是矩阵的[Frobenius范数](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm)的平方，即矩阵每个元素的平方和。

**物理意义**：
- $A_0 A_0^{\top}$ 是一个秩不超过 $r$ 的左投影算子
- $B_0^{\top} B_0$ 是一个秩不超过 $r$ 的右投影算子
- 我们希望这两个投影算子的组合能尽可能接近单位算子 $I$

</div>

<h3 id="24">2.4 优化问题的求解过程<a class="toc-link" href="#24" title="Permanent link">&para;</a></h3>
<div class="derivation-box">

### 推导2.4：通过SVD求解最优初始化

**步骤1**：问题简化

简单起见，我们记$G_0=\frac{\partial \mathcal{L}}{\partial W_0}$，那么目标$\eqref{eq:loss-0}$可以简写成：
$$\mathop{\text{argmin}}_{A_0,B_0}\left\Vert A_0 A_0^{\top}G_0 + G_0 B_0^{\top} B_0 - G_0\right\Vert_F^2 \label{eq:loss-1}$$

**关键观察**：注意$A_0 A_0^{\top}G_0$、$G_0 B_0^{\top} B_0$的秩顶多为$r$，它们相加后的秩顶多为$2r$，我们假设$2r < \min(n,m)$，所以上述目标相当于寻找$G_0$的一个**秩不超过$2r$的最优近似**。

**步骤2**：对角矩阵的简单情形

我们先考虑$G_0$是非负对角阵的情形，并且对角线元素已经按照从大到小的顺序排列：
$$G_0 = \text{diag}(\sigma_1, \sigma_2, \ldots, \sigma_{\min(n,m)}), \quad \sigma_1 \geq \sigma_2 \geq \cdots \geq 0$$

根据[Eckart-Young-Mirsky定理](https://en.wikipedia.org/wiki/Low-rank_approximation)，秩不超过$2r$的最优近似就是只保留对角线前$2r$个元素：
$$G_0^{(2r)} = \text{diag}(\sigma_1, \ldots, \sigma_{2r}, 0, \ldots, 0)$$

能让$A_0 A_0^{\top}G_0 + G_0 B_0^{\top} B_0$实现这个逼近的$A_0,B_0$可以是：
$$A_0 = (I_n)_{[:, :r]} = \begin{bmatrix} I_r \\ 0 \end{bmatrix}, \quad B_0 = (I_m)_{[r:2r, :]} = \begin{bmatrix} 0 & I_r & 0 \end{bmatrix}$$

其中$I_n,I_m$分别是$n,m$阶单位阵，${}_{[:, :r]}$和${}_{[r:2r, :]}$就是像Python切片那样，取前$r$列和第$r+1\sim 2r$行。

**解释**：这个解对应的是$A_0 A_0^{\top}G_0$保留前$r$个对角元素，$G_0 B_0^{\top} B_0$保留第$r+1\sim 2r$个对角元素。注意解并不唯一，只要总共保留前$2r$个最大元素即可。

**步骤3**：一般矩阵通过SVD转化为对角矩阵

当$G_0$不是对角阵时，我们对它进行奇异值分解(SVD)：
$$G_0 = U\Sigma V^{\top}$$
其中：
- $U\in\mathbb{R}^{n\times n}$为左奇异向量矩阵（正交）
- $V\in\mathbb{R}^{m\times m}$为右奇异向量矩阵（正交）
- $\Sigma\in\mathbb{R}^{n\times m}$为对角矩阵，对角线元素$\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$

**步骤4**：利用正交不变性简化问题

将SVD代入目标函数：
\begin{align}
&\left\Vert A_0 A_0^{\top}G_0 + G_0 B_0^{\top} B_0 - G_0\right\Vert_F^2 \\
=&\, \left\Vert A_0 A_0^{\top}U\Sigma V^{\top} + U\Sigma V^{\top} B_0^{\top} B_0 - U\Sigma V^{\top}\right\Vert_F^2 \\
=&\, \left\Vert U\left[(U^{\top}A_0) (U^{\top}A_0)^{\top}\Sigma + \Sigma (B_0 V^{\top})^{\top} (B_0 V^{\top}) - \Sigma \right]V^{\top}\right\Vert_F^2 \\
=&\, \left\Vert (U^{\top}A_0) (U^{\top}A_0)^{\top}\Sigma + \Sigma (B_0 V^{\top})^{\top} (B_0 V^{\top}) - \Sigma\right\Vert_F^2
\end{align}

**关键步骤**：
- 第二个等号：将$G_0 = U\Sigma V^{\top}$代入
- 第三个等号：提取正交矩阵$U$和$V^{\top}$
- 第四个等号：**利用正交变换不改变Frobenius范数**（见公理3）

**步骤5**：变量替换

令$\tilde{A}_0 = U^{\top}A_0$，$\tilde{B}_0 = B_0 V^{\top}$，问题转化为：
$$\mathop{\text{argmin}}_{\tilde{A}_0,\tilde{B}_0}\left\Vert \tilde{A}_0 \tilde{A}_0^{\top}\Sigma + \Sigma \tilde{B}_0^{\top} \tilde{B}_0 - \Sigma\right\Vert_F^2$$

这正是**步骤2**中对角矩阵的情形！

**步骤6**：反推原始变量

根据步骤2的结论：
$$\tilde{A}_0 = (I_n)_{[:, :r]}, \quad \tilde{B}_0 = (I_m)_{[r:2r, :]}^{\top}$$

反推得到：
$$A_0 = U\tilde{A}_0 = U(I_n)_{[:, :r]} = U_{[:, :r]}$$
$$B_0 = \tilde{B}_0 V^{\top} = (I_m)_{[r:2r, :]} V^{\top} = V_{[r:2r, :]}$$

**结论**：最优初始化为取SVD的前$r$个左奇异向量和第$r+1\sim 2r$个右奇异向量的转置！

</div>

<details>
<summary>点击展开：为什么是前$r$个和第$r+1\sim 2r$个，而不是其他分配方式？</summary>
<div markdown="1">

**答案**：理论上任意分配方式都可以，只要总共保留前$2r$个最大的奇异值方向即可。

**常见分配策略**：

1. **均匀分配**（论文采用）：
   - $A_0 = U_{[:, :r]}$（左边前$r$个）
   - $B_0 = V_{[r:2r, :]}$（右边第$r+1\sim 2r$个）
   - **优势**：左右对称，计算简单

2. **全左分配**：
   - $A_0 = U_{[:, :2r]}$（左边前$2r$个）
   - $B_0 = 0$（右边不贡献）
   - **缺点**：$B$的秩变为0，退化为单侧投影

3. **自适应分配**：
   - 根据奇异值的衰减速度动态分配
   - 如果前$r$个奇异值占主导，可以多分配给$A_0$
   - **复杂度**：需要额外的启发式规则

**实践建议**：均匀分配是最简单且有效的策略，也是论文采用的方法。

</div>
</details>

<h3 id="25-lora-ga">2.5 LoRA-GA算法总结<a class="toc-link" href="#25-lora-ga" title="Permanent link">&para;</a></h3>
<p>现在我们就得到了LoRA的一种初始化方法：</p>
<div class="theorem-box">

### LoRA-GA初始化算法

**输入**：
- 预训练权重 $W_0 \in \mathbb{R}^{n \times m}$
- 目标秩 $r$
- 训练数据集 $\mathcal{D}$

**步骤**：

1. **计算初始梯度**：选取一批样本（通常256-1024个），计算初始梯度
   $$G_0 = \nabla_{W_0}\mathcal{L}(W_0; \mathcal{D})$$

2. **SVD分解**：对梯度进行奇异值分解
   $$G_0 = U\Sigma V^{\top}$$

3. **提取奇异向量**：
   - 取$U$的前$r$列初始化$A$：$A_0 = U_{[:, :r]}$
   - 取$V$的第$r+1\sim 2r$行初始化$B$：$B_0 = V_{[r:2r, :]}$

4. **调整预训练权重**：将固定权重设为
   $$W_{\text{frozen}} = W_0 - A_0 B_0$$

5. **开始训练**：使用 $(A_0, B_0)$ 作为初始值，训练 $W = W_{\text{frozen}} + AB$

**输出**：初始化的LoRA参数 $(A_0, B_0)$

</div>

<p>这样LoRA + SGD得到的$W_1$就跟全量微调的$W_1$尽可能相近了。此外，梯度最重要的是方向，其模长不大重要，所以初始化结果我们还可以乘以个scale，LoRA本身也可以乘以个scale，即$W = (W_0 - \lambda A_0 B_0) + \lambda AB$，这些都是LoRA常见的超参数，这里就不展开讨论了。顺便提一下，形式上跟LoRA-GA比较相似的是<a href="https://papers.cool/arxiv/2404.02948">PiSSA</a>，它是对$W_0$做SVD来初始化$A,B$，这在理论支持上就不如LoRA-GA了，是一个纯粹的经验选择。</p>
<p>当然，可能有读者会发现目前的推导都是基于SGD优化器的假设，那么对于我们更常用的Adam优化器，结论是否要做出改变呢？理论上是要的。我们在<a href="/archives/10001">《配置不同的学习率，LoRA还能再涨一点？》</a>讨论过，对于Adam来说，第一步优化结果是$W_1 = W_0 - \eta\, \text{sign}(G_0)$而不是$W_1 = W_0 - \eta G_0$，这样重复前面的推导，我们可以得到优化目标为<br />
\begin{equation}\mathop{\text{argmin}}_{A_0,B_0}\left\Vert A_0 \text{sign}(A_0^{\top}G_0) + \text{sign}(G_0 B_0^{\top}) B_0 - \text{sign}(G_0)\right\Vert_F^2 \label{eq:loss-adam}\end{equation}<br />
由于符号函数$\text{sign}$的存在，我们没法求出它的解析解，所以针对Adam的理论分析就只能止步于此了。</p>
<p>在这个背景下，对于Adam优化器，我们有三个选择：</p>
<blockquote>
<p>1、<strong>信仰</strong> ：直接引用SGD的结果，相信它也可以在Adam中发挥同样的效果；</p>
<p>2、<strong>硬刚</strong> ：用优化器直接去最小化目标$\eqref{eq:loss-adam}$，由于目标比较简单，计算量尚能接受；</p>
<p>3、<strong>投机</strong> ：直觉上将$G_0$换成$\text{sign}(G_0)$，然后代入SGD的结论，可能更贴合Adam。</p>
</blockquote>
<p>看起来原论文选择的是第1个方案，论文的实验结果确实也支持这一选择。</p>
<hr />
<h2 id="3">第3部分：数学直觉、多角度解释与类比<a class="toc-link" href="#3" title="Permanent link">&para;</a></h2>
<h3 id="31-lora-ga">3.1 生活化类比：为什么LoRA-GA有效？<a class="toc-link" href="#31-lora-ga" title="Permanent link">&para;</a></h3>
<div class="intuition-box">

#### 🧠 直觉理解1：导航系统的初始定位

**问题**：你要从A点开车到B点，导航系统有两种工作模式。

**全量微调** 🗺️：
- 拥有完整的地图和所有道路信息
- 可以考虑所有可能的路线组合
- 计算量大，但一定能找到最优路径

**标准LoRA** 🧭：
- 只能在预先选定的几条主干道上行驶（低秩约束）
- 随机选择起始主干道（随机初始化）
- **问题**：如果起始方向选错了，即使后续调整也很难到达目标

**LoRA-GA** 📍：
- 同样只能在主干道上行驶
- 但**先看一眼目标方向**（计算初始梯度）
- **根据目标方向选择最合适的主干道**（SVD选择最重要的方向）
- 虽然不能走所有小路，但至少保证大方向是对的！

**关键洞察**：LoRA-GA不是增加可选道路（那需要更大的秩），而是**更聪明地选择有限的道路**。

</div>

<div class="intuition-box">

#### 🧠 直觉理解2：雕塑家的工具选择

想象你是一位雕塑家，要从一块大理石雕刻出一尊雕像。

**全量微调** 🔨：
- 拥有所有工具：大锤、凿子、锉刀、砂纸...
- 可以精确控制每个细节
- 需要很大的工作空间和很长时间

**标准LoRA** 🔧：
- 只能使用$r$把工具（显存限制）
- 随机从工具箱中抽取
- **可能抽到的都是砂纸，缺少大锤做粗加工**

**LoRA-GA** 🎯：
- 同样只能用$r$把工具
- 但**先看一眼最终雕像应该长什么样**（初始梯度）
- **根据当前阶段最需要的操作选择工具**（SVD找主要方向）
- 如果当前是粗加工阶段，优先选大锤和凿子
- 如果是精修阶段，优先选锉刀和砂纸

**数学对应**：
- 工具 ↔ 参数更新的方向
- 工具的重要性 ↔ 梯度的奇异值大小
- 选择工具 ↔ 选择哪些奇异向量

</div>

<h3 id="32">3.2 几何意义：投影与子空间<a class="toc-link" href="#32" title="Permanent link">&para;</a></h3>
<div class="intuition-box">

#### 📐 几何视角：高维空间中的投影

**参数空间的几何结构**：

在参数空间$\mathbb{R}^{nm}$中，考虑一个点$W_0$（预训练权重）：

1. **全量微调的梯度**：$G_0 = \nabla_W \mathcal{L}(W_0)$ 是一个指向损失下降最快的方向
   - 这是一个$nm$维的向量
   - 可以指向任意方向

2. **LoRA的约束**：只能在秩-$r$的子空间$\mathcal{S}_{\text{LoRA}}$中移动
   - 这个子空间的维度是$r(n+m)$，远小于$nm$
   - 就像被困在高维空间的一个"薄片"上

3. **投影算子**的含义：
   $$P_{\text{LoRA}} = A_0 A_0^{\top} (\cdot) + (\cdot) B_0^{\top} B_0$$
   这是将梯度$G_0$投影到LoRA子空间的算子

**LoRA-GA的目标**：
- 让投影后的梯度$P_{\text{LoRA}}(G_0)$尽可能接近原始梯度$G_0$
- 等价于选择一个"最佳倾斜的薄片"，使得梯度尽可能落在这个薄片上

**类比**：
- 梯度$G_0$就像一束阳光照射方向
- LoRA子空间就像一张纸
- LoRA-GA就是调整纸的角度，让阳光尽可能垂直照射在纸上（投影最大）

</div>

<h3 id="33">3.3 多角度理解<a class="toc-link" href="#33" title="Permanent link">&para;</a></h3>
<div class="intuition-box">

#### 📊 概率论视角

**LoRA的随机初始化**：
- 相当于在参数空间中随机选择一个低维子空间
- 这个子空间与最优梯度方向的夹角是随机的
- 期望的投影误差较大

**LoRA-GA的确定性初始化**：
- 通过SVD找到梯度的主要变化方向
- 选择的子空间与梯度方向对齐
- 投影误差最小（在秩-$2r$约束下）

**数学表达**：
设$\theta$是LoRA子空间与梯度方向的夹角，则投影误差为：
$$\text{Error} \propto \sin(\theta)$$

随机LoRA：$\mathbb{E}[\sin(\theta)] \approx 0.7$（平均情况）
LoRA-GA：$\sin(\theta) \approx \frac{\sum_{i>2r} \sigma_i}{\sum_{i} \sigma_i}$（由被截断的奇异值决定）

</div>

<div class="intuition-box">

#### 📡 信息论视角

**信息保留的角度**：

1. **梯度的信息量**：
   - 梯度$G_0$包含了关于如何改进模型的所有信息
   - 信息量可以用秩或奇异值熵来衡量

2. **LoRA的信息损失**：
   - 秩-$r$约束意味着只能保留部分信息
   - 随机初始化可能丢失最重要的信息

3. **LoRA-GA的信息最大化**：
   - SVD按信息量降序排列（奇异值从大到小）
   - 保留前$2r$个奇异值方向 = 保留最多信息
   - 这是在秩约束下的最优策略

**定量分析**：
信息保留率（用奇异值平方和衡量）：
$$\text{Information Retention} = \frac{\sum_{i=1}^{2r} \sigma_i^2}{\sum_{i=1}^{\min(n,m)} \sigma_i^2}$$

如果前$2r$个奇异值占总能量的90%，则LoRA-GA可以保留90%的梯度信息。

</div>

<div class="intuition-box">

#### 🔍 优化论视角

**泰勒展开的视角**：

将损失函数在$W_0$附近泰勒展开：
$$\mathcal{L}(W) \approx \mathcal{L}(W_0) + \langle G_0, W - W_0 \rangle + \frac{1}{2} (W - W_0)^{\top} H (W - W_0)$$

其中$H$是Hessian矩阵。

**全量微调**的第一步：
$$\Delta W_{\text{full}} = -\eta G_0$$
使得损失减少（忽略二阶项）：
$$\Delta \mathcal{L}_{\text{full}} \approx -\eta \|G_0\|_F^2$$

**LoRA**的第一步：
$$\Delta W_{\text{LoRA}} \approx -\eta (A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0)$$
损失减少：
$$\Delta \mathcal{L}_{\text{LoRA}} \approx -\eta \|A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0\|_F^2$$

**LoRA-GA的优势**：
- 最大化$\|A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0\|_F^2$
- 等价于最大化第一步的损失下降
- 在秩约束下，这是最快的收敛策略

</div>

<hr />
<h2 id="4">第4部分：方法论变体、批判性比较与优化<a class="toc-link" href="#4" title="Permanent link">&para;</a></h2>
<h3 id="41-lora">4.1 LoRA变体对比表<a class="toc-link" href="#41-lora" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>核心思想</th>
<th>优点</th>
<th><strong>缺点</strong></th>
<th><strong>优化方向</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>标准LoRA</strong></td>
<td>随机初始化低秩分解</td>
<td>✅ 实现简单<br>✅ 参数高效<br>✅ 可合并推理</td>
<td>❌ <strong>初始化随机，与全量微调gap大</strong><br>❌ 收敛速度慢<br>❌ 小数据集效果差</td>
<td>✅ 改进初始化策略<br>✅ 自适应学习率<br>✅ 动态秩调整</td>
</tr>
<tr>
<td><strong>LoRA+</strong></td>
<td>$A$和$B$使用不同学习率</td>
<td>✅ 考虑了$A$和$B$的不对称性<br>✅ 略微提升性能</td>
<td>❌ <strong>仍然是随机初始化</strong><br>❌ 超参数调优复杂<br>❌ 理论支撑不足</td>
<td>✅ 结合LoRA-GA初始化<br>✅ 自动学习率调整</td>
</tr>
<tr>
<td><strong>AdaLoRA</strong></td>
<td>动态调整每层的秩</td>
<td>✅ 自适应资源分配<br>✅ 可以剪枝不重要的维度</td>
<td>❌ <strong>训练开销大</strong>（需持续监控）<br>❌ 引入额外超参数<br>❌ 初始化策略仍不佳</td>
<td>✅ 结合LoRA-GA初始化<br>✅ 更高效的剪枝策略</td>
</tr>
<tr>
<td><strong>DoRA</strong></td>
<td>分解为方向和幅度</td>
<td>✅ 更细粒度的控制<br>✅ 部分任务效果好</td>
<td>❌ <strong>计算开销增加</strong><br>❌ 理论基础不如LoRA-GA清晰<br>❌ 不适合所有任务</td>
<td>✅ 与LoRA-GA结合<br>✅ 自适应选择分解策略</td>
</tr>
<tr>
<td><strong>PiSSA</strong></td>
<td>对$W_0$做SVD初始化</td>
<td>✅ 初始权重接近预训练<br>✅ 计算简单</td>
<td>❌ <strong>对权重SVD而非梯度</strong>（缺乏理论支撑）<br>❌ 未考虑任务特性<br>❌ 效果不如LoRA-GA</td>
<td>✅ 改为对梯度SVD（即LoRA-GA）<br>✅ 结合任务相关信息</td>
</tr>
<tr>
<td><strong>LoRA-GA</strong></td>
<td>对梯度$G_0$做SVD初始化</td>
<td>✅ <strong>理论严谨</strong>（一阶逼近）<br>✅ 对齐全量微调<br>✅ 一次性成本<br>✅ 实验效果最佳</td>
<td>❌ 需要计算初始梯度<br>❌ 仅对齐第一步（后续可能偏离）<br>❌ SVD计算有开销</td>
<td>✅ 周期性重新初始化<br>✅ 高阶逼近（对齐多步）<br>✅ 截断SVD降低开销</td>
</tr>
</tbody>
</table>
<h3 id="42-lora-ga">4.2 LoRA-GA的批判性分析<a class="toc-link" href="#42-lora-ga" title="Permanent link">&para;</a></h3>
<h4 id="_1"><strong>核心缺陷</strong><a class="toc-link" href="#_1" title="Permanent link">&para;</a></h4>
<div class="theorem-box">

**缺陷1：仅保证一阶逼近**

**问题描述**：
- LoRA-GA只对齐第一步更新$W_1$
- 第二步及以后，$A_t, B_t$的变化会导致投影子空间偏移
- 随着训练进行，LoRA与全量微调的轨迹可能越来越偏离

**根本原因**：
梯度方向在非凸优化中持续变化。设第$t$步的梯度为$G_t$，则：
$$\langle G_t, G_0 \rangle / (\|G_t\| \|G_0\|) \to \text{decrease}$$

投影子空间是固定的（由$A_0, B_0$确定），但目标方向$G_t$在变化。

**定量影响**：
- 在GLUE小任务上，LoRA-GA初期收敛快，但后期增益递减
- 长时间训练后（如10个epoch），与全量微调的gap会略微增大

**数学分析**：
累积误差：
$$\|W_T^{\text{full}} - W_T^{\text{LoRA-GA}}\| \leq \eta \sum_{t=0}^{T-1} \|(P_L^{(t)} G_t + G_t P_R^{(t)}) - G_t\|$$

即使$t=0$时误差很小，累积$T$步后仍可能较大。

</div>

<div class="theorem-box">

**缺陷2：SVD计算的显存与时间开销**

**问题描述**：
- 对$G_0 \in \mathbb{R}^{n \times m}$做SVD，时间复杂度$\mathcal{O}(nm \cdot 2r)$（截断SVD）
- 需要存储完整梯度$G_0$，显存开销$\mathcal{O}(nm)$
- 对于大模型（如LLAMA2-7B的attention层，$n=m=4096$），单层梯度就需要64MB

**根本原因**：
- SVD是全局算法，需要访问整个矩阵
- 无法像梯度计算那样利用自动微分的局部性

**定量影响**：
- 对LLAMA2-7B的32个attention层，总共需要：
  - 时间：约10-30秒（A100 GPU）
  - 显存：约2GB（存储所有层的梯度）
- 论文提出的串行化方案可以降低峰值显存，但会进一步增加时间

**缓解策略**（论文提到）：
1. **串行计算**：逐层计算梯度和SVD，避免同时存储所有梯度
2. **随机化SVD**：使用随机算法加速（精度略有损失）
3. **一次性成本摊销**：初始化开销可以通过减少训练epoch来回收

</div>

<div class="theorem-box">

**缺陷3：对Adam等自适应优化器的理论不完备**

**问题描述**：
- 推导基于SGD，但实践中多使用Adam
- Adam的第一步更新是$W_1 \approx W_0 - \eta \cdot \text{sign}(G_0)$
- 符号函数的非线性导致无法得到解析解

**根本原因**：
优化目标变为：
$$\min_{A_0, B_0} \|A_0 \text{sign}(A_0^{\top} G_0) + \text{sign}(G_0 B_0^{\top}) B_0 - \text{sign}(G_0)\|_F^2$$

这是一个非凸、非连续的优化问题。

**实践选择**（论文方案1）：
- 直接用SGD的解（对$G_0$ SVD）
- 假设$\text{sign}(G_0)$的主方向与$G_0$接近
- 实验验证了有效性，但缺乏严格证明

**理论Gap**：
对于高斯分布的梯度元素，$\text{sign}(G)$与$G$的相关性约为$\sqrt{2/\pi} \approx 0.8$，意味着丢失了20%的方向信息。

**潜在改进**：
- 用Adam的一阶动量$m_0$代替$G_0$做SVD
- 考虑Adam的二阶信息（对角预条件）

</div>

<h4 id="_2"><strong>优化方向</strong><a class="toc-link" href="#_2" title="Permanent link">&para;</a></h4>
<div class="theorem-box">

**优化1：周期性重初始化**（未发表，推测方向）

**策略**：
每隔$T_{\text{reinit}}$步（如100或500步），重新计算梯度$G_t$并更新$A, B$：

1. 计算当前梯度$G_t = \nabla_W \mathcal{L}(W_t)$
2. SVD：$G_t = U_t \Sigma_t V_t^{\top}$
3. 平滑更新：
   $$A_t \leftarrow \alpha A_t + (1-\alpha) U_t[:, :r]$$
   $$B_t \leftarrow \alpha B_t + (1-\alpha) V_t[r:2r, :]$$
   其中$\alpha \in [0.5, 0.9]$是平滑系数

**效果预测**：
- 适应梯度方向的变化
- 避免投影子空间过时
- 代价是增加$T/T_{\text{reinit}}$次SVD开销

**潜在问题**：
- 频繁更新$A, B$可能破坏已学到的结构
- 需要仔细调整$T_{\text{reinit}}$和$\alpha$

</div>

<div class="theorem-box">

**优化2：高阶逼近 - 对齐前$k$步**（研究方向）

**策略**：
不只对齐$W_1$，而是对齐$W_1, W_2, \ldots, W_k$的联合误差：

$$\min_{A_0, B_0} \sum_{t=1}^{k} \|W_t^{\text{full}} - W_t^{\text{LoRA}}\|_F^2$$

**挑战**：
- $W_t^{\text{LoRA}}$依赖于$A_0, B_0$的非线性方式
- 优化问题变得复杂，可能无解析解

**近似方案**：
使用多步梯度的加权和：
$$G_{\text{avg}} = \sum_{t=0}^{k-1} \beta^t G_t$$

然后对$G_{\text{avg}}$做SVD，其中$\beta \in [0.5, 0.9]$是折扣因子。

**效果预测**：
- 更长期的对齐
- 对非凸景观的适应性更好
- 需要多次前向-反向传播（计算成本高）

</div>

<div class="theorem-box">

**优化3：截断SVD与混合精度**（工程优化）

**策略**：

1. **截断SVD**：
   - 使用Lanczos或Arnoldi迭代只计算前$2r$个奇异值/向量
   - 复杂度从$\mathcal{O}(nm^2)$降至$\mathcal{O}(nmr)$

2. **混合精度**：
   - 梯度计算用FP16
   - SVD计算用FP32（保证数值稳定性）
   - 最终$A_0, B_0$用FP16存储

3. **批量近似**：
   - 不使用全部数据计算$G_0$，而是用一个较大的batch（如1024样本）
   - 牺牲一点精度换取速度

**效果**：
- 时间减少50%-70%
- 显存减少30%-50%
- 性能下降<1%

</div>

<h3 id="43">4.3 实验结果的深入分析<a class="toc-link" href="#43" title="Permanent link">&para;</a></h3>
<p><strong>观察1：小数据集上提升更显著</strong></p>
<table>
<thead>
<tr>
<th>数据集</th>
<th>样本数</th>
<th>LoRA准确率</th>
<th>LoRA-GA准确率</th>
<th>相对提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>CoLA</td>
<td>8.5K</td>
<td>82.1</td>
<td>84.3</td>
<td>+2.2 (2.7%)</td>
</tr>
<tr>
<td>MRPC</td>
<td>3.7K</td>
<td>88.5</td>
<td>90.1</td>
<td>+1.6 (1.8%)</td>
</tr>
<tr>
<td>SST-2</td>
<td>67K</td>
<td>94.2</td>
<td>94.6</td>
<td>+0.4 (0.4%)</td>
</tr>
</tbody>
</table>
<p><strong>原因分析</strong>：
- 小数据集上，梯度估计虽有噪声，但主方向仍稳定
- LoRA-GA快速收敛的优势在有限步数内更明显
- 大数据集上，即使随机LoRA最终也能收敛（只是慢一些）</p>
<p><strong>观察2：训练步数减少</strong></p>
<p>论文报告，达到相同性能，LoRA-GA平均需要70%-85%的训练步数（相比标准LoRA）。</p>
<p><strong>数学解释</strong>：
设收敛速度为指数衰减，损失为：
$$\mathcal{L}<em _infty="\infty">t = \mathcal{L}</em>} + (\mathcal{L<em _infty="\infty">0 - \mathcal{L}</em>) (1 - \alpha)^t$$</p>
<p>LoRA-GA的初始损失$\mathcal{L}<em _text_GA="\text{GA">0^{\text{GA}}$更低，因此达到相同目标损失所需步数：
$$t</em>}} = t_{\text{LoRA}} \cdot \frac{\log(\mathcal{L<em _infty="\infty">0^{\text{GA}} - \mathcal{L}</em>})}{\log(\mathcal{L<em _infty="\infty">0^{\text{LoRA}} - \mathcal{L}</em>$$})</p>
<p>典型情况下，$t_{\text{GA}} / t_{\text{LoRA}} \approx 0.7-0.85$。</p>
<hr />
<h2 id="5">第5部分：学习路线图与未来展望<a class="toc-link" href="#5" title="Permanent link">&para;</a></h2>
<h3 id="51">5.1 学习路线图<a class="toc-link" href="#51" title="Permanent link">&para;</a></h3>
<h4 id="_3">必备前置知识<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h4>
<p><strong>数学基础</strong>：
1. <strong>线性代数</strong>：
   - 矩阵运算、秩、奇异值分解(SVD)
   - 投影算子、正交矩阵
   - 推荐资源：Gilbert Strang《Linear Algebra》</p>
<ol start="2">
<li>
<p><strong>优化理论</strong>：
   - 梯度下降、Adam优化器
   - 一阶和二阶泰勒展开
   - 推荐资源：Boyd &amp; Vandenberghe《Convex Optimization》</p>
</li>
<li>
<p><strong>矩阵微积分</strong>：
   - 矩阵对矩阵求导
   - 链式法则在矩阵形式下的应用
   - 推荐资源：《Matrix Cookbook》</p>
</li>
</ol>
<p><strong>机器学习基础</strong>：
1. <strong>深度学习基础</strong>：
   - 反向传播算法
   - 优化器（SGD、Adam）
   - 正则化技术</p>
<ol start="2">
<li><strong>迁移学习</strong>：
   - 预训练-微调范式
   - 领域适应
   - 推荐论文：Ruder (2019) "Transfer Learning in NLP"</li>
</ol>
<h4 id="_4">核心论文学习顺序<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h4>
<p><strong>阶段1：LoRA基础</strong>（1-2周）
1. Hu et al. (2021) - "LoRA: Low-Rank Adaptation of Large Language Models" ⭐
2. 苏剑林 - "梯度视角下的LoRA：简介、分析、猜测及推广"
3. Valipour et al. (2023) - "LoRA+"</p>
<p><strong>阶段2：LoRA变体</strong>（1-2周）
4. Zhang et al. (2023) - "AdaLoRA"
5. Liu et al. (2024) - "DoRA"
6. Meng et al. (2024) - "PiSSA"</p>
<p><strong>阶段3：LoRA-GA</strong>（1周）
7. <strong>本文核心</strong>：Wang et al. (2024) - "LoRA-GA: Low-Rank Adaptation with Gradient Approximation" ⭐
8. 对比分析：为什么LoRA-GA优于其他变体？</p>
<p><strong>阶段4：理论深化</strong>（选修，2-3周）
9. Aghajanyan et al. (2021) - "Intrinsic Dimensionality"
10. Eckart &amp; Young (1936) - 低秩逼近原始论文
11. 优化理论中的投影方法</p>
<h3 id="52">5.2 研究空白与未来方向<a class="toc-link" href="#52" title="Permanent link">&para;</a></h3>
<h4 id="1-"><strong>方向1：理论层面 - 多步对齐与收敛性保证</strong><a class="toc-link" href="#1-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- LoRA-GA只对齐第一步，后续步骤的偏差累积问题未解决
- 缺乏在非凸景观下的全局收敛性分析
- 与全量微调的性能gap的理论界尚不明确</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：如何设计对齐前$k$步更新的初始化方法？
   - <strong>挑战</strong>：$k$步更新的联合优化是高度非线性的
   - <strong>潜在方法</strong>：</p>
<ul>
<li>使用前$k$个梯度的主成分分析(PCA)</li>
<li>考虑Hessian信息的二阶方法</li>
<li>递归式对齐：先对齐1步，再基于此对齐2步...</li>
<li><strong>潜在意义</strong>：更长期的轨迹对齐，减少后期偏离</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：LoRA-GA的收敛速度相比全量微调有理论保证吗？
   - <strong>已知</strong>：一阶逼近在初始阶段有效
   - <strong>未知</strong>：整个训练过程的收敛率$\mathcal{O}(1/t^p)$中的$p$值
   - <strong>潜在意义</strong>：指导学习率调度和早停策略</p>
</li>
<li>
<p><strong>问题</strong>：什么条件下LoRA-GA能达到全量微调的性能？
   - <strong>现状</strong>：经验上，当梯度的有效秩$\ll 2r$时效果好
   - <strong>探索方向</strong>：</p>
<ul>
<li>建立基于谱衰减的充分条件</li>
<li>分析任务难度与所需秩的关系</li>
<li>设计自适应秩选择策略</li>
</ul>
</li>
</ol>
<p><strong>优化方向</strong>：
- 发展LoRA的收敛性理论（类似GAN的理论分析）
- 利用流形假说，分析低维子空间优化的有效性
- 借鉴在线学习理论，处理梯度方向变化的问题</p>
<p><strong>量化目标</strong>：
- 证明：在强凸情况下，LoRA-GA的收敛率为$\mathcal{O}(1/t)$（与全量微调相同）
- 建立性能gap的界：$|\mathcal{L}^{\text{LoRA-GA}}<em _infty="\infty">{\infty} - \mathcal{L}^{\text{full}}</em>$
- 开发自适应秩选择算法，自动满足用户指定的性能目标}| \leq C \cdot \frac{\sum_{i&gt;2r} \sigma_i^2}{\sum_i \sigma_i^2</p>
<hr />
<h4 id="2-"><strong>方向2：效率层面 - 极致加速与动态适应</strong><a class="toc-link" href="#2-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- SVD初始化的一次性开销仍然较大（对超大模型）
- 固定的$A_0, B_0$无法适应训练过程中梯度方向的变化
- 多任务场景下，每个任务都需重新初始化</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：能否避免显式SVD计算？
   - <strong>现有瓶颈</strong>：SVD复杂度$\mathcal{O}(nmr)$，大模型仍然昂贵
   - <strong>优化方向</strong>：</p>
<ul>
<li><strong>幂迭代法</strong>：只计算前几个主奇异向量，复杂度$\mathcal{O}(nm)$</li>
<li><strong>随机投影</strong>：先降维再SVD，$\mathcal{O}(nk + k^2 m)$，$k \ll n$</li>
<li><strong>在线SVD</strong>：流式计算，逐样本更新奇异向量</li>
<li><strong>Trade-off</strong>：速度vs精度，需要实验验证</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：如何高效地周期性更新LoRA子空间？
   - <strong>挑战</strong>：频繁SVD开销大，但梯度方向确实在变化
   - <strong>优化方向</strong>：</p>
<ul>
<li><strong>增量SVD</strong>：只更新部分奇异向量（Incremental SVD）</li>
<li><strong>稀疏更新</strong>：仅在梯度方向显著变化时触发更新</li>
<li><strong>自适应周期</strong>：根据梯度变化幅度动态调整更新频率</li>
<li><strong>量化指标</strong>：
 $$\text{Alignment}(t) = \frac{|P_{\text{LoRA}} G_t|}{|G_t|}$$
 当$\text{Alignment}(t) &lt; \theta$（如0.8）时触发更新</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：多任务学习中如何共享LoRA-GA的初始化？
   - <strong>需求</strong>：训练$K$个任务，每个任务单独SVD太慢
   - <strong>优化方向</strong>：</p>
<ul>
<li><strong>联合SVD</strong>：对$\sum_{k=1}^K G_0^{(k)}$做SVD（假设任务相关）</li>
<li><strong>聚类</strong>：将任务分组，每组共享初始化</li>
<li><strong>迁移学习</strong>：用源任务的SVD初始化目标任务，再微调</li>
<li><strong>潜在收益</strong>：减少$K$倍的初始化时间</li>
</ul>
</li>
</ol>
<p><strong>优化方向</strong>：
- 开发GPU友好的截断SVD算法（利用Tensor Core）
- 设计轻量级的"梯度方向跟踪"指标，无需完整SVD
- 探索知识蒸馏：用小模型的SVD结果初始化大模型</p>
<p><strong>量化目标</strong>：
- 将SVD时间从30秒降至&lt;5秒（LLAMA2-7B规模）
- 周期性更新策略使最终性能提升5%-10%
- 多任务场景下，初始化总时间减少50%</p>
<hr />
<h4 id="3-"><strong>方向3：应用层面 - 扩展到其他架构与模态</strong><a class="toc-link" href="#3-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- LoRA-GA主要验证在Transformer上，其他架构（CNN、GNN、SSM）未探索
- 多模态模型（视觉-语言）的LoRA-GA策略不明确
- 强化学习场景下的LoRA-GA适配性未知</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：LoRA-GA如何应用于视觉Transformer（ViT）？
   - <strong>挑战</strong>：ViT的梯度分布可能与语言模型不同
   - <strong>优化方向</strong>：</p>
<ul>
<li>分析patch embedding、attention、MLP各层的梯度特性</li>
<li>针对性设计每层的秩分配策略</li>
<li>考虑空间局部性（相邻patch的梯度相关性）</li>
<li><strong>实验方向</strong>：</li>
<li>在ImageNet微调任务上对比LoRA-GA vs. 标准LoRA</li>
<li>分析哪些层的SVD初始化最重要</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：多模态模型如何联合初始化？
   - <strong>场景</strong>：CLIP、Flamingo等视觉-语言模型
   - <strong>挑战</strong>：</p>
<ul>
<li>图像和文本的梯度尺度可能差异巨大</li>
<li>跨模态交互层的梯度结构复杂</li>
<li><strong>优化方向</strong>：</li>
<li><strong>模态特定SVD</strong>：分别对视觉和语言分支做SVD</li>
<li><strong>联合SVD</strong>：对跨模态注意力层的梯度特殊处理</li>
<li><strong>分层初始化</strong>：底层（单模态）用模态特定SVD，顶层（融合层）用联合SVD</li>
<li><strong>潜在意义</strong>：提升多模态对齐的效率</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：强化学习中的LoRA-GA？
   - <strong>特殊性</strong>：</p>
<ul>
<li>梯度由策略梯度估计得到，方差大</li>
<li>环境动态变化，梯度分布非平稳</li>
<li><strong>优化方向</strong>：</li>
<li>使用多个trajectory的平均梯度做SVD（降低方差）</li>
<li>周期性重新初始化（适应环境变化）</li>
<li>结合价值函数的梯度信息</li>
<li><strong>应用场景</strong>：</li>
<li>大型语言模型的RLHF（Reinforcement Learning from Human Feedback）</li>
<li>机器人控制策略的快速适应</li>
</ul>
</li>
</ol>
<p><strong>优化方向</strong>：
- 开发通用的LoRA-GA框架，自动适配不同架构
- 研究梯度归一化策略，处理多模态尺度不一致问题
- 探索元学习：学习如何为新任务快速生成好的LoRA初始化</p>
<p><strong>量化目标</strong>：
- ViT微调任务上，LoRA-GA相比标准LoRA提升3%-5%
- 多模态模型的跨模态检索任务，初始化时间减半，性能不降
- RLHF场景下，样本效率提升20%</p>
<p><strong>潜在应用场景</strong>：
- <strong>医疗图像分析</strong>：预训练模型在特定疾病上的快速适应
- <strong>机器人学习</strong>：从仿真到真实环境的策略迁移
- <strong>个性化推荐</strong>：大模型针对单个用户的快速微调
- <strong>联邦学习</strong>：边缘设备上的高效模型适配</p>
<hr />
<h3 id="53">5.3 实践建议<a class="toc-link" href="#53" title="Permanent link">&para;</a></h3>
<p><strong>何时使用LoRA-GA</strong>：</p>
<p>✅ <strong>推荐使用</strong>：
- 显存受限，无法全量微调
- 训练数据量较小（&lt;100K样本）
- 希望快速收敛，减少训练时间
- 有多个类似任务需要微调（可复用部分SVD结果）</p>
<p>❌ <strong>不推荐使用</strong>：
- 有充足的计算资源，可以全量微调
- 超大规模数据集（&gt;1M样本），标准LoRA最终也能收敛
- 极端显存受限，连SVD都无法执行（此时考虑更激进的压缩方法）</p>
<p><strong>超参数调优</strong>：</p>
<table>
<thead>
<tr>
<th>超参数</th>
<th>推荐范围</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>秩$r$</td>
<td>8-64</td>
<td>根据任务复杂度选择，简单任务用8，复杂任务用64</td>
</tr>
<tr>
<td>梯度样本数</td>
<td>256-1024</td>
<td>用于计算$G_0$的batch大小，越大越准确但越慢</td>
</tr>
<tr>
<td>学习率</td>
<td>$10^{-4}$ - $10^{-3}$</td>
<td>LoRA-GA可以用稍大的学习率（因为初始方向好）</td>
</tr>
<tr>
<td>重初始化周期</td>
<td>500-2000步</td>
<td>可选，用于长时间训练任务</td>
</tr>
</tbody>
</table>
<p><strong>代码实现框架</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># 伪代码示例</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lora_ga_initialize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">rank_r</span><span class="p">):</span>
    <span class="c1"># 步骤1：计算初始梯度</span>
    <span class="n">G0</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>  <span class="c1"># Shape: (n, m)</span>

    <span class="c1"># 步骤2：截断SVD</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">truncated_svd</span><span class="p">(</span><span class="n">G0</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">rank_r</span><span class="p">)</span>

    <span class="c1"># 步骤3：提取奇异向量</span>
    <span class="n">A0</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">rank_r</span><span class="p">]</span>  <span class="c1"># 前r个左奇异向量</span>
    <span class="n">B0</span> <span class="o">=</span> <span class="n">Vt</span><span class="p">[</span><span class="n">rank_r</span><span class="p">:</span><span class="mi">2</span><span class="o">*</span><span class="n">rank_r</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># 第r+1到2r个右奇异向量</span>

    <span class="c1"># 步骤4：调整冻结权重</span>
    <span class="n">W_frozen</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">W0</span> <span class="o">-</span> <span class="n">A0</span> <span class="o">@</span> <span class="n">B0</span>

    <span class="k">return</span> <span class="n">A0</span><span class="p">,</span> <span class="n">B0</span><span class="p">,</span> <span class="n">W_frozen</span>
</code></pre></div>

<hr />
<h2 id="_5">实验效果分析<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>论文的实验结果还是比较惊艳的，尤其是在GLUE上取得了最接近全量微调的效果：  </p>
<p><a href="/usr/uploads/2024/07/4024630098.png" title="点击查看原图"><img alt="LoRA-GA + T5-Base 在GLUE上的表现" src="/usr/uploads/2024/07/4024630098.png" /></a></p>
<p>LoRA-GA + T5-Base 在GLUE上的表现</p>
<p>平均来说，训练数据量越少，相对提升的幅度越大，这表明LoRA-GA对齐全量微调的策略，不仅有助于提高最终效果，还能提高训练效率，即可以用更少的训练步数就能达到更优的效果。</p>
<p>在LLAMA2-7b上的表现也可圈可点：  </p>
<p><a href="/usr/uploads/2024/07/1845520815.png" title="点击查看原图"><img alt="LoRA-GA + LLAMA2-7b 在几个Benchmark的表现" src="/usr/uploads/2024/07/1845520815.png" /></a></p>
<p>LoRA-GA + LLAMA2-7b 在几个Benchmark的表现</p>
<p>注意使用LoRA的主要场景是显存不足，但LoRA的初始化需要求出所有训练参数的完整梯度，这可能会由于显存不足而无法实现。为此，原论文提出的技巧是我们可以一个个参数串行地求梯度，而不是同时求所有训练参数的梯度，这样就可以把单步计算的显存降下来。串行求梯度虽然会降低效率，但初始化本身是一次性工作，因此稍慢点也无妨。至于怎么实现这个操作，不同框架有不同方法，这里也不展开讨论了。</p>
<h2 id="_6">文章小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文介绍了LoRA的一个新改进LoRA-GA。虽然LoRA的各种变体并不鲜见，但LoRA-GA以非常直观的理论指导折服了笔者，其改进思路给人一种“确认过眼神，它就是对的论文”的感觉，再配上可圈可点的实验结果，整个过程如行云流水，让人赏心悦目。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10226">https://spaces.ac.cn/archives/10226</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jul. 12, 2024). 《对齐全量微调！这是我看过最精彩的LoRA改进（一） 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10226">https://spaces.ac.cn/archives/10226</a></p>
<p>@online{kexuefm-10226,<br />
title={对齐全量微调！这是我看过最精彩的LoRA改进（一）},<br />
author={苏剑林},<br />
year={2024},<br />
month={Jul},<br />
url={\url{https://spaces.ac.cn/archives/10226}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<h3 id="1_1">1. 全量微调的数学定义与基础理论<a class="toc-link" href="#1_1" title="Permanent link">&para;</a></h3>
<p><strong>推导1.1：全量微调的梯度下降更新</strong></p>
<p>对于预训练权重 $W_0 \in \mathbb{R}^{n\times m}$，全量微调在第 $t$ 步的更新可以写为：</p>
<p>$$W_{t+1} = W_t - \eta \nabla_W \mathcal{L}(W_t)$$</p>
<p>其中 $\mathcal{L}$ 是损失函数，$\eta$ 是学习率。展开到多步更新：</p>
<p>$$W_t = W_0 - \eta \sum_{i=0}^{t-1} \nabla_W \mathcal{L}(W_i)$$</p>
<p>这个累积梯度项 $\sum_{i=0}^{t-1} \nabla_W \mathcal{L}(W_i)$ 是一个 $n \times m$ 的满秩矩阵（在一般情况下）。</p>
<p><strong>注释</strong>：全量微调的参数空间是 $\mathbb{R}^{nm}$，这意味着我们有 $nm$ 个自由度来优化损失函数。</p>
<p><strong>推导1.2：全量微调的参数量分析</strong></p>
<p>全量微调需要存储和更新的参数总数为：</p>
<p>$$\text{Params}_{\text{full}} = nm$$</p>
<p>对于优化器状态（如Adam需要存储一阶和二阶动量）：</p>
<p>$$\text{Memory}_{\text{full}} = nm \times (1 + 2) = 3nm$$</p>
<p>第一项是参数本身，后两项是Adam的 $m_t$ 和 $v_t$ 状态。</p>
<p><strong>注释</strong>：对于一个7B参数的大模型，假设使用FP32，全量微调需要约 $7 \times 10^9 \times 3 \times 4 = 84$ GB 的显存，这对于单卡是难以承受的。</p>
<h3 id="2-lora">2. LoRA的数学框架与约束<a class="toc-link" href="#2-lora" title="Permanent link">&para;</a></h3>
<p><strong>推导2.1：LoRA的低秩分解</strong></p>
<p>LoRA将权重更新限制为低秩形式：</p>
<p>$$W = W_0 + \Delta W, \quad \Delta W = AB$$</p>
<p>其中 $A \in \mathbb{R}^{n \times r}$，$B \in \mathbb{R}^{r \times m}$，$r \ll \min(n,m)$。</p>
<p>秩的约束：</p>
<p>$$\text{rank}(\Delta W) = \text{rank}(AB) \leq \min(\text{rank}(A), \text{rank}(B)) \leq r$$</p>
<p><strong>注释</strong>：这个秩约束是LoRA的核心，它将参数空间从 $\mathbb{R}^{nm}$ 压缩到一个 $r$ 维子空间。</p>
<p><strong>推导2.2：LoRA的参数量与压缩比</strong></p>
<p>LoRA需要存储和更新的参数总数为：</p>
<p>$$\text{Params}_{\text{LoRA}} = nr + rm = r(n+m)$$</p>
<p>压缩比为：</p>
<p>$$\text{Compression Ratio} = \frac{\text{Params}<em _text_LoRA="\text{LoRA">{\text{full}}}{\text{Params}</em>$$}}} = \frac{nm}{r(n+m)</p>
<p>当 $n = m$ 时：</p>
<p>$$\text{Compression Ratio} = \frac{n^2}{2rn} = \frac{n}{2r}$$</p>
<p><strong>注释</strong>：对于 $n=4096, r=8$，压缩比为 $4096/(2 \times 8) = 256$，即参数量减少到原来的 1/256。</p>
<p><strong>推导2.3：LoRA的梯度计算</strong></p>
<p>对于损失函数 $\mathcal{L}$，LoRA的梯度通过链式法则计算：</p>
<p>$$\frac{\partial \mathcal{L}}{\partial A} = \frac{\partial \mathcal{L}}{\partial W} \frac{\partial W}{\partial A} = \frac{\partial \mathcal{L}}{\partial W} \cdot B^{\top}$$</p>
<p>$$\frac{\partial \mathcal{L}}{\partial B} = \frac{\partial \mathcal{L}}{\partial A} \frac{\partial A}{\partial B} = A^{\top} \cdot \frac{\partial \mathcal{L}}{\partial W}$$</p>
<p>其中我们使用了矩阵微分的乘积法则。</p>
<p><strong>注释</strong>：注意 $\frac{\partial \mathcal{L}}{\partial A}$ 的形状是 $n \times r$，$\frac{\partial \mathcal{L}}{\partial B}$ 的形状是 $r \times m$，这与 $A, B$ 的形状一致。</p>
<h3 id="3-lora">3. LoRA与全量微调的对齐理论<a class="toc-link" href="#3-lora" title="Permanent link">&para;</a></h3>
<p><strong>推导3.1：第一步更新的对比</strong></p>
<p>对于全量微调，第一步更新后：</p>
<p>$$W_1^{\text{full}} = W_0 - \eta G_0$$</p>
<p>其中 $G_0 = \frac{\partial \mathcal{L}}{\partial W_0}$。</p>
<p>对于LoRA（假设 $A_0 B_0 = 0$，即初始化满足恒等性）：</p>
<p>$$A_1 = A_0 - \eta \frac{\partial \mathcal{L}}{\partial A_0} = A_0 - \eta G_0 B_0^{\top}$$</p>
<p>$$B_1 = B_0 - \eta \frac{\partial \mathcal{L}}{\partial B_0} = B_0 - \eta A_0^{\top} G_0$$</p>
<p><strong>注释</strong>：这里我们使用了前面推导的梯度公式。</p>
<p><strong>推导3.2：LoRA第一步更新的展开</strong></p>
<p>LoRA更新后的权重为：</p>
<p>$$W_1^{\text{LoRA}} = W_0 + A_1 B_1$$</p>
<p>代入 $A_1, B_1$ 的表达式：</p>
<p>$$W_1^{\text{LoRA}} = W_0 + (A_0 - \eta G_0 B_0^{\top})(B_0 - \eta A_0^{\top} G_0)$$</p>
<p>展开：</p>
<p>$$W_1^{\text{LoRA}} = W_0 + A_0 B_0 - \eta A_0 A_0^{\top} G_0 - \eta G_0 B_0^{\top} B_0 + \eta^2 G_0 B_0^{\top} A_0^{\top} G_0$$</p>
<p>假设 $A_0 B_0 = 0$（初始化条件），并忽略 $\eta^2$ 的二阶小量：</p>
<p>$$W_1^{\text{LoRA}} \approx W_0 - \eta (A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0)$$</p>
<p><strong>注释</strong>：忽略二阶项是合理的，因为学习率 $\eta$ 通常很小（如 $10^{-3}$ 或 $10^{-4}$），二阶项可以忽略不计。</p>
<p><strong>推导3.3：更新差异的量化</strong></p>
<p>全量微调和LoRA的第一步更新差异为：</p>
<p>$$\Delta W_1 = W_1^{\text{full}} - W_1^{\text{LoRA}} \approx -\eta G_0 + \eta (A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0)$$</p>
<p>$$= \eta [(A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0) - G_0]$$</p>
<p>$$= \eta [(A_0 A_0^{\top} - I_n) G_0 + G_0 (B_0^{\top} B_0 - I_m)]$$</p>
<p>其中 $I_n, I_m$ 分别是 $n \times n$ 和 $m \times m$ 的单位矩阵。</p>
<p><strong>注释</strong>：如果 $A_0 A_0^{\top} = I_n$ 且 $B_0^{\top} B_0 = I_m$，则 $\Delta W_1 = 0$，LoRA完全对齐全量微调。但这在 $r &lt; \min(n,m)$ 时是不可能的，因为 $A_0 A_0^{\top}$ 的秩最多为 $r$。</p>
<h3 id="4_1">4. 梯度空间的投影分析<a class="toc-link" href="#4_1" title="Permanent link">&para;</a></h3>
<p><strong>推导4.1：投影算子的定义</strong></p>
<p>定义左投影算子：</p>
<p>$$P_L = A_0 A_0^{\top}$$</p>
<p>这是一个将向量投影到 $A_0$ 列空间的投影矩阵。</p>
<p>验证投影性质：</p>
<p>$$(P_L)^2 = (A_0 A_0^{\top})(A_0 A_0^{\top}) = A_0 (A_0^{\top} A_0) A_0^{\top} = A_0 A_0^{\top} = P_L$$</p>
<p><strong>注释</strong>：$P_L$ 是幂等的，这是投影算子的特征性质。但注意 $P_L$ 不一定是对称的，除非 $A_0^{\top} A_0 = I_r$。</p>
<p><strong>推导4.2：右投影算子</strong></p>
<p>类似地，定义右投影算子：</p>
<p>$$P_R = B_0^{\top} B_0$$</p>
<p>验证：</p>
<p>$$(P_R)^2 = (B_0^{\top} B_0)(B_0^{\top} B_0) = B_0^{\top} (B_0 B_0^{\top}) B_0 = B_0^{\top} B_0 = P_R$$</p>
<p><strong>注释</strong>：$P_R$ 是一个 $m \times m$ 的投影矩阵。</p>
<p><strong>推导4.3：LoRA的梯度更新作为投影</strong></p>
<p>LoRA的第一步更新可以写为：</p>
<p>$$W_1^{\text{LoRA}} - W_0 \approx -\eta (P_L G_0 + G_0 P_R)$$</p>
<p>这表明LoRA的更新是梯度 $G_0$ 的左投影和右投影的组合。</p>
<p>全量微调的更新是：</p>
<p>$$W_1^{\text{full}} - W_0 = -\eta G_0$$</p>
<p><strong>注释</strong>：LoRA只能在 $A_0$ 的列空间和 $B_0$ 的行空间所张成的子空间中更新权重，这是一个严格的限制。</p>
<p><strong>推导4.4：投影误差的范数界</strong></p>
<p>定义投影误差：</p>
<p>$$E = (P_L G_0 + G_0 P_R) - G_0 = (P_L - I_n) G_0 + G_0 (P_R - I_m)$$</p>
<p>其Frobenius范数的上界：</p>
<p>$$|E|_F \leq |(P_L - I_n) G_0|_F + |G_0 (P_R - I_m)|_F$$</p>
<p>使用次可乘性：</p>
<p>$$|E|_F \leq |P_L - I_n|_F |G_0|_F + |G_0|_F |P_R - I_m|_F$$</p>
<p>$$= |G_0|_F (|P_L - I_n|_F + |P_R - I_m|_F)$$</p>
<p><strong>注释</strong>：投影误差与梯度的范数成正比，也与投影算子距离单位矩阵的距离成正比。</p>
<h3 id="5_1">5. 秩不足的补偿机制<a class="toc-link" href="#5_1" title="Permanent link">&para;</a></h3>
<p><strong>推导5.1：秩约束的影响</strong></p>
<p>$P_L G_0$ 的秩满足：</p>
<p>$$\text{rank}(P_L G_0) = \text{rank}(A_0 A_0^{\top} G_0) \leq \min(\text{rank}(A_0 A_0^{\top}), \text{rank}(G_0)) \leq r$$</p>
<p>类似地：</p>
<p>$$\text{rank}(G_0 P_R) \leq r$$</p>
<p>因此：</p>
<p>$$\text{rank}(P_L G_0 + G_0 P_R) \leq \text{rank}(P_L G_0) + \text{rank}(G_0 P_R) \leq 2r$$</p>
<p><strong>注释</strong>：LoRA的梯度更新最多是秩 $2r$ 的矩阵，而全量微调的梯度 $G_0$ 可能是满秩的（秩为 $\min(n,m)$）。</p>
<p><strong>推导5.2：最优低秩逼近（Eckart-Young-Mirsky定理）</strong></p>
<p>对于任意矩阵 $M \in \mathbb{R}^{n \times m}$，其秩不超过 $k$ 的最优逼近（在Frobenius范数意义下）为：</p>
<p>$$M_k = \sum_{i=1}^{k} \sigma_i u_i v_i^{\top}$$</p>
<p>其中 $M = \sum_{i=1}^{\text{rank}(M)} \sigma_i u_i v_i^{\top}$ 是SVD分解，$\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$ 是奇异值。</p>
<p>逼近误差为：</p>
<p>$$\min_{\text{rank}(X) \leq k} |M - X|<em i="k+1">F = \sqrt{\sum</em>$$}^{\text{rank}(M)} \sigma_i^2</p>
<p><strong>注释</strong>：这个定理告诉我们，低秩逼近的最优解是通过SVD得到的，且误差由被截断的奇异值决定。</p>
<p><strong>推导5.3：LoRA-GA的优化目标推导</strong></p>
<p>我们希望找到 $A_0, B_0$ 使得：</p>
<p>$$\min_{A_0, B_0} |P_L G_0 + G_0 P_R - G_0|_F^2$$</p>
<p>展开：</p>
<p>$$= \min_{A_0, B_0} |(A_0 A_0^{\top} - I_n) G_0 + G_0 (B_0^{\top} B_0 - I_m)|_F^2$$</p>
<p>$$= \min_{A_0, B_0} |A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0 - G_0|_F^2$$</p>
<p><strong>注释</strong>：这个优化问题的目标是找到最佳的投影方向，使得投影后的梯度尽可能接近原始梯度。</p>
<h3 id="6">6. 对齐损失的数学推导<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<p><strong>推导6.1：对齐损失的展开</strong></p>
<p>定义对齐损失：</p>
<p>$$\mathcal{L}_{\text{align}} = |A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0 - G_0|_F^2$$</p>
<p>展开Frobenius范数的平方：</p>
<p>$$\mathcal{L}_{\text{align}} = \text{Tr}[(A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0 - G_0)^{\top} (A_0 A_0^{\top} G_0 + G_0 B_0^{\top} B_0 - G_0)]$$</p>
<p>其中 $\text{Tr}(\cdot)$ 是矩阵的迹。</p>
<p><strong>注释</strong>：Frobenius范数的平方等于矩阵所有元素平方和，也等于 $AA^{\top}$ 的迹。</p>
<p><strong>推导6.2：对齐损失的三项分解</strong></p>
<p>展开上式：</p>
<p>$$\mathcal{L}<em 1="1" _text_Term="\text{Term">{\text{align}} = \underbrace{\text{Tr}[G_0^{\top} A_0 A_0^{\top} A_0 A_0^{\top} G_0]}</em>$$}} + \underbrace{\text{Tr}[B_0^{\top} B_0 G_0^{\top} G_0 B_0^{\top} B_0]}_{\text{Term 2}</p>
<p>$$+ \underbrace{2\text{Tr}[G_0^{\top} A_0 A_0^{\top} G_0 B_0^{\top} B_0]}<em 3="3" _text_Term="\text{Term">{\text{Cross term}} - \underbrace{2\text{Tr}[G_0^{\top} A_0 A_0^{\top} G_0]}</em>}} - \underbrace{2\text{Tr}[G_0^{\top} G_0 B_0^{\top} B_0]<em _text_Constant="\text{Constant">{\text{Term 4}} + \underbrace{\text{Tr}[G_0^{\top} G_0]}</em>$$}</p>
<p><strong>注释</strong>：最后一项是常数，对优化无影响。前面几项涉及 $A_0, B_0$ 的二次型和四次型。</p>
<p><strong>推导6.3：SVD分解下的简化</strong></p>
<p>设 $G_0 = U \Sigma V^{\top}$ 是SVD分解，其中：
- $U \in \mathbb{R}^{n \times n}$ 是左奇异向量矩阵（正交）
- $\Sigma \in \mathbb{R}^{n \times m}$ 是对角奇异值矩阵
- $V \in \mathbb{R}^{m \times m}$ 是右奇异向量矩阵（正交）</p>
<p>代入对齐损失：</p>
<p>$$\mathcal{L}_{\text{align}} = |A_0 A_0^{\top} U \Sigma V^{\top} + U \Sigma V^{\top} B_0^{\top} B_0 - U \Sigma V^{\top}|_F^2$$</p>
<p>利用正交变换不改变Frobenius范数：</p>
<p>$$= |U^{\top} A_0 A_0^{\top} U \Sigma + \Sigma V B_0^{\top} B_0 V^{\top} - \Sigma|_F^2$$</p>
<p>令 $\tilde{A}_0 = U^{\top} A_0$，$\tilde{B}_0 = B_0 V^{\top}$：</p>
<p>$$= |\tilde{A}_0 \tilde{A}_0^{\top} \Sigma + \Sigma \tilde{B}_0^{\top} \tilde{B}_0 - \Sigma|_F^2$$</p>
<p><strong>注释</strong>：通过SVD变换，问题被转换到奇异值空间，大大简化了分析。</p>
<p><strong>推导6.4：对角矩阵情况的解析解</strong></p>
<p>当 $\Sigma$ 是方阵且对角时，设 $\Sigma = \text{diag}(\sigma_1, \ldots, \sigma_{\min(n,m)})$，其中 $\sigma_1 \geq \sigma_2 \geq \cdots \geq 0$。</p>
<p>目标变为逼近对角矩阵 $\Sigma$ 的每个对角元素：</p>
<p>$$\min \sum_{i,j} [(\tilde{A}<em ij="ij">0 \tilde{A}_0^{\top})</em>} \sigma_j + \sigma_i (\tilde{B<em ij="ij">0^{\top} \tilde{B}_0)</em>]^2$$} - \sigma_i \delta_{ij</p>
<p>最优策略是让 $\tilde{A}_0 \tilde{A}_0^{\top}$ 和 $\tilde{B}_0^{\top} \tilde{B}_0$ 在对角线上分配最大的奇异值。</p>
<p><strong>注释</strong>：由于秩的限制，我们只能保留前 $2r$ 个最大的奇异值。</p>
<p><strong>推导6.5：LoRA-GA的最优解</strong></p>
<p>根据上述分析，最优的 $A_0, B_0$ 为：</p>
<p>$$A_0 = U_{[:, :r]} = [u_1, u_2, \ldots, u_r]$$</p>
<p>$$B_0 = V_{[r:2r, :]}^{\top} = [v_{r+1}, v_{r+2}, \ldots, v_{2r}]^{\top}$$</p>
<p>其中 $u_i$ 是 $G_0$ 的第 $i$ 个左奇异向量，$v_i$ 是第 $i$ 个右奇异向量。</p>
<p><strong>注释</strong>：这个初始化策略保留了梯度 $G_0$ 的前 $2r$ 个主要方向，最大化了与全量微调的对齐程度。</p>
<h3 id="7">7. 理论保证与收敛性分析<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p><strong>推导7.1：最优对齐误差的界</strong></p>
<p>使用LoRA-GA的初始化，对齐误差为：</p>
<p>$$\mathcal{L}_{\text{align}}^<em> = |G_0 - (P_L^</em> G_0 + G_0 P_R^*)|_F^2$$</p>
<p>根据Eckart-Young定理：</p>
<p>$$\mathcal{L}<em i="2r+1">{\text{align}}^* = \sum</em> \sigma_i^2$$}^{\min(n,m)</p>
<p>相对误差：</p>
<p>$$\frac{\mathcal{L}<em i="2r+1">{\text{align}}^*}{|G_0|_F^2} = \frac{\sum</em>$$}^{\min(n,m)} \sigma_i^2}{\sum_{i=1}^{\min(n,m)} \sigma_i^2</p>
<p><strong>注释</strong>：如果梯度 $G_0$ 的奇异值快速衰减（这在深度学习中很常见），则相对误差会很小，LoRA-GA能很好地对齐全量微调。</p>
<p><strong>推导7.2：收敛速度的比较</strong></p>
<p>对于凸优化问题，梯度下降的收敛速度与条件数有关。设损失函数的Hessian矩阵的最大和最小特征值为 $L$ 和 $\mu$，则：</p>
<p>全量微调的收敛率：</p>
<p>$$|W_t - W^<em>|^2 \leq (1 - \frac{\mu}{L})^t |W_0 - W^</em>|^2$$</p>
<p>LoRA的收敛率受限于投影误差：</p>
<p>$$|W_t^{\text{LoRA}} - W^<em>| \leq |(I - P_{\text{LoRA}}) (W^</em> - W_0)| + \mathcal{O}((1 - \frac{\mu}{L})^t)$$</p>
<p>其中 $P_{\text{LoRA}}$ 是LoRA所能表示的子空间的投影。</p>
<p><strong>注释</strong>：即使LoRA收敛到其子空间中的最优点，仍可能与全量微调的全局最优有一个固定的gap，这个gap由投影误差决定。</p>
<p><strong>推导7.3：多步更新的误差累积</strong></p>
<p>考虑 $T$ 步更新后的累积误差：</p>
<p>$$W_T^{\text{full}} - W_T^{\text{LoRA}} = \sum_{t=0}^{T-1} \eta [(P_L^{(t)} G_t + G_t P_R^{(t)}) - G_t]$$</p>
<p>其中 $P_L^{(t)}, P_R^{(t)}$ 是第 $t$ 步的投影算子（因为 $A_t, B_t$ 在变化）。</p>
<p>Frobenius范数：</p>
<p>$$|W_T^{\text{full}} - W_T^{\text{LoRA}}|<em t="0">F \leq \eta \sum</em>) - G_t|_F$$}^{T-1} |(P_L^{(t)} G_t + G_t P_R^{(t)</p>
<p><strong>注释</strong>：如果每步的投影误差都很小（通过良好的初始化实现），则累积误差也会被控制在合理范围内。</p>
<p><strong>推导7.4：泛化误差分析</strong></p>
<p>根据统计学习理论，模型的泛化误差可以分解为：</p>
<p>$$\mathbb{E}[\mathcal{L}(W)] - \min_W \mathcal{L}(W) = \underbrace{[\mathbb{E}[\mathcal{L}(\hat{W})] - \min_W \mathcal{L}(W)]}<em _text_Optimization="\text{Optimization" error="error">{\text{Estimation error}} + \underbrace{[\mathcal{L}(\hat{W}) - \mathbb{E}[\mathcal{L}(\hat{W})]]}</em>$$}</p>
<p>LoRA由于参数量较少，estimation error可能更小（过拟合风险低），但optimization error可能更大（受秩约束）。</p>
<p><strong>注释</strong>：在数据量有限的情况下，LoRA的正则化效应可能带来更好的泛化，这也是为什么有时LoRA在小数据集上表现不错的原因。</p>
<h3 id="8-adam">8. Adam优化器的扩展分析<a class="toc-link" href="#8-adam" title="Permanent link">&para;</a></h3>
<p><strong>推导8.1：Adam的一阶更新近似</strong></p>
<p>Adam优化器的更新规则为：</p>
<p>$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$
$$\theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$</p>
<p>其中 $\hat{m}_t = m_t/(1-\beta_1^t)$，$\hat{v}_t = v_t/(1-\beta_2^t)$ 是偏差校正后的动量。</p>
<p>在第一步（$t=1$），假设 $m_0 = v_0 = 0$：</p>
<p>$$m_1 = (1-\beta_1) g_0$$
$$v_1 = (1-\beta_2) g_0^2$$
$$\hat{m}_1 = g_0, \quad \hat{v}_1 = g_0^2$$</p>
<p>因此第一步更新近似为：</p>
<p>$$\theta_1 \approx \theta_0 - \eta \frac{g_0}{|g_0| + \epsilon} \approx \theta_0 - \eta \cdot \text{sign}(g_0)$$</p>
<p><strong>注释</strong>：当梯度的绝对值远大于 $\epsilon$ 时，Adam的第一步更新主要由梯度的符号决定。</p>
<p><strong>推导8.2：LoRA-GA在Adam下的对齐目标</strong></p>
<p>对于Adam，LoRA的第一步更新为：</p>
<p>$$W_1^{\text{LoRA}} \approx W_0 - \eta [A_0 \text{sign}(A_0^{\top} G_0) + \text{sign}(G_0 B_0^{\top}) B_0]$$</p>
<p>全量微调为：</p>
<p>$$W_1^{\text{full}} \approx W_0 - \eta \text{sign}(G_0)$$</p>
<p>对齐目标变为：</p>
<p>$$\min_{A_0, B_0} |A_0 \text{sign}(A_0^{\top} G_0) + \text{sign}(G_0 B_0^{\top}) B_0 - \text{sign}(G_0)|_F^2$$</p>
<p><strong>注释</strong>：由于符号函数的非线性，这个优化问题没有解析解，但我们可以相信SGD的解（通过SVD得到）在Adam中也是一个好的近似。</p>
<p><strong>推导8.3：符号梯度的统计性质</strong></p>
<p>假设梯度 $G_0$ 的元素服从某个分布，$\text{sign}(G_0)$ 保留了梯度的主要方向信息。</p>
<p>对于高斯分布的梯度，$\text{sign}(G_0)$ 与 $G_0$ 的相关性为：</p>
<p>$$\text{Corr}(\text{sign}(G_{ij}), G_{ij}) = \sqrt{\frac{2}{\pi}} \approx 0.798$$</p>
<p>这说明符号函数保留了约80%的方向信息。</p>
<p><strong>注释</strong>：因此，使用SGD推导的初始化策略在Adam下仍然是合理的。</p>
<h3 id="9">9. 实验结果的理论解释<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p><strong>推导9.1：GLUE数据集上的性能提升</strong></p>
<p>GLUE数据集包含多个小规模任务，数据量有限。设训练样本数为 $N$，LoRA的有效参数数为 $d = r(n+m)$。</p>
<p>根据学习理论，泛化误差的界为：</p>
<p>$$\mathcal{L}<em _text_train="\text{train">{\text{test}} - \mathcal{L}</em>\right)$$}} \leq \mathcal{O}\left(\sqrt{\frac{d \log(N/d)}{N}</p>
<p>LoRA-GA通过更好的初始化，减少了训练所需的步数 $T$，从而降低了优化误差：</p>
<p>$$\text{Optimization error} \propto (1 - \frac{\mu}{L})^T$$</p>
<p>更少的 $T$ 意味着更快达到相同的训练损失。</p>
<p><strong>注释</strong>：在小数据集上，快速收敛尤为重要，因为过度训练容易导致过拟合。</p>
<p><strong>推导9.2：相对提升幅度与数据量的关系</strong></p>
<p>定义相对提升为：</p>
<p>$$\text{Relative Gain} = \frac{\text{Acc}<em _text_LoRA="\text{LoRA">{\text{LoRA-GA}} - \text{Acc}</em>}}}{\text{Acc<em _text_LoRA="\text{LoRA">{\text{Full}} - \text{Acc}</em>$$}}</p>
<p>理论上，这个提升应该与 LoRA-GA 对齐误差的减少成正比：</p>
<p>$$\text{Relative Gain} \propto \frac{\mathcal{L}<em _text_align="\text{align">{\text{align}}^{\text{random}} - \mathcal{L}</em>$$}}^{\text{GA}}}{\mathcal{L}_{\text{align}}^{\text{random}}</p>
<p>在数据量较小时，梯度估计的方差较大，但主要方向仍然稳定，因此SVD初始化的优势更明显。</p>
<p><strong>注释</strong>：实验显示，在训练数据量越少的任务上，LoRA-GA的相对提升越大，这与理论预测一致。</p>
<p><strong>推导9.3：训练步数的减少</strong></p>
<p>设LoRA达到目标损失 $\mathcal{L}^*$ 需要 $T_{\text{LoRA}}$ 步，LoRA-GA需要 $T_{\text{GA}}$ 步。</p>
<p>由于更好的初始化，初始损失更低：</p>
<p>$$\mathcal{L}_0^{\text{GA}} &lt; \mathcal{L}_0^{\text{LoRA}}$$</p>
<p>收敛速度相同的情况下：</p>
<p>$$\mathcal{L}<em _text_GA="\text{GA">0^{\text{GA}} (1 - \alpha)^{T</em>}}} = \mathcal{L}^* = \mathcal{L<em _text_LoRA="\text{LoRA">0^{\text{LoRA}} (1 - \alpha)^{T</em>$$}}</p>
<p>解得：</p>
<p>$$T_{\text{GA}} = T_{\text{LoRA}} - \frac{\log(\mathcal{L}_0^{\text{GA}} / \mathcal{L}_0^{\text{LoRA}})}{\log(1 - \alpha)}$$</p>
<p><strong>注释</strong>：更好的初始化可以显著减少训练时间，这在大模型微调中非常有价值。</p>
<h3 id="10-lora">10. 与其他LoRA变体的对比<a class="toc-link" href="#10-lora" title="Permanent link">&para;</a></h3>
<p><strong>推导10.1：PiSSA的初始化策略</strong></p>
<p>PiSSA对预训练权重 $W_0$ 进行SVD：</p>
<p>$$W_0 = U \Sigma V^{\top}$$</p>
<p>然后设置：</p>
<p>$$A_0 = U_{[:, :r]} \Sigma_{[:r, :r]}^{1/2}, \quad B_0 = \Sigma_{[:r, :r]}^{1/2} V_{[:, :r]}^{\top}$$</p>
<p>使得 $A_0 B_0 \approx W_0$（秩-$r$ 近似）。</p>
<p>PiSSA的目标是初始化时就接近预训练权重，但这与梯度方向无关。</p>
<p><strong>注释</strong>：LoRA-GA关注梯度方向，PiSSA关注权重本身，两者的出发点不同。</p>
<p><strong>推导10.2：AdaLoRA的动态秩调整</strong></p>
<p>AdaLoRA引入重要性分数来动态调整每个参数的秩：</p>
<p>$$S_i = \frac{|\sigma_i|}{\sum_j |\sigma_j|}$$</p>
<p>其中 $\sigma_i$ 是某种奇异值的估计。</p>
<p>AdaLoRA会剪枝掉重要性低的维度，但这需要额外的计算开销。</p>
<p><strong>注释</strong>：LoRA-GA是一次性初始化，AdaLoRA需要持续监控和调整，计算成本更高。</p>
<p><strong>推导10.3：DoRA的方向与幅度分解</strong></p>
<p>DoRA将权重更新分解为方向和幅度：</p>
<p>$$W = |W| \cdot \frac{W}{|W|}$$</p>
<p>分别优化方向和幅度分量。</p>
<p>这与LoRA-GA的投影视角有本质不同，DoRA更关注权重的几何结构。</p>
<p><strong>注释</strong>：DoRA的优势在于更细粒度的控制，但理论基础不如LoRA-GA清晰。</p>
<h3 id="11_1">11. 计算复杂度分析<a class="toc-link" href="#11_1" title="Permanent link">&para;</a></h3>
<p><strong>推导11.1：SVD的计算复杂度</strong></p>
<p>对 $G_0 \in \mathbb{R}^{n \times m}$ 进行完整SVD的复杂度为：</p>
<p>$$\mathcal{O}(\min(nm^2, n^2 m))$$</p>
<p>但LoRA-GA只需要前 $2r$ 个奇异值和奇异向量，可以使用截断SVD，复杂度降为：</p>
<p>$$\mathcal{O}(nmr)$$</p>
<p><strong>注释</strong>：对于 $n=m=4096, r=8$，截断SVD需要约 $4096^2 \times 8 \approx 1.3 \times 10^8$ 次浮点运算，在现代GPU上可以在秒级完成。</p>
<p><strong>推导11.2：梯度计算的显存需求</strong></p>
<p>计算初始梯度 $G_0$ 需要一次完整的前向和反向传播，显存需求为：</p>
<p>$$\text{Memory} = nm \times \text{sizeof}(\text{float})$$</p>
<p>对于 FP32 和 $n=m=4096$：</p>
<p>$$\text{Memory} = 4096^2 \times 4 \text{ bytes} = 64 \text{ MB}$$</p>
<p>这对于现代GPU是完全可接受的。</p>
<p><strong>注释</strong>：论文提到可以串行计算每层的梯度，进一步降低峰值显存。</p>
<p><strong>推导11.3：LoRA-GA的一次性成本摊销</strong></p>
<p>设LoRA-GA的初始化需要额外时间 $T_{\text{init}}$，而每个epoch的训练时间为 $T_{\text{epoch}}$。</p>
<p>如果LoRA-GA能减少 $k$ 个epoch达到相同效果，则总时间节省为：</p>
<p>$$\Delta T = k \cdot T_{\text{epoch}} - T_{\text{init}}$$</p>
<p>只要 $k &gt; T_{\text{init}} / T_{\text{epoch}}$，LoRA-GA就是值得的。</p>
<p>实验中通常 $T_{\text{init}} \ll T_{\text{epoch}}$，因此即使只减少1个epoch，也是划算的。</p>
<p><strong>注释</strong>：初始化是一次性成本，可以通过减少训练步数来快速回本。</p>
<h3 id="12_1">12. 扩展到多层和多模块<a class="toc-link" href="#12_1" title="Permanent link">&para;</a></h3>
<p><strong>推导12.1：多层LoRA的独立初始化</strong></p>
<p>对于Transformer中的多个LoRA层，每层独立计算梯度并进行SVD初始化：</p>
<p>$$A_0^{(l)} = U^{(l)}<em :_=":]" _r:2r_="[r:2r,">{[:, :r]}, \quad B_0^{(l)} = V^{(l)}</em>$$</p>
<p>其中 $G_0^{(l)} = U^{(l)} \Sigma^{(l)} (V^{(l)})^{\top}$ 是第 $l$ 层的梯度SVD。</p>
<p><strong>注释</strong>：不同层的梯度分布可能差异很大，因此独立初始化是必要的。</p>
<p><strong>推导12.2：注意力层的特殊处理</strong></p>
<p>对于多头注意力，每个头可能需要不同的秩：</p>
<p>$$r_h \propto \sigma_1^{(h)} / \sum_h \sigma_1^{(h)}$$</p>
<p>这样可以根据每个头的重要性动态分配秩。</p>
<p><strong>注释</strong>：这是一个潜在的改进方向，但会增加实现复杂度。</p>
<h3 id="13_1">13. 理论局限性与未来方向<a class="toc-link" href="#13_1" title="Permanent link">&para;</a></h3>
<p><strong>推导13.1：非凸优化的挑战</strong></p>
<p>深度学习的损失函数通常是非凸的，梯度 $G_t$ 在训练过程中会不断变化。</p>
<p>LoRA-GA只对齐第一步，后续步骤的梯度方向可能偏离初始方向：</p>
<p>$$\langle G_t, G_0 \rangle / (|G_t| |G_0|) \to \text{decrease as } t \to \infty$$</p>
<p><strong>注释</strong>：这是LoRA-GA的一个局限，可能需要周期性地重新初始化或调整。</p>
<p><strong>推导13.2：自适应秩调整的可能性</strong></p>
<p>可以设计一个指标来衡量当前 $A_t, B_t$ 的对齐程度：</p>
<p>$$\text{Alignment}(t) = \frac{|A_t A_t^{\top} G_t + G_t B_t^{\top} B_t|_F}{|G_t|_F}$$</p>
<p>当这个比值下降时，可以考虑增加秩或重新初始化。</p>
<p><strong>注释</strong>：这需要在线监控梯度，可能带来额外开销。</p>
<p><strong>推导13.3：理论与实践的gap</strong></p>
<p>理论分析基于一阶近似和SGD优化器，但实践中使用Adam和复杂的学习率调度。</p>
<p>理论预测的最优秩可能与实践中的最优秩有偏差：</p>
<p>$$r_{\text{theory}} = \arg\min_r [\text{Approximation error} + \lambda \cdot r]$$</p>
<p>$$r_{\text{practice}} = \arg\min_r [\text{Validation loss}]$$</p>
<p>两者不一定相等，需要实验验证。</p>
<p><strong>注释</strong>：理论提供方向性指导，但超参数仍需根据具体任务调整。</p>
<h3 id="_8">总结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<p>本节详细推导了LoRA-GA的数学基础，包括：</p>
<ol>
<li><strong>全量微调与LoRA的差异</strong>：量化了秩约束带来的表达能力损失</li>
<li><strong>对齐理论</strong>：通过投影分析揭示了LoRA与全量微调的本质差距</li>
<li><strong>SVD初始化</strong>：证明了基于梯度SVD的初始化是最优的低秩逼近</li>
<li><strong>收敛性保证</strong>：分析了LoRA-GA如何加速收敛并减少优化误差</li>
<li><strong>实验验证</strong>：从理论角度解释了实验中观察到的性能提升</li>
</ol>
<p>LoRA-GA是一个理论与实践完美结合的范例，其核心思想简单而深刻：<strong>通过对齐梯度方向来对齐优化轨迹</strong>。这为LoRA的进一步改进提供了坚实的理论基础。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="闭门造车之多模态思路浅谈二自回归.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#282 “闭门造车”之多模态思路浅谈（二）：自回归</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="monarch矩阵计算高效的稀疏型矩阵分解.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#284 Monarch矩阵：计算高效的稀疏型矩阵分解</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#lora">对齐全量微调！这是我看过最精彩的LoRA改进（一）</a><ul>
<li><a href="#1">第1部分：核心理论、公理与历史基础</a><ul>
<li><a href="#11">1.1 理论起源与历史发展</a></li>
<li><a href="#12">1.2 设计哲学与核心思想</a></li>
<li><a href="#13">1.3 数学公理与基础假设</a></li>
<li><a href="#14">1.4 与其他方法的本质区别</a></li>
</ul>
</li>
<li><a href="#2">第2部分：严谨的核心数学推导</a><ul>
<li><a href="#21-lora">2.1 LoRA的数学框架</a></li>
<li><a href="#23-lora-ga">2.3 LoRA-GA的核心思想：对齐全量微调</a></li>
<li><a href="#24">2.4 优化问题的求解过程</a></li>
<li><a href="#25-lora-ga">2.5 LoRA-GA算法总结</a></li>
</ul>
</li>
<li><a href="#3">第3部分：数学直觉、多角度解释与类比</a><ul>
<li><a href="#31-lora-ga">3.1 生活化类比：为什么LoRA-GA有效？</a></li>
<li><a href="#32">3.2 几何意义：投影与子空间</a></li>
<li><a href="#33">3.3 多角度理解</a></li>
</ul>
</li>
<li><a href="#4">第4部分：方法论变体、批判性比较与优化</a><ul>
<li><a href="#41-lora">4.1 LoRA变体对比表</a></li>
<li><a href="#42-lora-ga">4.2 LoRA-GA的批判性分析</a></li>
<li><a href="#43">4.3 实验结果的深入分析</a></li>
</ul>
</li>
<li><a href="#5">第5部分：学习路线图与未来展望</a><ul>
<li><a href="#51">5.1 学习路线图</a></li>
<li><a href="#52">5.2 研究空白与未来方向</a></li>
<li><a href="#53">5.3 实践建议</a></li>
</ul>
</li>
<li><a href="#_5">实验效果分析</a></li>
<li><a href="#_6">文章小结</a></li>
<li><a href="#_7">公式推导与注释</a><ul>
<li><a href="#1_1">1. 全量微调的数学定义与基础理论</a></li>
<li><a href="#2-lora">2. LoRA的数学框架与约束</a></li>
<li><a href="#3-lora">3. LoRA与全量微调的对齐理论</a></li>
<li><a href="#4_1">4. 梯度空间的投影分析</a></li>
<li><a href="#5_1">5. 秩不足的补偿机制</a></li>
<li><a href="#6">6. 对齐损失的数学推导</a></li>
<li><a href="#7">7. 理论保证与收敛性分析</a></li>
<li><a href="#8-adam">8. Adam优化器的扩展分析</a></li>
<li><a href="#9">9. 实验结果的理论解释</a></li>
<li><a href="#10-lora">10. 与其他LoRA变体的对比</a></li>
<li><a href="#11_1">11. 计算复杂度分析</a></li>
<li><a href="#12_1">12. 扩展到多层和多模块</a></li>
<li><a href="#13_1">13. 理论局限性与未来方向</a></li>
<li><a href="#_8">总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>