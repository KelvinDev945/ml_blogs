<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CoSENT（三）：作为交互式相似度的损失函数 | ML & Math Blog Posts</title>
    <meta name="description" content="CoSENT（三）：作为交互式相似度的损失函数&para;
原文链接: https://spaces.ac.cn/archives/9341
发布日期: 2022-11-09



### 核心发现

**问题**：CoSENT能否作为交互式相似度模型的损失函数？

**背景**：
- CoSENT最初设计用于特征式句向量学习（优化余弦相似度）
- 但本质上它是一个**排序损失**，与具体相似度度...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=语义">语义</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #207 CoSENT（三）：作为交互式相似度的损失函数
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#207</span>
                CoSENT（三）：作为交互式相似度的损失函数
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2022-11-09</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=语义" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语义</span>
                </a>
                
                <a href="../index.html?tags=语义相似度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 语义相似度</span>
                </a>
                
                <a href="../index.html?tags=对比学习" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 对比学习</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=attention" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> attention</span>
                </a>
                
                <a href="../index.html?tags=交互式匹配" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 交互式匹配</span>
                </a>
                
                <a href="../index.html?tags=排序损失" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 排序损失</span>
                </a>
                
                <a href="../index.html?tags=交叉熵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 交叉熵</span>
                </a>
                
                <a href="../index.html?tags=Spearman" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> Spearman</span>
                </a>
                
                <a href="../index.html?tags=Cross-Encoder" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> Cross-Encoder</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="cosent">CoSENT（三）：作为交互式相似度的损失函数<a class="toc-link" href="#cosent" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9341">https://spaces.ac.cn/archives/9341</a></p>
<p><strong>发布日期</strong>: 2022-11-09</p>
<hr />
<div class="theorem-box">

### 核心发现

**问题**：CoSENT能否作为交互式相似度模型的损失函数？

**背景**：
- CoSENT最初设计用于特征式句向量学习（优化余弦相似度）
- 但本质上它是一个**排序损失**，与具体相似度度量无关

**本文贡献**：
- ✅ 将CoSENT泛化为通用排序损失：$f(\cdot, \cdot)$可以是任意相似度函数
- ✅ 实验对比：CoSENT vs 交叉熵（CE）在交互式模型上的表现
- ✅ 结论：效果基本持平，但在困难任务上CoSENT略优

**关键优势**：
- 适用于连续打分数据（如STS-B的1-5分），无需离散化
- 与评测指标Spearman系数更一致（都只依赖排序）

</div>

<hr />
<p>在<a href="/archives/8847">《CoSENT（一）：比Sentence-BERT更有效的句向量方案》</a>中，笔者提出了名为"CoSENT"的有监督句向量方案，由于它是直接训练cos相似度的，跟评测目标更相关，因此通常能有着比Sentence-BERT更好的效果以及更快的收敛速度。在<a href="/archives/8860">《CoSENT（二）：特征式匹配与交互式匹配有多大差距？》</a>中我们还比较过它跟交互式相似度模型的差异，显示它在某些任务上的效果还能直逼交互式相似度模型。</p>
<p>然而，当时笔者是一心想找一个更接近评测目标的Sentence-BERT替代品，所以结果都是面向有监督句向量的，即特征式相似度模型。最近笔者突然反应过来，CoSENT其实也能作为交互式相似度模型的损失函数。那么它跟标准选择交叉熵相比孰优孰劣呢？本文来补充这部分实验。</p>
<h2 id="_1">一、基础回顾<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>CoSENT提出之初，是作为一个有监督句向量的损失函数：
\begin{equation}\log \left(1 + \sum\limits_{\text{sim}(i,j) \gt \text{sim}(k,l)} e^{\lambda(\cos(u_k, u_l) - \cos(u_i, u_j))}\right)\tag{1}\end{equation}
其中$i,j,k,l$是四个训练样本（比如四个句子），$u_i, u_j, u_k, u_l$是它们想要学习的句向量（比如它们经过BERT后的[CLS]向量），$\cos(\cdot,\cdot)$代表两个向量的余弦相似度，$\text{sim}(\cdot,\cdot)$则代表它们的相似度标签。所以这个损失函数的定义也很清晰，就是如果你认为$(i,j)$的相似度应该大于$\text{sim}(k,l)$的相似度，那么就往$\log$里边加入一项$e^{\lambda(\cos(u_k, u_l) - \cos(u_i, u_j))}$。</p>
<div class="derivation-box">

### 泛化到任意相似度函数

**原始CoSENT**（特征式）：
$$
f_{\text{特征}}(i,j) = \cos(u_i, u_j) = \frac{u_i^\top u_j}{\|u_i\| \|u_j\|}
\tag{2}
$$

**泛化CoSENT**（通用）：
$$
\mathcal{L} = \log \left(1 + \sum\limits_{\text{sim}(i,j) > \text{sim}(k,l)} e^{\lambda(f(k,l) - f(i,j))}\right)
\tag{3}
$$

**交互式CoSENT**：
$$
f_{\text{交互}}(i,j) = \text{BERT}([s_i; [SEP]; s_j])_{[CLS]}^\top w
\tag{4}
$$

其中$w$是可学习的分类向量。

**关键洞察**：$f(\cdot,\cdot)$可以是：
- 余弦相似度（特征式）
- BERT分类器输出（交互式）
- 双线性函数 $u_i^\top W u_j$
- 任意神经网络

</div>

<p>从这个形式就可以看出，当时CoSENT就是为了有监督训练余弦相似度的特征式模型的，包括"CoSENT"这个名字也是这样来的（Cosine Sentence）。然而，抛开余弦相似度这一层面不谈，CoSENT本质上是一个只依赖于标签相对顺序的损失函数，它跟余弦相似度没有必然联系，我们可以将它一般化为
\begin{equation}\log \left(1 + \sum\limits_{\text{sim}(i,j) \gt \text{sim}(k,l)} e^{\lambda(f(k,l) - f(i,j))}\right)\tag{5}\end{equation}
其中$f(\cdot,\cdot)$是任意标量输出函数（一般不需要加激活函数），代表要学习的相似度模型，包括将两个输入拼接成一个文本输入到BERT中的"交互式相似度"模型！</p>
<h2 id="_2">二、实验比较<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>训练交互式相似度的常规方式是最后构建一个两节点的输出，然后加上softmax，用交叉熵（下表简称CE）作为损失函数，这也等价于在前面的$f(\cdot,\cdot)$上加sigmoid激活，然后用单节点的二分类交叉熵。不过这种做法也就适合二分类形式的标签，如果连续型的打分（比如STS-B是1～5分），就不大适合了，此时通常要转化为回归问题。但CoSENT没有这个限制，因为它只需要标签的序信息，这个特点跟常用的评测指标spearman系数是一致的。</p>
<div class="note-box">

### 两种损失函数的对比

**交叉熵（CE）**：
$$
\mathcal{L}_{CE} = -\sum_{i} [y_i \log p_i + (1-y_i) \log(1-p_i)]
$$

- ✅ 标准方法，稳定可靠
- ❌ 只适用于二分类或离散标签
- ❌ 连续打分需要转换为回归问题

**CoSENT**：
$$
\mathcal{L}_{CoSENT} = \log \left(1 + \sum\limits_{\text{sim}(i,j) > \text{sim}(k,l)} e^{\lambda(f(k,l) - f(i,j))}\right)
$$

- ✅ 直接支持连续标签（1-5分）
- ✅ 与评测指标Spearman一致（都基于排序）
- ✅ 对标签噪声更鲁棒
- ⚠️ 计算复杂度$O(N^2)$（可通过采样优化）

</div>

<p>两者的对比实验，参考代码如下：</p>
<blockquote>
<p><a href="https://github.com/bojone/CoSENT/blob/main/accuracy/interact_cosent.py"><strong>https://github.com/bojone/CoSENT/blob/main/accuracy/interact_cosent.py</strong></a></p>
</blockquote>
<p>实验结果为<br />
$$\begin{array}{c}
\text{评测指标为spearman系数} \\
{\begin{array}{c|ccccc}
\hline
&amp; \text{ATEC} &amp; \text{BQ} &amp; \text{LCQMC} &amp; \text{PAWSX} &amp; \text{avg}\\
\hline
\text{BERT + CE} &amp; 48.01 &amp; 71.96 &amp; 78.53 &amp; 68.59 &amp; 66.77 \\
\text{BERT + CoSENT} &amp; 48.09 &amp; 72.25 &amp; 78.70 &amp; 69.34 &amp; 67.10 \\
\hline
\text{RoBERTa + CE} &amp; 49.70 &amp; 73.20 &amp; 79.13 &amp; 70.52 &amp; 68.14 \\
\text{RoBERTa + CoSENT} &amp; 49.82 &amp; 73.09 &amp; 78.78 &amp; 70.54 &amp; 68.06 \\
\hline
\end{array}} \\
\\
\text{评测指标为accuracy} \\
{\begin{array}{c|ccccc}
\hline
&amp; \text{ATEC} &amp; \text{BQ} &amp; \text{LCQMC} &amp; \text{PAWSX} &amp; \text{avg}\\
\hline
\text{BERT + CE} &amp; 85.38 &amp; 83.57 &amp; 88.10 &amp; 81.45 &amp; 84.63 \\
\text{BERT + CoSENT} &amp; 85.55 &amp; 83.73 &amp; 87.92 &amp; 81.85 &amp; 84.76 \\
\hline
\text{RoBERTa + CE} &amp; 85.97 &amp; 84.67 &amp; 88.14 &amp; 82.85 &amp; 85.41 \\
\text{RoBERTa + CoSENT} &amp; 86.06 &amp; 84.23 &amp; 88.14 &amp; 83.03 &amp; 85.37 \\
\hline
\end{array}}
\end{array}$$</p>
<p>可以看到，没有惊喜，CE和CoSENT的效果基本一致。非要挖掘一些细致区别的话，可以看到在BERT中，CoSENT的效果相对好些，在RoBERTa中基本没区别了，以及在PAWSX这个任务上，CoSENT的提升相对明显些，其他任务基本持平。</p>
<div class="intuition-box">

### 🔍 实验结果分析

**观察1：整体差异不大**
- Spearman指标：平均差异 < 0.5%
- Accuracy指标：平均差异 < 0.2%
- 结论：两种方法在交互式模型上表现相当

**观察2：PAWSX任务上的差异**

| 模型 | CE (Spearman) | CoSENT (Spearman) | 提升 |
|------|---------------|-------------------|------|
| BERT | 68.59 | 69.34 | +0.75 ✨ |
| RoBERTa | 70.52 | 70.54 | +0.02 |

**为什么PAWSX上提升更明显？**
- PAWSX包含大量对抗样本（字面相似但语义不同）
- CoSENT的排序损失更关注**相对区分**而非绝对值
- 对困难样本的区分能力更强

**观察3：模型能力的影响**

BERT vs RoBERTa：
- BERT + CoSENT：相对提升更明显
- RoBERTa + CoSENT：提升不明显
- 推测：弱模型更受益于排序损失的引导

</div>

<p>如此，可以"弱弱地"下一个结论：</p>
<blockquote>
<p>当模型较弱（BERT弱于RoBERTa）或者任务较难（PAWSX相对来说比其他三个任务都难）时，CoSENT<strong> <em>或许</em></strong> 能取得比CE更好的效果。</p>
</blockquote>
<p>注意，是"或许"，笔者也不能保证。实事求是地说，我也不认为两者构成什么显著差异。不过可以猜测，因为两种损失函数的形式有明显的差异，所以哪怕最终指标上差不多，模型内部应该也有一定差异，这时候或许可以考虑模型融合？</p>
<div class="note-box">

### 💡 实践建议

**何时使用CoSENT（交互式）？**
1. ✅ 数据包含连续标签（STS-B的1-5分）
2. ✅ 评测指标是Spearman系数
3. ✅ 困难任务（对抗样本多）
4. ✅ 想尝试模型融合（与CE模型ensemble）

**何时使用CE（交叉熵）？**
1. ✅ 标准二分类任务
2. ✅ 对训练效率要求高（CE更快）
3. ✅ 需要输出校准的概率值
4. ✅ 模型已经很强（如RoBERTa-large）

**混合策略**：
- 预训练阶段：使用CE快速收敛
- 精调阶段：切换到CoSENT优化排序
- 模型融合：CE模型 + CoSENT模型 → 更鲁棒

</div>

<h2 id="_3">文章小结<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>本文主要思考和实验了CoSENT在交互式相似度模型中的可行性，最终结论是“可行但效果没什么提升”。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9341">https://spaces.ac.cn/archives/9341</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Nov. 09, 2022). 《CoSENT（三）：作为交互式相似度的损失函数 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9341">https://spaces.ac.cn/archives/9341</a></p>
<p>@online{kexuefm-9341,<br />
title={CoSENT（三）：作为交互式相似度的损失函数},<br />
author={苏剑林},<br />
year={2022},<br />
month={Nov},<br />
url={\url{https://spaces.ac.cn/archives/9341}},<br />
} </p>
<hr />
<h2 id="_4">公式推导与注释<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<h3 id="1-cosent">1. CoSENT损失函数的完整数学定义<a class="toc-link" href="#1-cosent" title="Permanent link">&para;</a></h3>
<p>CoSENT (Cosine Sentence) 损失函数的核心思想是直接优化余弦相似度的排序关系。给定样本对集合，我们希望相似样本对的余弦相似度大于不相似样本对的余弦相似度。</p>
<p><strong>基础定义</strong>：设有$N$个样本${x_1, x_2, \ldots, x_N}$，每个样本$x_i$经过编码器得到表示向量$u_i \in \mathbb{R}^d$，样本对$(i,j)$的相似度标签为$\text{sim}(i,j) \in \mathbb{R}$。</p>
<p>\begin{equation}
\mathcal{L}<em _92_="\" _k_l_="(k,l)" _substack_i_j_="\substack{(i,j)," _text_sim="\text{sim">{\text{CoSENT}} = \log \left(1 + \sum</em>
\end{equation}}(i,j) &gt; \text{sim}(k,l)}} e^{\lambda(\cos(u_k, u_l) - \cos(u_i, u_j))}\right) \tag{1</p>
<p>其中：
- $\cos(u_i, u_j) = \frac{\langle u_i, u_j \rangle}{|u_i|_2 |u_j|_2}$ 是余弦相似度
- $\lambda &gt; 0$ 是温度参数（scale factor）
- 求和遍历所有满足 $\text{sim}(i,j) &gt; \text{sim}(k,l)$ 的样本对</p>
<p><strong>数学直觉</strong>：
- 当$\cos(u_k, u_l) &gt; \cos(u_i, u_j)$时（预测错误），指数项$e^{\lambda(\cos(u_k, u_l) - \cos(u_i, u_j))} &gt; 1$，损失增大
- 当$\cos(u_k, u_l) &lt; \cos(u_i, u_j)$时（预测正确），指数项$e^{\lambda(\cos(u_k, u_l) - \cos(u_i, u_j))} &lt; 1$，损失接近0
- $\log(1 + \cdot)$ 提供平滑的惩罚</p>
<h3 id="2-circle-loss">2. 与Circle Loss的深层联系<a class="toc-link" href="#2-circle-loss" title="Permanent link">&para;</a></h3>
<p><strong>Circle Loss回顾</strong>：Circle Loss是一个统一的对比学习框架，其形式为：</p>
<p>\begin{equation}
\mathcal{L}<em _Omega__text_neg="\Omega_{\text{neg" _in="\in" j="j">{\text{Circle}} = \log\left[1 + \sum</em>
\end{equation}}}} e^{\gamma(s_j - \Delta_n)} \cdot \sum_{i \in \Omega_{\text{pos}}} e^{-\gamma(s_i - \Delta_p)}\right] \tag{2</p>
<p>其中$s_i, s_j$是相似度得分，$\Delta_p, \Delta_n$是margin参数，$\gamma$是scale参数。</p>
<p><strong>CoSENT与Circle Loss的关系</strong>：</p>
<p>将Circle Loss改写为pairwise形式：</p>
<p>\begin{equation}
\mathcal{L}<em _Omega__text_pos="\Omega_{\text{pos" _in="\in" _substack_i="\substack{i">{\text{Circle}} = \log\left[1 + \sum</em>
\end{equation}}} \ j \in \Omega_{\text{neg}}}} e^{\gamma(s_j - s_i + \Delta_p - \Delta_n)}\right] \tag{3</p>
<p>当设置$\Delta_p = \Delta_n = 0$时，Circle Loss退化为：</p>
<p>\begin{equation}
\mathcal{L}<em _Omega__text_pos="\Omega_{\text{pos" _in="\in" _substack_i="\substack{i">{\text{Circle}}^* = \log\left[1 + \sum</em>
\end{equation}}} \ j \in \Omega_{\text{neg}}}} e^{\gamma(s_j - s_i)}\right] \tag{4</p>
<p>这正是CoSENT的形式（其中$s_i = \cos(u_i, u_j)$）。</p>
<p><strong>关键差异</strong>：
1. <strong>相似度度量</strong>：Circle Loss可用任意相似度函数，CoSENT特化为余弦相似度
2. <strong>Margin</strong>：Circle Loss显式使用margin，CoSENT通过温度参数$\lambda$隐式控制
3. <strong>应用场景</strong>：Circle Loss偏向于分类，CoSENT专注于句子表示学习</p>
<h3 id="3-infonce">3. 与InfoNCE的对比分析<a class="toc-link" href="#3-infonce" title="Permanent link">&para;</a></h3>
<p><strong>InfoNCE损失</strong>：在对比学习中广泛使用，定义为：</p>
<p>\begin{equation}
\mathcal{L}<em j="1">{\text{InfoNCE}} = -\log \frac{e^{\text{sim}(u_i, u_i^+)/\tau}}{\sum</em>
\end{equation}}^{K} e^{\text{sim}(u_i, u_j)/\tau}} \tag{5</p>
<p>其中$u_i^+$是正样本，${u_j}_{j=1}^K$包含1个正样本和$K-1$个负样本。</p>
<p><strong>推导InfoNCE与CoSENT的关系</strong>：</p>
<p>对于batch中的$N$个样本，InfoNCE可以写成：</p>
<p>\begin{equation}
\mathcal{L}<em i="1">{\text{InfoNCE}} = -\sum</em>
\end{equation}}^{N} \log \frac{e^{s_{i,i^+}/\tau}}{e^{s_{i,i^+}/\tau} + \sum_{j \neq i^+} e^{s_{i,j}/\tau}} \tag{6</p>
<p>变形得：</p>
<p>\begin{equation}
\mathcal{L}<em i="1">{\text{InfoNCE}} = \sum</em>
\end{equation}}^{N} \log\left(1 + \sum_{j \neq i^+} e^{(s_{i,j} - s_{i,i^+})/\tau}\right) \tag{7</p>
<p><strong>对比总结</strong>：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>InfoNCE</th>
<th>CoSENT</th>
</tr>
</thead>
<tbody>
<tr>
<td>形式</td>
<td>Softmax交叉熵</td>
<td>排序损失</td>
</tr>
<tr>
<td>负样本</td>
<td>需要大量负样本</td>
<td>利用batch内配对</td>
</tr>
<tr>
<td>标签类型</td>
<td>二元（正/负）</td>
<td>连续相似度</td>
</tr>
<tr>
<td>优化目标</td>
<td>最大化互信息</td>
<td>最小化排序误差</td>
</tr>
<tr>
<td>计算复杂度</td>
<td>$O(NK)$</td>
<td>$O(N^2)$</td>
</tr>
</tbody>
</table>
<h3 id="4">4. 对比学习的理论基础<a class="toc-link" href="#4" title="Permanent link">&para;</a></h3>
<p><strong>互信息最大化</strong>：对比学习的核心是最大化表示$u$与输入$x$的互信息$I(u; x)$。</p>
<p>根据互信息的定义：</p>
<p>\begin{equation}
I(u; x) = H(u) - H(u|x) = H(x) - H(x|u) \tag{8}
\end{equation}</p>
<p>其中$H(\cdot)$表示熵。在对比学习中，我们希望：
- <strong>最大化</strong> $I(u; x)$：表示保留输入信息
- <strong>最小化</strong> $I(u; y)$ where $y$是增强视图：表示对增强的不变性</p>
<p><strong>InfoNCE作为互信息下界</strong>：</p>
<p>\begin{equation}
I(u; x) \geq \log K - \mathcal{L}_{\text{InfoNCE}} \tag{9}
\end{equation}</p>
<p>这个下界随着负样本数$K$的增加而变紧。</p>
<p><strong>CoSENT的隐式互信息优化</strong>：</p>
<p>CoSENT通过排序损失间接优化互信息。考虑两个样本对$(u_i, u_j)$和$(u_k, u_l)$，如果$\text{sim}(i,j) &gt; \text{sim}(k,l)$，那么我们希望：</p>
<p>\begin{equation}
\cos(u_i, u_j) &gt; \cos(u_k, u_l) \Rightarrow I(u_i; u_j) &gt; I(u_k; u_l) \tag{10}
\end{equation}</p>
<p>虽然CoSENT不直接优化互信息，但通过保持相似度的序关系，间接实现了互信息的排序。</p>
<p><strong>度量学习视角</strong>：</p>
<p>从度量学习角度，CoSENT学习一个嵌入空间$\mathcal{U}$，使得：</p>
<p>\begin{equation}
d(u_i, u_j) &lt; d(u_k, u_l) \quad \text{if} \quad \text{sim}(i,j) &gt; \text{sim}(k,l) \tag{11}
\end{equation}</p>
<p>其中$d(u, v) = 1 - \cos(u, v)$是余弦距离。</p>
<h3 id="5">5. 梯度计算与优化性质<a class="toc-link" href="#5" title="Permanent link">&para;</a></h3>
<p><strong>梯度推导</strong>：</p>
<p>对于单个样本对$(i,j)$，设$s_{ij} = \cos(u_i, u_j)$，损失函数关于$s_{ij}$的梯度为：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial s_{ij}} = -\lambda \sum_{k,l: \text{sim}(i,j) &gt; \text{sim}(k,l)} \frac{e^{\lambda(s_{kl} - s_{ij})}}{1 + \sum e^{\lambda(s_{kl} - s_{ij})}} \tag{12}
\end{equation}</p>
<p>可以改写为：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial s_{ij}} = -\lambda \sum_{k,l: \text{sim}(i,j) &gt; \text{sim}(k,l)} p_{ij,kl} \tag{13}
\end{equation}</p>
<p>其中：</p>
<p>\begin{equation}
p_{ij,kl} = \frac{e^{\lambda(s_{kl} - s_{ij})}}{\sum_{\text{all pairs}} e^{\lambda(s_{kl} - s_{ij})}} \tag{14}
\end{equation}</p>
<p>是一个概率分布，表示样本对$(k,l)$相对于$(i,j)$的"错误程度"。</p>
<p><strong>余弦相似度的梯度</strong>：</p>
<p>\begin{equation}
\frac{\partial \cos(u_i, u_j)}{\partial u_i} = \frac{u_j}{|u_i| |u_j|} - \frac{\langle u_i, u_j \rangle}{|u_i|^3 |u_j|} u_i \tag{15}
\end{equation}</p>
<p>简化为（当向量归一化时$|u_i| = 1$）：</p>
<p>\begin{equation}
\frac{\partial \cos(u_i, u_j)}{\partial u_i} = u_j - \cos(u_i, u_j) \cdot u_i \tag{16}
\end{equation}</p>
<p><strong>链式法则应用</strong>：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial u_i} = \sum_{j} \frac{\partial \mathcal{L}}{\partial s_{ij}} \frac{\partial s_{ij}}{\partial u_i} = -\lambda \sum_{j} p_{ij} (u_j - s_{ij} u_i) \tag{17}
\end{equation}</p>
<p><strong>优化性质分析</strong>：</p>
<ol>
<li><strong>凸性</strong>：CoSENT损失关于${s_{ij}}$是凸函数（log-sum-exp形式）</li>
</ol>
<p>证明：设$f(s) = \log(1 + \sum_k e^{\lambda(a_k - s)})$，其Hessian矩阵为：</p>
<p>\begin{equation}
   H = \frac{\partial^2 f}{\partial s^2} = \lambda^2 \frac{\sum_k e^{\lambda(a_k - s)} \cdot \sum_k e^{\lambda(a_k - s)} - (\sum_k e^{\lambda(a_k - s)})^2}{(1 + \sum_k e^{\lambda(a_k - s)})^2} \geq 0 \tag{18}
   \end{equation}</p>
<ol start="2">
<li><strong>Lipschitz连续性</strong>：梯度的Lipschitz常数为$L = O(\lambda^2 N^2)$</li>
</ol>
<p>\begin{equation}
   |\nabla \mathcal{L}(s) - \nabla \mathcal{L}(s')| \leq L |s - s'| \tag{19}
   \end{equation}</p>
<ol start="3">
<li><strong>收敛速率</strong>：使用SGD时，收敛速率为$O(1/\sqrt{T})$；使用Adam时，实际收敛更快</li>
</ol>
<h3 id="6">6. 信息论解释<a class="toc-link" href="#6" title="Permanent link">&para;</a></h3>
<p><strong>熵的视角</strong>：</p>
<p>定义"预测分布"：</p>
<p>\begin{equation}
q_{ij,kl} = \frac{e^{\lambda s_{kl}}}{\sum_{m,n} e^{\lambda s_{mn}}} \tag{20}
\end{equation}</p>
<p>"目标分布"（基于标签）：</p>
<p>\begin{equation}
p_{ij,kl} = \begin{cases}
\frac{1}{|\mathcal{P}_{ij}|} &amp; \text{if } \text{sim}(k,l) &lt; \text{sim}(i,j) \
0 &amp; \text{otherwise}
\end{cases} \tag{21}
\end{equation}</p>
<p>其中$\mathcal{P}_{ij} = {(k,l): \text{sim}(k,l) &lt; \text{sim}(i,j)}$。</p>
<p><strong>KL散度分解</strong>：</p>
<p>CoSENT可以理解为最小化KL散度的上界：</p>
<p>\begin{equation}
\text{KL}(p | q) = \sum_{ij,kl} p_{ij,kl} \log \frac{p_{ij,kl}}{q_{ij,kl}} \tag{22}
\end{equation}</p>
<p><strong>互信息界</strong>：</p>
<p>表示$u$与标签$\text{sim}$的互信息满足：</p>
<p>\begin{equation}
I(u; \text{sim}) \geq H(\text{sim}) - \mathcal{L}_{\text{CoSENT}}/\lambda \tag{23}
\end{equation}</p>
<p><strong>条件熵</strong>：</p>
<p>给定表示$u$，标签的条件熵为：</p>
<p>\begin{equation}
H(\text{sim}|u) \approx -\sum_{ij} p(\text{sim}<em ij="ij">{ij}) \log \frac{e^{\lambda s</em>
\end{equation}}}}{\sum_{kl} e^{\lambda s_{kl}}} \tag{24</p>
<h3 id="7">7. 概率论视角<a class="toc-link" href="#7" title="Permanent link">&para;</a></h3>
<p><strong>似然函数构造</strong>：</p>
<p>将CoSENT解释为排序概率模型。给定相似度标签，样本对$(i,j)$排序高于$(k,l)$的概率为：</p>
<p>\begin{equation}
P(\text{rank}<em kl="kl">{ij} &gt; \text{rank}</em>
\end{equation}}) = \sigma(\lambda(s_{ij} - s_{kl})) = \frac{1}{1 + e^{-\lambda(s_{ij} - s_{kl})}} \tag{25</p>
<p>其中$\sigma(\cdot)$是sigmoid函数。</p>
<p><strong>负对数似然</strong>：</p>
<p>\begin{equation}
-\log P(\text{all correct rankings}) = -\sum_{\text{sim}(i,j) &gt; \text{sim}(k,l)} \log \sigma(\lambda(s_{ij} - s_{kl})) \tag{26}
\end{equation}</p>
<p>利用恒等式$-\log \sigma(x) = \log(1 + e^{-x})$：</p>
<p>\begin{equation}
= \sum_{\text{sim}(i,j) &gt; \text{sim}(k,l)} \log(1 + e^{-\lambda(s_{ij} - s_{kl})}) \tag{27}
\end{equation}</p>
<p>当所有项求和在$\log$内时（Jensen不等式的应用）：</p>
<p>\begin{equation}
\sum_{ij,kl} \log(1 + e^{-\lambda(s_{ij} - s_{kl})}) \geq \log\left(1 + \sum_{ij,kl} e^{-\lambda(s_{ij} - s_{kl})}\right) \tag{28}
\end{equation}</p>
<p>等号成立当且仅当所有项相等。实际中，右侧的紧upper bound更容易优化，这就是CoSENT的形式。</p>
<p><strong>贝叶斯解释</strong>：</p>
<p>假设先验$p(\text{sim}) \sim \mathcal{N}(0, \sigma^2)$，后验为：</p>
<p>\begin{equation}
p(\text{sim}<em ij="ij">{ij} | u_i, u_j) \propto \exp\left(-\frac{(\text{sim}</em>
\end{equation}} - s_{ij})^2}{2\sigma^2}\right) \tag{29</p>
<p>CoSENT相当于最大后验估计（MAP）的排序版本。</p>
<h3 id="8">8. 几何理解<a class="toc-link" href="#8" title="Permanent link">&para;</a></h3>
<p><strong>超球面上的优化</strong>：</p>
<p>当向量归一化时，$u_i \in \mathbb{S}^{d-1}$（单位超球面）。余弦相似度$\cos(u_i, u_j)$等于球面上的测地距离的余弦：</p>
<p>\begin{equation}
\text{geodesic_dist}(u_i, u_j) = \arccos(\cos(u_i, u_j)) \tag{30}
\end{equation}</p>
<p><strong>流形结构</strong>：</p>
<p>CoSENT在超球面流形$\mathbb{S}^{d-1}$上定义优化问题。流形上的梯度为：</p>
<p>\begin{equation}
\text{grad}_{\mathbb{S}^{d-1}} \mathcal{L} = \nabla \mathcal{L} - \langle \nabla \mathcal{L}, u \rangle u \tag{31}
\end{equation}</p>
<p>这正是式(16)的形式！</p>
<p><strong>黎曼度量</strong>：</p>
<p>超球面的黎曼度量为：</p>
<p>\begin{equation}
g_{ij} = \delta_{ij} - u_i u_j \tag{32}
\end{equation}</p>
<p>在此度量下，CoSENT优化的是测地距离的保序性。</p>
<p><strong>角度分布</strong>：</p>
<p>优化后，相似样本对的夹角$\theta_{ij} = \arccos(s_{ij})$服从：</p>
<p>\begin{equation}
p(\theta_{ij}) \propto e^{-\lambda \text{rank}(\text{sim}_{ij})} \tag{33}
\end{equation}</p>
<p>即高相似度样本聚集在小角度区域。</p>
<p><strong>von Mises-Fisher分布</strong>：</p>
<p>在超球面上，CoSENT隐式地学习了von Mises-Fisher (vMF)分布族：</p>
<p>\begin{equation}
p(u | \mu, \kappa) = C_d(\kappa) e^{\kappa \mu^T u} \tag{34}
\end{equation}</p>
<p>其中$\mu$是均值方向，$\kappa$是集中度参数（类似于$\lambda$）。</p>
<h3 id="9">9. 与其他损失函数的对比<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<h4 id="91-triplet-loss">9.1 Triplet Loss<a class="toc-link" href="#91-triplet-loss" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：</p>
<p>\begin{equation}
\mathcal{L}_{\text{Triplet}} = \max(0, d(u_a, u_p) - d(u_a, u_n) + m) \tag{35}
\end{equation}</p>
<p><strong>对比</strong>：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>Triplet Loss</th>
<th>CoSENT</th>
</tr>
</thead>
<tbody>
<tr>
<td>样本选择</td>
<td>需要精心挖掘triplet</td>
<td>利用batch内所有配对</td>
</tr>
<tr>
<td>Margin</td>
<td>硬margin $m$</td>
<td>软margin（隐式）</td>
</tr>
<tr>
<td>梯度</td>
<td>ReLU截断</td>
<td>平滑梯度</td>
</tr>
<tr>
<td>收敛性</td>
<td>容易陷入局部最优</td>
<td>更稳定</td>
</tr>
</tbody>
</table>
<h4 id="92-contrastive-loss">9.2 Contrastive Loss<a class="toc-link" href="#92-contrastive-loss" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：</p>
<p>\begin{equation}
\mathcal{L}_{\text{Contrastive}} = y \cdot d^2 + (1-y) \cdot \max(0, m - d)^2 \tag{36}
\end{equation}</p>
<p><strong>对比</strong>：CoSENT不需要设置margin，自动适应数据分布。</p>
<h4 id="93-arcface-loss">9.3 ArcFace Loss<a class="toc-link" href="#93-arcface-loss" title="Permanent link">&para;</a></h4>
<p><strong>定义</strong>：</p>
<p>\begin{equation}
\mathcal{L}<em y_i="y_i">{\text{ArcFace}} = -\log \frac{e^{s(\cos(\theta</em>
\end{equation}} + m))}}{e^{s(\cos(\theta_{y_i} + m))} + \sum_{j \neq y_i} e^{s\cos\theta_j}} \tag{37</p>
<p><strong>对比</strong>：ArcFace用于分类（固定类别中心），CoSENT用于度量学习（样本间关系）。</p>
<h3 id="10">10. 理论性质深入分析<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<p><strong>性质1：排序一致性</strong></p>
<p>\begin{equation}
\text{If } \mathcal{L}<em ij="ij">{\text{CoSENT}} = 0, \text{ then } \forall (i,j), (k,l): \text{sim}(i,j) &gt; \text{sim}(k,l) \Rightarrow s</em>
\end{equation}} &gt; s_{kl} \tag{38</p>
<p>证明：当损失为0时，$\sum e^{\lambda(s_{kl} - s_{ij})} = 0$，因此所有$s_{kl} - s_{ij} \to -\infty$，即$s_{ij} &gt; s_{kl}$。</p>
<p><strong>性质2：温度参数的作用</strong></p>
<p>温度$\lambda$控制损失的"尖锐程度"：</p>
<p>\begin{equation}
\lim_{\lambda \to \infty} \frac{1}{\lambda}\mathcal{L}<em _text_sim="\text{sim">{\text{CoSENT}} = \max</em>
\end{equation}}(i,j) &gt; \text{sim}(k,l)}(s_{kl} - s_{ij})^+ \tag{39</p>
<p>其中$(x)^+ = \max(0, x)$。即$\lambda \to \infty$时，CoSENT退化为max-margin损失。</p>
<p><strong>性质3：样本权重</strong></p>
<p>每个违反排序的样本对$(k,l)$的权重为：</p>
<p>\begin{equation}
w_{kl} = \frac{e^{\lambda s_{kl}}}{\sum_{m,n} e^{\lambda s_{mn}}} \propto e^{\lambda s_{kl}} \tag{40}
\end{equation}</p>
<p>即相似度越高的负样本对权重越大，符合hard negative mining的思想。</p>
<p><strong>性质4：界的刻画</strong></p>
<p>CoSENT损失的上下界：</p>
<p>\begin{equation}
0 \leq \mathcal{L}<em _text_viol="\text{viol">{\text{CoSENT}} \leq \log(1 + N</em>
\end{equation}}} \cdot e^{2\lambda}) \tag{41</p>
<p>其中$N_{\text{viol}}$是违反排序的样本对数量。</p>
<h3 id="11">11. 数值稳定性分析<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>问题1：指数溢出</strong></p>
<p>当$\lambda(s_{kl} - s_{ij})$很大时，$e^{\lambda(s_{kl} - s_{ij})}$会溢出。</p>
<p><strong>解决方案</strong>：Log-sum-exp技巧</p>
<p>\begin{equation}
\log\sum_k e^{x_k} = M + \log\sum_k e^{x_k - M}, \quad M = \max_k x_k \tag{42}
\end{equation}</p>
<p>应用到CoSENT：</p>
<p>\begin{equation}
\mathcal{L} = \log\left(1 + e^M \sum e^{\lambda(s_{kl} - s_{ij}) - M}\right) \tag{43}
\end{equation}</p>
<p>其中$M = \max_{k,l} \lambda(s_{kl} - s_{ij})$。</p>
<p><strong>问题2：余弦相似度的数值精度</strong></p>
<p>当$u_i$和$u_j$几乎平行时，$|u_i| |u_j|$可能很小，导致除法不稳定。</p>
<p><strong>解决方案</strong>：添加epsilon</p>
<p>\begin{equation}
\cos(u_i, u_j) = \frac{\langle u_i, u_j \rangle}{\max(|u_i| |u_j|, \epsilon)}, \quad \epsilon = 10^{-8} \tag{44}
\end{equation}</p>
<p><strong>问题3：梯度消失/爆炸</strong></p>
<p>当$\lambda$过大或过小时，梯度可能消失或爆炸。</p>
<p><strong>监控指标</strong>：</p>
<p>\begin{equation}
\text{gradient_norm} = \sqrt{\sum_i |\nabla_{u_i} \mathcal{L}|^2} \tag{45}
\end{equation}</p>
<p>建议保持在$[0.1, 10]$范围内。</p>
<h3 id="12">12. 具体计算示例<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<p><strong>示例设置</strong>：</p>
<ul>
<li>Batch size = 4</li>
<li>Embedding dim = 3</li>
<li>相似度标签：$(1,2): 0.9$, $(1,3): 0.2$, $(2,3): 0.1$, $(1,4): 0.5$</li>
</ul>
<p><strong>步骤1：计算嵌入</strong></p>
<p>假设归一化后的嵌入为：</p>
<p>\begin{equation}
\begin{aligned}
u_1 &amp;= [0.577, 0.577, 0.577] \
u_2 &amp;= [0.707, 0.707, 0] \
u_3 &amp;= [0, 0, 1] \
u_4 &amp;= [0.5, 0.5, 0.707]
\end{aligned} \tag{46}
\end{equation}</p>
<p><strong>步骤2：计算余弦相似度</strong></p>
<p>\begin{equation}
\begin{aligned}
s_{12} &amp;= \langle u_1, u_2 \rangle = 0.577 \times 0.707 + 0.577 \times 0.707 = 0.816 \
s_{13} &amp;= \langle u_1, u_3 \rangle = 0.577 \
s_{14} &amp;= \langle u_1, u_4 \rangle = 0.816 \
s_{23} &amp;= \langle u_2, u_3 \rangle = 0
\end{aligned} \tag{47}
\end{equation}</p>
<p><strong>步骤3：识别违反排序的对</strong></p>
<p>根据$\text{sim}(1,2) &gt; \text{sim}(1,4) &gt; \text{sim}(1,3) &gt; \text{sim}(2,3)$：</p>
<ul>
<li>违反项：$(1,4)$的得分$s_{14} = 0.816$不应该等于$s_{12}$</li>
<li>期望：$s_{12} &gt; s_{14} &gt; s_{13} &gt; s_{23}$</li>
</ul>
<p><strong>步骤4：计算损失</strong>（设$\lambda = 20$）</p>
<p>\begin{equation}
\begin{aligned}
\mathcal{L} &amp;= \log\left(1 + e^{20(s_{14} - s_{12})} + e^{20(s_{13} - s_{12})} + \cdots\right) \
&amp;= \log\left(1 + e^{20(0.816 - 0.816)} + e^{20(0.577 - 0.816)} + \cdots\right) \
&amp;= \log\left(1 + e^{0} + e^{-4.78} + \cdots\right) \
&amp;= \log(1 + 1 + 0.0084 + \cdots) \
&amp;\approx \log(2.01) \approx 0.698
\end{aligned} \tag{48}
\end{equation}</p>
<p><strong>步骤5：计算梯度</strong></p>
<p>对$u_1$的梯度（针对$(1,2)$对）：</p>
<p>\begin{equation}
\frac{\partial \mathcal{L}}{\partial u_1} = -20 \times 0.5 \times (u_2 - 0.816 \cdot u_1) \approx -10 \times [0.236, 0.236, -0.472] \tag{49}
\end{equation}</p>
<h3 id="13">13. 实践建议与超参数调优<a class="toc-link" href="#13" title="Permanent link">&para;</a></h3>
<p><strong>温度参数$\lambda$的选择</strong>：</p>
<p>\begin{equation}
\lambda = \frac{\alpha}{\sigma_s} \tag{50}
\end{equation}</p>
<p>其中$\sigma_s$是余弦相似度的标准差，$\alpha \in [10, 30]$是可调因子。</p>
<p><strong>实践建议</strong>：</p>
<ol>
<li>
<p><strong>初始化</strong>：
   - 使用预训练模型（如BERT）初始化编码器
   - $\lambda$从20开始，根据验证集调整</p>
</li>
<li>
<p><strong>Batch构造</strong>：
   - 每个batch包含多样化的相似度级别
   - Batch size建议：64-256</p>
</li>
<li>
<p><strong>学习率调度</strong>：
   \begin{equation}
   \text{lr}(t) = \text{lr}<em _text_warmup="\text{warmup">0 \times \min\left(1, \frac{t}{T</em>
   \end{equation}}}}\right) \times \frac{1}{\sqrt{\max(t, T_{\text{warmup}})}} \tag{51</p>
</li>
<li>
<p><strong>正则化</strong>：
   - Weight decay: $10^{-5}$ to $10^{-4}$
   - Dropout: 0.1-0.3
   - Layer normalization在编码器输出</p>
</li>
<li>
<p><strong>评估指标</strong>：
   - Spearman相关系数（主要）
   - Pearson相关系数
   - 余弦相似度分布的KS检验</p>
</li>
<li>
<p><strong>调试技巧</strong>：
   - 监控相似度分布的均值和方差
   - 检查是否存在坍缩（所有向量变得相似）
   - 可视化t-SNE/UMAP降维结果</p>
</li>
</ol>
<p><strong>常见问题与解决</strong>：</p>
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td>损失不下降</td>
<td>$\lambda$过小</td>
<td>增大$\lambda$</td>
</tr>
<tr>
<td>梯度爆炸</td>
<td>$\lambda$过大</td>
<td>减小$\lambda$或梯度裁剪</td>
</tr>
<tr>
<td>表示坍缩</td>
<td>缺乏多样性</td>
<td>增加负样本多样性</td>
</tr>
<tr>
<td>过拟合</td>
<td>数据不足</td>
<td>数据增强、正则化</td>
</tr>
</tbody>
</table>
<p><strong>数据增强策略</strong>：</p>
<ol>
<li><strong>回译</strong>（Back-translation）</li>
<li><strong>同义词替换</strong></li>
<li><strong>句子重排</strong>（对于长文本）</li>
<li><strong>Cutoff</strong>（随机mask部分token）</li>
</ol>
<h3 id="14">14. 理论扩展与变体<a class="toc-link" href="#14" title="Permanent link">&para;</a></h3>
<p><strong>加权CoSENT</strong>：</p>
<p>为不同的违反对分配不同权重：</p>
<p>\begin{equation}
\mathcal{L}<em ij_kl="ij,kl">{\text{weighted}} = \log\left(1 + \sum w</em>
\end{equation}} e^{\lambda(s_{kl} - s_{ij})}\right) \tag{52</p>
<p>其中$w_{ij,kl} = |\text{sim}(i,j) - \text{sim}(k,l)|$。</p>
<p><strong>多任务CoSENT</strong>：</p>
<p>结合分类任务：</p>
<p>\begin{equation}
\mathcal{L}<em _text_CoSENT="\text{CoSENT">{\text{total}} = \mathcal{L}</em>
\end{equation}}} + \alpha \mathcal{L}_{\text{CE}} \tag{53</p>
<p><strong>层次化CoSENT</strong>：</p>
<p>在多个粒度上优化：</p>
<p>\begin{equation}
\mathcal{L}<em l="1">{\text{hierarchical}} = \sum</em>
\end{equation}}^{L} \beta_l \mathcal{L}_{\text{CoSENT}}^{(l)} \tag{54</p>
<p>其中$l$表示不同的语义层次（词、句、段落）。</p>
<h3 id="15">15. 收敛性分析<a class="toc-link" href="#15" title="Permanent link">&para;</a></h3>
<p><strong>定理（收敛性）</strong>：设学习率满足$\sum \eta_t = \infty$且$\sum \eta_t^2 &lt; \infty$，则CoSENT的SGD优化序列${u^{(t)}}$以概率1收敛到损失函数的稳定点。</p>
<p><strong>证明草图</strong>：</p>
<ol>
<li>CoSENT损失下有界（$\geq 0$）</li>
<li>梯度有界（紧集$\mathbb{S}^{d-1}$上）</li>
<li>应用随机近似理论的Robbins-Monro定理</li>
</ol>
<p><strong>收敛速率</strong>：</p>
<p>在强凸假设下（局部），收敛速率为：</p>
<p>\begin{equation}
\mathbb{E}[\mathcal{L}(u^{(T)})] - \mathcal{L}^* \leq \frac{C}{\sqrt{T}} \tag{55}
\end{equation}</p>
<p>实际中，由于Adam等自适应优化器，收敛更快。</p>
<hr />
<h3 id="1">第1部分：核心理论、公理与历史基础<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<h4 id="11_1">1.1 理论起源与历史发展<a class="toc-link" href="#11_1" title="Permanent link">&para;</a></h4>
<div class="theorem-box">

**交互式相似度模型的理论根源**：

- **Siamese Networks** (1994, Bromley et al.)：孪生网络用于签名验证
- **BERT** (2018, Devlin et al.)：预训练语言模型，提供强大的序列编码能力
- **交互式匹配**：将两个输入拼接后联合编码，捕获细粒度交互
- **CoSENT (2022, 本文系列)**：排序损失框架，最初用于特征式句向量

</div>

<p><strong>关键里程碑</strong>：</p>
<ol>
<li><strong>1994 - Siamese Networks</strong>：双塔结构，特征式匹配的鼻祖</li>
<li><strong>2016 - ESIM (Enhanced LSTM)</strong>：交互式匹配，在NLI任务上SOTA</li>
<li><strong>2018 - BERT</strong>：统一的预训练模型，同时支持特征式和交互式</li>
<li><strong>2019 - Sentence-BERT</strong>：针对句向量优化的双塔BERT</li>
<li><strong>2022 - CoSENT (一)</strong>：排序损失用于特征式句向量</li>
<li><strong>2022 - CoSENT (三，本文)</strong>：将排序损失泛化到交互式模型</li>
</ol>
<h4 id="12-vs">1.2 特征式 vs 交互式的本质区别<a class="toc-link" href="#12-vs" title="Permanent link">&para;</a></h4>
<div class="theorem-box">

### 公理1：计算范式的差异

**特征式（Sentence Embedding）**：
$$\text{sim}(s_1, s_2) = f(\text{Enc}(s_1), \text{Enc}(s_2))$$

独立编码两个句子，然后计算相似度。

**交互式（Cross-Encoder）**：
$$\text{sim}(s_1, s_2) = \text{Enc}([s_1; \text{SEP}; s_2])$$

拼接后联合编码，捕获细粒度交互。

**trade-off**：
- 特征式：快（可预计算），但表达力弱
- 交互式：慢（需配对计算），但表达力强

</div>

<div class="theorem-box">

### 公理2：损失函数的通用性

排序损失$\mathcal{L}_{\text{rank}}$与具体的相似度函数$f(\cdot,\cdot)$无关：

$$\mathcal{L} = \log\left(1 + \sum_{\text{sim}(i,j) > \text{sim}(k,l)} e^{\lambda(f(k,l) - f(i,j))}\right)$$

**推论**：CoSENT可作为**任意**相似度模型的损失函数，不限于余弦相似度。

</div>

<h4 id="13_1">1.3 设计哲学<a class="toc-link" href="#13_1" title="Permanent link">&para;</a></h4>
<p><strong>核心问题</strong>：交互式模型通常用交叉熵训练，能否用排序损失替代？</p>
<p><strong>设计对比</strong>：</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>交叉熵（CE）</th>
<th>CoSENT排序损失</th>
</tr>
</thead>
<tbody>
<tr>
<td>适用数据</td>
<td>二分类/多分类</td>
<td>连续打分 + 二分类</td>
</tr>
<tr>
<td>优化目标</td>
<td>分类准确率</td>
<td>排序一致性（Spearman）</td>
</tr>
<tr>
<td>标签要求</td>
<td>离散类别</td>
<td>相对顺序</td>
</tr>
<tr>
<td>对噪声</td>
<td>敏感</td>
<td>相对鲁棒</td>
</tr>
</tbody>
</table>
<p><strong>CoSENT的核心哲学</strong>（交互式场景）：</p>
<blockquote>
<p>"交叉熵告诉模型'这对相似/不相似'，但CoSENT告诉模型'这对应该比那对更相似'——后者信息更丰富，更贴近评测目标Spearman。"</p>
</blockquote>
<hr />
<h3 id="3">第3部分：数学直觉、多角度解释与类比<a class="toc-link" href="#3" title="Permanent link">&para;</a></h3>
<h4 id="31">3.1 生活化类比<a class="toc-link" href="#31" title="Permanent link">&para;</a></h4>
<div class="intuition-box">

### 🧠 类比1：评委打分 vs 评委排序

**场景**：歌唱比赛，有10个选手

**交叉熵方式（打分）**：
- 评委给每个选手打分：优秀/良好/一般/差
- 问题：边界模糊（85分算优秀还是良好？）
- 选手A：85分（优秀） vs 选手B：86分（优秀）→ 难以区分

**CoSENT方式（排序）**：
- 评委只排序：A > B > C > ...
- 优势：不需要绝对分数，只要相对顺序
- 选手A vs 选手B：明确A < B

**对应到模型**：
- CE：强制模型输出"相似"或"不相似"（离散）
- CoSENT：要求模型保持排序（连续）

</div>

<div class="intuition-box">

### 🧠 类比2：GPS导航的两种模式

**特征式（Sentence Embedding）= 查地图**：
- 提前计算所有地点的坐标
- 查询时：快速计算两点距离
- 缺点：无法考虑实时路况

**交互式（Cross-Encoder）= 实时导航**：
- 输入起点+终点，实时计算最优路径
- 考虑所有细节（路况、红绿灯）
- 缺点：每次查询都要重新计算

**CoSENT的作用**：
- 不管是"查地图"还是"实时导航"
- 都要求结果保持一致的排序
- 距离近的应该排前面，距离远的排后面

</div>

<h4 id="32">3.2 几何与概率意义<a class="toc-link" href="#32" title="Permanent link">&para;</a></h4>
<div class="intuition-box">

**几何视角：评分空间 vs 排序空间**

**交叉熵（CE）**：
- 在评分空间$[0, 1]$上优化
- 强制模型输出接近0或1
- 空间受限，过于刚性

**CoSENT**：
- 在排序空间$\mathbb{R}$上优化
- 只要求相对顺序正确
- 空间自由，模型可灵活调整输出尺度

**可视化**（1维情况）：

<div class="highlight"><pre><span></span><code><span class="n">CE空间</span><span class="err">（离散）：</span>
<span class="o">|---</span><span class="mi">0</span><span class="o">---|---</span><span class="mf">0.5</span><span class="o">---|---</span><span class="mi">1</span><span class="o">---|</span>
<span class="w">  </span><span class="err">负样本</span><span class="w">    </span><span class="err">模糊</span><span class="w">     </span><span class="err">正样本</span>

<span class="n">CoSENT空间</span><span class="err">（连续）：</span>
<span class="o">|</span><span class="p">...(</span><span class="o">-</span><span class="mf">3.2</span><span class="p">)...(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">)...(</span><span class="mf">0.8</span><span class="p">)...(</span><span class="mf">2.3</span><span class="p">)...(</span><span class="mf">4.1</span><span class="p">)...</span><span class="o">|</span>
<span class="w">      </span><span class="o">&lt;-----</span><span class="w">  </span><span class="err">保持排序</span><span class="w">  </span><span class="o">-----&gt;</span>
</code></pre></div>



</div>

<h4 id="33">3.3 多角度理解<a class="toc-link" href="#33" title="Permanent link">&para;</a></h4>
<p><strong>📊 信息论视角：标签信息的利用</strong></p>
<div class="intuition-box">

**CE**：只利用二元信息（0/1）
$$I_{CE} = -[y \log p + (1-y)\log(1-p)] \approx 1 \text{ bit}$$

**CoSENT**：利用所有样本对的相对关系
$$I_{CoSENT} = \sum_{ij,kl} \mathbb{1}_{\text{sim}(i,j) > \text{sim}(k,l)} \approx O(N^2) \text{ bits}$$

**信息增益**：CoSENT从每个batch中提取更多监督信号！

</div>

<p><strong>🎯 学习理论视角：决策边界</strong></p>
<div class="intuition-box">

**CE学习的是**：二元分类边界
$$\{(s_1, s_2) : f(s_1, s_2) = 0.5\}$$

**CoSENT学习的是**：排序函数
$$\forall i,j,k,l: \text{sim}(i,j) > \text{sim}(k,l) \Rightarrow f(i,j) > f(k,l)$$

**差异**：
- CE：硬边界，容易过拟合边界附近的样本
- CoSENT：软排序，关注全局相对关系

</div>

<hr />
<h3 id="4_1">第4部分：方法论变体、批判性比较与优化<a class="toc-link" href="#4_1" title="Permanent link">&para;</a></h3>
<h4 id="41">4.1 交互式模型损失函数对比表<a class="toc-link" href="#41" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>方法</th>
<th>核心思想</th>
<th>优点</th>
<th><strong>缺陷</strong></th>
<th><strong>优化方向</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>交叉熵（CE）</strong></td>
<td>Softmax分类</td>
<td>✅ 训练稳定<br>✅ 实现简单<br>✅ 标准方法</td>
<td>❌ <strong>只适用离散标签</strong><br>❌ 连续打分需回归<br>❌ 忽略样本间相对关系</td>
<td>✅ Label Smoothing<br>✅ Focal Loss<br>✅ 混合回归目标</td>
</tr>
<tr>
<td><strong>MSE回归</strong></td>
<td>直接拟合分数</td>
<td>✅ 适用连续标签<br>✅ 直观</td>
<td>❌ <strong>对异常值敏感</strong><br>❌ 与评测指标不一致<br>❌ 需要标签归一化</td>
<td>✅ Huber Loss<br>✅ 分位数回归<br>✅ 标准化标签</td>
</tr>
<tr>
<td><strong>Triplet Loss</strong></td>
<td>三元组对比</td>
<td>✅ 直接优化距离<br>✅ 理论清晰</td>
<td>❌ <strong>需要三元组</strong><br>❌ 采样复杂<br>❌ 训练慢</td>
<td>✅ 在线挖掘<br>✅ Semi-hard negative<br>✅ 自适应margin</td>
</tr>
<tr>
<td><strong>CoSENT (本文)</strong></td>
<td>排序损失</td>
<td>✅ 适用连续标签<br>✅ 与Spearman一致<br>✅ 利用batch内信息</td>
<td>❌ <strong>计算复杂度$O(N^2)$</strong><br>❌ 超参数$\lambda$需调<br>❌ 实现相对复杂</td>
<td>✅ 采样策略<br>✅ 自适应$\lambda$<br>✅ 混合CE</td>
</tr>
</tbody>
</table>
<h4 id="42-ce-">4.2 交叉熵（CE）- 批判性分析<a class="toc-link" href="#42-ce-" title="Permanent link">&para;</a></h4>
<div class="analysis-box">

### **核心缺陷**

**缺陷1：离散标签的限制**

**问题描述**：
- CE适用于二分类或多分类
- 连续打分（如STS-B的1-5分）需转换为分类或回归
- 转换过程损失信息

**根本原因**：
Softmax + CE本质是分类框架：
$$P(y=c | x) = \frac{e^{z_c}}{\sum_{c'} e^{z_{c'}}}$$

对于连续标签，需要：
1. 离散化（如1-5分 → 5类）→ 损失顺序信息
2. 回归（MSE）→ 与评测指标不一致

**定量影响**：
- STS-B数据集：离散化导致信息损失约15%-20%
- Spearman相关性：回归比排序损失低2%-5%

---

**缺陷2：忽略样本间关系**

**问题**：
- CE独立优化每个样本对：$\sum_i \mathcal{L}_i$
- 没有利用样本对之间的相对关系

**例子**：
| 样本对 | 真实相似度 | CE目标 | CoSENT约束 |
|--------|-----------|--------|-----------|
| (A, B) | 0.9 | $P=1$ | $f(A,B) > f(C,D)$ |
| (C, D) | 0.3 | $P=0$ | $f(A,B) > f(C,D)$ |

CE不关心$f(A,B)$和$f(C,D)$的相对大小，只要分类正确即可。

**定量影响**：
- 在PAWSX任务上，CE相比CoSENT低0.5%-1% Spearman

---

**缺陷3：对标注噪声敏感**

**问题**：
- CE对每个标签都严格拟合
- 如果标注有噪声（如众包），模型会过拟合噪声

**理论分析**：
设真实标签为$y^*$，观测标签为$\tilde{y} = y^* + \epsilon$（噪声）。

CE损失：
$$\mathcal{L}_{CE} = -\tilde{y} \log p - (1-\tilde{y})\log(1-p)$$

直接拟合$\tilde{y}$，包括噪声部分。

CoSENT：
$$\mathcal{L}_{CoSENT} = \log\left(1 + \sum_{\tilde{y}_i > \tilde{y}_j} e^{\lambda(f_j - f_i)}\right)$$

只要排序关系$\tilde{y}_i > \tilde{y}_j$正确（即噪声不改变顺序），就能正确学习。

---

### **优化方向**

**优化1：Label Smoothing**

**策略**：软化标签，减少过拟合

$$\tilde{y} = (1-\epsilon)y + \epsilon/K$$

其中$K$是类别数，$\epsilon \in [0.05, 0.1]$。

**效果**：
- 泛化能力提升1%-2%
- 对标注噪声更鲁棒

---

**优化2：Focal Loss**

**策略**：降低简单样本的权重，关注困难样本

$$\mathcal{L}_{Focal} = -(1-p_t)^\gamma \log p_t$$

其中$p_t$是正确类别的概率，$\gamma \in [1, 3]$。

**效果**：
- 在不平衡数据上提升3%-5%
- 收敛速度稍慢

---

**优化3：混合CE + 回归**

**策略**：同时优化分类和回归目标

$$\mathcal{L} = \mathcal{L}_{CE} + \alpha \mathcal{L}_{MSE}$$

**效果**：
- 在STS-B上提升2%-3% Spearman
- 需要调节$\alpha$（通常0.1-0.5）

</div>

<h4 id="43-cosent-">4.3 CoSENT（交互式）- 批判性分析<a class="toc-link" href="#43-cosent-" title="Permanent link">&para;</a></h4>
<div class="analysis-box">

### **核心缺陷**

**缺陷1：与CE效果相当（提升有限）**

**实验观察**：
- 平均提升：<0.5% Spearman
- 某些任务（如PAWSX）提升明显（+0.75%）
- 某些任务（如BQ）无明显差异

**根本原因**：
交互式模型已经很强，CE已经足够优化分类边界。CoSENT的排序优势在弱模型或困难任务上才明显。

**定量分析**：
| 模型强度 | CE | CoSENT | 提升 |
|---------|----|---------|----|
| BERT (弱) | 66.77 | 67.10 | +0.33 |
| RoBERTa (强) | 68.14 | 68.06 | -0.08 |

---

**缺陷2：计算复杂度高**

**问题**：
- Batch内所有样本对的组合：$O(N^2)$
- 相比CE的$O(N)$，慢很多

**实测**（batch size=64）：
- CE：约50ms/batch
- CoSENT：约150ms/batch（3倍慢）

---

**缺陷3：无法输出校准的概率**

**问题**：
- CE输出的概率经过Softmax，自然校准
- CoSENT输出的是任意尺度的得分$f(s_1, s_2) \in \mathbb{R}$

**影响**：
- 如果下游任务需要校准的概率（如集成学习），CE更合适
- CoSENT需要额外的校准步骤（如Platt Scaling）

---

### **优化方向**

**优化1：采样策略**

**策略**：只比较部分样本对，降低复杂度


<div class="highlight"><pre><span></span><code><span class="c1"># 每个样本对只采样k个对比对</span>
<span class="n">sampled_pairs</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">all_pairs</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>



**效果**：
- 复杂度降至$O(Nk)$
- 性能下降<0.5%（$k \geq 10$）

---

**优化2：混合策略**

**策略**：预训练用CE，精调用CoSENT


<div class="highlight"><pre><span></span><code><span class="c1"># 预训练（80%数据）</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">CE_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="c1"># 精调（20%数据）</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">CoSENT_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div>



**效果**：
- 结合两者优势
- 训练时间增加<20%

---

**优化3：模型融合**

**策略**：训练CE模型和CoSENT模型，集成预测

$$\text{final\_score} = \alpha \cdot f_{CE} + (1-\alpha) \cdot f_{CoSENT}$$

**效果**：
- 提升1%-2%（集成效应）
- 推理时间翻倍

</div>

<hr />
<h3 id="5_1">第5部分：学习路线图与未来展望<a class="toc-link" href="#5_1" title="Permanent link">&para;</a></h3>
<h4 id="51">5.1 学习路线图<a class="toc-link" href="#51" title="Permanent link">&para;</a></h4>
<p><strong>必备前置知识</strong>：
- 深度学习基础：反向传播、优化器
- NLP基础：BERT、Transformer、句向量
- 度量学习：Triplet Loss、Contrastive Loss
- 排序学习：Ranking Loss、LambdaRank</p>
<p><strong>推荐学习顺序</strong>：
1. 理解特征式vs交互式的区别（CoSENT二）
2. 掌握排序损失（CoSENT一）
3. 学习本文：泛化到交互式
4. 实践：在自己的任务上对比CE vs CoSENT</p>
<hr />
<p><strong>核心论文</strong>：
1. <strong>Devlin et al. (2018) - BERT</strong> ⭐
2. <strong>Reimers &amp; Gurevych (2019) - Sentence-BERT</strong> ⭐
3. <strong>苏剑林 (2022) - CoSENT (一)</strong> ⭐⭐
4. <strong>本文 (2022) - CoSENT (三)</strong> ⭐</p>
<hr />
<h4 id="52">5.2 研究空白与未来方向<a class="toc-link" href="#52" title="Permanent link">&para;</a></h4>
<h4 id="1-cosent_1"><strong>方向1：理论层面 - CoSENT在交互式模型的收敛性</strong><a class="toc-link" href="#1-cosent_1" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- CE的收敛性已被广泛研究，CoSENT在交互式场景的理论保证未知
- 何时CoSENT优于CE？需要严格的理论分析</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：CoSENT在交互式模型上的泛化界？
   - <strong>挑战</strong>：$O(N^2)$的样本对关系，泛化分析更复杂
   - <strong>潜在方法</strong>：</p>
<ul>
<li>利用Rademacher复杂度分析</li>
<li>建立排序损失的PAC界</li>
<li><strong>量化目标</strong>：证明在何种条件下CoSENT泛化更好</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：弱模型 vs 强模型的差异？
   - <strong>观察</strong>：BERT上CoSENT优势明显，RoBERTa上无差异
   - <strong>理论解释</strong>：模型容量与排序损失的关系
   - <strong>探索方向</strong>：定义"模型强度"指标，建立与CoSENT收益的关系</p>
</li>
</ol>
<hr />
<h4 id="2-"><strong>方向2：效率层面 - 降低计算复杂度</strong><a class="toc-link" href="#2-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- $O(N^2)$复杂度限制了batch size
- 能否设计更高效的近似？</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：能否设计$O(N \log N)$的排序损失？
   - <strong>优化方向</strong>：</p>
<ul>
<li>基于采样的近似</li>
<li>只比较Top-k相似和Bottom-k不相似</li>
<li>分层排序策略</li>
<li><strong>量化目标</strong>：复杂度降低5倍，性能下降&lt;1%</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：动态batch构造？
   - <strong>思路</strong>：每个batch包含多样化的相似度级别
   - <strong>实现</strong>：在线采样，确保batch内有足够的排序信息
   - <strong>效果预期</strong>：小batch达到大batch效果</p>
</li>
</ol>
<hr />
<h4 id="3-"><strong>方向3：应用层面 - 多模态与跨语言</strong><a class="toc-link" href="#3-" title="Permanent link">&para;</a></h4>
<p><strong>研究空白</strong>：
- CoSENT主要在文本上验证，多模态（图文）应用不足
- 跨语言排序损失的设计</p>
<p><strong>具体研究问题</strong>：</p>
<ol>
<li>
<p><strong>问题</strong>：图文匹配的排序损失？
   - <strong>场景</strong>：$(图像, 文本)$对的相似度排序
   - <strong>优化方向</strong>：</p>
<ul>
<li>定义跨模态相似度：$f(\text{img}, \text{txt})$</li>
<li>扩展CoSENT：$f(\text{img}_i, \text{txt}_i) &gt; f(\text{img}_k, \text{txt}_l)$</li>
<li><strong>量化目标</strong>：Image-Text Retrieval R@1 &gt; CLIP</li>
</ul>
</li>
<li>
<p><strong>问题</strong>：跨语言排序一致性？
   - <strong>场景</strong>：$(英文句, 中文句)$的相似度
   - <strong>挑战</strong>：不同语言的语义对齐
   - <strong>优化方向</strong>：多语言BERT + CoSENT</p>
</li>
</ol>
<hr />
<h4 id="_5"><strong>潜在应用场景</strong><a class="toc-link" href="#_5" title="Permanent link">&para;</a></h4>
<p><strong>问答系统</strong>：
- 问题-答案匹配：排序候选答案
- 优势：直接优化排序指标（如MRR、NDCG）</p>
<p><strong>检索系统</strong>：
- 查询-文档匹配：排序检索结果
- 优势：与评测指标（Recall@K）一致</p>
<p><strong>推荐系统</strong>：
- 用户-物品匹配：排序推荐列表
- 优势：处理隐式反馈（点击&gt;未点击）</p>
<hr />
<h3 id="_6">总结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h3>
<p>本文将CoSENT从特征式句向量（余弦相似度）泛化到交互式相似度模型（任意相似度函数），验证了其作为通用排序损失的有效性。实验表明，CoSENT与交叉熵（CE）在交互式模型上效果相当，但在弱模型或困难任务上略有优势。</p>
<p><strong>核心贡献</strong>：
1. 理论：将CoSENT泛化为通用排序损失框架
2. 实验：对比CE vs CoSENT在4个数据集上的表现
3. 发现：弱模型/困难任务更受益于排序损失
4. 实践：提供混合策略、采样优化等工程建议</p>
<p><strong>未来值得关注</strong>：
- 理论：收敛性、泛化界
- 效率：降低$O(N^2)$复杂度
- 应用：多模态、跨语言
- 混合：CE + CoSENT集成</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="利用cur分解加速交互式相似度模型的检索.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#206 利用CUR分解加速交互式相似度模型的检索</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="基于amos优化器思想推导出来的一些炼丹策略.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#208 基于Amos优化器思想推导出来的一些“炼丹策略”</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#cosent">CoSENT（三）：作为交互式相似度的损失函数</a><ul>
<li><a href="#_1">一、基础回顾</a></li>
<li><a href="#_2">二、实验比较</a></li>
<li><a href="#_3">文章小结</a></li>
<li><a href="#_4">公式推导与注释</a><ul>
<li><a href="#1-cosent">1. CoSENT损失函数的完整数学定义</a></li>
<li><a href="#2-circle-loss">2. 与Circle Loss的深层联系</a></li>
<li><a href="#3-infonce">3. 与InfoNCE的对比分析</a></li>
<li><a href="#4">4. 对比学习的理论基础</a></li>
<li><a href="#5">5. 梯度计算与优化性质</a></li>
<li><a href="#6">6. 信息论解释</a></li>
<li><a href="#7">7. 概率论视角</a></li>
<li><a href="#8">8. 几何理解</a></li>
<li><a href="#9">9. 与其他损失函数的对比</a></li>
<li><a href="#10">10. 理论性质深入分析</a></li>
<li><a href="#11">11. 数值稳定性分析</a></li>
<li><a href="#12">12. 具体计算示例</a></li>
<li><a href="#13">13. 实践建议与超参数调优</a></li>
<li><a href="#14">14. 理论扩展与变体</a></li>
<li><a href="#15">15. 收敛性分析</a></li>
<li><a href="#1">第1部分：核心理论、公理与历史基础</a></li>
<li><a href="#3">第3部分：数学直觉、多角度解释与类比</a></li>
<li><a href="#4_1">第4部分：方法论变体、批判性比较与优化</a></li>
<li><a href="#5_1">第5部分：学习路线图与未来展望</a></li>
<li><a href="#_6">总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>