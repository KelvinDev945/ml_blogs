<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>生成扩散模型漫谈（四）：DDIM = 高观点DDPM | ML & Math Blog Posts</title>
    <meta name="description" content="生成扩散模型漫谈（四）：DDIM = 高观点DDPM&para;
原文链接: https://spaces.ac.cn/archives/9181
发布日期: 

相信很多读者都听说过甚至读过克莱因的《高观点下的初等数学》这套书，顾名思义，这是在学到了更深入、更完备的数学知识后，从更高的视角重新审视过往学过的初等数学，以得到更全面的认知，甚至达到温故而知新的效果。类似的书籍还有很多，比如《重温微积...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=微分方程">微分方程</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #222 生成扩散模型漫谈（四）：DDIM = 高观点DDPM
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#222</span>
                生成扩散模型漫谈（四）：DDIM = 高观点DDPM
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/9181" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=微分方程" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 微分方程</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
                <a href="../index.html?tags=DDPM" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> DDPM</span>
                </a>
                
                <a href="../index.html?tags=扩散" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 扩散</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="ddim-ddpm">生成扩散模型漫谈（四）：DDIM = 高观点DDPM<a class="toc-link" href="#ddim-ddpm" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/9181">https://spaces.ac.cn/archives/9181</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>相信很多读者都听说过甚至读过克莱因的<a href="https://book.douban.com/subject/3249247/">《高观点下的初等数学》</a>这套书，顾名思义，这是在学到了更深入、更完备的数学知识后，从更高的视角重新审视过往学过的初等数学，以得到更全面的认知，甚至达到温故而知新的效果。类似的书籍还有很多，比如<a href="https://book.douban.com/subject/1239791/">《重温微积分》</a>、<a href="https://book.douban.com/subject/3788399/">《复分析：可视化方法》</a>等。</p>
<p>回到扩散模型，目前我们已经通过三篇文章从不同视角去解读了DDPM，那么它是否也存在一个更高的理解视角，让我们能从中得到新的收获呢？当然有，<a href="https://papers.cool/arxiv/2010.02502">《Denoising Diffusion Implicit Models》</a>介绍的DDIM模型就是经典的案例，本文一起来欣赏它。</p>
<h2 id="_1">思路分析<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>在<a href="/archives/9164">《生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪》</a>中，我们提到过该文章所介绍的推导跟DDIM紧密相关。具体来说，文章的推导路线可以简单归纳如下：<br />
\begin{equation}p(\boldsymbol{x}<em t-1="t-1">t|\boldsymbol{x}</em>})\xrightarrow{\text{推导}}p(\boldsymbol{x<em t-1="t-1">t|\boldsymbol{x}_0)\xrightarrow{\text{推导}}p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0)\xrightarrow{\text{近似}}p(\boldsymbol{x}</em>}|\boldsymbol{x}_t)\end{equation<br />
这个过程是一步步递进的。然而，我们发现最终结果有着两个特点：</p>
<blockquote>
<p>1、损失函数只依赖于$p(\boldsymbol{x}_t|\boldsymbol{x}_0)$；</p>
<p>2、采样过程只依赖于$p(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t)$。</p>
</blockquote>
<p>也就是说，尽管整个过程是以$p(\boldsymbol{x}<em t-1="t-1">t|\boldsymbol{x}</em>})$为出发点一步步往前推的，但是从结果上来看，压根儿就没$p(\boldsymbol{x<em t-1="t-1">t|\boldsymbol{x}</em>)$的事。那么，我们大胆地“异想天开”一下：</p>
<blockquote>
<p><strong>高观点1：</strong> 既然结果跟$p(\boldsymbol{x}<em t-1="t-1">t|\boldsymbol{x}</em>})$无关，可不可以干脆“过河拆桥”，将$p(\boldsymbol{x<em t-1="t-1">t|\boldsymbol{x}</em>)$从整个推导过程中去掉？</p>
</blockquote>
<p>DDIM正是这个“异想天开”的产物！</p>
<h2 id="_2">待定系数<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>可能有读者会想，根据上一篇文章所用的贝叶斯定理<br />
\begin{equation}p(\boldsymbol{x}<em t-1="t-1">{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0) = \frac{p(\boldsymbol{x}_t|\boldsymbol{x}</em>})p(\boldsymbol{x<em t-1="t-1">{t-1}|\boldsymbol{x}_0)}{p(\boldsymbol{x}_t|\boldsymbol{x}_0)}\end{equation}<br />
没有给定$p(\boldsymbol{x}_t|\boldsymbol{x}</em>})$怎么能得到$p(\boldsymbol{x<em t-1="t-1">{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0)$？这其实是思维过于定式了，理论上在没有给定$p(\boldsymbol{x}_t|\boldsymbol{x}</em>})$的情况下，$p(\boldsymbol{x<em t-1="t-1">{t-1}|\boldsymbol{x}_t, \boldsymbol{x}_0)$的解空间更大，某种意义上来说是更加容易推导，此时它只需要满足边际分布条件：<br />
\begin{equation}\int p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0) p(\boldsymbol{x}_t|\boldsymbol{x}_0) d\boldsymbol{x}_t = p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">0)\label{eq:margin}\end{equation}<br />
我们用待定系数法来求解这个方程。在上一篇文章中，所解出的$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0)$是一个正态分布，所以这一次我们可以更一般地设<br />
\begin{equation}p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0) = \mathcal{N}(\boldsymbol{x}</em>}; \kappa_t \boldsymbol{x<em t-1="t-1">t + \lambda_t \boldsymbol{x}_0, \sigma_t^2 \boldsymbol{I})\end{equation}<br />
其中$\kappa_t,\lambda_t,\sigma_t$都是待定系数，而为了不重新训练模型，我们不改变$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">0)$和$p(\boldsymbol{x}_t|\boldsymbol{x}_0)$，于是我们可以列出<br />
\begin{array}{c|c|c}<br />
\hline<br />
\text{记号} &amp; \text{含义} &amp; \text{采样}\\<br />
\hline<br />
p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">0) &amp; \mathcal{N}(\boldsymbol{x}</em>};\bar{\alpha<em t-1="t-1">{t-1} \boldsymbol{x}_0,\bar{\beta}</em>}^2 \boldsymbol{I}) &amp; \boldsymbol{x<em t-1="t-1">{t-1} = \bar{\alpha}</em>} \boldsymbol{x<em t-1="t-1">0 + \bar{\beta}</em> \\} \boldsymbol{\varepsilon<br />
\hline<br />
p(\boldsymbol{x}<em t-1="t-1">t|\boldsymbol{x}_0) &amp; \mathcal{N}(\boldsymbol{x}_t;\bar{\alpha}_t \boldsymbol{x}_0,\bar{\beta}_t^2 \boldsymbol{I}) &amp; \boldsymbol{x}_t = \bar{\alpha}_t \boldsymbol{x}_0 + \bar{\beta}_t \boldsymbol{\varepsilon}_1 \\<br />
\hline<br />
p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0) &amp; \mathcal{N}(\boldsymbol{x}</em>}; \kappa_t \boldsymbol{x<em t-1="t-1">t + \lambda_t \boldsymbol{x}_0, \sigma_t^2 \boldsymbol{I}) &amp; \boldsymbol{x}</em>} = \kappa_t \boldsymbol{x<em t-1="t-1">t + \lambda_t \boldsymbol{x}_0 + \sigma_t \boldsymbol{\varepsilon}_2 \\<br />
\hline<br />
{\begin{array}{c}\int p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0) \\<br />
p(\boldsymbol{x}_t|\boldsymbol{x}_0) d\boldsymbol{x}_t\end{array}} &amp; &amp; {\begin{aligned}\boldsymbol{x}</em>} =&amp;\, \kappa_t \boldsymbol{x<em t-1="t-1">t + \lambda_t \boldsymbol{x}_0 + \sigma_t \boldsymbol{\varepsilon}_2 \\<br />
=&amp;\, \kappa_t (\bar{\alpha}_t \boldsymbol{x}_0 + \bar{\beta}_t \boldsymbol{\varepsilon}_1) + \lambda_t \boldsymbol{x}_0 + \sigma_t \boldsymbol{\varepsilon}_2 \\<br />
=&amp;\, (\kappa_t \bar{\alpha}_t + \lambda_t) \boldsymbol{x}_0 + (\kappa_t\bar{\beta}_t \boldsymbol{\varepsilon}_1 + \sigma_t \boldsymbol{\varepsilon}_2) \\<br />
\end{aligned}} \\<br />
\hline<br />
\end{array}<br />
其中$\boldsymbol{\varepsilon},\boldsymbol{\varepsilon}_1,\boldsymbol{\varepsilon}_2\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})$，并且由正态分布的叠加性我们知道$\kappa_t\bar{\beta}_t \boldsymbol{\varepsilon}_1 + \sigma_t \boldsymbol{\varepsilon}_2\sim \sqrt{\kappa_t^2\bar{\beta}_t^2 + \sigma_t^2} \boldsymbol{\varepsilon}$。对比$\boldsymbol{x}</em>$成立，只需要满足两个方程}$的两个采样形式，我们发现要想$\eqref{eq:margin<br />
\begin{equation}\bar{\alpha}<em t-1="t-1">{t-1} = \kappa_t \bar{\alpha}_t + \lambda_t, \qquad\bar{\beta}</em>} = \sqrt{\kappa_t^2\bar{\beta<em t-1="t-1">t^2 + \sigma_t^2}\end{equation}<br />
可以看到有三个未知数，但只有两个方程，这就是为什么说没有给定$p(\boldsymbol{x}_t|\boldsymbol{x}</em>)$时解空间反而更大了。将$\sigma_t$视为可变参数，可以解出<br />
\begin{equation}\kappa_t = \frac{\sqrt{\bar{\beta}<em t-1="t-1">{t-1}^2 - \sigma_t^2}}{\bar{\beta}_t},\qquad \lambda_t = \bar{\alpha}</em>} - \frac{\bar{\alpha<em t-1="t-1">t\sqrt{\bar{\beta}</em>}^2 - \sigma_t^2}}{\bar{\beta<em t-1="t-1">t}\end{equation}<br />
或者写成<br />
\begin{equation}p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0) = \mathcal{N}\left(\boldsymbol{x}</em>}; \frac{\sqrt{\bar{\beta<em t-1="t-1">{t-1}^2 - \sigma_t^2}}{\bar{\beta}_t} \boldsymbol{x}_t + \left(\bar{\alpha}</em>} - \frac{\bar{\alpha<em t-1="t-1">t\sqrt{\bar{\beta}</em>}^2 - \sigma_t^2}}{\bar{\beta}_t}\right) \boldsymbol{x}_0, \sigma_t^2 \boldsymbol{I}\right)\label{eq:p-xt-x0}\end{equation<br />
方便起见，我们约定$\bar{\alpha}_0=1, \bar{\beta}_0=0$。特别地，这个结果并不需要限定$\bar{\alpha}_t^2 + \bar{\beta}_t^2 = 1$，不过为了简化参数设置，同时也为了跟以往的结果对齐，这里还是约定$\bar{\alpha}_t^2 + \bar{\beta}_t^2 = 1$。</p>
<h2 id="_3">一如既往<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>现在我们在只给定$p(\boldsymbol{x}<em t-1="t-1">t|\boldsymbol{x}_0)$、$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">0)$的情况下，通过待定系数法求解了$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0)$的一簇解，它带有一个自由参数$\sigma_t$。用<a href="/archives/9119">《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》</a>中的“拆楼-建楼”类比来说，就是我们知道楼会被拆成什么样【$p(\boldsymbol{x}_t|\boldsymbol{x}_0)$、$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">0)$】，但是不知道每一步怎么拆【$p(\boldsymbol{x}_t|\boldsymbol{x}</em>})$】，然后希望能够从中学会每一步怎么建【$p(\boldsymbol{x<em t-1="t-1">{t-1}|\boldsymbol{x}_t)$】。当然，如果我们想看看每一步怎么拆的话，也可以反过来用贝叶斯公式<br />
\begin{equation} p(\boldsymbol{x}_t|\boldsymbol{x}</em>}, \boldsymbol{x<em t-1="t-1">0) = \frac{p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0) p(\boldsymbol{x}_t|\boldsymbol{x}_0)}{p(\boldsymbol{x}</em>}|\boldsymbol{x}_0)}\end{equation</p>
<p>接下来的事情，就跟上一篇文章一模一样了：我们最终想要$p(\boldsymbol{x}<em t-1="t-1">{t-1}|\boldsymbol{x}_t)$而不是$p(\boldsymbol{x}</em>}|\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, \boldsymbol{x}_0)$，所以我们希望用<br />
\begin{equation}\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t) = \frac{1}{\bar{\alpha}_t}\left(\boldsymbol{x}_t - \bar{\beta}_t \boldsymbol{\epsilon}</em>}}(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t, t)\right)\end{equation}<br />
来估计$\boldsymbol{x}_0$，由于没有改动$p(\boldsymbol{x}_t|\boldsymbol{x}_0)$，所以训练所用的目标函数依然是$\left\Vert\boldsymbol{\varepsilon} - \boldsymbol{\epsilon}</em>}}(\bar{\alpha<em t-1="t-1">t \boldsymbol{x}_0 + \bar{\beta}_t \boldsymbol{\varepsilon}, t)\right\Vert^2$（除去权重系数），也就是说训练过程没有改变，我们可以用回DDPM训练好的模型。而用$\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)$替换掉式$\eqref{eq:p-xt-x0}$中的$\boldsymbol{x}_0$后，得到<br />
\begin{equation}\begin{aligned}<br />
p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t) \approx&amp;\, p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0=\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)) \\<br />
=&amp;\, \mathcal{N}\left(\boldsymbol{x}</em>}; \frac{1}{\alpha_t}\left(\boldsymbol{x<em t-1="t-1">t - \left(\bar{\beta}_t - \alpha_t\sqrt{\bar{\beta}</em>}^2 - \sigma_t^2}\right) \boldsymbol{\epsilon<em t-1="t-1">{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\right), \sigma_t^2 \boldsymbol{I}\right)<br />
\end{aligned}\label{eq:p-xt-x0-2}\end{equation}<br />
这就求出了生成过程所需要的$p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t)$，其中$\alpha_t=\frac{\bar{\alpha}_t}{\bar{\alpha}</em>$。它的特点是训练过程没有变化（也就是说最终保存下来的模型没有变化），但生成过程却有一个可变动的参数$\sigma_t$，就是这个参数给DDPM带来了新鲜的结果。}</p>
<h2 id="_4">几个例子<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>原则上来说，我们对$\sigma_t$没有过多的约束，但是不同$\sigma_t$的采样过程会呈现出不同的特点，我们举几个例子进行分析。</p>
<p>第一个简单例子就是取$\sigma_t = \frac{\bar{\beta}<em t-1="t-1">{t-1}\beta_t}{\bar{\beta}_t}$，其中$\beta_t = \sqrt{1 - \alpha_t^2}$，相应地有<br />
\begin{equation}\small{p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t) \approx p(\boldsymbol{x}</em>}|\boldsymbol{x<em t-1="t-1">t, \boldsymbol{x}_0=\bar{\boldsymbol{\mu}}(\boldsymbol{x}_t)) = \mathcal{N}\left(\boldsymbol{x}</em>}; \frac{1}{\alpha_t}\left(\boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">t - \frac{\beta_t^2}{\bar{\beta}_t}\boldsymbol{\epsilon}</em>}}(\boldsymbol{x<em t-1="t-1">t, t)\right),\frac{\bar{\beta}</em>}^2\beta_t^2}{\bar{\beta<em t-1="t-1">t^2} \boldsymbol{I}\right)}\label{eq:choice-1}\end{equation}<br />
这就是上一篇文章所推导的DDPM。特别是，DDIM论文中还对$\sigma_t = \eta\frac{\bar{\beta}</em>$做了对比实验，其中$\eta\in[0, 1]$。}\beta_t}{\bar{\beta}_t</p>
<p>第二个例子就是取$\sigma_t = \beta_t$，这也是前两篇文章所指出的$\sigma_t$的两个选择之一，在此选择下式$\eqref{eq:p-xt-x0-2}$未能做进一步的化简，但DDIM的实验结果显示此选择在DDPM的标准参数设置下表现还是很好的。</p>
<p>最特殊的一个例子是取$\sigma_t = 0$，此时从$\boldsymbol{x}<em t-1="t-1">t$到$\boldsymbol{x}</em>$是一个确定性变换<br />
\begin{equation}\boldsymbol{x}<em t-1="t-1">{t-1} = \frac{1}{\alpha_t}\left(\boldsymbol{x}_t - \left(\bar{\beta}_t - \alpha_t \bar{\beta}</em>}\right) \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\right)\label{eq:sigma=0}\end{equation<br />
这也是DDIM论文中特别关心的一个例子，准确来说，原论文的DDIM就是特指$\sigma_t=0$的情形，其中“I”的含义就是“Implicit”，意思这是一个隐式的概率模型，因为跟其他选择所不同的是，此时从给定的$\boldsymbol{x}_T = \boldsymbol{z}$出发，得到的生成结果$\boldsymbol{x}_0$是不带随机性的。后面我们将会看到，这在理论上和实用上都带来了一些好处。</p>
<h2 id="_5">加速生成<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>值得指出的是，在这篇文章中我们没有以$p(\boldsymbol{x}<em t-1="t-1">t|\boldsymbol{x}</em>})$为出发点，所以前面的所有结果实际上全都是以$\bar{\alpha<em t-1="t-1">t,\bar{\beta}_t$相关记号给出的，而$\alpha_t,\beta_t$则是通过$\alpha_t=\frac{\bar{\alpha}_t}{\bar{\alpha}</em>_t$，训练过程也就确定了。}}$和$\beta_t = \sqrt{1 - \alpha_t^2}$派生出来的记号。从损失函数$\left\Vert\boldsymbol{\varepsilon} - \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\bar{\alpha}_t \boldsymbol{x}_0 + \bar{\beta}_t \boldsymbol{\varepsilon}, t)\right\Vert^2$可以看出，给定了各个$\bar{\alpha</p>
<p>从这个过程中，DDIM进一步留意到了如下事实：</p>
<blockquote>
<p><strong>高观点2：</strong> DDPM的训练结果实质上包含了它的任意子序列参数的训练结果。</p>
</blockquote>
<p>具体来说，设$\boldsymbol{\tau} = [\tau_1,\tau_2,\dots,\tau_{\dim(\boldsymbol{\tau})}]$是$[1,2,\cdots,T]$的任意子序列，那么我们以$\bar{\alpha}<em _tau_2="\tau_2">{\tau_1},\bar{\alpha}</em>_T$的$T$步DDPM的目标函数的一个子集！所以在模型拟合能力足够好的情况下，它其实包含了任意子序列参数的训练结果。},\cdots,\bar{\alpha}_{\dim(\boldsymbol{\tau})}$为参数训练一个扩散步数为$\dim(\boldsymbol{\tau})$步的DDPM，其目标函数实际上是原来以$\bar{\alpha}_1,\bar{\alpha}_2,\cdots,\bar{\alpha</p>
<p>那么反过来想，如果有一个训练好的$T$步DDPM模型，我们也可以将它当成是以$\bar{\alpha}<em _tau_2="\tau_2">{\tau_1},\bar{\alpha}</em>},\cdots,\bar{\alpha<em _tau__i-1="\tau_{i-1">{\dim(\boldsymbol{\tau})}$为参数训练出来的$\dim(\boldsymbol{\tau})$步模型，而既然是$\dim(\boldsymbol{\tau})$步的模型，生成过程也就只需要$\dim(\boldsymbol{\tau})$步了，根据式$\eqref{eq:p-xt-x0-2}$有：<br />
\begin{equation}p(\boldsymbol{x}</em>}}|\boldsymbol{x<em _tau__i-1="\tau_{i-1">{\tau_i}) \approx \mathcal{N}\left(\boldsymbol{x}</em>}}; \frac{\bar{\alpha<em i-1="i-1">{\tau</em>}}}{\bar{\alpha<em _tau_i="\tau_i">{\tau_i}}\left(\boldsymbol{x}</em>} - \left(\bar{\beta<em _tau_i="\tau_i">{\tau_i} - \frac{\bar{\alpha}</em>}}{\bar{\alpha<em i-1="i-1">{\tau</em>}}}\sqrt{\bar{\beta<em i-1="i-1">{\tau</em>}}^2 - \tilde{\sigma<em _boldsymbol_theta="\boldsymbol{\theta">{\tau_i}^2}\right) \boldsymbol{\epsilon}</em>}}(\boldsymbol{x<em _tau_i="\tau_i">{\tau_i}, \tau_i)\right), \tilde{\sigma}</em>}^2 \boldsymbol{I}\right)\end{equation<br />
这就是加速采样的生成过程了，从原来的$T$步扩散生成变成了$\dim(\boldsymbol{\tau})$步。要注意不能直接将式$\eqref{eq:p-xt-x0-2}$的$\alpha_t$换成$\alpha_{\tau_i}$，因为我们说过$\alpha_t$是派生记号而已，它实际上等于$\frac{\bar{\alpha}<em t-1="t-1">t}{\bar{\alpha}</em>}}$，因此$\alpha_t$要换成$\frac{\bar{\alpha<em _tau__i-1="\tau_{i-1">{\tau_i}}{\bar{\alpha}</em>}}}$才对。同理，$\tilde{\sigma<em _tau_i="\tau_i">{\tau_i}$也不是直接取$\sigma</em>}$，而是在将其定义全部转化为$\bar{\alpha},\bar{\beta}$符号后，将$t$替换为$\tau_i$、$t-1$替换为$\tau_{i-1}$，比如式$\eqref{eq:choice-1}$对应的$\tilde{\sigma<em t-1="t-1">{\tau_i}$为<br />
\begin{equation}\sigma_t = \frac{\bar{\beta}</em>}\beta_t}{\bar{\beta<em t-1="t-1">t}=\frac{\bar{\beta}</em>}}{\bar{\beta<em t-1="t-1">t}\sqrt{1 - \frac{\bar{\alpha}_t^2}{\bar{\alpha}</em>}^2}}\quad\to\quad\frac{\bar{\beta<em i-1="i-1">{\tau</em>}}}{\bar{\beta<em _tau_i="\tau_i">{\tau_i}}\sqrt{1 - \frac{\bar{\alpha}</em>}^2}{\bar{\alpha<em i-1="i-1">{\tau</em>}}^2}}=\tilde{\sigma}_{\tau_i}\end{equation</p>
<p>可能读者又想问，我们为什么干脆不直接训练一个$\dim(\boldsymbol{\tau})$步的扩散模型，而是要先训练$T &gt; \dim(\boldsymbol{\tau})$步然后去做子序列采样？笔者认为可能有两方面的考虑：一方面从$\dim(\boldsymbol{\tau})$步生成来说，训练更多步数的模型也许能增强泛化能力；另一方面，通过子序列$\boldsymbol{\tau}$进行加速只是其中一种加速手段，训练更充分的$T$步允许我们尝试更多的其他加速手段，但并不会显著增加训练成本。</p>
<h2 id="_6">实验结果<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>原论文对不同的噪声强度和扩散步数$\dim(\boldsymbol{\tau})$做了组合对比，大致上的结果是“噪声越小，加速后的生成效果越好”，如下图  </p>
<p><a href="/usr/uploads/2022/07/724883953.jpg" title="点击查看原图"><img alt="DDIM的实验结果，显示噪声越小，加速后的生成效果越好" src="/usr/uploads/2022/07/724883953.jpg" /></a></p>
<p>DDIM的实验结果，显示噪声越小，加速后的生成效果越好</p>
<p>笔者的参考实现如下：</p>
<blockquote>
<p><strong>Github：<a href="https://github.com/bojone/Keras-DDPM/blob/main/ddim.py">https://github.com/bojone/Keras-DDPM/blob/main/ddim.py</a></strong></p>
</blockquote>
<p>个人的实验结论是：</p>
<blockquote>
<p>1、可能跟直觉相反，生成过程中的$\sigma_t$越小，最终生成图像的噪声和多样性反而相对来说越大；</p>
<p>2、扩散步数$\dim(\boldsymbol{\tau})$越少，生成的图片更加平滑，多样性也会有所降低；</p>
<p>3、结合1、2两点得知，在扩散步数$\dim(\boldsymbol{\tau})$减少时，可以适当缩小$\sigma_t$，以保持生成图片质量大致不变，这跟DDIM原论文的实验结论是一致的；</p>
<p>4、在$\sigma_t$较小时，相比可训练的Embedding层，用固定的Sinusoidal编码来表示$t$所生成图片的噪声要更小；</p>
<p>5、在$\sigma_t$较小时，原论文的U-Net架构（Github中的<a href="https://github.com/bojone/Keras-DDPM/blob/main/ddpm2.py">ddpm2.py</a>）要比笔者自行构思的U-Net架构（Github中的<a href="https://github.com/bojone/Keras-DDPM/blob/main/ddpm.py">ddpm.py</a>）所生成图片的噪声要更小；</p>
<p>6、但个人感觉，总体来说不带噪声的生成过程的生成效果不如带噪声的生成过程，不带噪声时生成效果受模型架构影响较大。</p>
</blockquote>
<p>此外，对于$\sigma_t=0$时的DDIM，它就是将任意正态噪声向量变换为图片的一个确定性变换，这已经跟GAN几乎一致了，所以跟GAN类似，我们可以对噪声向量进行插值，然后观察对应的生成效果。但要注意的是，DDPM或DDIM对噪声分布都比较敏感，所以我们不能用线性插值而要用球面插值，因为由正态分布的叠加性，如果$\boldsymbol{z}_1,\boldsymbol{z}_2\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$，$\lambda\boldsymbol{z}_1 + (1-\lambda)\boldsymbol{z}_2$一般就不服从$\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$，要改为<br />
\begin{equation}\boldsymbol{z} = \boldsymbol{z}_1 \cos\frac{\lambda\pi}{2} + \boldsymbol{z}_2 \sin\frac{\lambda\pi}{2},\quad \lambda\in[0, 1]\end{equation}</p>
<p>插值效果演示（笔者自己训练的模型）：  </p>
<p><a href="/usr/uploads/2022/07/1078989049.jpg" title="点击查看原图"><img alt="DDIM随机向量的插值生成效果" src="/usr/uploads/2022/07/1078989049.jpg" /></a></p>
<p>DDIM随机向量的插值生成效果</p>
<h2 id="_7">微分方程<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>最后，我们来重点分析一下$\sigma_t = 0$的情形。此时$\eqref{eq:sigma=0}$可以等价地改写成：<br />
\begin{equation}\frac{\boldsymbol{x}<em t-1="t-1">t}{\bar{\alpha}_t} - \frac{\boldsymbol{x}</em>}}{\bar{\alpha<em t-1="t-1">{t-1}} = \left(\frac{\bar{\beta}_t}{\bar{\alpha}_t} - \frac{\bar{\beta}</em>}}{\bar{\alpha<em _boldsymbol_theta="\boldsymbol{\theta">{t-1}}\right) \boldsymbol{\epsilon}</em>}}(\boldsymbol{x<em t-1="t-1">t, t)\end{equation}<br />
当$T$足够大，或者说$\alpha_t$与$\alpha</em>$足够小时，我们可以将上式视为某个常微分方程的差分形式。特别地，引入虚拟的时间参数$s$，我们得到<br />
\begin{equation}\frac{d}{ds}\left(\frac{\boldsymbol{x}(s)}{\bar{\alpha}(s)}\right) = \boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\boldsymbol{x}(s), t(s)\right)\frac{d}{ds}\left(\frac{\bar{\beta}(s)}{\bar{\alpha}(s)}\right)\label{eq:ode}\end{equation}<br />
不失一般性，假设$s\in[0,1]$，其中$s=0$对应$t=0$、$s=1$对应$t=T$。注意DDIM原论文直接用$\frac{\bar{\beta}(s)}{\bar{\alpha}(s)}$作为虚拟时间参数，这原则上是不大适合的，因为它的范围是$[0,\infty)$，无界的区间不利于数值求解。</p>
<p>那么现在我们要做的事情就是在给定$\boldsymbol{x}(1)\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})$的情况下，去求解出$\boldsymbol{x}(0)$。而DDPM或者DDIM的迭代过程，对应于该常微分方程的<a href="https://en.wikipedia.org/wiki/Euler_method">欧拉方法</a>。众所周知欧拉法的效率相对来说是最慢的，如果要想加速求解，可以用<a href="https://en.wikipedia.org/wiki/Heun%27s_method">Heun方法</a>、<a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods">R-K方法</a>等。也就是说，将生成过程等同于求解常微分方程后，可以借助常微分方程的数值解法，为生成过程的加速提供更丰富多样的手段。</p>
<p>以DDPM的默认参数$T=1000$、$\alpha_t = \sqrt{1 - \frac{0.02t}{T}}$为例，我们重复<a href="/archives/9119">《生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼》</a>所做的估计<br />
\begin{equation}\log \bar{\alpha}<em i="k">t = \sum</em>}^t \log\alpha_k = \frac{1}{2} \sum_{k=1}^t \log\left(1 - \frac{0.02k}{T}\right) &lt; \frac{1}{2} \sum_{k=1}^t \left(- \frac{0.02k}{T}\right) = -\frac{0.005t(t+1)}{T}\end{equation<br />
事实上，由于每个$\alpha_k$都很接近于1，所以上述估计其实也是一个很好的近似。而我们说了本文的出发点是$p(\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">t|\boldsymbol{x}_0)$，所以应该以$\bar{\alpha}_t$为起点，根据上述近似，我们可以直接简单地取<br />
\begin{equation}\bar{\alpha}_t = \exp\left(-\frac{0.005t^2}{T}\right) = \exp\left(-\frac{5t^2}{T^2}\right)\end{equation}<br />
如果取$s=t/T$为参数，那么正好$s\in[0,1]$，此时$\bar{\alpha}(s)=e^{-5s^2}$，代入到式$\eqref{eq:ode}$化简得<br />
\begin{equation}\frac{d\boldsymbol{x}(s)}{ds} = 10s\left(\frac{\boldsymbol{\epsilon}</em>}}\left(\boldsymbol{x}(s), sT\right)}{\sqrt{1-e^{-10s^2}}} - \boldsymbol{x}(s)\right)\end{equation<br />
也可以取$s=t^2/T^2$为参数，此时也有$s\in[0,1]$，以及$\bar{\alpha}(s)=e^{-5s}$，代入到式$\eqref{eq:ode}$化简得<br />
\begin{equation}\frac{d\boldsymbol{x}(s)}{ds} = 5\left(\frac{\boldsymbol{\epsilon}_{\boldsymbol{\theta}}\left(\boldsymbol{x}(s), \sqrt{s}T\right)}{\sqrt{1-e^{-10s}}} - \boldsymbol{x}(s)\right)\end{equation}</p>
<h2 id="_8">文章小结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>本文接着上一篇DDPM的推导思路来介绍了DDIM，它重新审视了DDPM的出发点，去掉了推导过程中的$p(\boldsymbol{x}<em t-1="t-1">t|\boldsymbol{x}</em>)$，从而获得了一簇更广泛的解和加速生成过程的思路，最后这簇新解还允许我们将生成过程跟常微分方程的求解联系起来，从而借助常微分方程的方法进一步对生成过程进行研究。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/9181">https://spaces.ac.cn/archives/9181</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jul. 27, 2022). 《生成扩散模型漫谈（四）：DDIM = 高观点DDPM 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/9181">https://spaces.ac.cn/archives/9181</a></p>
<p>@online{kexuefm-9181,<br />
title={生成扩散模型漫谈（四）：DDIM = 高观点DDPM},<br />
author={苏剑林},<br />
year={2022},<br />
month={Jul},<br />
url={\url{https://spaces.ac.cn/archives/9181}},<br />
} </p>
<hr />
<h2 id="_9">公式推导与注释<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="ladder-side-tuning预训练模型的过墙梯.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#221 Ladder Side-Tuning：预训练模型的“过墙梯”</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="个性邮箱.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#223 个性邮箱</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#ddim-ddpm">生成扩散模型漫谈（四）：DDIM = 高观点DDPM</a><ul>
<li><a href="#_1">思路分析</a></li>
<li><a href="#_2">待定系数</a></li>
<li><a href="#_3">一如既往</a></li>
<li><a href="#_4">几个例子</a></li>
<li><a href="#_5">加速生成</a></li>
<li><a href="#_6">实验结果</a></li>
<li><a href="#_7">微分方程</a></li>
<li><a href="#_8">文章小结</a></li>
<li><a href="#_9">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>