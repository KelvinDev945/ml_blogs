<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>让人惊叹的Johnson-Lindenstrauss引理：应用篇 | ML & Math Blog Posts</title>
    <meta name="description" content="让人惊叹的Johnson-Lindenstrauss引理：应用篇&para;
原文链接: https://spaces.ac.cn/archives/8706
发布日期: 

上一篇文章《让人惊叹的Johnson-Lindenstrauss引理：理论篇》中，我们比较详细地介绍了Johnson-Lindenstrauss引理（JL引理）的理论推导，这一篇我们来关注它的应用。
作为一个内容上本身就跟降...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering with equation numbering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true,
        tags: 'ams',  // Enable equation numbering with AMS style
        tagSide: 'right',  // Place equation numbers on the right
        tagIndent: '0.8em',  // Indentation for equation numbers
        multlineWidth: '85%'
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=模型">模型</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #85 让人惊叹的Johnson-Lindenstrauss引理：应用篇
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#85</span>
                让人惊叹的Johnson-Lindenstrauss引理：应用篇
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> </span>
                
                <span class="ms-3">
                    <i class="fas fa-link"></i>
                    <a href="https://spaces.ac.cn/archives/8706" target="_blank" rel="noopener">原文链接</a>
                </span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 模型</span>
                </a>
                
                <a href="../index.html?tags=分析" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 分析</span>
                </a>
                
                <a href="../index.html?tags=维度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 维度</span>
                </a>
                
                <a href="../index.html?tags=机器学习" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 机器学习</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="johnson-lindenstrauss">让人惊叹的Johnson-Lindenstrauss引理：应用篇<a class="toc-link" href="#johnson-lindenstrauss" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/8706">https://spaces.ac.cn/archives/8706</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>上一篇文章<a href="/archives/8679">《让人惊叹的Johnson-Lindenstrauss引理：理论篇》</a>中，我们比较详细地介绍了Johnson-Lindenstrauss引理（JL引理）的理论推导，这一篇我们来关注它的应用。</p>
<p>作为一个内容上本身就跟降维相关的结论，JL引理最基本的自然就是作为一个降维方法来用。但除了这个直接应用外，很多看似不相关的算法，比如局部敏感哈希（LSH）、随机SVD等，本质上也依赖于JL引理。此外，对于机器学习模型来说，JL引理通常还能为我们的维度选择提供一些理论解释。</p>
<h2 id="_1">降维的工具<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>JL引理提供了一个非常简单直接的“随机投影”降维思路：</p>
<blockquote>
<p>给定$N$个向量$v_1,v_2,\cdots,v_N\in\mathbb{R}^m$，如果想要将它降到$n$维，那么只需要从$\mathcal{N}(0,1/n)$中采样一个$n\times m$矩阵$A$，然后$Av_1,Av_2,\cdots,Av_N$就是降维后的结果。</p>
</blockquote>
<p>这个思路简单快速是毋庸置疑的，读者随之而来的疑问就是：它跟PCA、t-SNE等降维方法相比效果如何？</p>
<p>其实，正如“存在就是合理的”，更复杂的PCA、t-SNE等方法既然还没有被淘汰，那就说明它肯定有比随机投影更好的地方。事实上，JL引理的随机投影只是提供了一种非常基本的降维方法，显示出哪怕在这么简单的方法之后，降维后的维度也只需要$\mathcal{O}(\log N)$，它更多的是一个理论证明。</p>
<p>所以，真要追求降维精度的话，多数情况下PCA、t-SNE等这些专门的降维方法，效果肯定是要比随机投影要好的。而且上一篇文章中我们也提过，JL引理是一个非常充分的条件，它得到的$n &gt; \frac{24\log N}{\varepsilon^2}$甚至$n &gt; \frac{16\log N}{\varepsilon^2}$都只是非常充分的界，比如取$\varepsilon=0.1$的话，就有$n &gt; 1600\log N$了，基本没有实用价值。而换用PCA、t-SNE等更精准的降维方法，可以放宽这个要求，即在更小的维度下达到更好的效果。</p>
<h2 id="_2">局部的哈希<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>局部敏感哈希（Locality-Sensitive Hashing，LSH），是近似查找某种度量下的最邻近元素的一种方案。通常来说，我们很少将LSH与JL引理联系起来，但笔者认为，LSH的哈希函数选择上，其实跟JL引理也是紧密相关的。简单来说，LSH就是一个将向量二值化的算法，并且二值化之后的向量能近似保持度量不变。常见的一种方案是通过随机投影来（近似）保持cos值的不变性。</p>
<p>具体来说，根据JL引理，我们从$\mathcal{N}(0,1/n)$中采样一个$n\times m$矩阵$A$，那么对于任意$v_i,v_j\in\mathbb{R}^m$，都有$\cos(v_i,v_j)\approx \cos(Av_i, Av_j)$。当然，随机投影还不是LSH的全部，我们留意到，经过$A$的投影后，$Av_i,Av_j$的正负分布情况是比较均匀的，所以我们进一步做近似<br />
\begin{equation}\cos(v_i,v_j)\approx \cos(Av_i, Av_j)\approx \cos(\text{sign}(Av_i), \text{sign}(Av_j))\end{equation}<br />
即每个元素我们根据正负号二值化为$\pm 1$，这就实现了向量的二值化，并且保持了余弦值近似不变。有了二值化向量后，我们可以建索引、分通等，以加快检索速度，这些就不细说了。</p>
<p>总之，在LSH过程中，关键的一步也是随机投影，这一步本身与JL引理也是紧密相关的。当然，二值化通常会比较明显地牺牲精度，所以根据实际场景的不同，我们并不总是“降维”，即$n$并不会总是小于$m$，有时候我们可能还会选择$n &gt; m$。相关的讨论读者可以参考笔者之前写的<a href="/archives/8159">《一个二值化词向量模型，是怎么跟果蝇搭上关系的？》</a>。</p>
<h2 id="_3">随机的分解<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>矩阵分解是解决许多机器学习问题的强大工具，而奇异值分解（SVD）则是其中的典型方法之一。然而，当矩阵比较大的时候，计算精确的SVD分解成本相当大，而实际场景中，待分解矩阵虽然大，但往往也是低秩的，计算精确的SVD分解也没有必要。这时候，“随机SVD分解”便派上用场了。</p>
<p>设待分解矩阵为$M\in\mathbb{R}^{m\times n}$，$m,n$都比较大。根据JL引理，我们可以选择比较小的$k &lt; \min(m,n)$，使得从$\mathcal{N}(0,1/k)$中采样出来$n\times k$矩阵$Q$依然能比较高精度地满足$QQ^{\top}\approx I$（近似正交矩阵），从而$M\approx MQQ^{\top}$。这样，我们可以只对$m\times k$矩阵$B=MQ$做SVD分解，得到$MQ=B=U_B\Sigma_B V_B^{\top}$，那么<br />
\begin{equation}M\approx MQQ^{\top} = U_B\Sigma_B V_B^{\top}Q^{\top} = U_B \Sigma_B (QV_B)^{\top}\end{equation}<br />
就得到了原始矩阵$M$的一个近似SVD分解。注意，上述$Q$还只是近似正交矩阵，我们可以通过QR分解（或施密特正交化）使得它变成严格正交，这是一个小细节。在整个过程中，JL引理所告诉我们的是$k$可以选得比较小，以至于对$B=MQ$做SVD是比较低成本的，但总体精度也不会太差。</p>
<h2 id="_4">词向量维度<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>我们说JL引理的通俗理解是“塞下$N$个向量只需要$\mathcal{O}(\log N)$维空间”，那么回到词向量维度选择问题上，也就是说如果词表大小为$N$，那么词向量维度是$\mathcal{O}(\log N)$就够了。</p>
<p>非常让人惊震的是，在笔者之前的文章<a href="/archives/7695">《最小熵原理（六）：词向量的维度应该怎么选择？》</a>中，曾计算出了一个Skip Gram词向量模型的维度选择公式：<br />
\begin{equation}n &gt; 8.33\log N\end{equation}<br />
其结果与JL引理所给出的$\mathcal{O}(\log N)$如出一辙！上述公式是基于熵的思想进行估计的，与JL引理的出发点几乎没有交集之处，但竟然殊途同归地得到了$\log N$。</p>
<p>而且，不仅仅是主体$\log N$，我们还看到，基于熵的估计，我们还把$\log N$前面的系数$8.33$也计算出来了，并且以往的实验经验还显示，$8.33\log N$这个结果还是挺符合经验的，虽然未必是最优，但至少范围上差不远。这是不是可以反过来说，我们可以通过熵来比较精确地估计具体问题下$\log N$前面的系数？</p>
<h2 id="_5">多头注意力<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>关于Attention机制，常见的面试题就是“为什么要多头？”、“head_size为768的单头注意力，跟head_size为64的12头注意力有什么区别？”等，也就是说，像BERT这样的Attention模型，为什么要先把head_size降低到64再做内积？64真的够了吗？</p>
<p>这个问题本质上来说Attention机制是否足以拟合任何概率模式的问题。具体来说，Attention的计算公式为：<br />
\begin{equation}a_{i,j} = \frac{e^{\langle q_i, k_j\rangle}}{\sum\limits_{j=1}^L e^{\langle q_i, k_j\rangle}}\end{equation}<br />
其中$q_i,k_j\in\mathbb{R}^{d}$，所谓“够不够”，就是指对于任意给定的概率矩阵$p_{i,j}$，上述定义的$a_{i,j}$是否都能很好地逼近它？</p>
<p>看到$a_{i,j}$的定义，不知道有没有读者觉得熟悉的？如果我们抛开Attention的背景，将$q_i,k_j$分别视为两个“词向量”，那么$a_{i,j}$的定义跟Skip Gram模型一模一样！也就是说，单纯看Attention矩阵的计算公式，它跟Skip Gram模型本质上是一样的，所以Attention的head_size选择，本质上也就是词向量的维度选择。</p>
<p>让我们再来捋一捋过程。我们要回答的是“head_size多少才够”的问题，这变成了“$a_{i,j}$能否逼近任意概率矩阵$p_{i,j}$”的问题，也就是说，对于给定$p_{i,j}$，我们是否能找到一组$q_1,\cdots,q_L,k_1,\cdots,k_L\in\mathbb{R}^d$，使得$a_{i,j}$与$p_{i,j}$足够近似，这个问题跟Skip Gram词向量模型的维度选择是数学等价的。</p>
<p>因此，词向量维度选择的结果，也就可以用于Attention的head_size选择，只不过词表大小变成了序列长度，即$d &gt; 8.33\log L$，常见的预训练长度是$L=512$，代入计算约等于52，同样非常让人震惊，跟常见的head_size=64确实相差无几！所以，64真的够了，再大也不会有明显提升，倒不如将多出来的计算量用来增加head的数目～</p>
<p>（注：相关讨论还可以参考文献<a href="https://papers.cool/arxiv/2106.03764">《On the Expressive Power of Self-Attention Matrices》</a>。）</p>
<h2 id="_6">又到了小结<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>本文主要介绍了Johnson-Lindenstrauss引理（JL引理）的几个直接或间接的应用，可以看到，从降维、哈希的方法，到词向量维度、Attention的头大小等，多多少少都与JL引理有所关联，这进一步显示了JL引理的适用范围之广。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/8706">https://spaces.ac.cn/archives/8706</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 24, 2021). 《让人惊叹的Johnson-Lindenstrauss引理：应用篇 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/8706">https://spaces.ac.cn/archives/8706</a></p>
<p>@online{kexuefm-8706,<br />
title={让人惊叹的Johnson-Lindenstrauss引理：应用篇},<br />
author={苏剑林},<br />
year={2021},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/8706}},<br />
} </p>
<hr />
<h2 id="_7">公式推导与注释<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="transformer升级之路3从performer到线性attention.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#84 Transformer升级之路：3、从Performer到线性Attention</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="univae基于transformer的单模型多尺度的vae模型.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#86 UniVAE：基于Transformer的单模型、多尺度的VAE模型</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#johnson-lindenstrauss">让人惊叹的Johnson-Lindenstrauss引理：应用篇</a><ul>
<li><a href="#_1">降维的工具</a></li>
<li><a href="#_2">局部的哈希</a></li>
<li><a href="#_3">随机的分解</a></li>
<li><a href="#_4">词向量维度</a></li>
<li><a href="#_5">多头注意力</a></li>
<li><a href="#_6">又到了小结</a></li>
<li><a href="#_7">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>