<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>通向最优分布之路：概率空间的最小化 | ML & Math Blog Posts</title>
    <meta name="description" content="通向最优分布之路：概率空间的最小化&para;
原文链接: https://spaces.ac.cn/archives/10289
发布日期: 

当要求函数的最小值时，我们通常会先求导函数然后寻找其零点，比较幸运的情况下，这些零点之一正好是原函数的最小值点。如果是向量函数，则将导数改为梯度并求其零点。当梯度零点不易求得时，我们可以使用梯度下降来逐渐逼近最小值点。
以上这些都是无约束优化的基础结果...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=详细推导">详细推导</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #153 通向最优分布之路：概率空间的最小化
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#153</span>
                通向最优分布之路：概率空间的最小化
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2024-08-06</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=详细推导" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 详细推导</span>
                </a>
                
                <a href="../index.html?tags=概率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 概率</span>
                </a>
                
                <a href="../index.html?tags=优化" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=扩散" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 扩散</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="_1">通向最优分布之路：概率空间的最小化<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/10289">https://spaces.ac.cn/archives/10289</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>当要求函数的最小值时，我们通常会先求导函数然后寻找其零点，比较幸运的情况下，这些零点之一正好是原函数的最小值点。如果是向量函数，则将导数改为梯度并求其零点。当梯度零点不易求得时，我们可以使用梯度下降来逐渐逼近最小值点。</p>
<p>以上这些都是无约束优化的基础结果，相信不少读者都有所了解。然而，本文的主题是概率空间中的优化，即目标函数的输入是一个概率分布，这类目标的优化更为复杂，因为它的搜索空间不再是无约束的，如果我们依旧去求解梯度零点或者执行梯度下降，所得结果未必能保证是一个概率分布。因此，我们需要寻找一种新的分析和计算方法，以确保优化结果能够符合概率分布的特性。</p>
<p>对此，笔者一直以来也感到颇为头疼，所以近来决定”痛定思痛“，针对概率分布的优化问题系统学习了一番，最后将学习所得整理在此，供大家参考。</p>
<h2 id="_2">梯度下降<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>我们先来重温一下无约束优化的相关内容。首先，假设我们的目标是<br />
\begin{equation}\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">* = \mathop{\text{argmin}}</em>}\in\mathbb{R}^n} F(\boldsymbol{x})\end{equation<br />
高中生都知道，要求函数的最值，往往先求导在让它等于零来找极值点，这对很多人来说已经成为“常识”。但这里不妨考考各位读者，有多少人能证明这个结论？换句话说，函数的最值为什么会跟“导数等于零”扯上关系呢？</p>
<h3 id="_3">搜索视角<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h3>
<p>我们可以从搜索的视角来探究这个问题。假设我们当前所知的$\boldsymbol{x}$为$\boldsymbol{x}<em t_eta="t+\eta">t$，我们怎么判断$\boldsymbol{x}_t$是不是最小值点呢？这个问题可以反过来思考：如果我们能找到$\boldsymbol{x}</em>}$，使得$F(\boldsymbol{x<em t_eta="t+\eta">{t+\eta}) &lt; F(\boldsymbol{x}_t)$，那么$\boldsymbol{x}_t$自然就不可能是最小值点了。为此，我们可以搜索如下格式的$\boldsymbol{x}</em>$：<br />
\begin{equation}\boldsymbol{x}<em t_eta="t+\eta">{t+\eta} = \boldsymbol{x}_t + \eta \boldsymbol{u}_t,\quad 0 &lt; \eta \ll 1\end{equation}<br />
当$F(\boldsymbol{x})$足够光滑、$\eta$足够小时，我们认为一阶近似的精度是够用的，于是可以利用一阶近似：<br />
\begin{equation}F(\boldsymbol{x}</em>}) = F(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t + \eta \boldsymbol{u}_t) \approx F(\boldsymbol{x}_t) + \eta \boldsymbol{u}_t \cdot \nabla</em><em _boldsymbol_x="\boldsymbol{x">t}F(\boldsymbol{x}_t)\end{equation}<br />
只要$\nabla</em><em _boldsymbol_x="\boldsymbol{x">t}F(\boldsymbol{x}_t)\neq 0$，我们我们就可以选取$\boldsymbol{u}_t = -\nabla</em><em t_eta="t+\eta">t}F(\boldsymbol{x}_t)$，使得<br />
\begin{equation}F(\boldsymbol{x}</em>}) \approx F(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t) - \eta \Vert\nabla</em><em _boldsymbol_x="\boldsymbol{x">t}F(\boldsymbol{x}_t)\Vert^2 &lt; F(\boldsymbol{x}_t)\end{equation}<br />
这意味着，对于足够光滑的函数，它的最小值只能在满足$\nabla</em><em _boldsymbol_x="\boldsymbol{x">t}F(\boldsymbol{x}_t) = 0$的点或者无穷远处取到，这也就是为什么求最值的第一步通常是“导数等于零”。如果$\nabla</em><em t_eta="t+\eta">t}F(\boldsymbol{x}_t) \neq 0$，我们总可选择足够小的$\eta$，通过<br />
\begin{equation}\boldsymbol{x}</em>} = \boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t-\eta\nabla</em><em _boldsymbol_x="\boldsymbol{x">t}F(\boldsymbol{x}_t)\label{eq:gd}\end{equation}<br />
来得到让$f$更小的点，这便是梯度下降。如果让$\eta\to 0$，我们可以得到ODE：<br />
\begin{equation}\frac{d\boldsymbol{x}_t}{dt} = -\nabla</em>}_t}F(\boldsymbol{x}_t)\end{equation<br />
这就是<a href="/archives/9660">《梯度流：探索通向最小值之路》</a>介绍过的“梯度流”，它可以视为我们用梯度下降搜索最小值点的轨迹。</p>
<h3 id="_4">投影下降<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h3>
<p>刚才我们说的都是无约束优化，现在我们来简单讨论梯度下降在约束优化中的一个简单推广。假设我们面临的问题是：<br />
\begin{equation}\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">* = \mathop{\text{argmin}}</em>}\in\mathbb{X}} F(\boldsymbol{x})\label{eq:c-loss}\end{equation<br />
其中$\mathbb{X}$是$\mathbb{R}^n$的一个子集，如果是理论分析，通常要给$\mathbb{X}$加上“有界凸集”的要求，但如果是简单了解的话，我们就可以先不管这些细节了。</p>
<p>如果此时我们仍用梯度下降$\eqref{eq:gd}$，那么最大的问题就是无法保证$\boldsymbol{x}<em _mathbb_X="\mathbb{X">{t+\eta}\in\mathbb{X}$，但其实我们可以多加一步投影运算<br />
\begin{equation}\Pi</em>}} (\boldsymbol{y}) = \mathop{\text{argmin}<em t_eta="t+\eta">{\boldsymbol{x}\in\mathbb{X}}\Vert\boldsymbol{x}-\boldsymbol{y}\Vert\label{eq:project}\end{equation}<br />
从而形成“投影梯度下降”：<br />
\begin{equation}\boldsymbol{x}</em>} = \Pi_{\mathbb{X}}(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t-\eta\nabla</em>}_t}F(\boldsymbol{x}_t))\label{eq:pgd}\end{equation<br />
说白了，投影梯度下降就是先梯度下降，然后在$\mathbb{X}$中找到跟梯度下降结果最相近的点作为输出，这样就保证了输出结果一定在$\mathbb{X}$内。在<a href="/archives/9902#域内投影">《让炼丹更科学一些（一）：SGD的平均损失收敛》</a>中我们证明了，在一定假设下，投影梯度下降可以找到约束优化问题$\eqref{eq:c-loss}$的最优解。</p>
<p>从结果来看，投影梯度下降将约束优化$\eqref{eq:c-loss}$转化为了“梯度下降+投影”两步，而投影$\eqref{eq:project}$本身也是一个约束优化问题，虽然优化目标已经固定了，但仍属于未解决的问题，需要具体$\mathbb{X}$具体分析，因此还需要进一步探索下去。</p>
<h2 id="_5">离散分布<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>本文聚焦于概率空间中的优化，即搜索对象必须是一个概率分布，这一节我们先关注离散分布，搜索空间我们记为$\Delta^{n-1}$，它是全体$n$元离散型概率分布的集合，即<br />
\begin{equation}\Delta^{n-1} = \left\{\boldsymbol{p}=(p_1,p_2,\cdots,p_n)\left|\, p_1,p_2,\cdots,p_n\geq 0,\sum_{i=1}^n p_i = 1\right.\right\}\end{equation}<br />
我们的优化目标则是<br />
\begin{equation}\boldsymbol{p}<em _boldsymbol_p="\boldsymbol{p">* = \mathop{\text{argmin}}</em>}\in\Delta^{n-1}} F(\boldsymbol{p})\label{eq:p-loss}\end{equation</p>
<h3 id="_6">拉氏乘子<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h3>
<p>对于等式或不等式约束下的优化问题，标准方法通常是“<a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">拉格朗日乘子法</a>”，它将约束优化问题$\eqref{eq:p-loss}$转化为一个弱约束的$\text{min-max}$问题：<br />
\begin{equation}\min_{\boldsymbol{p}\in\Delta^{n-1}} F(\boldsymbol{p}) = \min_{\boldsymbol{p}\in\mathbb{R}^n} \max_{\mu_i \geq 0,\lambda\in\mathbb{R}}F(\boldsymbol{p}) - \sum_{i=1}^n \mu_i p_i + \lambda\left(\sum_{i=1}^n p_i - 1\right)\label{eq:min-max}\end{equation}<br />
注意在这个$\text{min-max}$优化中，我们去掉了$\boldsymbol{p}\in\Delta^{n-1}$这个约束，只是在$\max$这一步有一个比较简单的$\mu_i \geq 0$的约束。怎么证明右边的优化问题等价于左边呢？其实并不难，分三步来理解：</p>
<blockquote>
<p>1、我们先要理解右端的$\text{min-max}$含义：$\min$在左，$\max$在右，这意味着我们最终是要求一个尽可能小的结果，但这个目标函数是先要对某些变量取$\max$；</p>
<p>2、当$p_i &lt; 0$是，那么$\max$这一步必然有$\mu_i\to\infty$，此时结果目标函数值是$\infty$，而如果$p_i \geq 0$，那么$\max$这一步就必然有$\mu_i p_i =0$，此时目标函数值是有限的，显然后者更小一点，因此当右端取最优值时$p_i\geq 0$成立，同理我们也可以证明$\sum_{i=1}^n p_i = 1$成立；</p>
<p>3、由第2步的分析可知，当右端取最优值时，必然满足$\boldsymbol{p}\in\Delta^{n-1}$，且多出来的项为零，那么就等价于左边的优化问题。</p>
</blockquote>
<p>接下来要用到一个“<a href="https://en.wikipedia.org/wiki/Minimax_theorem">Minimax定理</a>”：</p>
<blockquote>
<p>如果$\mathbb{X},\mathbb{Y}$是两个凸集，$\boldsymbol{x}\in\mathbb{X},\boldsymbol{y}\in\mathbb{Y}$，并且$f(\boldsymbol{x},\boldsymbol{y})$关于$\boldsymbol{x}$是凸函数的（对于任意固定$\boldsymbol{y}$），关于$\boldsymbol{y}$是凹函数的（对于任意固定$\boldsymbol{x}$），那么成立 \begin{equation}\min_{\boldsymbol{x}\in\mathbb{X}}\max_{\boldsymbol{y}\in\mathbb{Y}} f(\boldsymbol{x},\boldsymbol{y}) = \max_{\boldsymbol{y}\in\mathbb{Y}}\min_{\boldsymbol{x}\in\mathbb{X}} f(\boldsymbol{x},\boldsymbol{y})\end{equation}</p>
</blockquote>
<p>Minimax定理提供了$\min,\max$可交换的一个充分条件，这里边出现了一个新名词“凸集”，指的是集合内任意两点的加权平均，结果依然在集合内，即<br />
\begin{equation}(1-\lambda)\boldsymbol{x}_1 + \lambda \boldsymbol{x}_2\in \mathbb{X},\qquad\forall \boldsymbol{x}_1,\boldsymbol{x}_2\in \mathbb{X},\quad\forall \lambda\in [0, 1]\end{equation}<br />
由此可见凸集的条件并不是太苛刻，$\mathbb{R}^n,\Delta^{n-1}$都是凸集，还有全体非负数也是凸集，等等。</p>
<p>对于式$\eqref{eq:min-max}$右端的目标函数，它关于$\mu_i,\lambda$是一次函数，因此符合关于$\mu_i,\lambda$是凹函数的条件，并且除开$F(\boldsymbol{p})$外的项关于$\boldsymbol{p}$也是一次的，所以整个目标函数关于$\boldsymbol{p}$的凸性，等价于$F(\boldsymbol{p})$关于$\boldsymbol{p}$的凸性，即如果$F(\boldsymbol{p})$是关于$\boldsymbol{p}$的凸函数，那么式$\eqref{eq:min-max}$的$\min,\max$就可以交换：<br />
\begin{equation}\small\min_{\boldsymbol{p}\in\mathbb{R}^n} \max_{\mu_i \geq 0,\lambda\in\mathbb{R}}F(\boldsymbol{p}) - \sum_{i=1}^n \mu_i p_i + \lambda\left(\sum_{i=1}^n p_i - 1\right) = \max_{\mu_i \geq 0,\lambda\in\mathbb{R}} \min_{\boldsymbol{p}\in\mathbb{R}^n}<br />
F(\boldsymbol{p}) - \sum_{i=1}^n \mu_i p_i + \lambda\left(\sum_{i=1}^n p_i - 1\right)\end{equation}<br />
这样我们就可以先对$\boldsymbol{p}$求$\min$了，这是一个无约束最小化问题，可以通过求解梯度等于零的方程组来完成，结果将带有参数$\lambda$和$\mu_i$，最后通过$p_i \geq 0$、$\mu_i p_i = 0$和$\sum_{i=1}^n p_i = 1$来确定这些参数。</p>
<h3 id="_7">凸集搜索<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<p>然而，尽管拉格朗日乘子法被视为求解约束优化问题的标准方法，但它并不算直观，而且它只能通过解方程来求得精确解，并不能导出类似梯度下降的迭代逼近算法，因此我们不能满足于拉格朗日乘子法。</p>
<p>从搜索的视角看，求解概率空间中的优化问题的关键，是保证在搜索过程中试探点都在集合$\Delta^{n-1}$内。换句话说，假设当前概率分布为$\boldsymbol{p}<em t_eta="t+\eta">t\in \Delta^{n-1}$，我们怎么构造下一个试探点$\boldsymbol{p}</em>}$呢？它有两个要求，一是$\boldsymbol{p<em t_eta="t+\eta">{t+\eta}\in \Delta^{n-1}$，二是可以通过控制$\eta$的大小来控制它跟$\boldsymbol{p}_t$的接近程度。这时候$\Delta^{n-1}$的“凸集”性质就派上用场了，利用这一性质我们可以将$\boldsymbol{p}</em>$定为<br />
\begin{equation}\boldsymbol{p}<em t_eta="t+\eta">{t+\eta} = (1-\eta)\boldsymbol{p}_t + \eta \boldsymbol{q}_t,\quad \boldsymbol{q}_t\in \Delta^{n-1}\end{equation}<br />
那么有<br />
\begin{equation}F(\boldsymbol{p}</em>}) = F((1-\eta)\boldsymbol{p<em _boldsymbol_p="\boldsymbol{p">t + \eta \boldsymbol{q}_t) \approx F(\boldsymbol{p}_t) + \eta(\boldsymbol{q}_t - \boldsymbol{p}_t)\cdot\nabla</em><em _boldsymbol_q="\boldsymbol{q">t} F(\boldsymbol{p}_t)\end{equation}<br />
假设一阶近似的精度足够，那么要获得下降最快的方向，就相当于求解<br />
\begin{equation}\mathop{\text{argmin}}</em><em _boldsymbol_p="\boldsymbol{p">t\in\Delta^{n-1}}\,\boldsymbol{q}_t\cdot\nabla</em><em _boldsymbol_p="\boldsymbol{p">t} F(\boldsymbol{p}_t) \end{equation}<br />
这个目标函数倒是很简单，答案是<br />
\begin{equation}\boldsymbol{q}_t = \text{onehot}(\text{argmin}(\nabla</em><em _boldsymbol_p="\boldsymbol{p">t} F(\boldsymbol{p}_t)))\end{equation}<br />
这里$\nabla</em><em _boldsymbol_p="\boldsymbol{p">t} F(\boldsymbol{p}_t)$是一个向量，对一个向量做$\text{argmin}$指的是找出最小分量的位置。所以，上式也就是说$\boldsymbol{q}_t$是一个one hot分布，其中$1$所在的位置，就是梯度$\nabla</em>_t)$最小的分量所在的位置。}_t} F(\boldsymbol{p</p>
<p>由此可见，概率空间的梯度下降形式是<br />
\begin{equation}\boldsymbol{p}<em _boldsymbol_p="\boldsymbol{p">{t+\eta} = (1 - \eta)\boldsymbol{p}_t + \eta\, \text{onehot}(\text{argmin}(\nabla</em><em _boldsymbol_p="\boldsymbol{p">t} F(\boldsymbol{p}_t)))\end{equation}<br />
以及$\boldsymbol{p}_t$是$F(\boldsymbol{p}_t)$极小值点的条件是：<br />
\begin{equation}\boldsymbol{p}_t\cdot\nabla</em><em _boldsymbol_p="\boldsymbol{p">t} F(\boldsymbol{p}_t) = (\nabla</em><em _min="\min">t} F(\boldsymbol{p}_t))</em>}\label{eq:p-min}\end{equation<br />
这里对向量的$\min$是指返回最小的那个分量。</p>
<h3 id="_8">一个例子<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<p>以<a href="/archives/10145">《通向概率分布之路：盘点Softmax及其替代品》</a>介绍的Sparsemax为例，它的原始定义为<br />
\begin{equation}Sparsemax(\boldsymbol{x}) = \mathop{\text{argmin}}\limits_{\boldsymbol{p}\in\Delta^{n-1}}\Vert \boldsymbol{p} - \boldsymbol{x}\Vert^2\end{equation}<br />
其中$\boldsymbol{x}\in\mathbb{R}^n$。不难发现，从前面投影梯度下降的角度来看，Sparsemax正好是从$\mathbb{R}^n$到$\Delta^{n-1}$的“投影”操作。</p>
<p>我们记$F(\boldsymbol{p})=\Vert \boldsymbol{p} - \boldsymbol{x}\Vert^2$，它对$\boldsymbol{p}$的梯度是$2(\boldsymbol{p} - \boldsymbol{x})$，所以根据式$\eqref{eq:p-min}$，极小值点满足的方程就是<br />
\begin{equation}\boldsymbol{p}\cdot(\boldsymbol{p}-\boldsymbol{x}) = (\boldsymbol{p}-\boldsymbol{x})_{\min}\end{equation}<br />
我们约定$x_i = x_j\Leftrightarrow p_i = p_j$，这里没有加粗的下标如$p_i$表示向量$\boldsymbol{p}$的第$i$个分量（即是一个标量），前一节加粗的下标如$\boldsymbol{p}_t$表示$\boldsymbol{p}$的第$t$次迭代结果（即还是一个向量），请读者细心区分。</p>
<p>在该约定下，由上式可以得到<br />
\begin{equation}p_i &gt; 0 \quad \Leftrightarrow \quad p_i-x_i = (\boldsymbol{p}-\boldsymbol{x})<em _min="\min">{\min}\end{equation}<br />
因为$\boldsymbol{p}$可以由$\boldsymbol{x}$确定，所以$(\boldsymbol{p}-\boldsymbol{x})</em>) &lt; 0$。基于这两点，我们可以统一记}$是$\boldsymbol{x}$的函数，我们记为$-\lambda(\boldsymbol{x})$，那么$p_i = x_i - \lambda(\boldsymbol{x})$，但这只是对于$p_i &gt; 0$成立，对于$p_i=0$，我们有$p_i-x_i &gt; (\boldsymbol{p}-\boldsymbol{x})_{\min}$，即$x_i - \lambda(\boldsymbol{x<br />
\begin{equation}p_i = \text{relu}(x_i - \lambda(\boldsymbol{x}))\end{equation}<br />
其中$\lambda(\boldsymbol{x})$由$\boldsymbol{p}$的各分量之和为1来确定，其他细节内容请参考<a href="/archives/10145">《通向概率分布之路：盘点Softmax及其替代品》</a>。</p>
<h2 id="_9">连续分布<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>说完离散分布，接下来我们就转到连续分布了。看上去连续型分布只是离散型分布的极限版本，结果似乎不应该有太大差别，但事实上它们之间的特性有着本质不同，以至于我们需要为连续分布构建全新的方法论。</p>
<h3 id="_10">目标泛函<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<p>首先我们来说说目标函数。我们知道，描述连续型分布的方式是概率密度函数，所以此时目标函数的输入是一个概率密度函数，而此时的目标函数其实也不是普通的函数了，我们通常称之为“泛函”——从一整个函数到一个标量的映射。换言之，我们需要寻找一个概率密度函数，使得某个目标泛函最小化。</p>
<p>尽管很多人觉得“泛函分析心犯寒”，但实际上我们大部份人都接触过泛函，因为满足“输入函数，输出标量”的映射太多了，比如定积分<br />
\begin{equation}\mathcal{I}[f]\triangleq \int_a^b f(x) dx\end{equation}<br />
就是一个函数到标量的映射，所以它也是泛函。事实上，我们在实际应用中会遇到的泛函，基本上都是由定积分构建出来的，比如概率分布的KL散度：<br />
\begin{equation}\mathcal{KL}[p\Vert q] = \int p(\boldsymbol{x})\log \frac{p(\boldsymbol{x})}{q(\boldsymbol{x})}d\boldsymbol{x}\end{equation}<br />
其中积分默认在全空间（整个$\mathbb{R}^n$）进行。更一般的泛函的被积函数里边可能还包含导数项，如理论物理中的最小作用量：<br />
\begin{equation}\mathcal{A}[x] = \int_{t_a}^{t_b} L(x(t),x'(t),t)dt\end{equation}</p>
<p>而接下来我们要最小化的目标泛函，则可以一般地写成<br />
\begin{equation}\mathcal{F}[p] = \int F(p(\boldsymbol{x}))d\boldsymbol{x}\end{equation}<br />
方便起见，我们还可以定义泛函导数<br />
\begin{equation}\frac{\delta\mathcal{F}[p]}{\delta p}(\boldsymbol{x}) = \frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\end{equation}</p>
<h3 id="_11">紧支撑集<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<p>此外，我们还需要一个连续型概率空间的记号，它的基本定义是<br />
\begin{equation}\mathbb{P} = \left\{p(\boldsymbol{x}) \,\Bigg|\, p(\boldsymbol{x})\geq 0(\forall\boldsymbol{x}\in\mathbb{R}^n),\int p(\boldsymbol{x})d\boldsymbol{x} = 1\right\}\end{equation}<br />
不难证明，如果概率密度函数$p(\boldsymbol{x})$的极限$\lim_{\Vert\boldsymbol{x}\Vert\to\infty} p(\boldsymbol{x})$存在，那么必然有$\lim_{\Vert\boldsymbol{x}\Vert\to\infty} p(\boldsymbol{x}) = 0$，这也是后面的证明中要用到的一个性质。</p>
<p>然而，可以举例证明的是，并非所有概率密度函数在无穷远处都存在极限。为了避免理论上的困难，我们通常在理论证明时假设$p(\boldsymbol{x})$的支撑集是紧集。这里边又有两个概念：<strong>支撑集（Support）</strong> 和<strong>紧集（Compact Set）</strong> ， <em>支撑集</em> 指的是让$p(\boldsymbol{x}) &gt; 0$的全体$\boldsymbol{x}$的集合，即<br />
\begin{equation}\text{supp}(p) = \{\boldsymbol{x} | p(\boldsymbol{x}) &gt; 0\}\end{equation}<br />
<em>紧集</em> 的一般定义比较复杂，不过在$\mathbb{R}^n$中，紧集等价于有界闭集。所以说白了，$p(\boldsymbol{x})$的支撑集是紧集的假设，直接作用是让$p(\boldsymbol{x})$具有“存在常数$C$，使得$\forall |\boldsymbol{x}| &gt; C$都有$p(\boldsymbol{x}) = 0$”的性质，简化了$p(\boldsymbol{x})$在无穷远处的性态，从根本上避免了$\lim_{\Vert\boldsymbol{x}\Vert\to\infty} p(\boldsymbol{x}) = 0$的讨论。</p>
<p>从理论上来看，这个假设是非常强的，它甚至排除了像正态分布这样的简单分布（正态分布的支撑集是$\mathbb{R}^n$）。不过，从实践上来说，这个假设并不算离谱，因为我们说了如果极限$\lim_{\Vert\boldsymbol{x}\Vert\to\infty} p(\boldsymbol{x})$存在就必然为零，因此在超出一定范围后它就跟等于零没有太大区别了。极限不存在的例子确实有，但一般都需要比较刻意构造，对于我们实际能遇到的数据，基本上都满足极限存在的条件。</p>
<h3 id="_12">旧路不通<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<p>直觉上，连续分布的优化应该是照搬离散分布的思路，即设$\boldsymbol{p}<em t_eta="t+\eta">{t+\eta}(\boldsymbol{x}) = (1 - \eta)\boldsymbol{p}_t(\boldsymbol{x}) + \eta \boldsymbol{q}_t(\boldsymbol{x})$，因为跟离散分布一样，连续分布的概率密度函数集$\mathbb{P}$同样是一个凸集。现在我们将它代入目标泛函<br />
\begin{equation}\begin{aligned}<br />
\mathcal{F}[p</em> \\}] =&amp;\, \int F(p_{t+\eta}(\boldsymbol{x}))d\boldsymbol{x<br />
=&amp;\, \int F((1 - \eta)\boldsymbol{p}<em _mathbb_P="\mathbb{P" q_t_in="q_t\in">t(\boldsymbol{x}) + \eta \boldsymbol{q}_t(\boldsymbol{x}))d\boldsymbol{x} \\<br />
\approx&amp;\,\int \left[F(p_t(\boldsymbol{x})) + \eta\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\Big(q_t(\boldsymbol{x}) - p_t(\boldsymbol{x})\Big)\right]d\boldsymbol{x}<br />
\end{aligned}\end{equation}<br />
假设一阶近似够用，那么问题转化为<br />
\begin{equation}\mathop{\text{argmin}}</em>}}\int\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}q_t(\boldsymbol{x})d\boldsymbol{x}\end{equation<br />
这个问题倒也是不难解，答案跟离散分布的one hot类似<br />
\begin{equation}q_t(\boldsymbol{x}) = \delta\left(\boldsymbol{x} - \mathop{\text{argmin}}_{\boldsymbol{x}'} \frac{\partial F(p_t(\boldsymbol{x}'))}{\partial p_t(\boldsymbol{x}')}\right)\end{equation}<br />
这里的$\delta(\cdot)$是<a href="/archives/1870">狄拉克delta函数</a>，表示单点分布的概率密度。</p>
<p>看上去很顺利，然而实际上此路并不通。首先，狄拉克delta函数并不是常规意义下的函数，它是广义函数（也是泛函的一种）；其次，如果我们用普通函数的视角去看的话，狄拉克delta函数在某点处具有无穷大的值，而既然是无穷大的值，那么推导过程中的“一阶近似够用”的假设就不可能成立了。</p>
<h3 id="_13">变量代换<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<p>我们可以考虑继续对上一节的推导做一些修补，比如加上$q_t(\boldsymbol{x}) \leq C$的限制，以获得有意义的结果，然而这种缝缝补补的做法终究显得不够优雅。可是如果不利用凸集的性质，又该如何构建下一步的试探分布$\boldsymbol{p}_{t+\eta}(\boldsymbol{x})$呢？</p>
<p>这时候就要充分发挥概率密度函数的特性了——我们可以通过变量代换，来将一个概率密度函数变换为另一个概率密度函数，这是连续型分布的独有性质。具体来说，如果$p(\boldsymbol{x})$是一个概率密度函数，$\boldsymbol{y}=\boldsymbol{T}(\boldsymbol{x})$是一个可逆变换，那么$p(\boldsymbol{T}(\boldsymbol{x}))\left|\frac{\partial \boldsymbol{T}(\boldsymbol{x})}{\partial\boldsymbol{x}}\right|$同样是一个概率密度函数，其中$|\cdot|$表示矩阵的行列式绝对值。</p>
<p>基于这个特性，我们将下一步要试探的概率分布定义为<br />
\begin{equation}\begin{aligned}<br />
p_{t+\eta}(\boldsymbol{x}) =&amp;\, p_t(\boldsymbol{x} + \eta\boldsymbol{\mu}<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x}))\left|\boldsymbol{I} + \eta\frac{\partial \boldsymbol{\mu}_t(\boldsymbol{x})}{\partial\boldsymbol{x}}\right| \\<br />
\approx &amp;\, \Big[p_t(\boldsymbol{x}) + \eta\boldsymbol{\mu}_t(\boldsymbol{x})\cdot\nabla</em>}} p_t(\boldsymbol{x})\Big]\left[1 + \eta\,\text{Tr}\frac{\partial \boldsymbol{\mu<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x})}{\partial\boldsymbol{x}}\right] \\[3pt]<br />
\approx &amp;\, p_t(\boldsymbol{x}) + \eta\boldsymbol{\mu}_t(\boldsymbol{x})\cdot\nabla</em>}} p_t(\boldsymbol{x}) + \eta\, p_t(\boldsymbol{x})\nabla_{\boldsymbol{x}}\cdot\boldsymbol{\mu<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x}) \\[5pt]<br />
= &amp;\, p_t(\boldsymbol{x}) + \eta\nabla</em>)\big] \\}}\cdot\big[p_t(\boldsymbol{x})\boldsymbol{\mu}_t(\boldsymbol{x<br />
\end{aligned}\end{equation}<br />
同样的结果我们在<a href="/archives/9280">《生成扩散模型漫谈（十二）：“硬刚”扩散ODE》</a>已经推导过，其中行列式的近似展开，可以参考<a href="/archives/2383">《行列式的导数》</a>一文。</p>
<h3 id="_14">积分变换<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h3>
<p>利用这个新的$p_{t+\eta}(\boldsymbol{x})$，我们可以得到<br />
\begin{equation}\begin{aligned}<br />
\mathcal{F}[p_{t+\eta}] =&amp;\, \int F(p_{t+\eta}(\boldsymbol{x}))d\boldsymbol{x} \\<br />
\approx&amp;\, \int F\Big(p_t(\boldsymbol{x}) + \eta\nabla_{\boldsymbol{x}}\cdot\big[p_t(\boldsymbol{x})\boldsymbol{\mu}<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x})\big]\Big)d\boldsymbol{x} \\<br />
\approx&amp;\, \int \left[F(p_t(\boldsymbol{x})) + \eta\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\nabla</em>}}\cdot\big[p_t(\boldsymbol{x})\boldsymbol{\mu<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x})\big]\right]d\boldsymbol{x} \\<br />
=&amp;\, \mathcal{F}[p_t] + \eta\int \frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\nabla</em>}}\cdot\big[p_t(\boldsymbol{x})\boldsymbol{\mu<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x})\big] d\boldsymbol{x} \\<br />
\end{aligned}\label{eq:px-approx}\end{equation}<br />
接下来需要像<a href="/archives/9461">《测试函数法推导连续性方程和Fokker-Planck方程》</a>一样，推导一个概率密度相关的积分恒等式。首先我们有<br />
\begin{equation}\begin{aligned}<br />
&amp;\,\int \nabla</em>}}\cdot\left[\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})} p_t(\boldsymbol{x})\boldsymbol{\mu<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x})\right] d\boldsymbol{x} \\[5pt]<br />
=&amp;\, \int \frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\nabla</em>}}\cdot\big[p_t(\boldsymbol{x})\boldsymbol{\mu<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x})\big] d\boldsymbol{x} + \int \left(\nabla</em>}}\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\right)\cdot\big[p_t(\boldsymbol{x})\boldsymbol{\mu<em _Omega="\Omega">t(\boldsymbol{x})\big] d\boldsymbol{x}<br />
\end{aligned}\end{equation}<br />
根据<a href="https://en.wikipedia.org/wiki/Divergence_theorem">散度定理</a>，我们有<br />
\begin{equation}\int</em>} \nabla_{\boldsymbol{x}}\cdot\left[\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})} p_t(\boldsymbol{x})\boldsymbol{\mu<em _partial_Omega="\partial\Omega">t(\boldsymbol{x})\right] d\boldsymbol{x} = \int</em>} \left[\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})} p_t(\boldsymbol{x})\boldsymbol{\mu<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x})\right]\cdot \hat{\boldsymbol{n}} dS\end{equation}<br />
其中$\Omega$是积分区域，在这里是整个$\mathbb{R}^n$，$\partial\Omega$是区域边界，$\mathbb{R}^n$的边界自然是无穷远处，$\hat{\boldsymbol{n}}$是边界的外向单位法向量，$dS$是面积微元。在紧支撑集的假设下，无穷远处$p_t(\boldsymbol{x})=0$，所以上式右端实际上就是零的积分，结果是零。因此我们有<br />
\begin{equation}\int \frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\nabla</em>}}\cdot\big[p_t(\boldsymbol{x})\boldsymbol{\mu<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x})\big] d\boldsymbol{x} = - \int \left(\nabla</em>}}\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\right)\cdot\big[p_t(\boldsymbol{x})\boldsymbol{\mu<em t_eta="t+\eta">t(\boldsymbol{x})\big] d\boldsymbol{x}\end{equation}<br />
代入式$\eqref{eq:px-approx}$得到<br />
\begin{equation}\mathcal{F}[p</em>}] \approx \mathcal{F}[p_t] - \eta\int \left(p_t(\boldsymbol{x})\nabla_{\boldsymbol{x}}\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\right)\cdot \boldsymbol{\mu}_t(\boldsymbol{x}) d\boldsymbol{x} \label{eq:px-approx-2}\end{equation</p>
<h3 id="_15">梯度之流<a class="toc-link" href="#_15" title="Permanent link">&para;</a></h3>
<p>根据式$\eqref{eq:px-approx-2}$，让$\mathcal{F}[p_{t+\eta}] \leq \mathcal{F}[p_t]$的一个简单选择是<br />
\begin{equation}\boldsymbol{\mu}<em _boldsymbol_x="\boldsymbol{x">t(\boldsymbol{x}) = \nabla</em>}}\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\end{equation<br />
相应的迭代格式是<br />
\begin{equation}p_{t+\eta}(\boldsymbol{x}) \approx p_t(\boldsymbol{x}) + \eta\nabla_{\boldsymbol{x}}\cdot\left[p_t(\boldsymbol{x})\nabla_{\boldsymbol{x}}\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\right] \end{equation}<br />
我们还可以取$\eta\to 0$的极限得<br />
\begin{equation}\frac{\partial}{\partial t}p_t(\boldsymbol{x}) = \nabla_{\boldsymbol{x}}\cdot\left[p_t(\boldsymbol{x})\nabla_{\boldsymbol{x}}\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\right] \end{equation}<br />
或者简写成<br />
\begin{equation}\frac{\partial p_t}{\partial t} = \nabla\cdot\left[p_t\nabla\frac{\delta \mathcal{F}[p_t]}{\delta p_t}\right] \end{equation}<br />
这就是<a href="/archives/9660">《梯度流：探索通向最小值之路》</a>介绍过的Wasserstein梯度流，但这里我们没有引入Wasserstein距离的概念也得到了相同的结果。</p>
<p>由于$p_{t+\eta}(\boldsymbol{x})$是$p_t(\boldsymbol{x})$通过变换$\boldsymbol{x}\to \boldsymbol{x} + \eta \boldsymbol{\mu}<em t_eta="t+\eta">t(\boldsymbol{x})$得到的，所以我们还可以写出$\boldsymbol{x}$的运动轨迹ODE：<br />
\begin{equation}\boldsymbol{x}_t = \boldsymbol{x}</em>} + \eta \boldsymbol{\mu<em t_eta="t+\eta">t(\boldsymbol{x}</em>})\quad\Rightarrow\quad \frac{d\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">t}{dt} = -\boldsymbol{\mu}_t(\boldsymbol{x}_t) = -\nabla</em>}}\frac{\partial F(p_t(\boldsymbol{x}))}{\partial p_t(\boldsymbol{x})}\end{equation<br />
这个ODE的意义是，从分布$p_0(\boldsymbol{x})$的采样结果$\boldsymbol{x}_0$出发，按照此ODE运动到$\boldsymbol{x}_t$时，$\boldsymbol{x}_t$所服从的分布正是$p_t(\boldsymbol{x})$。</p>
<h2 id="_16">文章小结<a class="toc-link" href="#_16" title="Permanent link">&para;</a></h2>
<p>本文系统整理了概率空间中目标函数的最小化方法，包括取到极小值的必要条件、类似梯度下降的迭代法等内容，相关结果在最优化、生成模型（尤其是扩散模型）等场景中时有用到。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/10289">https://spaces.ac.cn/archives/10289</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Aug. 06, 2024). 《通向最优分布之路：概率空间的最小化 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/10289">https://spaces.ac.cn/archives/10289</a></p>
<p>@online{kexuefm-10289,<br />
title={通向最优分布之路：概率空间的最小化},<br />
author={苏剑林},<br />
year={2024},<br />
month={Aug},<br />
url={\url{https://spaces.ac.cn/archives/10289}},<br />
} </p>
<hr />
<h2 id="_17">公式推导与注释<a class="toc-link" href="#_17" title="Permanent link">&para;</a></h2>
<p>本节提供极其详细的数学推导，深入探讨概率空间中的优化理论，包括Wasserstein梯度流、最优传输、f-散度等核心概念。</p>
<h3 id="1">1. 概率空间的数学定义<a class="toc-link" href="#1" title="Permanent link">&para;</a></h3>
<p><strong>推导1：概率测度空间</strong></p>
<p>设 $(\mathcal{X}, \mathcal{B}, \mu)$ 是一个测度空间，其中：<br />
- $\mathcal{X}$ 是样本空间（通常取 $\mathbb{R}^n$）<br />
- $\mathcal{B}$ 是 $\sigma$-代数（Borel集）<br />
- $\mu$ 是概率测度，满足 $\mu(\mathcal{X}) = 1$</p>
<p><strong>推导2：概率密度函数</strong></p>
<p>对于连续型分布，存在密度函数 $p:\mathcal{X}\to\mathbb{R}_+$ 使得：</p>
<p>$$<br />
\mu(A) = \int_A p(\boldsymbol{x})d\boldsymbol{x}, \quad \forall A \in \mathcal{B}<br />
$$</p>
<p>并且满足归一化条件：</p>
<p>$$<br />
\int_{\mathcal{X}} p(\boldsymbol{x})d\boldsymbol{x} = 1<br />
$$</p>
<p><strong>注释1</strong>：概率密度函数 $p(\boldsymbol{x})$ 必须非负且积分为1，这两个约束使得优化问题变得复杂。</p>
<p><strong>推导3：概率空间的拓扑结构</strong></p>
<p>定义概率测度空间 $\mathcal{P}(\mathcal{X})$ 为 $\mathcal{X}$ 上所有概率测度的集合。这是一个无穷维空间，需要合适的度量（metric）来定义"距离"。</p>
<h3 id="2">2. 泛函与泛函导数<a class="toc-link" href="#2" title="Permanent link">&para;</a></h3>
<p><strong>推导4：能量泛函的定义</strong></p>
<p>考虑形如下式的能量泛函：</p>
<p>$$<br />
\mathcal{F}[p] = \int_{\mathcal{X}} F(p(\boldsymbol{x}), \boldsymbol{x})d\boldsymbol{x}<br />
$$</p>
<p>常见例子：</p>
<ol>
<li>KL散度：$\mathcal{F}[p] = \int p(\boldsymbol{x})\log p(\boldsymbol{x})d\boldsymbol{x}$</li>
<li>势能：$\mathcal{F}[p] = \int p(\boldsymbol{x})V(\boldsymbol{x})d\boldsymbol{x}$</li>
<li>交互能：$\mathcal{F}[p] = \iint p(\boldsymbol{x})p(\boldsymbol{y})W(\boldsymbol{x},\boldsymbol{y})d\boldsymbol{x}d\boldsymbol{y}$</li>
</ol>
<p><strong>推导5：泛函导数的定义</strong></p>
<p>泛函 $\mathcal{F}[p]$ 在 $p$ 处沿方向 $h$ 的Gateaux导数定义为：</p>
<p>$$<br />
\frac{d}{d\epsilon}\mathcal{F}[p+\epsilon h]\bigg|_{\epsilon=0} = \int \frac{\delta\mathcal{F}}{\delta p}(\boldsymbol{x})h(\boldsymbol{x})d\boldsymbol{x}<br />
$$</p>
<p>其中 $\frac{\delta\mathcal{F}}{\delta p}$ 称为泛函导数。</p>
<p><strong>推导6：简单泛函的泛函导数</strong></p>
<p>对于 $\mathcal{F}[p] = \int F(p(\boldsymbol{x}))d\boldsymbol{x}$：</p>
<p>$$<br />
\begin{aligned}<br />
\frac{d}{d\epsilon}\mathcal{F}[p+\epsilon h]\bigg|<em _epsilon="0">{\epsilon=0} &amp;= \frac{d}{d\epsilon}\int F(p(\boldsymbol{x})+\epsilon h(\boldsymbol{x}))d\boldsymbol{x}\bigg|</em> \<br />
&amp;= \int \frac{\partial F}{\partial p}(p(\boldsymbol{x}))h(\boldsymbol{x})d\boldsymbol{x}<br />
\end{aligned}<br />
$$</p>
<p>因此：</p>
<p>$$<br />
\frac{\delta\mathcal{F}}{\delta p}(\boldsymbol{x}) = \frac{\partial F}{\partial p}(p(\boldsymbol{x}))<br />
$$</p>
<p><strong>注释2</strong>：泛函导数可以理解为无穷维空间中的"梯度"。</p>
<h3 id="3-kl">3. KL散度的变分推导<a class="toc-link" href="#3-kl" title="Permanent link">&para;</a></h3>
<p><strong>推导7：KL散度的定义</strong></p>
<p>给定参考分布 $q(\boldsymbol{x})$，KL散度定义为：</p>
<p>$$<br />
\text{KL}[p|q] = \int p(\boldsymbol{x})\log\frac{p(\boldsymbol{x})}{q(\boldsymbol{x})}d\boldsymbol{x}<br />
$$</p>
<p><strong>推导8：KL散度的分解</strong></p>
<p>$$<br />
\begin{aligned}<br />
\text{KL}[p|q] &amp;= \int p(\boldsymbol{x})\log p(\boldsymbol{x})d\boldsymbol{x} - \int p(\boldsymbol{x})\log q(\boldsymbol{x})d\boldsymbol{x} \<br />
&amp;= H[p] + \mathbb{E}_p[-\log q(\boldsymbol{x})]<br />
\end{aligned}<br />
$$</p>
<p>其中 $H[p] = -\int p(\boldsymbol{x})\log p(\boldsymbol{x})d\boldsymbol{x}$ 是熵。</p>
<p><strong>推导9：KL散度的泛函导数</strong></p>
<p>计算 $\frac{\delta}{\delta p}\text{KL}[p|q]$：</p>
<p>$$<br />
\begin{aligned}<br />
\frac{d}{d\epsilon}\text{KL}[p+\epsilon h|q]\bigg|<em _epsilon="0">{\epsilon=0} &amp;= \frac{d}{d\epsilon}\int (p+\epsilon h)\log\frac{p+\epsilon h}{q}d\boldsymbol{x}\bigg|</em> \<br />
&amp;= \int h\log\frac{p}{q}d\boldsymbol{x} + \int p\cdot\frac{1}{p}\cdot h d\boldsymbol{x} \<br />
&amp;= \int h\left(\log\frac{p}{q}+1\right)d\boldsymbol{x}<br />
\end{aligned}<br />
$$</p>
<p>因此：</p>
<p>$$<br />
\frac{\delta\text{KL}[p|q]}{\delta p} = \log\frac{p(\boldsymbol{x})}{q(\boldsymbol{x})} + 1<br />
$$</p>
<p><strong>推导10：最小化KL散度</strong></p>
<p>考虑约束优化问题：</p>
<p>$$<br />
\min_p \text{KL}[p|q], \quad \text{s.t. } \int p(\boldsymbol{x})d\boldsymbol{x} = 1<br />
$$</p>
<p>使用拉格朗日乘子 $\lambda$：</p>
<p>$$<br />
\mathcal{L}[p] = \text{KL}[p|q] + \lambda\left(\int p(\boldsymbol{x})d\boldsymbol{x} - 1\right)<br />
$$</p>
<p>令泛函导数为零：</p>
<p>$$<br />
\frac{\delta\mathcal{L}}{\delta p} = \log\frac{p(\boldsymbol{x})}{q(\boldsymbol{x})} + 1 + \lambda = 0<br />
$$</p>
<p>解得：</p>
<p>$$<br />
p^*(\boldsymbol{x}) = q(\boldsymbol{x})e^{-1-\lambda}<br />
$$</p>
<p>由归一化条件 $\int p^<em>(\boldsymbol{x})d\boldsymbol{x} = 1$ 得 $e^{-1-\lambda} = 1$，因此 $p^</em>(\boldsymbol{x}) = q(\boldsymbol{x})$。</p>
<p><strong>注释3</strong>：KL散度在 $p=q$ 时达到最小值0，这验证了我们的推导。</p>
<h3 id="4-f-">4. f-散度的一般框架<a class="toc-link" href="#4-f-" title="Permanent link">&para;</a></h3>
<p><strong>推导11：f-散度的定义</strong></p>
<p>给定凸函数 $f:\mathbb{R}_+ \to \mathbb{R}$ 满足 $f(1)=0$，f-散度定义为：</p>
<p>$$<br />
D_f[p|q] = \int q(\boldsymbol{x})f\left(\frac{p(\boldsymbol{x})}{q(\boldsymbol{x})}\right)d\boldsymbol{x}<br />
$$</p>
<p><strong>推导12：常见f-散度的例子</strong></p>
<ol>
<li>
<p><strong>KL散度</strong>：$f(t) = t\log t$<br />
   $$D_f[p|q] = \int q \cdot \frac{p}{q}\log\frac{p}{q}d\boldsymbol{x} = \text{KL}[p|q]$$</p>
</li>
<li>
<p><strong>反向KL散度</strong>：$f(t) = -\log t$<br />
   $$D_f[p|q] = \int q(-\log\frac{p}{q})d\boldsymbol{x} = \text{KL}[q|p]$$</p>
</li>
<li>
<p><strong>$\chi^2$散度</strong>：$f(t) = (t-1)^2$<br />
   $$D_f[p|q] = \int q\left(\frac{p}{q}-1\right)^2d\boldsymbol{x} = \int\frac{(p-q)^2}{q}d\boldsymbol{x}$$</p>
</li>
<li>
<p><strong>Hellinger距离</strong>：$f(t) = (\sqrt{t}-1)^2$<br />
   $$D_f[p|q] = \int q(\sqrt{p/q}-1)^2d\boldsymbol{x} = \int(\sqrt{p}-\sqrt{q})^2d\boldsymbol{x}$$</p>
</li>
</ol>
<p><strong>推导13：f-散度的泛函导数</strong></p>
<p>$$<br />
\begin{aligned}<br />
\frac{d}{d\epsilon}D_f[p+\epsilon h|q]\bigg|<em _epsilon="0">{\epsilon=0} &amp;= \frac{d}{d\epsilon}\int q f\left(\frac{p+\epsilon h}{q}\right)d\boldsymbol{x}\bigg|</em> \<br />
&amp;= \int q \cdot f'\left(\frac{p}{q}\right)\cdot\frac{h}{q}d\boldsymbol{x} \<br />
&amp;= \int f'\left(\frac{p(\boldsymbol{x})}{q(\boldsymbol{x})}\right)h(\boldsymbol{x})d\boldsymbol{x}<br />
\end{aligned}<br />
$$</p>
<p>因此：</p>
<p>$$<br />
\frac{\delta D_f[p|q]}{\delta p}(\boldsymbol{x}) = f'\left(\frac{p(\boldsymbol{x})}{q(\boldsymbol{x})}\right)<br />
$$</p>
<h3 id="5-wasserstein">5. Wasserstein距离与最优传输<a class="toc-link" href="#5-wasserstein" title="Permanent link">&para;</a></h3>
<p><strong>推导14：Wasserstein-2距离的定义</strong></p>
<p>设 $p, q$ 是 $\mathbb{R}^n$ 上的两个概率分布，Wasserstein-2距离定义为：</p>
<p>$$<br />
W_2^2(p, q) = \inf_{\gamma \in \Gamma(p,q)} \int\int |\boldsymbol{x}-\boldsymbol{y}|^2 d\gamma(\boldsymbol{x},\boldsymbol{y})<br />
$$</p>
<p>其中 $\Gamma(p,q)$ 是边际分布为 $p$ 和 $q$ 的所有联合分布的集合。</p>
<p><strong>注释4</strong>：$\gamma(\boldsymbol{x},\boldsymbol{y})$ 称为传输计划（transport plan），描述了如何将质量从 $\boldsymbol{x}$ 移动到 $\boldsymbol{y}$。</p>
<p><strong>推导15：Kantorovich对偶形式</strong></p>
<p>Wasserstein距离有对偶表示：</p>
<p>$$<br />
W_2^2(p,q) = \sup_{\phi,\psi} \left{\int \phi(\boldsymbol{x})dp(\boldsymbol{x}) + \int \psi(\boldsymbol{y})dq(\boldsymbol{y})\right}<br />
$$</p>
<p>其中上确界取遍满足 $\phi(\boldsymbol{x}) + \psi(\boldsymbol{y}) \leq |\boldsymbol{x}-\boldsymbol{y}|^2$ 的函数对。</p>
<p><strong>推导16：Brenier定理（最优传输映射）</strong></p>
<p>在合适的条件下，存在唯一的最优传输映射 $T:\mathbb{R}^n\to\mathbb{R}^n$ 使得：</p>
<p>$$<br />
T_#p = q<br />
$$</p>
<p>即 $q(\boldsymbol{y}) = \int \delta(\boldsymbol{y}-T(\boldsymbol{x}))p(\boldsymbol{x})d\boldsymbol{x}$，且 $T$ 是某个凸函数的梯度：</p>
<p>$$<br />
T(\boldsymbol{x}) = \nabla\phi(\boldsymbol{x})<br />
$$</p>
<p><strong>推导17：Wasserstein距离的一阶变分</strong></p>
<p>考虑分布族 $p_t$，其Wasserstein-2距离关于 $t$ 的导数：</p>
<p>$$<br />
\frac{d}{dt}W_2^2(p_t, q) = -2\int (\boldsymbol{x}-T_t(\boldsymbol{x}))\cdot\boldsymbol{v}_t(\boldsymbol{x})p_t(\boldsymbol{x})d\boldsymbol{x}<br />
$$</p>
<p>其中 $\boldsymbol{v}_t$ 是速度场，$T_t$ 是从 $p_t$ 到 $q$ 的最优传输映射。</p>
<h3 id="6-wasserstein">6. Wasserstein梯度流的详细推导<a class="toc-link" href="#6-wasserstein" title="Permanent link">&para;</a></h3>
<p><strong>推导18：梯度流的基本概念</strong></p>
<p>在欧氏空间中，梯度流定义为：</p>
<p>$$<br />
\frac{d\boldsymbol{x}}{dt} = -\nabla_{\boldsymbol{x}} E(\boldsymbol{x})<br />
$$</p>
<p>在概率空间中，我们需要定义概率分布的"梯度"。</p>
<p><strong>推导19：连续性方程</strong></p>
<p>设粒子按速度场 $\boldsymbol{v}_t(\boldsymbol{x})$ 运动，密度 $p_t(\boldsymbol{x})$ 满足连续性方程：</p>
<p>$$<br />
\frac{\partial p_t}{\partial t} + \nabla \cdot (p_t \boldsymbol{v}_t) = 0<br />
$$</p>
<p><strong>注释5</strong>：这是质量守恒的数学表达——密度的变化等于流量的散度。</p>
<p><strong>推导20：Wasserstein梯度的定义</strong></p>
<p>泛函 $\mathcal{F}[p]$ 的Wasserstein梯度定义为满足以下条件的向量场 $\nabla_W\mathcal{F}[p]$：</p>
<p>$$<br />
\frac{d}{dt}\mathcal{F}[p_t]\bigg|_{t=0} = -\int |\nabla_W\mathcal{F}<a href="\boldsymbol{x}">p</a>|^2 p(\boldsymbol{x})d\boldsymbol{x}<br />
$$</p>
<p>当 $p_t$ 沿速度场 $\boldsymbol{v}_t = -\nabla_W\mathcal{F}[p_t]$ 演化时。</p>
<p><strong>推导21：简单泛函的Wasserstein梯度</strong></p>
<p>对于 $\mathcal{F}[p] = \int F(p(\boldsymbol{x}))d\boldsymbol{x}$：</p>
<p>$$<br />
\begin{aligned}<br />
\frac{d\mathcal{F}[p_t]}{dt} &amp;= \int \frac{\partial F}{\partial p}\frac{\partial p_t}{\partial t}d\boldsymbol{x} \<br />
&amp;= -\int \frac{\partial F}{\partial p}\nabla\cdot(p_t\boldsymbol{v}_t)d\boldsymbol{x} \<br />
&amp;= \int \nabla\left(\frac{\partial F}{\partial p}\right)\cdot(p_t\boldsymbol{v}_t)d\boldsymbol{x} \quad \text{(分部积分)} \<br />
&amp;= \int p_t\nabla\left(\frac{\partial F}{\partial p}\right)\cdot\boldsymbol{v}_td\boldsymbol{x}<br />
\end{aligned}<br />
$$</p>
<p><strong>推导22：化简为标准形式</strong></p>
<p>要使上式等于 $-\int p_t|\boldsymbol{v}_t|^2d\boldsymbol{x}$，需要：</p>
<p>$$<br />
\boldsymbol{v}_t = -\nabla\left(\frac{\partial F}{\partial p}\right) = -\nabla\frac{\delta\mathcal{F}}{\delta p}<br />
$$</p>
<p>因此Wasserstein梯度为：</p>
<p>$$<br />
\nabla_W\mathcal{F}<a href="\boldsymbol{x}">p</a> = \nabla_{\boldsymbol{x}}\frac{\delta\mathcal{F}}{\delta p}(\boldsymbol{x})<br />
$$</p>
<p><strong>推导23：Wasserstein梯度流方程</strong></p>
<p>将连续性方程和速度场结合：</p>
<p>$$<br />
\frac{\partial p_t}{\partial t} = \nabla \cdot \left(p_t\nabla\frac{\delta\mathcal{F}}{\delta p_t}\right)<br />
$$</p>
<p>这就是Wasserstein梯度流的一般形式。</p>
<h3 id="7-fokker-planck">7. Fokker-Planck方程<a class="toc-link" href="#7-fokker-planck" title="Permanent link">&para;</a></h3>
<p><strong>推导24：随机微分方程（SDE）</strong></p>
<p>考虑粒子的随机运动：</p>
<p>$$<br />
d\boldsymbol{X}_t = \boldsymbol{b}(\boldsymbol{X}_t, t)dt + \boldsymbol{\sigma}(\boldsymbol{X}_t, t)d\boldsymbol{W}_t<br />
$$</p>
<p>其中 $\boldsymbol{b}$ 是漂移项，$\boldsymbol{\sigma}$ 是扩散项，$\boldsymbol{W}_t$ 是Wiener过程。</p>
<p><strong>推导25：Fokker-Planck方程的推导</strong></p>
<p>粒子密度 $p_t(\boldsymbol{x})$ 的演化满足Fokker-Planck方程：</p>
<p>$$<br />
\frac{\partial p_t}{\partial t} = -\nabla \cdot (\boldsymbol{b}p_t) + \frac{1}{2}\sum_{i,j}\frac{\partial^2}{\partial x_i\partial x_j}(D_{ij}p_t)<br />
$$</p>
<p>其中 $\boldsymbol{D} = \boldsymbol{\sigma}\boldsymbol{\sigma}^T$ 是扩散矩阵。</p>
<p><strong>推导26：常数扩散的简化</strong></p>
<p>当 $\boldsymbol{\sigma} = \sqrt{2}\boldsymbol{I}$ 时：</p>
<p>$$<br />
\frac{\partial p_t}{\partial t} = -\nabla \cdot (\boldsymbol{b}p_t) + \Delta p_t<br />
$$</p>
<p>其中 $\Delta = \sum_i \frac{\partial^2}{\partial x_i^2}$ 是拉普拉斯算子。</p>
<p><strong>推导27：梯度流形式</strong></p>
<p>取漂移项 $\boldsymbol{b} = -\nabla V$：</p>
<p>$$<br />
\frac{\partial p_t}{\partial t} = \nabla \cdot (p_t\nabla V) + \Delta p_t = \nabla \cdot \left(p_t\nabla V + \nabla p_t\right)<br />
$$</p>
<p>进一步：</p>
<p>$$<br />
\frac{\partial p_t}{\partial t} = \nabla \cdot \left(p_t\nabla(V + \log p_t)\right)<br />
$$</p>
<p><strong>注释6</strong>：这正是自由能 $\mathcal{F}[p] = \int p(V+\log p)d\boldsymbol{x}$ 的Wasserstein梯度流！</p>
<p><strong>推导28：稳态分布</strong></p>
<p>稳态时 $\frac{\partial p}{\partial t} = 0$，要求：</p>
<p>$$<br />
p\nabla(V+\log p) = 0<br />
$$</p>
<p>解得：</p>
<p>$$<br />
p^*(\boldsymbol{x}) = \frac{e^{-V(\boldsymbol{x})}}{Z}, \quad Z = \int e^{-V(\boldsymbol{y})}d\boldsymbol{y}<br />
$$</p>
<p>这是Boltzmann分布。</p>
<h3 id="8-jkoeuler">8. JKO格式（隐式Euler离散化）<a class="toc-link" href="#8-jkoeuler" title="Permanent link">&para;</a></h3>
<p><strong>推导29：时间离散化</strong></p>
<p>将Wasserstein梯度流离散化，时间步长为 $\tau$：</p>
<p>$$<br />
\frac{p_{k+1} - p_k}{\tau} \approx -\nabla_W\mathcal{F}[p_{k+1}]<br />
$$</p>
<p><strong>推导30：JKO格式的推导</strong></p>
<p>上式等价于优化问题：</p>
<p>$$<br />
p_{k+1} = \arg\min_p \left{\mathcal{F}[p] + \frac{1}{2\tau}W_2^2(p, p_k)\right}<br />
$$</p>
<p><strong>注释7</strong>：这个格式由Jordan-Kinderlehrer-Otto提出，是数值求解Wasserstein梯度流的标准方法。</p>
<p><strong>推导31：能量衰减性质</strong></p>
<p>定义离散能量：</p>
<p>$$<br />
\mathcal{E}_k = \mathcal{F}[p_k]<br />
$$</p>
<p>根据JKO格式的定义：</p>
<p>$$<br />
\mathcal{F}[p_{k+1}] + \frac{1}{2\tau}W_2^2(p_{k+1}, p_k) \leq \mathcal{F}[p_k]<br />
$$</p>
<p>因此：</p>
<p>$$<br />
\mathcal{E}<em k_1="k+1">{k+1} \leq \mathcal{E}_k - \frac{1}{2\tau}W_2^2(p</em>, p_k)<br />
$$</p>
<p>能量单调递减！</p>
<h3 id="9">9. 最优传输的计算方法<a class="toc-link" href="#9" title="Permanent link">&para;</a></h3>
<p><strong>推导32：熵正则化</strong></p>
<p>原始最优传输问题的计算复杂度很高。引入熵正则化：</p>
<p>$$<br />
W_{2,\epsilon}^2(p,q) = \min_{\gamma\in\Gamma(p,q)} \left{\int\int |\boldsymbol{x}-\boldsymbol{y}|^2d\gamma + \epsilon H[\gamma]\right}<br />
$$</p>
<p>其中熵项为：</p>
<p>$$<br />
H[\gamma] = -\int\int \gamma(\boldsymbol{x},\boldsymbol{y})\log\gamma(\boldsymbol{x},\boldsymbol{y})d\boldsymbol{x}d\boldsymbol{y}<br />
$$</p>
<p><strong>推导33：Sinkhorn算法</strong></p>
<p>正则化问题的解具有特殊结构：</p>
<p>$$<br />
\gamma^*(\boldsymbol{x},\boldsymbol{y}) = \alpha(\boldsymbol{x})\exp\left(-\frac{|\boldsymbol{x}-\boldsymbol{y}|^2}{\epsilon}\right)\beta(\boldsymbol{y})<br />
$$</p>
<p>其中 $\alpha, \beta$ 通过迭代求解：</p>
<p>$$<br />
\begin{aligned}<br />
\alpha^{(k+1)}(\boldsymbol{x}) &amp;= \frac{p(\boldsymbol{x})}{\int \exp(-|\boldsymbol{x}-\boldsymbol{y}|^2/\epsilon)\beta^{(k)}(\boldsymbol{y})q(\boldsymbol{y})d\boldsymbol{y}} \<br />
\beta^{(k+1)}(\boldsymbol{y}) &amp;= \frac{q(\boldsymbol{y})}{\int \exp(-|\boldsymbol{x}-\boldsymbol{y}|^2/\epsilon)\alpha^{(k+1)}(\boldsymbol{x})p(\boldsymbol{x})d\boldsymbol{x}}<br />
\end{aligned}<br />
$$</p>
<p><strong>注释8</strong>：Sinkhorn算法收敛速度快，适合大规模问题。</p>
<h3 id="10">10. 与生成模型的联系<a class="toc-link" href="#10" title="Permanent link">&para;</a></h3>
<p><strong>推导34：生成模型的目标</strong></p>
<p>给定数据分布 $p_{\text{data}}$，我们希望找到模型分布 $p_\theta$ 使得：</p>
<p>$$<br />
\min_\theta D(p_{\text{data}}|p_\theta)<br />
$$</p>
<p>其中 $D$ 可以是KL散度、Wasserstein距离等。</p>
<p><strong>推导35：GAN的Wasserstein损失</strong></p>
<p>Wasserstein GAN（WGAN）使用：</p>
<p>$$<br />
\min_G \max_{D\in\mathcal{D}<em _boldsymbol_x="\boldsymbol{x">1} \mathbb{E}</em>))]}\sim p_{\text{data}}}[D(\boldsymbol{x})] - \mathbb{E}_{\boldsymbol{z}\sim p_z}[D(G(\boldsymbol{z<br />
$$</p>
<p>其中 $\mathcal{D}<em _text_data="\text{data">1$ 是1-Lipschitz函数类。这等价于最小化 $W_1(p</em>, p_G)$。}</p>
<p><strong>推导36：扩散模型的分数匹配</strong></p>
<p>扩散模型学习分数函数 $\nabla_{\boldsymbol{x}}\log p_t(\boldsymbol{x})$，目标为：</p>
<p>$$<br />
\min_\theta \mathbb{E}<em _boldsymbol_x="\boldsymbol{x">t\mathbb{E}</em><em>t}\left[\left|\boldsymbol{s}</em>\theta(\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">t, t) - \nabla</em>_t)\right|^2\right]}}\log p_t(\boldsymbol{x<br />
$$</p>
<p><strong>推导37：逆向SDE</strong></p>
<p>前向扩散过程：</p>
<p>$$<br />
d\boldsymbol{X}_t = \boldsymbol{f}(\boldsymbol{X}_t,t)dt + g(t)d\boldsymbol{W}_t<br />
$$</p>
<p>对应的逆向SDE（Anderson, 1982）：</p>
<p>$$<br />
d\boldsymbol{X}<em _boldsymbol_x="\boldsymbol{x">t = [\boldsymbol{f}(\boldsymbol{X}_t,t) - g(t)^2\nabla</em>_t}}\log p_t(\boldsymbol{X}_t)]dt + g(t)d\bar{\boldsymbol{W}<br />
$$</p>
<p>其中 $\bar{\boldsymbol{W}}_t$ 是反向时间的Wiener过程。</p>
<p><strong>注释9</strong>：扩散模型通过学习分数函数来实现从噪声到数据的采样。</p>
<h3 id="11">11. 收敛性分析<a class="toc-link" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>推导38：Lyapunov函数方法</strong></p>
<p>对于Wasserstein梯度流：</p>
<p>$$<br />
\frac{\partial p_t}{\partial t} = \nabla\cdot\left(p_t\nabla\frac{\delta\mathcal{F}}{\delta p_t}\right)<br />
$$</p>
<p>泛函 $\mathcal{F}[p_t]$ 沿着流单调递减：</p>
<p>$$<br />
\frac{d\mathcal{F}[p_t]}{dt} = -\int p_t\left|\nabla\frac{\delta\mathcal{F}}{\delta p_t}\right|^2d\boldsymbol{x} \leq 0<br />
$$</p>
<p><strong>推导39：Fisher信息</strong></p>
<p>对于KL散度 $\mathcal{F}[p] = \text{KL}[p|q]$，有：</p>
<p>$$<br />
\frac{\delta\mathcal{F}}{\delta p} = \log\frac{p}{q} + 1<br />
$$</p>
<p>因此：</p>
<p>$$<br />
\frac{d\mathcal{F}[p_t]}{dt} = -\int p_t\left|\nabla\log\frac{p_t}{q}\right|^2d\boldsymbol{x} = -I[p_t|q]<br />
$$</p>
<p>其中 $I[p|q]$ 是Fisher信息，衡量了收敛速度。</p>
<p><strong>推导40：对数Sobolev不等式</strong></p>
<p>在强凸条件下，存在常数 $C_{\text{LS}}$ 使得：</p>
<p>$$<br />
\text{KL}[p|q] \leq \frac{1}{2C_{\text{LS}}}I[p|q]<br />
$$</p>
<p>结合前面的结果：</p>
<p>$$<br />
\frac{d\text{KL}[p_t|q]}{dt} \leq -2C_{\text{LS}}\text{KL}[p_t|q]<br />
$$</p>
<p>这给出指数收敛：</p>
<p>$$<br />
\text{KL}[p_t|q] \leq e^{-2C_{\text{LS}}t}\text{KL}[p_0|q]<br />
$$</p>
<h3 id="12">12. 扩展到约束优化<a class="toc-link" href="#12" title="Permanent link">&para;</a></h3>
<p><strong>推导41：等式约束</strong></p>
<p>考虑约束 $\int g_i(\boldsymbol{x})p(\boldsymbol{x})d\boldsymbol{x} = c_i$，拉格朗日函数：</p>
<p>$$<br />
\mathcal{L}[p] = \mathcal{F}[p] + \sum_i \lambda_i\left(\int g_i(\boldsymbol{x})p(\boldsymbol{x})d\boldsymbol{x} - c_i\right)<br />
$$</p>
<p>梯度流：</p>
<p>$$<br />
\frac{\partial p_t}{\partial t} = \nabla\cdot\left(p_t\nabla\left(\frac{\delta\mathcal{F}}{\delta p_t} + \sum_i\lambda_i g_i\right)\right)<br />
$$</p>
<p><strong>推导42：投影梯度流</strong></p>
<p>对于不等式约束 $p(\boldsymbol{x}) \geq 0$（自动满足）和 $\int p(\boldsymbol{x})d\boldsymbol{x} = 1$，可以证明Wasserstein梯度流自动保持这些约束。</p>
<h3 id="13">13. 数值实现的细节<a class="toc-link" href="#13" title="Permanent link">&para;</a></h3>
<p><strong>推导43：有限元离散化</strong></p>
<p>在计算机上，连续分布需要离散化。使用粒子近似：</p>
<p>$$<br />
p(\boldsymbol{x}) \approx \frac{1}{N}\sum_{i=1}^N \delta(\boldsymbol{x}-\boldsymbol{x}_i)<br />
$$</p>
<p><strong>推导44：粒子的演化方程</strong></p>
<p>将Wasserstein梯度流应用于粒子：</p>
<p>$$<br />
\frac{d\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">i}{dt} = -\nabla</em>_i)}}\frac{\delta\mathcal{F}}{\delta p}(\boldsymbol{x<br />
$$</p>
<p>这是有限粒子系统的ODE。</p>
<p><strong>推导45：核密度估计</strong></p>
<p>使用光滑核代替delta函数：</p>
<p>$$<br />
p(\boldsymbol{x}) \approx \frac{1}{N}\sum_{i=1}^N K_h(\boldsymbol{x}-\boldsymbol{x}_i)<br />
$$</p>
<p>其中 $K_h(\boldsymbol{x}) = \frac{1}{h^n}K(\boldsymbol{x}/h)$ 是带宽为 $h$ 的核函数。</p>
<h3 id="14-steinsvgd">14. Stein变分梯度下降（SVGD）<a class="toc-link" href="#14-steinsvgd" title="Permanent link">&para;</a></h3>
<p><strong>推导46：SVGD的动机</strong></p>
<p>目标：从先验 $p_0$ 出发，通过变换 $T$ 得到后验 $p = T_#p_0$，使得 $\text{KL}[p|q]$ 最小。</p>
<p><strong>推导47：扰动分析</strong></p>
<p>考虑无穷小变换 $T(\boldsymbol{x}) = \boldsymbol{x} + \epsilon\phi(\boldsymbol{x})$，KL散度的变化：</p>
<p>$$<br />
\frac{d}{d\epsilon}\text{KL}[T_#p|q]\bigg|_{\epsilon=0} = -\mathbb{E}_p[\text{Tr}(\nabla\phi) + \nabla\log q\cdot\phi]<br />
$$</p>
<p><strong>推导48：最优方向</strong></p>
<p>在再生核Hilbert空间（RKHS）中，最优 $\phi$ 为：</p>
<p>$$<br />
\phi^*(\boldsymbol{x}) = \mathbb{E}_p[k(\boldsymbol{x},\cdot)\nabla\log q + \nabla k(\boldsymbol{x},\cdot)]<br />
$$</p>
<p>其中 $k$ 是核函数（如RBF核）。</p>
<p><strong>推导49：SVGD更新规则</strong></p>
<p>粒子更新：</p>
<p>$$<br />
\boldsymbol{x}_i^{(t+1)} = \boldsymbol{x}_i^{(t)} + \epsilon\phi(\boldsymbol{x}_i^{(t)})<br />
$$</p>
<p>其中：</p>
<p>$$<br />
\phi(\boldsymbol{x}<em j="1">i) = \frac{1}{N}\sum</em>}^N \left[k(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">j,\boldsymbol{x}_i)\nabla</em><em _boldsymbol_x="\boldsymbol{x">j}\log q(\boldsymbol{x}_j) + \nabla</em>_i)\right]}_j}k(\boldsymbol{x}_j,\boldsymbol{x<br />
$$</p>
<p><strong>注释10</strong>：第一项是吸引力（向高概率区移动），第二项是排斥力（保持粒子多样性）。</p>
<h3 id="15-cnf">15. 连续归一化流（CNF）<a class="toc-link" href="#15-cnf" title="Permanent link">&para;</a></h3>
<p><strong>推导50：ODE形式的生成模型</strong></p>
<p>定义从简单分布 $p_0$ 到复杂分布 $p_1$ 的连续变换：</p>
<p>$$<br />
\frac{d\boldsymbol{z}_t}{dt} = \boldsymbol{f}_t(\boldsymbol{z}_t)<br />
$$</p>
<p>密度演化遵循Liouville方程：</p>
<p>$$<br />
\frac{\partial\log p_t(\boldsymbol{z}_t)}{\partial t} = -\text{Tr}\left(\frac{\partial\boldsymbol{f}_t}{\partial\boldsymbol{z}_t}\right)<br />
$$</p>
<p><strong>推导51：对数似然的计算</strong></p>
<p>$$<br />
\log p_1(\boldsymbol{z}_1) = \log p_0(\boldsymbol{z}_0) - \int_0^1 \text{Tr}\left(\frac{\partial\boldsymbol{f}_t}{\partial\boldsymbol{z}_t}\right)dt<br />
$$</p>
<p>通过ODE求解器可以精确计算。</p>
<h3 id="16">16. 总结与统一视角<a class="toc-link" href="#16" title="Permanent link">&para;</a></h3>
<p><strong>推导52：优化算法的统一框架</strong></p>
<p>所有基于梯度的优化都可以写成：</p>
<p>$$<br />
\frac{dx_t}{dt} = -M\nabla f(x_t)<br />
$$</p>
<p>其中 $M$ 是度量张量：</p>
<ul>
<li>欧氏空间：$M = I$（梯度下降）</li>
<li>黎曼流形：$M = G^{-1}$（自然梯度）</li>
<li>概率空间：$M$ 对应Wasserstein度量</li>
</ul>
<p><strong>推导53：信息几何的视角</strong></p>
<p>在概率流形上，Fisher度量定义为：</p>
<p>$$<br />
g_{ij}(\theta) = \mathbb{E}<em>{p</em>\theta}\left[\frac{\partial\log p_\theta}{\partial\theta_i}\frac{\partial\log p_\theta}{\partial\theta_j}\right]<br />
$$</p>
<p>自然梯度为：</p>
<p>$$<br />
\tilde{\nabla}<em>\theta = G^{-1}\nabla</em>\theta<br />
$$</p>
<h3 id="17">17. 实际应用中的技巧<a class="toc-link" href="#17" title="Permanent link">&para;</a></h3>
<p><strong>推导54：自适应步长</strong></p>
<p>使用线搜索选择步长 $\eta_t$：</p>
<p>$$<br />
\eta_t = \arg\min_\eta \mathcal{F}[p_t - \eta\nabla_W\mathcal{F}[p_t]]<br />
$$</p>
<p><strong>推导55：动量方法</strong></p>
<p>引入动量加速收敛：</p>
<p>$$<br />
\begin{aligned}<br />
\boldsymbol{v}<em t_1="t+1">{t+1} &amp;= \beta\boldsymbol{v}_t - \nabla_W\mathcal{F}[p_t] \<br />
p</em>} &amp;= p_t + \eta\boldsymbol{v}_{t+1<br />
\end{aligned}<br />
$$</p>
<p><strong>推导56：随机梯度</strong></p>
<p>当泛函涉及期望时：</p>
<p>$$<br />
\mathcal{F}[p] = \mathbb{E}_p[f(\boldsymbol{x})]<br />
$$</p>
<p>可以使用蒙特卡洛估计：</p>
<p>$$<br />
\nabla_W\mathcal{F}[p] \approx \frac{1}{N}\sum_{i=1}^N \nabla f(\boldsymbol{x}_i)<br />
$$</p>
<h3 id="_18">总结<a class="toc-link" href="#_18" title="Permanent link">&para;</a></h3>
<p>本节系统推导了概率空间中的优化理论，关键要点：</p>
<ol>
<li><strong>泛函导数</strong>：概率分布空间中的"梯度"概念</li>
<li><strong>Wasserstein梯度流</strong>：保持概率约束的梯度下降</li>
<li><strong>Fokker-Planck方程</strong>：描述概率密度的演化</li>
<li><strong>最优传输</strong>：定义概率分布之间的距离</li>
<li><strong>收敛性</strong>：通过Lyapunov函数和Fisher信息分析</li>
<li><strong>应用</strong>：生成模型、变分推断、采样算法等</li>
</ol>
<p>这些理论工具在现代机器学习中有广泛应用，特别是在生成模型、贝叶斯推断和强化学习等领域。</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="对齐全量微调这是我看过最精彩的lora改进二.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#152 对齐全量微调！这是我看过最精彩的LoRA改进（二）</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="cool-papers-站内搜索的一些新尝试.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#154 “Cool Papers + 站内搜索”的一些新尝试</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#_1">通向最优分布之路：概率空间的最小化</a><ul>
<li><a href="#_2">梯度下降</a><ul>
<li><a href="#_3">搜索视角</a></li>
<li><a href="#_4">投影下降</a></li>
</ul>
</li>
<li><a href="#_5">离散分布</a><ul>
<li><a href="#_6">拉氏乘子</a></li>
<li><a href="#_7">凸集搜索</a></li>
<li><a href="#_8">一个例子</a></li>
</ul>
</li>
<li><a href="#_9">连续分布</a><ul>
<li><a href="#_10">目标泛函</a></li>
<li><a href="#_11">紧支撑集</a></li>
<li><a href="#_12">旧路不通</a></li>
<li><a href="#_13">变量代换</a></li>
<li><a href="#_14">积分变换</a></li>
<li><a href="#_15">梯度之流</a></li>
</ul>
</li>
<li><a href="#_16">文章小结</a></li>
<li><a href="#_17">公式推导与注释</a><ul>
<li><a href="#1">1. 概率空间的数学定义</a></li>
<li><a href="#2">2. 泛函与泛函导数</a></li>
<li><a href="#3-kl">3. KL散度的变分推导</a></li>
<li><a href="#4-f-">4. f-散度的一般框架</a></li>
<li><a href="#5-wasserstein">5. Wasserstein距离与最优传输</a></li>
<li><a href="#6-wasserstein">6. Wasserstein梯度流的详细推导</a></li>
<li><a href="#7-fokker-planck">7. Fokker-Planck方程</a></li>
<li><a href="#8-jkoeuler">8. JKO格式（隐式Euler离散化）</a></li>
<li><a href="#9">9. 最优传输的计算方法</a></li>
<li><a href="#10">10. 与生成模型的联系</a></li>
<li><a href="#11">11. 收敛性分析</a></li>
<li><a href="#12">12. 扩展到约束优化</a></li>
<li><a href="#13">13. 数值实现的细节</a></li>
<li><a href="#14-steinsvgd">14. Stein变分梯度下降（SVGD）</a></li>
<li><a href="#15-cnf">15. 连续归一化流（CNF）</a></li>
<li><a href="#16">16. 总结与统一视角</a></li>
<li><a href="#17">17. 实际应用中的技巧</a></li>
<li><a href="#_18">总结</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>