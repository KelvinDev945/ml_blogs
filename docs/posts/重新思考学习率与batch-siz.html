<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>重新思考学习率与Batch Siz... | ML & Math Blog Posts</title>
    <meta name="description" content="重新思考学习率与Batch Siz...&para;
原文链接: https://spaces.ac.cn/archives/11301
发布日期: 

我们在《重新思考学习率与Batch Size（二）：平均场》中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在《配置不同的学习率，LoRA还能再涨一点？》、...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=学习率">学习率</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #223 重新思考学习率与Batch Siz...
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#223</span>
                重新思考学习率与Batch Siz...
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-09-22</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=学习率" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 学习率</span>
                </a>
                
                <a href="../index.html?tags=优化器" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 优化器</span>
                </a>
                
                <a href="../index.html?tags=尺度定律" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 尺度定律</span>
                </a>
                
                <a href="../index.html?tags=平均场" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 平均场</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="batch-siz">重新思考学习率与Batch Siz...<a class="toc-link" href="#batch-siz" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11301">https://spaces.ac.cn/archives/11301</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>我们在<a href="/archives/11280">《重新思考学习率与Batch Size（二）：平均场》</a>中提到，关注SignSGD的原因之一是我们通常将它作为Adam的理论近似，这是Adam做理论分析时常用的简化策略。除了分析学习率的场景外，在<a href="/archives/10001">《配置不同的学习率，LoRA还能再涨一点？》</a>、<a href="/archives/10770">《初探MuP：超参数的跨模型尺度迁移规律》</a>等地方我们也用了这个简化。</p>
<p>然而，SignSGD真是Adam的良好近似吗？一个明显差异是SignSGD的Update RMS总是1，而Adam并非如此。笔者发现，导致这一差异的核心原因是动量，它普遍存在于Adam、Lion、Muon等优化器中。所以，本文我们来考察动量——更广义地说是EMA——的影响。</p>
<h2 id="_1">问题分析<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>从Adam的视角看，SignSGD对应$\beta_1=\beta_2=0$这个特例，或者对应于Adam的第一步更新量（不管$\beta_1,\beta_2$如何）。因此，我们认为它跟Adam肯定有一些共性，能够捕捉到一些通用的规律。</p>
<p>但是，它们之间也有一些明显的差异。比较典型的就是Update RMS的差异，SignSGD总是1，但Adam往往明显小于1；还有，Adam看上去更贴近SGD，它更像是SignSGD和SGD的一个中间版本。一开始，笔者以为这是Adam分母中的$\epsilon$导致的差异，所以在<a href="/archives/10563">《Adam的epsilon如何影响学习率的Scaling Law？》</a>还特意计算了带$\epsilon$的SoftSignSGD。</p>
<p>后来，我们在<a href="/archives/11267">《为什么Adam的Update RMS是0.2？》</a>从模拟和理论两方面估计了Adam的Update RMS，其实平均场近似的估计结果为$\sqrt{\frac{1-\beta_1}{1+\beta_1}}$，并且验证了它跟模拟结果和实际实验都很吻合。这个结果显式地依赖于$\beta_1$，所以很明显，它将我们的思考方向引向动量。</p>
<p>这就有了下面的分析过程。综下所述，我们可以确认，$\epsilon$的角色确实是次要的，真正的主角其实是动量——它是梯度的“滑动平均”——这也正是本文的主角“EMA（Exponential Moving Average）”。</p>
<h2 id="_2">梯度下降<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>为了分析EMA带来的变数，我们从SGDM入手，也就是带动量的SGD，实际上我们在用SGD的时候极少情况是不加动量的：<br />
\begin{equation}\begin{aligned}<br />
&amp;\boldsymbol{m}<em t-1="t-1">t = \beta_1 \boldsymbol{m}</em>} + \left(1 - \beta_1\right) \boldsymbol{g<em t-1="t-1">t \\[4pt]<br />
&amp;\boldsymbol{w}_t = \boldsymbol{w}</em>} - \eta_t \boldsymbol{m<em B_t="B,t">t<br />
\end{aligned}\end{equation}<br />
实际使用中，$\boldsymbol{g}_t$替换为$\tilde{\boldsymbol{g}}</em>}$，它是一个随机变量，均值为$\boldsymbol{g<em B_t="B,t">t$，协方差矩阵为$\boldsymbol{\Sigma}_t/B$，这些基本设置跟<a href="/archives/11260">《重新思考学习率与Batch Size（一）：现状》</a>是一样的。这里的噪声，是由随机采样不同的Batch引起的，所以我们可以合理地假设，不同$t$之间的$\tilde{\boldsymbol{g}}</em>$是相互独立的。</p>
<p>我们的任务，是计算<br />
\begin{equation}\newcommand{tr}{\mathop{\text{tr}}}\eta^* \approx \frac{\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em s="1">B]^{\top}\boldsymbol{g}}{\tr(\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]\boldsymbol{H})}\label{eq:eta-opt}\end{equation}<br />
相关推导在前面几篇文章已经给出，这里就不再重复。对于SGDM来说$\tilde{\boldsymbol{\varphi}}_B = \boldsymbol{m}_t$，它可以展开成<br />
\begin{equation}\boldsymbol{m}_t = (1 - \beta_1)\sum\limits</em>}^t \beta_1^{t-s}\tilde{\boldsymbol{g}}_{B,s}\end{equation</p>
<h2 id="_3">放大批量<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>现在可以计算<br />
\begin{equation}\mathbb{E}[\boldsymbol{m}<em s="1">t] = (1 - \beta_1)\sum</em>}^t \beta_1^{t-s}\mathbb{E}[\tilde{\boldsymbol{g}<em s="1">{B,s}] = (1 - \beta_1)\sum</em>}^t \beta_1^{t-s}\boldsymbol{g<em s="1">s\end{equation}<br />
我们进一步假设当模型训练进入“正轨”后，梯度是缓变的，那么我们可以用当前梯度$\boldsymbol{g}_t$近似$\boldsymbol{g}_s$，得到<br />
\begin{equation}\mathbb{E}[\boldsymbol{m}_t] = (1 - \beta_1)\sum</em>}^t \beta_1^{t-s}\boldsymbol{g<em s="1">t = (1 - \beta_1^t) \boldsymbol{g}_t \approx \boldsymbol{g}_t \qquad (t\to\infty)\end{equation}<br />
至于$\mathbb{E}[\boldsymbol{m}_t \boldsymbol{m}_t^{\top}]$，我们利用恒等式$\mathbb{E}[\boldsymbol{m}_t \boldsymbol{m}_t^{\top}] = \mathbb{E}[\boldsymbol{m}_t] \mathbb{E}[\boldsymbol{m}_t]^{\top} + \mathbb{C}\text{ov}[\boldsymbol{m}_t,\boldsymbol{m}_t]$，然后利用方差的可加性得到：<br />
\begin{equation}\mathbb{C}\text{ov}[\boldsymbol{m}_t,\boldsymbol{m}_t] = (1 - \beta_1)^2\sum</em>}^t \beta_1^{2(t-s)}\boldsymbol{\Sigma<em s="1">s/B\end{equation}<br />
类似地，我们假设协方差矩阵的缓变性，那么<br />
\begin{equation}\mathbb{C}\text{ov}[\boldsymbol{m}_t] \approx (1 - \beta_1)^2\sum</em>}^t \beta_1^{2(t-s)}\boldsymbol{\Sigma<em _max="\max">t/B = (1 - \beta_1)^2\frac{1-\beta_1^{2t}}{1-\beta_1^2}\boldsymbol{\Sigma}_t/B = \frac{1 - \beta_1}{1 + \beta_1}\boldsymbol{\Sigma}_t/B \qquad (t\to\infty)\end{equation}<br />
代入式$\eqref{eq:eta-opt}$得<br />
\begin{equation}\eta^* \approx \frac{\eta</em>}}{1 + \frac{1 - \beta_1}{1 + \beta_1}\mathcal{B<em _max="\max">{\text{noise}}/B},\qquad \eta</em>} = \frac{\boldsymbol{g}^{\top}\boldsymbol{g}}{\boldsymbol{g}^{\top}\boldsymbol{H}\boldsymbol{g}},\quad\mathcal{B}_{\text{noise}} = \frac{\tr(\boldsymbol{\Sigma}\boldsymbol{H})}{\boldsymbol{g}^{\top}\boldsymbol{H}\boldsymbol{g}}\end{equation<br />
从这个结果可以看出，动量机制的引入，相当于把SGD的Batch Size放大到了$\frac{1 + \beta_1}{1 - \beta_1}$倍。按照笔者的理解，动量就是通过对优化轨迹上的梯度做EMA来低成本地消除梯度噪声，所以这个结果这跟笔者所理解的动量意义是相符的。</p>
<h2 id="_4">符号动量<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>进一步地，我们考虑SignSGDM，它可以视作<a href="/archives/9473">Lion</a>的一个特例，也就是SGDM多加了个$\newcommand{sign}{\mathop{\text{sign}}}\sign$：<br />
\begin{equation}\begin{aligned}<br />
&amp;\boldsymbol{m}<em t-1="t-1">t = \beta_1 \boldsymbol{m}</em>} + \left(1 - \beta_1\right) \boldsymbol{g<em t-1="t-1">t \\[4pt]<br />
&amp;\boldsymbol{w}_t = \boldsymbol{w}</em>} - \eta_t \sign(\boldsymbol{m<em B_t="B,t">t)<br />
\end{aligned}\end{equation}<br />
实际训练中$\boldsymbol{g}_t$同样替换为$\tilde{\boldsymbol{g}}</em>}$。对SignSGDM来说$\tilde{\boldsymbol{\varphi}<em _text_simple="\text{simple">B = \sign(\boldsymbol{m}_t)$，那么根据平均场近似得<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B] = \mathbb{E}\bigg[\frac{\boldsymbol{m}_t}{\sqrt{\boldsymbol{m}_t^2}}\bigg]\approx \frac{\mathbb{E}[\boldsymbol{m}_t]}{\sqrt{\mathbb{E}[\boldsymbol{m}_t^2]}}\end{equation}<br />
其中向量乘法默认是Hadamard积。分子$\mathbb{E}[\boldsymbol{m}_t]$我们在上一节已经算了，分母$\mathbb{E}[\boldsymbol{m}_t^2]$其实等于$\newcommand{diag}{\mathop{\text{diag}}}\diag(\mathbb{E}[\boldsymbol{m}_t \boldsymbol{m}_t^{\top}])$，所以也可以代入上一节的结果，得到<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B] \approx \frac{\boldsymbol{g}_t}{\sqrt{\boldsymbol{g}_t^2 + \frac{1 - \beta_1}{1 + \beta_1}\boldsymbol{\sigma}_t^2/B}} = \frac{\sign(\boldsymbol{g}_t)}{\sqrt{1 + \frac{1 - \beta_1}{1 + \beta_1}(\boldsymbol{\sigma}_t^2/\boldsymbol{g}_t^2)/B}} \approx \frac{\sign(\boldsymbol{g}_t)}{\sqrt{1 + \frac{1 - \beta_1}{1 + \beta_1} \mathcal{B}</em>}}/B}}\end{equation<br />
其中$\boldsymbol{\sigma}<em _text_simple="\text{simple">t^2 = \diag(\boldsymbol{\Sigma}_t), \mathcal{B}</em>$倍。}} = \tr(\boldsymbol{\Sigma}_t)/\boldsymbol{g}_t^{\top}\boldsymbol{g}_t$。上式相当于SignSGD的$B$换成了$\frac{1 + \beta_1}{1 - \beta_1}B$，如果我们进一步计算$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B\tilde{\boldsymbol{\varphi}}_B^{\top}]$就会发现结论也是如此。所以，跟SGDM一样，动量相当于把SignSGD的Batch Size放大到了$\frac{1 + \beta_1}{1 - \beta_1</p>
<p>在<a href="/archives/11285">《重新思考学习率与Batch Size（三）：Muon》</a>中我们计算过Muon的学习率规律，发现它跟SignSGD一致，所以我们可以断言，动量在Muon中的作用跟SignSGDM一样，都约等于将Batch Size放大成$\frac{1 + \beta_1}{1 - \beta_1}$倍。</p>
<h2 id="_5">双重滑动<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>最后我们来看Adam：<br />
\begin{equation}\begin{aligned}<br />
&amp;\boldsymbol{m}<em t-1="t-1">t = \beta_1 \boldsymbol{m}</em>} + \left(1 - \beta_1\right) \boldsymbol{g<em t-1="t-1">t\\<br />
&amp;\boldsymbol{v}_t = \beta_2 \boldsymbol{v}</em>} + \left(1 - \beta_2\right) \boldsymbol{g<em t-1="t-1">t^2\\<br />
&amp;\hat{\boldsymbol{m}}_t = \boldsymbol{m}_t\left/\left(1 - \beta_1^t\right)\right.\\<br />
&amp;\hat{\boldsymbol{v}}_t = \boldsymbol{v}_t\left/\left(1 - \beta_2^t\right)\right.\\<br />
&amp;\boldsymbol{\theta}_t = \boldsymbol{\theta}</em>} - \eta_t \hat{\boldsymbol{m}<em B_t="B,t">t\left/\left(\sqrt{\hat{\boldsymbol{v}}_t} + \epsilon\right)\right.<br />
\end{aligned}\end{equation}<br />
实际训练中$\boldsymbol{g}_t$替换为$\tilde{\boldsymbol{g}}</em>_t$。}$。我们考虑的都是训练已经进入“正轨”的状态，即$t\to\infty$，所以不区分$\boldsymbol{m}_t$和$\hat{\boldsymbol{m}}_t$、$\boldsymbol{v}_t$和$\hat{\boldsymbol{v}}_t$，同时我们聚焦于EMA的作用，所以设$\epsilon = 0$。那么对于Adam来说有$\tilde{\boldsymbol{\varphi}}_B=\boldsymbol{m}_t/\sqrt{\boldsymbol{v}_t}$，它跟SignSGDM的区别，就是分母的$\boldsymbol{m}_t^2$换成了另一个EMA的统计量$\boldsymbol{v</p>
<p>由平均场近似得<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em s="1">B] = \mathbb{E}\bigg[\frac{\boldsymbol{m}_t}{\sqrt{\boldsymbol{v}_t}}\bigg]\approx \frac{\mathbb{E}[\boldsymbol{m}_t]}{\sqrt{\mathbb{E}[\boldsymbol{v}_t]}}\end{equation}<br />
$\mathbb{E}[\boldsymbol{m}_t]$我们已经算过，只需算$\mathbb{E}[\boldsymbol{v}_t]$：<br />
\begin{equation}\mathbb{E}[\boldsymbol{v}_t] = (1 - \beta_2)\sum</em>}^t \beta_2^{t-s}\mathbb{E}[\tilde{\boldsymbol{g}<em s="1">{B,s}^2] = (1 - \beta_2)\sum</em>}^t \beta_2^{t-s}(\boldsymbol{g<em _text_simple="\text{simple">s^2 + \boldsymbol{\sigma}_s^2/B)\approx \boldsymbol{g}_t^2 + \boldsymbol{\sigma}_t^2/B\end{equation}<br />
跟前面一样，最后一个约等号假设了梯度和方差的缓变性，以及$t\to\infty$。于是我们有<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B] \approx \frac{\boldsymbol{g}_t}{\sqrt{\boldsymbol{g}_t^2 + \boldsymbol{\sigma}_t^2/B}} \approx \frac{\sign(\boldsymbol{g}_t)}{\sqrt{1 + \mathcal{B}</em>}}/B}}\end{equation<br />
这个结果倒是跟SignSGD相同，所以单从一阶矩看，SignSGD作为Adam的近似是合理的。但我们还有二阶矩$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B \tilde{\boldsymbol{\varphi}}_B^{\top}]$，在分量独立的假设下，我们只需要算$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B^2]$：<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B^2] = \mathbb{E}\bigg[\frac{\boldsymbol{m}_t^2}{\boldsymbol{v}_t}\bigg]\approx \frac{\mathbb{E}[\boldsymbol{m}_t^2]}{\mathbb{E}[\boldsymbol{v}_t]} \approx \frac{\boldsymbol{g}_t^2 + \frac{1 - \beta_1}{1 + \beta_1}\boldsymbol{\sigma}_t^2/B}{\boldsymbol{g}_t^2 + \boldsymbol{\sigma}_t^2/B}\label{eq:u2-adam}\end{equation}</p>
<h2 id="_6">两个特例<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>我们观察两个特例。首先是$\beta_1=0$，这时候分子分母相同，$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B^2]$是全1向量，跟SignSGD一致。所以说，SignSGD是$\beta_1=0$的Adam——也就是<a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a>——的良好近似，当$\beta_1$增大时，近似程度开始变差。</p>
<p>当$\beta_1=1$时，我们有<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em _text_simple="\text{simple">B^2] \approx \frac{\boldsymbol{g}_t^2}{\boldsymbol{g}_t^2 + \boldsymbol{\sigma}_t^2/B}\approx \mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^2\end{equation}<br />
由此得到$\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B \tilde{\boldsymbol{\varphi}}_B^{\top}] \approx \mathbb{E}[\tilde{\boldsymbol{\varphi}}_B] \mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top}$，代入到式$\eqref{eq:eta-opt}$得<br />
\begin{equation}\eta^* \approx \frac{\Vert \boldsymbol{g}\Vert_1 \sqrt{1 + \mathcal{B}</em>}}/B}}{\sign(\boldsymbol{g})^{\top} \boldsymbol{H} \sign(\boldsymbol{g})}\end{equation<br />
注意，它是关于$B$的单调递减函数，即当Batch Size增大时学习率应该减小。由此我们可以推测，Adam的$\beta_1$的增大，将会加速“<a href="/archives/11280#%E5%8F%8D%E5%B8%B8%E7%8E%B0%E8%B1%A1">Surge现象</a>”的出现。</p>
<p>这个结论看似有点费解，但其实换个角度就容易理解了。“Surge现象”指当Batch Size超过某个阈值后，最优学习率随着Batch Size的增大而减少，而前面SGDM、SignSGDM的结果都表明，动量的引入约等于将Batch Size扩大到$\frac{1 + \beta_1}{1 - \beta_1} &gt; 1$倍，这自然增加了超过阈值的可能性。</p>
<p>换句话说，“随着$\beta_1$的增大，‘Surge现象’将更容易出现”的结论，即便对于SignSGDM也是成立的。而Adam相比SignSGDM有一些新的特性，但“动量机制约等于放大Batch Size”这一点始终是成立的，所以出现同样的结论就不难理解了。</p>
<h2 id="_7">一般分析<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h2>
<p>我们改写一下式$\eqref{eq:u2-adam}$：<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}<em i_i="i,i">B^2] \approx \frac{\boldsymbol{g}_t^2 + \frac{1 - \beta_1}{1 + \beta_1}\boldsymbol{\sigma}_t^2/B}{\boldsymbol{g}_t^2 + \boldsymbol{\sigma}_t^2/B} = \frac{2\beta_1}{1+\beta_1}\frac{\boldsymbol{g}_t^2}{\boldsymbol{g}_t^2 + \boldsymbol{\sigma}_t^2/B} + \frac{1 - \beta_1}{1 + \beta_1} \approx \frac{2\beta_1}{1+\beta_1}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^2 + \frac{1 - \beta_1}{1 + \beta_1}\end{equation}<br />
由此我们可以写出<br />
\begin{equation}\mathbb{E}[\tilde{\boldsymbol{\varphi}}_B \tilde{\boldsymbol{\varphi}}_B^{\top}] \approx \mathbb{E}[\tilde{\boldsymbol{\varphi}}_B] \mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^{\top} + \frac{1 - \beta_1}{1 + \beta_1}\diag\left(1 - \mathbb{E}[\tilde{\boldsymbol{\varphi}}_B]^2\right)\end{equation}<br />
那么<br />
\begin{equation}\eta^<em> \approx \frac{\sum_i |g_i|}{\frac{1}{\beta}\frac{1 - \beta_1}{1 + \beta_1}\sum_i H_{i,i} + \beta\left(\sum_{i,j} H_{i,j}\sign(g_i g_j) - \frac{1 - \beta_1}{1 + \beta_1}\sum_i H_{i,i}\right)}\end{equation}<br />
这里没有下标的$\beta$等于$(1 + \mathcal{B}_{\text{simple}}/B)^{-1/2}$，不仔细看的话可能会跟$\beta_1,\beta_2$混淆，笔者表示很抱歉，因为这是前两篇文章的记号，这里只好沿用了。跟SignSGD不同的是，SignSGD如果假设Hessian矩阵是对角阵，那么就不会出现Surge现象，但上式即便是在对角Hessian假设下依然出现Surge现象，此时：<br />
\begin{equation}\eta^</em> \approx \frac{\sum_i |g_i|}{\left(\frac{1}{\beta}\frac{1 - \beta_1}{1 + \beta_1} + \beta\frac{2\beta_1}{1 + \beta_1}\right)\sum_i H</em>}}\end{equation<br />
由均值不等式知上式在$\beta^<em>=\sqrt{\frac{1-\beta_1}{2\beta_1}}$处取到最大值，但要注意根据$\beta$定义，它是$\in(0,1)$的，所以还要判断$\beta^</em>\in(0,1)$，即$\beta_1 &gt; 1/3$，不满足这个条件时最大值依然在$\beta=1$取到，此时没有Surge现象。反之，当$\beta_1 &gt; 1/3$且$\beta &gt; \beta^*$（即$B &gt; \frac{1-\beta_1}{3\beta_1-1}\mathcal{B}_{\text{simple}}$）时，学习率应该随着Batch Size的增加而减小。</p>
<p>这个结论可以初步解释为啥Muon能支持更大Batch Size。由<a href="/archives/11285">《重新思考学习率与Batch Size（三）：Muon》</a>可知，Muon的表现跟SignSGDM类似，在特定Hessian结构假设下它不会出现Surge现象，这意味着增大Batch Size总可以提高学习效率，尽管相对收益会越来越小。</p>
<p>相反，Adam在常用设置（如$\beta_1=0.9$）下，哪怕假设Hessian是对角阵也会出现Surge现象，这意味着Batch Size超过一定值后，学习效率就下降了。</p>
<h2 id="_8">文章小结<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h2>
<p>本文初步分析了优化器的EMA机制对学习率与Batch Size的尺度定律的影响，确认了EMA特别是动量机制的引入会稍微改变尺度定律，而Adam这种带有双重EMA运算的优化器，则会呈现出一些跟SignSGD不同的新特性。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11301">https://spaces.ac.cn/archives/11301</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Sep. 22, 2025). 《重新思考学习率与Batch Size（四）：EMA 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11301">https://spaces.ac.cn/archives/11301</a></p>
<p>@online{kexuefm-11301,<br />
title={重新思考学习率与Batch Size（四）：EMA},<br />
author={苏剑林},<br />
year={2025},<br />
month={Sep},<br />
url={\url{https://spaces.ac.cn/archives/11301}},<br />
} </p>
<hr />
<h2 id="_9">公式推导与注释<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h2>
<p>TODO: 添加详细的数学公式推导和注释</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="重新思考学习率与batch-size四ema.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#222 重新思考学习率与Batch Size（四）：EMA</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="adamw的weight-rms的.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#224 AdamW的Weight RMS的...</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#batch-siz">重新思考学习率与Batch Siz...</a><ul>
<li><a href="#_1">问题分析</a></li>
<li><a href="#_2">梯度下降</a></li>
<li><a href="#_3">放大批量</a></li>
<li><a href="#_4">符号动量</a></li>
<li><a href="#_5">双重滑动</a></li>
<li><a href="#_6">两个特例</a></li>
<li><a href="#_7">一般分析</a></li>
<li><a href="#_8">文章小结</a></li>
<li><a href="#_9">公式推导与注释</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>