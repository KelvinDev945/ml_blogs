<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>msign的导数 | ML & Math Blog Posts</title>
    <meta name="description" content="msign的导数&para;
原文链接: https://spaces.ac.cn/archives/11025
发布日期: 

这篇文章我们来推导$\newcommand{msign}{\mathop{\text{msign}}}\msign$算子的求导公式。如果读者想要像《Test-Time Training Done Right》一样，将TTT和Muon结合起来，那么本文可能会对你有帮助。...">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">

    <!-- Google Fonts -->
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../assets/css/main.css">
    <link rel="stylesheet" href="../assets/css/post.css">

    <!-- Custom JS -->
    <script src="../assets/js/collapsible.js" defer></script>

    <!-- MathJax for math rendering -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Syntax highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/github.min.css">
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/highlight.min.js"></script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light bg-light border-bottom">
        <div class="container">
            <a class="navbar-brand" href="../index.html">
                <i class="fas fa-brain"></i> ML & Math Blog
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="../index.html"><i class="fas fa-home"></i> 首页</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Post Content -->
    <article class="container post-container my-5">
        <!-- Breadcrumb Navigation -->
        <nav aria-label="breadcrumb">
            <ol class="breadcrumb">
                <li class="breadcrumb-item"><a href="../index.html"><i class="fas fa-home"></i> 首页</a></li>
                
                <li class="breadcrumb-item"><a href="../index.html?tags=微积分">微积分</a></li>
                
                <li class="breadcrumb-item active" aria-current="page">
                    #202 msign的导数
                </li>
            </ol>
        </nav>

        <!-- Post Header -->
        <header class="post-header mb-4">
            <h1 class="post-title">
                <span class="post-number">#202</span>
                msign的导数
            </h1>
            <div class="post-meta">
                <span><i class="far fa-calendar"></i> 2025-06-13</span>
                
            </div>
            
            <div class="post-tags mt-3">
                
                <a href="../index.html?tags=微积分" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 微积分</span>
                </a>
                
                <a href="../index.html?tags=矩阵" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 矩阵</span>
                </a>
                
                <a href="../index.html?tags=梯度" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 梯度</span>
                </a>
                
                <a href="../index.html?tags=muon" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> muon</span>
                </a>
                
                <a href="../index.html?tags=生成模型" class="tag-link">
                    <span class="tag"><i class="fas fa-tag"></i> 生成模型</span>
                </a>
                
            </div>
            
        </header>

        <div class="row">
            <!-- Main Content -->
            <div class="col-lg-9">
                <!-- Post Body -->
                <div class="post-content">
                    <h1 id="msign">msign的导数<a class="toc-link" href="#msign" title="Permanent link">&para;</a></h1>
<p><strong>原文链接</strong>: <a href="https://spaces.ac.cn/archives/11025">https://spaces.ac.cn/archives/11025</a></p>
<p><strong>发布日期</strong>: </p>
<hr />
<p>这篇文章我们来推导$\newcommand{msign}{\mathop{\text{msign}}}\msign$算子的求导公式。如果读者想要像<a href="https://papers.cool/arxiv/2505.23884">《Test-Time Training Done Right》</a>一样，将<a href="https://papers.cool/arxiv/2407.04620">TTT</a>和<a href="/archives/10592">Muon</a>结合起来，那么本文可能会对你有帮助。</p>
<h2 id="_1">两种定义<a class="toc-link" href="#_1" title="Permanent link">&para;</a></h2>
<p>本文依然假设大家已经对$\msign$有所了解，如果还没有，可以先移步阅读<a href="/archives/10592">《Muon优化器赏析：从向量到矩阵的本质跨越》</a>和<a href="/archives/10922">《msign算子的Newton-Schulz迭代（上）》</a>。现设有矩阵$\boldsymbol{M}\in\mathbb{R}^{n\times m}$，那么<br />
\begin{equation}\boldsymbol{U},\boldsymbol{\Sigma},\boldsymbol{V}^{\top} = \text{SVD}(\boldsymbol{M}) \quad\Rightarrow\quad \msign(\boldsymbol{M}) = \boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>}^{\top}\end{equation<br />
其中$\boldsymbol{U}\in\mathbb{R}^{n\times n},\boldsymbol{\Sigma}\in\mathbb{R}^{n\times m},\boldsymbol{V}\in\mathbb{R}^{m\times m}$，$r$是$\boldsymbol{M}$的秩。简单来说，$\msign$就是把矩阵的所有非零奇异值都变成1后所得的新矩阵。基于SVD，我们还可以证明<br />
\begin{equation}\msign(\boldsymbol{M}) = (\boldsymbol{M}\boldsymbol{M}^{\top})^{-1/2}\boldsymbol{M}= \boldsymbol{M}(\boldsymbol{M}^{\top}\boldsymbol{M})^{-1/2}\end{equation}<br />
这里的$^{-1/2}$的矩阵的$1/2$次幂的逆，由于$\boldsymbol{M}\boldsymbol{M}^{\top}$和$\boldsymbol{M}^{\top}\boldsymbol{M}$是（半）正定对称的，所以$1/2$次幂总是可求，但逆未必，不可逆的时候我们可以用“<a href="/archives/10366">伪逆</a>”。$\msign$这个名字，源于上式与实数符号函数$\newcommand{sign}{\mathop{\text{sign}}}\sign(x) = x/\sqrt{x^2}$的相似性。然而，我们之前也提到过，符号函数还有<a href="https://en.wikipedia.org/wiki/Matrix_sign_function">另一个矩阵版</a>，这里称为$\newcommand{mcsgn}{\mathop{\text{mcsgn}}}\newcommand{csgn}{\mathop{\text{csgn}}}\mcsgn$：<br />
\begin{equation}\mcsgn(\boldsymbol{M}) = \boldsymbol{M}(\boldsymbol{M}^2)^{-1/2}\end{equation}<br />
即$\msign$的$\boldsymbol{M}^{\top}\boldsymbol{M}$换成了$\boldsymbol{M}^2$。由于只有方阵才能算平方，所以这种定义只适用于方阵。在一篇文章内引入两种相似但不同的定义是容易引起混淆的，但很不幸，后面的计算中两种定义都需要用到，所以不得不同时出现。</p>
<p>$\mcsgn$具备相似不变性，如果$\boldsymbol{M}=\boldsymbol{P}\boldsymbol{\Lambda}\boldsymbol{P}^{-1}$，那么$\mcsgn(\boldsymbol{M})=\boldsymbol{P}\mcsgn(\boldsymbol{\Lambda})\boldsymbol{P}^{-1}$。进一步地，如果$\boldsymbol{\Lambda}$是对角阵（在复数域内几乎总是可以做到），那么有<br />
\begin{equation}\mcsgn(\boldsymbol{M}) = \boldsymbol{P}\csgn(\boldsymbol{\Lambda})\boldsymbol{P}^{-1}\end{equation}<br />
$\csgn(\boldsymbol{\Lambda})$表示对角线的元素都取$\csgn$，其中$\csgn(z) = z/\sqrt{z^2}$是符号函数的复数版，如果$z$的实部非零那么它等于$\sign(\mathop{\text{Re}}[z])$。这样看来，$\msign$和$\mcsgn$的区别在于，前者是在奇异值分解基础上对奇异值取符号函数，后者是在特征值分解基础上对特征值取符号函数。当$\boldsymbol{M}$是对称矩阵时，它们是相等的。</p>
<h2 id="_2">同一计算<a class="toc-link" href="#_2" title="Permanent link">&para;</a></h2>
<p>目前而言，$\msign$的数值计算主要靠如下格式的“Newton-Schulz迭代”：<br />
\begin{equation}\boldsymbol{X}<em t_1="t+1">0 = \frac{\boldsymbol{M}}{\Vert\boldsymbol{M}\Vert_F},\qquad \boldsymbol{X}</em>} = a_{t+1}\boldsymbol{X<em t_1="t+1">t + b</em>}\boldsymbol{X<em t_1="t+1">t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t) + c</em>}\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t)^2\end{equation<br />
至于系数的选择，这我们在<a href="/archives/10922">《msign算子的Newton-Schulz迭代（上）》</a>和<a href="/archives/10996">《msign算子的Newton-Schulz迭代（下）》</a>已经详细探讨过了，其中出自下篇的比较新的结果是：<br />
$$\begin{array}{c|ccc}<br />
\hline<br />
t &amp; a\times 1.01 &amp; b\times 1.01^3 &amp; c\times 1.01^5 \\<br />
\hline<br />
\quad 1\quad &amp; 8.28721 &amp; -23.5959 &amp; 17.3004 \\<br />
2 &amp; 4.10706 &amp; -2.94785 &amp; 0.544843 \\<br />
3 &amp; 3.94869 &amp; -2.9089 &amp; 0.551819 \\<br />
4 &amp; 3.31842 &amp; -2.48849 &amp; 0.510049 \\<br />
5 &amp; 2.30065 &amp; -1.6689 &amp; 0.418807 \\<br />
6 &amp; 1.8913 &amp; 1.268 &amp; 0.376804 \\<br />
7 &amp; 1.875 &amp; -1.25 &amp; 0.375 \\<br />
8 &amp; 1.875 &amp; -1.25 &amp; 0.375 \\<br />
\hline<br />
\end{array}$$<br />
这个结果的好处是可以任意截断和叠加，比如只保留前5行它就是最佳的5步迭代，保留前6行就是最佳6步迭代，并且近似程度是有保证地优于5步迭代，依此类推。</p>
<p>至于$\mcsgn$，它只是把$\msign$的$\boldsymbol{M}^{\top}\boldsymbol{M}$换成了$\boldsymbol{M}^2$，所以理论上也可以用Newton-Schulz迭代，但由于特征值可以是复数，因此一般的收敛会困难得多。不过，如果我们可以实现确认矩阵$\boldsymbol{M}$的特征值都是实数（比如本文后面要应用$\mcsgn$的分块三角矩阵），那么就可以复用$\msign$的迭代和系数：<br />
\begin{equation}\newcommand{tr}{\mathop{\text{tr}}}\boldsymbol{X}<em t_1="t+1">0 = \frac{\boldsymbol{M}}{\sqrt{\tr(\boldsymbol{M}^2)}},\qquad \boldsymbol{X}</em>} = a_{t+1}\boldsymbol{X<em t_1="t+1">t + b</em>}\boldsymbol{X<em t_1="t+1">t^3 + c</em>}\boldsymbol{X}_t^5\end{equation</p>
<h2 id="_3">推导过程<a class="toc-link" href="#_3" title="Permanent link">&para;</a></h2>
<p>下面正式进入主题——求$\boldsymbol{O}=\msign(\boldsymbol{M})$的导数。如果读者只是将Muon当成一个普通优化器用，那么本文多半跟你无关了。当我们需要参考TTT，用Muon优化器来构建RNN模型时，才需要$\msign$的导数，此时$\msign$在模型表现为前向传播，而要对整个模型反向传播，自然就涉及到了$\msign$的导数。</p>
<p>由于$\msign$是通过Newton-Schulz迭代计算的，实际上它可以直接反向传播，所以$\msign$的数值求导本身不是问题，但基于迭代反传意味着有很多中间状态要存，显存往往要爆炸，所以希望能得到导数的解析解来简化。另一方面，在<a href="/archives/10878">《SVD的导数》</a>中我们其实也求过$\msign$的导数，但那是基于SVD的表达式，而SVD并不是GPU高效的算法。</p>
<p>所以，我们的目的是寻求一个不依赖于SVD的、能够高效计算的结果。我们从恒等式<br />
\begin{equation}\boldsymbol{M} = \boldsymbol{O}\boldsymbol{M}^{\top}\boldsymbol{O}\end{equation}<br />
出发（由$\msign$的定义即可证明），两边微分得到<br />
\begin{equation}d\boldsymbol{M} = (d\boldsymbol{O})\boldsymbol{M}^{\top}\boldsymbol{O} + \boldsymbol{O}(d\boldsymbol{M}^{\top})\boldsymbol{O} + \boldsymbol{O}\boldsymbol{M}^{\top}(d\boldsymbol{O})\label{eq:dm-do}\end{equation}<br />
这个结果的难度在于无法简单地分离出$d\boldsymbol{M}=f(d\boldsymbol{O})$或$d\boldsymbol{O}=f(d\boldsymbol{M})$的形式，因此不大好看出$\nabla_{\boldsymbol{O}}\mathcal{L}$与$\nabla_{\boldsymbol{W}}\mathcal{L}$的关系（$\mathcal{L}$是损失函数）。这种情况下最好的办法是回归到矩阵求导的根本思路——“迹技巧”：</p>
<blockquote>
<p><strong>迹技巧（trace trick）</strong> 如果我们能找到跟$\boldsymbol{M}$同形状的矩阵$\boldsymbol{G}$满足 \begin{equation}d\mathcal{L}=\langle \boldsymbol{G}, d\boldsymbol{M}\rangle_F = \tr(\boldsymbol{G}^{\top} (d\boldsymbol{M}))\end{equation} 那么$\boldsymbol{G} = \nabla_{\boldsymbol{M}}\mathcal{L}$。</p>
</blockquote>
<p>迹技巧的要义是化矩阵/向量为标量，然后化标量为迹，继而可以利用迹的恒等式：<br />
\begin{equation}\tr(\boldsymbol{A}\boldsymbol{B}) = \tr(\boldsymbol{B}\boldsymbol{A}) = \tr(\boldsymbol{A}^{\top}\boldsymbol{B}^{\top}) = \tr(\boldsymbol{B}^{\top}\boldsymbol{A}^{\top})\end{equation}<br />
现在设$\boldsymbol{X}$是任意跟$\boldsymbol{M}$同形状矩阵，给式$\eqref{eq:dm-do}$两边乘$\boldsymbol{X}^{\top}$，然后求迹<br />
\begin{equation}\begin{aligned}<br />
\tr(\boldsymbol{X}^{\top}(d\boldsymbol{M})) =&amp;\, \tr(\boldsymbol{X}^{\top}(d\boldsymbol{O})\boldsymbol{M}^{\top}\boldsymbol{O}) + \tr(\boldsymbol{X}^{\top}\boldsymbol{O}(d\boldsymbol{M}^{\top})\boldsymbol{O}) + \tr(\boldsymbol{X}^{\top}\boldsymbol{O}\boldsymbol{M}^{\top}(d\boldsymbol{O})) \\[7pt]<br />
=&amp;\, \tr(\boldsymbol{M}^{\top}\boldsymbol{O}\boldsymbol{X}^{\top}(d\boldsymbol{O})) + \tr(\boldsymbol{O}\boldsymbol{X}^{\top}\boldsymbol{O}(d\boldsymbol{M}^{\top})) + \tr(\boldsymbol{X}^{\top}\boldsymbol{O}\boldsymbol{M}^{\top}(d\boldsymbol{O})) \\[7pt]<br />
=&amp;\, \tr(\boldsymbol{M}^{\top}\boldsymbol{O}\boldsymbol{X}^{\top}(d\boldsymbol{O})) + \tr(\boldsymbol{O}^{\top}\boldsymbol{X}\boldsymbol{O}^{\top}(d\boldsymbol{M})) + \tr(\boldsymbol{X}^{\top}\boldsymbol{O}\boldsymbol{M}^{\top}(d\boldsymbol{O})) \\[7pt]<br />
\end{aligned}\end{equation}<br />
由此可得<br />
\begin{equation}\tr((\boldsymbol{X}^{\top} - \boldsymbol{O}^{\top}\boldsymbol{X}\boldsymbol{O}^{\top})(d\boldsymbol{M})) = \tr((\boldsymbol{M}^{\top}\boldsymbol{O}\boldsymbol{X}^{\top} + \boldsymbol{X}^{\top}\boldsymbol{O}\boldsymbol{M}^{\top})(d\boldsymbol{O}))\end{equation}<br />
如果我们让$\boldsymbol{M}^{\top}\boldsymbol{O}\boldsymbol{X}^{\top} + \boldsymbol{X}^{\top}\boldsymbol{O}\boldsymbol{M}^{\top}=(\nabla_{\boldsymbol{O}}\mathcal{L})^{\top}$，那么上式便具有$d\mathcal{L}$的含义，那么根据迹技巧就有$\boldsymbol{X}^{\top} - \boldsymbol{O}^{\top}\boldsymbol{X}\boldsymbol{O}^{\top}=(\nabla_{\boldsymbol{M}}\mathcal{L})^{\top}$，这表明$\nabla_{\boldsymbol{M}}\mathcal{L}$和$\nabla_{\boldsymbol{O}}\mathcal{L}$的关系，由下述方程组描述<br />
\begin{gather}\boldsymbol{X} - \boldsymbol{O}\boldsymbol{X}^{\top}\boldsymbol{O} = \nabla_{\boldsymbol{M}}\mathcal{L} \label{eq:g-m}\\[7pt]<br />
\boldsymbol{X}\boldsymbol{O}^{\top}\boldsymbol{M} + \boldsymbol{M}\boldsymbol{O}^{\top}\boldsymbol{X} = \nabla_{\boldsymbol{O}}\mathcal{L}\label{eq:g-o}\end{gather}</p>
<h2 id="_4">理论形式<a class="toc-link" href="#_4" title="Permanent link">&para;</a></h2>
<p>所以，现在问题变成了，从式$\eqref{eq:g-o}$中解出$\boldsymbol{X}$，然后代入式$\eqref{eq:g-m}$得到$\nabla_{\boldsymbol{M}}\mathcal{L}$，即将$\nabla_{\boldsymbol{M}}\mathcal{L}$表示为$\nabla_{\boldsymbol{O}}\mathcal{L}$的函数，避免直接求$\nabla_{\boldsymbol{M}}\boldsymbol{O}$。很明显，唯一的难度就是方程$\eqref{eq:g-o}$的求解。</p>
<p>这一节我们先基于SVD求一个不那么实用的理论解，它可以帮助我们了解方程$\eqref{eq:g-o}$的性质，并且跟之前的结果对齐。设$\boldsymbol{X}=\boldsymbol{U}\boldsymbol{Y}\boldsymbol{V}^{\top}$，然后我们还有$\boldsymbol{O}^{\top}\boldsymbol{M} = (\boldsymbol{M}^{\top}\boldsymbol{M})^{1/2} = \boldsymbol{V}(\boldsymbol{\Sigma}^{\top}\boldsymbol{\Sigma})^{1/2}\boldsymbol{V}^{\top}$和$\boldsymbol{M}\boldsymbol{O}^{\top}=(\boldsymbol{M}\boldsymbol{M}^{\top})^{1/2} = \boldsymbol{U}(\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\top})^{1/2}\boldsymbol{U}^{\top}$，将这些等式代入方程$\eqref{eq:g-o}$得到<br />
\begin{equation}\boldsymbol{U}\boldsymbol{Y}(\boldsymbol{\Sigma}^{\top}\boldsymbol{\Sigma})^{1/2}\boldsymbol{V}^{\top} + \boldsymbol{U}(\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\top})^{1/2}\boldsymbol{Y}\boldsymbol{V}^{\top} = \nabla_{\boldsymbol{O}}\mathcal{L}\end{equation}<br />
即<br />
\begin{equation}\boldsymbol{Y}(\boldsymbol{\Sigma}^{\top}\boldsymbol{\Sigma})^{1/2} + (\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\top})^{1/2}\boldsymbol{Y} = \boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}\label{eq:g-o-2}\end{equation}<br />
上式左端如果写成分量形式是$\boldsymbol{Y}<em i_j="i,j">{i,j}\sigma_j + \sigma_i \boldsymbol{Y}</em>} = (\sigma_i + \sigma_j)\boldsymbol{Y<em r_1="r+1">{i,j}$，其中$\sigma_1,\sigma_2,\cdots,\sigma_r$是$\boldsymbol{M}$的非零奇异值，而$0=\sigma</em>$是满秩方阵时，可以解得}=\sigma_{r+2}=\cdots$。很明显，如果当$\boldsymbol{M<br />
\begin{equation}\boldsymbol{Y} = (\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}) \oslash \boldsymbol{S}\end{equation}<br />
其中$\boldsymbol{S}_{i,j} = \sigma_i+\sigma_j$，$\oslash$是Hadamard除（逐位相除）。这时候我们将$\boldsymbol{X}=\boldsymbol{U}\boldsymbol{Y}\boldsymbol{V}^{\top}$代入式$\eqref{eq:g-m}$，就得到跟<a href="/archives/10878">《SVD的导数》</a>里边一致的结果。这个殊途同归也增强了我们的信心，看来至少到目前为止我们的推导都还是正确的。</p>
<p>若$\boldsymbol{M}$不满秩或不是方阵呢？此时如果右端的$\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}$“不配合”，那么方程$\eqref{eq:g-o-2}$无解。但方程$\eqref{eq:g-o-2}$是从实际问题得到的，所以它肯定有解，那么右端“必须配合”！怎样才是配合呢？如果$\boldsymbol{M}$的秩为$r$，那么矩阵$\boldsymbol{S}$只有$\boldsymbol{S}<em _boldsymbol_O="\boldsymbol{O">{[:r,:r]}$是非零的，为了使得方程$\eqref{eq:g-o-2}$有解，$(\boldsymbol{U}^{\top}(\nabla</em>)}}\mathcal{L})\boldsymbol{V<em 0="0" _epsilon_to="\epsilon\to">{[:r,:r]}$以外的部分只能是零。在这个条件下，我们可以写出<br />
\begin{equation}\boldsymbol{Y} = \lim</em>}\,\, (\boldsymbol{U}^{\top}(\nabla_{\boldsymbol{O}}\mathcal{L})\boldsymbol{V}) \oslash (\boldsymbol{S} + \epsilon) \end{equation<br />
这相当于说，我们可以给奇异值加些扰动，转化为全体奇异值非零的情况，计算完成后再让扰动趋于零，从而得到正确的结果。</p>
<h2 id="_5">高效求解<a class="toc-link" href="#_5" title="Permanent link">&para;</a></h2>
<p>上一节的SVD解往往只有理论价值，为了在GPU中高效计算，我们还需要寻求其他形式的解。引入记号$\boldsymbol{M}\boldsymbol{O}^{\top}=\boldsymbol{A},\boldsymbol{O}^{\top}\boldsymbol{M}=\boldsymbol{B},\nabla_{\boldsymbol{O}}\mathcal{L}=\boldsymbol{C}$，那么式$\eqref{eq:g-o}$实际上是一个<a href="https://en.wikipedia.org/wiki/Sylvester_equation">Sylvester方程</a>：<br />
\begin{equation}\boldsymbol{A}\boldsymbol{X}+\boldsymbol{X}\boldsymbol{B} = \boldsymbol{C}\end{equation}<br />
求解Sylvester方程的方法有很多，其中最精妙、对GPU最高效的，可能是基于$\mcsgn$（不是$\msign$）的求解方案（这里参考了<a href="https://papers.cool/arxiv/2201.08663">《Fast Differentiable Matrix Square Root》</a>）。首先，从上述方程出发，我们可以验证下式成立<br />
\begin{equation}\begin{bmatrix} \boldsymbol{A} &amp; -\boldsymbol{C} \\ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix} = \begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \\ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\begin{bmatrix} \boldsymbol{A} &amp; \boldsymbol{0} \\ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \\ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}^{-1}<br />
\end{equation}<br />
两边取$\mcsgn$，根据$\mcsgn$的性质，我们有<br />
\begin{equation}\mcsgn\left(\begin{bmatrix} \boldsymbol{A} &amp; -\boldsymbol{C} \\ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}\right) = \begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \\ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\begin{bmatrix} \mcsgn(\boldsymbol{A}) &amp; \boldsymbol{0} \\ \boldsymbol{0} &amp; -\mcsgn(\boldsymbol{B})\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \\ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}^{-1}<br />
\end{equation}<br />
注意$\boldsymbol{A}=\boldsymbol{M}\boldsymbol{O}^{\top}=(\boldsymbol{M}\boldsymbol{M}^{\top})^{1/2}, \boldsymbol{B}=\boldsymbol{O}^{\top}\boldsymbol{M}=(\boldsymbol{M}^{\top}\boldsymbol{M})^{1/2}$，假设$\boldsymbol{M}$是满秩方阵，那么$\boldsymbol{A},\boldsymbol{B}$都是正定对称的，正定对称矩阵的$\mcsgn$都是方阵，所以<br />
\begin{equation}\mcsgn\left(\begin{bmatrix} \boldsymbol{A} &amp; -\boldsymbol{C} \\ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}\right) = \begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \\ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{0} \\ \boldsymbol{0} &amp; -\boldsymbol{I}\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \\ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}^{-1} = \begin{bmatrix} \boldsymbol{I} &amp; -2\boldsymbol{X} \\ \boldsymbol{0} &amp; -\boldsymbol{I}\end{bmatrix}<br />
\end{equation}<br />
最后的化简用到了等式$\begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \\ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}^{-1}=\begin{bmatrix} \boldsymbol{I} &amp; -\boldsymbol{X} \\ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}$。从该结果可以看出，我们只需要对分块矩阵$\begin{bmatrix} \boldsymbol{A} &amp; -\boldsymbol{C} \\ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}$算$\mcsgn$，然后就可以从结果的右上角读出$\boldsymbol{X}$。$\mcsgn$可以通过Newton-Schulz迭代高效地计算，因此该方案是GPU友好的。</p>
<p>当$\boldsymbol{M}$不满秩或非方阵时，$\boldsymbol{A},\boldsymbol{B}$只是半正定的，这时候它们就$\mcsgn$就不是$\boldsymbol{I}$。不过，上一节的经验告诉我们，由于$\nabla_{\boldsymbol{O}}\mathcal{L}$“必须配合”，所以只需要给$\boldsymbol{\Sigma}$加点扰动，让它变成正定的情况即可解。这里给$\boldsymbol{\Sigma}$加扰动，相当于给$\boldsymbol{A},\boldsymbol{B}$加$\epsilon \boldsymbol{I}$，所以<br />
\begin{equation}\boldsymbol{X} = -\frac{1}{2} \left(\lim_{\epsilon\to 0}\,\, \mcsgn\left(\begin{bmatrix} \boldsymbol{A} + \epsilon \boldsymbol{I} &amp; -\boldsymbol{C} \\ \boldsymbol{0} &amp; -\boldsymbol{B} - \epsilon \boldsymbol{I}\end{bmatrix}\right)\right)_{[:n,n:]}<br />
\end{equation}<br />
实际计算时，就只能选择一个比较小的$\epsilon &gt; 0$来近似计算了，可以考虑$\epsilon=10^{-3}$，它在我们之前寻找Newton-Schulz迭代的下界范围内。</p>
<h2 id="_6">详细数学推导<a class="toc-link" href="#_6" title="Permanent link">&para;</a></h2>
<p>在本节中，我们将从多个角度深入探讨$\msign$算子的导数理论，包括Fréchet导数的严格定义、基于Lyapunov方程的推导、积分表示、数值计算方法以及在深度学习中的实际应用。</p>
<h3 id="frechet">Fréchet导数的定义与性质<a class="toc-link" href="#frechet" title="Permanent link">&para;</a></h3>
<p>首先，我们需要严格定义矩阵函数的导数。对于从$\mathbb{R}^{n\times m}$到$\mathbb{R}^{p\times q}$的映射$F$，其<strong>Fréchet导数</strong>（或方向导数）定义如下：</p>
<p><strong>定义1（Fréchet导数）</strong>：设$F: \mathbb{R}^{n\times m} \to \mathbb{R}^{p\times q}$是矩阵函数，如果存在线性算子$\mathcal{D}F(\boldsymbol{M}): \mathbb{R}^{n\times m} \to \mathbb{R}^{p\times q}$使得<br />
\begin{equation}<br />
F(\boldsymbol{M} + \boldsymbol{H}) = F(\boldsymbol{M}) + \mathcal{D}F(\boldsymbol{M})[\boldsymbol{H}] + o(\Vert\boldsymbol{H}\Vert_F)<br />
\end{equation}<br />
其中$\lim_{\Vert\boldsymbol{H}\Vert_F\to 0} \frac{\Vert o(\Vert\boldsymbol{H}\Vert_F)\Vert_F}{\Vert\boldsymbol{H}\Vert_F} = 0$，则称$\mathcal{D}F(\boldsymbol{M})$为$F$在$\boldsymbol{M}$处的Fréchet导数。</p>
<p>对于我们的情况，$F(\boldsymbol{M}) = \msign(\boldsymbol{M})$，我们需要找到$\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}]$，即当$\boldsymbol{M}$有一个微小扰动$\boldsymbol{H}$时，$\msign(\boldsymbol{M})$的变化。</p>
<p><strong>性质1（链式法则）</strong>：如果$F = G \circ H$，则<br />
\begin{equation}<br />
\mathcal{D}F(\boldsymbol{M})[\boldsymbol{H}] = \mathcal{D}G(H(\boldsymbol{M}))[\mathcal{D}H(\boldsymbol{M})[\boldsymbol{H}]]<br />
\end{equation}</p>
<p><strong>性质2（线性性）</strong>：Fréchet导数关于扰动$\boldsymbol{H}$是线性的，即<br />
\begin{equation}<br />
\mathcal{D}F(\boldsymbol{M})[\alpha\boldsymbol{H}_1 + \beta\boldsymbol{H}_2] = \alpha\mathcal{D}F(\boldsymbol{M})[\boldsymbol{H}_1] + \beta\mathcal{D}F(\boldsymbol{M})[\boldsymbol{H}_2]<br />
\end{equation}</p>
<p><strong>性质3（乘积法则）</strong>：如果$F(\boldsymbol{M}) = \boldsymbol{A}(\boldsymbol{M})\boldsymbol{B}(\boldsymbol{M})$，则<br />
\begin{equation}<br />
\mathcal{D}F(\boldsymbol{M})[\boldsymbol{H}] = \mathcal{D}\boldsymbol{A}(\boldsymbol{M})[\boldsymbol{H}]\cdot\boldsymbol{B}(\boldsymbol{M}) + \boldsymbol{A}(\boldsymbol{M})\cdot\mathcal{D}\boldsymbol{B}(\boldsymbol{M})[\boldsymbol{H}]<br />
\end{equation}</p>
<h3 id="msignfrechet">msign的Fréchet导数显式公式<a class="toc-link" href="#msignfrechet" title="Permanent link">&para;</a></h3>
<p>现在我们推导$\msign$的Fréchet导数。记$\boldsymbol{O} = \msign(\boldsymbol{M})$，从恒等式<br />
\begin{equation}<br />
\boldsymbol{M} = \boldsymbol{O}\boldsymbol{M}^{\top}\boldsymbol{O}<br />
\end{equation}<br />
出发，对两边取Fréchet导数（使用乘积法则）：<br />
\begin{equation}<br />
\boldsymbol{H} = \mathcal{D}\boldsymbol{O}[\boldsymbol{H}]\cdot\boldsymbol{M}^{\top}\boldsymbol{O} + \boldsymbol{O}\cdot\boldsymbol{H}^{\top}\cdot\boldsymbol{O} + \boldsymbol{O}\cdot\boldsymbol{M}^{\top}\cdot\mathcal{D}\boldsymbol{O}[\boldsymbol{H}]<br />
\end{equation}</p>
<p>记$d\boldsymbol{O} = \mathcal{D}\boldsymbol{O}[\boldsymbol{H}]$为$\boldsymbol{O}$在方向$\boldsymbol{H}$上的导数，我们有<br />
\begin{equation}<br />
\boldsymbol{H} = (d\boldsymbol{O})\boldsymbol{M}^{\top}\boldsymbol{O} + \boldsymbol{O}\boldsymbol{H}^{\top}\boldsymbol{O} + \boldsymbol{O}\boldsymbol{M}^{\top}(d\boldsymbol{O})<br />
\end{equation}</p>
<p>定义算子$\mathcal{L}: \mathbb{R}^{n\times m} \to \mathbb{R}^{n\times m}$为<br />
\begin{equation}<br />
\mathcal{L}(\boldsymbol{X}) = \boldsymbol{X}\boldsymbol{M}^{\top}\boldsymbol{O} + \boldsymbol{O}\boldsymbol{M}^{\top}\boldsymbol{X}<br />
\end{equation}</p>
<p>则我们的导数方程可以写成算子方程的形式：<br />
\begin{equation}<br />
\mathcal{L}(d\boldsymbol{O}) = \boldsymbol{H} - \boldsymbol{O}\boldsymbol{H}^{\top}\boldsymbol{O}<br />
\end{equation}</p>
<p>因此，$\msign$的Fréchet导数的<strong>显式公式</strong>为：<br />
\begin{equation}<br />
\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}] = \mathcal{L}^{-1}(\boldsymbol{H} - \boldsymbol{O}\boldsymbol{H}^{\top}\boldsymbol{O})<br />
\end{equation}</p>
<p>其中$\mathcal{L}^{-1}$是算子$\mathcal{L}$的逆。</p>
<h3 id="lyapunov">基于Lyapunov方程的推导<a class="toc-link" href="#lyapunov" title="Permanent link">&para;</a></h3>
<p>注意到算子$\mathcal{L}$定义的方程实际上是一个<strong>连续时间Lyapunov方程</strong>的特殊形式。标准的Lyapunov方程为：<br />
\begin{equation}<br />
\boldsymbol{A}\boldsymbol{X} + \boldsymbol{X}\boldsymbol{B} = \boldsymbol{C}<br />
\end{equation}</p>
<p>在我们的情况下，令$\boldsymbol{A} = \boldsymbol{M}\boldsymbol{O}^{\top}, \boldsymbol{B} = \boldsymbol{O}^{\top}\boldsymbol{M}$（注意这里是转置），$\boldsymbol{X} = d\boldsymbol{O}$，$\boldsymbol{C} = \boldsymbol{H} - \boldsymbol{O}\boldsymbol{H}^{\top}\boldsymbol{O}$，我们可以将导数方程改写为：<br />
\begin{equation}<br />
\boldsymbol{M}\boldsymbol{O}^{\top}(d\boldsymbol{O}) + (d\boldsymbol{O})\boldsymbol{O}^{\top}\boldsymbol{M} = \boldsymbol{H} - \boldsymbol{O}\boldsymbol{H}^{\top}\boldsymbol{O}<br />
\end{equation}</p>
<p>这正是Sylvester方程的形式。我们知道$\boldsymbol{A} = \boldsymbol{M}\boldsymbol{O}^{\top} = (\boldsymbol{M}\boldsymbol{M}^{\top})^{1/2}$和$\boldsymbol{B} = \boldsymbol{O}^{\top}\boldsymbol{M} = (\boldsymbol{M}^{\top}\boldsymbol{M})^{1/2}$都是半正定矩阵。</p>
<p><strong>定理1（Lyapunov方程可解性）</strong>：如果$\boldsymbol{A}$和$\boldsymbol{B}$的特征值满足$\lambda_i(\boldsymbol{A}) + \lambda_j(\boldsymbol{B}) \neq 0$对所有$i,j$成立，则Lyapunov方程$\boldsymbol{A}\boldsymbol{X} + \boldsymbol{X}\boldsymbol{B} = \boldsymbol{C}$有唯一解。</p>
<p>在我们的情况下，$\boldsymbol{A}$和$\boldsymbol{B}$的特征值都是非负的（它们是平方根矩阵的特征值），因此$\lambda_i(\boldsymbol{A}) + \lambda_j(\boldsymbol{B}) \geq 0$。只有当$\boldsymbol{M}$的某些奇异值为零时，和才可能为零。</p>
<p><strong>引理1</strong>：当$\boldsymbol{M}$满秩时，Lyapunov方程有唯一解。当$\boldsymbol{M}$不满秩时，方程有解当且仅当$\boldsymbol{C}$满足相容性条件。</p>
<p>相容性条件可以通过SVD分析得到。设$\boldsymbol{M} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，秩为$r$，则<br />
\begin{equation}<br />
\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}^{1/2}<em ext="ext">{ext}\boldsymbol{U}^{\top}, \quad \boldsymbol{B} = \boldsymbol{V}\boldsymbol{\Sigma}^{1/2}</em>}\boldsymbol{V}^{\top<br />
\end{equation}<br />
其中$\boldsymbol{\Sigma}_{ext}$是扩展的对角矩阵，保持前$r$个奇异值，其余为零。</p>
<p>将$d\boldsymbol{O} = \boldsymbol{U}\boldsymbol{Y}\boldsymbol{V}^{\top}$代入Lyapunov方程，得到<br />
\begin{equation}<br />
\boldsymbol{U}\boldsymbol{\Sigma}^{1/2}<em ext="ext">{ext}\boldsymbol{U}^{\top}\boldsymbol{U}\boldsymbol{Y}\boldsymbol{V}^{\top} + \boldsymbol{U}\boldsymbol{Y}\boldsymbol{V}^{\top}\boldsymbol{V}\boldsymbol{\Sigma}^{1/2}</em>}\boldsymbol{V}^{\top} = \boldsymbol{C<br />
\end{equation}</p>
<p>简化后得到<br />
\begin{equation}<br />
\boldsymbol{\Sigma}^{1/2}<em ext="ext">{ext}\boldsymbol{Y} + \boldsymbol{Y}\boldsymbol{\Sigma}^{1/2}</em>} = \boldsymbol{U}^{\top}\boldsymbol{C}\boldsymbol{V<br />
\end{equation}</p>
<p>写成分量形式：<br />
\begin{equation}<br />
(\sigma_i^{1/2} + \sigma_j^{1/2})\boldsymbol{Y}<em ij="ij">{ij} = [\boldsymbol{U}^{\top}\boldsymbol{C}\boldsymbol{V}]</em><br />
\end{equation}</p>
<p>因此，解为<br />
\begin{equation}<br />
\boldsymbol{Y}<em ij="ij">{ij} = \begin{cases}<br />
\frac{[\boldsymbol{U}^{\top}\boldsymbol{C}\boldsymbol{V}]</em> i,j \leq r \}}{\sigma_i^{1/2} + \sigma_j^{1/2}} &amp; \text{if <br />
0 &amp; \text{otherwise}<br />
\end{cases}<br />
\end{equation}</p>
<p>相容性条件就是：当$i &gt; r$或$j &gt; r$时，$[\boldsymbol{U}^{\top}\boldsymbol{C}\boldsymbol{V}]_{ij} = 0$。</p>
<h3 id="_7">基于积分表示的推导<a class="toc-link" href="#_7" title="Permanent link">&para;</a></h3>
<p>除了代数方法，我们还可以从积分表示的角度理解$\msign$的导数。考虑矩阵符号函数的<strong>Cauchy积分表示</strong>：</p>
<p>对于可对角化的矩阵$\boldsymbol{M}$（特征值不在负实轴上），符号函数可以表示为<br />
\begin{equation}<br />
\text{sign}(\boldsymbol{M}) = \frac{1}{2\pi i}\oint_{\Gamma} \text{sign}(z)(\boldsymbol{M} - z\boldsymbol{I})^{-1}dz<br />
\end{equation}</p>
<p>其中$\Gamma$是包围$\boldsymbol{M}$所有特征值的闭合路径。</p>
<p>对于$\msign$，我们有类似的表示。设$\boldsymbol{M} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，定义<br />
\begin{equation}<br />
\msign(\boldsymbol{M}) = \boldsymbol{U}\text{sign}(\boldsymbol{\Sigma})\boldsymbol{V}^{\top}<br />
\end{equation}</p>
<p>其中$\text{sign}(\boldsymbol{\Sigma})$对对角元素逐个应用符号函数（正数变1，负数变-1，零保持为零）。</p>
<p>更一般地，我们可以使用<strong>谱分解</strong>来表示$\msign$：<br />
\begin{equation}<br />
\msign(\boldsymbol{M}) = \sum_{i=1}^{r} \boldsymbol{u}_i\boldsymbol{v}_i^{\top}<br />
\end{equation}</p>
<p>其中$\boldsymbol{u}_i, \boldsymbol{v}_i$是对应非零奇异值的左右奇异向量。</p>
<p>这个表示的导数可以通过对每一项求导得到：<br />
\begin{equation}<br />
\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}] = \sum_{i=1}^{r} \left(\mathcal{D}\boldsymbol{u}_i[\boldsymbol{H}]\boldsymbol{v}_i^{\top} + \boldsymbol{u}_i\mathcal{D}\boldsymbol{v}_i[\boldsymbol{H}]^{\top}\right)<br />
\end{equation}</p>
<p>奇异向量的导数可以通过扰动理论得到。对于$\boldsymbol{M}\boldsymbol{v}_i = \sigma_i\boldsymbol{u}_i$，扰动后我们有<br />
\begin{equation}<br />
(\boldsymbol{M} + \boldsymbol{H})(\boldsymbol{v}_i + \delta\boldsymbol{v}_i) = (\sigma_i + \delta\sigma_i)(\boldsymbol{u}_i + \delta\boldsymbol{u}_i)<br />
\end{equation}</p>
<p>忽略高阶项，得到<br />
\begin{equation}<br />
\boldsymbol{M}\delta\boldsymbol{v}_i + \boldsymbol{H}\boldsymbol{v}_i = \sigma_i\delta\boldsymbol{u}_i + \delta\sigma_i\boldsymbol{u}_i<br />
\end{equation}</p>
<p>左乘$\boldsymbol{u}_i^{\top}$，利用$\boldsymbol{u}_i^{\top}\boldsymbol{M}\delta\boldsymbol{v}_i = \sigma_i\boldsymbol{u}_i^{\top}\delta\boldsymbol{v}_i$和正交性条件$\boldsymbol{u}_i^{\top}\delta\boldsymbol{u}_i = 0$，得到<br />
\begin{equation}<br />
\delta\sigma_i = \boldsymbol{u}_i^{\top}\boldsymbol{H}\boldsymbol{v}_i<br />
\end{equation}</p>
<p>这给出了奇异值的一阶扰动公式。</p>
<p>对于奇异向量的扰动，我们有<br />
\begin{equation}<br />
\delta\boldsymbol{u}<em i="i" j_neq="j\neq">i = \sum</em>}\frac{\boldsymbol{u<em i="i" j_neq="j\neq">j^{\top}\boldsymbol{H}\boldsymbol{v}_i}{\sigma_i - \sigma_j}\boldsymbol{u}_j<br />
\end{equation}<br />
\begin{equation}<br />
\delta\boldsymbol{v}_i = \sum</em>_j}\frac{\boldsymbol{u}_i^{\top}\boldsymbol{H}\boldsymbol{v}_j}{\sigma_i - \sigma_j}\boldsymbol{v<br />
\end{equation}</p>
<p>但要注意，当奇异值重复时，这个公式会出现奇异性。对于$\msign$，由于所有非零奇异值都被映射到1，这种奇异性是不可避免的。</p>
<h3 id="_8">反向传播中的梯度计算<a class="toc-link" href="#_8" title="Permanent link">&para;</a></h3>
<p>在深度学习的反向传播中，我们通常不直接计算Fréchet导数，而是计算标量损失函数关于参数的梯度。设$\mathcal{L}$是标量损失函数，我们已知$\nabla_{\boldsymbol{O}}\mathcal{L}$（即损失对$\boldsymbol{O} = \msign(\boldsymbol{M})$的梯度），需要计算$\nabla_{\boldsymbol{M}}\mathcal{L}$。</p>
<p>根据链式法则，我们有<br />
\begin{equation}<br />
\langle \nabla_{\boldsymbol{M}}\mathcal{L}, \boldsymbol{H} \rangle = \langle \nabla_{\boldsymbol{O}}\mathcal{L}, \mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}] \rangle<br />
\end{equation}</p>
<p>其中$\langle \cdot, \cdot \rangle$表示Frobenius内积。这可以改写为<br />
\begin{equation}<br />
\tr((\nabla_{\boldsymbol{M}}\mathcal{L})^{\top}\boldsymbol{H}) = \tr((\nabla_{\boldsymbol{O}}\mathcal{L})^{\top}\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}])<br />
\end{equation}</p>
<p>从前面的推导，我们知道$\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}]$满足<br />
\begin{equation}<br />
\boldsymbol{H} = \mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}]\cdot\boldsymbol{M}^{\top}\boldsymbol{O} + \boldsymbol{O}\boldsymbol{H}^{\top}\boldsymbol{O} + \boldsymbol{O}\boldsymbol{M}^{\top}\cdot\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}]<br />
\end{equation}</p>
<p>记$d\boldsymbol{O} = \mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}]$，$\boldsymbol{G} = \nabla_{\boldsymbol{O}}\mathcal{L}$，我们需要求$\nabla_{\boldsymbol{M}}\mathcal{L}$使得<br />
\begin{equation}<br />
\tr((\nabla_{\boldsymbol{M}}\mathcal{L})^{\top}\boldsymbol{H}) = \tr(\boldsymbol{G}^{\top}d\boldsymbol{O})<br />
\end{equation}</p>
<p>引入辅助矩阵$\boldsymbol{X}$满足<br />
\begin{equation}<br />
\boldsymbol{X}\boldsymbol{O}^{\top}\boldsymbol{M} + \boldsymbol{M}\boldsymbol{O}^{\top}\boldsymbol{X} = \boldsymbol{G}<br />
\end{equation}</p>
<p>这也是一个Sylvester方程，与前向传播的方程形式相同。</p>
<p>将$\boldsymbol{H} = (d\boldsymbol{O})\boldsymbol{M}^{\top}\boldsymbol{O} + \boldsymbol{O}\boldsymbol{H}^{\top}\boldsymbol{O} + \boldsymbol{O}\boldsymbol{M}^{\top}(d\boldsymbol{O})$代入梯度计算，经过迹的循环性质变换，可以得到<br />
\begin{equation}<br />
\nabla_{\boldsymbol{M}}\mathcal{L} = \boldsymbol{X} - \boldsymbol{O}\boldsymbol{X}^{\top}\boldsymbol{O}<br />
\end{equation}</p>
<p><strong>算法1（msign反向传播）</strong>：<br />
1. 输入：$\boldsymbol{M}, \boldsymbol{O} = \msign(\boldsymbol{M}), \boldsymbol{G} = \nabla_{\boldsymbol{O}}\mathcal{L}$<br />
2. 计算$\boldsymbol{A} = \boldsymbol{M}\boldsymbol{O}^{\top}, \boldsymbol{B} = \boldsymbol{O}^{\top}\boldsymbol{M}$<br />
3. 求解Sylvester方程：$\boldsymbol{X}\boldsymbol{B} + \boldsymbol{A}\boldsymbol{X} = \boldsymbol{G}$<br />
4. 计算$\nabla_{\boldsymbol{M}}\mathcal{L} = \boldsymbol{X} - \boldsymbol{O}\boldsymbol{X}^{\top}\boldsymbol{O}$<br />
5. 输出：$\nabla_{\boldsymbol{M}}\mathcal{L}$</p>
<h3 id="mcsgn">基于mcsgn的高效求解算法<a class="toc-link" href="#mcsgn" title="Permanent link">&para;</a></h3>
<p>前文提到，Sylvester方程可以通过$\mcsgn$高效求解。我们详细展开这个方法。</p>
<p>构造增广矩阵<br />
\begin{equation}<br />
\boldsymbol{T} = \begin{bmatrix} \boldsymbol{A} &amp; -\boldsymbol{G} \ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}<br />
\end{equation}</p>
<p>其中$\boldsymbol{A} = \boldsymbol{M}\boldsymbol{O}^{\top}, \boldsymbol{B} = \boldsymbol{O}^{\top}\boldsymbol{M}$。</p>
<p><strong>引理2</strong>：如果$\boldsymbol{X}$满足$\boldsymbol{A}\boldsymbol{X} + \boldsymbol{X}\boldsymbol{B} = \boldsymbol{G}$，则<br />
\begin{equation}<br />
\boldsymbol{T} = \begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\begin{bmatrix} \boldsymbol{A} &amp; \boldsymbol{0} \ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; -\boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}<br />
\end{equation}</p>
<p>证明：展开右边<br />
\begin{align}<br />
&amp;\begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\begin{bmatrix} \boldsymbol{A} &amp; \boldsymbol{0} \ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; -\boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\<br />
&amp;= \begin{bmatrix} \boldsymbol{A} &amp; -\boldsymbol{X}\boldsymbol{B} \ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; -\boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\<br />
&amp;= \begin{bmatrix} \boldsymbol{A} &amp; -\boldsymbol{A}\boldsymbol{X} - \boldsymbol{X}\boldsymbol{B} \ \boldsymbol{0} &amp; -\boldsymbol{B}\end{bmatrix}<br />
\end{align}</p>
<p>如果$\boldsymbol{A}\boldsymbol{X} + \boldsymbol{X}\boldsymbol{B} = \boldsymbol{G}$，则右上角正好是$-\boldsymbol{G}$。$\square$</p>
<p>应用$\mcsgn$的相似不变性，我们有<br />
\begin{equation}<br />
\mcsgn(\boldsymbol{T}) = \begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\begin{bmatrix} \mcsgn(\boldsymbol{A}) &amp; \boldsymbol{0} \ \boldsymbol{0} &amp; \mcsgn(-\boldsymbol{B})\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; -\boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}<br />
\end{equation}</p>
<p>当$\boldsymbol{M}$满秩时，$\boldsymbol{A}, \boldsymbol{B}$都是正定的，因此$\mcsgn(\boldsymbol{A}) = \boldsymbol{I}, \mcsgn(-\boldsymbol{B}) = -\boldsymbol{I}$，从而<br />
\begin{equation}<br />
\mcsgn(\boldsymbol{T}) = \begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; \boldsymbol{0} \ \boldsymbol{0} &amp; -\boldsymbol{I}\end{bmatrix}\begin{bmatrix} \boldsymbol{I} &amp; -\boldsymbol{X} \ \boldsymbol{0} &amp; \boldsymbol{I}\end{bmatrix} = \begin{bmatrix} \boldsymbol{I} &amp; -2\boldsymbol{X} \ \boldsymbol{0} &amp; -\boldsymbol{I}\end{bmatrix}<br />
\end{equation}</p>
<p>因此<br />
\begin{equation}<br />
\boldsymbol{X} = -\frac{1}{2}[\mcsgn(\boldsymbol{T})]_{[:n,n:]}<br />
\end{equation}</p>
<p>其中$[\cdot]_{[:n,n:]}$表示取右上角的$n\times m$块。</p>
<p><strong>算法2（基于mcsgn的反向传播）</strong>：<br />
1. 输入：$\boldsymbol{M}, \boldsymbol{O} = \msign(\boldsymbol{M}), \boldsymbol{G} = \nabla_{\boldsymbol{O}}\mathcal{L}$<br />
2. 计算$\boldsymbol{A} = \boldsymbol{M}\boldsymbol{O}^{\top}, \boldsymbol{B} = \boldsymbol{O}^{\top}\boldsymbol{M}$<br />
3. 构造$\boldsymbol{T} = \begin{bmatrix} \boldsymbol{A} + \epsilon\boldsymbol{I} &amp; -\boldsymbol{G} \ \boldsymbol{0} &amp; -\boldsymbol{B} - \epsilon\boldsymbol{I}\end{bmatrix}$（$\epsilon$用于数值稳定）<br />
4. 使用Newton-Schulz迭代计算$\mcsgn(\boldsymbol{T})$<br />
5. 提取$\boldsymbol{X} = -\frac{1}{2}[\mcsgn(\boldsymbol{T})]<em _boldsymbol_M="\boldsymbol{M">{[:n,n:]}$<br />
6. 计算$\nabla</em>$}}\mathcal{L} = \boldsymbol{X} - \boldsymbol{O}\boldsymbol{X}^{\top}\boldsymbol{O<br />
7. 输出：$\nabla_{\boldsymbol{M}}\mathcal{L}$</p>
<h3 id="newton-schulz">Newton-Schulz迭代的导数<a class="toc-link" href="#newton-schulz" title="Permanent link">&para;</a></h3>
<p>前面提到，$\msign$可以通过Newton-Schulz迭代计算。迭代格式为<br />
\begin{equation}<br />
\boldsymbol{X}<em t_1="t+1">0 = \frac{\boldsymbol{M}}{\Vert\boldsymbol{M}\Vert_F}, \quad \boldsymbol{X}</em>_t)^2} = a_t\boldsymbol{X}_t + b_t\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X}_t) + c_t\boldsymbol{X}_t(\boldsymbol{X}_t^{\top}\boldsymbol{X<br />
\end{equation}</p>
<p>这个迭代过程本身是可微的，可以直接反向传播。设$\boldsymbol{X}_T \approx \msign(\boldsymbol{M})$，我们需要反向传播梯度。</p>
<p>对于第$t$步的更新，设$\boldsymbol{G}<em _boldsymbol_X="\boldsymbol{X">t = \nabla</em><em t-1="t-1">t}\mathcal{L}$，我们需要计算$\boldsymbol{G}</em>$。} = \nabla_{\boldsymbol{X}_{t-1}}\mathcal{L</p>
<p>记$\boldsymbol{P}<em t-1="t-1">t = \boldsymbol{X}_t^{\top}\boldsymbol{X}_t$，则<br />
\begin{equation}<br />
\boldsymbol{X}_t = a</em>}\boldsymbol{X<em t-1="t-1">{t-1} + b</em>}\boldsymbol{X<em t-1="t-1">{t-1}\boldsymbol{P}</em>} + c_{t-1}\boldsymbol{X<em t-1="t-1">{t-1}\boldsymbol{P}</em>^2<br />
\end{equation}</p>
<p>对$\boldsymbol{X}<em t-1="t-1">{t-1}$求导（这里需要使用矩阵微积分的乘积法则和链式法则）：<br />
\begin{align}<br />
\boldsymbol{G}</em>} &amp;= a_{t-1}\boldsymbol{G<em t-1="t-1">t + b</em>}\boldsymbol{G<em t-1="t-1">t\boldsymbol{P}</em>} + c_{t-1}\boldsymbol{G<em t-1="t-1">t\boldsymbol{P}</em>^2\<br />
&amp;\quad + b_{t-1}\nabla_{\boldsymbol{X}<em t-1="t-1">{t-1}}(\boldsymbol{X}</em>}\boldsymbol{P<em t-1="t-1">{t-1})^{\top}\boldsymbol{G}_t\<br />
&amp;\quad + c</em>}\nabla_{\boldsymbol{X<em t-1="t-1">{t-1}}(\boldsymbol{X}</em>_t}\boldsymbol{P}_{t-1}^2)^{\top}\boldsymbol{G<br />
\end{align}</p>
<p>对于$\boldsymbol{P}<em _boldsymbol_X="\boldsymbol{X">t = \boldsymbol{X}_t^{\top}\boldsymbol{X}_t$的导数，我们有<br />
\begin{equation}<br />
\nabla</em><em _boldsymbol_X="\boldsymbol{X">t}\tr(\boldsymbol{G}_P^{\top}\boldsymbol{P}_t) = \nabla</em>_P}_t}\tr(\boldsymbol{G}_P^{\top}\boldsymbol{X}_t^{\top}\boldsymbol{X}_t) = 2\boldsymbol{X}_t\boldsymbol{G<br />
\end{equation}</p>
<p>因此，从$\nabla_{\boldsymbol{X}_t\boldsymbol{P}_t}\mathcal{L}$反向传播到$\boldsymbol{X}_t$时，需要考虑$\boldsymbol{P}_t$对$\boldsymbol{X}_t$的依赖。</p>
<p>完整的反向传播公式为：<br />
\begin{align}<br />
\boldsymbol{G}<em t-1="t-1">{P,t-1} &amp;= b</em>}\boldsymbol{X<em t-1="t-1">{t-1}^{\top}\boldsymbol{G}_t + 2c</em>}\boldsymbol{P<em t-1="t-1">{t-1}\boldsymbol{X}</em>}^{\top}\boldsymbol{G<em t-1="t-1">t\<br />
\boldsymbol{G}</em>} &amp;= a_{t-1}\boldsymbol{G<em t-1="t-1">t + b</em>}\boldsymbol{G<em t-1="t-1">t\boldsymbol{P}</em>} + c_{t-1}\boldsymbol{G<em t-1="t-1">t\boldsymbol{P}</em>}^2 + 2\boldsymbol{X<em P_t-1="P,t-1">{t-1}\boldsymbol{G}</em><br />
\end{align}</p>
<p>这个递推关系使得我们可以从$\boldsymbol{G}_T$逐步反向传播到$\boldsymbol{G}_0$。</p>
<h3 id="_9">自动微分的实现<a class="toc-link" href="#_9" title="Permanent link">&para;</a></h3>
<p>在现代深度学习框架（如PyTorch, JAX）中，我们可以利用自动微分（Automatic Differentiation, AD）来自动计算梯度。这里讨论两种实现方式：</p>
<p><strong>方式1：直接反向传播Newton-Schulz迭代</strong></p>
<p>最简单的方式是将Newton-Schulz迭代作为一系列可微操作，让自动微分框架自动处理反向传播。</p>
<pre class="highlight"><code class="language-python">def msign_autograd(M, num_iters=8):
    &quot;&quot;&quot;使用自动微分计算msign的梯度&quot;&quot;&quot;
    # 初始化
    X = M / torch.norm(M, 'fro')

    # Newton-Schulz迭代（使用预定义系数）
    coeffs = [
        (8.28721 * 1.01, -23.5959 * 1.01**3, 17.3004 * 1.01**5),
        (4.10706 * 1.01, -2.94785 * 1.01**3, 0.544843 * 1.01**5),
        # ... 更多系数
    ]

    for i in range(num_iters):
        a, b, c = coeffs[i]
        XTX = X.T @ X
        X = a * X + b * X @ XTX + c * X @ (XTX @ XTX)

    return X
</code></pre>

<p>这种方式的优点是实现简单，缺点是需要存储所有中间状态$\boldsymbol{X}_0, \boldsymbol{X}_1, \ldots, \boldsymbol{X}_T$，显存开销大。</p>
<p><strong>方式2：自定义反向传播函数</strong></p>
<p>更高效的方式是自定义反向传播，使用基于Sylvester方程的解析解。</p>
<pre class="highlight"><code class="language-python">class MSignFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, M, epsilon=1e-3):
        &quot;&quot;&quot;前向传播：计算O = msign(M)&quot;&quot;&quot;
        O = compute_msign_newton_schulz(M)
        ctx.save_for_backward(M, O)
        ctx.epsilon = epsilon
        return O

    @staticmethod
    def backward(ctx, grad_output):
        &quot;&quot;&quot;反向传播：计算grad_M&quot;&quot;&quot;
        M, O = ctx.saved_tensors
        G = grad_output
        epsilon = ctx.epsilon

        # 构造增广矩阵
        A = M @ O.T
        B = O.T @ M
        n, m = M.shape

        # 添加正则化以处理奇异情况
        A_reg = A + epsilon * torch.eye(n, device=M.device)
        B_reg = B + epsilon * torch.eye(m, device=M.device)

        # 构造T矩阵
        T = torch.cat([
            torch.cat([A_reg, -G], dim=1),
            torch.cat([torch.zeros(m, n, device=M.device), -B_reg], dim=1)
        ], dim=0)

        # 计算mcsgn(T)
        T_sgn = compute_mcsgn_newton_schulz(T)

        # 提取X
        X = -0.5 * T_sgn[:n, n:]

        # 计算最终梯度
        grad_M = X - O @ X.T @ O

        return grad_M, None
</code></pre>

<p><strong>方式3：隐函数定理</strong></p>
<p>另一种思路是利用隐函数定理（Implicit Function Theorem）。由于$\boldsymbol{O}$满足$\boldsymbol{M} = \boldsymbol{O}\boldsymbol{M}^{\top}\boldsymbol{O}$，可以将这个约束视为隐式定义$\boldsymbol{O}$的方程。</p>
<p>设$F(\boldsymbol{O}, \boldsymbol{M}) = \boldsymbol{O}\boldsymbol{M}^{\top}\boldsymbol{O} - \boldsymbol{M} = \boldsymbol{0}$，根据隐函数定理<br />
\begin{equation}<br />
\frac{\partial \boldsymbol{O}}{\partial \boldsymbol{M}} = -\left(\frac{\partial F}{\partial \boldsymbol{O}}\right)^{-1}\frac{\partial F}{\partial \boldsymbol{M}}<br />
\end{equation}</p>
<p>这需要求解一个线性系统，但这个线性系统的结构正是我们前面讨论的Sylvester方程。</p>
<h3 id="svd">与SVD导数的关系<a class="toc-link" href="#svd" title="Permanent link">&para;</a></h3>
<p>前文提到，$\msign$可以通过SVD定义：$\msign(\boldsymbol{M}) = \boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>$。因此，$\msign$的导数与SVD的导数密切相关。}^{\top</p>
<p>SVD的导数公式已在文献中充分研究。对于$\boldsymbol{M} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，我们有<br />
\begin{align}<br />
d\boldsymbol{U} &amp;= (\boldsymbol{I} - \boldsymbol{U}\boldsymbol{U}^{\top})(d\boldsymbol{M})\boldsymbol{V}\boldsymbol{\Sigma}^{-1} + \boldsymbol{U}\boldsymbol{\Omega}_U\<br />
d\boldsymbol{V} &amp;= (\boldsymbol{I} - \boldsymbol{V}\boldsymbol{V}^{\top})(d\boldsymbol{M})^{\top}\boldsymbol{U}\boldsymbol{\Sigma}^{-1} + \boldsymbol{V}\boldsymbol{\Omega}_V\<br />
d\boldsymbol{\Sigma} &amp;= \boldsymbol{U}^{\top}(d\boldsymbol{M})\boldsymbol{V}<br />
\end{align}</p>
<p>其中$\boldsymbol{\Omega}<em ij="ij">U, \boldsymbol{\Omega}_V$是反对称矩阵，满足<br />
\begin{equation}<br />
[\boldsymbol{\Omega}_U]</em>} = \frac{\boldsymbol{u}_i^{\top}(d\boldsymbol{M})\boldsymbol{v}_j\sigma_j - \boldsymbol{u}_j^{\top}(d\boldsymbol{M})\boldsymbol{v}_i\sigma_i}{\sigma_i^2 - \sigma_j^2<br />
\end{equation}</p>
<p>对于$\msign(\boldsymbol{M}) = \boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]}\boldsymbol{V}</em>$，其导数为}^{\top<br />
\begin{equation}<br />
d[\msign(\boldsymbol{M})] = (d\boldsymbol{U}<em _:_:r_="[:,:r]">{[:,:r]})\boldsymbol{V}</em>}^{\top} + \boldsymbol{U<em _:_:r_="[:,:r]">{[:,:r]}(d\boldsymbol{V}</em>})^{\top<br />
\end{equation}</p>
<p>注意到$\msign$不依赖于$\boldsymbol{\Sigma}$（因为非零奇异值都变为1），所以$\msign$对奇异值的变化不敏感，只对奇异向量的变化敏感。</p>
<p>将SVD导数公式代入，经过复杂的代数运算，可以验证这与我们前面基于恒等式$\boldsymbol{M} = \boldsymbol{O}\boldsymbol{M}^{\top}\boldsymbol{O}$得到的结果是一致的。这为我们的推导提供了另一个验证。</p>
<p><strong>定理2（SVD导数与Sylvester方程的等价性）</strong>：基于SVD的$\msign$导数公式与基于Sylvester方程的解在数值上是等价的（当$\boldsymbol{M}$满秩时）。</p>
<p>证明思路：将SVD表达式代入Sylvester方程，利用奇异向量的正交性和特殊结构，可以证明两者给出相同的结果。详细证明较为冗长，这里省略。</p>
<h3 id="_10">链式法则的应用<a class="toc-link" href="#_10" title="Permanent link">&para;</a></h3>
<p>在深度学习中，$\msign$通常不是单独出现，而是作为某个复合函数的一部分。例如，在Muon优化器中，参数更新涉及到<br />
\begin{equation}<br />
\boldsymbol{W}_{t+1} = \boldsymbol{W}_t - \eta\msign(\boldsymbol{G}_t)<br />
\end{equation}</p>
<p>其中$\boldsymbol{G}_t$是梯度或动量。</p>
<p>如果我们要分析优化器的二阶性质（例如计算Hessian或进行超参数优化），就需要对优化器本身求导，这涉及到嵌套的链式法则。</p>
<p><strong>例1：嵌套msign</strong></p>
<p>考虑$f(\boldsymbol{M}) = \msign(\msign(\boldsymbol{M}))$。根据链式法则<br />
\begin{equation}<br />
\mathcal{D}f(\boldsymbol{M})[\boldsymbol{H}] = \mathcal{D}<a href="\msign(\boldsymbol{M})">\msign</a>[\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}]]<br />
\end{equation}</p>
<p>注意到$\msign(\msign(\boldsymbol{M})) = \msign(\boldsymbol{M})$（因为$\msign$是幂等的），所以外层导数实际上是在$\boldsymbol{O} = \msign(\boldsymbol{M})$处计算的。</p>
<p><strong>例2：msign的线性组合</strong></p>
<p>考虑$f(\boldsymbol{M}) = \boldsymbol{A}\msign(\boldsymbol{M})\boldsymbol{B}$，其中$\boldsymbol{A}, \boldsymbol{B}$是常数矩阵。导数为<br />
\begin{equation}<br />
\mathcal{D}f(\boldsymbol{M})[\boldsymbol{H}] = \boldsymbol{A}\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}]\boldsymbol{B}<br />
\end{equation}</p>
<p>在反向传播中，如果$\nabla_f\mathcal{L} = \boldsymbol{G}$，则<br />
\begin{equation}<br />
\nabla_{\msign(\boldsymbol{M})}\mathcal{L} = \boldsymbol{A}^{\top}\boldsymbol{G}\boldsymbol{B}^{\top}<br />
\end{equation}</p>
<p>然后再用我们的Sylvester方程方法计算$\nabla_{\boldsymbol{M}}\mathcal{L}$。</p>
<p><strong>例3：迹函数</strong></p>
<p>考虑$f(\boldsymbol{M}) = \tr(\boldsymbol{C}^{\top}\msign(\boldsymbol{M}))$，这在某些正则化项中出现。</p>
<p>\begin{equation}<br />
\frac{\partial f}{\partial \boldsymbol{M}} = \mathcal{D}<a href="\boldsymbol{M}">\msign</a>^*[\boldsymbol{C}]<br />
\end{equation}</p>
<p>其中$\mathcal{D}<a href="\boldsymbol{M}">\msign</a>^<em>$是伴随算子（adjoint operator），满足<br />
\begin{equation}<br />
\langle \mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}], \boldsymbol{C} \rangle = \langle \boldsymbol{H}, \mathcal{D}<a href="\boldsymbol{M}">\msign</a>^</em>[\boldsymbol{C}] \rangle<br />
\end{equation}</p>
<p>这正是我们反向传播算法计算的量。</p>
<h3 id="rnnttt">在RNN和TTT中的应用<a class="toc-link" href="#rnnttt" title="Permanent link">&para;</a></h3>
<p>Test-Time Training (TTT) 结合 Muon 优化器需要将优化步骤嵌入到模型的前向传播中。具体来说，RNN的隐藏状态更新可能涉及<br />
\begin{equation}<br />
\boldsymbol{h}_{t+1} = f(\boldsymbol{h}_t, \boldsymbol{x}_t; \boldsymbol{\theta}_t)<br />
\end{equation}</p>
<p>其中$\boldsymbol{\theta}<em t_1="t+1">t$通过Muon优化器在测试时更新：<br />
\begin{equation}<br />
\boldsymbol{\theta}</em>_t)} = \boldsymbol{\theta}_t - \eta\msign(\boldsymbol{g<br />
\end{equation}</p>
<p>$\boldsymbol{g}_t$是某种梯度或动量估计。</p>
<p>为了训练整个TTT-RNN系统，我们需要通过整个展开的计算图反向传播，这需要$\msign$的导数。</p>
<p><strong>梯度流分析</strong>：</p>
<p>设最终损失为$\mathcal{L}(\boldsymbol{h}<em t="0">T)$，我们需要计算$\frac{\partial\mathcal{L}}{\partial\boldsymbol{\theta}_0}$。根据链式法则<br />
\begin{equation}<br />
\frac{\partial\mathcal{L}}{\partial\boldsymbol{\theta}_0} = \sum</em>}^{T-1}\frac{\partial\mathcal{L}}{\partial\boldsymbol{\theta}_t}\frac{\partial\boldsymbol{\theta}_t}{\partial\boldsymbol{\theta}_0<br />
\end{equation}</p>
<p>其中<br />
\begin{equation}<br />
\frac{\partial\boldsymbol{\theta}_{t+1}}{\partial\boldsymbol{\theta}_t} = \boldsymbol{I} - \eta\frac{\partial\msign(\boldsymbol{g}_t)}{\partial\boldsymbol{\theta}_t}<br />
\end{equation}</p>
<p>如果$\boldsymbol{g}<em _boldsymbol_theta="\boldsymbol{\theta">t$本身依赖于$\boldsymbol{\theta}_t$（例如$\boldsymbol{g}_t = \nabla</em><em _text_aux="\text{aux">t}\mathcal{L}</em>$），则需要进一步展开链式法则。}</p>
<p>这种嵌套的微分结构使得TTT+Muon的训练在计算上非常昂贵，但我们的高效Sylvester求解方法可以显著减少计算成本。</p>
<h3 id="_11">数值稳定性考虑<a class="toc-link" href="#_11" title="Permanent link">&para;</a></h3>
<p>在实际实现中，数值稳定性至关重要。以下是几个关键的稳定性问题和解决方案：</p>
<p><strong>问题1：奇异值接近零</strong></p>
<p>当$\boldsymbol{M}$接近秩亏时，$\boldsymbol{A} = (\boldsymbol{M}\boldsymbol{M}^{\top})^{1/2}$和$\boldsymbol{B} = (\boldsymbol{M}^{\top}\boldsymbol{M})^{1/2}$会有接近零的特征值，导致Sylvester方程病态。</p>
<p><strong>解决方案</strong>：添加正则化项<br />
\begin{equation}<br />
\boldsymbol{A}<em _text_reg="\text{reg">{\text{reg}} = \boldsymbol{A} + \epsilon\boldsymbol{I}, \quad \boldsymbol{B}</em>}} = \boldsymbol{B} + \epsilon\boldsymbol{I<br />
\end{equation}</p>
<p>其中$\epsilon \sim 10^{-3}$。这相当于假设$\boldsymbol{M}$的最小奇异值至少为$\sqrt{2\epsilon}$。</p>
<p><strong>问题2：Newton-Schulz迭代的数值误差累积</strong></p>
<p>Newton-Schulz迭代是高阶方法，但每步都会引入舍入误差。多步迭代后，误差可能累积。</p>
<p><strong>解决方案</strong>：<br />
1. 使用混合精度计算（FP32用于累积，FP16用于矩阵乘法）<br />
2. 限制迭代次数（通常5-8步足够）<br />
3. 监控迭代收敛性，提前停止</p>
<p><strong>问题3：梯度爆炸/消失</strong></p>
<p>在反向传播中，梯度可能因为多次矩阵乘法而爆炸或消失。</p>
<p><strong>解决方案</strong>：<br />
1. 梯度裁剪（gradient clipping）<br />
2. 使用归一化的梯度（如$\frac{\nabla_{\boldsymbol{M}}\mathcal{L}}{\Vert\nabla_{\boldsymbol{M}}\mathcal{L}\Vert_F}$）<br />
3. 自适应学习率</p>
<p><strong>问题4：mcsgn的特征值要求</strong></p>
<p>$\mcsgn$要求矩阵的特征值不在负实轴上。对于增广矩阵$\boldsymbol{T}$，这通常是满足的，但在极端情况下可能失败。</p>
<p><strong>解决方案</strong>：<br />
1. 验证$\boldsymbol{M}\boldsymbol{O}^{\top}$和$\boldsymbol{O}^{\top}\boldsymbol{M}$的正定性<br />
2. 如果检测到负特征值，增加$\epsilon$<br />
3. 回退到直接SVD方法（作为备用方案）</p>
<p><strong>问题5：大矩阵的显存限制</strong></p>
<p>对于大规模矩阵（如$1000 \times 1000$），构造增广矩阵$\boldsymbol{T}$（大小为$2000 \times 2000$）可能超出显存。</p>
<p><strong>解决方案</strong>：<br />
1. 分块处理（如果矩阵有稀疏或分块结构）<br />
2. 使用低秩近似（Nyström方法）<br />
3. 迭代求解Sylvester方程（不显式构造$\boldsymbol{T}$）</p>
<h3 id="_12">计算复杂度分析<a class="toc-link" href="#_12" title="Permanent link">&para;</a></h3>
<p>让我们分析各种方法的计算复杂度：</p>
<p><strong>方法1：直接反向传播Newton-Schulz迭代</strong><br />
- 时间复杂度：$O(Tn^2m)$，其中$T$是迭代次数<br />
- 空间复杂度：$O(Tnm)$（存储所有中间状态）</p>
<p><strong>方法2：基于SVD的解析解</strong><br />
- 时间复杂度：$O(n^2m + nm^2)$（SVD计算）<br />
- 空间复杂度：$O(nm)$</p>
<p><strong>方法3：基于Sylvester方程和mcsgn</strong><br />
- 时间复杂度：$O(T'(n+m)^3)$，其中$T'$是mcsgn的迭代次数<br />
- 空间复杂度：$O((n+m)^2)$（增广矩阵）</p>
<p>对于$n \approx m$的方阵，方法3的时间复杂度约为$O(T'n^3)$。由于$T' \sim 5$-$8$通常远小于直接方法的操作数，且GPU上矩阵乘法高度优化，方法3在实践中往往最快。</p>
<p><strong>优化技巧</strong>：<br />
1. <strong>缓存复用</strong>：在前向传播中计算的$\boldsymbol{O}$在反向传播中复用<br />
2. <strong>融合算子</strong>：将多个矩阵操作融合为单个CUDA kernel<br />
3. <strong>异步计算</strong>：利用GPU的流（streams）并行计算不同部分<br />
4. <strong>量化</strong>：使用低精度（如FP16）加速，关键步骤用FP32</p>
<h3 id="_13">实验验证：梯度检验<a class="toc-link" href="#_13" title="Permanent link">&para;</a></h3>
<p>实现$\msign$的自定义梯度后，务必进行<strong>梯度检验</strong>（gradient check）以验证正确性。</p>
<p><strong>有限差分法</strong>：<br />
\begin{equation}<br />
\frac{\partial \mathcal{L}}{\partial M_{ij}} \approx \frac{\mathcal{L}(\boldsymbol{M} + \epsilon\boldsymbol{E}<em ij="ij">{ij}) - \mathcal{L}(\boldsymbol{M} - \epsilon\boldsymbol{E}</em>})}{2\epsilon<br />
\end{equation}</p>
<p>其中$\boldsymbol{E}_{ij}$是第$(i,j)$位置为1、其余为0的矩阵。</p>
<p><strong>检验代码示例</strong>：</p>
<pre class="highlight"><code class="language-python">def gradient_check(M, loss_fn, epsilon=1e-5):
    &quot;&quot;&quot;梯度检验&quot;&quot;&quot;
    # 自动微分计算的梯度
    M_tensor = torch.tensor(M, requires_grad=True)
    loss = loss_fn(msign(M_tensor))
    loss.backward()
    grad_auto = M_tensor.grad.numpy()

    # 有限差分计算的梯度
    grad_fd = np.zeros_like(M)
    for i in range(M.shape[0]):
        for j in range(M.shape[1]):
            M_plus = M.copy()
            M_plus[i, j] += epsilon
            M_minus = M.copy()
            M_minus[i, j] -= epsilon

            loss_plus = loss_fn(msign(torch.tensor(M_plus))).item()
            loss_minus = loss_fn(msign(torch.tensor(M_minus))).item()

            grad_fd[i, j] = (loss_plus - loss_minus) / (2 * epsilon)

    # 比较
    relative_error = np.linalg.norm(grad_auto - grad_fd) / np.linalg.norm(grad_fd)
    print(f&quot;Relative error: {relative_error}&quot;)
    return relative_error &lt; 1e-4  # 通过阈值
</code></pre>

<p>如果相对误差小于$10^{-4}$，则梯度实现基本正确。</p>
<h3 id="_14">高阶导数<a class="toc-link" href="#_14" title="Permanent link">&para;</a></h3>
<p>在某些高级应用中（如元学习、超参数优化），可能需要二阶导数（Hessian）。</p>
<p>$\msign$的二阶导数可以通过对一阶导数再次应用Fréchet导数得到。设<br />
\begin{equation}<br />
\mathcal{D}^2<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}_1, \boldsymbol{H}_2] = \mathcal{D}\left(\mathcal{D}<a href="\boldsymbol{M}">\msign</a>[\boldsymbol{H}_1]\right)[\boldsymbol{H}_2]<br />
\end{equation}</p>
<p>从一阶导数的Sylvester方程<br />
\begin{equation}<br />
\boldsymbol{A}\boldsymbol{X} + \boldsymbol{X}\boldsymbol{B} = \boldsymbol{C}<br />
\end{equation}</p>
<p>对$\boldsymbol{M}$再次求导，得到<br />
\begin{equation}<br />
(d\boldsymbol{A})\boldsymbol{X} + \boldsymbol{A}(d\boldsymbol{X}) + (d\boldsymbol{X})\boldsymbol{B} + \boldsymbol{X}(d\boldsymbol{B}) = d\boldsymbol{C}<br />
\end{equation}</p>
<p>其中$d\boldsymbol{A}, d\boldsymbol{B}, d\boldsymbol{C}$可以从一阶导数计算得出，而$d\boldsymbol{X}$就是我们要求的二阶导数，它满足另一个Sylvester方程。</p>
<p>这个过程可以递归地进行，理论上可以计算任意阶导数，但实际中很少需要三阶及以上。</p>
<h3 id="_15">小结<a class="toc-link" href="#_15" title="Permanent link">&para;</a></h3>
<p>在本节中，我们从多个角度深入探讨了$\msign$算子的导数理论：</p>
<ol>
<li><strong>Fréchet导数框架</strong>：建立了严格的数学基础，定义了矩阵函数的导数</li>
<li><strong>Lyapunov方程方法</strong>：将导数计算转化为标准的线性代数问题</li>
<li><strong>积分表示和谱分解</strong>：提供了另一个理论视角，揭示了与奇异值/特征值的关系</li>
<li><strong>反向传播算法</strong>：给出了实用的、GPU高效的梯度计算方法</li>
<li><strong>与SVD导数的联系</strong>：验证了不同方法的等价性</li>
<li><strong>链式法则应用</strong>：展示了如何在复杂的深度学习模型中使用$\msign$</li>
<li><strong>数值稳定性</strong>：讨论了实际实现中的各种数值问题和解决方案</li>
<li><strong>计算复杂度</strong>：分析了不同方法的效率，指出了最优选择</li>
</ol>
<p>这些理论和技术为在TTT+Muon等先进架构中使用$\msign$提供了坚实的基础。</p>
<h2 id="_16">文章小结<a class="toc-link" href="#_16" title="Permanent link">&para;</a></h2>
<p>本文讨论了$\msign$算子的导数计算，如果你关心"TTT + Muon"的组合，那么本文也许对你有帮助。</p>
<p><em><strong>转载到请包括本文地址：</strong><a href="https://spaces.ac.cn/archives/11025">https://spaces.ac.cn/archives/11025</a></em></p>
<p><em><strong>更详细的转载事宜请参考：</strong></em><a href="https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8" title="《科学空间FAQ》">《科学空间FAQ》</a></p>
<p><strong>如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。</strong></p>
<p><strong>如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！</strong></p>
<p>打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/wx.png" /></p>
<p>微信打赏</p>
<p><img alt="科学空间" src="https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png" /></p>
<p>支付宝打赏</p>
<p>因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以<a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=tN7d1drY3drrx8H0xcWa19vZ"><strong>点击这里</strong></a>或在下方评论区留言来告知你的建议或需求。</p>
<p><strong>如果您需要引用本文，请参考：</strong></p>
<p>苏剑林. (Jun. 13, 2025). 《msign的导数 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/11025">https://spaces.ac.cn/archives/11025</a></p>
<p>@online{kexuefm-11025,<br />
title={msign的导数},<br />
author={苏剑林},<br />
year={2025},<br />
month={Jun},<br />
url={\url{https://spaces.ac.cn/archives/11025}},<br />
}</p>
                </div>

                <!-- Previous/Next Navigation -->
                <nav class="post-navigation" aria-label="文章导航">
                    <div class="row g-3">
                        <div class="col-6">
                            
                            <a href="通过msign来计算奇异值裁剪mclip上.html" class="nav-link-prev">
                                <div class="nav-direction"><i class="fas fa-chevron-left"></i> 上一篇</div>
                                <div class="nav-title">#201 通过msign来计算奇异值裁剪mclip（上）</div>
                            </a>
                            
                        </div>
                        <div class="col-6">
                            
                            <a href="苏剑林-就是反向构造出来的.html" class="nav-link-next">
                                <div class="nav-direction">下一篇 <i class="fas fa-chevron-right"></i></div>
                                <div class="nav-title">#203 苏剑林: 就是反向构造出来的。</div>
                            </a>
                            
                        </div>
                    </div>
                </nav>

                <!-- Back to Home -->
                <div class="text-center mt-4 mb-4">
                    <a href="../index.html" class="btn btn-outline-primary">
                        <i class="fas fa-arrow-left"></i> 返回首页
                    </a>
                </div>
            </div>

            <!-- Sidebar (TOC) -->
            <div class="col-lg-3">
                <aside class="sidebar">
                    
                    <div class="toc-sidebar">
                        <h5 class="toc-title"><i class="fas fa-list"></i> 目录</h5>
                        <div class="toc-content">
                            <div class="toc">
<ul>
<li><a href="#msign">msign的导数</a><ul>
<li><a href="#_1">两种定义</a></li>
<li><a href="#_2">同一计算</a></li>
<li><a href="#_3">推导过程</a></li>
<li><a href="#_4">理论形式</a></li>
<li><a href="#_5">高效求解</a></li>
<li><a href="#_6">详细数学推导</a><ul>
<li><a href="#frechet">Fréchet导数的定义与性质</a></li>
<li><a href="#msignfrechet">msign的Fréchet导数显式公式</a></li>
<li><a href="#lyapunov">基于Lyapunov方程的推导</a></li>
<li><a href="#_7">基于积分表示的推导</a></li>
<li><a href="#_8">反向传播中的梯度计算</a></li>
<li><a href="#mcsgn">基于mcsgn的高效求解算法</a></li>
<li><a href="#newton-schulz">Newton-Schulz迭代的导数</a></li>
<li><a href="#_9">自动微分的实现</a></li>
<li><a href="#svd">与SVD导数的关系</a></li>
<li><a href="#_10">链式法则的应用</a></li>
<li><a href="#rnnttt">在RNN和TTT中的应用</a></li>
<li><a href="#_11">数值稳定性考虑</a></li>
<li><a href="#_12">计算复杂度分析</a></li>
<li><a href="#_13">实验验证：梯度检验</a></li>
<li><a href="#_14">高阶导数</a></li>
<li><a href="#_15">小结</a></li>
</ul>
</li>
<li><a href="#_16">文章小结</a></li>
</ul>
</li>
</ul>
</div>

                        </div>
                        <div class="toc-actions">
                            <button class="btn btn-sm btn-outline-secondary" onclick="expandAll()">
                                <i class="fas fa-expand"></i> 全部展开
                            </button>
                            <button class="btn btn-sm btn-outline-secondary" onclick="collapseAll()">
                                <i class="fas fa-compress"></i> 全部折叠
                            </button>
                        </div>
                    </div>
                    

                    <!-- Back to Top Button -->
                    <button id="backToTop" class="back-to-top" title="回到顶部">
                        <i class="fas fa-arrow-up"></i>
                    </button>
                </aside>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>
                博客来源: <a href="https://spaces.ac.cn" target="_blank">科学空间</a> |
                内容经过整理并添加详细数学推导
            </p>
            <p>
                Powered by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>
            </p>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Syntax highlighting -->
    <script>hljs.highlightAll();</script>

    <!-- Back to top functionality -->
    <script>
        // Show/hide back to top button based on scroll position
        window.addEventListener('scroll', function() {
            const backToTop = document.getElementById('backToTop');
            if (window.pageYOffset > 300) {
                backToTop.classList.add('show');
            } else {
                backToTop.classList.remove('show');
            }
        });

        // Smooth scroll to top
        document.getElementById('backToTop').addEventListener('click', function() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Sticky TOC sidebar
        window.addEventListener('scroll', function() {
            const sidebar = document.querySelector('.sidebar');
            if (!sidebar) return;

            const sidebarTop = sidebar.offsetTop;
            const scrollTop = window.pageYOffset;

            if (scrollTop > sidebarTop - 20) {
                sidebar.classList.add('sticky');
            } else {
                sidebar.classList.remove('sticky');
            }
        });
    </script>
</body>
</html>