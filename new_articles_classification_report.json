{
  "扩散模型": [
    {
      "title": "用albert和electra之前请确认你真的了解它们",
      "slug": "用albert和electra之前请确认你真的了解它们",
      "number": 245,
      "matched_keywords": [
        "ode"
      ],
      "confidence": 10
    },
    {
      "title": "中文任务还是sota吗我们给simcse补充了一些实验",
      "slug": "中文任务还是sota吗我们给simcse补充了一些实验",
      "number": 250,
      "matched_keywords": [
        "ode"
      ],
      "confidence": 10
    },
    {
      "title": "曾被嫌弃的预训练任务nsp做出了优秀的zero shot效果",
      "slug": "曾被嫌弃的预训练任务nsp做出了优秀的zero-shot效果",
      "number": 265,
      "matched_keywords": [
        "ode"
      ],
      "confidence": 10
    },
    {
      "title": "生成扩散模型漫谈三十一预测数",
      "slug": "生成扩散模型漫谈三十一预测数",
      "number": 298,
      "matched_keywords": [
        "扩散模型",
        "扩散",
        "diffusion",
        "ode"
      ],
      "confidence": 40
    },
    {
      "title": "强大的nvae以后再也不能说vae生成的图像模糊了",
      "slug": "强大的nvae以后再也不能说vae生成的图像模糊了",
      "number": 337,
      "matched_keywords": [
        "ode"
      ],
      "confidence": 10
    },
    {
      "title": "苏剑林 从公式12到公式15都在推导和解释你说的这个",
      "slug": "苏剑林-从公式12到公式15都在推导和解释你说的这个",
      "number": 350,
      "matched_keywords": [
        "扩散模型",
        "ddpm",
        "扩散",
        "得分匹配",
        "ode"
      ],
      "confidence": 70
    },
    {
      "title": "如何应对seq2seq中的根本停不下来问题",
      "slug": "如何应对seq2seq中的根本停不下来问题",
      "number": 354,
      "matched_keywords": [
        "ode",
        "consistency"
      ],
      "confidence": 20
    }
  ],
  "Transformer": [
    {
      "title": "bert4keras在手baseline我有百度lic2020",
      "slug": "bert4keras在手baseline我有百度lic2020",
      "number": 236,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "搜狐文本匹配基于条件layernorm的多任务baseline",
      "slug": "搜狐文本匹配基于条件layernorm的多任务baseline",
      "number": 237,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "一个二值化词向量模型是怎么跟果蝇搭上关系的",
      "slug": "一个二值化词向量模型是怎么跟果蝇搭上关系的",
      "number": 240,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "提速不掉点基于词颗粒度的中文wobert",
      "slug": "提速不掉点基于词颗粒度的中文wobert",
      "number": 242,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "线性注意力简史从模仿创新到反哺",
      "slug": "线性注意力简史从模仿创新到反哺",
      "number": 243,
      "matched_keywords": [
        "transformer",
        "attention",
        "注意力",
        "mla",
        "flash"
      ],
      "confidence": 63
    },
    {
      "title": "对抗训练浅谈意义方法和思考附keras实现",
      "slug": "对抗训练浅谈意义方法和思考附keras实现",
      "number": 244,
      "matched_keywords": [
        "rope"
      ],
      "confidence": 9
    },
    {
      "title": "鱼与熊掌兼得融合检索和生成的simbert模型",
      "slug": "鱼与熊掌兼得融合检索和生成的simbert模型",
      "number": 246,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "crf用过了不妨再了解下更快的memm",
      "slug": "crf用过了不妨再了解下更快的memm",
      "number": 247,
      "matched_keywords": [
        "attention",
        "softmax"
      ],
      "confidence": 18
    },
    {
      "title": "gelu的两个初等函数近似是怎么来的",
      "slug": "gelu的两个初等函数近似是怎么来的",
      "number": 248,
      "matched_keywords": [
        "transformer",
        "attention",
        "gau"
      ],
      "confidence": 27
    },
    {
      "title": "动手做个dialogpt基于lm的生成式多轮对话模型",
      "slug": "动手做个dialogpt基于lm的生成式多轮对话模型",
      "number": 253,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "通过互信息思想来缓解类别不平衡问题",
      "slug": "通过互信息思想来缓解类别不平衡问题",
      "number": 255,
      "matched_keywords": [
        "attention",
        "softmax"
      ],
      "confidence": 18
    },
    {
      "title": "用开源的人工标注数据来增强roformer sim",
      "slug": "用开源的人工标注数据来增强roformer-sim",
      "number": 257,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "2020年全年天象",
      "slug": "2020年全年天象",
      "number": 258,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "t5 pegasus开源一个中文生成式预训练模型",
      "slug": "t5-pegasus开源一个中文生成式预训练模型",
      "number": 259,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "从动力学角度看优化算法六为什么simsiam不退化",
      "slug": "从动力学角度看优化算法六为什么simsiam不退化",
      "number": 261,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "p tuning自动构建模版释放语言模型潜能",
      "slug": "p-tuning自动构建模版释放语言模型潜能",
      "number": 262,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "那个屠榜的t5模型现在可以在中文上玩玩了",
      "slug": "那个屠榜的t5模型现在可以在中文上玩玩了",
      "number": 264,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "nyströmformer基于矩阵分解的线性化attention方案",
      "slug": "nyströmformer基于矩阵分解的线性化attention方案",
      "number": 266,
      "matched_keywords": [
        "attention",
        "注意力",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "seq2seq中exposure bias现象的浅析与对策",
      "slug": "seq2seq中exposure-bias现象的浅析与对策",
      "number": 268,
      "matched_keywords": [
        "transformer",
        "attention",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "google新作synthesizer我们还不够了解自注意力",
      "slug": "google新作synthesizer我们还不够了解自注意力",
      "number": 269,
      "matched_keywords": [
        "transformer",
        "attention",
        "注意力",
        "softmax"
      ],
      "confidence": 36
    },
    {
      "title": "突破瓶颈打造更强大的transformer",
      "slug": "突破瓶颈打造更强大的transformer",
      "number": 270,
      "matched_keywords": [
        "transformer",
        "attention",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "基于conditional layer normalization的条件文本生成",
      "slug": "基于conditional-layer-normalization的条件文本生成",
      "number": 272,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "bert可以上几年级了seq2seq硬刚小学数学应用题",
      "slug": "bert可以上几年级了seq2seq硬刚小学数学应用题",
      "number": 273,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "transformer升级之路5作为无限维的线性attention",
      "slug": "transformer升级之路5作为无限维的线性attention",
      "number": 276,
      "matched_keywords": [
        "transformer",
        "attention",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "transformer升级之路19第二类旋转位置编码",
      "slug": "transformer升级之路19第二类旋转位置编码",
      "number": 277,
      "matched_keywords": [
        "transformer",
        "attention",
        "rope",
        "位置编码"
      ],
      "confidence": 36
    },
    {
      "title": "关于维度公式n 833 log n的可用性分析",
      "slug": "关于维度公式n-833-log-n的可用性分析",
      "number": 281,
      "matched_keywords": [
        "attention",
        "注意力"
      ],
      "confidence": 18
    },
    {
      "title": "mitchell近似乘法变为加法误差不超过19",
      "slug": "mitchell近似乘法变为加法误差不超过19",
      "number": 284,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "层次分解位置编码让bert可以处理超长文本",
      "slug": "层次分解位置编码让bert可以处理超长文本",
      "number": 286,
      "matched_keywords": [
        "attention",
        "位置编码",
        "外推"
      ],
      "confidence": 27
    },
    {
      "title": "必须要gpt3吗不bert的mlm模型也能小样本学习",
      "slug": "必须要gpt3吗不bert的mlm模型也能小样本学习",
      "number": 288,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "teaforn让teacher forcing更有远见一些",
      "slug": "teaforn让teacher-forcing更有远见一些",
      "number": 290,
      "matched_keywords": [
        "transformer",
        "attention",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "万能的seq2seq基于seq2seq的阅读理解问答",
      "slug": "万能的seq2seq基于seq2seq的阅读理解问答",
      "number": 291,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "realformer把残差转移到attention矩阵上面去",
      "slug": "realformer把残差转移到attention矩阵上面去",
      "number": 295,
      "matched_keywords": [
        "transformer",
        "attention",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "线性transformer应该不是你要等的那个模型",
      "slug": "线性transformer应该不是你要等的那个模型",
      "number": 299,
      "matched_keywords": [
        "transformer",
        "attention",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "抛开约束增强模型一行代码提升albert表现",
      "slug": "抛开约束增强模型一行代码提升albert表现",
      "number": 300,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "有限内存下全局打乱几百g文件python",
      "slug": "有限内存下全局打乱几百g文件python",
      "number": 302,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "transformer升级之路3从performer到线性attention",
      "slug": "transformer升级之路3从performer到线性attention",
      "number": 304,
      "matched_keywords": [
        "transformer",
        "attention",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "univae基于transformer的单模型多尺度的vae模型",
      "slug": "univae基于transformer的单模型多尺度的vae模型",
      "number": 306,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "transformer升级之路4二维位置的旋转式位置编码",
      "slug": "transformer升级之路4二维位置的旋转式位置编码",
      "number": 307,
      "matched_keywords": [
        "transformer",
        "attention",
        "rope",
        "位置编码"
      ],
      "confidence": 36
    },
    {
      "title": "printemps 苏神怎么看log linear attention这种变长的h",
      "slug": "printemps-苏神怎么看log-linear-attention这种变长的h",
      "number": 310,
      "matched_keywords": [
        "attention",
        "注意力",
        "softmax"
      ],
      "confidence": 27
    },
    {
      "title": "学会提问的bert端到端地从篇章中构建问答对",
      "slug": "学会提问的bert端到端地从篇章中构建问答对",
      "number": 313,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "将softmax交叉熵推广到多标签分类问题",
      "slug": "将softmax交叉熵推广到多标签分类问题",
      "number": 314,
      "matched_keywords": [
        "attention",
        "softmax"
      ],
      "confidence": 18
    },
    {
      "title": "短文本匹配baseline脱敏数据使用预训练模型的尝试",
      "slug": "短文本匹配baseline脱敏数据使用预训练模型的尝试",
      "number": 316,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "我们可以无损放大一个transformer模型吗一",
      "slug": "我们可以无损放大一个transformer模型吗一",
      "number": 317,
      "matched_keywords": [
        "transformer",
        "attention",
        "注意力"
      ],
      "confidence": 27
    },
    {
      "title": "transformer升级之路2博采众长的旋转式位置编码",
      "slug": "transformer升级之路2博采众长的旋转式位置编码",
      "number": 318,
      "matched_keywords": [
        "transformer",
        "attention",
        "rope",
        "位置编码"
      ],
      "confidence": 36
    },
    {
      "title": "globalpointer用统一的方式处理嵌套和非嵌套ner",
      "slug": "globalpointer用统一的方式处理嵌套和非嵌套ner",
      "number": 319,
      "matched_keywords": [
        "transformer",
        "attention",
        "rope",
        "位置编码"
      ],
      "confidence": 36
    },
    {
      "title": "expx在x0处的偶次泰勒展开式总是正的",
      "slug": "expx在x0处的偶次泰勒展开式总是正的",
      "number": 320,
      "matched_keywords": [
        "attention",
        "softmax"
      ],
      "confidence": 18
    },
    {
      "title": "spaces抽取 生成式长文本摘要法研杯总结",
      "slug": "spaces抽取-生成式长文本摘要法研杯总结",
      "number": 322,
      "matched_keywords": [
        "attention",
        "softmax"
      ],
      "confidence": 18
    },
    {
      "title": "苏剑林 相对位置编码似乎没有太多选择了要不rope这种算是乘性了",
      "slug": "苏剑林-相对位置编码似乎没有太多选择了要不rope这种算是乘性了",
      "number": 323,
      "matched_keywords": [
        "transformer",
        "attention",
        "rope",
        "位置编码",
        "外推"
      ],
      "confidence": 45
    },
    {
      "title": "performer用随机投影将attention的复杂度线性化",
      "slug": "performer用随机投影将attention的复杂度线性化",
      "number": 326,
      "matched_keywords": [
        "attention",
        "softmax"
      ],
      "confidence": 18
    },
    {
      "title": "线性attention的探索attention必须有个softmax吗",
      "slug": "线性attention的探索attention必须有个softmax吗",
      "number": 327,
      "matched_keywords": [
        "transformer",
        "attention",
        "注意力",
        "softmax"
      ],
      "confidence": 36
    },
    {
      "title": "第1000篇文章",
      "slug": "第1000篇文章",
      "number": 329,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "日食记",
      "slug": "日食记",
      "number": 333,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "从三角不等式到margin softmax",
      "slug": "从三角不等式到margin-softmax",
      "number": 334,
      "matched_keywords": [
        "attention",
        "softmax"
      ],
      "confidence": 18
    },
    {
      "title": "你的crf层的学习率可能不够大",
      "slug": "你的crf层的学习率可能不够大",
      "number": 335,
      "matched_keywords": [
        "attention",
        "softmax"
      ],
      "confidence": 18
    },
    {
      "title": "无监督语义相似度哪家强我们做了个比较全面的评测",
      "slug": "无监督语义相似度哪家强我们做了个比较全面的评测",
      "number": 336,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "龟鱼记全陶粒的同程底滤生态缸",
      "slug": "龟鱼记全陶粒的同程底滤生态缸",
      "number": 338,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "现在可以用keras玩中文gpt2了gpt2 ml",
      "slug": "现在可以用keras玩中文gpt2了gpt2-ml",
      "number": 342,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "也来扯几句全国青少年科技创新大赛",
      "slug": "也来扯几句全国青少年科技创新大赛",
      "number": 347,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "也来盘点一些最近的非transformer工作",
      "slug": "也来盘点一些最近的非transformer工作",
      "number": 348,
      "matched_keywords": [
        "transformer",
        "attention",
        "注意力"
      ],
      "confidence": 27
    },
    {
      "title": "当gpt遇上中国象棋写过文章解过题要不再来下盘棋",
      "slug": "当gpt遇上中国象棋写过文章解过题要不再来下盘棋",
      "number": 351,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "无监督分词和句法分析原来bert还可以这样用",
      "slug": "无监督分词和句法分析原来bert还可以这样用",
      "number": 352,
      "matched_keywords": [
        "attention",
        "注意力"
      ],
      "confidence": 18
    },
    {
      "title": "bert of theseus基于模块替换的模型压缩方法",
      "slug": "bert-of-theseus基于模块替换的模型压缩方法",
      "number": 353,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    },
    {
      "title": "最小熵原理六词向量的维度应该怎么选择",
      "slug": "最小熵原理六词向量的维度应该怎么选择",
      "number": 355,
      "matched_keywords": [
        "transformer"
      ],
      "confidence": 9
    },
    {
      "title": "transformer升级之路1sinusoidal位置编码追根溯源",
      "slug": "transformer升级之路1sinusoidal位置编码追根溯源",
      "number": 356,
      "matched_keywords": [
        "transformer",
        "attention",
        "位置编码"
      ],
      "confidence": 27
    },
    {
      "title": "跟风玩玩目前最大的中文gpt2模型bert4keras",
      "slug": "跟风玩玩目前最大的中文gpt2模型bert4keras",
      "number": 357,
      "matched_keywords": [
        "attention"
      ],
      "confidence": 9
    },
    {
      "title": "修改transformer结构设计一个更快更好的mlm模型",
      "slug": "修改transformer结构设计一个更快更好的mlm模型",
      "number": 358,
      "matched_keywords": [
        "transformer",
        "attention"
      ],
      "confidence": 18
    }
  ],
  "优化理论": [
    {
      "title": "让炼丹更科学一些三sgd的终",
      "slug": "让炼丹更科学一些三sgd的终",
      "number": 238,
      "matched_keywords": [
        "优化器",
        "sgd"
      ],
      "confidence": 16
    },
    {
      "title": "让炼丹更科学一些二将结论推广",
      "slug": "让炼丹更科学一些二将结论推广",
      "number": 239,
      "matched_keywords": [
        "优化器",
        "sgd",
        "学习率"
      ],
      "confidence": 24
    },
    {
      "title": "muon优化器指南快速上手与关键细节",
      "slug": "muon优化器指南快速上手与关键细节",
      "number": 241,
      "matched_keywords": [
        "优化器",
        "optimizer",
        "adam",
        "muon",
        "学习率"
      ],
      "confidence": 40
    },
    {
      "title": "我们真的需要把训练集的损失降低到零吗",
      "slug": "我们真的需要把训练集的损失降低到零吗",
      "number": 254,
      "matched_keywords": [
        "学习率",
        "梯度下降"
      ],
      "confidence": 16
    },
    {
      "title": "殊途同归的策略梯度与零阶优化",
      "slug": "殊途同归的策略梯度与零阶优化",
      "number": 256,
      "matched_keywords": [
        "优化器",
        "adam",
        "sgd"
      ],
      "confidence": 24
    },
    {
      "title": "对比学习可以使用梯度累积吗",
      "slug": "对比学习可以使用梯度累积吗",
      "number": 260,
      "matched_keywords": [
        "优化器",
        "batch size"
      ],
      "confidence": 24
    },
    {
      "title": "节省显存的重计算技巧也有了keras版了",
      "slug": "节省显存的重计算技巧也有了keras版了",
      "number": 278,
      "matched_keywords": [
        "batch size"
      ],
      "confidence": 16
    },
    {
      "title": "让炼丹更科学一些四新恒等式",
      "slug": "让炼丹更科学一些四新恒等式",
      "number": 279,
      "matched_keywords": [
        "优化器",
        "sgd",
        "学习率",
        "learning rate"
      ],
      "confidence": 40
    },
    {
      "title": "从动力学角度看优化算法五为什么学习率不宜过小",
      "slug": "从动力学角度看优化算法五为什么学习率不宜过小",
      "number": 283,
      "matched_keywords": [
        "优化器",
        "学习率",
        "梯度下降"
      ],
      "confidence": 24
    },
    {
      "title": "苏剑林 是梯度均值为零的假设这个问题不是在数值模拟一节讨论过了吗",
      "slug": "苏剑林-是梯度均值为零的假设这个问题不是在数值模拟一节讨论过了吗",
      "number": 293,
      "matched_keywords": [
        "优化器",
        "adam",
        "muon",
        "学习率"
      ],
      "confidence": 32
    },
    {
      "title": "也来谈谈rnn的梯度消失爆炸问题",
      "slug": "也来谈谈rnn的梯度消失爆炸问题",
      "number": 301,
      "matched_keywords": [
        "优化器",
        "梯度下降"
      ],
      "confidence": 16
    },
    {
      "title": "l2正则没有想象那么好可能是权重尺度偏移惹的祸",
      "slug": "l2正则没有想象那么好可能是权重尺度偏移惹的祸",
      "number": 303,
      "matched_keywords": [
        "优化器",
        "adam",
        "sgd",
        "学习率"
      ],
      "confidence": 32
    },
    {
      "title": "泛化性乱弹从随机噪声梯度惩罚到虚拟对抗训练",
      "slug": "泛化性乱弹从随机噪声梯度惩罚到虚拟对抗训练",
      "number": 315,
      "matched_keywords": [
        "batch size",
        "梯度下降"
      ],
      "confidence": 24
    },
    {
      "title": "隐藏在动量中的梯度累积少更新几步效果反而更好",
      "slug": "隐藏在动量中的梯度累积少更新几步效果反而更好",
      "number": 324,
      "matched_keywords": [
        "优化器",
        "sgd",
        "batch size",
        "梯度下降"
      ],
      "confidence": 40
    },
    {
      "title": "从动力学角度看优化算法七sgd svm",
      "slug": "从动力学角度看优化算法七sgd-svm",
      "number": 325,
      "matched_keywords": [
        "sgd",
        "梯度下降"
      ],
      "confidence": 16
    },
    {
      "title": "滑动平均视角下的权重衰减和学习率",
      "slug": "滑动平均视角下的权重衰减和学习率",
      "number": 331,
      "matched_keywords": [
        "优化器",
        "adam",
        "sgd",
        "muon",
        "学习率"
      ],
      "confidence": 72
    },
    {
      "title": "adafactor优化器浅析附开源实现",
      "slug": "adafactor优化器浅析附开源实现",
      "number": 332,
      "matched_keywords": [
        "优化器",
        "adam",
        "sgd",
        "adafactor",
        "学习率"
      ],
      "confidence": 72
    },
    {
      "title": "为什么梯度裁剪能加速训练过程一个简明的分析",
      "slug": "为什么梯度裁剪能加速训练过程一个简明的分析",
      "number": 340,
      "matched_keywords": [
        "优化器",
        "学习率",
        "梯度下降"
      ],
      "confidence": 24
    },
    {
      "title": "simbertv2来了融合检索和生成的roformer sim模型",
      "slug": "simbertv2来了融合检索和生成的roformer-sim模型",
      "number": 341,
      "matched_keywords": [
        "batch size"
      ],
      "confidence": 16
    },
    {
      "title": "flatnce小批次对比学习效果差的原因竟是浮点误差",
      "slug": "flatnce小批次对比学习效果差的原因竟是浮点误差",
      "number": 343,
      "matched_keywords": [
        "优化器",
        "adam",
        "学习率",
        "batch size"
      ],
      "confidence": 40
    },
    {
      "title": "adax优化器浅析附开源实现",
      "slug": "adax优化器浅析附开源实现",
      "number": 344,
      "matched_keywords": [
        "优化器",
        "adam",
        "adafactor",
        "学习率"
      ],
      "confidence": 32
    },
    {
      "title": "adamw的weight rms的渐近估计上",
      "slug": "adamw的weight-rms的渐近估计上",
      "number": 345,
      "matched_keywords": [
        "优化器",
        "adam",
        "batch size"
      ],
      "confidence": 32
    }
  ],
  "矩阵理论": [
    {
      "title": "seq2seq重复解码现象的理论分析尝试",
      "slug": "seq2seq重复解码现象的理论分析尝试",
      "number": 251,
      "matched_keywords": [
        "矩阵"
      ],
      "confidence": 7
    },
    {
      "title": "为什么deltanet要加l2 n",
      "slug": "为什么deltanet要加l2-n",
      "number": 252,
      "matched_keywords": [
        "矩阵",
        "特征值",
        "eigenvalue",
        "正交"
      ],
      "confidence": 28
    },
    {
      "title": "从一个单位向量变换到另一个单位向量的正交矩阵",
      "slug": "从一个单位向量变换到另一个单位向量的正交矩阵",
      "number": 287,
      "matched_keywords": [
        "矩阵",
        "matrix",
        "正交"
      ],
      "confidence": 21
    },
    {
      "title": "让人惊叹的johnson lindenstrauss引理应用篇",
      "slug": "让人惊叹的johnson-lindenstrauss引理应用篇",
      "number": 305,
      "matched_keywords": [
        "矩阵",
        "svd",
        "低秩",
        "奇异值",
        "正交"
      ],
      "confidence": 35
    },
    {
      "title": "self orthogonality module一个即插即用的核正交化模块",
      "slug": "self-orthogonality-module一个即插即用的核正交化模块",
      "number": 339,
      "matched_keywords": [
        "矩阵",
        "matrix",
        "正交",
        "orthogonal"
      ],
      "confidence": 28
    }
  ],
  "随机矩阵/概率": [
    {
      "title": "从emdwmd到wrd文本向量序列的相似度计算",
      "slug": "从emdwmd到wrd文本向量序列的相似度计算",
      "number": 235,
      "matched_keywords": [
        "分布"
      ],
      "confidence": 6
    },
    {
      "title": "两个多元正态分布的kl散度巴氏距离和w距离",
      "slug": "两个多元正态分布的kl散度巴氏距离和w距离",
      "number": 249,
      "matched_keywords": [
        "概率",
        "正态",
        "分布",
        "方差",
        "统计"
      ],
      "confidence": 30
    },
    {
      "title": "如何划分一个跟测试集更接近的验证集",
      "slug": "如何划分一个跟测试集更接近的验证集",
      "number": 267,
      "matched_keywords": [
        "概率",
        "分布"
      ],
      "confidence": 12
    },
    {
      "title": "你可能不需要bert flow一个线性变换媲美bert flow",
      "slug": "你可能不需要bert-flow一个线性变换媲美bert-flow",
      "number": 271,
      "matched_keywords": [
        "正态",
        "分布",
        "方差",
        "统计"
      ],
      "confidence": 24
    },
    {
      "title": "变分自编码器六从几何视角来理解vae的尝试",
      "slug": "变分自编码器六从几何视角来理解vae的尝试",
      "number": 274,
      "matched_keywords": [
        "概率",
        "分布",
        "贝叶斯"
      ],
      "confidence": 18
    },
    {
      "title": "浅谈transformer的初始化参数化与标准化",
      "slug": "浅谈transformer的初始化参数化与标准化",
      "number": 275,
      "matched_keywords": [
        "随机",
        "random",
        "正态",
        "分布",
        "distribution"
      ],
      "confidence": 36
    },
    {
      "title": "搜出来的文本三基于bert的文本采样",
      "slug": "搜出来的文本三基于bert的文本采样",
      "number": 280,
      "matched_keywords": [
        "概率",
        "随机",
        "random",
        "分布"
      ],
      "confidence": 24
    },
    {
      "title": "变分自编码器七球面上的vaevmf vae",
      "slug": "变分自编码器七球面上的vaevmf-vae",
      "number": 282,
      "matched_keywords": [
        "概率",
        "分布"
      ],
      "confidence": 12
    },
    {
      "title": "非自回归也不差基于mlm的阅读理解问答",
      "slug": "非自回归也不差基于mlm的阅读理解问答",
      "number": 289,
      "matched_keywords": [
        "概率",
        "分布"
      ],
      "confidence": 12
    },
    {
      "title": "苏剑林 x与单位阵的平均平方误差mse作为它跟单位阵的差距有什",
      "slug": "苏剑林-x与单位阵的平均平方误差mse作为它跟单位阵的差距有什",
      "number": 292,
      "matched_keywords": [
        "概率",
        "随机",
        "random",
        "正态",
        "分布"
      ],
      "confidence": 48
    },
    {
      "title": "搜出来的文本二从mcmc到模拟退火",
      "slug": "搜出来的文本二从mcmc到模拟退火",
      "number": 294,
      "matched_keywords": [
        "概率",
        "随机",
        "分布"
      ],
      "confidence": 18
    },
    {
      "title": "搜出来的文本一从文本生成到搜索采样",
      "slug": "搜出来的文本一从文本生成到搜索采样",
      "number": 297,
      "matched_keywords": [
        "概率",
        "分布"
      ],
      "confidence": 12
    },
    {
      "title": "用bert4keras做三元组抽取",
      "slug": "用bert4keras做三元组抽取",
      "number": 308,
      "matched_keywords": [
        "概率",
        "分布"
      ],
      "confidence": 12
    },
    {
      "title": "让人惊叹的johnson lindenstrauss引理理论篇",
      "slug": "让人惊叹的johnson-lindenstrauss引理理论篇",
      "number": 309,
      "matched_keywords": [
        "概率",
        "随机",
        "分布"
      ],
      "confidence": 18
    },
    {
      "title": "eae自编码器 bn 最大熵 生成模型",
      "slug": "eae自编码器-bn-最大熵-生成模型",
      "number": 311,
      "matched_keywords": [
        "随机",
        "正态",
        "分布",
        "方差",
        "统计"
      ],
      "confidence": 30
    },
    {
      "title": "designing gans又一个gan生产车间",
      "slug": "designing-gans又一个gan生产车间",
      "number": 312,
      "matched_keywords": [
        "概率",
        "分布"
      ],
      "confidence": 12
    },
    {
      "title": "搜出来的文本四通过增删改来用词造句",
      "slug": "搜出来的文本四通过增删改来用词造句",
      "number": 321,
      "matched_keywords": [
        "概率",
        "分布",
        "期望"
      ],
      "confidence": 18
    },
    {
      "title": "从采样看优化可导优化与不可导优化的统一视角",
      "slug": "从采样看优化可导优化与不可导优化的统一视角",
      "number": 328,
      "matched_keywords": [
        "概率",
        "随机",
        "分布",
        "期望"
      ],
      "confidence": 24
    },
    {
      "title": "积分梯度一种新颖的神经网络可视化方法",
      "slug": "积分梯度一种新颖的神经网络可视化方法",
      "number": 330,
      "matched_keywords": [
        "概率",
        "期望",
        "统计"
      ],
      "confidence": 18
    },
    {
      "title": "概率视角下的线性模型逻辑回归有解析解吗",
      "slug": "概率视角下的线性模型逻辑回归有解析解吗",
      "number": 346,
      "matched_keywords": [
        "概率",
        "正态",
        "分布",
        "distribution",
        "方差"
      ],
      "confidence": 36
    },
    {
      "title": "变分自编码器五vae bn 更好的vae",
      "slug": "变分自编码器五vae-bn-更好的vae",
      "number": 349,
      "matched_keywords": [
        "分布",
        "贝叶斯"
      ],
      "confidence": 12
    }
  ],
  "损失函数": [
    {
      "title": "又是dropout两次这次它做到了有监督任务的sota",
      "slug": "又是dropout两次这次它做到了有监督任务的sota",
      "number": 285,
      "matched_keywords": [
        "损失",
        "loss",
        "交叉熵",
        "kl",
        "对比学习"
      ],
      "confidence": 25
    },
    {
      "title": "再谈类别不平衡问题调节权重与魔改loss的对比联系",
      "slug": "再谈类别不平衡问题调节权重与魔改loss的对比联系",
      "number": 296,
      "matched_keywords": [
        "损失",
        "loss",
        "交叉熵",
        "cross entropy"
      ],
      "confidence": 25
    }
  ],
  "梯度分析": [
    {
      "title": "wgan的成功可能跟wasserstein距离没啥关系",
      "slug": "wgan的成功可能跟wasserstein距离没啥关系",
      "number": 263,
      "matched_keywords": [
        "梯度",
        "gradient",
        "梯度惩罚",
        "gradient penalty"
      ],
      "confidence": 20
    }
  ],
  "RNN/SSM": [],
  "VQ/量化": [],
  "语言模型": [],
  "其他": []
}