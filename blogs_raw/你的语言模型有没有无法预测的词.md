---
title: 你的语言模型有没有“无法预测的词”？
slug: 你的语言模型有没有无法预测的词
date: 2022-04-20
tags: 语言模型, 多任务, 生成模型, attention, 优化
status: pending
---

# 你的语言模型有没有“无法预测的词”？

**原文链接**: [https://spaces.ac.cn/archives/9046](https://spaces.ac.cn/archives/9046)

**发布日期**: 

---

众所周知，分类模型通常都是先得到编码向量，然后接一个Dense层预测每个类别的概率，而预测时则是输出概率最大的类别。但大家是否想过这样一种可能：训练好的分类模型可能存在“无法预测的类别”，即不管输入是什么，都不可能预测出某个类别$k$，类别$k$永远不可能成为概率最大的那个。

当然，这种情况一般只出现在类别数远远超过编码向量维度的场景，常规的分类问题很少这么极端的。然而，我们知道语言模型本质上也是一个分类模型，它的类别数也就是词表的总大小，往往是远超过向量维度的，那么我们的语言模型是否有“无法预测的词”？（只考虑Greedy解码）

## 是否存在 #

ACL2022的论文[《Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice》](https://papers.cool/arxiv/2203.06462)首先探究了这个问题，正如其标题所言，答案是“理论上存在但实际出现概率很小”。

首先我们来看“理论上存在”。为了证明其存在性，我们只需要具体地构建一个例子。设各个类别向量分为$\boldsymbol{w}_1,\boldsymbol{w}_2,\cdots,\boldsymbol{w}_n\in\mathbb{R}^d$，偏置项为$b_1,b_2,\cdots,b_n$，假设类别$k$是可预测的，那么就存在$\boldsymbol{z}\in\mathbb{R}^d$，同时满足  
\begin{equation}\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k > \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i\quad (\forall i \neq k)\end{equation}  
反过来，如果类别$k$不可预测，那么对于任意$\boldsymbol{z}\in\mathbb{R}^d$，必须存在某个$i\neq k$，满足  
\begin{equation}\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k \leq \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i\end{equation}  
由于现在我们只需要举例子，所以简单起见我们先考虑无偏置项的情况，并设$k=n$，此时条件为$\langle \boldsymbol{w}_i - \boldsymbol{w}_n, \boldsymbol{z}\rangle \geq 0$，也就是说，任意向量$\boldsymbol{z}$必然能找到向量$\boldsymbol{w}_i - \boldsymbol{w}_n$与之夹角小于等于90度。不难想象，当向量数大于空间维度、向量均匀分布在空间中时，这是有可能出现的，比如二维平面上的任意向量，就必然与$(0,1),(1,0),(0,-1),(-1,0)$之一的夹角小于90度，从而我们可以构造出例子：  
\begin{equation}\left\\{\begin{aligned}  
&\boldsymbol{w}_5 = (1, 1) \quad(\boldsymbol{w}_5\text{可以随便选})\\\  
&\boldsymbol{w}_1 = (1, 1) + (0, 1) = (1, 2)\\\  
&\boldsymbol{w}_2 = (1, 1) + (1, 0) = (2, 1)\\\  
&\boldsymbol{w}_3 = (1, 1) + (0, -1) = (1, 0)\\\  
&\boldsymbol{w}_4 = (1, 1) + (-1, 0) = (0, 1)\\\  
\end{aligned}\right.\end{equation}  
在这个例子中，类别5就是不可预测的了，不信大家可以代入一些$\boldsymbol{z}$试试。

## 怎么判断 #

现在我们已经确认了“无法预测的类别”是可能存在的，那么一个很自然的问题就是，对于一个训练好的模型，也就是给定$\boldsymbol{w}_1,\boldsymbol{w}_2,\cdots,\boldsymbol{w}_n\in\mathbb{R}^d$和$b_1,b_2,\cdots,b_n$，怎么判断其中是否存在不可预测的类别呢？

根据前一节的描述，从解不等式的角度来看，如果类别$k$是可预测的，那么下述不等式组的解集就会非空  
\begin{equation}\langle\boldsymbol{w}_k - \boldsymbol{w}_i,\boldsymbol{z}\rangle + (b_k - b_i) > 0\quad (\forall i \neq k)\end{equation}  
不失一般性，我们同样设$k=n$，并且记$\Delta\boldsymbol{w}_i = \boldsymbol{w}_n - \boldsymbol{w}_i, \Delta b_i = b_n - b_i$，留意到  
\begin{equation}\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i > 0\,(i = 1,2,\cdots,n-1)\quad\Leftrightarrow\quad \min_i \langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i > 0\end{equation}  
所以，只要我们尽量最大化$\min\limits_i \langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i$，如果最终结果是正的，那么类别$n$就是可预测的，否则就是不可预测的。如果之前读过[《多任务学习漫谈（二）：行梯度之事》](/archives/8896)的读者，就会发现该问题“似曾相识”，特别是如果没有偏置项的情况下，它跟多任务学习中寻找“帕累托最优”的过程是几乎一致的。

现在问题变为  
\begin{equation}\max_{\boldsymbol{z}} \min_i \langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i\end{equation}  
为了避免发散到无穷，我们可以加个约束$\Vert \boldsymbol{z}\Vert\leq r$：  
\begin{equation}\max_{\Vert \boldsymbol{z}\Vert\leq r} \min_i \langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i \end{equation}  
其中$r$是一个常数，只要$r$取得足够大，它就能跟实际情况足够吻合，因为神经网络的输出通常来说也是有界的。接下来的过程就跟[《多任务学习漫谈（二）：行梯度之事》](/archives/8896)的几乎一样了，首先引入  
\begin{equation}\mathbb{P}^{n-1} = \left\\{(\alpha_1,\alpha_2,\cdots,\alpha_{n-1})\left|\alpha_1,\alpha_2,\cdots,\alpha_{n-1}\geq 0, \sum_i \alpha_i = 1\right.\right\\}\end{equation}  
那么问题变成  
\begin{equation}\max_{\Vert \boldsymbol{z}\Vert\leq r} \min_{\alpha\in\mathbb{P}^{n-1}} \left\langle\sum_i \alpha_i \Delta\boldsymbol{w}_i,\boldsymbol{z}\right\rangle + \sum_i \alpha_i \Delta b_i\end{equation}  
根据冯·诺依曼的[Minimax定理](https://en.wikipedia.org/wiki/Minimax_theorem)，可以交换$\max$和$\min$的顺序  
\begin{equation}\min_{\alpha\in\mathbb{P}^{n-1}} \max_{\Vert \boldsymbol{z}\Vert\leq r}\left\langle\sum_i \alpha_i \Delta\boldsymbol{w}_i,\boldsymbol{z}\right\rangle + \sum_i \alpha_i \Delta b_i\end{equation}  
很显然，$\max$这一步在$\Vert\boldsymbol{z}\Vert=r$且$\boldsymbol{z}$跟$\sum\limits_i \alpha_i \Delta\boldsymbol{w}_i$同向时取到，结果为  
\begin{equation}\min_{\alpha\in\mathbb{P}^{n-1}} r\left\Vert\sum_i \alpha_i \Delta\boldsymbol{w}_i\right\Vert + \sum_i \alpha_i \Delta b_i\end{equation}  
当$r$足够大时，偏置项的影响就非常小了，所以这几乎就等价于没有偏置项的情形  
\begin{equation}\min_{\alpha\in\mathbb{P}^{n-1}} \left\Vert\sum_i \alpha_i \Delta\boldsymbol{w}_i\right\Vert\end{equation}  
最后的$\min$的求解过程已经在[《多任务学习漫谈（二）：行梯度之事》](/archives/8896)中讨论过了，主要用到了[Frank-Wolfe算法](https://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm)，不再重复。

**（注：以上判别过程是笔者自己给出的，跟论文[《Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice》](https://papers.cool/arxiv/2203.06462)中的方法并不相同。）**

## 实践如何 #

前面的讨论都是理论上的，那么实际的语言模型出现“无法预测的词”的概率大不大呢？原论文对一些训练好的语言模型和生成模型进行了检验，发现实际上出现的概率很小，比如下表中的机器翻译模型检验结果：  


[![机器翻译模型的检验结果](/usr/uploads/2022/04/3980694051.png)](/usr/uploads/2022/04/3980694051.png "点击查看原图")

机器翻译模型的检验结果

其实这不难理解，从前面的讨论中我们知道“无法预测的词”一般只出现在类别数远远大于向量维度的情况，也就是原论文标题中的“Low-Rank”。但由于“维度灾难”的原因，“远远大于”这个概念其实并非我们直观所想的那样，比如对于2维空间来说，类别数为4就可以称得上“远远大于”，但如果是200维空间，那么即便是类别数为40000也算不上“远远大于”。常见的语言模型向量维度基本上都有几百维，而词表顶多也就是数十万的级别，因此其实还是算不上“远远大于”，因此出现“无法预测的词”的概率就很小了。

另外，我们还可以证明，如果所有的$\boldsymbol{w}_i$互不相同但是模长都相等，那么是绝对不会出现“无法预测的词”，因此这种不可预测的情况只出现在$\boldsymbol{w}_i$模长差异较大的情况，而在当前主流的深度模型中，由于各种Normalization技术的应用，$\boldsymbol{w}_i$模长差异较大的情况很少出现了，这进一步降低了“无法预测的词”的出现概率了。

当然，还是文章开头说了，本文的“无法预测的词”指的是最大化预测，也就是Greedy Search，如果用Beam Search或者随机采样，那么即便存在“无法预测的词”，也依然是可能生成出来的。这个“无法预测的词”，更多是一个好玩但实用价值不大的理论概念了，

## 最后小结 #

本文向大家介绍了一个没什么实用价值但是颇为有意思的现象：你的语言模型可能存在一些“无法预测的词”，它永远不可能成为概率最大者。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/9046>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Apr. 20, 2022). 《你的语言模型有没有“无法预测的词”？ 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/9046>

@online{kexuefm-9046,  
title={你的语言模型有没有“无法预测的词”？},  
author={苏剑林},  
year={2022},  
month={Apr},  
url={\url{https://spaces.ac.cn/archives/9046}},  
} 


---

## 公式推导与注释

### 一、不可预测类别的数学定义

#### 1.1 分类模型的标准形式

对于一个$n$类分类模型，设编码向量为$\boldsymbol{z}\in\mathbb{R}^d$，其中$d$是向量维度。输出层的标准形式为：

\begin{equation}
\text{logit}_k = \langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k,\quad k=1,2,\cdots,n
\tag{1}
\end{equation}

**注释**：这里$\boldsymbol{w}_k\in\mathbb{R}^d$是类别$k$对应的权重向量，$b_k$是偏置项，$\langle\cdot,\cdot\rangle$表示内积。

预测概率通过Softmax计算：

\begin{equation}
p(k|\boldsymbol{z}) = \frac{\exp(\text{logit}_k)}{\sum_{j=1}^n \exp(\text{logit}_j)} = \frac{\exp(\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k)}{\sum_{j=1}^n \exp(\langle\boldsymbol{w}_j,\boldsymbol{z}\rangle + b_j)}
\tag{2}
\end{equation}

#### 1.2 可预测性的严格定义

**定义1（可预测类别）**：类别$k$称为可预测的（Argmaxable），当且仅当存在$\boldsymbol{z}\in\mathbb{R}^d$使得：

\begin{equation}
k = \arg\max_{i=1,\cdots,n} \text{logit}_i
\tag{3}
\end{equation}

**数学直觉**：这意味着存在某个输入能使类别$k$成为概率最大的预测结果。

等价地，类别$k$可预测当且仅当存在$\boldsymbol{z}\in\mathbb{R}^d$满足：

\begin{equation}
\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k > \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i,\quad \forall i \neq k
\tag{4}
\end{equation}

**定义2（不可预测类别）**：类别$k$称为不可预测的（Unargmaxable），当且仅当对任意$\boldsymbol{z}\in\mathbb{R}^d$，都存在某个$i\neq k$使得：

\begin{equation}
\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle + b_k \leq \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle + b_i
\tag{5}
\end{equation}

#### 1.3 几何解释

将不等式(4)改写为：

\begin{equation}
\langle\boldsymbol{w}_k - \boldsymbol{w}_i,\boldsymbol{z}\rangle > b_i - b_k,\quad \forall i \neq k
\tag{6}
\end{equation}

**几何意义**：在$d$维空间中，每个不等式定义一个半空间。类别$k$可预测等价于这$n-1$个半空间的交集非空。

记$\Delta\boldsymbol{w}_i = \boldsymbol{w}_k - \boldsymbol{w}_i$，$\Delta b_i = b_k - b_i$，则条件变为：

\begin{equation}
\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle > -\Delta b_i,\quad \forall i \neq k
\tag{7}
\end{equation}

### 二、不可预测类别的存在性证明

#### 2.1 无偏置情形的构造性证明

**定理1**：当类别数$n > d$且无偏置项时，存在配置使得某个类别不可预测。

**证明**：不失一般性，设类别$n$为待检验的类别。类别$n$不可预测等价于：

\begin{equation}
\forall \boldsymbol{z}\in\mathbb{R}^d,\, \exists i\in\{1,\cdots,n-1\}:\, \langle\boldsymbol{w}_i - \boldsymbol{w}_n,\boldsymbol{z}\rangle \geq 0
\tag{8}
\end{equation}

定义差向量集合：

\begin{equation}
\mathcal{V} = \{\boldsymbol{v}_i = \boldsymbol{w}_i - \boldsymbol{w}_n\,|\, i=1,\cdots,n-1\}
\tag{9}
\end{equation}

条件(8)等价于：对任意单位向量$\hat{\boldsymbol{z}}$，至少存在一个$\boldsymbol{v}_i$与$\hat{\boldsymbol{z}}$的夹角$\theta_i$满足$\theta_i \leq \pi/2$。

**关键观察**：在$d$维空间中，如果$n-1$个向量$\{\boldsymbol{v}_1,\cdots,\boldsymbol{v}_{n-1}\}$"覆盖"了所有方向，则条件成立。

#### 2.2 二维情形的具体构造

对于$d=2$的情况，构造如下配置：

选择$\boldsymbol{w}_5 = (1, 1)$作为参考点，然后定义：

\begin{equation}
\begin{aligned}
\boldsymbol{v}_1 &= (0, 1) \quad &(\text{指向正}\,y\,\text{方向}) \\
\boldsymbol{v}_2 &= (1, 0) \quad &(\text{指向正}\,x\,\text{方向}) \\
\boldsymbol{v}_3 &= (0, -1) \quad &(\text{指向负}\,y\,\text{方向}) \\
\boldsymbol{v}_4 &= (-1, 0) \quad &(\text{指向负}\,x\,\text{方向})
\end{aligned}
\tag{10}
\end{equation}

则其他类别的权重向量为：

\begin{equation}
\boldsymbol{w}_i = \boldsymbol{w}_5 + \boldsymbol{v}_i,\quad i=1,2,3,4
\tag{11}
\end{equation}

即：

\begin{equation}
\begin{aligned}
\boldsymbol{w}_1 &= (1, 2) \\
\boldsymbol{w}_2 &= (2, 1) \\
\boldsymbol{w}_3 &= (1, 0) \\
\boldsymbol{w}_4 &= (0, 1)
\end{aligned}
\tag{12}
\end{equation}

**验证**：对于任意向量$\boldsymbol{z} = (z_x, z_y)$，考察四个内积：

\begin{equation}
\begin{aligned}
\langle\boldsymbol{v}_1,\boldsymbol{z}\rangle &= z_y \\
\langle\boldsymbol{v}_2,\boldsymbol{z}\rangle &= z_x \\
\langle\boldsymbol{v}_3,\boldsymbol{z}\rangle &= -z_y \\
\langle\boldsymbol{v}_4,\boldsymbol{z}\rangle &= -z_x
\end{aligned}
\tag{13}
\end{equation}

分四种情况：
- 若$z_y \geq 0$且$z_y \geq |z_x|$，则$\langle\boldsymbol{v}_1,\boldsymbol{z}\rangle \geq 0$
- 若$z_x \geq 0$且$z_x \geq |z_y|$，则$\langle\boldsymbol{v}_2,\boldsymbol{z}\rangle \geq 0$
- 若$z_y \leq 0$且$|z_y| \geq |z_x|$，则$\langle\boldsymbol{v}_3,\boldsymbol{z}\rangle \geq 0$
- 若$z_x \leq 0$且$|z_x| \geq |z_y|$，则$\langle\boldsymbol{v}_4,\boldsymbol{z}\rangle \geq 0$

这四种情况覆盖了所有可能，因此类别5确实不可预测。$\square$

#### 2.3 高维推广

**引理1**：在$d$维空间中，至多需要$2d$个向量即可覆盖所有方向。

**证明**：选择$2d$个标准基向量及其负方向：

\begin{equation}
\{\boldsymbol{e}_1, -\boldsymbol{e}_1, \boldsymbol{e}_2, -\boldsymbol{e}_2, \cdots, \boldsymbol{e}_d, -\boldsymbol{e}_d\}
\tag{14}
\end{equation}

其中$\boldsymbol{e}_i$是第$i$个坐标为1、其余为0的单位向量。

对任意$\boldsymbol{z} = (z_1, z_2, \cdots, z_d)$，设$|z_k| = \max_i |z_i|$。则：
- 若$z_k \geq 0$，则$\langle\boldsymbol{e}_k,\boldsymbol{z}\rangle = z_k \geq 0$
- 若$z_k < 0$，则$\langle-\boldsymbol{e}_k,\boldsymbol{z}\rangle = -z_k > 0$

因此总存在某个向量与$\boldsymbol{z}$内积非负。$\square$

### 三、判别算法的凸优化理论

#### 3.1 问题转化为Minimax优化

给定$\{\boldsymbol{w}_1,\cdots,\boldsymbol{w}_n\}$和$\{b_1,\cdots,b_n\}$，判断类别$k$是否可预测。

不失一般性设$k=n$，定义：

\begin{equation}
\Delta\boldsymbol{w}_i = \boldsymbol{w}_n - \boldsymbol{w}_i,\quad \Delta b_i = b_n - b_i,\quad i=1,\cdots,n-1
\tag{15}
\end{equation}

类别$n$可预测等价于下述优化问题有正解：

\begin{equation}
\max_{\boldsymbol{z}\in\mathbb{R}^d} \min_{i=1,\cdots,n-1} \left(\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i\right)
\tag{16}
\end{equation}

**数学直觉**：我们寻找一个$\boldsymbol{z}$，使得所有不等式的最小"松弛量"尽可能大。

#### 3.2 添加有界约束

为避免发散，添加球约束：

\begin{equation}
\max_{\|\boldsymbol{z}\| \leq r} \min_{i=1,\cdots,n-1} \left(\langle\Delta\boldsymbol{w}_i,\boldsymbol{z}\rangle + \Delta b_i\right)
\tag{17}
\end{equation}

其中$r > 0$是足够大的常数。

**性质**：当$r \to \infty$时，若最优值趋于正数，则类别可预测；若趋于非正数，则不可预测。

#### 3.3 引入概率单纯形

定义$(n-1)$维概率单纯形：

\begin{equation}
\mathbb{P}^{n-1} = \left\{(\alpha_1,\cdots,\alpha_{n-1})\,\Big|\,\alpha_i \geq 0,\, \sum_{i=1}^{n-1}\alpha_i = 1\right\}
\tag{18}
\end{equation}

**关键观察**：最小值等价于对所有凸组合的下界：

\begin{equation}
\min_{i=1,\cdots,n-1} f_i = \min_{\alpha\in\mathbb{P}^{n-1}} \sum_{i=1}^{n-1} \alpha_i f_i
\tag{19}
\end{equation}

**证明**：设最小值在$j$处达到，即$f_j = \min_i f_i$。取$\alpha_j = 1$，其余$\alpha_i = 0$，则凸组合等于$f_j$。

反之，对任意$\alpha\in\mathbb{P}^{n-1}$：

\begin{equation}
\sum_{i=1}^{n-1} \alpha_i f_i \geq \sum_{i=1}^{n-1} \alpha_i \min_j f_j = \min_j f_j
\tag{20}
\end{equation}

因此两者相等。$\square$

#### 3.4 Minimax定理的应用

应用式(19)，问题(17)变为：

\begin{equation}
\max_{\|\boldsymbol{z}\| \leq r} \min_{\alpha\in\mathbb{P}^{n-1}} \left\langle\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i,\boldsymbol{z}\right\rangle + \sum_{i=1}^{n-1}\alpha_i\Delta b_i
\tag{21}
\end{equation}

**冯·诺依曼Minimax定理**：对于凸-凹函数$f(\boldsymbol{x},\boldsymbol{y})$，若定义域紧致，则：

\begin{equation}
\max_{\boldsymbol{x}} \min_{\boldsymbol{y}} f(\boldsymbol{x},\boldsymbol{y}) = \min_{\boldsymbol{y}} \max_{\boldsymbol{x}} f(\boldsymbol{x},\boldsymbol{y})
\tag{22}
\end{equation}

**验证凸凹性**：
- 函数关于$\boldsymbol{z}$是线性的（凸且凹）
- 函数关于$\alpha$是线性的（凸且凹）
- $\|\boldsymbol{z}\| \leq r$是凸集
- $\mathbb{P}^{n-1}$是凸集

因此可交换max和min：

\begin{equation}
\min_{\alpha\in\mathbb{P}^{n-1}} \max_{\|\boldsymbol{z}\| \leq r} \left\langle\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i,\boldsymbol{z}\right\rangle + \sum_{i=1}^{n-1}\alpha_i\Delta b_i
\tag{23}
\end{equation}

#### 3.5 内层最大化问题的解析解

固定$\alpha$，内层问题为：

\begin{equation}
\max_{\|\boldsymbol{z}\| \leq r} \langle\boldsymbol{g}_\alpha,\boldsymbol{z}\rangle
\tag{24}
\end{equation}

其中$\boldsymbol{g}_\alpha = \sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i$。

**解析解**：由Cauchy-Schwarz不等式：

\begin{equation}
\langle\boldsymbol{g}_\alpha,\boldsymbol{z}\rangle \leq \|\boldsymbol{g}_\alpha\| \cdot \|\boldsymbol{z}\| \leq r\|\boldsymbol{g}_\alpha\|
\tag{25}
\end{equation}

等号成立当且仅当$\boldsymbol{z} = r\frac{\boldsymbol{g}_\alpha}{\|\boldsymbol{g}_\alpha\|}$（假设$\boldsymbol{g}_\alpha \neq \boldsymbol{0}$）。

因此最优值为：

\begin{equation}
r\|\boldsymbol{g}_\alpha\| + \sum_{i=1}^{n-1}\alpha_i\Delta b_i
\tag{26}
\end{equation}

#### 3.6 外层最小化问题

问题简化为：

\begin{equation}
\min_{\alpha\in\mathbb{P}^{n-1}} \left(r\left\|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i\right\| + \sum_{i=1}^{n-1}\alpha_i\Delta b_i\right)
\tag{27}
\end{equation}

**渐近分析**：当$r \to \infty$时，第一项占主导，问题渐近于：

\begin{equation}
\min_{\alpha\in\mathbb{P}^{n-1}} \left\|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i\right\|
\tag{28}
\end{equation}

**物理意义**：寻找向量$\{\Delta\boldsymbol{w}_i\}$的最短凸组合。

### 四、Frank-Wolfe算法详解

#### 4.1 算法原理

Frank-Wolfe算法（又称条件梯度法）求解约束优化问题：

\begin{equation}
\min_{\boldsymbol{x}\in\mathcal{C}} f(\boldsymbol{x})
\tag{29}
\end{equation}

其中$f$是凸函数，$\mathcal{C}$是凸集。

**算法思想**：在当前点$\boldsymbol{x}_t$处，用线性近似代替原函数：

\begin{equation}
f(\boldsymbol{x}) \approx f(\boldsymbol{x}_t) + \langle\nabla f(\boldsymbol{x}_t), \boldsymbol{x} - \boldsymbol{x}_t\rangle
\tag{30}
\end{equation}

最小化线性部分等价于：

\begin{equation}
\boldsymbol{s}_t = \arg\min_{\boldsymbol{x}\in\mathcal{C}} \langle\nabla f(\boldsymbol{x}_t), \boldsymbol{x}\rangle
\tag{31}
\end{equation}

然后沿方向$\boldsymbol{s}_t - \boldsymbol{x}_t$进行线搜索：

\begin{equation}
\boldsymbol{x}_{t+1} = (1-\gamma_t)\boldsymbol{x}_t + \gamma_t\boldsymbol{s}_t
\tag{32}
\end{equation}

其中步长$\gamma_t \in [0,1]$。

#### 4.2 应用于问题(28)

对于$f(\alpha) = \left\|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i\right\|$，梯度为：

\begin{equation}
\frac{\partial f}{\partial \alpha_j} = \frac{\Delta\boldsymbol{w}_j \cdot \sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i}{\left\|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i\right\|}
\tag{33}
\end{equation}

记$\boldsymbol{g}_t = \sum_{i=1}^{n-1}\alpha_i^{(t)}\Delta\boldsymbol{w}_i$，则梯度向量为：

\begin{equation}
\nabla f(\alpha^{(t)}) = \frac{1}{\|\boldsymbol{g}_t\|}(\Delta\boldsymbol{w}_1 \cdot \boldsymbol{g}_t, \cdots, \Delta\boldsymbol{w}_{n-1} \cdot \boldsymbol{g}_t)
\tag{34}
\end{equation}

**线性子问题**：

\begin{equation}
\boldsymbol{s}_t = \arg\min_{\alpha\in\mathbb{P}^{n-1}} \sum_{j=1}^{n-1} \alpha_j (\Delta\boldsymbol{w}_j \cdot \boldsymbol{g}_t)
\tag{35}
\end{equation}

**解析解**：选择使内积最小的索引：

\begin{equation}
j^* = \arg\min_{j=1,\cdots,n-1} (\Delta\boldsymbol{w}_j \cdot \boldsymbol{g}_t)
\tag{36}
\end{equation}

令$\alpha_{j^*} = 1$，其余为0。

#### 4.3 步长选择

线搜索问题为：

\begin{equation}
\gamma_t = \arg\min_{\gamma\in[0,1]} \|(1-\gamma)\boldsymbol{g}_t + \gamma\Delta\boldsymbol{w}_{j^*}\|
\tag{37}
\end{equation}

展开平方：

\begin{equation}
\begin{aligned}
&\|(1-\gamma)\boldsymbol{g}_t + \gamma\Delta\boldsymbol{w}_{j^*}\|^2 \\
=&\, (1-\gamma)^2\|\boldsymbol{g}_t\|^2 + 2\gamma(1-\gamma)(\boldsymbol{g}_t \cdot \Delta\boldsymbol{w}_{j^*}) + \gamma^2\|\Delta\boldsymbol{w}_{j^*}\|^2
\end{aligned}
\tag{38}
\end{equation}

对$\gamma$求导并令其为0：

\begin{equation}
-2(1-\gamma)\|\boldsymbol{g}_t\|^2 + 2(1-2\gamma)(\boldsymbol{g}_t \cdot \Delta\boldsymbol{w}_{j^*}) + 2\gamma\|\Delta\boldsymbol{w}_{j^*}\|^2 = 0
\tag{39}
\end{equation}

解得：

\begin{equation}
\gamma_t = \frac{\|\boldsymbol{g}_t\|^2 - \boldsymbol{g}_t \cdot \Delta\boldsymbol{w}_{j^*}}{\|\boldsymbol{g}_t\|^2 - 2(\boldsymbol{g}_t \cdot \Delta\boldsymbol{w}_{j^*}) + \|\Delta\boldsymbol{w}_{j^*}\|^2} = \frac{\|\boldsymbol{g}_t\|^2 - \boldsymbol{g}_t \cdot \Delta\boldsymbol{w}_{j^*}}{\|\boldsymbol{g}_t - \Delta\boldsymbol{w}_{j^*}\|^2}
\tag{40}
\end{equation}

若计算值超出$[0,1]$，则截断到边界。

#### 4.4 算法伪代码

```
输入：向量集合{Δw_1, ..., Δw_{n-1}}
输出：最小范数凸组合及其范数

1. 初始化：α^(0) = (1/(n-1), ..., 1/(n-1))
2. For t = 0, 1, 2, ..., T:
   a. 计算：g_t = Σ_{i=1}^{n-1} α_i^(t) Δw_i
   b. 计算梯度方向：找到j* = argmin_j (Δw_j · g_t)
   c. 计算步长：γ_t 根据式(40)
   d. 更新：α^(t+1) = (1-γ_t)α^(t) + γ_t e_{j*}
      其中e_{j*}是第j*个标准基向量
   e. 若||g_{t+1} - g_t|| < ε，停止
3. 返回：g_T 和 ||g_T||
```

#### 4.5 收敛性分析

**定理2**：Frank-Wolfe算法对光滑凸函数有收敛率$O(1/T)$。

**证明思路**：
1. 设$f^* = \min_{\alpha\in\mathbb{P}^{n-1}} f(\alpha)$
2. 利用凸性：$f(\alpha^{(t+1)}) - f^* \leq \langle\nabla f(\alpha^{(t)}), \alpha^{(t+1)} - \alpha^*\rangle$
3. 结合步长选择和光滑性条件，得到递推不等式
4. 累加得到$O(1/T)$收敛率

对于我们的问题，$f(\alpha) = \|\sum_i \alpha_i \Delta\boldsymbol{w}_i\|$虽然在某些点不可微（当和向量为零时），但在实践中几乎总是可微的。

### 五、实践中的考虑

#### 5.1 数值稳定性

**问题**：当$\|\boldsymbol{g}_t\| \approx 0$时，梯度计算式(34)可能数值不稳定。

**解决方案**：添加小的正则化项：

\begin{equation}
\nabla f(\alpha) \approx \frac{1}{\|\boldsymbol{g}\| + \epsilon}(\Delta\boldsymbol{w}_1 \cdot \boldsymbol{g}, \cdots, \Delta\boldsymbol{w}_{n-1} \cdot \boldsymbol{g})
\tag{41}
\end{equation}

其中$\epsilon = 10^{-8}$是小常数。

#### 5.2 维度诅咒的影响

**观察**：文中提到"远远大于"的相对性。在$d$维空间中，随机向量间的夹角分布集中在$\pi/2$附近。

**定量分析**：两个随机单位向量的内积$X = \boldsymbol{u} \cdot \boldsymbol{v}$近似服从：

\begin{equation}
X \sim \mathcal{N}\left(0, \frac{1}{d}\right)
\tag{42}
\end{equation}

因此内积的标准差为$1/\sqrt{d}$，当$d$很大时迅速趋于0。

**推论**：在高维空间中，向量趋于正交，因此需要指数级多的向量才能"覆盖"所有方向。

例如，要使任意方向与至少一个向量夹角小于$\theta$，需要的向量数量大约为：

\begin{equation}
N \sim \left(\frac{\pi}{\theta}\right)^{d-1}
\tag{43}
\end{equation}

这解释了为什么$n \gg d$在高维时的含义与低维不同。

#### 5.3 模长一致性的保护作用

**定理3**：若所有$\boldsymbol{w}_i$满足$\|\boldsymbol{w}_i\| = c$（常数），且互不相同，则所有类别都可预测。

**证明**：对于类别$k$，选择$\boldsymbol{z} = \boldsymbol{w}_k$。则：

\begin{equation}
\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle = \|\boldsymbol{w}_k\|^2 = c^2
\tag{44}
\end{equation}

\begin{equation}
\langle\boldsymbol{w}_i,\boldsymbol{z}\rangle = \boldsymbol{w}_i \cdot \boldsymbol{w}_k < \|\boldsymbol{w}_i\| \cdot \|\boldsymbol{w}_k\| = c^2
\tag{45}
\end{equation}

最后一个不等式因为$\boldsymbol{w}_i \neq \boldsymbol{w}_k$严格成立（Cauchy-Schwarz不等式等号成立当且仅当向量共线）。

因此$\langle\boldsymbol{w}_k,\boldsymbol{z}\rangle > \langle\boldsymbol{w}_i,\boldsymbol{z}\rangle$对所有$i \neq k$成立，类别$k$可预测。$\square$

**实践意义**：Layer Normalization等技术通过约束向量模长，降低了不可预测类别出现的概率。

#### 5.4 Beam Search与随机采样

Greedy解码选择：

\begin{equation}
\hat{k} = \arg\max_{k} p(k|\boldsymbol{z})
\tag{46}
\end{equation}

即使某类别$k_0$不可预测（即$k_0 \neq \hat{k}$对所有$\boldsymbol{z}$成立），在Beam Search中仍可能出现：

**Beam Search**保留top-$K$候选：

\begin{equation}
\mathcal{B} = \{k_1, k_2, \cdots, k_K\}\,\text{ where }\, p(k_1|\boldsymbol{z}) \geq p(k_2|\boldsymbol{z}) \geq \cdots \geq p(k_K|\boldsymbol{z})
\tag{47}
\end{equation}

只要$k_0 \in \mathcal{B}$，它就有机会被选择。

**随机采样**按概率分布采样：

\begin{equation}
k \sim p(\cdot|\boldsymbol{z})
\tag{48}
\end{equation}

即使$p(k_0|\boldsymbol{z})$很小但非零，$k_0$仍有非零概率被采样。

### 六、计算示例

#### 6.1 二维实例的数值验证

使用式(12)的构造，验证类别5不可预测。

测试向量$\boldsymbol{z} = (1, 0.5)$：

\begin{equation}
\begin{aligned}
\langle\boldsymbol{w}_1 - \boldsymbol{w}_5, \boldsymbol{z}\rangle &= \langle(0,1), (1,0.5)\rangle = 0.5 > 0 \\
\langle\boldsymbol{w}_2 - \boldsymbol{w}_5, \boldsymbol{z}\rangle &= \langle(1,0), (1,0.5)\rangle = 1.0 > 0 \\
\langle\boldsymbol{w}_3 - \boldsymbol{w}_5, \boldsymbol{z}\rangle &= \langle(0,-1), (1,0.5)\rangle = -0.5 < 0 \\
\langle\boldsymbol{w}_4 - \boldsymbol{w}_5, \boldsymbol{z}\rangle &= \langle(-1,0), (1,0.5)\rangle = -1.0 < 0
\end{aligned}
\tag{49}
\end{equation}

最大值为1.0，对应类别2，验证了类别5不是最优。

测试其他方向可以类似验证。

#### 6.2 Frank-Wolfe迭代示例

假设$n-1=3$，$\Delta\boldsymbol{w}_1 = (1,0)$，$\Delta\boldsymbol{w}_2 = (0,1)$，$\Delta\boldsymbol{w}_3 = (-0.5,-0.5)$。

**迭代0**：
- $\alpha^{(0)} = (1/3, 1/3, 1/3)$
- $\boldsymbol{g}_0 = (1/3)(1,0) + (1/3)(0,1) + (1/3)(-0.5,-0.5) = (1/6, 1/6)$
- $\|\boldsymbol{g}_0\| \approx 0.236$

内积：
- $\Delta\boldsymbol{w}_1 \cdot \boldsymbol{g}_0 = 1/6 \approx 0.167$
- $\Delta\boldsymbol{w}_2 \cdot \boldsymbol{g}_0 = 1/6 \approx 0.167$
- $\Delta\boldsymbol{w}_3 \cdot \boldsymbol{g}_0 = -1/6 \approx -0.167$（最小）

选择$j^* = 3$，更新$\alpha$朝向$(0,0,1)$方向...

（迭代多步后收敛到最优解）

### 七、理论扩展与开放问题

#### 7.1 有偏置情形

当包含偏置项时，问题变为：

\begin{equation}
\min_{\alpha\in\mathbb{P}^{n-1}} \left(r\left\|\sum_{i=1}^{n-1}\alpha_i\Delta\boldsymbol{w}_i\right\| + \sum_{i=1}^{n-1}\alpha_i\Delta b_i\right)
\tag{50}
\end{equation}

**挑战**：偏置项打破了纯几何结构，需要权衡两项的影响。

**启发式方法**：通过增广向量将偏置融入：

\begin{equation}
\tilde{\boldsymbol{w}}_i = (\boldsymbol{w}_i, b_i) \in \mathbb{R}^{d+1}
\tag{51}
\end{equation}

在$(d+1)$维空间中应用无偏置算法。

#### 7.2 近似算法的误差界

**问题**：Frank-Wolfe算法给出的解有多接近真实最优解？

**误差界**：经过$T$次迭代后：

\begin{equation}
f(\alpha^{(T)}) - f^* \leq \frac{2LC}T}
\tag{52}
\end{equation}

其中$L$是Lipschitz常数，$C$是初始对偶间隙。

对于$f(\alpha) = \|\sum_i \alpha_i \Delta\boldsymbol{w}_i\|$：
- Lipschitz常数：$L = \max_i \|\Delta\boldsymbol{w}_i\|$
- 初始间隙：$C = f(\alpha^{(0)}) - \langle\nabla f(\alpha^{(0)}), \alpha^{(0)} - \alpha^*\rangle$

#### 7.3 概率性结果

**定理4**（启发式）：在随机模型下，若$\boldsymbol{w}_i \sim \mathcal{N}(\boldsymbol{0}, \frac{1}{d}I_d)$独立采样，则不可预测类别出现的概率随$n/e^d$指数衰减。

**直觉**：需要覆盖$S^{d-1}$球面，单位球面"面积"增长为$O(1)$，而随机向量的覆盖半径为$O(1/\sqrt{d})$，需要的向量数为$O(e^d)$。

### 八、实践建议

#### 8.1 监控指标

在训练语言模型时，可以定期检查：

1. **权重矩阵的条件数**：
   \begin{equation}
   \kappa(\boldsymbol{W}) = \frac{\sigma_{\max}(\boldsymbol{W})}{\sigma_{\min}(\boldsymbol{W})}
   \tag{53}
   \end{equation}
   条件数过大可能导致不可预测类别。

2. **最小凸组合范数**：
   对每个类别$k$，计算$\min_\alpha \|\sum_{i \neq k} \alpha_i (\boldsymbol{w}_i - \boldsymbol{w}_k)\|$。若某个类别的值接近0，则可能不可预测。

#### 8.2 正则化策略

**谱归一化**：约束权重矩阵的最大奇异值：

\begin{equation}
\boldsymbol{W}_{\text{SN}} = \frac{\boldsymbol{W}}{\sigma_{\max}(\boldsymbol{W})}
\tag{54}
\end{equation}

**权重衰减与模长平衡**：添加损失项：

\begin{equation}
\mathcal{L}_{\text{reg}} = \sum_{i=1}^n (\|\boldsymbol{w}_i\| - \bar{c})^2
\tag{55}
\end{equation}

其中$\bar{c}$是目标模长。

#### 8.3 架构选择

**Layer Normalization**自动平衡特征尺度：

\begin{equation}
\text{LN}(\boldsymbol{z}) = \frac{\boldsymbol{z} - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta
\tag{56}
\end{equation}

确保输出向量的范数在合理范围内。

**总结**：通过理论分析与数值方法的结合，我们能够有效判断和预防语言模型中不可预测类别的出现，这对于理解模型行为和改进训练策略都具有重要意义。

