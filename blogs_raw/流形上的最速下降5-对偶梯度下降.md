---
title: 流形上的最速下降：5. 对偶梯度下降
slug: 流形上的最速下降5-对偶梯度下降
date: 2025-11-03
source: https://spaces.ac.cn/archives/11388
tags: 详细推导, 机器学习
status: completed
---
# 流形上的最速下降：5. 对偶梯度下降

**原文链接**: [https://spaces.ac.cn/archives/11388](https://spaces.ac.cn/archives/11388)

**发布日期**: 2025-11-03

---

前四篇文章我们求解了几个具体的给参数加等式约束的最速下降问题，其中第三、四篇的问题没法找到解析解，所以笔者提出了相应的不动点迭代法。其中的其中，第三篇文章[《流形上的最速下降：3. Muon + Stiefel》](https://kexue.fm/archives/11221)所研究的“Stiefel流形上的Muon”，问题提出自Jeremy Bernstein的[《Orthogonal manifold》](https://docs.modula.systems/algorithms/manifold/orthogonal/)一文。

对于这个问题，Jeremy Bernstein最后也给出了一个自己的解法，笔者称之为“对偶梯度下降（Dual Gradient Descent）”，也颇为值得学习一番。

## 基本概念

Jeremy Bernstein的解法，最后发表在Thinking Machines Lab的博客[《Modular Manifolds》](https://thinkingmachines.ai/blog/modular-manifolds/)中，是该实验室的第二篇博客，文章中将它称为“对偶上升（Dual Ascent）”，但笔者这里还是结合前四篇的内容，将其称为“对偶梯度下降”。

本文介绍Jeremy Bernstein提出的对偶梯度下降方法，这是解决Stiefel流形上最速下降问题的另一种优雅途径。与前文的迭代Lyapunov方程方法不同，对偶梯度下降从优化的对偶问题出发，通过Lagrange对偶理论和凸优化技术求解。

## 对偶视角

对偶梯度下降的核心思想是将原始约束优化问题转化为对偶问题，在对偶空间中求解，然后映射回原始空间。这种方法在凸优化中有深厚的理论基础，Jeremy Bernstein巧妙地将其应用到流形优化中。

## 算法框架

对偶梯度下降方法包含以下关键步骤：
1. 构造Lagrangian函数
2. 求解对偶问题
3. 从对偶解恢复原始解
4. 迭代更新直到收敛

这种方法的优势在于将复杂的流形约束转化为更易处理的对偶变量优化，在某些情况下可以获得更快的收敛速度和更好的数值稳定性。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/11388>_


---

## 公式推导与注释

### 1. 对偶空间的数学定义

**定义1.1（对偶空间）**：给定向量空间$V$，其对偶空间$V^*$定义为所有从$V$到$\mathbb{R}$的线性泛函的集合：

$$
V^* = \{f : V \to \mathbb{R} \mid f \text{ is linear}\}
$$

对于$\mathbb{R}^n$，对偶空间同构于$\mathbb{R}^n$本身，通过内积$\langle\cdot,\cdot\rangle$建立对应关系。

**注释**：在矩阵空间$\mathbb{R}^{n\times m}$中，对偶空间同样是$\mathbb{R}^{n\times m}$，配对通过Frobenius内积：

$$
\langle\boldsymbol{A},\boldsymbol{B}\rangle_F = \text{tr}(\boldsymbol{A}^{\top}\boldsymbol{B})
$$

**推导1.1（Riesz表示定理）**：对任意线性泛函$f\in V^*$，存在唯一的$\boldsymbol{v}\in V$使得：

$$
f(\boldsymbol{u}) = \langle\boldsymbol{v},\boldsymbol{u}\rangle \quad \forall\boldsymbol{u}\in V
$$

这建立了$V$与$V^*$的同构。

### 2. Lagrange对偶理论

**定义2.1（原始问题）**：考虑带约束的优化问题：

$$
\begin{aligned}
\min_{\boldsymbol{x}} \quad & f(\boldsymbol{x})\\
\text{s.t.} \quad & g_i(\boldsymbol{x}) \leq 0, \quad i=1,\ldots,m\\
& h_j(\boldsymbol{x}) = 0, \quad j=1,\ldots,p
\end{aligned}
$$

**定义2.2（Lagrangian函数）**：引入Lagrange乘子$\boldsymbol{\lambda}\in\mathbb{R}^m$和$\boldsymbol{\nu}\in\mathbb{R}^p$，Lagrangian函数为：

$$
\mathcal{L}(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{\nu}) = f(\boldsymbol{x}) + \sum_{i=1}^m\lambda_i g_i(\boldsymbol{x}) + \sum_{j=1}^p\nu_j h_j(\boldsymbol{x})
$$

**定义2.3（Lagrange对偶函数）**：对偶函数定义为Lagrangian关于原始变量的下确界：

$$
d(\boldsymbol{\lambda},\boldsymbol{\nu}) = \inf_{\boldsymbol{x}}\mathcal{L}(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{\nu})
$$

**定理2.1（弱对偶性）**：对任意$\boldsymbol{\lambda}\geq\boldsymbol{0}$和$\boldsymbol{\nu}$，有：

$$
d(\boldsymbol{\lambda},\boldsymbol{\nu}) \leq p^*
$$

其中$p^*$是原始问题的最优值。

**推导2.1（弱对偶性的证明）**：设$\tilde{\boldsymbol{x}}$是原始问题的任意可行解，则$g_i(\tilde{\boldsymbol{x}})\leq 0$和$h_j(\tilde{\boldsymbol{x}})=0$。因此：

$$
\begin{aligned}
d(\boldsymbol{\lambda},\boldsymbol{\nu}) &= \inf_{\boldsymbol{x}}\mathcal{L}(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{\nu})\\
&\leq \mathcal{L}(\tilde{\boldsymbol{x}},\boldsymbol{\lambda},\boldsymbol{\nu})\\
&= f(\tilde{\boldsymbol{x}}) + \sum_i\lambda_i g_i(\tilde{\boldsymbol{x}}) + \sum_j\nu_j h_j(\tilde{\boldsymbol{x}})\\
&\leq f(\tilde{\boldsymbol{x}})
\end{aligned}
$$

对所有可行$\tilde{\boldsymbol{x}}$取下确界，得$d(\boldsymbol{\lambda},\boldsymbol{\nu})\leq p^*$。

### 3. 强对偶与Slater条件

**定义3.1（对偶问题）**：对偶问题定义为：

$$
\begin{aligned}
\max_{\boldsymbol{\lambda},\boldsymbol{\nu}} \quad & d(\boldsymbol{\lambda},\boldsymbol{\nu})\\
\text{s.t.} \quad & \boldsymbol{\lambda} \geq \boldsymbol{0}
\end{aligned}
$$

**定义3.2（对偶间隙）**：原始最优值$p^*$与对偶最优值$d^*$之差：

$$
\text{gap} = p^* - d^*
$$

**定理3.1（强对偶性）**：若Slater条件成立（存在严格可行点），且问题是凸的，则对偶间隙为零：$p^*=d^*$。

**推导3.1（Slater条件）**：对于凸优化问题，若存在$\boldsymbol{x}_0$满足：

$$
g_i(\boldsymbol{x}_0) < 0, \quad \forall i, \quad h_j(\boldsymbol{x}_0) = 0, \quad \forall j
$$

则强对偶性成立。这是因为Slater条件保证了约束规范（constraint qualification）。

### 4. KKT条件

**定理4.1（KKT必要条件）**：设$\boldsymbol{x}^*$和$(\boldsymbol{\lambda}^*,\boldsymbol{\nu}^*)$分别是原始和对偶问题的最优解，且强对偶性成立，则：

1. **原始可行性**：$g_i(\boldsymbol{x}^*)\leq 0$，$h_j(\boldsymbol{x}^*)=0$
2. **对偶可行性**：$\boldsymbol{\lambda}^*\geq\boldsymbol{0}$
3. **互补松弛**：$\lambda_i^* g_i(\boldsymbol{x}^*)=0, \forall i$
4. **稳定性**：$\nabla_{\boldsymbol{x}}\mathcal{L}(\boldsymbol{x}^*,\boldsymbol{\lambda}^*,\boldsymbol{\nu}^*)=\boldsymbol{0}$

**推导4.1（互补松弛的证明）**：由于强对偶性：

$$
f(\boldsymbol{x}^*) = d(\boldsymbol{\lambda}^*,\boldsymbol{\nu}^*) = \inf_{\boldsymbol{x}}\mathcal{L}(\boldsymbol{x},\boldsymbol{\lambda}^*,\boldsymbol{\nu}^*) \leq \mathcal{L}(\boldsymbol{x}^*,\boldsymbol{\lambda}^*,\boldsymbol{\nu}^*)
$$

但又有：

$$
\mathcal{L}(\boldsymbol{x}^*,\boldsymbol{\lambda}^*,\boldsymbol{\nu}^*) = f(\boldsymbol{x}^*) + \sum_i\lambda_i^* g_i(\boldsymbol{x}^*) + \sum_j\nu_j^* h_j(\boldsymbol{x}^*) = f(\boldsymbol{x}^*) + \sum_i\lambda_i^* g_i(\boldsymbol{x}^*)
$$

由于$\lambda_i^*\geq 0$和$g_i(\boldsymbol{x}^*)\leq 0$，要使上式等于$f(\boldsymbol{x}^*)$，必须$\lambda_i^* g_i(\boldsymbol{x}^*)=0$。

### 5. Fenchel对偶

**定义5.1（Fenchel共轭）**：函数$f:\mathbb{R}^n\to\mathbb{R}$的Fenchel共轭$f^*:\mathbb{R}^n\to\mathbb{R}$定义为：

$$
f^*(\boldsymbol{y}) = \sup_{\boldsymbol{x}}\{\langle\boldsymbol{y},\boldsymbol{x}\rangle - f(\boldsymbol{x})\}
$$

**性质5.1（Fenchel-Young不等式）**：对所有$\boldsymbol{x},\boldsymbol{y}$：

$$
f(\boldsymbol{x}) + f^*(\boldsymbol{y}) \geq \langle\boldsymbol{x},\boldsymbol{y}\rangle
$$

等号成立当且仅当$\boldsymbol{y}\in\partial f(\boldsymbol{x})$（$\boldsymbol{y}$是$f$在$\boldsymbol{x}$处的次梯度）。

**推导5.1（Fenchel共轭的性质）**：Fenchel共轭是凸的、下半连续的，即使$f$本身不是凸函数。这是因为：

$$
f^*(\boldsymbol{y}) = \sup_{\boldsymbol{x}}\{\langle\boldsymbol{y},\boldsymbol{x}\rangle - f(\boldsymbol{x})\}
$$

是仿射函数$\langle\boldsymbol{y},\boldsymbol{x}\rangle - f(\boldsymbol{x})$（关于$\boldsymbol{y}$）的上确界，而仿射函数的上确界是凸的。

**例5.1（常见函数的Fenchel共轭）**：

1. $f(\boldsymbol{x})=\frac{1}{2}\|\boldsymbol{x}\|^2 \Rightarrow f^*(\boldsymbol{y})=\frac{1}{2}\|\boldsymbol{y}\|^2$
2. $f(\boldsymbol{x})=\|\boldsymbol{x}\|_1 \Rightarrow f^*(\boldsymbol{y})=\mathbb{I}_{\|\cdot\|_{\infty}\leq 1}(\boldsymbol{y})$（指示函数）
3. $f(\boldsymbol{x})=e^x \Rightarrow f^*(\boldsymbol{y})=y\log y - y$（$y>0$）

### 6. Legendre变换

**定义6.1（Legendre变换）**：对于凸函数$f$，若$f$是严格凸且可微的，Legendre变换定义为：

$$
f^*(\boldsymbol{y}) = \langle\nabla f^{-1}(\boldsymbol{y}),\boldsymbol{y}\rangle - f(\nabla f^{-1}(\boldsymbol{y}))
$$

其中$\nabla f^{-1}$是梯度映射的逆。

**定理6.1（Legendre变换的对合性）**：若$f$是严格凸、可微且下确界有限，则$(f^*)^*=f$。

**推导6.1（Legendre变换与Fenchel共轭的关系）**：当$f$严格凸可微时，Legendre变换等价于Fenchel共轭。由一阶最优性条件：

$$
\boldsymbol{y} = \nabla f(\boldsymbol{x}^*)
$$

其中$\boldsymbol{x}^*$是$\sup_{\boldsymbol{x}}\{\langle\boldsymbol{y},\boldsymbol{x}\rangle - f(\boldsymbol{x})\}$的最优解。

**例6.1（熵函数的Legendre变换）**：设$f(\boldsymbol{p})=\sum_i p_i\log p_i$（Shannon熵的负值），则：

$$
f^*(\boldsymbol{y}) = \log\left(\sum_i e^{y_i}\right)
$$

这是softmax函数的对数。

### 7. 镜像下降算法框架

**定义7.1（Bregman散度）**：给定严格凸可微函数$\phi$，Bregman散度定义为：

$$
D_{\phi}(\boldsymbol{x},\boldsymbol{y}) = \phi(\boldsymbol{x}) - \phi(\boldsymbol{y}) - \langle\nabla\phi(\boldsymbol{y}),\boldsymbol{x}-\boldsymbol{y}\rangle
$$

**性质7.1（Bregman散度的非负性）**：由于$\phi$严格凸，$D_{\phi}(\boldsymbol{x},\boldsymbol{y})\geq 0$，等号成立当且仅当$\boldsymbol{x}=\boldsymbol{y}$。

**算法7.1（镜像下降）**：迭代更新：

1. **对偶空间更新**：
$$
\boldsymbol{y}_{t+1} = \nabla\phi(\boldsymbol{x}_t) - \eta_t\boldsymbol{g}_t
$$
其中$\boldsymbol{g}_t=\nabla f(\boldsymbol{x}_t)$是梯度。

2. **原始空间恢复**：
$$
\boldsymbol{x}_{t+1} = \nabla\phi^*(\boldsymbol{y}_{t+1})
$$

**推导7.1（镜像下降的等价形式）**：镜像下降等价于最小化：

$$
\boldsymbol{x}_{t+1} = \arg\min_{\boldsymbol{x}}\{\langle\boldsymbol{g}_t,\boldsymbol{x}\rangle + \frac{1}{\eta_t}D_{\phi}(\boldsymbol{x},\boldsymbol{x}_t)\}
$$

一阶最优性条件给出：

$$
\boldsymbol{g}_t + \frac{1}{\eta_t}(\nabla\phi(\boldsymbol{x}_{t+1})-\nabla\phi(\boldsymbol{x}_t)) = \boldsymbol{0}
$$

即$\nabla\phi(\boldsymbol{x}_{t+1}) = \nabla\phi(\boldsymbol{x}_t) - \eta_t\boldsymbol{g}_t$，这正是对偶空间更新。

### 8. 对偶梯度下降的收敛性

**定理8.1（镜像下降的收敛率）**：对于凸的Lipschitz连续函数$f$，镜像下降在$T$步后满足：

$$
f\left(\frac{1}{T}\sum_{t=1}^T\boldsymbol{x}_t\right) - f(\boldsymbol{x}^*) \leq \frac{D_{\phi}(\boldsymbol{x}^*,\boldsymbol{x}_1) + \frac{L^2}{2}\sum_{t=1}^T\eta_t^2}{\sum_{t=1}^T\eta_t}
$$

其中$L$是梯度的Lipschitz常数。

**推导8.1（关键引理）**：对每次迭代，由Bregman散度的三点恒等式：

$$
D_{\phi}(\boldsymbol{x},\boldsymbol{x}_{t+1}) = D_{\phi}(\boldsymbol{x},\boldsymbol{x}_t) - D_{\phi}(\boldsymbol{x}_{t+1},\boldsymbol{x}_t) - \langle\nabla\phi(\boldsymbol{x}_{t+1})-\nabla\phi(\boldsymbol{x}_t),\boldsymbol{x}-\boldsymbol{x}_{t+1}\rangle
$$

结合对偶更新$\nabla\phi(\boldsymbol{x}_{t+1})=\nabla\phi(\boldsymbol{x}_t)-\eta_t\boldsymbol{g}_t$：

$$
D_{\phi}(\boldsymbol{x},\boldsymbol{x}_{t+1}) \leq D_{\phi}(\boldsymbol{x},\boldsymbol{x}_t) + \eta_t\langle\boldsymbol{g}_t,\boldsymbol{x}-\boldsymbol{x}_{t+1}\rangle
$$

利用凸性$f(\boldsymbol{x})\geq f(\boldsymbol{x}_t)+\langle\boldsymbol{g}_t,\boldsymbol{x}-\boldsymbol{x}_t\rangle$，可以建立遗憾界。

### 9. Stiefel流形的对偶梯度下降

**问题9.1（Stiefel流形上的Muon）**：回顾Stiefel流形上的优化问题：

$$
\max_{\boldsymbol{\Phi}} \text{tr}(\boldsymbol{G}^{\top}\boldsymbol{\Phi}) \quad \text{s.t.} \quad \|\boldsymbol{\Phi}\|_2=1, \quad \boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W}=\boldsymbol{0}
$$

**推导9.1（Lagrangian的构造）**：引入对称矩阵Lagrange乘子$\boldsymbol{X}\in\mathbb{R}^{m\times m}$，Lagrangian为：

$$
\mathcal{L}(\boldsymbol{\Phi},\boldsymbol{X}) = \text{tr}(\boldsymbol{G}^{\top}\boldsymbol{\Phi}) + \text{tr}(\boldsymbol{X}(\boldsymbol{W}^{\top}\boldsymbol{\Phi}+\boldsymbol{\Phi}^{\top}\boldsymbol{W}))
$$

由于切空间条件是线性的，可以整理为：

$$
\mathcal{L}(\boldsymbol{\Phi},\boldsymbol{X}) = \text{tr}((\boldsymbol{G}+\boldsymbol{W}\boldsymbol{X}+\boldsymbol{W}\boldsymbol{X}^{\top})^{\top}\boldsymbol{\Phi}) = \text{tr}((\boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X})^{\top}\boldsymbol{\Phi})
$$

最后一步使用了$\boldsymbol{X}$对称。

**推导9.2（对偶函数）**：固定$\boldsymbol{X}$，关于$\boldsymbol{\Phi}$最大化（受谱范数约束）：

$$
d(\boldsymbol{X}) = \max_{\|\boldsymbol{\Phi}\|_2=1}\text{tr}((\boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X})^{\top}\boldsymbol{\Phi}) = \|\boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X}\|_*
$$

其中$\|\cdot\|_*$是核范数（奇异值之和）。

### 10. 对偶问题的求解

**定理10.1（对偶问题）**：原问题的对偶为：

$$
\min_{\boldsymbol{X}=\boldsymbol{X}^{\top}} \|\boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X}\|_*
$$

这是一个关于对称矩阵$\boldsymbol{X}$的凸优化问题。

**推导10.1（次梯度计算）**：核范数的次梯度为：

$$
\partial\|\boldsymbol{M}\|_* = \{\boldsymbol{U}\boldsymbol{V}^{\top} : \boldsymbol{M}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}, \|\boldsymbol{U}\|_2\leq 1, \|\boldsymbol{V}\|_2\leq 1\}
$$

当$\boldsymbol{M}$满秩时，次梯度简化为$\boldsymbol{U}\boldsymbol{V}^{\top}$（来自SVD）。

对于$d(\boldsymbol{X})=\|\boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X}\|_*$，次梯度为：

$$
\partial_{\boldsymbol{X}}d(\boldsymbol{X}) = 2\boldsymbol{W}^{\top}\partial\|\boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X}\|_*
$$

**算法10.1（次梯度下降）**：迭代更新：

1. 计算$\boldsymbol{Z}_t = \boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X}_t$
2. SVD分解：$\boldsymbol{Z}_t = \boldsymbol{U}_t\boldsymbol{\Sigma}_t\boldsymbol{V}_t^{\top}$
3. 次梯度：$\boldsymbol{G}_t = 2\boldsymbol{W}^{\top}\boldsymbol{U}_t\boldsymbol{V}_t^{\top}$
4. 投影到对称矩阵：$\boldsymbol{G}_t \leftarrow [\boldsymbol{G}_t]_{\text{sym}}$
5. 更新：$\boldsymbol{X}_{t+1} = \boldsymbol{X}_t - \eta_t\boldsymbol{G}_t$
6. 对称化：$\boldsymbol{X}_{t+1} \leftarrow [\boldsymbol{X}_{t+1}]_{\text{sym}}$

### 11. 近端梯度法

**定义11.1（近端算子）**：函数$f$的近端算子定义为：

$$
\text{prox}_{f,\lambda}(\boldsymbol{x}) = \arg\min_{\boldsymbol{y}}\left\{f(\boldsymbol{y}) + \frac{1}{2\lambda}\|\boldsymbol{y}-\boldsymbol{x}\|^2\right\}
$$

**算法11.1（近端梯度下降）**：对于$\min_{\boldsymbol{x}}f(\boldsymbol{x})+g(\boldsymbol{x})$（$f$光滑，$g$可能非光滑）：

$$
\boldsymbol{x}_{t+1} = \text{prox}_{g,\eta_t}(\boldsymbol{x}_t - \eta_t\nabla f(\boldsymbol{x}_t))
$$

**推导11.1（对偶问题的近端形式）**：若原问题可分解为：

$$
\min_{\boldsymbol{X}} f(\boldsymbol{X}) + g(\boldsymbol{X})
$$

其中$f(\boldsymbol{X})=\|\boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X}\|_*$，$g(\boldsymbol{X})=\mathbb{I}_{\text{sym}}(\boldsymbol{X})$（对称约束的指示函数）。

近端算子为：

$$
\text{prox}_{g,\lambda}(\boldsymbol{M}) = [\boldsymbol{M}]_{\text{sym}}
$$

即投影到对称矩阵。

### 12. 加速方法

**算法12.1（Nesterov加速）**：对于凸光滑函数，Nesterov加速梯度下降为：

1. $\boldsymbol{y}_t = \boldsymbol{x}_t + \frac{t-1}{t+2}(\boldsymbol{x}_t - \boldsymbol{x}_{t-1})$（动量项）
2. $\boldsymbol{x}_{t+1} = \boldsymbol{y}_t - \eta_t\nabla f(\boldsymbol{y}_t)$

**定理12.1（加速收敛率）**：Nesterov加速达到$O(1/T^2)$收敛率，相比标准梯度下降的$O(1/T)$。

**推导12.1（在对偶问题上的应用）**：将Nesterov加速应用到对偶问题$\min_{\boldsymbol{X}}d(\boldsymbol{X})$：

1. $\boldsymbol{Y}_t = \boldsymbol{X}_t + \beta_t(\boldsymbol{X}_t - \boldsymbol{X}_{t-1})$
2. $\boldsymbol{Z}_t = \boldsymbol{G}+2\boldsymbol{W}\boldsymbol{Y}_t$，SVD得到$\boldsymbol{U}_t\boldsymbol{V}_t^{\top}$
3. $\boldsymbol{X}_{t+1} = [\boldsymbol{Y}_t - 2\eta_t\boldsymbol{W}^{\top}\boldsymbol{U}_t\boldsymbol{V}_t^{\top}]_{\text{sym}}$

其中$\beta_t = \frac{t-1}{t+2}$。

### 13. 对偶间隙作为停止准则

**定义13.1（对偶间隙）**：当前迭代的对偶间隙为：

$$
\text{gap}_t = \text{primal}_t - \text{dual}_t
$$

其中：
- 原始目标：$\text{primal}_t = \text{tr}(\boldsymbol{G}^{\top}\boldsymbol{\Phi}_t)$
- 对偶目标：$\text{dual}_t = \|\boldsymbol{G}+2\boldsymbol{W}\boldsymbol{X}_t\|_*$

**定理13.1（间隙的非负性）**：由弱对偶性，$\text{gap}_t\geq 0$，且$\text{gap}_t\to 0$当且仅当收敛到最优解。

**推导13.1（实用停止准则）**：当$\text{gap}_t < \epsilon$时停止，保证解的最优性在$\epsilon$范围内。

### 14. Bregman散度的几何解释

**定理14.1（Bregman投影）**：Bregman散度诱导的投影定义为：

$$
\Pi_{\mathcal{C}}^{\phi}(\boldsymbol{x}) = \arg\min_{\boldsymbol{y}\in\mathcal{C}}D_{\phi}(\boldsymbol{y},\boldsymbol{x})
$$

**推导14.1（与欧氏投影的对比）**：
- 欧氏投影：$\Pi_{\mathcal{C}}(\boldsymbol{x})=\arg\min_{\boldsymbol{y}\in\mathcal{C}}\|\boldsymbol{y}-\boldsymbol{x}\|^2$
- Bregman投影：使用$\phi$诱导的"距离"

不同的$\phi$选择导致不同的几何结构：
- $\phi(\boldsymbol{x})=\frac{1}{2}\|\boldsymbol{x}\|^2$：退化为欧氏投影
- $\phi(\boldsymbol{x})=\sum_i x_i\log x_i$（熵）：KL散度

### 15. 在不同流形上的应用

**应用15.1（单位球面）**：对于$\|\boldsymbol{x}\|=1$的约束：

镜像函数选择：$\phi(\boldsymbol{x})=-\log(1-\|\boldsymbol{x}\|^2)$

**应用15.2（概率单纯形）**：对于$\sum_i x_i=1, x_i\geq 0$的约束：

镜像函数选择：$\phi(\boldsymbol{x})=\sum_i x_i\log x_i$（负熵）

对偶更新变为乘法更新：

$$
x_{i,t+1} = x_{i,t}\exp(-\eta_t g_{i,t})
$$

归一化后即为经典的指数权重算法。

**应用15.3（正定矩阵锥）**：对于$\boldsymbol{X}\succ\boldsymbol{0}$的约束：

镜像函数选择：$\phi(\boldsymbol{X})=-\log\det(\boldsymbol{X})$（对数行列式势垒）

### 16. 平滑化技术

**定理16.1（Moreau envelope）**：对于非光滑函数$f$，其Moreau envelope定义为：

$$
f_{\lambda}(\boldsymbol{x}) = \min_{\boldsymbol{y}}\left\{f(\boldsymbol{y})+\frac{1}{2\lambda}\|\boldsymbol{y}-\boldsymbol{x}\|^2\right\}
$$

$f_{\lambda}$是$f$的光滑近似，满足$\nabla f_{\lambda}(\boldsymbol{x})=\frac{1}{\lambda}(\boldsymbol{x}-\text{prox}_{f,\lambda}(\boldsymbol{x}))$。

**推导16.1（核范数的平滑化）**：对于$f(\boldsymbol{M})=\|\boldsymbol{M}\|_*$：

$$
f_{\lambda}(\boldsymbol{M}) = \min_{\boldsymbol{N}}\left\{\|\boldsymbol{N}\|_* + \frac{1}{2\lambda}\|\boldsymbol{N}-\boldsymbol{M}\|_F^2\right\}
$$

这产生了一个可微的近似，便于优化。

### 17. 自适应步长选择

**方法17.1（Armijo准则）**：选择步长$\eta_t$满足：

$$
f(\boldsymbol{x}_t-\eta_t\nabla f(\boldsymbol{x}_t)) \leq f(\boldsymbol{x}_t) - c\eta_t\|\nabla f(\boldsymbol{x}_t)\|^2
$$

其中$c\in(0,1)$（通常$c=0.1$）。

**方法17.2（回溯线搜索）**：从较大步长开始，逐步缩小直到满足Armijo条件：

$$
\eta_t = \beta^k\eta_0, \quad k=0,1,2,\ldots
$$

其中$\beta\in(0,1)$（通常$\beta=0.5$）。

**应用17.1（对偶问题的线搜索）**：在对偶空间$\min_{\boldsymbol{X}}d(\boldsymbol{X})$上应用回溯线搜索，确保每步都有足够下降。

### 18. 随机对偶梯度下降

**算法18.1（随机版本）**：当梯度$\boldsymbol{G}$是随机的（来自小批量）：

1. 采样小批量，计算随机梯度$\boldsymbol{G}_t$
2. 对偶更新：$\boldsymbol{X}_{t+1} = [\boldsymbol{X}_t - \eta_t\nabla d(\boldsymbol{X}_t;\boldsymbol{G}_t)]_{\text{sym}}$

**定理18.1（随机收敛率）**：对于凸问题，随机对偶梯度下降的期望收敛率为：

$$
\mathbb{E}[f(\bar{\boldsymbol{x}}_T)-f(\boldsymbol{x}^*)] = O\left(\frac{1}{\sqrt{T}}\right)
$$

其中$\bar{\boldsymbol{x}}_T = \frac{1}{T}\sum_{t=1}^T\boldsymbol{x}_t$。

### 19. 方差减少技术

**算法19.1（SVRG for dual）**：随机方差减少梯度法的对偶版本：

1. 每隔$m$步，计算完整梯度$\tilde{\boldsymbol{G}}=\nabla d(\tilde{\boldsymbol{X}})$
2. 在内循环中，计算方差减少的梯度估计：
$$
\boldsymbol{G}_t^{\text{SVRG}} = \nabla d(\boldsymbol{X}_t;\boldsymbol{G}_t^i) - \nabla d(\tilde{\boldsymbol{X}};\boldsymbol{G}_t^i) + \tilde{\boldsymbol{G}}
$$
3. 更新：$\boldsymbol{X}_{t+1} = [\boldsymbol{X}_t - \eta_t\boldsymbol{G}_t^{\text{SVRG}}]_{\text{sym}}$

**优势**：在强凸情况下达到线性收敛率，优于标准随机梯度下降。

### 20. 分布式对偶梯度下降

**设定20.1（分布式优化）**：$N$个节点各持有局部梯度$\boldsymbol{G}^{(i)}$，目标是最小化全局对偶函数：

$$
\min_{\boldsymbol{X}} \frac{1}{N}\sum_{i=1}^N d(\boldsymbol{X};\boldsymbol{G}^{(i)})
$$

**算法20.1（同步分布式方法）**：

1. 每个节点$i$计算局部次梯度$\boldsymbol{H}^{(i)}_t$
2. 全局聚合：$\boldsymbol{H}_t = \frac{1}{N}\sum_i\boldsymbol{H}^{(i)}_t$
3. 广播更新：$\boldsymbol{X}_{t+1} = [\boldsymbol{X}_t - \eta_t\boldsymbol{H}_t]_{\text{sym}}$

**算法20.2（异步分布式方法）**：允许节点异步更新，通过参数服务器协调。

### 21. 对偶分解

**定理21.1（可分离结构）**：若原问题具有可分离结构：

$$
\min_{\boldsymbol{x}_1,\ldots,\boldsymbol{x}_N} \sum_{i=1}^N f_i(\boldsymbol{x}_i) \quad \text{s.t.} \quad \sum_i\boldsymbol{A}_i\boldsymbol{x}_i = \boldsymbol{b}
$$

对偶问题为：

$$
\max_{\boldsymbol{\lambda}} -\sum_{i=1}^N f_i^*(\boldsymbol{A}_i^{\top}\boldsymbol{\lambda}) - \langle\boldsymbol{\lambda},\boldsymbol{b}\rangle
$$

可以并行求解各个$f_i^*$。

**应用21.1（多层Stiefel约束）**：当多个层都有Stiefel约束时，可以使用对偶分解并行优化。

### 22. 正则化路径

**定义22.1（正则化参数）**：考虑参数化的对偶问题：

$$
\min_{\boldsymbol{X}} d(\boldsymbol{X}) + \mu R(\boldsymbol{X})
$$

其中$R$是正则项，$\mu\geq 0$是正则化参数。

**算法22.1（路径跟踪）**：从大$\mu$开始，逐步减小到目标值，每次使用前一个$\mu$的解作为初值（warm start）。

**优势**：在强正则化下，问题更易求解；逐步减小$\mu$提供良好的初始化。

### 23. 投影的高效实现

**算法23.1（对称化投影）**：对于非对称矩阵$\boldsymbol{M}$，投影到对称矩阵：

$$
[\boldsymbol{M}]_{\text{sym}} = \frac{\boldsymbol{M}+\boldsymbol{M}^{\top}}{2}
$$

复杂度：$O(m^2)$

**算法23.2（Stiefel投影）**：将矩阵$\boldsymbol{M}$投影到Stiefel流形（QR分解法）：

1. QR分解：$\boldsymbol{M}=\boldsymbol{Q}\boldsymbol{R}$
2. 符号调整：$\boldsymbol{D}=\text{diag}(\text{sign}(R_{ii}))$
3. 返回：$\boldsymbol{Q}\boldsymbol{D}$

复杂度：$O(nm^2)$

### 24. 数值稳定性技巧

**技巧24.1（梯度裁剪）**：当次梯度过大时：

$$
\tilde{\boldsymbol{G}}_t = \begin{cases}
\boldsymbol{G}_t & \text{if } \|\boldsymbol{G}_t\|_F \leq \tau\\
\tau\frac{\boldsymbol{G}_t}{\|\boldsymbol{G}_t\|_F} & \text{otherwise}
\end{cases}
$$

**技巧24.2（对偶变量的投影）**：限制对偶变量$\boldsymbol{X}$的范数：

$$
\boldsymbol{X}_{t+1} \leftarrow \begin{cases}
\boldsymbol{X}_{t+1} & \text{if } \|\boldsymbol{X}_{t+1}\|_F \leq B\\
B\frac{\boldsymbol{X}_{t+1}}{\|\boldsymbol{X}_{t+1}\|_F} & \text{otherwise}
\end{cases}
$$

### 25. 二阶方法

**算法25.1（牛顿法的对偶版本）**：对于光滑的对偶函数$d(\boldsymbol{X})$：

$$
\boldsymbol{X}_{t+1} = \boldsymbol{X}_t - [\nabla^2 d(\boldsymbol{X}_t)]^{-1}\nabla d(\boldsymbol{X}_t)
$$

**挑战**：Hessian$\nabla^2 d(\boldsymbol{X})$的计算和存储在高维时不可行。

**算法25.2（拟牛顿法）**：使用BFGS或L-BFGS近似Hessian：

$$
\boldsymbol{H}_{t+1} = \boldsymbol{H}_t + \frac{\boldsymbol{y}_t\boldsymbol{y}_t^{\top}}{\boldsymbol{y}_t^{\top}\boldsymbol{s}_t} - \frac{\boldsymbol{H}_t\boldsymbol{s}_t\boldsymbol{s}_t^{\top}\boldsymbol{H}_t}{\boldsymbol{s}_t^{\top}\boldsymbol{H}_t\boldsymbol{s}_t}
$$

其中$\boldsymbol{s}_t=\boldsymbol{X}_{t+1}-\boldsymbol{X}_t$，$\boldsymbol{y}_t=\nabla d(\boldsymbol{X}_{t+1})-\nabla d(\boldsymbol{X}_t)$。

### 26. 与原始-对偶方法的比较

**方法26.1（原始-对偶交替）**：同时更新原始和对偶变量：

1. 原始更新：$\boldsymbol{\Phi}_{t+1}=\arg\max_{\|\boldsymbol{\Phi}\|_2=1}\mathcal{L}(\boldsymbol{\Phi},\boldsymbol{X}_t)$
2. 对偶更新：$\boldsymbol{X}_{t+1}=\boldsymbol{X}_t+\eta_t\nabla_{\boldsymbol{X}}\mathcal{L}(\boldsymbol{\Phi}_{t+1},\boldsymbol{X}_t)$

**比较**：
- 纯对偶方法：只在对偶空间迭代，最后恢复原始解
- 原始-对偶方法：两个空间交替更新，可能更稳定但代价更高

### 27. 复杂度分析

**定理27.1（单次迭代复杂度）**：对偶梯度下降的单次迭代：

1. SVD分解：$O(nm^2)$
2. 矩阵乘法$\boldsymbol{W}^{\top}\boldsymbol{U}\boldsymbol{V}^{\top}$：$O(nm^2)$
3. 对称化：$O(m^2)$

总复杂度：$O(nm^2)$

**比较27.1（与Lyapunov迭代的对比）**：
- Lyapunov方法：每次迭代$O(nm^2+m^3)$，需要5-10次迭代
- 对偶方法：每次迭代$O(nm^2)$，但可能需要更多迭代

在$m$较大时，对偶方法可能更快。

### 28. 实验验证

**实验28.1（收敛曲线）**：在标准测试问题上，对偶梯度下降与Lyapunov迭代的对比：

- 对偶方法：单调下降对偶目标，对偶间隙线性收敛
- Lyapunov方法：直接求解方程，不保证单调性，但每步更接近解

**观察28.1（初值敏感性）**：对偶方法对初值$\boldsymbol{X}_0$较敏感，建议使用：

$$
\boldsymbol{X}_0 = -[\boldsymbol{W}^{\top}\boldsymbol{G}]_{\text{sym}}
$$

这与Lyapunov方法的初值一致。

### 29. 理论保证

**定理29.1（全局收敛性）**：对于凸的对偶问题，对偶梯度下降在适当步长下全局收敛到最优解。

**证明思路**：利用对偶函数的凸性和梯度（或次梯度）下降的标准收敛分析。

**定理29.2（强对偶性的充分条件）**：当Stiefel流形上的问题满足Slater条件（存在严格可行点）时，强对偶性成立。

### 30. 开放问题与未来方向

**问题30.1（非凸对偶）**：当原问题非凸时，对偶问题也可能非凸。如何在非凸对偶空间有效优化？

**问题30.2（对偶间隙的紧致性）**：在什么条件下对偶间隙为零？Stiefel流形的特殊结构是否保证强对偶性？

**问题30.3（自适应镜像函数）**：能否根据问题结构自适应选择镜像函数$\phi$，以加速收敛？

**问题30.4（在线对偶梯度下降）**：在在线学习场景中，如何高效维护对偶变量并保证遗憾界？

### 31. 总结与展望

本节全面介绍了对偶梯度下降在流形优化中的应用：

**核心内容**：
1. 对偶空间与Legendre变换的数学基础
2. Lagrange对偶理论与KKT条件
3. 镜像下降与Bregman散度的几何解释
4. 对偶梯度下降在Stiefel流形上的具体实现
5. 收敛性分析与加速技术

**理论贡献**：
- 将凸优化的对偶理论引入流形优化
- 提供了与Lyapunov迭代互补的求解途径
- 建立了镜像下降与流形约束的联系

**实践意义**：
- 为Stiefel流形优化提供了新的算法选择
- 在某些问题上可能比直接迭代更高效
- 对偶间隙提供了明确的收敛判据

**与前文方法的对比**：
- Lyapunov迭代：直接求解方程，收敛快但每步代价高
- 对偶梯度下降：在对偶空间优化，单步便宜但可能需要更多迭代
- 选择取决于问题规模、精度要求和可用计算资源

通过以上31个部分的详细推导，我们建立了对偶梯度下降的完整理论框架，为流形优化提供了坚实的数学基础和实用的算法工具。对偶视角不仅丰富了我们对流形优化的理解，也为未来的研究开辟了新的方向。

