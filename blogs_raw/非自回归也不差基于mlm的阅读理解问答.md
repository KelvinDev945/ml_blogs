---
title: “非自回归”也不差：基于MLM的阅读理解问答
slug: 非自回归也不差基于mlm的阅读理解问答
date: 
source: https://spaces.ac.cn/archives/7148
tags: 问答, 语言模型, 生成模型, 文本生成, 生成模型
status: pending
---

# “非自回归”也不差：基于MLM的阅读理解问答

**原文链接**: [https://spaces.ac.cn/archives/7148](https://spaces.ac.cn/archives/7148)

**发布日期**: 

---

前段时间写了[《万能的seq2seq：基于seq2seq的阅读理解问答》](/archives/7115)，探索了以最通用的seq2seq的方式来做阅读理解式问答，并且取得相当不错的成绩（单模型0.77，超过参加比赛时精调的最佳模型）。这篇文章我们继续做这个任务，不过换一个思路，直接基于MLM模型来做，最终成绩基本一致，但能提高预测速度。

## 两种生成 #

广义来讲，MLM的生成方式也算是seq2seq模型，只不过它属于“非自回归”生成，而我们通常说的（狭义的）seq2seq，则是指自回归生成。本节对这两个概念做简单的介绍。

### 自回归生成 #

顾名思义，自回归生成指的是解码阶段是逐字逐字地递归生成的，它建模的是如下的概率分布  
\begin{equation}p(y_1,y_2,\dots,y_n|x)=p(y_1|x)p(y_2|x,y_1)\dots p(y_n|x,y_1,\dots,y_{n-1})\label{eq:at}\end{equation}  
更详细的介绍可以参考[《玩转Keras之seq2seq自动生成标题》](/archives/5861)和[《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》](/archives/6933)，在此不对自回归生成做过多介绍。

### 非自回归生成 #

由于自回归生成需要递归地进行解码，无法并行，所以解码速度比较慢，因此近年来有不少工作在研究非自回归生成，也取得不少成果。简单来说，非自回归生成就是想办法使得每个字的解码可并行化，最简单的非自回归模型就是直接假设每个字是独立的：  
\begin{equation}p(y_1,y_2,\dots,y_n|x)=p(y_1|x)p(y_2|x)\dots p(y_n|x)\label{eq:nat}\end{equation}  
这是一个很强的假设，只有在一些比较特殊的情况下才适用，直接用它来做普通的文本生成如自动摘要的话，效果会很差的。更复杂的非自回归生成的相关工作，大家在Arxiv或Google上搜索non-autoregressive text generation就可以找到很多。

标题已经提到了，本文做阅读理解的方式是“基于MLM”，对Bert模型有所了解的读者应该知道，MLM（Masked Language Model）实际上也是$\eqref{eq:nat}$的一个特例，所以基于MLM所做的生成模型，属于非自回归生成的范畴。

## 模型简介 #

所谓“简介”，真的很简，因为基于MLM来做阅读理解确实非常简单。

### 模型图示 #

首先定一个最大长度$l_{\max}$，然后将问题和篇章拼接起来，再在里边拼接$l_{\max}$个[MASK]标记，输入到Bert中，最后让[MASK]对应的部分来预测答案即可（不管是训练还是预测阶段都如此），如下图示：  


[![用MLM做阅读理解的模型图示（其中\[M\]表示\[MASK\]标记）](/usr/uploads/2019/12/1024911876.png)](/usr/uploads/2019/12/1024911876.png "点击查看原图")

用MLM做阅读理解的模型图示（其中[M]表示[MASK]标记）

### 代码效果 #

代码链接：[task_reading_comprehension_by_mlm.py](https://github.com/bojone/bert4keras/blob/master/examples/task_reading_comprehension_by_mlm.py)

最终在[SogouQA自带的评估脚本](https://github.com/bojone/dgcnn_for_reading_comprehension)上，valid集的分数大概是0.77 (Accuracy=0.7282149325820084 F1=0.8207266829447049 Final=0.7744708077633566)，跟前段时间的[《万能的seq2seq：基于seq2seq的阅读理解问答》](/archives/7115)模型持平，但是预测速度明显提升，之前的seq2seq方案每秒只能预测2条数据左右，现在每秒预测能达到12条，6倍的提速而且不降低效果。

## 选哪种好？ #

原则上来说seq2seq是万能的，而且原则上seq2seq所建模的式$\eqref{eq:at}$要比MLM所建模的$\eqref{eq:nat}$要合理，为什么MLM方案能取得跟seq2seq持平的效果？什么时候该用MLM，什么时候该用seq2seq？

### 训练和预测 #

首先，seq2seq最大的问题就是慢，如果长文本生成就更慢了，因此如果要求高效率，那自然就不得不放弃seq2seq这种方案了。

如果不考虑效率，那么是不是seq2seq就最好呢？也不是。尽管从建模上来看，$\eqref{eq:at}$更加准确，但是seq2seq的训练是通过teacher forcing的方式来做的，所以存在“exposure bias”的问题：训练的时候，每个时刻的输入都是来自于真实的答案文本；而生成的时候，每个时刻的输入来自于前一时刻的输出；所以一旦有一个字生成的不好，错误可能会接着传递，使得生成的越来越糟糕。

说白了，也就是训练和预测时存在不一致性，这种不一致性可能导致误差累积。相反，基于MLM的方案在训练和预测时的行为是一致的，因为不需要真实标签作为输入（预测时答案部分的位置也输入[MASK]），因此不存在误差累积情况。而且也正好因为这个特点，因此解码时不再需要递归，而是可并行化，提高了解码速度。

### 正确答案唯一 #

此外，MLM等非自回归生成，相对来说更加适用于短文本生成，因为文本越短越贴近这个独立假设，同时，非自回归生成适用于“正确的答案只有一个”的场景，而本文所做的阅读理解任务主要是抽取式的，刚好就对应着这种场景，因此MLM表现也不俗。

事实上，序列标注模型如逐帧Softmax或者CRF，其实也可以视为非自回归生成模型，它们之所以有效，我认为根本原因也在于“正确的答案序列是唯一的”，而不是直觉上认为的“输入与输出对齐”。也就是说，如果满足“正确的答案只有一个”这个条件，那么就可以考虑用非自回归生成来做。

注意这个的答案唯一并不是指每个样本只有一个人工标注答案，而是指这个任务从设计上就让答案变得唯一了，比如分词，设计好标注方式后，每个句子只对应唯一一种正确的分词方案，再比如标题生成，显然同一篇文章可以起不同的标题，因此标题生成答案就不是唯一的（哪怕训练数据里边每篇文章只有一个标题）。

## 文章小结 #

本文试验了通过MLM的非自回归生成方式来做阅读理解式问答，发现最后的效果也不赖，而且速度有了好几倍的提升。此外，文章还简单对比了自回归和非自回归生成的异同，分析了非自回归方案何时适用及其原因。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/7148>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Dec. 26, 2019). 《“非自回归”也不差：基于MLM的阅读理解问答 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/7148>

@online{kexuefm-7148,  
title={“非自回归”也不差：基于MLM的阅读理解问答},  
author={苏剑林},  
year={2019},  
month={Dec},  
url={\url{https://spaces.ac.cn/archives/7148}},  
} 


---

## 公式推导与注释

TODO: 添加详细的数学公式推导和注释

