---
title: 低秩近似之路（一）：伪逆
slug: 低秩近似之路一伪逆
date: 2024-09-15
tags: 近似, 矩阵, 低秩, 生成模型, attention
status: completed
---

# 低秩近似之路（一）：伪逆

**原文链接**: [https://spaces.ac.cn/archives/10366](https://spaces.ac.cn/archives/10366)

**发布日期**: 

---

可能很多读者跟笔者一样，对矩阵的低秩近似有种熟悉而又陌生的感觉。熟悉是因为，低秩近似的概念和意义都不难理解，加之目前诸如LoRA等基于低秩近似的微调技术遍地开花，让低秩近似的概念在耳濡目染间就已经深入人心；然而，低秩近似所覆盖的内容非常广，在低秩近似相关的论文中时常能看到一些不熟悉但又让我们叹为观止的新技巧，这就导致了一种似懂非懂的陌生感。

因此，在这个系列文章中，笔者将试图系统梳理一下矩阵低秩近似相关的理论内容，以补全对低秩近似的了解。而在第一篇文章中，我们主要介绍低秩近似系列中相对简单的一个概念——伪逆。

## 优化视角 #

伪逆（Pseudo Inverse），也称“广义逆（Generalized Inverse）”，顾名思义就是“广义的逆矩阵”，它实际上是“逆矩阵”的概念对于不可逆矩阵的推广。

我们知道，对于矩阵方程$\boldsymbol{A}\boldsymbol{B}=\boldsymbol{M}$，如果$\boldsymbol{A}$是方阵且可逆，那么直接得到$\boldsymbol{B}=\boldsymbol{A}^{-1}\boldsymbol{M}$，可如果$\boldsymbol{A}$不可逆或者干脆不是方阵呢？这种情况下我们很可能找不到$\boldsymbol{B}$满足$\boldsymbol{A}\boldsymbol{B}=\boldsymbol{M}$，这时候我们如果还想继续下去的话，通常是转为优化问题  
\begin{equation}\mathop{\text{argmin}}_\boldsymbol{B} \Vert \boldsymbol{A}\boldsymbol{B} - \boldsymbol{M}\Vert_F^2\label{eq:loss-ab-m}\end{equation}  
其中$\boldsymbol{A}\in\mathbb{R}^{n\times r}, \boldsymbol{B}\in\mathbb{R}^{r\times m}, \boldsymbol{M}\in\mathbb{R}^{n\times m}$，注意数域是$\mathbb{R}$，表明本系列文章关注的都是实矩阵，而$\Vert\cdot\Vert_F$是矩阵的$F$范数（Frobenius Norm），用来衡量矩阵$\boldsymbol{A}\boldsymbol{B} - \boldsymbol{M}$与全零矩阵的距离，其定义为  
\begin{equation}\Vert \boldsymbol{M}\Vert_F = \sqrt{\sum_{i=1}^n\sum_{j=1}^m M_{i,j}^2}\end{equation}  
说白了，就是从求精确的逆矩阵改为最小化$\boldsymbol{A}\boldsymbol{B}$与$\boldsymbol{M}$的平方误差。本系列的主题是低秩近似，所以下面假设$r \ll \min(n,m)$，其机器学习意义就是通过低维的、有损的输入矩阵$\boldsymbol{A}$和线性变换$\boldsymbol{B}$来重建完整的$\boldsymbol{M}$矩阵。

当$m=n$且$\boldsymbol{M}$取单位阵$\boldsymbol{I}_n$时，我们就得到一个只依赖于$\boldsymbol{A}$的结果，记为  
\begin{equation}\boldsymbol{A}^{\dagger} = \mathop{\text{argmin}}_\boldsymbol{B} \Vert \boldsymbol{A}\boldsymbol{B} - \boldsymbol{I}_n\Vert_F^2\label{eq:loss-ab-m-b}\end{equation}  
它的作用类似于$\boldsymbol{A}$的逆矩阵，所以称为$\boldsymbol{A}$的“(右)伪逆”。类似地，如果给定的是$\boldsymbol{B}$矩阵，那么我们也可以将优化参数改为$\boldsymbol{A}$，得到$\boldsymbol{B}$的“(左)伪逆”：  
\begin{equation}\boldsymbol{B}^{\dagger} = \mathop{\text{argmin}}_\boldsymbol{A} \Vert \boldsymbol{A}\boldsymbol{B} - \boldsymbol{I}_n\Vert_F^2\end{equation}

## 范数相关 #

在进一步推导之前，我们先补充一下$F$范数的相关介绍。向量的范数想必很多读者已经熟知，比较经典的就是“$p$-范数”：对于$\boldsymbol{x}=(x_1,x_2,\cdots,x_m)$，其$p$-范数定义为  
\begin{equation}\Vert \boldsymbol{x}\Vert_p = \sqrt[\uproot{10}p]{\sum_{i=1}^m |x_i|^p}\end{equation}  
而$p$-范数中，最常见的就是$p=2$的情形，它就是我们经常说的向量模长，也叫“欧几里得范数”，如果省略范数的下标只写$\Vert \boldsymbol{x}\Vert$，那么基本上就是默认$p=2$。

矩阵的范数稍微复杂一些，它至少有两种不同但都常用的范数，其中一种就是上一节已经提到的$F$范数，它是直接将矩阵展平为向量来计算的范数：  
\begin{equation}\Vert \boldsymbol{M}\Vert_F = \Vert \text{vec}(\boldsymbol{M})\Vert_2 = \sqrt{\sum_{i=1}^n\sum_{j=1}^m M_{i,j}^2}\end{equation}  
其他矩阵范数我们遇到时再作介绍。由于矩阵范数的多样性，所以$\Vert \boldsymbol{M}\Vert_F$的下标${}_F$通常不能省略，以免引起混淆。$F$范数是将矩阵当成向量然后照搬向量范数的定义而来的，由此启发我们可以尝试把更多的向量运算搬到矩阵中去，比如内积：  
\begin{equation}\langle \boldsymbol{P}, \boldsymbol{Q} \rangle_F = \langle \text{vec}(\boldsymbol{P}), \text{vec}(\boldsymbol{Q}) \rangle = \sum_{i=1}^n\sum_{j=1}^m P_{i,j} Q_{i,j}\end{equation}  
这称为矩阵$\boldsymbol{P},\boldsymbol{Q}$的$F$内积（Frobenius Inner Product），其中$\boldsymbol{P},\boldsymbol{Q}\in\mathbb{R}^{n\times m}$，它可以用向量的迹运算来表示  
\begin{equation}\langle \boldsymbol{P}, \boldsymbol{Q} \rangle_F = \text{Tr}(\boldsymbol{P}^{\top} \boldsymbol{Q})\end{equation}  
这可以直接由矩阵乘法和迹的定义来证明（请读者尝试一下）。当$\boldsymbol{P},\boldsymbol{Q}$是由多个矩阵连乘而来时，转换为等价的迹运算通常能帮助我们进行化简。比如，利用它我们可以证明正交变换不改变$F$范数：假设$\boldsymbol{U}$是一个正交矩阵，利用$\Vert \boldsymbol{M}\Vert_F^2 = \langle \boldsymbol{M}, \boldsymbol{M} \rangle_F$以及$F$内积与迹的关系，得到  
\begin{equation}\Vert \boldsymbol{U}\boldsymbol{M}\Vert_F^2 = \text{Tr}((\boldsymbol{U}\boldsymbol{M})^{\top} \boldsymbol{U}\boldsymbol{M})= \text{Tr}(\boldsymbol{M}^{\top} \boldsymbol{U}^{\top}\boldsymbol{U}\boldsymbol{M})=\text{Tr}(\boldsymbol{M}^{\top} \boldsymbol{M}) = \Vert \boldsymbol{M}\Vert_F^2\end{equation}

## 矩阵求导 #

言归正传，对于一个优化目标，最理想的结果自然是能够通过求导来求出解析解，而$\eqref{eq:loss-ab-m}$正好能实现这一点！改结论可以简单“目测”出来：$\boldsymbol{A}\boldsymbol{B}-\boldsymbol{M}$是关于$\boldsymbol{B}$的线性函数，所以$\Vert \boldsymbol{A}\boldsymbol{B}-\boldsymbol{M}\Vert_F^2$是关于$\boldsymbol{A}$或$\boldsymbol{B}$的二次函数，二次函数的最小值有解析解的。

要求$\mathcal{L}=\Vert \boldsymbol{A}\boldsymbol{B}-\boldsymbol{M}\Vert_F^2$关于$\boldsymbol{B}$的导数，首先要求$\mathcal{L}$关于$\boldsymbol{E}=\boldsymbol{A}\boldsymbol{B}-\boldsymbol{M}$的导数，然后求$\boldsymbol{E}$关于$\boldsymbol{B}$的导数，最后通过链式法则组合起来，即  
\begin{equation}\frac{\partial \mathcal{L}}{\partial B_{i,j}} = \sum_{k,l}\frac{\partial \mathcal{L}}{\partial E_{k,l}}\frac{\partial E_{k,l}}{\partial B_{i,j}} \end{equation}  
根据定义$\mathcal{L}=\Vert \boldsymbol{E}\Vert_F^2 = \sum_{i,j} E_{i,j}^2$，很明显在求和的众多平方中只有当$(i,j)=(k,l)$时，关于$E_{k,l}$的导数才不为零，所以$\mathcal{L}$关于$E_{k,l}$的导数就是$E_{k,l}^2$关于$E_{k,l}$的导数，即$\frac{\partial \mathcal{L}}{\partial E_{k,l}}=2E_{k,l}$；接着，根据矩阵乘法的定义有  
\begin{equation}E_{k,l} = \left(\sum_{\alpha} A_{k,\alpha}B_{\alpha,l}\right) - M_{k,l}\end{equation}  
类似地，只有当$(\alpha,l)=(i,j)$时，上式对$B_{i,j}$的导数才会产生非零的结果$A_{k,i}$，所以我们可以写出$\frac{\partial E_{k,l}}{\partial B_{i,j}}=A_{k,i}\delta_{l,j}$，这里的$\delta_{l,j}$是Kronecker符号，用来声明$l=j$的条件。将结果组合起来，我们就得到  
\begin{equation}\frac{\partial \mathcal{L}}{\partial B_{i,j}} = 2\sum_{k,l}E_{k,l}A_{k,i}\delta_{l,j} = 2\sum_k E_{k,j}A_{k,i}\end{equation}  
如果我们约定，标量对矩阵的梯度形状跟矩阵本身一致，那么可以写出  
\begin{equation}\frac{\partial \mathcal{L}}{\partial \boldsymbol{B}} = 2\boldsymbol{A}^{\top}(\boldsymbol{A}\boldsymbol{B}-\boldsymbol{M})\end{equation}  
虽然推导过程破费周折，但好在结果还是很直观的：直觉上$\frac{\partial \mathcal{L}}{\partial \boldsymbol{B}}$就是$2(\boldsymbol{A}\boldsymbol{B}-\boldsymbol{M})$与$\boldsymbol{A}$的乘积（类比标量求导），而我们已经约定$\frac{\partial \mathcal{L}}{\partial \boldsymbol{B}}$形状跟$\boldsymbol{B}$形状一致（即$r\times m$），所以就要想办法通过$2(\boldsymbol{A}\boldsymbol{B}-\boldsymbol{M})\in\mathbb{R}^{n\times m}$和$\boldsymbol{A}\in\mathbb{R}^{n\times r}$相乘来凑出一个$r\times m$的结果来，结果就只有上式右端一种方式。根据同样原理，我们可以快速写出  
\begin{equation}\frac{\partial \mathcal{L}}{\partial \boldsymbol{A}} = 2(\boldsymbol{A}\boldsymbol{B}-\boldsymbol{M})\boldsymbol{B}^{\top}\end{equation}

## 基本结果 #

现在我们已经分别求出了$\frac{\partial \mathcal{L}}{\partial \boldsymbol{B}}$和$\frac{\partial \mathcal{L}}{\partial \boldsymbol{A}}$，让它们等于零便可以解出相应的最优解  
\begin{align}  
2\boldsymbol{A}^{\top}(\boldsymbol{A}\boldsymbol{B}-\boldsymbol{M})=0\quad\Rightarrow\quad (\boldsymbol{A}^{\top} \boldsymbol{A})^{-1}\boldsymbol{A}^{\top}\boldsymbol{M} = \mathop{\text{argmin}}_\boldsymbol{B} \Vert \boldsymbol{A}\boldsymbol{B} - \boldsymbol{M}\Vert_F^2 \\\  
2(\boldsymbol{A}\boldsymbol{B}-\boldsymbol{M})\boldsymbol{B}^{\top}=0\quad\Rightarrow\quad \boldsymbol{M}\boldsymbol{B}^{\top}(\boldsymbol{B} \boldsymbol{B}^{\top})^{-1} = \mathop{\text{argmin}}_\boldsymbol{A} \Vert \boldsymbol{A}\boldsymbol{B} - \boldsymbol{M}\Vert_F^2  
\end{align}  
代入$\boldsymbol{M}=\boldsymbol{I}_n$，就得到  
\begin{align}\boldsymbol{A}^{\dagger} = (\boldsymbol{A}^{\top} \boldsymbol{A})^{-1}\boldsymbol{A}^{\top} \label{eq:p-inv-a}\\\  
\boldsymbol{B}^{\dagger} = \boldsymbol{B}^{\top}(\boldsymbol{B} \boldsymbol{B}^{\top})^{-1}\label{eq:p-inv-b}\end{align}  
如果$\boldsymbol{A}$或$\boldsymbol{B}$是可逆方阵，那么容易证明伪逆就等于逆矩阵，即$\boldsymbol{A}^{\dagger}=\boldsymbol{A}^{-1},\boldsymbol{B}^{\dagger}=\boldsymbol{B}^{-1}$。此外，根据上式我们还可以验证：

> 1、$(\boldsymbol{A}^{\dagger})^{\dagger}=\boldsymbol{A},(\boldsymbol{B}^{\dagger})^{\dagger}=\boldsymbol{B}$，即 _伪逆的伪逆_ 等于自身，这意味着伪逆在作为近似逆矩阵的同时，保全了自身的信息；
> 
> 2、$\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A}=\boldsymbol{A},\boldsymbol{B}^{\dagger}\boldsymbol{B}\boldsymbol{B}^{\dagger}=\boldsymbol{B}^{\dagger}$，即$\boldsymbol{A}\boldsymbol{A}^{\dagger},\boldsymbol{B}^{\dagger}\boldsymbol{B}$虽然没法成为单位阵$\boldsymbol{I}$，但对$\boldsymbol{A},\boldsymbol{B}^{\dagger}$来说它们起到了单位阵的作用。

顺便说一下，矩阵的伪逆实际上是一个很宽泛的概念，它有很多种不同的形式，这里我们介绍的实际上是最常见的“[Moore–Penrose逆](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)”，除此之外还有“Drazin逆”、“Bott–Duffin逆”等，但这些笔者也不了解，所以就不作展开，读者可以自行参考维基百科的“[广义逆](https://en.wikipedia.org/wiki/Generalized_inverse)”条目。

## 一般形式 #

不过，事情还没完。式$\eqref{eq:p-inv-a},\eqref{eq:p-inv-b}$成立还有个关键前提是相应的$\boldsymbol{A}^{\top} \boldsymbol{A}$或$\boldsymbol{B} \boldsymbol{B}^{\top}$可逆，如果不可逆呢？

我们以$\boldsymbol{A}^{\dagger}$为例，假设$\boldsymbol{A}^{\top} \boldsymbol{A}$不可逆，那么意味着$\boldsymbol{A}$的秩不足$r$，我们只能从中找到$s < r$个列向量构成的极大线性无关组构成矩阵$\boldsymbol{A}_s\in\mathbb{R}^{n\times s}$，然后$\boldsymbol{A}$可以表示成$\boldsymbol{A}_s \boldsymbol{P}$，其中$\boldsymbol{P}\in\mathbb{R}^{s\times r}$是$\boldsymbol{A}_s$到$\boldsymbol{A}$的矩阵。此时  
\begin{equation}\mathop{\text{argmin}}_\boldsymbol{B} \Vert \boldsymbol{A}\boldsymbol{B} - \boldsymbol{I}_n\Vert_F^2 = \mathop{\text{argmin}}_\boldsymbol{B} \Vert \boldsymbol{A}_s \boldsymbol{P}\boldsymbol{B} - \boldsymbol{I}_n\Vert_F^2\end{equation}  
如果$\boldsymbol{B}$的最优解仍然记为$\boldsymbol{A}^{\dagger}$，那么我们只能确定  
\begin{equation}\boldsymbol{P}\boldsymbol{A}^{\dagger} = \boldsymbol{A}_s^{\dagger} = (\boldsymbol{A}_s^{\top} \boldsymbol{A}_s)^{-1}\boldsymbol{A}_s^{\top}\end{equation}  
由于已经假设了$\boldsymbol{A}_s$是极大线性无关组，所以$\boldsymbol{A}_s^{\top} \boldsymbol{A}_s$必然可逆，因此上式是良好定义的。然而，从$\boldsymbol{A}^{\dagger}$到$\boldsymbol{P}\boldsymbol{A}^{\dagger}$是一个降维到过程，这意味着存在多个$\boldsymbol{A}^{\dagger}$使得$\boldsymbol{P}\boldsymbol{A}^{\dagger} = \boldsymbol{A}_s^{\dagger}$，即此时目标$\eqref{eq:loss-ab-m-b}$的最优解并不唯一，换言之当$\boldsymbol{A}^{\top} \boldsymbol{A}$不可逆时我们无法只凭目标$\eqref{eq:loss-ab-m-b}$来确定唯一的伪逆$\boldsymbol{A}^{\dagger}$。

一个可能的思路是补充$(\boldsymbol{A}^{\dagger})^{\dagger}=\boldsymbol{A}$或$\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger}=\boldsymbol{A}^{\dagger}$，这样结合$\boldsymbol{P}\boldsymbol{A}^{\dagger} = \boldsymbol{A}_s^{\dagger}$就可以唯一地确定$\boldsymbol{A}^{\dagger}$。然而，这样打补丁的味道太浓了，实际上我们可以用一个精妙的技巧更优雅和统一地处理这个问题。问题出在当$\boldsymbol{A}^{\top} \boldsymbol{A}$不可逆时，目标函数$\eqref{eq:loss-ab-m-b}$的最优解不唯一，我们可以加上一个正则项让它变得唯一，求出结果后再让正则项的权重趋于零：  
\begin{equation}\boldsymbol{A}^{\dagger} = \lim_{\epsilon\to 0}\,\mathop{\text{argmin}}_\boldsymbol{B} \Vert \boldsymbol{A}\boldsymbol{B} - \boldsymbol{I}_n\Vert_F^2 + \epsilon\Vert \boldsymbol{B}\Vert_F^2\end{equation}  
这里$\epsilon > 0$，$\epsilon\to 0$是指正向趋于零。由上式可以解得  
\begin{equation}\boldsymbol{A}^{\dagger} = \lim_{\epsilon\to 0}\,(\boldsymbol{A}^{\top} \boldsymbol{A} + \epsilon \boldsymbol{I}_r)^{-1}\boldsymbol{A}^{\top}\label{eq:a-pinv-lim}\end{equation}  
当$\epsilon > 0$时，$\boldsymbol{A}^{\top} \boldsymbol{A} + \epsilon \boldsymbol{I}_r$必然是可逆的（请读者证明一下），因此上式是良好定义的。由于$\epsilon\to 0$时，正则项可以忽略不计，所以上述极限必然是存在的。注意，这里说的是整体极限的存在性，当$\boldsymbol{A}^{\top} \boldsymbol{A}$不可逆时，极限$\lim\limits_{\epsilon\to 0}\,(\boldsymbol{A}^{\top} \boldsymbol{A} + \epsilon \boldsymbol{I}_r)^{-1}$是不存在的（结果会出现无穷大），只有乘上$\boldsymbol{A}^{\top}$后整体再取极限才是正常的。

式$\eqref{eq:a-pinv-lim}$作为伪逆的一般推广有什么优点呢？首先，我们已经有了$\boldsymbol{A}^{\top} \boldsymbol{A}$可逆时的$\boldsymbol{A}^{\dagger}$表达式$\eqref{eq:p-inv-a}$，式$\eqref{eq:a-pinv-lim}$作为它的推广，具有直观且形式一致的理论优雅性；其次，形式上的一致也使得$\boldsymbol{A}^{\top} \boldsymbol{A}$可逆时$\boldsymbol{A}^{\dagger}$的性质如$(\boldsymbol{A}^{\dagger})^{\dagger}$能够得以保留，从而让我们在讨论$\boldsymbol{A}^{\dagger}$时几乎可以完全不考虑$\boldsymbol{A}^{\top} \boldsymbol{A}$的可逆性。

## 数值计算 #

当然，目前的式$\eqref{eq:a-pinv-lim}$只是一个形式化的定义，如果直接利用它来数值计算的话，就必须取一个足够小的$\epsilon$然后把$(\boldsymbol{A}^{\top} \boldsymbol{A} + \epsilon \boldsymbol{I}_r)^{-1}$算出来，这样必然会面临严重的数值不稳定性。为了得到一个稳定的计算方式，我们利用实对称矩阵总可以正交对角化这一特点（[谱定理](https://en.wikipedia.org/wiki/Spectral_theorem)），对$\boldsymbol{A}^{\top} \boldsymbol{A}$作如下分解：  
\begin{equation}\boldsymbol{A}^{\top} \boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Lambda} \boldsymbol{U}^{\top}\end{equation}  
其中$\boldsymbol{U}$是正交矩阵，$\boldsymbol{\Lambda}=\text{diag}(\lambda_1,\lambda_2,\cdots,\lambda_r)$是特征值组成的对角矩阵，由于$\boldsymbol{A}^{\top} \boldsymbol{A}$的半正定性，它的特征值总是非负的。利用这个分解，我们有  
\begin{equation}\begin{aligned}  
(\boldsymbol{A}^{\top} \boldsymbol{A} + \epsilon \boldsymbol{I}_r)^{-1}\boldsymbol{A}^{\top} =&\, (\boldsymbol{U}\boldsymbol{\Lambda} \boldsymbol{U}^{\top} + \epsilon \boldsymbol{I}_r)^{-1} \boldsymbol{A}^{\top} \\\  
=&\, [\boldsymbol{U}(\boldsymbol{\Lambda} + \epsilon \boldsymbol{I}_r) \boldsymbol{U}^{\top}]^{-1}\boldsymbol{A}^{\top} \\\  
=&\, \boldsymbol{U}(\boldsymbol{\Lambda} + \epsilon \boldsymbol{I}_r)^{-1} \boldsymbol{U}^{\top}\boldsymbol{A}^{\top}  
\end{aligned}\end{equation}  
对于$(\boldsymbol{\Lambda} + \epsilon \boldsymbol{I}_r)^{-1}$我们有  
\begin{equation}(\boldsymbol{\Lambda} + \epsilon \boldsymbol{I}_r)^{-1} = \text{diag}\Big((\lambda_1 + \epsilon)^{-1},(\lambda_2 + \epsilon)^{-1},\cdots,(\lambda_r + \epsilon)^{-1}\Big)\end{equation}  
如果$\lambda_i > 0$，那么$\lim\limits_{\epsilon\to 0}\,(\lambda_i + \epsilon)^{-1}=\lambda_i^{-1}$，这是有限的结果，不妨碍计算，问题出现在$\lambda_i = 0$时$\lim\limits_{\epsilon\to 0}\,(\lambda_i + \epsilon)^{-1}=\lim\limits_{\epsilon\to 0}\,\epsilon^{-1}\to\infty$上。然而，我们知道$\epsilon\to 0$时正则项的影响就会消失，所以断定极限$\eqref{eq:a-pinv-lim}$必然不会出现无穷大值，因此如果存在$\lambda_i=0$，那么右端所乘的$\boldsymbol{U}^{\top}\boldsymbol{A}^{\top}$必然有办法抵消$\lim\limits_{\epsilon\to 0}\, \epsilon^{-1}$带来的无穷大。而能抵消这种无穷大的，唯有“乘以0”，即$\lim\limits_{\epsilon\to 0}\, \epsilon^{-1}\times 0 = 0$。

换句话说，如果$\lambda_i=0$，那么$\boldsymbol{U}^{\top}\boldsymbol{A}^{\top}$给$(\lambda_i+\epsilon)^{-1}$所乘的因子必然是$0$。既然如此，由于“0乘任何数都得0”，所以其实$\lambda_i=0$时$(\lambda_i+\epsilon)^{-1}$的取值反而不重要，我们可以简单地让它等于0。这样一来，我们就得到了一种通用的计算$\boldsymbol{A}^{\dagger}$的简单方法：  
\begin{equation}\boldsymbol{A}^{\dagger} = \boldsymbol{U}\boldsymbol{\Lambda}^{\dagger}\boldsymbol{U}^{\top}\boldsymbol{A}^{\top}, \quad \boldsymbol{A}^{\top} \boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Lambda} \boldsymbol{U}^{\top}\end{equation}  
其中$\boldsymbol{\Lambda}^{\dagger}$表示对角线上的元素如果等于零则不变，非零则取倒数。

可能有读者疑问，既然“0乘任何数都得0”，那么为什么等于零的$\lambda_i$要不变呢？随意取一个别的值可以吗？其实这里随便取一个值也不会影响结果的，但由于我们用了$\boldsymbol{\Lambda}^{\dagger}$这个记号，那么就要保持它跟式$\eqref{eq:a-pinv-lim}$的一致性，即它跟将对角阵$\boldsymbol{\Lambda}$代入式$\eqref{eq:a-pinv-lim}$的直接计算结果要一致  
\begin{equation}\boldsymbol{\Lambda}^{\dagger} = \lim_{\epsilon\to 0}\,(\boldsymbol{\Lambda}^{\top} \boldsymbol{\Lambda} + \epsilon \boldsymbol{I}_r)^{-1}\boldsymbol{\Lambda}^{\top} = \text{diag}(\kappa_1,\kappa_2,\cdots,\kappa_n),\quad \kappa_i = \left\\{\begin{aligned}\lambda_i^{-1}, \,\,\lambda_i\neq 0 \\\ 0, \,\,\lambda_i=0 \end{aligned} \right. \end{equation}

## 文章小结 #

在这篇文章中，我们从低秩近似的角度介绍了伪逆，这是逆矩阵概念对于非方阵或不可逆方阵的扩展，使我们可以更有效地分析和求解一般的矩阵方程。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/10366>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Sep. 15, 2024). 《低秩近似之路（一）：伪逆 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/10366>

@online{kexuefm-10366,  
title={低秩近似之路（一）：伪逆},  
author={苏剑林},  
year={2024},  
month={Sep},  
url={\url{https://spaces.ac.cn/archives/10366}},  
} 


---

## 推导

### 一、Moore-Penrose伪逆的公理化定义

在前面的内容中，我们从优化的角度引入了伪逆的概念。实际上，Moore-Penrose伪逆有一个更加数学化的公理化定义，它通过四个性质来唯一确定。

**定义（Moore-Penrose伪逆）**：给定矩阵$\boldsymbol{A}\in\mathbb{R}^{n\times r}$，如果矩阵$\boldsymbol{A}^{\dagger}\in\mathbb{R}^{r\times n}$满足以下四个条件（称为Penrose条件）：

\begin{align}
\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A} &= \boldsymbol{A} \tag{MP1}\\\
\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger} &= \boldsymbol{A}^{\dagger} \tag{MP2}\\\
(\boldsymbol{A}\boldsymbol{A}^{\dagger})^{\top} &= \boldsymbol{A}\boldsymbol{A}^{\dagger} \tag{MP3}\\\
(\boldsymbol{A}^{\dagger}\boldsymbol{A})^{\top} &= \boldsymbol{A}^{\dagger}\boldsymbol{A} \tag{MP4}
\end{align}

那么称$\boldsymbol{A}^{\dagger}$为$\boldsymbol{A}$的Moore-Penrose伪逆。

这四个条件有各自的几何意义：
- (MP1)和(MP2)要求$\boldsymbol{A}^{\dagger}$在某种意义上是$\boldsymbol{A}$的"弱逆"
- (MP3)要求$\boldsymbol{A}\boldsymbol{A}^{\dagger}$是对称矩阵，它是到$\boldsymbol{A}$列空间的正交投影
- (MP4)要求$\boldsymbol{A}^{\dagger}\boldsymbol{A}$是对称矩阵，它是到$\boldsymbol{A}$行空间的正交投影

**定理1（伪逆的唯一性）**：满足四个Penrose条件的矩阵$\boldsymbol{A}^{\dagger}$是唯一的。

**证明**：假设存在两个矩阵$\boldsymbol{X}$和$\boldsymbol{Y}$都满足四个条件。我们需要证明$\boldsymbol{X}=\boldsymbol{Y}$。

利用四个条件，我们有：
\begin{align}
\boldsymbol{X} &= \boldsymbol{X}\boldsymbol{A}\boldsymbol{X} \quad\text{（对}\boldsymbol{X}\text{应用MP2）}\\\
&= \boldsymbol{X}\boldsymbol{A}(\boldsymbol{Y}\boldsymbol{A}\boldsymbol{Y}) \quad\text{（对}\boldsymbol{Y}\text{应用MP2，}Y=YAY\text{）}\\\
&= (\boldsymbol{X}\boldsymbol{A}\boldsymbol{Y})\boldsymbol{A}\boldsymbol{Y} \quad\text{（结合律）}\\\
&= (\boldsymbol{X}\boldsymbol{A}\boldsymbol{Y})^{\top}\boldsymbol{A}\boldsymbol{Y} \quad\text{（}XAY\text{是对称的，见下）}\\\
&= \boldsymbol{Y}^{\top}\boldsymbol{A}^{\top}\boldsymbol{X}^{\top}\boldsymbol{A}\boldsymbol{Y}\\\
&= \boldsymbol{Y}^{\top}(\boldsymbol{A}^{\top}\boldsymbol{X}^{\top})(\boldsymbol{A}\boldsymbol{Y})\\\
&= \boldsymbol{Y}^{\top}(\boldsymbol{X}\boldsymbol{A})^{\top}(\boldsymbol{A}\boldsymbol{Y})\\\
&= \boldsymbol{Y}^{\top}(\boldsymbol{X}\boldsymbol{A})(\boldsymbol{A}\boldsymbol{Y}) \quad\text{（对}\boldsymbol{X}\text{应用MP4）}\\\
&= \boldsymbol{Y}^{\top}\boldsymbol{X}(\boldsymbol{A}\boldsymbol{A})\boldsymbol{Y}\\\
&= (\boldsymbol{Y}\boldsymbol{A})^{\top}(\boldsymbol{X}\boldsymbol{A})\boldsymbol{Y}\\\
&= (\boldsymbol{Y}\boldsymbol{A})(\boldsymbol{X}\boldsymbol{A})\boldsymbol{Y} \quad\text{（对}\boldsymbol{Y}\text{应用MP4）}\\\
&= \boldsymbol{Y}(\boldsymbol{A}\boldsymbol{X})\boldsymbol{A}\boldsymbol{Y}\\\
&= \boldsymbol{Y}(\boldsymbol{A}\boldsymbol{X})^{\top}\boldsymbol{A}\boldsymbol{Y} \quad\text{（对}\boldsymbol{X}\text{应用MP3）}\\\
&= \boldsymbol{Y}\boldsymbol{X}^{\top}\boldsymbol{A}^{\top}\boldsymbol{A}\boldsymbol{Y}\\\
&= \boldsymbol{Y}(\boldsymbol{X}\boldsymbol{A})^{\top}\boldsymbol{A}\boldsymbol{Y}\\\
&= \boldsymbol{Y}(\boldsymbol{X}\boldsymbol{A})\boldsymbol{A}\boldsymbol{Y} \quad\text{（对}\boldsymbol{X}\text{应用MP4）}\\\
&= \boldsymbol{Y}\boldsymbol{X}\boldsymbol{A}\boldsymbol{A}\boldsymbol{Y}\\\
&= \boldsymbol{Y}\boldsymbol{X}(\boldsymbol{A}\boldsymbol{A}\boldsymbol{Y})\\\
&= \boldsymbol{Y}\boldsymbol{X}\boldsymbol{A}(\boldsymbol{A}\boldsymbol{Y})\\\
&= \boldsymbol{Y}(\boldsymbol{X}\boldsymbol{A}\boldsymbol{A})\boldsymbol{Y}\\\
&= \boldsymbol{Y}((\boldsymbol{A}^{\top}\boldsymbol{X}^{\top})^{\top}\boldsymbol{A})\boldsymbol{Y}\\\
&= \boldsymbol{Y}((\boldsymbol{A}\boldsymbol{X})^{\top}\boldsymbol{A})\boldsymbol{Y}\\\
&= \boldsymbol{Y}((\boldsymbol{A}\boldsymbol{X})\boldsymbol{A})\boldsymbol{Y} \quad\text{（对}\boldsymbol{X}\text{应用MP3）}\\\
&= \boldsymbol{Y}\boldsymbol{A}(\boldsymbol{X}\boldsymbol{A})\boldsymbol{Y}\\\
&= (\boldsymbol{Y}\boldsymbol{A}\boldsymbol{X})\boldsymbol{A}\boldsymbol{Y}\\\
&= (\boldsymbol{Y}\boldsymbol{A}\boldsymbol{X})^{\top}\boldsymbol{A}\boldsymbol{Y} \quad\text{（}YAX\text{是对称的）}\\\
&= \boldsymbol{X}^{\top}\boldsymbol{A}^{\top}\boldsymbol{Y}^{\top}\boldsymbol{A}\boldsymbol{Y}\\\
&= \boldsymbol{X}^{\top}(\boldsymbol{A}^{\top}\boldsymbol{Y}^{\top})(\boldsymbol{A}\boldsymbol{Y})\\\
&= \boldsymbol{X}^{\top}(\boldsymbol{Y}\boldsymbol{A})^{\top}(\boldsymbol{A}\boldsymbol{Y})\\\
&= \boldsymbol{X}^{\top}(\boldsymbol{Y}\boldsymbol{A})(\boldsymbol{A}\boldsymbol{Y}) \quad\text{（对}\boldsymbol{Y}\text{应用MP4）}\\\
&= \boldsymbol{X}^{\top}\boldsymbol{Y}(\boldsymbol{A}\boldsymbol{A})\boldsymbol{Y}\\\
&= (\boldsymbol{X}\boldsymbol{Y})^{\top}(\boldsymbol{A}\boldsymbol{A})\boldsymbol{Y}\\\
&= (\boldsymbol{Y}^{\top}\boldsymbol{X}^{\top})(\boldsymbol{A}\boldsymbol{A})\boldsymbol{Y}
\end{align}

上面证明中关键的一步是证明$\boldsymbol{X}\boldsymbol{A}\boldsymbol{Y}$是对称的。我们有：
\begin{equation}
(\boldsymbol{X}\boldsymbol{A}\boldsymbol{Y})^{\top} = \boldsymbol{Y}^{\top}\boldsymbol{A}^{\top}\boldsymbol{X}^{\top} = (\boldsymbol{Y}\boldsymbol{A})^{\top}(\boldsymbol{A}\boldsymbol{X})^{\top} = (\boldsymbol{Y}\boldsymbol{A})(\boldsymbol{A}\boldsymbol{X}) = \boldsymbol{Y}\boldsymbol{A}\boldsymbol{A}\boldsymbol{X} = \boldsymbol{Y}(\boldsymbol{A}\boldsymbol{A}\boldsymbol{X})
\end{equation}

利用(MP1)对$\boldsymbol{X}$，我们有$\boldsymbol{A}\boldsymbol{X}\boldsymbol{A}=\boldsymbol{A}$，因此$\boldsymbol{A}\boldsymbol{A}\boldsymbol{X}=\boldsymbol{A}(\boldsymbol{A}\boldsymbol{X}\boldsymbol{A})\boldsymbol{X}^{-1}$...

实际上，上述证明过程比较繁琐。我们可以用一个更简洁的证明：

\begin{align}
\boldsymbol{X} &= \boldsymbol{X}\boldsymbol{A}\boldsymbol{X} \quad\text{（MP2对}\boldsymbol{X}\text{）}\\\
&= \boldsymbol{X}\boldsymbol{A}(\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger})\boldsymbol{A}\boldsymbol{X} \quad\text{（插入}YAY=Y\text{）}\\\
&= \boldsymbol{X}(\boldsymbol{A}\boldsymbol{Y})(\boldsymbol{A}\boldsymbol{Y})^{\top}(\boldsymbol{A}\boldsymbol{X}) \quad\text{（MP3对}\boldsymbol{Y}\text{）}\\\
&= [(\boldsymbol{A}\boldsymbol{Y})^{\top}(\boldsymbol{A}\boldsymbol{X})]^{\top}(\boldsymbol{A}\boldsymbol{Y})^{\top}(\boldsymbol{A}\boldsymbol{X})
\end{align}

简洁证明：利用四个条件，我们计算：
\begin{align}
\boldsymbol{X} &= \boldsymbol{X}\boldsymbol{A}\boldsymbol{X} = \boldsymbol{X}\boldsymbol{A}(\boldsymbol{Y}\boldsymbol{A}\boldsymbol{Y})\boldsymbol{A}\boldsymbol{X} = (\boldsymbol{X}\boldsymbol{A}\boldsymbol{Y})(\boldsymbol{A}\boldsymbol{Y}\boldsymbol{A})\boldsymbol{X}\\\
&= (\boldsymbol{X}\boldsymbol{A}\boldsymbol{Y})(\boldsymbol{A}\boldsymbol{Y}\boldsymbol{A})^{\top}\boldsymbol{X} = (\boldsymbol{X}\boldsymbol{A}\boldsymbol{Y})\boldsymbol{A}^{\top}(\boldsymbol{Y}\boldsymbol{A})^{\top}\boldsymbol{X}\\\
&= (\boldsymbol{X}\boldsymbol{A}\boldsymbol{Y})\boldsymbol{A}^{\top}(\boldsymbol{Y}\boldsymbol{A})\boldsymbol{X} = \boldsymbol{X}(\boldsymbol{A}\boldsymbol{Y}\boldsymbol{A})^{\top}(\boldsymbol{Y}\boldsymbol{A})\boldsymbol{X}\\\
&= \boldsymbol{X}(\boldsymbol{A}\boldsymbol{Y}\boldsymbol{A})(\boldsymbol{Y}\boldsymbol{A})\boldsymbol{X} = \boldsymbol{X}\boldsymbol{A}(\boldsymbol{Y}\boldsymbol{A}\boldsymbol{Y})\boldsymbol{A}\boldsymbol{X}\\\
&= \boldsymbol{X}\boldsymbol{A}\boldsymbol{Y}\boldsymbol{A}\boldsymbol{X} = (\boldsymbol{X}\boldsymbol{A}\boldsymbol{X})\boldsymbol{Y}(\boldsymbol{A}\boldsymbol{X}\boldsymbol{A}) = \boldsymbol{X}\boldsymbol{Y}\boldsymbol{A}
\end{align}

同理可得$\boldsymbol{Y}=\boldsymbol{Y}\boldsymbol{X}\boldsymbol{A}$，因此：
\begin{equation}
\boldsymbol{X} = \boldsymbol{X}\boldsymbol{Y}\boldsymbol{A} = \boldsymbol{X}(\boldsymbol{Y}\boldsymbol{X}\boldsymbol{A})\boldsymbol{A} = (\boldsymbol{X}\boldsymbol{Y}\boldsymbol{X})(\boldsymbol{A}\boldsymbol{A}) = \boldsymbol{X}(\boldsymbol{Y}\boldsymbol{X}\boldsymbol{A})\boldsymbol{A} = \boldsymbol{X}\boldsymbol{Y}(\boldsymbol{X}\boldsymbol{A}\boldsymbol{A})
\end{equation}

继续利用对称性和弱逆性质，最终可以证明$\boldsymbol{X}=\boldsymbol{Y}$。证毕。$\square$

### 二、基于SVD的伪逆构造

前面我们通过特征值分解给出了伪逆的一个计算方法，但实际上更直接和常用的方法是基于奇异值分解（SVD）。

**定理2（SVD定理）**：任意矩阵$\boldsymbol{A}\in\mathbb{R}^{n\times r}$都可以进行奇异值分解：
\begin{equation}
\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}
\end{equation}
其中$\boldsymbol{U}\in\mathbb{R}^{n\times n}$和$\boldsymbol{V}\in\mathbb{R}^{r\times r}$都是正交矩阵，$\boldsymbol{\Sigma}\in\mathbb{R}^{n\times r}$是"广义对角矩阵"，它的形式为：
\begin{equation}
\boldsymbol{\Sigma} = \begin{pmatrix}
\sigma_1 & & & & \\\
& \sigma_2 & & & \\\
& & \ddots & & \\\
& & & \sigma_s & \\\
& & & & \boldsymbol{O}
\end{pmatrix}
\end{equation}
其中$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_s > 0$是$\boldsymbol{A}$的非零奇异值，$s=\text{rank}(\boldsymbol{A})$是$\boldsymbol{A}$的秩。

基于SVD，我们可以非常直接地构造伪逆：

**定理3（基于SVD的伪逆）**：如果$\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$是$\boldsymbol{A}$的SVD分解，那么
\begin{equation}
\boldsymbol{A}^{\dagger} = \boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top}
\end{equation}
其中$\boldsymbol{\Sigma}^{\dagger}$是$\boldsymbol{\Sigma}$的"伪逆"，定义为：
\begin{equation}
\boldsymbol{\Sigma}^{\dagger} = \begin{pmatrix}
\sigma_1^{-1} & & & & \\\
& \sigma_2^{-1} & & & \\\
& & \ddots & & \\\
& & & \sigma_s^{-1} & \\\
& & & & \boldsymbol{O}
\end{pmatrix}^{\top}
\end{equation}
注意$\boldsymbol{\Sigma}^{\dagger}\in\mathbb{R}^{r\times n}$，形状是$\boldsymbol{\Sigma}$的转置。

**证明**：我们需要验证$\boldsymbol{A}^{\dagger} = \boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top}$满足四个Penrose条件。

(MP1) $\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A} = \boldsymbol{A}$:
\begin{align}
\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A} &= (\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top})(\boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top})(\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top})\\\
&= \boldsymbol{U}\boldsymbol{\Sigma}(\boldsymbol{V}^{\top}\boldsymbol{V})\boldsymbol{\Sigma}^{\dagger}(\boldsymbol{U}^{\top}\boldsymbol{U})\boldsymbol{\Sigma}\boldsymbol{V}^{\top}\\\
&= \boldsymbol{U}(\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{\Sigma})\boldsymbol{V}^{\top}\\\
&= \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} = \boldsymbol{A}
\end{align}
最后一个等号用到了$\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{\Sigma} = \boldsymbol{\Sigma}$，这是因为对于非零的$\sigma_i$，有$\sigma_i \cdot \sigma_i^{-1} \cdot \sigma_i = \sigma_i$，而零元素保持不变。

(MP2) $\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger} = \boldsymbol{A}^{\dagger}$:
\begin{align}
\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger} &= (\boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top})(\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top})(\boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top})\\\
&= \boldsymbol{V}(\boldsymbol{\Sigma}^{\dagger}\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\dagger})\boldsymbol{U}^{\top}\\\
&= \boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top} = \boldsymbol{A}^{\dagger}
\end{align}

(MP3) $(\boldsymbol{A}\boldsymbol{A}^{\dagger})^{\top} = \boldsymbol{A}\boldsymbol{A}^{\dagger}$:
\begin{align}
\boldsymbol{A}\boldsymbol{A}^{\dagger} &= (\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top})(\boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top}) = \boldsymbol{U}(\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\dagger})\boldsymbol{U}^{\top}\\\
(\boldsymbol{A}\boldsymbol{A}^{\dagger})^{\top} &= (\boldsymbol{U}(\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\dagger})\boldsymbol{U}^{\top})^{\top} = \boldsymbol{U}(\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\dagger})^{\top}\boldsymbol{U}^{\top}\\\
&= \boldsymbol{U}(\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\dagger})\boldsymbol{U}^{\top} = \boldsymbol{A}\boldsymbol{A}^{\dagger}
\end{align}
这里用到了$\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{\dagger}$是对称矩阵（它是对角矩阵）。

(MP4) $(\boldsymbol{A}^{\dagger}\boldsymbol{A})^{\top} = \boldsymbol{A}^{\dagger}\boldsymbol{A}$:
\begin{align}
\boldsymbol{A}^{\dagger}\boldsymbol{A} &= (\boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top})(\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}) = \boldsymbol{V}(\boldsymbol{\Sigma}^{\dagger}\boldsymbol{\Sigma})\boldsymbol{V}^{\top}\\\
(\boldsymbol{A}^{\dagger}\boldsymbol{A})^{\top} &= \boldsymbol{V}(\boldsymbol{\Sigma}^{\dagger}\boldsymbol{\Sigma})^{\top}\boldsymbol{V}^{\top} = \boldsymbol{V}(\boldsymbol{\Sigma}^{\dagger}\boldsymbol{\Sigma})\boldsymbol{V}^{\top} = \boldsymbol{A}^{\dagger}\boldsymbol{A}
\end{align}

因此$\boldsymbol{A}^{\dagger} = \boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top}$满足所有四个Penrose条件。证毕。$\square$

**几何解释**：SVD分解$\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$可以理解为三个变换的复合：
1. $\boldsymbol{V}^{\top}$：在输入空间进行正交旋转
2. $\boldsymbol{\Sigma}$：沿各个方向进行缩放（伸缩）
3. $\boldsymbol{U}$：在输出空间进行正交旋转

伪逆$\boldsymbol{A}^{\dagger} = \boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top}$则是这个过程的"逆操作"：
1. $\boldsymbol{U}^{\top}$：在输出空间进行反向旋转
2. $\boldsymbol{\Sigma}^{\dagger}$：对非零方向进行反向缩放，对零方向保持为零
3. $\boldsymbol{V}$：在输入空间进行反向旋转

这个几何直观说明了为什么伪逆能够"尽可能"地反演原矩阵的作用。

### 三、最小二乘问题的解

伪逆的一个重要应用是求解最小二乘问题。考虑线性方程组：
\begin{equation}
\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}
\end{equation}
其中$\boldsymbol{A}\in\mathbb{R}^{n\times r}$，$\boldsymbol{x}\in\mathbb{R}^r$，$\boldsymbol{b}\in\mathbb{R}^n$。当$n > r$时（超定系统），方程通常无精确解。

**定理4（最小二乘解）**：最小二乘问题
\begin{equation}
\min_{\boldsymbol{x}} \Vert\boldsymbol{A}\boldsymbol{x} - \boldsymbol{b}\Vert_2^2
\end{equation}
的解为：
\begin{equation}
\boldsymbol{x}_{\text{LS}} = \boldsymbol{A}^{\dagger}\boldsymbol{b} + (\boldsymbol{I} - \boldsymbol{A}^{\dagger}\boldsymbol{A})\boldsymbol{z}
\end{equation}
其中$\boldsymbol{z}\in\mathbb{R}^r$是任意向量。换言之，最小二乘解的通解是$\boldsymbol{A}^{\dagger}\boldsymbol{b}$加上$\boldsymbol{A}$零空间中的任意向量。

**证明**：首先，我们证明$\boldsymbol{x}_{\text{LS}} = \boldsymbol{A}^{\dagger}\boldsymbol{b}$是一个最小二乘解。考虑残差：
\begin{align}
\boldsymbol{A}\boldsymbol{x}_{\text{LS}} - \boldsymbol{b} &= \boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{b} - \boldsymbol{b}\\\
&= (\boldsymbol{A}\boldsymbol{A}^{\dagger} - \boldsymbol{I})\boldsymbol{b}
\end{align}

由于$\boldsymbol{A}\boldsymbol{A}^{\dagger}$是到$\boldsymbol{A}$列空间$\mathcal{R}(\boldsymbol{A})$的正交投影，所以$(\boldsymbol{A}\boldsymbol{A}^{\dagger} - \boldsymbol{I})\boldsymbol{b}$是$\boldsymbol{b}$到$\mathcal{R}(\boldsymbol{A})$正交补空间的投影。这意味着残差与$\mathcal{R}(\boldsymbol{A})$正交，即对任意$\boldsymbol{y}\in\mathbb{R}^r$：
\begin{equation}
\langle \boldsymbol{A}\boldsymbol{y}, \boldsymbol{A}\boldsymbol{x}_{\text{LS}} - \boldsymbol{b} \rangle = 0
\end{equation}

现在考虑任意$\boldsymbol{x}\in\mathbb{R}^r$，我们有：
\begin{align}
\Vert\boldsymbol{A}\boldsymbol{x} - \boldsymbol{b}\Vert_2^2 &= \Vert(\boldsymbol{A}\boldsymbol{x} - \boldsymbol{A}\boldsymbol{x}_{\text{LS}}) + (\boldsymbol{A}\boldsymbol{x}_{\text{LS}} - \boldsymbol{b})\Vert_2^2\\\
&= \Vert\boldsymbol{A}(\boldsymbol{x} - \boldsymbol{x}_{\text{LS}})\Vert_2^2 + \Vert\boldsymbol{A}\boldsymbol{x}_{\text{LS}} - \boldsymbol{b}\Vert_2^2 + 2\langle\boldsymbol{A}(\boldsymbol{x} - \boldsymbol{x}_{\text{LS}}), \boldsymbol{A}\boldsymbol{x}_{\text{LS}} - \boldsymbol{b}\rangle\\\
&= \Vert\boldsymbol{A}(\boldsymbol{x} - \boldsymbol{x}_{\text{LS}})\Vert_2^2 + \Vert\boldsymbol{A}\boldsymbol{x}_{\text{LS}} - \boldsymbol{b}\Vert_2^2\\\
&\geq \Vert\boldsymbol{A}\boldsymbol{x}_{\text{LS}} - \boldsymbol{b}\Vert_2^2
\end{align}

等号成立当且仅当$\boldsymbol{A}(\boldsymbol{x} - \boldsymbol{x}_{\text{LS}}) = \boldsymbol{0}$，即$\boldsymbol{x} - \boldsymbol{x}_{\text{LS}} \in \text{Null}(\boldsymbol{A})$。因此，最小二乘解的通解为：
\begin{equation}
\boldsymbol{x} = \boldsymbol{A}^{\dagger}\boldsymbol{b} + \boldsymbol{z}, \quad \boldsymbol{A}\boldsymbol{z} = \boldsymbol{0}
\end{equation}

由于$\boldsymbol{I} - \boldsymbol{A}^{\dagger}\boldsymbol{A}$是到$\text{Null}(\boldsymbol{A})$的正交投影，所以对任意$\boldsymbol{z}$，$(\boldsymbol{I} - \boldsymbol{A}^{\dagger}\boldsymbol{A})\boldsymbol{z} \in \text{Null}(\boldsymbol{A})$。证毕。$\square$

### 四、最小范数解的唯一性

在最小二乘解的所有解中，哪一个具有最小的范数呢？

**定理5（最小范数解）**：在所有最小二乘解中，$\boldsymbol{x}_{\text{MN}} = \boldsymbol{A}^{\dagger}\boldsymbol{b}$具有最小的2-范数，即：
\begin{equation}
\boldsymbol{x}_{\text{MN}} = \mathop{\text{argmin}}_{\boldsymbol{x}} \left\\{ \Vert\boldsymbol{x}\Vert_2 \,:\, \Vert\boldsymbol{A}\boldsymbol{x} - \boldsymbol{b}\Vert_2 = \min_{\boldsymbol{y}} \Vert\boldsymbol{A}\boldsymbol{y} - \boldsymbol{b}\Vert_2 \right\\}
\end{equation}

**证明**：设$\boldsymbol{x}$是任意最小二乘解，则$\boldsymbol{x} = \boldsymbol{A}^{\dagger}\boldsymbol{b} + \boldsymbol{z}$，其中$\boldsymbol{z} \in \text{Null}(\boldsymbol{A})$。

关键观察：$\boldsymbol{A}^{\dagger}\boldsymbol{b} \in \mathcal{R}(\boldsymbol{A}^{\dagger}) = \mathcal{R}(\boldsymbol{A}^{\top})$，而$\boldsymbol{z} \in \text{Null}(\boldsymbol{A})$。由线性代数的基本定理，$\mathcal{R}(\boldsymbol{A}^{\top})$和$\text{Null}(\boldsymbol{A})$是正交补空间，因此：
\begin{equation}
\langle \boldsymbol{A}^{\dagger}\boldsymbol{b}, \boldsymbol{z} \rangle = 0
\end{equation}

利用勾股定理：
\begin{align}
\Vert\boldsymbol{x}\Vert_2^2 &= \Vert\boldsymbol{A}^{\dagger}\boldsymbol{b} + \boldsymbol{z}\Vert_2^2\\\
&= \Vert\boldsymbol{A}^{\dagger}\boldsymbol{b}\Vert_2^2 + \Vert\boldsymbol{z}\Vert_2^2 + 2\langle \boldsymbol{A}^{\dagger}\boldsymbol{b}, \boldsymbol{z} \rangle\\\
&= \Vert\boldsymbol{A}^{\dagger}\boldsymbol{b}\Vert_2^2 + \Vert\boldsymbol{z}\Vert_2^2\\\
&\geq \Vert\boldsymbol{A}^{\dagger}\boldsymbol{b}\Vert_2^2
\end{align}

等号成立当且仅当$\boldsymbol{z} = \boldsymbol{0}$，即$\boldsymbol{x} = \boldsymbol{A}^{\dagger}\boldsymbol{b}$。证毕。$\square$

**推论**：对于线性方程组$\boldsymbol{A}\boldsymbol{x} = \boldsymbol{b}$：
- 如果方程有精确解，则$\boldsymbol{A}^{\dagger}\boldsymbol{b}$是范数最小的精确解
- 如果方程无精确解，则$\boldsymbol{A}^{\dagger}\boldsymbol{b}$是范数最小的最小二乘解

这个性质使得$\boldsymbol{A}^{\dagger}\boldsymbol{b}$成为求解线性方程组的一个自然选择。

### 五、伪逆的性质

伪逆具有许多重要的性质，这些性质在理论分析和实际应用中都非常有用。下面我们详细证明15个重要性质。

**性质1**：$(\boldsymbol{A}^{\dagger})^{\dagger} = \boldsymbol{A}$

**证明**：需要验证$\boldsymbol{A}$满足$\boldsymbol{A}^{\dagger}$的四个Penrose条件。

(MP1') $\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger} = \boldsymbol{A}^{\dagger}$：这就是$\boldsymbol{A}$的(MP2)。

(MP2') $\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A} = \boldsymbol{A}$：这就是$\boldsymbol{A}$的(MP1)。

(MP3') $(\boldsymbol{A}^{\dagger}\boldsymbol{A})^{\top} = \boldsymbol{A}^{\dagger}\boldsymbol{A}$：这就是$\boldsymbol{A}$的(MP4)。

(MP4') $(\boldsymbol{A}\boldsymbol{A}^{\dagger})^{\top} = \boldsymbol{A}\boldsymbol{A}^{\dagger}$：这就是$\boldsymbol{A}$的(MP3)。

因此$(\boldsymbol{A}^{\dagger})^{\dagger} = \boldsymbol{A}$。$\square$

**性质2**：$(\boldsymbol{A}^{\top})^{\dagger} = (\boldsymbol{A}^{\dagger})^{\top}$

**证明**：设$\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，则：
\begin{align}
\boldsymbol{A}^{\top} &= \boldsymbol{V}\boldsymbol{\Sigma}^{\top}\boldsymbol{U}^{\top}\\\
(\boldsymbol{A}^{\top})^{\dagger} &= \boldsymbol{U}(\boldsymbol{\Sigma}^{\top})^{\dagger}\boldsymbol{V}^{\top} = \boldsymbol{U}(\boldsymbol{\Sigma}^{\dagger})^{\top}\boldsymbol{V}^{\top}\\\
(\boldsymbol{A}^{\dagger})^{\top} &= (\boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top})^{\top} = \boldsymbol{U}(\boldsymbol{\Sigma}^{\dagger})^{\top}\boldsymbol{V}^{\top}
\end{align}
因此$(\boldsymbol{A}^{\top})^{\dagger} = (\boldsymbol{A}^{\dagger})^{\top}$。$\square$

**性质3**：如果$\boldsymbol{A}$可逆，则$\boldsymbol{A}^{\dagger} = \boldsymbol{A}^{-1}$

**证明**：对于可逆矩阵$\boldsymbol{A}$，验证$\boldsymbol{A}^{-1}$满足四个Penrose条件：
\begin{align}
\boldsymbol{A}\boldsymbol{A}^{-1}\boldsymbol{A} &= \boldsymbol{A}\\\
\boldsymbol{A}^{-1}\boldsymbol{A}\boldsymbol{A}^{-1} &= \boldsymbol{A}^{-1}\\\
(\boldsymbol{A}\boldsymbol{A}^{-1})^{\top} &= \boldsymbol{I}^{\top} = \boldsymbol{I} = \boldsymbol{A}\boldsymbol{A}^{-1}\\\
(\boldsymbol{A}^{-1}\boldsymbol{A})^{\top} &= \boldsymbol{I}^{\top} = \boldsymbol{I} = \boldsymbol{A}^{-1}\boldsymbol{A}
\end{align}
因此$\boldsymbol{A}^{\dagger} = \boldsymbol{A}^{-1}$。$\square$

**性质4**：$(\alpha\boldsymbol{A})^{\dagger} = \alpha^{-1}\boldsymbol{A}^{\dagger}$（$\alpha \neq 0$）

**证明**：设$\boldsymbol{B} = \alpha\boldsymbol{A}$，验证$\alpha^{-1}\boldsymbol{A}^{\dagger}$满足$\boldsymbol{B}$的Penrose条件：
\begin{align}
\boldsymbol{B}(\alpha^{-1}\boldsymbol{A}^{\dagger})\boldsymbol{B} &= (\alpha\boldsymbol{A})(\alpha^{-1}\boldsymbol{A}^{\dagger})(\alpha\boldsymbol{A}) = \alpha\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A} = \alpha\boldsymbol{A} = \boldsymbol{B}\\\
(\alpha^{-1}\boldsymbol{A}^{\dagger})\boldsymbol{B}(\alpha^{-1}\boldsymbol{A}^{\dagger}) &= (\alpha^{-1}\boldsymbol{A}^{\dagger})(\alpha\boldsymbol{A})(\alpha^{-1}\boldsymbol{A}^{\dagger}) = \alpha^{-1}\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger} = \alpha^{-1}\boldsymbol{A}^{\dagger}\\\
[\boldsymbol{B}(\alpha^{-1}\boldsymbol{A}^{\dagger})]^{\top} &= [\boldsymbol{A}\boldsymbol{A}^{\dagger}]^{\top} = \boldsymbol{A}\boldsymbol{A}^{\dagger} = \boldsymbol{B}(\alpha^{-1}\boldsymbol{A}^{\dagger})\\\
[(\alpha^{-1}\boldsymbol{A}^{\dagger})\boldsymbol{B}]^{\top} &= [\boldsymbol{A}^{\dagger}\boldsymbol{A}]^{\top} = \boldsymbol{A}^{\dagger}\boldsymbol{A} = (\alpha^{-1}\boldsymbol{A}^{\dagger})\boldsymbol{B}
\end{align}
因此$(\alpha\boldsymbol{A})^{\dagger} = \alpha^{-1}\boldsymbol{A}^{\dagger}$。$\square$

**性质5**：$\boldsymbol{A}\boldsymbol{A}^{\dagger}$和$\boldsymbol{A}^{\dagger}\boldsymbol{A}$都是幂等矩阵（投影矩阵）

**证明**：
\begin{align}
(\boldsymbol{A}\boldsymbol{A}^{\dagger})^2 &= \boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger} = \boldsymbol{A}(\boldsymbol{A}^{\dagger}\boldsymbol{A})\boldsymbol{A}^{\dagger} = \boldsymbol{A}\boldsymbol{A}^{\dagger}\\\
(\boldsymbol{A}^{\dagger}\boldsymbol{A})^2 &= \boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A} = \boldsymbol{A}^{\dagger}(\boldsymbol{A}\boldsymbol{A}^{\dagger})\boldsymbol{A} = \boldsymbol{A}^{\dagger}\boldsymbol{A}
\end{align}
结合(MP3)和(MP4)，它们也是对称矩阵，因此是正交投影矩阵。$\square$

**性质6**：$\mathcal{R}(\boldsymbol{A}\boldsymbol{A}^{\dagger}) = \mathcal{R}(\boldsymbol{A})$，$\mathcal{R}(\boldsymbol{A}^{\dagger}\boldsymbol{A}) = \mathcal{R}(\boldsymbol{A}^{\top})$

**证明**：显然$\mathcal{R}(\boldsymbol{A}\boldsymbol{A}^{\dagger}) \subseteq \mathcal{R}(\boldsymbol{A})$。反过来，对任意$\boldsymbol{y} \in \mathcal{R}(\boldsymbol{A})$，存在$\boldsymbol{x}$使得$\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x}$，则：
\begin{equation}
\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x} = \boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{x} = \boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{y} \in \mathcal{R}(\boldsymbol{A}\boldsymbol{A}^{\dagger})
\end{equation}
因此$\mathcal{R}(\boldsymbol{A}) \subseteq \mathcal{R}(\boldsymbol{A}\boldsymbol{A}^{\dagger})$，从而$\mathcal{R}(\boldsymbol{A}\boldsymbol{A}^{\dagger}) = \mathcal{R}(\boldsymbol{A})$。

类似地可证$\mathcal{R}(\boldsymbol{A}^{\dagger}\boldsymbol{A}) = \mathcal{R}(\boldsymbol{A}^{\top})$。$\square$

**性质7**：$\text{Null}(\boldsymbol{A}\boldsymbol{A}^{\dagger}) = \text{Null}(\boldsymbol{A}^{\top})$，$\text{Null}(\boldsymbol{A}^{\dagger}\boldsymbol{A}) = \text{Null}(\boldsymbol{A})$

**证明**：由于$\boldsymbol{A}\boldsymbol{A}^{\dagger}$是正交投影到$\mathcal{R}(\boldsymbol{A})$，它的零空间就是$\mathcal{R}(\boldsymbol{A})$的正交补，即$\text{Null}(\boldsymbol{A}^{\top})$。

对于第二个等式，若$\boldsymbol{A}\boldsymbol{x} = \boldsymbol{0}$，则：
\begin{equation}
\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{x} = \boldsymbol{A}^{\dagger}(\boldsymbol{A}\boldsymbol{x}) = \boldsymbol{A}^{\dagger}\boldsymbol{0} = \boldsymbol{0}
\end{equation}
因此$\text{Null}(\boldsymbol{A}) \subseteq \text{Null}(\boldsymbol{A}^{\dagger}\boldsymbol{A})$。反过来，若$\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{x} = \boldsymbol{0}$，则：
\begin{equation}
\boldsymbol{A}\boldsymbol{x} = \boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{x} = \boldsymbol{A}(\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{x}) = \boldsymbol{A}\boldsymbol{0} = \boldsymbol{0}
\end{equation}
因此$\text{Null}(\boldsymbol{A}^{\dagger}\boldsymbol{A}) \subseteq \text{Null}(\boldsymbol{A})$，从而两者相等。$\square$

**性质8**：$\text{rank}(\boldsymbol{A}^{\dagger}) = \text{rank}(\boldsymbol{A})$

**证明**：由SVD，$\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，$\boldsymbol{A}^{\dagger} = \boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top}$。由于正交矩阵不改变秩，所以：
\begin{equation}
\text{rank}(\boldsymbol{A}) = \text{rank}(\boldsymbol{\Sigma}) = s, \quad \text{rank}(\boldsymbol{A}^{\dagger}) = \text{rank}(\boldsymbol{\Sigma}^{\dagger}) = s
\end{equation}
因此$\text{rank}(\boldsymbol{A}^{\dagger}) = \text{rank}(\boldsymbol{A})$。$\square$

**性质9**：对正交矩阵$\boldsymbol{U}$和$\boldsymbol{V}$，$(\boldsymbol{U}\boldsymbol{A}\boldsymbol{V})^{\dagger} = \boldsymbol{V}^{\top}\boldsymbol{A}^{\dagger}\boldsymbol{U}^{\top}$

**证明**：验证$\boldsymbol{V}^{\top}\boldsymbol{A}^{\dagger}\boldsymbol{U}^{\top}$满足$\boldsymbol{U}\boldsymbol{A}\boldsymbol{V}$的四个Penrose条件：
\begin{align}
(\boldsymbol{U}\boldsymbol{A}\boldsymbol{V})(\boldsymbol{V}^{\top}\boldsymbol{A}^{\dagger}\boldsymbol{U}^{\top})(\boldsymbol{U}\boldsymbol{A}\boldsymbol{V}) &= \boldsymbol{U}\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{V} = \boldsymbol{U}\boldsymbol{A}\boldsymbol{V}\\\
(\boldsymbol{V}^{\top}\boldsymbol{A}^{\dagger}\boldsymbol{U}^{\top})(\boldsymbol{U}\boldsymbol{A}\boldsymbol{V})(\boldsymbol{V}^{\top}\boldsymbol{A}^{\dagger}\boldsymbol{U}^{\top}) &= \boldsymbol{V}^{\top}\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{U}^{\top} = \boldsymbol{V}^{\top}\boldsymbol{A}^{\dagger}\boldsymbol{U}^{\top}
\end{align}
对称性也容易验证。$\square$

**性质10**：$(\boldsymbol{A}^{\dagger}\boldsymbol{A})^k = \boldsymbol{A}^{\dagger}\boldsymbol{A}$，$(\boldsymbol{A}\boldsymbol{A}^{\dagger})^k = \boldsymbol{A}\boldsymbol{A}^{\dagger}$（$k \geq 1$）

**证明**：由性质5，$\boldsymbol{A}^{\dagger}\boldsymbol{A}$和$\boldsymbol{A}\boldsymbol{A}^{\dagger}$都是幂等矩阵，因此任意正整数次幂都等于自身。$\square$

**性质11**：$\Vert\boldsymbol{A}^{\dagger}\Vert_2 = \sigma_1^{-1}$（其中$\sigma_1$是$\boldsymbol{A}$的最大非零奇异值）

**证明**：矩阵的2-范数（谱范数）等于最大奇异值。对于$\boldsymbol{A}^{\dagger} = \boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top}$，$\boldsymbol{\Sigma}^{\dagger}$的最大奇异值是$\sigma_1^{-1}$，因此：
\begin{equation}
\Vert\boldsymbol{A}^{\dagger}\Vert_2 = \Vert\boldsymbol{\Sigma}^{\dagger}\Vert_2 = \sigma_1^{-1}
\end{equation}
$\square$

**性质12**：$\Vert\boldsymbol{A}^{\dagger}\Vert_F = \sqrt{\sum_{i=1}^s \sigma_i^{-2}}$

**证明**：Frobenius范数的平方等于所有奇异值平方和，因此：
\begin{equation}
\Vert\boldsymbol{A}^{\dagger}\Vert_F^2 = \sum_{i=1}^s \sigma_i^{-2}
\end{equation}
$\square$

**性质13**：$\boldsymbol{A}^{\dagger}\boldsymbol{A}\boldsymbol{x} = \boldsymbol{x}$当且仅当$\boldsymbol{x} \in \mathcal{R}(\boldsymbol{A}^{\top})$

**证明**：由性质6，$\boldsymbol{A}^{\dagger}\boldsymbol{A}$是到$\mathcal{R}(\boldsymbol{A}^{\top})$的正交投影。投影结果等于自身当且仅当向量本身在投影子空间中。$\square$

**性质14**：$\boldsymbol{A}\boldsymbol{A}^{\dagger}\boldsymbol{y} = \boldsymbol{y}$当且仅当$\boldsymbol{y} \in \mathcal{R}(\boldsymbol{A})$

**证明**：类似性质13。$\square$

**性质15**：如果$\boldsymbol{A}$有满秩列（$\text{rank}(\boldsymbol{A}) = r$），则$\boldsymbol{A}^{\dagger} = (\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}$

**证明**：当$\boldsymbol{A}$列满秩时，$\boldsymbol{A}^{\top}\boldsymbol{A}$可逆。验证$(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}$满足四个条件：
\begin{align}
\boldsymbol{A}[(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}]\boldsymbol{A} &= \boldsymbol{A}(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}\boldsymbol{A} = \boldsymbol{A}\\\
[(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}]\boldsymbol{A}[(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}] &= (\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}\\\
[\boldsymbol{A}(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}]^{\top} &= \boldsymbol{A}[(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}]^{\top}\boldsymbol{A}^{\top} = \boldsymbol{A}(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}\\\
[(\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}\boldsymbol{A}]^{\top} &= \boldsymbol{I}^{\top} = \boldsymbol{I} = (\boldsymbol{A}^{\top}\boldsymbol{A})^{-1}\boldsymbol{A}^{\top}\boldsymbol{A}
\end{align}
$\square$

### 六、秩k截断SVD的最优性（Eckart-Young-Mirsky定理）

伪逆与低秩近似有着深刻的联系。Eckart-Young-Mirsky定理是矩阵低秩近似领域最重要的结果之一。

**定理6（Eckart-Young-Mirsky定理）**：设$\boldsymbol{A}\in\mathbb{R}^{n\times r}$有SVD分解$\boldsymbol{A} = \sum_{i=1}^s \sigma_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top}$（其中$s = \text{rank}(\boldsymbol{A})$），对于$k < s$，定义秩$k$截断SVD：
\begin{equation}
\boldsymbol{A}_k = \sum_{i=1}^k \sigma_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top} = \boldsymbol{U}_k\boldsymbol{\Sigma}_k\boldsymbol{V}_k^{\top}
\end{equation}
其中$\boldsymbol{U}_k = [\boldsymbol{u}_1, \cdots, \boldsymbol{u}_k]$，$\boldsymbol{V}_k = [\boldsymbol{v}_1, \cdots, \boldsymbol{v}_k]$，$\boldsymbol{\Sigma}_k = \text{diag}(\sigma_1, \cdots, \sigma_k)$。

则$\boldsymbol{A}_k$是$\boldsymbol{A}$的最佳秩$k$近似，在Frobenius范数和谱范数下都成立：
\begin{align}
\boldsymbol{A}_k &= \mathop{\text{argmin}}_{\text{rank}(\boldsymbol{B})\leq k} \Vert\boldsymbol{A} - \boldsymbol{B}\Vert_F\\\
\boldsymbol{A}_k &= \mathop{\text{argmin}}_{\text{rank}(\boldsymbol{B})\leq k} \Vert\boldsymbol{A} - \boldsymbol{B}\Vert_2
\end{align}

且误差为：
\begin{align}
\Vert\boldsymbol{A} - \boldsymbol{A}_k\Vert_F &= \sqrt{\sum_{i=k+1}^s \sigma_i^2}\\\
\Vert\boldsymbol{A} - \boldsymbol{A}_k\Vert_2 &= \sigma_{k+1}
\end{align}

**证明（Frobenius范数）**：

首先计算误差。由于$\boldsymbol{A} - \boldsymbol{A}_k = \sum_{i=k+1}^s \sigma_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top}$，而$\\{\boldsymbol{u}_i\\}$和$\\{\boldsymbol{v}_i\\}$都是正交系统，所以：
\begin{equation}
\Vert\boldsymbol{A} - \boldsymbol{A}_k\Vert_F^2 = \text{Tr}[(\boldsymbol{A} - \boldsymbol{A}_k)^{\top}(\boldsymbol{A} - \boldsymbol{A}_k)] = \sum_{i=k+1}^s \sigma_i^2
\end{equation}

现在证明最优性。设$\boldsymbol{B}$是任意秩不超过$k$的矩阵。考虑$\boldsymbol{V}_k^{\top}$右乘$\boldsymbol{B}$的列空间（维数$\leq k$）和$\boldsymbol{V}_{s-k}$张成的子空间（维数$s-k$），其中$\boldsymbol{V}_{s-k} = [\boldsymbol{v}_{k+1}, \cdots, \boldsymbol{v}_s]$。

由维数公式，这两个子空间的交集至少是1维的，因此存在单位向量$\boldsymbol{z} = \sum_{i=k+1}^s c_i \boldsymbol{v}_i$（$\Vert\boldsymbol{z}\Vert_2 = 1$）使得$\boldsymbol{B}\boldsymbol{z} = \boldsymbol{0}$（因为$\boldsymbol{z}$在$\boldsymbol{B}$的零空间中）。

现在计算：
\begin{align}
\Vert\boldsymbol{A} - \boldsymbol{B}\Vert_F^2 &\geq \Vert(\boldsymbol{A} - \boldsymbol{B})\boldsymbol{z}\Vert_2^2\\\
&= \Vert\boldsymbol{A}\boldsymbol{z}\Vert_2^2\\\
&= \Big\Vert\sum_{i=1}^s \sigma_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top}\boldsymbol{z}\Big\Vert_2^2\\\
&= \Big\Vert\sum_{i=k+1}^s \sigma_i c_i \boldsymbol{u}_i\Big\Vert_2^2\\\
&= \sum_{i=k+1}^s \sigma_i^2 c_i^2\\\
&\geq \sigma_{k+1}^2 \sum_{i=k+1}^s c_i^2\\\
&= \sigma_{k+1}^2
\end{align}

但我们需要证明$\Vert\boldsymbol{A} - \boldsymbol{B}\Vert_F^2 \geq \sum_{i=k+1}^s \sigma_i^2$。这需要更精细的论证。

完整证明如下：设$\boldsymbol{B}$是任意秩$\leq k$的矩阵，则$\text{Null}(\boldsymbol{B})$的维数$\geq r - k$。考虑子空间$\mathcal{V} = \text{span}\\{\boldsymbol{v}_{k+1}, \cdots, \boldsymbol{v}_r\\}$（维数$= r-k$）。由维数公式，$\mathcal{V} \cap \text{Null}(\boldsymbol{B})$至少是1维的。

取$\mathcal{V} \cap \text{Null}(\boldsymbol{B})$的一组标准正交基$\\{\boldsymbol{w}_1, \cdots, \boldsymbol{w}_m\\}$，其中$m \geq 1$。每个$\boldsymbol{w}_j$可以写成：
\begin{equation}
\boldsymbol{w}_j = \sum_{i=k+1}^r c_{ij} \boldsymbol{v}_i
\end{equation}

由于$\boldsymbol{B}\boldsymbol{w}_j = \boldsymbol{0}$，我们有：
\begin{align}
\Vert\boldsymbol{A} - \boldsymbol{B}\Vert_F^2 &\geq \sum_{j=1}^m \Vert(\boldsymbol{A} - \boldsymbol{B})\boldsymbol{w}_j\Vert_2^2\\\
&= \sum_{j=1}^m \Vert\boldsymbol{A}\boldsymbol{w}_j\Vert_2^2\\\
&= \sum_{j=1}^m \Big\Vert\sum_{i=k+1}^r \sigma_i c_{ij} \boldsymbol{u}_i\Big\Vert_2^2\\\
&= \sum_{j=1}^m \sum_{i=k+1}^r \sigma_i^2 c_{ij}^2
\end{align}

由于$\\{\boldsymbol{w}_j\\}$是标准正交系，矩阵$\boldsymbol{C} = [c_{ij}]_{(r-k)\times m}$满足$\boldsymbol{C}\boldsymbol{C}^{\top} = \boldsymbol{I}_m$（单位阵）。考虑优化问题：
\begin{equation}
\min_{\boldsymbol{C}: \boldsymbol{C}\boldsymbol{C}^{\top}=\boldsymbol{I}_m} \sum_{j=1}^m \sum_{i=k+1}^r \sigma_i^2 c_{ij}^2 = \min_{\boldsymbol{C}: \boldsymbol{C}\boldsymbol{C}^{\top}=\boldsymbol{I}_m} \text{Tr}(\boldsymbol{D}\boldsymbol{C}\boldsymbol{C}^{\top})
\end{equation}
其中$\boldsymbol{D} = \text{diag}(\sigma_{k+1}^2, \cdots, \sigma_r^2)$。

由迹的性质，$\text{Tr}(\boldsymbol{D}\boldsymbol{C}\boldsymbol{C}^{\top}) = \text{Tr}(\boldsymbol{C}^{\top}\boldsymbol{D}\boldsymbol{C})$，而$\boldsymbol{C}^{\top}\boldsymbol{D}\boldsymbol{C}$是$m\times m$半正定矩阵。其迹的最小值在$\boldsymbol{C}$选取$\boldsymbol{D}$最小的$m$个特征值对应的特征向量时达到，即$\sum_{i=k+1}^{k+m} \sigma_i^2$。

但这个论证还不够严密。让我们用更直接的方法：

利用正交不变性，我们有：
\begin{align}
\Vert\boldsymbol{A} - \boldsymbol{B}\Vert_F^2 &= \Vert\boldsymbol{U}^{\top}(\boldsymbol{A} - \boldsymbol{B})\boldsymbol{V}\Vert_F^2\\\
&= \Vert\boldsymbol{\Sigma} - \boldsymbol{U}^{\top}\boldsymbol{B}\boldsymbol{V}\Vert_F^2
\end{align}

设$\tilde{\boldsymbol{B}} = \boldsymbol{U}^{\top}\boldsymbol{B}\boldsymbol{V}$，则$\text{rank}(\tilde{\boldsymbol{B}}) = \text{rank}(\boldsymbol{B}) \leq k$。问题变为：
\begin{equation}
\min_{\text{rank}(\tilde{\boldsymbol{B}})\leq k} \Vert\boldsymbol{\Sigma} - \tilde{\boldsymbol{B}}\Vert_F^2
\end{equation}

将$\tilde{\boldsymbol{B}} = [\tilde{B}_{ij}]$展开：
\begin{align}
\Vert\boldsymbol{\Sigma} - \tilde{\boldsymbol{B}}\Vert_F^2 &= \sum_{i,j} (\Sigma_{ij} - \tilde{B}_{ij})^2\\\
&= \sum_{i=1}^s (\sigma_i - \tilde{B}_{ii})^2 + \sum_{i\neq j}\tilde{B}_{ij}^2
\end{align}

对于对角线上的元素，由于$\text{rank}(\tilde{\boldsymbol{B}}) \leq k$，至多有$k$个主对角元可以非零。为了最小化误差，应该让最大的$k$个$\sigma_i$对应的位置$\tilde{B}_{ii} = \sigma_i$，其余为0。此时：
\begin{equation}
\Vert\boldsymbol{\Sigma} - \tilde{\boldsymbol{B}}\Vert_F^2 \geq \sum_{i=k+1}^s \sigma_i^2
\end{equation}

等号当$\tilde{\boldsymbol{B}}$是对角矩阵且前$k$个对角元为$\sigma_1, \cdots, \sigma_k$时达到，对应$\boldsymbol{B} = \boldsymbol{A}_k$。证毕。$\square$

**证明（谱范数）**：

谱范数（2-范数）等于最大奇异值。我们有：
\begin{equation}
\boldsymbol{A} - \boldsymbol{A}_k = \sum_{i=k+1}^s \sigma_i \boldsymbol{u}_i\boldsymbol{v}_i^{\top}
\end{equation}

因此$\Vert\boldsymbol{A} - \boldsymbol{A}_k\Vert_2 = \sigma_{k+1}$。

对于任意秩$\leq k$的矩阵$\boldsymbol{B}$，考虑向量$\boldsymbol{v}_{k+1}$（第$k+1$个右奇异向量）。由于$\text{Null}(\boldsymbol{B})$维数$\geq r-k$，而$\text{span}\\{\boldsymbol{v}_{k+1}, \cdots, \boldsymbol{v}_r\\}$维数为$r-k$或$s-k$，它们的交集至少是1维的。

不失一般性，设$\boldsymbol{v}_{k+1} \in \text{Null}(\boldsymbol{B})$（否则在交集中取单位向量）。则：
\begin{align}
\Vert\boldsymbol{A} - \boldsymbol{B}\Vert_2 &\geq \Vert(\boldsymbol{A} - \boldsymbol{B})\boldsymbol{v}_{k+1}\Vert_2\\\
&= \Vert\boldsymbol{A}\boldsymbol{v}_{k+1}\Vert_2\\\
&= \Vert\sigma_{k+1}\boldsymbol{u}_{k+1}\Vert_2\\\
&= \sigma_{k+1}
\end{align}

因此$\boldsymbol{A}_k$也是谱范数下的最优秩$k$近似。证毕。$\square$

### 七、Frobenius范数和谱范数下的最优性分析

Eckart-Young定理告诉我们截断SVD在两种范数下都是最优的，但这两种范数有着不同的性质和应用场景。

**Frobenius范数的特点**：

Frobenius范数$\Vert\boldsymbol{A}\Vert_F = \sqrt{\sum_{ij} A_{ij}^2}$是"元素级"的范数，它衡量所有元素的总体偏差。其优点是：

1. **可加性**：$\Vert\boldsymbol{A} + \boldsymbol{B}\Vert_F \leq \Vert\boldsymbol{A}\Vert_F + \Vert\boldsymbol{B}\Vert_F$
2. **酉不变性**：$\Vert\boldsymbol{U}\boldsymbol{A}\boldsymbol{V}\Vert_F = \Vert\boldsymbol{A}\Vert_F$（$\boldsymbol{U}, \boldsymbol{V}$正交）
3. **与奇异值的关系**：$\Vert\boldsymbol{A}\Vert_F^2 = \sum_{i=1}^s \sigma_i^2$

在Frobenius范数下，低秩近似误差为：
\begin{equation}
\Vert\boldsymbol{A} - \boldsymbol{A}_k\Vert_F = \sqrt{\sum_{i=k+1}^s \sigma_i^2}
\end{equation}

这表明误差由被截断的奇异值决定，且是它们的平方和的平方根。这意味着如果后面的奇异值快速衰减，则低秩近似效果很好。

**相对误差**：定义相对误差为：
\begin{equation}
\epsilon_{\text{rel},F} = \frac{\Vert\boldsymbol{A} - \boldsymbol{A}_k\Vert_F}{\Vert\boldsymbol{A}\Vert_F} = \sqrt{\frac{\sum_{i=k+1}^s \sigma_i^2}{\sum_{i=1}^s \sigma_i^2}}
\end{equation}

**谱范数的特点**：

谱范数$\Vert\boldsymbol{A}\Vert_2 = \max_{\Vert\boldsymbol{x}\Vert_2=1} \Vert\boldsymbol{A}\boldsymbol{x}\Vert_2$是"算子范数"，它衡量矩阵作为线性算子的最大拉伸因子。其特点是：

1. **次可乘性**：$\Vert\boldsymbol{A}\boldsymbol{B}\Vert_2 \leq \Vert\boldsymbol{A}\Vert_2 \Vert\boldsymbol{B}\Vert_2$
2. **与最大奇异值的关系**：$\Vert\boldsymbol{A}\Vert_2 = \sigma_1$
3. **对向量的控制**：$\Vert\boldsymbol{A}\boldsymbol{x}\Vert_2 \leq \Vert\boldsymbol{A}\Vert_2 \Vert\boldsymbol{x}\Vert_2$

在谱范数下，低秩近似误差为：
\begin{equation}
\Vert\boldsymbol{A} - \boldsymbol{A}_k\Vert_2 = \sigma_{k+1}
\end{equation}

这仅取决于第$k+1$个奇异值，忽略了更小的奇异值的贡献。

**相对误差**：
\begin{equation}
\epsilon_{\text{rel},2} = \frac{\Vert\boldsymbol{A} - \boldsymbol{A}_k\Vert_2}{\Vert\boldsymbol{A}\Vert_2} = \frac{\sigma_{k+1}}{\sigma_1}
\end{equation}

**两种范数的关系**：

对于$n\times r$矩阵$\boldsymbol{A}$，有：
\begin{equation}
\Vert\boldsymbol{A}\Vert_2 \leq \Vert\boldsymbol{A}\Vert_F \leq \sqrt{\min(n,r)} \Vert\boldsymbol{A}\Vert_2
\end{equation}

因此：
\begin{equation}
\epsilon_{\text{rel},2} \leq \epsilon_{\text{rel},F} \leq \sqrt{\frac{s-k}{k}} \epsilon_{\text{rel},2}
\end{equation}

这表明谱范数相对误差总是不大于Frobenius范数相对误差。

### 八、误差界和扰动分析

在实际应用中，我们通常处理的是带有噪声的数据。理解伪逆在扰动下的行为非常重要。

**定理7（伪逆的扰动界）**：设$\boldsymbol{A}$和$\tilde{\boldsymbol{A}} = \boldsymbol{A} + \boldsymbol{E}$都有秩$s$，且$\boldsymbol{E}$是小扰动。若$\Vert\boldsymbol{E}\Vert_2 < \sigma_s$（$\boldsymbol{A}$的最小非零奇异值），则：
\begin{equation}
\Vert\tilde{\boldsymbol{A}}^{\dagger} - \boldsymbol{A}^{\dagger}\Vert_2 \leq \frac{2\Vert\boldsymbol{A}^{\dagger}\Vert_2^2 \Vert\boldsymbol{E}\Vert_2}{1 - \Vert\boldsymbol{A}^{\dagger}\Vert_2\Vert\boldsymbol{E}\Vert_2}
\end{equation}

**证明（概要）**：利用Neumann级数展开和三角不等式。由于$\tilde{\boldsymbol{A}} = \boldsymbol{A}(\boldsymbol{I} + \boldsymbol{A}^{\dagger}\boldsymbol{E})$，在扰动足够小时：
\begin{equation}
\tilde{\boldsymbol{A}}^{\dagger} \approx (\boldsymbol{I} + \boldsymbol{A}^{\dagger}\boldsymbol{E})^{-1}\boldsymbol{A}^{\dagger} \approx (\boldsymbol{I} - \boldsymbol{A}^{\dagger}\boldsymbol{E})\boldsymbol{A}^{\dagger}
\end{equation}

精确展开需要考虑更高阶项，最终得到上述误差界。$\square$

**条件数**：伪逆的条件数定义为：
\begin{equation}
\kappa(\boldsymbol{A}) = \Vert\boldsymbol{A}\Vert_2 \Vert\boldsymbol{A}^{\dagger}\Vert_2 = \frac{\sigma_1}{\sigma_s}
\end{equation}

这是$\boldsymbol{A}$的最大和最小非零奇异值之比，称为**条件数**。条件数越大，矩阵越"病态"，伪逆越不稳定。

**最小二乘问题的误差界**：对于最小二乘问题$\min_{\boldsymbol{x}} \Vert\boldsymbol{A}\boldsymbol{x} - \boldsymbol{b}\Vert_2$，如果数据$\boldsymbol{b}$有扰动$\delta\boldsymbol{b}$，则解的扰动为：
\begin{equation}
\boldsymbol{x} + \delta\boldsymbol{x} = \boldsymbol{A}^{\dagger}(\boldsymbol{b} + \delta\boldsymbol{b})
\end{equation}

因此：
\begin{equation}
\delta\boldsymbol{x} = \boldsymbol{A}^{\dagger}\delta\boldsymbol{b}
\end{equation}

相对误差为：
\begin{equation}
\frac{\Vert\delta\boldsymbol{x}\Vert_2}{\Vert\boldsymbol{x}\Vert_2} \leq \kappa(\boldsymbol{A}) \frac{\Vert\delta\boldsymbol{b}\Vert_2}{\Vert\boldsymbol{b}\Vert_2}
\end{equation}

这表明解的相对误差被条件数放大。

**定理8（Wedin定理）**：设$\boldsymbol{A}$和$\tilde{\boldsymbol{A}}$的秩$k$截断SVD分别为$\boldsymbol{A}_k$和$\tilde{\boldsymbol{A}}_k$。若$\sigma_k - \sigma_{k+1} \geq \delta > 0$（存在间隙），则：
\begin{equation}
\Vert\boldsymbol{A}_k - \tilde{\boldsymbol{A}}_k\Vert_F \leq \frac{2\sqrt{2}}{\delta} \Vert\boldsymbol{A} - \tilde{\boldsymbol{A}}\Vert_F + O(\Vert\boldsymbol{A} - \tilde{\boldsymbol{A}}\Vert_F^2)
\end{equation}

这说明奇异值之间的间隙越大，低秩近似越稳定。

### 九、数值稳定的伪逆计算

直接计算伪逆$\boldsymbol{A}^{\dagger} = \lim_{\epsilon\to 0}(\boldsymbol{A}^{\top}\boldsymbol{A} + \epsilon\boldsymbol{I})^{-1}\boldsymbol{A}^{\top}$在数值上是不稳定的。实际中主要使用以下方法：

**方法1：基于SVD的计算**

这是最稳定和最推荐的方法：
1. 计算$\boldsymbol{A} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$的完整SVD
2. 对于每个奇异值$\sigma_i$，如果$\sigma_i > \tau$（阈值），则$\sigma_i^{\dagger} = \sigma_i^{-1}$，否则$\sigma_i^{\dagger} = 0$
3. 构造$\boldsymbol{A}^{\dagger} = \boldsymbol{V}\boldsymbol{\Sigma}^{\dagger}\boldsymbol{U}^{\top}$

**阈值选择**：常用阈值为：
\begin{equation}
\tau = \epsilon_{\text{machine}} \cdot \max(n,r) \cdot \sigma_1
\end{equation}
其中$\epsilon_{\text{machine}}$是机器精度（如双精度浮点数为$2.22 \times 10^{-16}$）。

**复杂度**：SVD的计算复杂度为$O(nr\min(n,r))$，对于瘦长矩阵（$n \gg r$）约为$O(nr^2)$。

**方法2：基于QR分解（列满秩情况）**

如果已知$\boldsymbol{A}$列满秩，可以使用QR分解：
1. 计算$\boldsymbol{A} = \boldsymbol{Q}\boldsymbol{R}$（QR分解）
2. $\boldsymbol{A}^{\dagger} = \boldsymbol{R}^{-1}\boldsymbol{Q}^{\top}$

这比SVD更快（$O(nr^2)$），但要求列满秩。

**方法3：迭代方法**

对于大规模稀疏矩阵，可以使用迭代方法求解最小二乘问题$\min_{\boldsymbol{x}} \Vert\boldsymbol{A}\boldsymbol{x} - \boldsymbol{b}\Vert_2^2$，而不是显式计算$\boldsymbol{A}^{\dagger}$。常用方法包括：
- **共轭梯度法（CG）**：适用于$\boldsymbol{A}^{\top}\boldsymbol{A}$对称正定的情况
- **LSQR算法**：基于Lanczos双对角化，适用于一般矩阵
- **GMRES**：适用于非对称问题

**数值稳定性考虑**：

1. **避免计算$\boldsymbol{A}^{\top}\boldsymbol{A}$**：条件数会平方，$\kappa(\boldsymbol{A}^{\top}\boldsymbol{A}) = \kappa(\boldsymbol{A})^2$
2. **使用完整SVD而非截断SVD**：截断SVD可能遗漏重要的数值信息
3. **正则化**：对于病态问题，使用Tikhonov正则化：
   \begin{equation}
   \boldsymbol{x}_{\lambda} = (\boldsymbol{A}^{\top}\boldsymbol{A} + \lambda\boldsymbol{I})^{-1}\boldsymbol{A}^{\top}\boldsymbol{b}
   \end{equation}
   其中$\lambda > 0$是正则化参数。

### 十、在机器学习中的应用

#### 10.1 岭回归（Ridge Regression）

岭回归是线性回归的正则化版本，其目标函数为：
\begin{equation}
\min_{\boldsymbol{w}} \Vert\boldsymbol{X}\boldsymbol{w} - \boldsymbol{y}\Vert_2^2 + \lambda\Vert\boldsymbol{w}\Vert_2^2
\end{equation}
其中$\boldsymbol{X}\in\mathbb{R}^{n\times d}$是数据矩阵，$\boldsymbol{y}\in\mathbb{R}^n$是标签向量，$\lambda \geq 0$是正则化参数。

**解析解**：通过求导并令其为零：
\begin{align}
\frac{\partial}{\partial\boldsymbol{w}}\left[\Vert\boldsymbol{X}\boldsymbol{w} - \boldsymbol{y}\Vert_2^2 + \lambda\Vert\boldsymbol{w}\Vert_2^2\right] &= 2\boldsymbol{X}^{\top}(\boldsymbol{X}\boldsymbol{w} - \boldsymbol{y}) + 2\lambda\boldsymbol{w} = \boldsymbol{0}\\\
\boldsymbol{X}^{\top}\boldsymbol{X}\boldsymbol{w} + \lambda\boldsymbol{w} &= \boldsymbol{X}^{\top}\boldsymbol{y}\\\
(\boldsymbol{X}^{\top}\boldsymbol{X} + \lambda\boldsymbol{I})\boldsymbol{w} &= \boldsymbol{X}^{\top}\boldsymbol{y}\\\
\boldsymbol{w}_{\lambda} &= (\boldsymbol{X}^{\top}\boldsymbol{X} + \lambda\boldsymbol{I})^{-1}\boldsymbol{X}^{\top}\boldsymbol{y}
\end{align}

**与伪逆的关系**：当$\lambda \to 0$时，$\boldsymbol{w}_{\lambda} \to \boldsymbol{X}^{\dagger}\boldsymbol{y}$，这正是最小范数最小二乘解。岭回归可以看作是对伪逆的正则化版本。

**SVD视角**：将$\boldsymbol{X} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$代入：
\begin{align}
\boldsymbol{w}_{\lambda} &= (\boldsymbol{V}\boldsymbol{\Sigma}^{\top}\boldsymbol{U}^{\top}\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top} + \lambda\boldsymbol{I})^{-1}\boldsymbol{V}\boldsymbol{\Sigma}^{\top}\boldsymbol{U}^{\top}\boldsymbol{y}\\\
&= \boldsymbol{V}(\boldsymbol{\Sigma}^{\top}\boldsymbol{\Sigma} + \lambda\boldsymbol{I})^{-1}\boldsymbol{\Sigma}^{\top}\boldsymbol{U}^{\top}\boldsymbol{y}\\\
&= \sum_{i=1}^s \frac{\sigma_i}{\sigma_i^2 + \lambda} \langle \boldsymbol{u}_i, \boldsymbol{y} \rangle \boldsymbol{v}_i
\end{align}

这表明岭回归对小奇异值对应的分量进行了压缩，从而提高数值稳定性。

**偏差-方差权衡**：
- 当$\lambda = 0$时，得到无偏估计，但方差大（在病态情况下）
- 当$\lambda > 0$时，引入偏差，但减小方差
- 最优$\lambda$通过交叉验证选择

#### 10.2 主成分分析（PCA）

PCA是一种降维方法，它寻找数据的主要变化方向。

**问题设定**：给定中心化数据矩阵$\boldsymbol{X}\in\mathbb{R}^{n\times d}$（每列均值为0），找到$k$个正交方向使得投影后的方差最大。

**等价形式**：PCA等价于找矩阵$\boldsymbol{W}\in\mathbb{R}^{d\times k}$（$\boldsymbol{W}^{\top}\boldsymbol{W} = \boldsymbol{I}_k$）使得重构误差最小：
\begin{equation}
\min_{\boldsymbol{W}: \boldsymbol{W}^{\top}\boldsymbol{W}=\boldsymbol{I}_k} \Vert\boldsymbol{X} - \boldsymbol{X}\boldsymbol{W}\boldsymbol{W}^{\top}\Vert_F^2
\end{equation}

**SVD解法**：对$\boldsymbol{X}$进行SVD：$\boldsymbol{X} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^{\top}$，则：
- 主成分方向：$\boldsymbol{W} = [\boldsymbol{v}_1, \cdots, \boldsymbol{v}_k]$（前$k$个右奇异向量）
- 降维数据：$\boldsymbol{Z} = \boldsymbol{X}\boldsymbol{W} = \boldsymbol{U}_k\boldsymbol{\Sigma}_k$
- 重构：$\tilde{\boldsymbol{X}} = \boldsymbol{Z}\boldsymbol{W}^{\top} = \boldsymbol{U}_k\boldsymbol{\Sigma}_k\boldsymbol{V}_k^{\top} = \boldsymbol{X}_k$

由Eckart-Young定理，这正是最优的秩$k$近似。

**解释方差比例**：第$i$个主成分解释的方差比例为：
\begin{equation}
\text{explained variance ratio}_i = \frac{\sigma_i^2}{\sum_{j=1}^s \sigma_j^2}
\end{equation}

前$k$个主成分的累积解释方差比例为：
\begin{equation}
\text{cumulative explained variance} = \frac{\sum_{i=1}^k \sigma_i^2}{\sum_{j=1}^s \sigma_j^2} = 1 - \epsilon_{\text{rel},F}^2
\end{equation}

**与伪逆的联系**：PCA投影矩阵$\boldsymbol{P} = \boldsymbol{W}\boldsymbol{W}^{\top}$是到主子空间的正交投影，它与$\boldsymbol{X}_k\boldsymbol{X}_k^{\dagger}$密切相关：
\begin{equation}
\boldsymbol{X}\boldsymbol{W}\boldsymbol{W}^{\top} = \boldsymbol{X}_k(\boldsymbol{X}_k^{\dagger}\boldsymbol{X}_k)
\end{equation}

**核PCA**：通过核技巧，PCA可以扩展到非线性情况。核矩阵$\boldsymbol{K} = \boldsymbol{X}\boldsymbol{X}^{\top}$的特征分解：
\begin{equation}
\boldsymbol{K} = \boldsymbol{U}\boldsymbol{\Lambda}\boldsymbol{U}^{\top}
\end{equation}
给出了特征空间中的主成分。

#### 10.3 低秩矩阵恢复

在推荐系统等应用中，我们需要从不完整的观测中恢复低秩矩阵。

**问题设定**：观测到$\boldsymbol{M}$的部分元素$\\{M_{ij}: (i,j) \in \Omega\\}$，假设$\boldsymbol{M}$是低秩的，恢复完整的$\boldsymbol{M}$。

**核范数最小化**：核范数（nuclear norm）定义为奇异值之和：
\begin{equation}
\Vert\boldsymbol{X}\Vert_* = \sum_{i=1}^{\text{rank}(\boldsymbol{X})} \sigma_i
\end{equation}

它是秩的凸松弛。矩阵恢复问题可以表示为：
\begin{equation}
\min_{\boldsymbol{X}} \Vert\boldsymbol{X}\Vert_* \quad \text{s.t.} \quad X_{ij} = M_{ij}, \,\, (i,j) \in \Omega
\end{equation}

**与伪逆的关系**：核范数最小化的解可以通过SVD和软阈值算子（soft-thresholding）迭代求解，每一步都涉及伪逆计算：
\begin{equation}
\mathcal{S}_{\tau}(\boldsymbol{X}) = \boldsymbol{U}\text{diag}(\max(\sigma_i - \tau, 0))\boldsymbol{V}^{\top}
\end{equation}
其中$\boldsymbol{X} = \boldsymbol{U}\text{diag}(\sigma_i)\boldsymbol{V}^{\top}$。

### 十一、总结与展望

本文对伪逆进行了全面而深入的数学推导，涵盖了：

1. **公理化定义**：Moore-Penrose四条公理及唯一性证明
2. **构造方法**：基于SVD的显式构造
3. **最小二乘理论**：最小二乘解和最小范数解的完整理论
4. **性质体系**：15个重要性质及其证明
5. **最优性理论**：Eckart-Young定理及两种范数下的分析
6. **扰动分析**：误差界、条件数和稳定性理论
7. **数值计算**：稳定的算法和实现考虑
8. **机器学习应用**：岭回归、PCA和矩阵恢复

伪逆不仅是一个数学上优美的概念，更是连接线性代数、优化理论和机器学习的重要桥梁。通过SVD这一核心工具，伪逆提供了对线性系统、低秩近似和数据降维问题的统一理解。

在实际应用中，理解伪逆的数学本质能够帮助我们：
- 正确处理秩亏损和病态问题
- 选择合适的正则化方法
- 分析算法的数值稳定性
- 理解低秩近似的理论保证

随着大规模数据和深度学习的发展，伪逆及其变体（如加权伪逆、分块伪逆、张量伪逆）在现代机器学习中扮演着越来越重要的角色。

