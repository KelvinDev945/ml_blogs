---
title: 生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配
slug: 生成扩散模型漫谈十八得分匹配-条件得分匹配
date: 2023-02-28
tags: 概率, 分析, 生成模型, 扩散, 生成模型
status: completed
---

# 生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配

**原文链接**: [https://spaces.ac.cn/archives/9509](https://spaces.ac.cn/archives/9509)

**发布日期**: 

---

在前面的介绍中，我们多次提及“得分匹配”和“条件得分匹配”，它们是扩散模型、能量模型等经常出现的概念，特别是很多文章直接说扩散模型的训练目标是“得分匹配”，但事实上当前主流的扩散模型如DDPM的训练目标是“条件得分匹配”才对。

那么“得分匹配”与“条件得分匹配”具体是什么关系呢？它们两者是否等价呢？本文详细讨论这个问题。

## 得分匹配 #

首先，得分匹配（Score Matching）是指训练目标：  
\begin{equation}\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2\right]\label{eq:sm}\end{equation}  
其中$\boldsymbol{\theta}$是训练参数。很明显，得分匹配是想学习一个模型$\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)$来逼近$\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)$，这里的$\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)$我们就称为“得分”。

在扩散模型场景，$p_t(\boldsymbol{x}_t)$由下式给出：  
\begin{equation}p_t(\boldsymbol{x}_t) = \int p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)p_0(\boldsymbol{x}_0)d\boldsymbol{x}_0 = \mathbb{E}_{\boldsymbol{x}_0\sim p_0(\boldsymbol{x}_0)}\left[p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]\label{eq:pt}\end{equation}  
其中$p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)$一般都是已知概率密度解析式的简单分布（如条件正态分布），$p_0(\boldsymbol{x}_0)$也是给定的分布，但一般代表训练数据，也就是说我们只能从$p_0(\boldsymbol{x}_0)$中采样，但不知道$p_0(\boldsymbol{x}_0)$的具体表达式。

根据式$\eqref{eq:pt}$，我们可以推导得  
\begin{equation}\begin{aligned}  
\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t) =&\, \frac{\nabla_{\boldsymbol{x}_t}p_t(\boldsymbol{x}_t)}{p_t(\boldsymbol{x}_t)} \\\  
=&\, \frac{\int p_0(\boldsymbol{x}_0)\nabla_{\boldsymbol{x}_t} p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)d\boldsymbol{x}_0}{p_t(\boldsymbol{x}_t)} \\\  
=&\, \frac{\int p_0(\boldsymbol{x}_0)\nabla_{\boldsymbol{x}_t} p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)d\boldsymbol{x}_0}{\int p_0(\boldsymbol{x}_0) p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)d\boldsymbol{x}_0} \\\  
=&\, \frac{\mathbb{E}_{\boldsymbol{x}_0\sim p_0(\boldsymbol{x}_0)}\left[\nabla_{\boldsymbol{x}_t}p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]}{\mathbb{E}_{\boldsymbol{x}_0\sim p_0(\boldsymbol{x}_0)}\left[p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]} \\\  
\end{aligned}\label{eq:score-1}\end{equation}  
根据我们的假设，$\nabla_{\boldsymbol{x}_t}p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)$和$p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)$都是已知解析式的，因此理论上我们可以通过采样$\boldsymbol{x}_0$来估计$\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)$，但由于这里涉及到两个期望的除法，是一个有偏估计（参考[《简述无偏估计和有偏估计》](/archives/6747)），因此需要采样足够多的点才能做出比较准确的估计，因此直接用式$\eqref{eq:sm}$作为训练目标的话，需要较大的batch_size才有比较好的效果。

## 条件得分 #

实际上，一般扩散模型所用的训练目标是“条件得分匹配（Conditional Score Matching）”：  
\begin{equation}\mathbb{E}_{\boldsymbol{x}_0,\boldsymbol{x}_t\sim p_0(\boldsymbol{x}_0)p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2\right]\end{equation}  
根据假设，$\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)$是已知解析式的，因此上述目标是直接可用的，每次采样一对$(\boldsymbol{x}_0,\boldsymbol{x}_t)$进行估算。特别地，这是一个无偏估计，这意味着它不是特别依赖于大batch_size，因此是一个比较实用的训练目标。

为了分析“得分匹配”与“条件得分匹配”的关系，我们还需要$\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)$的另一个恒等式：  
\begin{equation}\begin{aligned}  
\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t) =&\, \frac{\nabla_{\boldsymbol{x}_t}p_t(\boldsymbol{x}_t)}{p_t(\boldsymbol{x}_t)} \\\  
=&\, \frac{\int p_0(\boldsymbol{x}_0)\nabla_{\boldsymbol{x}_t} p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)d\boldsymbol{x}_0}{p_t(\boldsymbol{x}_t)} \\\  
=&\, \frac{\int p_0(\boldsymbol{x}_0)p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) d\boldsymbol{x}_0}{p_t(\boldsymbol{x}_t)} \\\  
=&\, \int p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) d\boldsymbol{x}_0 \\\  
=&\, \mathbb{E}_{\boldsymbol{x}_0\sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right] \\\  
\end{aligned}\label{eq:score-2}\end{equation}

## 不等关系 #

首先，我们可以很快速地证明两者之间的第一个结果：条件得分匹配是得分匹配的一个上界。这也就意味着最小化条件得分匹配，某种程度上也在最小化得分匹配。

证明并不困难，之前我们在[《生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配》](/archives/9467)就已经证明过：  
\begin{equation}\begin{aligned}  
&\,\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2\right] \\\  
=&\,\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\left[\left\Vert\mathbb{E}_{\boldsymbol{x}_0\sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right] - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2\right] \\\  
\leq &\,\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\mathbb{E}_{\boldsymbol{x}_0\sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2\right] \\\  
= &\,\mathbb{E}_{\boldsymbol{x}_0\sim p_0(\boldsymbol{x}_0),\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2\right] \\\  
\end{aligned}\end{equation}  
第一个等号是因为恒等式$\eqref{eq:score-2}$，第二个不等号则是因为平方平均不等式的推广或者詹森不等式，第三个等号则是贝叶斯公式了。

## 等价关系 #

前两天，在微信群里大家讨论到得分匹配的时候，有群友指点到：条件得分匹配与得分匹配之差是一个跟优化无关的常数，所以两者实际上是完全等价的！刚听到这个结论的时候笔者也相当惊讶，两者居然还是等价关系，而不单单是上下界关系。不仅如此，笔者尝试证明了一下后，发现证明过程居然也很简单！

首先，关于得分匹配，我们有  
\begin{equation}\begin{aligned}  
&\,\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2\right] \\\  
=&\,\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\left[\color{orange}{\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)\right\Vert^2} + \color{red}{\left\Vert\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2} - \color{green}{2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\cdot\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)}\right] \\\  
\end{aligned}\end{equation}  
然后，关于条件得分匹配，我们有  
\begin{equation}\begin{aligned}  
&\,\mathbb{E}_{\boldsymbol{x}_0,\boldsymbol{x}_t\sim p_0(\boldsymbol{x}_0)p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2\right] \\\\[5pt]  
=&\,\mathbb{E}_{\boldsymbol{x}_0,\boldsymbol{x}_t\sim p_0(\boldsymbol{x}_0)p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\Vert^2 + \left\Vert\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2 - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\cdot\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right] \\\\[5pt]  
=&\,\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t),\boldsymbol{x}_0\sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\Vert^2 + \left\Vert\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2 - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\cdot\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right] \\\\[5pt]  
=&\,\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\left[{\begin{aligned}&\color{orange}{\mathbb{E}_{\boldsymbol{x}_0\sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\Vert^2\right]} + \color{red}{\left\Vert\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2} \\\  
&\qquad\qquad- \color{green}{2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\cdot\mathbb{E}_{\boldsymbol{x}_0\sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]}\end{aligned}}\right] \\\\[5pt]  
=&\,\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\left[\color{orange}{\mathbb{E}_{\boldsymbol{x}_0\sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\Vert^2\right]} + \color{red}{\left\Vert\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\right\Vert^2} - \color{green}{2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)\cdot\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)}\right] \\\  
\end{aligned}\end{equation}  
两者作差，可以发现结果是  
\begin{equation}\mathbb{E}_{\boldsymbol{x}_t\sim p_t(\boldsymbol{x}_t)}\left[\color{orange}{\mathbb{E}_{\boldsymbol{x}_0\sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\Vert^2\right] - \left\Vert\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)\right\Vert^2}\right]\end{equation}  
它跟参数$\boldsymbol{\theta}$无关。所以最小化得分匹配目标，跟最小化条件得分匹配，在理论上是等价的。根据群友介绍，相关结果首次出现在文章[《A Connection Between Score Matching and Denoising Autoencoders》](https://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf)中。

既然两者理论上等价，这是不是意味着我们前面的“得分匹配”比“条件得分匹配”需要更大batch_size的说法不成立？并不是。如果还是直接从式$\eqref{eq:score-1}$来估计$\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)$然后做得分匹配，那么结果确实还是有偏的，依赖于大batch_size；而当我们将目标$\eqref{eq:sm}$展开进一步化简后，已经逐步将有偏估计转化为无偏估计了，这时候不会太依赖于batch_size。也就是说，虽然两个目标从理论上是等价的，从统计量的角度，属于不同性质的统计量，它们的等价仅仅是在采样样本数趋于无穷时的精确等价。

## 文章小结 #

本文主要分析“得分匹配”和“条件得分匹配”两个训练目标之间的关联。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/9509>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Feb. 28, 2023). 《生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/9509>

@online{kexuefm-9509,  
title={生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配},  
author={苏剑林},  
year={2023},  
month={Feb},  
url={\url{https://spaces.ac.cn/archives/9509}},  
} 


---

## 详细数学推导

### 1. Score函数的定义与几何意义

#### 1.1 Score函数的定义

对于概率密度函数$p(\boldsymbol{x})$，其**Score函数**定义为对数概率密度的梯度：
$$
\boldsymbol{s}(\boldsymbol{x}) = \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})
$$

展开后可以得到：
$$
\boldsymbol{s}(\boldsymbol{x}) = \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x}) = \frac{\nabla_{\boldsymbol{x}} p(\boldsymbol{x})}{p(\boldsymbol{x})}
$$

这个定义说明，Score函数是概率密度函数的梯度与概率密度函数值之比。

**性质1**：Score函数具有不变性，即对于任意常数$Z$：
$$
\nabla_{\boldsymbol{x}} \log \frac{p(\boldsymbol{x})}{Z} = \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x}) - \nabla_{\boldsymbol{x}} \log Z = \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})
$$

因此，即使我们不知道归一化常数$Z$，也能计算Score函数。这一性质对于能量模型（Energy-Based Models）特别重要。

#### 1.2 Score函数的几何意义

从几何角度理解，Score函数指向概率密度函数增长最快的方向。具体来说：

1. **方向性**：$\boldsymbol{s}(\boldsymbol{x})$指向$p(\boldsymbol{x})$增长最快的方向
2. **大小**：$\|\boldsymbol{s}(\boldsymbol{x})\|$表示增长速率的快慢
3. **物理意义**：可以理解为"数据空间中指向高密度区域的力场"

对于多模态分布，Score函数在每个模式附近都指向该模式的中心，在不同模式之间的低密度区域，Score函数会发生方向的急剧变化。

#### 1.3 Score函数的统计性质

**期望性质**：对于任意光滑函数$f(\boldsymbol{x})$，有：
$$
\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}[f(\boldsymbol{x}) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})] = -\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}[\nabla_{\boldsymbol{x}} f(\boldsymbol{x})]
$$

这个恒等式称为**Stein恒等式**，证明如下：
$$
\begin{aligned}
\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}[f(\boldsymbol{x}) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})] &= \int f(\boldsymbol{x}) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x}) p(\boldsymbol{x}) d\boldsymbol{x} \\
&= \int f(\boldsymbol{x}) \frac{\nabla_{\boldsymbol{x}} p(\boldsymbol{x})}{p(\boldsymbol{x})} p(\boldsymbol{x}) d\boldsymbol{x} \\
&= \int f(\boldsymbol{x}) \nabla_{\boldsymbol{x}} p(\boldsymbol{x}) d\boldsymbol{x}
\end{aligned}
$$

使用分部积分（假设边界条件$\lim_{\|\boldsymbol{x}\| \to \infty} f(\boldsymbol{x}) p(\boldsymbol{x}) = 0$）：
$$
\begin{aligned}
\int f(\boldsymbol{x}) \nabla_{\boldsymbol{x}} p(\boldsymbol{x}) d\boldsymbol{x} &= \left[f(\boldsymbol{x}) p(\boldsymbol{x})\right]_{-\infty}^{\infty} - \int \nabla_{\boldsymbol{x}} f(\boldsymbol{x}) p(\boldsymbol{x}) d\boldsymbol{x} \\
&= -\int \nabla_{\boldsymbol{x}} f(\boldsymbol{x}) p(\boldsymbol{x}) d\boldsymbol{x} \\
&= -\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}[\nabla_{\boldsymbol{x}} f(\boldsymbol{x})]
\end{aligned}
$$

### 2. Score Matching的原始目标函数

#### 2.1 Fisher散度与Score Matching

我们希望训练一个神经网络$\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})$来逼近真实的Score函数$\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})$。最直接的想法是最小化两者的均方误差：
$$
\mathcal{L}_{\text{naive}}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[\frac{1}{2}\left\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}) - \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})\right\|^2\right]
$$

这个目标也称为**Fisher散度**（Fisher Divergence）的一半：
$$
\mathcal{D}_F(p, q) = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[\left\|\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x}) - \nabla_{\boldsymbol{x}} \log q(\boldsymbol{x})\right\|^2\right]
$$

展开这个目标函数：
$$
\begin{aligned}
\mathcal{L}_{\text{naive}}(\boldsymbol{\theta}) &= \frac{1}{2}\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[\left\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})\right\|^2 - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}) \cdot \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x}) + \left\|\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})\right\|^2\right]
\end{aligned}
$$

问题在于，第三项$\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}[\|\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})\|^2]$与参数$\boldsymbol{\theta}$无关，可以忽略。第二项需要知道$\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})$的值，这正是我们不知道的。

#### 2.2 利用Stein恒等式重写目标函数

关键观察：利用Stein恒等式，我们可以将第二项改写为不需要$\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})$的形式。

令$f(\boldsymbol{x}) = \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})$，根据Stein恒等式的第$i$个分量：
$$
\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[s_{\boldsymbol{\theta},i}(\boldsymbol{x}) \frac{\partial \log p(\boldsymbol{x})}{\partial x_i}\right] = -\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[\frac{\partial s_{\boldsymbol{\theta},i}(\boldsymbol{x})}{\partial x_i}\right]
$$

对所有分量求和：
$$
\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}) \cdot \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})\right] = -\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[\text{tr}(\nabla_{\boldsymbol{x}} \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}))\right]
$$

其中$\text{tr}(\nabla_{\boldsymbol{x}} \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}))$是Jacobian矩阵的迹（trace），即：
$$
\text{tr}(\nabla_{\boldsymbol{x}} \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})) = \sum_{i=1}^{d} \frac{\partial s_{\boldsymbol{\theta},i}(\boldsymbol{x})}{\partial x_i} = \nabla_{\boldsymbol{x}} \cdot \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})
$$

这也称为**散度**（divergence）。

#### 2.3 显式Score Matching目标

因此，Score Matching的显式目标函数为（忽略常数项）：
$$
\mathcal{L}_{\text{ESM}}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[\frac{1}{2}\left\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})\right\|^2 + \text{tr}(\nabla_{\boldsymbol{x}} \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}))\right]
$$

完整形式为：
$$
\mathcal{L}_{\text{ESM}}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x})}\left[\frac{1}{2}\left\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})\right\|^2 + \sum_{i=1}^{d} \frac{\partial s_{\boldsymbol{\theta},i}(\boldsymbol{x})}{\partial x_i}\right]
$$

**优点**：这个目标不需要知道$\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})$的值，只需要从$p(\boldsymbol{x})$中采样。

**缺点**：需要计算神经网络输出的Jacobian矩阵的迹，这对于高维数据（如图像）计算成本很高。对于$d$维数据，需要进行$d$次反向传播来计算所有的二阶导数$\frac{\partial s_{\boldsymbol{\theta},i}}{\partial x_i}$。

### 3. Denoising Score Matching

#### 3.1 动机与基本思想

为了避免计算Hessian矩阵，Vincent (2011)提出了**Denoising Score Matching**（去噪得分匹配）。核心思想是：

1. 对数据加噪声：$\tilde{\boldsymbol{x}} = \boldsymbol{x} + \boldsymbol{\epsilon}$，其中$\boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^2 \mathbf{I})$
2. 学习去噪后分布的Score函数
3. 证明这等价于原始的Score Matching

#### 3.2 加噪分布

设原始数据分布为$p_0(\boldsymbol{x})$，加噪核为：
$$
q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) = \mathcal{N}(\tilde{\boldsymbol{x}}|\boldsymbol{x}, \sigma^2 \mathbf{I})
$$

则加噪后的边缘分布为：
$$
q_{\sigma}(\tilde{\boldsymbol{x}}) = \int p_0(\boldsymbol{x}) q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) d\boldsymbol{x}
$$

我们的目标是学习$\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}})$。

#### 3.3 条件Score的计算

对于高斯加噪，条件Score有解析形式：
$$
\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) = \nabla_{\tilde{\boldsymbol{x}}} \log \mathcal{N}(\tilde{\boldsymbol{x}}|\boldsymbol{x}, \sigma^2 \mathbf{I})
$$

展开：
$$
\begin{aligned}
\log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) &= -\frac{d}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\|\tilde{\boldsymbol{x}} - \boldsymbol{x}\|^2 \\
&= -\frac{d}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}(\tilde{\boldsymbol{x}} - \boldsymbol{x})^T(\tilde{\boldsymbol{x}} - \boldsymbol{x})
\end{aligned}
$$

对$\tilde{\boldsymbol{x}}$求梯度：
$$
\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) = -\frac{1}{\sigma^2}(\tilde{\boldsymbol{x}} - \boldsymbol{x}) = -\frac{\boldsymbol{\epsilon}}{\sigma^2}
$$

其中$\boldsymbol{\epsilon} = \tilde{\boldsymbol{x}} - \boldsymbol{x}$是噪声。

#### 3.4 Denoising Score Matching等价性证明

**定理**：以下两个目标等价（相差一个与$\boldsymbol{\theta}$无关的常数）：

1. Score Matching: $\mathcal{L}_{\text{SM}}(\boldsymbol{\theta}) = \mathbb{E}_{\tilde{\boldsymbol{x}} \sim q_{\sigma}}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) - \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}})\|^2\right]$

2. Denoising Score Matching: $\mathcal{L}_{\text{DSM}}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{x} \sim p_0, \tilde{\boldsymbol{x}} \sim q_{\sigma}(\cdot|\boldsymbol{x})}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) - \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\|^2\right]$

**证明**：

首先，边缘Score可以写成条件Score的期望：
$$
\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}) = \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}|\tilde{\boldsymbol{x}})}\left[\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\right]
$$

这个恒等式的证明如下：
$$
\begin{aligned}
\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}) &= \frac{\nabla_{\tilde{\boldsymbol{x}}} q_{\sigma}(\tilde{\boldsymbol{x}})}{q_{\sigma}(\tilde{\boldsymbol{x}})} \\
&= \frac{\nabla_{\tilde{\boldsymbol{x}}} \int p_0(\boldsymbol{x}) q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) d\boldsymbol{x}}{q_{\sigma}(\tilde{\boldsymbol{x}})} \\
&= \frac{\int p_0(\boldsymbol{x}) \nabla_{\tilde{\boldsymbol{x}}} q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) d\boldsymbol{x}}{q_{\sigma}(\tilde{\boldsymbol{x}})} \\
&= \frac{\int p_0(\boldsymbol{x}) q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) d\boldsymbol{x}}{q_{\sigma}(\tilde{\boldsymbol{x}})} \\
&= \int \frac{p_0(\boldsymbol{x}) q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})}{q_{\sigma}(\tilde{\boldsymbol{x}})} \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) d\boldsymbol{x} \\
&= \int p(\boldsymbol{x}|\tilde{\boldsymbol{x}}) \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x}) d\boldsymbol{x} \\
&= \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}|\tilde{\boldsymbol{x}})}\left[\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\right]
\end{aligned}
$$

现在展开DSM目标：
$$
\begin{aligned}
\mathcal{L}_{\text{DSM}}(\boldsymbol{\theta}) &= \mathbb{E}_{\boldsymbol{x}, \tilde{\boldsymbol{x}}}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) - \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\|^2\right] \\
&= \mathbb{E}_{\tilde{\boldsymbol{x}} \sim q_{\sigma}} \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}|\tilde{\boldsymbol{x}})}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) - \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\|^2\right]
\end{aligned}
$$

展开平方项：
$$
\begin{aligned}
&\mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}|\tilde{\boldsymbol{x}})}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) - \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\|^2\right] \\
=& \|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\|^2 - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) \cdot \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}|\tilde{\boldsymbol{x}})}\left[\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\right] \\
&\quad + \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}|\tilde{\boldsymbol{x}})}\left[\|\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\|^2\right]
\end{aligned}
$$

利用前面的恒等式：
$$
\begin{aligned}
=& \|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\|^2 - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) \cdot \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}) \\
&\quad + \mathbb{E}_{\boldsymbol{x} \sim p(\boldsymbol{x}|\tilde{\boldsymbol{x}})}\left[\|\nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}}|\boldsymbol{x})\|^2\right]
\end{aligned}
$$

因此：
$$
\begin{aligned}
\mathcal{L}_{\text{DSM}}(\boldsymbol{\theta}) &= \mathbb{E}_{\tilde{\boldsymbol{x}} \sim q_{\sigma}}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\|^2 - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) \cdot \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}})\right] + C \\
&= \mathbb{E}_{\tilde{\boldsymbol{x}} \sim q_{\sigma}}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}}) - \nabla_{\tilde{\boldsymbol{x}}} \log q_{\sigma}(\tilde{\boldsymbol{x}})\|^2\right] + C'
\end{aligned}
$$

其中$C, C'$是与$\boldsymbol{\theta}$无关的常数。这证明了DSM与SM的等价性。

#### 3.5 实用形式

在扩散模型中，常用的Denoising Score Matching目标为：
$$
\mathcal{L}_{\text{DSM}}(\boldsymbol{\theta}) = \mathbb{E}_{t, \boldsymbol{x}_0, \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) - \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\|^2\right]
$$

其中：
- $t \sim \text{Uniform}(0, T)$
- $\boldsymbol{x}_0 \sim p_0(\boldsymbol{x}_0)$
- $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$
- $\boldsymbol{x}_t = \alpha_t \boldsymbol{x}_0 + \sigma_t \boldsymbol{\epsilon}$

对于这种形式，条件Score为：
$$
\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = -\frac{\boldsymbol{x}_t - \alpha_t \boldsymbol{x}_0}{\sigma_t^2} = -\frac{\boldsymbol{\epsilon}}{\sigma_t}
$$

### 4. Sliced Score Matching

#### 4.1 动机

Sliced Score Matching由Song et al. (2019)提出，用于进一步降低计算复杂度。核心思想是：不直接匹配整个Score向量，而是匹配其在随机方向上的投影。

#### 4.2 投影Score

对于随机单位向量$\boldsymbol{v} \sim p_{\boldsymbol{v}}$（例如，从单位球面均匀采样），定义**投影Score**：
$$
\boldsymbol{v}^T \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})
$$

Sliced Score Matching的目标是：
$$
\mathcal{L}_{\text{SSM}}(\boldsymbol{\theta}) = \frac{1}{2}\mathbb{E}_{\boldsymbol{x} \sim p, \boldsymbol{v} \sim p_{\boldsymbol{v}}}\left[\left(\boldsymbol{v}^T \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}) - \boldsymbol{v}^T \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})\right)^2\right]
$$

#### 4.3 等价形式推导

利用Stein恒等式的变体：
$$
\mathbb{E}_{\boldsymbol{x} \sim p}\left[\boldsymbol{v}^T \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}) \cdot \boldsymbol{v}^T \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})\right] = -\mathbb{E}_{\boldsymbol{x} \sim p}\left[\boldsymbol{v}^T \nabla_{\boldsymbol{x}} (\boldsymbol{v}^T \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}))\right]
$$

注意到：
$$
\boldsymbol{v}^T \nabla_{\boldsymbol{x}} (\boldsymbol{v}^T \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})) = \boldsymbol{v}^T \left(\nabla_{\boldsymbol{x}} \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})^T\right) \boldsymbol{v} = \boldsymbol{v}^T \mathbf{J}_{\boldsymbol{s}_{\boldsymbol{\theta}}} \boldsymbol{v}
$$

这是Jacobian矩阵$\mathbf{J}_{\boldsymbol{s}_{\boldsymbol{\theta}}}$的二次型。

因此，Sliced Score Matching的显式形式为：
$$
\mathcal{L}_{\text{SSM}}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{x}, \boldsymbol{v}}\left[\frac{1}{2}\|\boldsymbol{v}^T \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})\|^2 + \boldsymbol{v}^T \mathbf{J}_{\boldsymbol{s}_{\boldsymbol{\theta}}}(\boldsymbol{x}) \boldsymbol{v}\right]
$$

**优点**：计算$\boldsymbol{v}^T \mathbf{J}_{\boldsymbol{s}_{\boldsymbol{\theta}}} \boldsymbol{v}$只需要一次反向传播（使用自动微分的Jacobian-vector product技巧），而不是$d$次。

### 5. 条件Score Matching的完整理论

#### 5.1 时间依赖的扩散过程

在扩散模型中，我们考虑时间参数化的分布族$\{p_t(\boldsymbol{x}_t)\}_{t \in [0,T]}$，其中：
$$
p_t(\boldsymbol{x}_t) = \int p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) p_0(\boldsymbol{x}_0) d\boldsymbol{x}_0
$$

这里：
- $p_0(\boldsymbol{x}_0)$是数据分布
- $p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)$是扩散核（transition kernel）

#### 5.2 扩散核的参数化

常见的扩散核为高斯核：
$$
p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = \mathcal{N}(\boldsymbol{x}_t | \alpha_t \boldsymbol{x}_0, \sigma_t^2 \mathbf{I})
$$

其中$\alpha_t, \sigma_t$是时间的确定性函数，满足：
- $\alpha_0 = 1, \sigma_0 = 0$（初始时刻无噪声）
- $\alpha_T \approx 0, \sigma_T \gg 1$（最终时刻接近纯噪声）

#### 5.3 边缘Score与条件Score的关系

**关键恒等式**：
$$
\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t) = \mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]
$$

这个恒等式的详细证明：
$$
\begin{aligned}
p_t(\boldsymbol{x}_t) &= \int p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) p_0(\boldsymbol{x}_0) d\boldsymbol{x}_0
\end{aligned}
$$

两边对$\boldsymbol{x}_t$求梯度：
$$
\nabla_{\boldsymbol{x}_t} p_t(\boldsymbol{x}_t) = \int \nabla_{\boldsymbol{x}_t} p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) p_0(\boldsymbol{x}_0) d\boldsymbol{x}_0
$$

除以$p_t(\boldsymbol{x}_t)$：
$$
\begin{aligned}
\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t) &= \frac{\nabla_{\boldsymbol{x}_t} p_t(\boldsymbol{x}_t)}{p_t(\boldsymbol{x}_t)} \\
&= \frac{\int \nabla_{\boldsymbol{x}_t} p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) p_0(\boldsymbol{x}_0) d\boldsymbol{x}_0}{p_t(\boldsymbol{x}_t)} \\
&= \int \frac{p_0(\boldsymbol{x}_0) p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)}{p_t(\boldsymbol{x}_t)} \frac{\nabla_{\boldsymbol{x}_t} p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)}{p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)} d\boldsymbol{x}_0 \\
&= \int p_t(\boldsymbol{x}_0|\boldsymbol{x}_t) \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) d\boldsymbol{x}_0
\end{aligned}
$$

#### 5.4 方差分解

对于条件Score的方差，我们有以下分解：
$$
\begin{aligned}
&\mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\left\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\|^2\right] \\
=& \left\|\mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]\right\|^2 \\
&\quad + \mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\left\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) - \mathbb{E}\left[\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]\right\|^2\right]
\end{aligned}
$$

这是期望的平方加上方差，即：
$$
\mathbb{E}[\|X\|^2] = \|\mathbb{E}[X]\|^2 + \mathbb{E}[\|X - \mathbb{E}[X]\|^2]
$$

利用前面的恒等式，第一项等于$\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t)\|^2$。

### 6. Score Matching与Conditional Score Matching的等价性

#### 6.1 定理陈述

**定理（Score Matching = Conditional Score Matching）**：

对于任意模型$\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)$，以下两个目标函数相差一个与$\boldsymbol{\theta}$无关的常数：

$$
\mathcal{L}_{\text{SM}}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{x}_t \sim p_t}\left[\left\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\right\|^2\right]
$$

$$
\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{x}_0 \sim p_0, \boldsymbol{x}_t \sim p_t(\cdot|\boldsymbol{x}_0)}\left[\left\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\right\|^2\right]
$$

即：$\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta}) = \mathcal{L}_{\text{SM}}(\boldsymbol{\theta}) + C$，其中$C$与$\boldsymbol{\theta}$无关。

#### 6.2 详细证明

**第一步**：展开Score Matching目标

$$
\begin{aligned}
\mathcal{L}_{\text{SM}}(\boldsymbol{\theta}) &= \mathbb{E}_{\boldsymbol{x}_t \sim p_t}\left[\left\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\right\|^2\right] \\
&= \mathbb{E}_{\boldsymbol{x}_t}\left[\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t)\|^2 + \|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\|^2 - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t)\right]
\end{aligned}
$$

**第二步**：展开Conditional Score Matching目标

$$
\begin{aligned}
\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta}) &= \mathbb{E}_{\boldsymbol{x}_0, \boldsymbol{x}_t}\left[\left\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\right\|^2\right] \\
&= \mathbb{E}_{\boldsymbol{x}_0, \boldsymbol{x}_t}\left[\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\|^2 + \|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\|^2\right. \\
&\qquad\qquad\left. - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]
\end{aligned}
$$

**第三步**：将联合期望改写为条件期望

利用贝叶斯公式$p_0(\boldsymbol{x}_0) p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = p_t(\boldsymbol{x}_t) p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)$：

$$
\begin{aligned}
\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta}) &= \mathbb{E}_{\boldsymbol{x}_t \sim p_t} \mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\cdot|\boldsymbol{x}_t)}\left[\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\|^2 + \|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\|^2\right. \\
&\qquad\qquad\left. - 2\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]
\end{aligned}
$$

**第四步**：分离与$\boldsymbol{x}_0$无关的项

$$
\begin{aligned}
\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta}) &= \mathbb{E}_{\boldsymbol{x}_t}\left[\mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\cdot|\boldsymbol{x}_t)}\left[\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\|^2\right]\right] \\
&\quad + \mathbb{E}_{\boldsymbol{x}_t}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\|^2\right] \\
&\quad - 2\mathbb{E}_{\boldsymbol{x}_t}\left[\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\cdot|\boldsymbol{x}_t)}\left[\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]\right]
\end{aligned}
$$

**第五步**：应用关键恒等式

根据前面证明的恒等式：
$$
\mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\boldsymbol{x}_0|\boldsymbol{x}_t)}\left[\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right] = \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t)
$$

因此：
$$
\begin{aligned}
\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta}) &= \mathbb{E}_{\boldsymbol{x}_t}\left[\mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\cdot|\boldsymbol{x}_t)}\left[\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\|^2\right]\right] \\
&\quad + \mathbb{E}_{\boldsymbol{x}_t}\left[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\|^2\right] \\
&\quad - 2\mathbb{E}_{\boldsymbol{x}_t}\left[\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t)\right]
\end{aligned}
$$

**第六步**：计算差值

现在比较$\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta})$和$\mathcal{L}_{\text{SM}}(\boldsymbol{\theta})$：

包含$\boldsymbol{\theta}$的项（红色和绿色项）完全相同：
- $\mathbb{E}_{\boldsymbol{x}_t}[\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)\|^2]$
- $-2\mathbb{E}_{\boldsymbol{x}_t}[\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) \cdot \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t)]$

差异仅在于橙色项：
$$
\begin{aligned}
\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta}) - \mathcal{L}_{\text{SM}}(\boldsymbol{\theta}) &= \mathbb{E}_{\boldsymbol{x}_t}\left[\mathbb{E}_{\boldsymbol{x}_0 \sim p_t(\cdot|\boldsymbol{x}_t)}\left[\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\|^2\right]\right] \\
&\quad - \mathbb{E}_{\boldsymbol{x}_t}\left[\|\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t)\|^2\right]
\end{aligned}
$$

这个差值与$\boldsymbol{\theta}$完全无关！

**结论**：由于$\mathcal{L}_{\text{CSM}}(\boldsymbol{\theta}) = \mathcal{L}_{\text{SM}}(\boldsymbol{\theta}) + C$（$C$与$\boldsymbol{\theta}$无关），最小化这两个目标是完全等价的。

#### 6.3 差值的物理意义

差值$C$实际上衡量了条件Score的方差：
$$
C = \mathbb{E}_{\boldsymbol{x}_t}\left[\text{Var}_{\boldsymbol{x}_0 \sim p_t(\cdot|\boldsymbol{x}_t)}\left[\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0)\right]\right]
$$

这个量描述了：给定$\boldsymbol{x}_t$，不同的$\boldsymbol{x}_0$对应的条件Score有多大的变化。

- 当噪声较小（$t$接近0）时，$C \approx 0$，因为$\boldsymbol{x}_t$几乎确定了$\boldsymbol{x}_0$
- 当噪声较大（$t$接近$T$）时，$C$较大，因为$\boldsymbol{x}_t$包含的$\boldsymbol{x}_0$信息较少

### 7. 实际训练算法

#### 7.1 DDPM训练算法

在DDPM (Denoising Diffusion Probabilistic Models)中，我们训练一个噪声预测网络$\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)$：

**算法1：DDPM训练**

```
输入：数据集 {x₀⁽ⁱ⁾}
参数：噪声调度 {αₜ, σₜ}ₜ₌₀ᵀ

重复直到收敛：
  1. 从数据集采样 x₀ ~ p₀(x₀)
  2. 均匀采样时间 t ~ Uniform({1, ..., T})
  3. 采样噪声 ε ~ N(0, I)
  4. 计算加噪样本 xₜ = αₜx₀ + σₜε
  5. 计算损失 L = ‖ε - εθ(xₜ, t)‖²
  6. 梯度下降更新 θ
```

这等价于Conditional Score Matching，因为：
$$
\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{x}_0) = -\frac{\boldsymbol{\epsilon}}{\sigma_t}
$$

因此：
$$
\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t) = -\frac{\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)}{\sigma_t}
$$

#### 7.2 Score-Based SDE训练

在Score-Based SDE模型中，我们直接训练Score网络$\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t)$：

**算法2：Score-Based SDE训练**

```
输入：数据集 {x₀⁽ⁱ⁾}
参数：SDE系数 {β(t)}

重复直到收敛：
  1. 从数据集采样 x₀ ~ p₀(x₀)
  2. 均匀采样时间 t ~ Uniform(0, T)
  3. 计算 αₜ = exp(-½∫₀ᵗβ(s)ds)
  4. 计算 σₜ² = 1 - αₜ²
  5. 采样噪声 ε ~ N(0, I)
  6. 计算 xₜ = αₜx₀ + σₜε
  7. 计算目标 s* = -(xₜ - αₜx₀)/σₜ² = -ε/σₜ
  8. 计算损失 L = λ(t)‖sθ(xₜ, t) - s*‖²
  9. 梯度下降更新 θ
```

其中$\lambda(t)$是权重函数，常用选择包括：
- $\lambda(t) = 1$：原始Score Matching
- $\lambda(t) = \sigma_t^2$：使得不同时间步的损失具有相似量级
- $\lambda(t) = \sigma_t^2 / \alpha_t^2$：与似然加权一致

#### 7.3 与其他生成模型的联系

**与VAE的联系**：

扩散模型可以看作层次VAE，其ELBO为：
$$
\log p(\boldsymbol{x}_0) \geq \mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\left[\log \frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\right]
$$

**与流模型的联系**：

当离散时间步趋于无穷时，扩散过程可以用SDE描述：
$$
d\boldsymbol{x} = f(\boldsymbol{x}, t)dt + g(t)d\boldsymbol{w}
$$

其逆向SDE为：
$$
d\boldsymbol{x} = \left[f(\boldsymbol{x}, t) - g(t)^2 \nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x})\right]dt + g(t)d\bar{\boldsymbol{w}}
$$

这建立了Score函数与概率流的联系。

### 8. 高级主题

#### 8.1 多尺度Score Matching

对于多模态分布，不同噪声水平的Score提供不同尺度的信息：

- **大噪声**（$\sigma$大）：Score指向不同模式之间的方向，帮助探索全局结构
- **小噪声**（$\sigma$小）：Score指向每个模式内部的细节，帮助生成高质量样本

因此，训练时使用多个噪声水平$\{\sigma_1, ..., \sigma_L\}$：
$$
\mathcal{L}(\boldsymbol{\theta}) = \sum_{i=1}^{L} \lambda_i \mathbb{E}_{\boldsymbol{x}_0, \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_0 + \sigma_i \boldsymbol{\epsilon}, \sigma_i) + \frac{\boldsymbol{\epsilon}}{\sigma_i}\right\|^2\right]
$$

#### 8.2 条件生成

对于条件生成任务$p(\boldsymbol{x}|\boldsymbol{y})$，需要学习条件Score：
$$
\nabla_{\boldsymbol{x}} \log p_t(\boldsymbol{x}_t|\boldsymbol{y})
$$

两种主要方法：

**方法1：Classifier Guidance**

利用分类器$p(\boldsymbol{y}|\boldsymbol{x}_t)$：
$$
\nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t|\boldsymbol{y}) = \nabla_{\boldsymbol{x}_t} \log p_t(\boldsymbol{x}_t) + \nabla_{\boldsymbol{x}_t} \log p(\boldsymbol{y}|\boldsymbol{x}_t)
$$

**方法2：Classifier-Free Guidance**

联合训练条件和无条件模型：
$$
\tilde{\boldsymbol{s}}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \boldsymbol{y}) = \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \emptyset) + w \left[\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \boldsymbol{y}) - \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t, t, \emptyset)\right]
$$

其中$w > 1$是guidance强度。

#### 8.3 Score的正则化性质

**Lipschitz连续性**：

理想情况下，Score函数应该是Lipschitz连续的：
$$
\|\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x}) - \nabla_{\boldsymbol{x}'} \log p(\boldsymbol{x}')\| \leq L \|\boldsymbol{x} - \boldsymbol{x}'\|
$$

这保证了逆向扩散过程的稳定性。

**梯度惩罚**：

为了鼓励Lipschitz连续性，可以添加梯度惩罚项：
$$
\mathcal{L}_{\text{GP}}(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{x}}\left[\|\nabla_{\boldsymbol{x}} \boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x})\|^2\right]
$$

这等价于最小化Score的Hessian范数。

### 9. 总结与展望

本文详细推导了Score Matching的各种变体及其等价性：

1. **显式Score Matching**：需要计算Hessian，计算成本高
2. **Denoising Score Matching**：通过加噪避免Hessian计算
3. **Sliced Score Matching**：通过随机投影进一步降低计算量
4. **Conditional Score Matching**：在扩散模型中最常用

关键结论：**Score Matching和Conditional Score Matching在理论上完全等价**，差异仅在于一个与优化参数无关的常数。

从实践角度：
- Conditional Score Matching是无偏估计，不依赖大batch size
- Score Matching的直接估计是有偏的，需要大batch size或特殊技巧

从理论角度：
- 两者的全局最优解完全相同
- 等价性的证明依赖于边缘Score可以表示为条件Score的期望

这一等价性为扩散模型的理论分析提供了坚实基础，也解释了为何Denoising Score Matching能够有效训练扩散模型。

