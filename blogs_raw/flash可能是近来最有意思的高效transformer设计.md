---
title: FLASHï¼šå¯èƒ½æ˜¯è¿‘æ¥æœ€æœ‰æ„æ€çš„é«˜æ•ˆTransformerè®¾è®¡
slug: flashå¯èƒ½æ˜¯è¿‘æ¥æœ€æœ‰æ„æ€çš„é«˜æ•ˆtransformerè®¾è®¡
date: 2022-02-25
tags: è¯­è¨€æ¨¡å‹, ç”Ÿæˆæ¨¡å‹, attention, é«˜æ•ˆTransformer, GAU, é—¨æ§æ³¨æ„åŠ›, çº¿æ€§å¤æ‚åº¦, ç¨€ç–æ³¨æ„åŠ›, FLASH
status: completed
tags_reviewed: true
---

# FLASHï¼šå¯èƒ½æ˜¯è¿‘æ¥æœ€æœ‰æ„æ€çš„é«˜æ•ˆTransformerè®¾è®¡

**åŸæ–‡é“¾æ¥**: [https://spaces.ac.cn/archives/8934](https://spaces.ac.cn/archives/8934)

**å‘å¸ƒæ—¥æœŸ**: 

---

é«˜æ•ˆTransformerï¼Œæ³›æŒ‡æ‰€æœ‰æ¦‚ç‡Transformeræ•ˆç‡çš„å·¥ä½œï¼Œç¬”è€…ç®—æ˜¯å…³æ³¨å¾—æ¯”è¾ƒæ—©äº†ï¼Œæœ€æ—©çš„åšå®¢å¯ä»¥è¿½æº¯åˆ°2019å¹´çš„[ã€Šä¸ºèŠ‚çº¦è€Œç”Ÿï¼šä»æ ‡å‡†Attentionåˆ°ç¨€ç–Attentionã€‹](/archives/6853)ï¼Œå½“æ—¶åšè¿™å—çš„å·¥ä½œå¾ˆå°‘ã€‚åæ¥ï¼Œè¿™ç±»å·¥ä½œé€æ¸å¤šäº†ï¼Œç¬”è€…ä¹Ÿè·Ÿè¿›äº†ä¸€äº›ï¼Œæ¯”å¦‚[çº¿æ€§Attention](/archives/7546)ã€[Performer](/archives/7921)ã€[NystrÃ¶mformer](/archives/8180)ï¼Œç”šè‡³è‡ªå·±ä¹Ÿåšäº†ä¸€äº›æ¢ç´¢ï¼Œæ¯”å¦‚ä¹‹å‰çš„â€œ[Transformerå‡çº§ä¹‹è·¯](/search/Transformer%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF/)â€ã€‚å†åæ¥ï¼Œç›¸å…³å·¥ä½œè¶Šæ¥è¶Šå¤šï¼Œä½†å¤§å¤šéƒ½å¾ˆæ— è¶£ï¼Œæ‰€ä»¥ç¬”è€…å°±æ²¡æ€ä¹ˆå…³æ³¨äº†ã€‚

[![æœ¬æ–‡æ¨¡å‹è„‰ç»œå›¾](/usr/uploads/2022/02/1135966367.png)](/usr/uploads/2022/02/1135966367.png "ç‚¹å‡»æŸ¥çœ‹åŸå›¾")

æœ¬æ–‡æ¨¡å‹è„‰ç»œå›¾

å¤§æŠµæ˜¯â€œä¹…æ—±é€¢ç”˜éœ–â€çš„æ„Ÿè§‰ï¼Œæœ€è¿‘ç»ˆäºå‡ºç°äº†ä¸€ä¸ªæ¯”è¾ƒæœ‰æ„æ€çš„é«˜æ•ˆTransformerå·¥ä½œâ€”â€”æ¥è‡ªGoogleçš„[ã€ŠTransformer Quality in Linear Timeã€‹](https://papers.cool/arxiv/2202.10447)ï¼Œç»è¿‡ç»†è¯»ä¹‹åï¼Œç¬”è€…è®¤ä¸ºè®ºæ–‡é‡Œè¾¹çœŸç®—å¾—ä¸Šæ˜¯â€œæƒŠå–œæ»¡æ»¡â€äº†ï½

## ä½•å–œä¹‹æœ‰ #

ä»€ä¹ˆæ ·çš„ç»“æœå€¼å¾—æˆ‘ä»¬ç”¨â€œæƒŠå–œâ€æ¥å½¢å®¹ï¼Ÿæœ‰æ²¡æœ‰è¨€è¿‡å…¶å®ï¼Ÿæˆ‘ä»¬ä¸å¦¨å…ˆæ¥çœ‹çœ‹è®ºæ–‡åšåˆ°äº†ä»€ä¹ˆï¼š

> 1ã€æå‡ºäº†ä¸€ç§æ–°çš„Transformerå˜ä½“ï¼Œå®ƒä¾ç„¶å…·æœ‰äºŒæ¬¡çš„å¤æ‚åº¦ï¼Œä½†æ˜¯ç›¸æ¯”æ ‡å‡†çš„Transformerï¼Œå®ƒæœ‰ç€æ›´å¿«çš„é€Ÿåº¦ã€æ›´ä½çš„æ˜¾å­˜å ç”¨ä»¥åŠæ›´å¥½çš„æ•ˆæœï¼›
> 
> 2ã€æå‡ºä¸€ç§æ–°çš„çº¿æ€§åŒ–Transformeræ–¹æ¡ˆï¼Œå®ƒä¸ä½†æå‡äº†åŸæœ‰çº¿æ€§Attentionçš„æ•ˆæœï¼Œè¿˜ä¿æŒäº†åšDecoderçš„å¯èƒ½æ€§ï¼Œå¹¶ä¸”åšDecoderæ—¶è¿˜èƒ½ä¿æŒé«˜æ•ˆçš„è®­ç»ƒå¹¶è¡Œæ€§ã€‚

è¯´å®è¯ï¼Œç¬”è€…è§‰å¾—åšåˆ°ä»¥ä¸Šä»»æ„ä¸€ç‚¹éƒ½æ˜¯éå¸¸éš¾å¾—çš„ï¼Œè€Œè¿™ç¯‡è®ºæ–‡ä¸€ä¸‹å­åšåˆ°äº†ä¸¤ç‚¹ï¼Œæ‰€ä»¥æˆ‘æ„¿æ„ç”¨â€œæƒŠå–œæ»¡æ»¡â€æ¥å½¢å®¹å®ƒã€‚æ›´é‡è¦çš„æ˜¯ï¼Œè®ºæ–‡çš„æ”¹è¿›æ€»çš„æ¥è¯´è¿˜æ˜¯æ¯”è¾ƒè‡ªç„¶å’Œä¼˜é›…çš„ï¼Œä¸åƒå¾ˆå¤šç±»ä¼¼å·¥ä½œä¸€æ ·æ˜¾å¾—å¾ˆç”Ÿç¡¬ã€‚æ­¤å¤–ï¼Œç¬”è€…è‡ªå·±ä¹Ÿåšäº†ç®€å•çš„å¤ç°å®éªŒï¼Œç»“æœæ˜¾ç¤ºè®ºæ–‡çš„å¯å¤ç°æ€§åº”è¯¥æ˜¯è›®å¥½çš„ï¼Œæ‰€ä»¥çœŸçš„æœ‰ç§â€œTransformerå±çŸ£â€çš„æ„Ÿè§‰äº†ã€‚

## é—¨æ§æ³¨æ„ #

é—²è¯å°‘è¯´ï¼Œè¿›å…¥ä¸»é¢˜ã€‚æˆ‘ä»¬çŸ¥é“[æ ‡å‡†çš„Transformer](/archives/4765)å…¶å®æ˜¯Attentionå±‚å’ŒFFNå±‚äº¤æ›¿æ„å»ºçš„ï¼Œè€Œè¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ˜¯æå‡ºäº†ä¸€ä¸ªèåˆäº†ä¸¤è€…çš„æ–°è®¾è®¡GAUï¼ˆGated Attention Unitï¼Œé—¨æ§æ³¨æ„åŠ›å•å…ƒï¼‰ï¼Œå®ƒæ˜¯æ–°æ¨¡å‹æ›´å¿«ã€æ›´çœã€æ›´å¥½çš„å…³é”®ï¼Œæ­¤å¤–å®ƒä½¿å¾—æ•´ä¸ªæ¨¡å‹åªæœ‰ä¸€ç§å±‚ï¼Œä¹Ÿæ˜¾å¾—æ›´ä¸ºä¼˜é›…ã€‚

### å¨åŠ›åˆæ˜¾ #

æ€ä¹ˆåšåˆ°Attentionå’ŒFFNçš„èåˆå‘¢ï¼Ÿé¦–å…ˆï¼Œæ ‡å‡†çš„FFNæ˜¯ä¸¤å±‚MLPæ¨¡å‹ï¼š  
\begin{equation}\boldsymbol{O}=\phi(\boldsymbol{X}\boldsymbol{W}_u)\boldsymbol{W}_o\end{equation}  
è¿™é‡Œ$\boldsymbol{X}\in\mathbb{R}^{n\times d},\boldsymbol{W}_u\in\mathbb{R}^{d\times e},\boldsymbol{W}_o\in\mathbb{R}^{e\times d}$è€Œ$\phi$æ˜¯æ¿€æ´»å‡½æ•°ã€‚åæ¥ï¼Œ[ã€ŠGLU Variants Improve Transformerã€‹](https://papers.cool/arxiv/2002.05202)å‘ç°ä½¿ç”¨äº†GLUï¼ˆGated Linear Unitï¼Œé—¨æ§çº¿æ€§å•å…ƒï¼‰çš„FFNæ•ˆæœæ›´å¥½ï¼Œå¹¶ä¸ºåæ¥çš„[mT5](/archives/7867)æ‰€ç”¨ï¼Œå…¶å½¢å¼ä¸ºï¼š  
\begin{equation}\boldsymbol{O}=(\boldsymbol{U}\odot\boldsymbol{V})\boldsymbol{W}_o,\quad \boldsymbol{U}=\phi_u(\boldsymbol{X}\boldsymbol{W}_u),\quad\boldsymbol{V}=\phi_v(\boldsymbol{X}\boldsymbol{W}_v)\end{equation}  
è¿™é‡Œ$\boldsymbol{W}_u,\boldsymbol{W}_v\in\mathbb{R}^{d\times e}$è€Œ$\odot$æ˜¯é€ä½å¯¹åº”ç›¸ä¹˜ï¼ˆHadamardç§¯ï¼‰ã€‚GLUæ›´æœ‰æ•ˆå¹¶ä¸æ˜¯ä¸€ä»¶è®©äººæ„å¤–çš„äº‹æƒ…ï¼Œæ—©åœ¨2017å¹´Facebookçš„[ã€ŠConvolutional Sequence to Sequence Learningã€‹](https://papers.cool/arxiv/1705.03122)ä¸­GLUå°±èµ·åˆ°äº†å…³é”®ä½œç”¨ï¼Œæ­¤å¤–ç¬”è€…ä¹‹å‰ç ”ç©¶çš„[DGCNN](/archives/5409)ä¹Ÿè‚¯å®šäº†GLUçš„æœ‰æ•ˆæ€§ã€‚

ä¸€èˆ¬æƒ…å†µä¸‹çš„GLUæ˜¯$\boldsymbol{U}$ä¸åŠ æ¿€æ´»å‡½æ•°è€Œ$\boldsymbol{V}$åŠ Sigmoidï¼Œä½†è¿™ç¯‡è®ºæ–‡$\boldsymbol{U},\boldsymbol{V}$éƒ½åŠ äº†æ¿€æ´»å‡½æ•°[Swish](https://papers.cool/arxiv/1710.05941)ï¼ˆä¹Ÿå«[SiLU](https://papers.cool/arxiv/1606.08415)ï¼ŒSigmoid Linear Unitï¼‰ï¼Œè¿™å¯ä»¥åœ¨é™„å½•ä¸­çš„æºç æ‰¾åˆ°ï¼Œæ­¤å¤„è·Ÿä¸»æµGLUç”¨æ³•ç•¥æœ‰ä¸åŒï¼Œç‰¹åˆ«æŒ‡å‡ºä¸€ä¸‹ã€‚

### å¼ºå¼ºè”åˆ #

æ—¢ç„¶GLUå¼çš„FFNæ›´æœ‰æ•ˆï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ä»¥å®ƒä¸ºåŸºç¡€è¿›è¡Œä¿®æ”¹ã€‚æ³¨æ„åˆ°FFNä¸èƒ½å–ä»£Attentionï¼Œæ˜¯å› ä¸ºå®ƒçš„å„ä¸ªtokenä¹‹é—´æ²¡æœ‰è¿›è¡Œäº¤äº’ï¼Œä¹Ÿå°±æ˜¯çŸ©é˜µ$\boldsymbol{U},\boldsymbol{V}$çš„æ¯ä¸€è¡Œéƒ½æ˜¯ç‹¬ç«‹è¿ç®—çš„ã€‚ä¸ºäº†è¡¥å……è¿™ç‚¹ä¸è¶³ï¼Œä¸€ä¸ªè‡ªç„¶çš„æƒ³æ³•å°±æ˜¯æŠŠtokenä¹‹é—´çš„è”ç³»è¡¥å……åˆ°$\boldsymbol{U},\boldsymbol{V}$ä¸Šå»ï¼Œè€Œä¸ºäº†ä½“ç°å‡ºè·ŸAttetionçš„ç»“åˆï¼Œé‚£ä¹ˆä¸€ä¸ªæ¯”è¾ƒè‡ªç„¶çš„è®¾è®¡å°±æ˜¯  
\begin{equation}\boldsymbol{O}=(\boldsymbol{U}\odot\boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o\label{eq:mix}\end{equation}  
å…¶ä¸­$\boldsymbol{A}\in\mathbb{R}^{n\times n}$æ˜¯AttentionçŸ©é˜µï¼Œå®ƒè´Ÿè´£èåˆtokenä¹‹é—´çš„ä¿¡æ¯ã€‚è¿™æ ·å‡ºæ¥çš„$\boldsymbol{O}$å°±åŒ…å«äº†tokenä¹‹é—´çš„äº¤äº’ï¼ŒåŸåˆ™ä¸Šå®ƒå¯ä»¥å–ä»£Attentionã€‚è‡³äº$\boldsymbol{A}$æ€ä¹ˆç®—ï¼Œæˆ‘ä»¬ç­‰ä¼šå†è¯´ã€‚

åœ¨å¼$\eqref{eq:mix}$ä¸­ï¼Œå¦‚æœ$\boldsymbol{A}$ç­‰äºå•ä½é˜µ$\boldsymbol{I}$ï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯GLUå¼çš„FFNï¼›è€Œå¦‚æœ$\boldsymbol{U}$æ˜¯å…¨1çŸ©é˜µï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯æ™®é€šçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æ‰€ä»¥è¯´ï¼Œ$\eqref{eq:mix}$æ˜¯Attentionå’ŒFFNçš„ä¸€ä¸ªç®€å•è€Œè‡ªç„¶çš„èåˆï¼Œæˆ‘ä»¬æœŸæœ›å®ƒèƒ½åŒæ—¶æ›¿æ¢æ‰Attentionå’ŒFFNï¼Œç”šè‡³æœ‰æ›´å¥½çš„è¡¨ç°ã€‚

### å¼±æ³¨æ„åŠ› #

åˆšæ‰è¯´äº†ï¼ŒGLUæœ¬èº«å°±å¾ˆå¼ºï¼Œä¸ç„¶Facebookä¹Ÿæ— æ³•å‡­å€ŸCNN+GLUåšåˆ°äº†å½“æ—¶Seq2Seqçš„SOTAï¼Œè€Œæ—¢ç„¶GLUé‚£ä¹ˆå¼ºï¼Œé‚£ä¹ˆä¸€ä¸ªçŒœæµ‹æ˜¯å®ƒä¼šå¼±åŒ–å¯¹Attentionçš„ä¾èµ–ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè™½ç„¶åœ¨å¼$\eqref{eq:mix}$ä¸­$\boldsymbol{A}$æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œä½†æˆ–è®¸æˆ‘ä»¬å¯ä»¥ç®€åŒ–å®ƒçš„å½¢å¼ã€‚äº‹å®ä¸Šç¡®å®å¦‚æ­¤ï¼ŒåŸè®ºæ–‡ä½¿ç”¨äº†å¦‚ä¸‹çš„ç®€åŒ–ç‰ˆAttentionçŸ©é˜µï¼š  
\begin{equation}\boldsymbol{A}=\frac{1}{n}\text{relu}^2\left(\frac{\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}}{\sqrt{s}}\right)=\frac{1}{ns}\text{relu}^2\left(\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}\right),\quad \boldsymbol{Z}=\phi_z(\boldsymbol{X}\boldsymbol{W}_z)\label{eq:relu-att}\end{equation}  
è¿™é‡Œ$\boldsymbol{W}_z\in\mathbb{R}^{d\times s}$ï¼Œ$s$å³æ³¨æ„åŠ›çš„head_sizeï¼Œæ–‡ä¸­å–äº†$s=128$ï¼Œè€Œ$\mathcal{Q},\mathcal{K}$æ˜¯ç®€å•çš„ä»¿å°„å˜æ¢ï¼ˆåƒLayer Normä¸­çš„ä¹˜$\gamma$åŠ $\beta$ï¼‰ï¼Œ$\text{relu}^2$åˆ™æ˜¯$\text{relu}$åå†å¹³æ–¹ã€‚

è·Ÿæ ‡å‡†çš„Scaled-Dot Self Attentionç±»ä¼¼ï¼Œè¿™é‡Œçš„æ³¨æ„åŠ›çŸ©é˜µè¿˜æ˜¯$\boldsymbol{Q},\boldsymbol{K}$çš„å†…ç§¯å¹¶é™¤ä»¥ç»´åº¦çš„å¹³æ–¹æ ¹è€Œæ¥ï¼Œå¤æ‚åº¦è¿˜æ˜¯$\mathcal{O}(n^2)$çš„ï¼Œä¸åŒçš„æ˜¯è¿™é‡Œç®€åŒ–äº†$\boldsymbol{Q},\boldsymbol{K}$çš„æ¥æºå˜æ¢ï¼Œå¹¶ä¸”æ¿€æ´»å‡½æ•°æ¢ç”¨äº†$\text{relu}^2$ã€‚å¤§å®¶å¯èƒ½å¯¹è¿™ä¸ªæ¿€æ´»å‡½æ•°æ¯”è¾ƒé™Œç”Ÿï¼Œäº‹å®ä¸Šè¿™æ˜¯ä½œè€…å›¢é˜Ÿåœ¨ä»–ä»¬ä¹‹å‰çš„è®ºæ–‡[ã€ŠPrimer: Searching for Efficient Transformers for Language Modelingã€‹](https://papers.cool/arxiv/2109.08668)ç”¨NASçš„æ–¹å¼æœå‡ºæ¥çš„ã€‚æœ€åçš„$1/n$æ˜¯ç®€å•çš„å½’ä¸€åŒ–å› å­ï¼Œç”¨ä»¥æ¶ˆé™¤é•¿åº¦çš„å½±å“ã€‚è¿™ä¸ªè®¾è®¡çš„æˆåŠŸä¹Ÿè¡¨æ˜ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„softmaxä¸æ˜¯å¿…é¡»çš„ï¼Œå¯ä»¥æ¢æˆå¸¸è§„çš„æ¿€æ´»å‡½æ•°åŠ ç®€å•çš„å½’ä¸€åŒ–ã€‚

æ³¨æ„ï¼ŒæŒ‰ç…§è®ºæ–‡é™„å½•çš„å‚è€ƒä»£ç ï¼ŒåŸè®ºæ–‡åŒ–ç®€åçš„ç¼©æ”¾å› å­å®é™…ä¸Šæ˜¯$\frac{1}{n^2}$è€Œä¸æ˜¯ä¸Šå¼çš„$\frac{1}{ns}$ï¼Œç¬”è€…è®¤ä¸º$\frac{1}{ns}$ä¼šæ›´åŠ åˆç†ä¸€äº›ï¼Œä¸ç„¶å½“$n$è¶³å¤Ÿå¤§æ—¶ï¼Œæ¯ä¸€é¡¹æ³¨æ„åŠ›éƒ½è¿‡å°äº†ã€‚å†µä¸”å¯¹ç…§æ ‡å‡†æ³¨æ„åŠ›æ‰€ç”¨çš„softmaxï¼Œå…¶åˆ†æ¯ä¹Ÿåªæ˜¯$\mathcal{O}(n)$çš„é‡çº§è€Œå·²ï¼Œè®¾æˆ$n^2$å®åœ¨æ„Ÿè§‰ä¸ç§‘å­¦ã€‚ç¬”è€…ä¹Ÿç®€å•åšè¿‡å¯¹æ¯”å®ç°ï¼Œå‘ç°åœ¨512é•¿åº¦ä¸‹$\frac{1}{ns}$ç‰ˆæœ¬è¿˜è½»å¾®å¥½ç‚¹ï¼Œæ‰€ä»¥è¿™é‡Œå°±æŒ‰ç¬”è€…çš„ç›´è§‰æ¥ä»‹ç»äº†ã€‚

[![GAUç¤ºæ„å›¾åŠå…¶ä¼ªä»£ç ](/usr/uploads/2022/02/1677181970.png)](/usr/uploads/2022/02/1677181970.png "ç‚¹å‡»æŸ¥çœ‹åŸå›¾")

GAUç¤ºæ„å›¾åŠå…¶ä¼ªä»£ç 

### ä»¥ä¸€å½“å #

æ¥ä¸‹æ¥è¯·å„ä½çœ‹å®˜ä¸è¦çœ¨çœ¼äº†ï¼ŒçœŸæ­£çš„â€œé‡ç£…â€è¦ç™»åœºäº†ï¼å¯èƒ½GLUçœŸçš„å¤ªå¼ºäº†ï¼Œå®ƒå¯¹Attentionçš„ä¾èµ–çœŸçš„éå¸¸éå¸¸å¼±ï¼Œä»¥è‡³äºä½œè€…ä»¬å‘ç°ï¼š**åªç”¨ä¸€ä¸ªå¤´å°±å¤Ÿäº†ï¼**

[![GAUä¸å¤šå¤´æ³¨æ„åŠ›çš„ä¸€äº›æ¶ˆèåˆ†æ](/usr/uploads/2022/02/1593889218.png)](/usr/uploads/2022/02/1593889218.png "ç‚¹å‡»æŸ¥çœ‹åŸå›¾")

GAUä¸å¤šå¤´æ³¨æ„åŠ›çš„ä¸€äº›æ¶ˆèåˆ†æ

æˆ‘ä»¬çŸ¥é“æ ‡å‡†çš„Transformerç”¨çš„æ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è¿ç®—è¿‡ç¨‹ä¸­éœ€è¦äº§ç”Ÿ$bhn^2$å¤§å°çš„çŸ©é˜µï¼Œ$b$æ˜¯batch_sizeè€Œ$h$æ˜¯å¤´æ•°ï¼Œè¯•æƒ³ä¸€ä¸‹ï¼Œå½“$n=1000$ã€$n=2000$ç”šè‡³æ›´å¤§æ—¶ï¼Œ$n^2$å·²ç»å¤Ÿâ€œæƒ¨â€çš„äº†ï¼Œè¿˜è¦æ´»ç”Ÿç”Ÿåœ°ä¹˜ä¸ª$h$ï¼Œä¸ç®¡å¯¹æ—¶é—´è¿˜æ˜¯ç©ºé—´å¤æ‚åº¦æ— ç–‘éƒ½æ˜¯â€œé›ªä¸ŠåŠ éœœâ€ã€‚è€Œå¦‚ä»Šï¼Œåªè¦ä¸€ä¸ªå¤´çš„GAUï¼Œå°±å¯ä»¥è¾¾åˆ°ç›¸åŒç”šè‡³æ›´å¥½çš„æ•ˆæœï¼Œä¸ä»…æé«˜äº†è®¡ç®—é€Ÿåº¦ï¼Œè¿˜é™ä½äº†æ˜¾å­˜å ç”¨é‡ï¼Œå‡ ä¹ç®—å¾—ä¸Šæ˜¯â€œå…è´¹çš„åˆé¤â€äº†ã€‚

å½“GAUåªæœ‰ä¸€ä¸ªå¤´æ—¶ï¼Œ$\boldsymbol{W}_z$çš„å‚æ•°é‡å°±å¾ˆå°‘äº†ï¼Œä¸»è¦å‚æ•°é‡åœ¨$\boldsymbol{W}_u,\boldsymbol{W}_v,\boldsymbol{W}_o$ä¸Šï¼Œæ‰€ä»¥GAUçš„å‚æ•°é‡å¤§çº¦ä¸º$3de$ï¼›è€Œåœ¨æ ‡å‡†çš„Transformerä¸­ï¼ŒAttentionçš„å‚æ•°é‡ä¸º$4d^2$ï¼ŒFFNçš„å‚æ•°é‡ä¸º$8d^2$ï¼ˆæ ‡å‡†FFNä¸­ä¸€èˆ¬æ˜¯$e=4d$ï¼‰ï¼Œæ‰€ä»¥æ€»å‚æ•°é‡ä¸º$12d^2$ã€‚å› æ­¤ï¼Œä»å‚æ•°é‡çœ‹ï¼Œå½“$e=2d$æ—¶ï¼Œä¸¤å±‚GAUå¤§è‡´ä¸Šå°±ç­‰äºåŸæ¥çš„Attention+FFNã€‚

æ‰€ä»¥ï¼Œåœ¨GAUçš„å®éªŒä¸­ï¼Œä½œè€…éƒ½å›ºå®š$e=2d$ï¼Œé‚£ä¹ˆâ€œ$n$å±‚Attention+$n$å±‚FFNâ€çš„æ ‡å‡†Transformeræ¨¡å‹ï¼Œå¯¹åº”çš„å°±æ˜¯â€œ$2n$å±‚GAUâ€çš„æ–°æ¨¡å‹ï¼Œæˆ‘ä»¬è®°ä¸ºFLASH-Quadï¼Œå…¶ä¸­Quadæ˜¯â€œQuadraticâ€çš„ç®€å†™ï¼Œè¡¨æ˜å¤æ‚åº¦ä¾ç„¶æ˜¯äºŒæ¬¡çš„ï¼Œè‡³äºFLASHçš„å«ä¹‰ï¼Œåé¢å†è°ˆã€‚

## é«˜æ•ˆçº¿æ€§ #

å…¶å®FLASH-Quadå·²ç»æ˜¯æ ‡å‡†Transformerçš„ä¸€ä¸ªéå¸¸ä¼˜ç§€çš„æ›¿ä»£å“äº†ï¼Œä½†ä½œè€…ä»¬è¿˜ä¸æ»¡æ„å…¶äºŒæ¬¡å¤æ‚åº¦ï¼Œç»§è€Œæå‡ºäº†å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„FLASHï¼ˆFast Linear Attention with a Single Headï¼‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§â€œåˆ†å—æ··åˆæ³¨æ„åŠ›ï¼ˆMixed Chunk Attentionï¼‰â€çš„æ–¹æ¡ˆï¼Œå®ƒä¸å•å¯ä»¥ç”¨äºå‰è¿°GAUä¸­ï¼Œä¹Ÿå¯ä»¥ç”¨äºæ ‡å‡†çš„Attentionä¸­ï¼Œæ˜¯ä¸€ç§è¾ƒä¸ºé€šç”¨çš„çº¿æ€§åŒ–æŠ€å·§ã€‚

### ç°æœ‰æ–¹æ³• #

ä¸»æµçš„é«˜æ•ˆTransformerå·¥ä½œå¯¹Attentionçš„æ”¹è¿›æ€è·¯å¤§ä½“ä¸Šå¯ä»¥ä¸¤å¤§ç±»ï¼Œåˆ†åˆ«æ˜¯â€œç¨€ç–åŒ–â€å’Œâ€œçº¿æ€§åŒ–â€ã€‚

æœ¬æ–‡å¼€å¤´æåˆ°çš„[ã€Šä¸ºèŠ‚çº¦è€Œç”Ÿï¼šä»æ ‡å‡†Attentionåˆ°ç¨€ç–Attentionã€‹](/archives/6853)ï¼Œå°±æ˜¯â€œç¨€ç–åŒ–â€çš„å·¥ä½œä¹‹ä¸€ï¼Œåé¢è¯¸å¦‚[Reformer](https://papers.cool/arxiv/2001.04451)ç­‰ä¹Ÿç®—æ˜¯æ­¤åˆ—ï¼Œè¿˜æœ‰ä¸€äº›è·ŸPoolingç»“åˆçš„å¦‚[Linformer](https://papers.cool/arxiv/2006.04768)ä¹Ÿå¯ä»¥ç†è§£ä¸ºå¹¿ä¹‰çš„â€œç¨€ç–åŒ–â€ã€‚è¿™ç±»å·¥ä½œçš„ç‰¹ç‚¹æ˜¯å¼•å…¥ä¸€å®šçš„å½’çº³å…ˆéªŒï¼Œå¼ºåˆ¶å¤§éƒ¨åˆ†æ³¨æ„åŠ›ä¸º0ï¼Œä»è€Œç†è®ºä¸Šå¯ä»¥å°‘å‡å°‘è®¡ç®—é‡ã€‚ä½†è¿™ç§æ–¹æ¡ˆçš„ç¼ºç‚¹æ˜¯å¾€å¾€éœ€è¦ä¸“é—¨çš„ç¼–ç¨‹ä¼˜åŒ–æ‰èƒ½å®ç°åŠ é€Ÿï¼Œæˆ–è€…æ˜¯éš¾ä»¥ç”¨æ¥åšDecoderï¼ˆPoolingç±»å·¥ä½œï¼‰ï¼Œæ­¤å¤–æ•ˆæœå¥½åæ¯”è¾ƒä¾èµ–äºå…¶å¼•å…¥çš„å½’çº³å…ˆéªŒï¼Œæ˜¾å¾—ä¸å¤Ÿè‡ªç„¶ã€‚

è‡³äºâ€œçº¿æ€§åŒ–â€ï¼Œæˆ‘ä»¬åœ¨[ã€Šçº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿã€‹](/archives/7546)æœ‰è¿‡ä»‹ç»ï¼Œç ”ç©¶çš„äººç›¸å¯¹å¤šä¸€äº›ï¼Œåé¢çš„[Performer](/archives/7921)ã€[NystrÃ¶mformer](/archives/8180)ä»¥åŠæœ€è¿‘çš„[cosFormer](https://papers.cool/arxiv/2202.08791)ã€[Flowformer](https://papers.cool/arxiv/2202.06258)éƒ½å¯ä»¥å½’å…¥æ­¤ç±»ã€‚ç®€å•æ¥çœ‹ï¼Œè¿™ç±»å·¥ä½œæ˜¯å°†æ ‡å‡†Attentionçš„$\phi(\boldsymbol{Q}\boldsymbol{K}^{\top})\boldsymbol{V}$æ”¹ä¸º$(\phi_q(\boldsymbol{Q})\phi_k(\boldsymbol{K})^{\top})\boldsymbol{V}=\phi_q(\boldsymbol{Q})(\phi_k(\boldsymbol{K})^{\top}\boldsymbol{V})$ä»è€Œå®ç°äº†çº¿æ€§å¤æ‚åº¦ã€‚è¿™ç±»æ–¹æ³•çš„å¥½å¤„æ˜¯æ˜“äºå®ç°ï¼Œä½†æœ‰ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼Œä¸€æ˜¯ä½ç§©æ€§ä¼šå¯¼è‡´æ•ˆæœæ˜æ˜¾å˜å·®ï¼ˆå‚è€ƒ[ã€ŠTransformerå‡çº§ä¹‹è·¯ï¼š3ã€ä»Performeråˆ°çº¿æ€§Attentionã€‹](/archives/8338)ï¼‰ï¼›å¦å¤–æ˜¯ç”¨æ¥åšDecoderï¼ˆCausalï¼‰æ—¶ä¼šç‰ºç‰²è®­ç»ƒå¹¶è¡Œæ€§ï¼Œå› ä¸ºå®ƒéœ€è¦è½¬åŒ–ä¸ºRNNæ¥è®¡ç®—ï¼Œåˆæˆ–è€…ä¸ç‰ºç‰²å¹¶è¡Œæ€§ï¼Œä½†éœ€è¦$bhns^2$çš„ç©ºé—´å¤æ‚åº¦ï¼Œç›¸æ¯”äºæ ‡å‡†Attentionçš„$bhn^2$ï¼Œèµ·ç è¦$n \gg s^2$æ‰æœ‰ä¼˜åŠ¿ï¼Œè€Œå“ªæ€•$s=64$ï¼Œéƒ½è¦$n \gg 4096$äº†ï¼Œå¤šæ•°æƒ…å†µä¸‹ä¸ç°å®ã€‚

### åˆ†å—æ··åˆ #

FLASHé‡‡å–äº†â€œå±€éƒ¨-å…¨å±€â€åˆ†å—æ··åˆçš„æ–¹å¼ï¼Œç»“åˆäº†â€œç¨€ç–åŒ–â€å’Œâ€œçº¿æ€§åŒ–â€çš„ä¼˜ç‚¹ã€‚é¦–å…ˆï¼Œå¯¹äºé•¿åº¦ä¸º$n$çš„è¾“å…¥åºåˆ—ï¼Œæˆ‘ä»¬å°†å®ƒä¸é‡å åœ°åˆ’åˆ†ä¸º$n/c$ä¸ªé•¿åº¦ä¸º$c$çš„å—ï¼ˆä¸å¤±ä¸€èˆ¬æ€§ï¼Œå‡è®¾$c$èƒ½è¢«$n$æ•´é™¤ï¼Œè®ºæ–‡å–$c=256$ï¼‰ï¼Œè®¾$\boldsymbol{U}_g,\boldsymbol{V}_g\in\mathbb{R}^{c\times e},\boldsymbol{Z}_g\in\mathbb{R}^{c\times s}$ä¸ºç¬¬$g$å—ï¼Œå…¶ä¸­$\boldsymbol{U},\boldsymbol{V},\boldsymbol{Z}$çš„å®šä¹‰åŒå‰ã€‚è·Ÿå¼$\eqref{eq:relu-att}$ä¸€æ ·ï¼Œæˆ‘ä»¬å°†$\boldsymbol{Z}_g$é€šè¿‡4ä¸ªç®€å•çš„ä»¿å°„å˜æ¢åˆ†åˆ«å¾—åˆ°$\boldsymbol{Q}_g^{\text{quad}},\boldsymbol{K}_g^{\text{quad}},\boldsymbol{Q}_g^{\text{lin}},\boldsymbol{K}_g^{\text{lin}}$ã€‚

å…¶ä¸­$\boldsymbol{Q}_g^{\text{quad}},\boldsymbol{K}_g^{\text{quad}}$æˆ‘ä»¬ç”¨æ¥ç®—å—å†…çš„è‡ªæ³¨æ„åŠ›ï¼š  
\begin{equation}\hat{\boldsymbol{V}}_g^{\text{quad}}=\frac{1}{cs}\text{relu}^2\left(\boldsymbol{Q}_g^{\text{quad}}{\boldsymbol{K}_g^{\text{quad}}}^{\top}\right)\boldsymbol{V}_g\end{equation}  
è¿™ä»£è¡¨çš„æ˜¯æ¯ä¸ªå—çš„tokenå†…éƒ¨è‡ªè¡Œäº¤äº’ï¼Œæœ¬è´¨ä¸Šä¹Ÿç®—æ˜¯â€œç¨€ç–åŒ–â€çš„ä¸€ç§ï¼Œå…¶å¤æ‚åº¦å¤§è‡´æ˜¯$\mathcal{O}(n/c\times c^2)=\mathcal{O}(nc)$ï¼Œæ­£æ¯”äº$n$ã€‚å®ç°æ—¶ç›¸å½“äºå¤´æ•°ä¸º$n/c$ã€åºåˆ—é•¿åº¦ä¸º$c$çš„å¤šå¤´æ³¨æ„åŠ›ï¼Œå¯ä»¥å……åˆ†åœ°å¹¶è¡Œï¼Œè€Œå¦‚æœæƒ³è¦åšDecoderï¼Œé‚£ä¹ˆmaskæ‰æ³¨æ„åŠ›çŸ©é˜µçš„ä¸Šä¸‰è§’éƒ¨åˆ†å³å¯ã€‚

å‰©ä¸‹çš„$\boldsymbol{Q}_g^{\text{lin}},\boldsymbol{K}_g^{\text{lin}}$åˆ™ç”¨æ¥åšå…¨å±€çš„Attentionï¼Œæˆ‘ä»¬ç›´æ¥ç”¨å‰è¿°çº¿æ€§Attentionçš„æ–¹å¼æ¥åšï¼š  
\begin{equation}\hat{\boldsymbol{V}}_g^{\text{lin}}=\frac{1}{n}\boldsymbol{Q}_g^{\text{lin}}\sum_{h=1}^{n/c} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h\end{equation}  
æ³¨æ„ï¼Œè¿™ä¸ªæ“ä½œè·Ÿç›´æ¥ç”¨å®Œæ•´çŸ©é˜µ$\boldsymbol{Q}^{\text{lin}},\boldsymbol{K}^{\text{lin}}\in\mathbb{R}^{n\times s}$ä¸$\boldsymbol{V}$åšçº¿æ€§Attentionæ˜¯å®Œå…¨ç­‰ä»·çš„ï¼Œå†™æˆè¿™æ ·åªæ˜¯æ›´å¥½åœ°ä½“ç°è·Ÿåˆ†å—çš„è”ç³»ã€‚å¦‚æœæ˜¯åšDecoderï¼Œé‚£ä¹ˆè¦é˜²æ­¢æ³„æ¼æœªæ¥ä¿¡æ¯ï¼Œæ‰€ä»¥è¦æ”¹ä¸ºcumsumå½¢å¼ï¼š  
\begin{equation}\hat{\boldsymbol{V}}_g^{\text{lin}}=\frac{1}{(g-1)n/c}\boldsymbol{Q}_g^{\text{lin}}\sum_{h=1}^{g-1} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h\end{equation}  
è¿™ç§æƒ…å†µä¸‹ï¼Œä¸ºäº†ä¿æŒå¹¶è¡Œæ€§ï¼Œæˆ‘ä»¬åªéœ€è¦$b(n/c)se$çš„ç©ºé—´å¤æ‚åº¦ï¼Œè€Œå¦‚æœä¸åˆ†å—ç›´æ¥ç”¨çº¿æ€§Attentionï¼Œé‚£ä¹ˆæ˜¯$bns^2$ï¼ˆè¦æ˜¯åŸå§‹çš„ç”¨æ³•è¿˜è¦åŠ ä¸Šå¤šå¤´ï¼Œé‚£å°±æ˜¯$bhns^2$ï¼‰ï¼Œåœ¨å½“å‰å‚æ•°è®¾ç½®ä¸‹æœ‰$e/c\ll s$ï¼Œæ‰€ä»¥æ˜¯æ›´çœæ˜¾å­˜äº†ã€‚

æœ€åï¼Œå°†ä¸¤ç§Attentionç»“æœç»“åˆèµ·æ¥ï¼Œæ•´åˆåˆ°GAUä¸­ï¼Œå¾—åˆ°çº¿æ€§ç‰ˆæœ¬çš„GAU  
\begin{equation}\boldsymbol{O}_g=\left[\boldsymbol{U}_g\odot\left(\hat{\boldsymbol{V}}_g^{\text{quad}} + \hat{\boldsymbol{V}}_g^{\text{lin}}\right)\right]\boldsymbol{W}_o\end{equation}  
åŸºäºçº¿æ€§ç‰ˆæœ¬GAUæ­å»ºçš„Transformeræ¨¡å‹ï¼Œä¾¿æ˜¯ä½œè€…ç¬”ä¸‹çš„FLASHæ¨¡å‹äº†ã€‚

### ä¸€äº›è®¨è®º #

ç¬”è€…è®¤ä¸ºï¼Œä¹‹æ‰€ä»¥è¿™æ ·åˆ†å—åšâ€œå±€éƒ¨-å…¨å±€â€çš„æ··åˆæ³¨æ„åŠ›ï¼Œé™¤äº†æ˜¯æƒ³é™ä½è®¡ç®—æˆæœ¬å¤–ï¼Œè¿˜å› ä¸ºè¿™æ ·åšèƒ½å¾—åˆ°æ›´è´´åˆå®é™…æƒ…å†µçš„æ³¨æ„åŠ›åˆ†å¸ƒã€‚æŒ‰ç…§æˆ‘ä»¬å¯¹NLPçš„ç»éªŒç†è§£ï¼Œè‡ªç„¶è¯­è¨€ä¸­çš„å…³è”ä¸»è¦è¿˜æ˜¯é›†ä¸­åœ¨å±€éƒ¨çš„ï¼Œè€Œå…¨å±€çš„ã€æåº¦é•¿è·ç¦»çš„å…³è”è™½ç„¶å­˜åœ¨ï¼Œä½†ä¸ä¼šæ˜¯ä¸»å¯¼åœ°ä½ï¼Œæ‰€ä»¥è¿™ç§æ··åˆå¼çš„æ³¨æ„åŠ›è®¾è®¡æ›´æœ‰åˆ©äºæ¨¡å‹å‡¸å‡ºå±€éƒ¨å…³è”ä½†ä¸èˆå¼ƒé•¿ç¨‹å…³è”ã€‚åŸè®ºæ–‡è¿˜åšäº†æ¶ˆèå®éªŒï¼Œæ˜¾ç¤ºç›¸å¯¹æ¥è¯´å±€éƒ¨æ³¨æ„åŠ›æ¯”å…¨å±€æ³¨æ„åŠ›æ›´é‡è¦ï¼Œè€Œæ··åˆå¼çš„æ•ˆæœæœ€å¥½ã€‚

[![å…¨å±€æ³¨æ„åŠ›å’Œå±€éƒ¨æ³¨æ„åŠ›çš„æ¶ˆèå®éªŒ](/usr/uploads/2022/02/3956184792.png)](/usr/uploads/2022/02/3956184792.png "ç‚¹å‡»æŸ¥çœ‹åŸå›¾")

å…¨å±€æ³¨æ„åŠ›å’Œå±€éƒ¨æ³¨æ„åŠ›çš„æ¶ˆèå®éªŒ

æ­¤å¤–ï¼Œå¯èƒ½ä¼šæœ‰äº›è¯»è€…æ‹…å¿ƒè¿™ç§éé‡å çš„åˆ†å—ä¼šä¸ä¼šä¸åˆ©äºè¾¹ç•Œè¯çš„é¢„æµ‹ï¼ŸåŸè®ºæ–‡æåˆ°äº†è¿™ä¸€ç‚¹ï¼Œå®ƒè¯´å¼•å…¥æ›´å¤æ‚çš„é‡å å¼å±€éƒ¨æ³¨æ„åŠ›ç¡®å®æœ‰åˆ©äºæå‡æ•ˆæœï¼Œä½†ä¹Ÿå¼•å…¥äº†é¢å¤–çš„è®¡ç®—æˆæœ¬ï¼Œåœ¨å¢åŠ åŒæ ·è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œå¼•å…¥é‡å å¼å±€éƒ¨æ³¨æ„åŠ›å¸¦æ¥çš„å¢ç›Šè¿˜ä¸å¦‚ç›´æ¥å¤šåŠ å‡ å±‚ç›®å‰çš„éé‡å å¼GAUã€‚æ‰€ä»¥è¯´ï¼Œç›®å‰çš„éé‡å è¶³å¤Ÿå¥½åœ°å¹³è¡¡äº†é€Ÿåº¦å’Œæ•ˆæœã€‚

æœ€åï¼Œè¿™ç§â€œåˆ†å—æ··åˆâ€çš„çº¿æ€§åŒ–æ–¹æ¡ˆæœ¬è´¨ä¸Šæ˜¯é€šç”¨çš„ï¼Œå®ƒä¸ä»…å¯ä»¥ç”¨äºGAUä¸­ï¼Œä¹Ÿå¯ä»¥ç”¨äºæ ‡å‡†çš„Transformerä¸­ï¼Œå³ä¿ç•™æ ‡å‡†çš„Attention+FFNç»„åˆï¼Œç„¶åAttentionç”¨åˆ†å—æ··åˆçš„æ–¹å¼è¿›è¡Œçº¿æ€§åŒ–ï¼ŒåŸè®ºæ–‡ç§°ä¹‹ä¸ºâ€œMC-TFMâ€ï¼Œå¹¶ä¹Ÿè¿›è¡Œäº†ç›¸åº”çš„æ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºGAUåœ¨çº¿æ€§åŒ–æ–¹é¢ä¹Ÿæ˜¾å¾—æ›´æœ‰ä¼˜åŠ¿ã€‚

## å®éªŒåˆ†æ #

å…³äºGAUå’ŒFLASHçš„å®éªŒç»“æœï¼Œç¬”è€…è®¤ä¸ºæœ€å€¼å¾—ç•™æ„çš„æœ‰ä¸¤ä¸ªã€‚

ç¬¬ä¸€ä¸ªæ˜¯æ–°è®¾è®¡çš„é—¨æ§æ³¨æ„åŠ›å•å…ƒGAUä¸æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›ä¹‹é—´MHSAçš„æ¯”è¾ƒï¼Œå…¶å®ä¹Ÿå°±æ˜¯FLASH-Quadå’Œæ ‡å‡†Transformerçš„æ¯”è¾ƒäº†ï¼Œå¦‚ä¸‹å›¾ï¼š  


[![GAUä¸å¤šå¤´æ³¨æ„åŠ›çš„å¯¹æ¯”](/usr/uploads/2022/02/1582248173.png)](/usr/uploads/2022/02/1582248173.png "ç‚¹å‡»æŸ¥çœ‹åŸå›¾")

GAUä¸å¤šå¤´æ³¨æ„åŠ›çš„å¯¹æ¯”

æ³¨æ„æ¨ªè½´æ˜¯é€Ÿåº¦ï¼Œçºµè½´æ˜¯æ•ˆæœï¼Œè¿™ç§å›¾è¶Šé è¿‘å³ä¸Šè§’çš„ç‚¹æ„å‘³ç€è¶Šç†æƒ³ï¼ˆé€Ÿåº¦å’Œæ•ˆæœéƒ½æœ€ä¼˜ï¼‰ï¼Œæ‰€ä»¥ä¸Šå›¾æ˜¾ç¤ºä¸ç®¡å“ªç§è§„æ ¼çš„æ¨¡å‹ï¼ŒGAUéƒ½æ¯”ç›¸åº”çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å‹æ›´æœ‰ä¼˜åŠ¿ã€‚

ç¬¬äºŒä¸ªåˆ™æ˜¯FLASHæ¨¡å‹çš„å®éªŒè¡¨æ ¼ï¼š  


[![FLASHä¸æ ‡å‡†Transformerçš„å¯¹æ¯”](/usr/uploads/2022/02/860010088.png)](/usr/uploads/2022/02/860010088.png "ç‚¹å‡»æŸ¥çœ‹åŸå›¾")

FLASHä¸æ ‡å‡†Transformerçš„å¯¹æ¯”

è¯¥è¡¨æ ¼æ›´ç›´æ¥åœ°æ˜¾ç¤ºå‡ºï¼š

> 1ã€å°½ç®¡FLASH-Quadå’ŒTransformeréƒ½æ˜¯äºŒæ¬¡å¤æ‚åº¦ï¼Œä½†FLASH-Quadæ•ˆæœæ›´å¥½ã€é€Ÿåº¦æ›´å¿«ï¼›
> 
> 2ã€åœ¨åºåˆ—è¶³è¾ƒé•¿æ—¶ï¼Œçº¿æ€§å¤æ‚åº¦çš„FLASHæ¯”FLASH-Quadæ›´å¿«ï¼Œå¹¶ä¸”æ•ˆæœç›¸ä»¿ã€‚

è¯´å®è¯ï¼Œå³ä¾¿æ˜¯FLASH-Quadè¿™ä¸ªä¾ç„¶æ˜¯äºŒæ¬¡å¤æ‚åº¦çš„æ¨¡å‹çš„é€Ÿåº¦æå‡å¹…åº¦ï¼Œå¾ˆå¤šå·ç§°æ˜¯çº¿æ€§å¤æ‚åº¦çš„å·¥ä½œéƒ½æœªå¿…èƒ½åšåˆ°ï¼ŒGAUçš„å¼ºå¤§å¯è§ä¸€æ–‘ã€‚å¯¹äº†ï¼Œè®ºæ–‡è¿˜ç‰¹åˆ«æŒ‡å‡ºç¬”è€…ä¹‹å‰æçš„[æ—‹è½¬ä½ç½®ç¼–ç RoPE](/archives/8265)èƒ½æ˜æ˜¾æé«˜Transformerå’ŒFLASHçš„æ•ˆæœï¼Œæ‰€ä»¥è®ºæ–‡å®éªŒçš„Transformer+ã€Transformer++ã€FLASH-Quadå’ŒFLASHéƒ½æ˜¯å¸¦æœ‰RoPEç¼–ç çš„ï¼Œåœ¨æ­¤æ²¾æ²¾è‡ªå–œä¸€ä¸‹ã€‚

å¦å¤–ï¼Œä¸Šè¿°è¡¨æ ¼å¹¶æ²¡æœ‰ç»™å‡ºæ˜¾å­˜å ç”¨çš„å¯¹æ¯”ã€‚äº‹å®ä¸Šï¼Œç¬”è€…æµ‹è¯•å‘ç°ï¼Œåœ¨baseé‡çº§å’Œåºåˆ—é•¿åº¦ä¸º1024æ—¶ï¼ŒFLASH-Quadå¯ç”¨çš„æœ€å¤§batch_sizeå°†è¿‘æ˜¯Transformerçš„ä¸¤å€ï¼Œè¿™æ„å‘³ç€FLASH-Quadæ˜æ˜¾é™ä½äº†æ˜¾å­˜æ¶ˆè€—ã€‚åŒæ—¶ï¼Œç¬”è€…ç®€å•å°è¯•äº†smallç‰ˆæœ¬FLASH-Quadçš„ä¸­æ–‡é¢„è®­ç»ƒï¼Œå‘ç°æ•ˆæœç”šè‡³æ¯”RoFormerï¼ˆRoPE+Transformerï¼‰è¦å¥½äº›ï¼Œæ‰€ä»¥è®ºæ–‡æ‰€æŠ¥å‘Šçš„ç»“æœç¡®å®ä¸è™šã€‚ä¸è¿‡æœ€è¿‘çš„å¡æœ‰é™ï¼Œå°±æ²¡æ³•è¿›è¡Œæ›´æ·±å…¥çš„æµ‹è¯•äº†ï¼Œä»¥åæœ‰æ–°ç»“æœå†è·Ÿå¤§å®¶åˆ†äº«ã€‚

## å»¶ä¼¸æ€è€ƒ #

è‡³æ­¤ï¼Œå¯¹GAUã€FLASHçš„ä»‹ç»ä¹ŸåŸºæœ¬ç»“æŸäº†ã€‚åˆ°å‘åšå®¢æ—¶ï¼Œä½œè€…è¿˜æ²¡æœ‰åœ¨Gihubä¸Šå¼€æ”¾å®Œæ•´æºä»£ç ï¼Œä½†æ˜¯é™„å½•å·²ç»è´´å‡ºäº†å‡ ä¹å¯ä»¥ç›´æ¥æŠ„æ¥ç”¨çš„å…³é”®æºç ï¼ˆtensorflowç‰ˆï¼‰ï¼Œæ‰€ä»¥ä»£ç çš„å®ç°åº”ä½†æ˜¯æ²¡æœ‰å›°éš¾çš„ï¼Œæœ‰å…´è¶£æœ‰ç®—åŠ›çš„åŒå­¦ï¼Œå¯ä»¥è‡ªè¡Œå‚è€ƒå®éªŒã€‚å¦å¤–è®ºæ–‡æœ‰ä»€ä¹ˆè¯»ä¸æ‡‚çš„åœ°æ–¹ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å‚è€ƒæºä»£ç ã€‚

ä¸‹é¢è¿›è¡Œâ€œæŒ‘éª¨å¤´â€ç¯èŠ‚ï¼Œè¯´ä¸€ä¸‹æˆ‘è§‰å¾—è¿™ç¯‡è®ºæ–‡è¿˜åšçš„ä¸å¤Ÿå®Œç¾çš„åœ°æ–¹ã€‚

é¦–å…ˆï¼Œç¬”è€…è®¤ä¸ºFLASH-Quadå’ŒFLASHè§£è€¦å¾—ä¸å¤Ÿå¥½ã€‚å¦‚æœ¬æ–‡å¼€å¤´çš„è§‚ç‚¹ï¼ŒFLASH-Quadå’ŒFLASHéƒ½ç®—å¾—ä¸Šæ˜¯â€œé‡ç£…â€çº§åˆ«çš„ç»“æœï¼Œç”šè‡³å¯¹ç¬”è€…æ¥è¯´FLASH-Quadæ›´æœ‰ä»·å€¼ï¼Œå› ä¸ºè‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦æœ¬èº«ä¹Ÿå¸¦æ¥äº†è¶³å¤Ÿå¤šçš„è‡ªç”±åº¦ï¼Œå¯ä»¥ç©å¾ˆå¤šåƒ[UniLM](/archives/6933)è¿™æ ·çš„èŠ±æ ·ï¼Œæ‰€ä»¥FLASH-Quadæœ¬èº«åº”è¯¥æ˜¯ä¸€ä¸ªå¾ˆç‹¬ç«‹ã€å¾ˆå€¼å¾—è‚¯å®šçš„æ¨¡å‹ï¼Œä½†åœ¨åŸè®ºæ–‡ä¸­ï¼Œå®ƒæ›´åƒæ˜¯FLASHçš„ä¸€ä¸ªè¿‡æ¸¡äº§å“ï¼Œè¿™æˆ‘è®¤ä¸ºæ˜¯è¿‡äºâ€œå†·è½â€äº†FLASH-Quadã€‚å¹¸å¥½ï¼Œä½œè€…å•ç‹¬åˆ†ç¦»å‡ºäº†GAUçš„æ¦‚å¿µï¼Œä¹Ÿç®—æ˜¯ç¼“è§£äº†è¿™ä¸ªä¸è¶³ã€‚

ç„¶åï¼ŒGAUæ—¢å¯ä»¥ä»£æ›¿Attentionï¼Œä¹Ÿå¯ä»¥ä»£æ›¿FFNï¼Œä»è®¾è®¡ä¸Šæ¥çœ‹ï¼Œå®ƒæ—¨åœ¨ä»£æ›¿çš„æ˜¯Self-Attentionï¼Œä½œè€…ä¼¼ä¹ä¸å…³å¿ƒå®ƒå¯¹Cross Attentionçš„å¯ä»£æ›¿æ€§ï¼Œè®ºæ–‡ä¹Ÿæ²¡æœ‰ç›¸åº”çš„å®éªŒã€‚é‚£ä¹ˆï¼ŒGAUæ˜¯å¦æœ‰å¯èƒ½ä»£æ›¿Cross Attentionå‘¢ï¼Ÿä»å¼$\eqref{eq:mix}$çš„å½¢å¼çœ‹ï¼Œç†è®ºä¸Šæ˜¯æœ‰å¯èƒ½çš„ï¼Œä½†ä¸çŸ¥é“GAUä»£æ›¿Cross Attentionæ—¶èƒ½å¦ä¾ç„¶åªä¿ç•™ä¸€ä¸ªå¤´ï¼Œå› ä¸ºåªéœ€ä¸€ä¸ªå¤´å¯è°“æ˜¯GAUæ›¿ä»£Self Attentionçš„æœ€å¤§äº®ç‚¹äº†ï¼Œå®ƒæ˜¯æ›´å¿«æ›´çœçš„å…³é”®ã€‚æ­¤å¤–ï¼Œè®ºæ–‡åªåšäº†LMå’ŒMLMçš„è¯­è¨€æ¨¡å‹å®éªŒï¼Œå¹¶æ²¡æœ‰åšâ€œé¢„è®­ç»ƒ+å¾®è°ƒâ€çš„å®éªŒï¼Œä¸ç¡®å®šGAUçš„è¿ç§»æ€§èƒ½å¦‚ä½•ã€‚æˆ–è®¸ç­‰æˆ‘æœ‰å¡äº†ï¼Œæˆ‘ä¹Ÿå»è¡¥å……ä¸€æ³¢å®éªŒã€‚

æœ€åï¼Œæœ‰ä¸€ä¸ªç¬”è€…ä¸å¤§ç†è§£çš„åœ°æ–¹ï¼Œå°±æ˜¯GAU/FLASH-Quad/FLASHåŒæ—¶ç”¨ä¸Šäº†åŠ æ€§ç»å¯¹ã€åŠ æ€§ç›¸å¯¹ä»¥åŠRoPEä¸‰ç§ä½ç½®ç¼–ç ï¼Œç†è®ºä¸Šä¸‰è€…åªç”¨å…¶ä¸€å°±è¡Œäº†ï¼Œç¬”è€…è‡ªå·±åšçš„GAUå®éªŒä¹Ÿåªç”¨RoPEä½†æ•ˆæœä¾ç„¶æŒºå¥½ï¼Œæ‰€ä»¥è¿™é‡ŒåŒæ—¶ç”¨ä¸‰ç§æœ‰ä»€ä¹ˆè®²ç©¶å—ï¼Ÿæœ€åï¼Œä»è®ºæ–‡é™„å½•æ‰€ç»™çš„æºç çœ‹ï¼Œä½œè€…å¹¶æ²¡æœ‰ä»”ç»†å¤„ç†å¥½paddingçš„é—®é¢˜ï¼Œä»¥åŠåšDecoderæ˜¯å½’ä¸€åŒ–å› å­é€’å½’ä¹Ÿæ²¡æœ‰å†™å¥½ï¼ˆå‰$t$é¡¹æ±‚å’Œåº”è¯¥é™¤ä»¥$t$è€Œä¸æ˜¯$n$ï¼‰ï¼Œè¿™äº›éƒ½æ˜¯ä¸å¤§ä¸å°çš„å¯æ”¹å–„çš„ç»†èŠ‚ã€‚å½“ç„¶ï¼Œä¸æ’é™¤ä½œè€…çš„åŸå§‹ä»£ç æ˜¯æ­£ç¡®çš„ï¼Œé™„å½•åªæ˜¯å‡ºäºå¯è¯»æ€§ç›®çš„åšäº†ç®€åŒ–ï¼Œå› ä¸ºé™„å½•é‡Œè¾¹çš„ä»£ç è¿˜æ˜¯ä»¥â€œä¼ªä»£ç â€è‡ªç§°ã€‚

## æœ¬æ–‡å°ç»“ #

æœ¬æ–‡ä»‹ç»äº†Googleæ–°å‡ºçš„ä¸€ä¸ªé«˜æ•ˆTransformerå·¥ä½œï¼Œé‡Œè¾¹å°†Attentionå’ŒFFNèåˆä¸ºä¸€ä¸ªæ–°çš„GAUå±‚ï¼Œä»è€Œå¾—åˆ°äº†Transformerå˜ä½“FLASH-Quadï¼Œä½œè€…è¿˜è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§â€œåˆ†å—æ··åˆâ€çº¿æ€§åŒ–æ–¹æ¡ˆï¼Œå¾—åˆ°äº†å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„FLASHã€‚ç›®å‰çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç®¡FLASH-Quadè¿˜æ˜¯FLASHï¼Œè·Ÿæ ‡å‡†Transformerç›¸æ¯”éƒ½æ˜¯æ›´å¿«ã€æ›´çœã€æ›´å¥½ã€‚ä¹Ÿè®¸ä¸ä¹…ä¹‹åï¼ŒAll You Needçš„å°±ä¸å†æ˜¯Attentionè€Œæ˜¯GAUäº†ã€‚

_**è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š**<https://spaces.ac.cn/archives/8934>_

_**æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š**_[ã€Šç§‘å­¦ç©ºé—´FAQã€‹](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "ã€Šç§‘å­¦ç©ºé—´FAQã€‹")

**å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚**

**å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼**

æ‰“èµ

![ç§‘å­¦ç©ºé—´](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

å¾®ä¿¡æ‰“èµ

![ç§‘å­¦ç©ºé—´](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

æ”¯ä»˜å®æ‰“èµ

å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥[**ç‚¹å‡»è¿™é‡Œ**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚

**å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š**

è‹å‰‘æ—. (Feb. 25, 2022). ã€ŠFLASHï¼šå¯èƒ½æ˜¯è¿‘æ¥æœ€æœ‰æ„æ€çš„é«˜æ•ˆTransformerè®¾è®¡ ã€‹[Blog post]. Retrieved from <https://spaces.ac.cn/archives/8934>

@online{kexuefm-8934,  
title={FLASHï¼šå¯èƒ½æ˜¯è¿‘æ¥æœ€æœ‰æ„æ€çš„é«˜æ•ˆTransformerè®¾è®¡},  
author={è‹å‰‘æ—},  
year={2022},  
month={Feb},  
url={\url{https://spaces.ac.cn/archives/8934}},  
} 



### ç¬¬1éƒ¨åˆ†ï¼šæ ¸å¿ƒç†è®ºã€å…¬ç†ä¸å†å²åŸºç¡€

#### 1.1 ç†è®ºèµ·æºä¸å†å²å‘å±•

<div class="theorem-box">

**FLASH/GAUçš„ç†è®ºæ ¹æº**å¯è¿½æº¯åˆ°å¤šä¸ªç ”ç©¶æ–¹å‘ï¼š

- **é—¨æ§æœºåˆ¶ç†è®º** (2016, Dauphin et al.)ï¼šGLUè¯æ˜é—¨æ§èƒ½æ˜¾è‘—æå‡åºåˆ—å»ºæ¨¡èƒ½åŠ›
- **é«˜æ•ˆTransformerç ”ç©¶** (2019-2021)ï¼šLinformerã€Performerã€NystrÃ¶mformerç­‰çº¿æ€§åŒ–å°è¯•
- **æ³¨æ„åŠ›æœºåˆ¶ç®€åŒ–** (2020)ï¼šå‘ç°Softmaxä¸æ˜¯Attentionçš„å¿…è¦ç»„ä»¶
- **æ··åˆå±€éƒ¨-å…¨å±€æ³¨æ„åŠ›** (2019, Longformer)ï¼šç¨€ç–æ³¨æ„åŠ›æ¨¡å¼çš„æ¢ç´¢
- **ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰** (2021, Primer)ï¼šè‡ªåŠ¨æœç´¢æœ€ä¼˜æ¿€æ´»å‡½æ•°

</div>

**å…³é”®é‡Œç¨‹ç¢‘**ï¼š

1. **2016 - GLU (Facebook)**ï¼šã€ŠLanguage Modeling with Gated Convolutional Networksã€‹
   - é¦–æ¬¡æå‡ºé—¨æ§çº¿æ€§å•å…ƒ
   - åœ¨seq2seqä»»åŠ¡ä¸Šè¶…è¶ŠLSTM
   - ä¸ºGAUçš„é—¨æ§è®¾è®¡å¥ å®šåŸºç¡€

2. **2019 - Sparse Attention (OpenAI)**ï¼šã€ŠGenerating Long Sequences with Sparse Transformersã€‹
   - æå‡ºå±€éƒ¨+å…¨å±€çš„æ··åˆæ³¨æ„åŠ›æ¨¡å¼
   - å¯å‘äº†FLASHçš„åˆ†å—è®¾è®¡

3. **2020 - Linformer (Facebook)**ï¼šã€ŠLinformer: Self-Attention with Linear Complexityã€‹
   - é¦–æ¬¡å®ç°çº¿æ€§å¤æ‚åº¦çš„Attention
   - æ­ç¤ºäº†ä½ç§©è¿‘ä¼¼çš„å¯èƒ½æ€§
   - ä½†å­˜åœ¨æ•ˆæœä¸‹é™é—®é¢˜

4. **2021 - Performer (Google)**ï¼šã€ŠRethinking Attention with Performersã€‹
   - ä½¿ç”¨æ ¸æ–¹æ³•å®ç°çº¿æ€§åŒ–
   - æå‡ºFAVOR+ç®—æ³•
   - ä¸ºçº¿æ€§Attentionæä¾›ç†è®ºæ”¯æ’‘

5. **2021 - Primer (Google)**ï¼šã€ŠPrimer: Searching for Efficient Transformersã€‹
   - é€šè¿‡NASå‘ç°$\text{relu}^2$æ¿€æ´»å‡½æ•°
   - ä¸ºFLASHçš„Attentionè®¾è®¡æä¾›ä¾æ®

6. **2022 - FLASH (Google)**ï¼šã€ŠTransformer Quality in Linear Timeã€‹ â­
   - æå‡ºGAUæ¶æ„
   - å®ç°å•å¤´Attentionçš„çªç ´
   - ç»Ÿä¸€Attentionå’ŒFFN

#### 1.2 æ•°å­¦å…¬ç†ä¸åŸºç¡€å‡è®¾

<div class="theorem-box">

### å…¬ç†1ï¼šé—¨æ§å……åˆ†æ€§å‡è®¾

**è¡¨è¿°**ï¼šåœ¨é—¨æ§æœºåˆ¶è¶³å¤Ÿå¼ºå¤§æ—¶ï¼ŒAttentionæœºåˆ¶å¯ä»¥è¢«å¤§å¹…ç®€åŒ–ã€‚

$$\text{Strong Gating} \implies \text{Simplified Attention}$$

**å…·ä½“åŒ–**ï¼š
- GLUå¼çš„é—¨æ§ï¼š$\boldsymbol{U} \odot \boldsymbol{V}$
- å…è®¸ä½¿ç”¨ç®€åŒ–çš„å•å¤´Attention
- æ¿€æ´»å‡½æ•°å¯ä»¥ä»Softmaxé™çº§ä¸º$\text{relu}^2$

**æ¨è®º**ï¼šå¤šå¤´Attentionçš„å¿…è¦æ€§é™ä½ï¼Œå•å¤´å³å¯è¾¾åˆ°ç›¸åŒæ•ˆæœã€‚

</div>

<div class="theorem-box">

### å…¬ç†2ï¼šå±€éƒ¨-å…¨å±€åˆ†ç¦»åŸç†

**è¡¨è¿°**ï¼šè‡ªç„¶è¯­è¨€çš„ä¾èµ–å…³ç³»ä¸»è¦é›†ä¸­åœ¨å±€éƒ¨ï¼Œå…¨å±€å…³è”è™½å­˜åœ¨ä½†ä¸å ä¸»å¯¼ã€‚

$$\text{Language Dependency} \approx \alpha \cdot \text{Local} + (1-\alpha) \cdot \text{Global}, \quad \alpha > 0.5$$

**åˆ†å—ç­–ç•¥**ï¼š
- å±€éƒ¨æ³¨æ„åŠ›ï¼ˆå—å†…ï¼‰ï¼š$\mathcal{O}(c^2)$ å¤æ‚åº¦ï¼Œ$c$ä¸ºå—å¤§å°
- å…¨å±€æ³¨æ„åŠ›ï¼ˆçº¿æ€§åŒ–ï¼‰ï¼š$\mathcal{O}(n)$ å¤æ‚åº¦
- æ€»å¤æ‚åº¦ï¼š$\mathcal{O}(nc)$ï¼Œçº¿æ€§äºåºåˆ—é•¿åº¦$n$

</div>

<div class="theorem-box">

### å…¬ç†3ï¼šæ³¨æ„åŠ›-FFNç­‰ä»·æ€§åŸç†

**è¡¨è¿°**ï¼šé€‚å½“è®¾è®¡çš„é—¨æ§æ³¨æ„åŠ›å•å…ƒå¯ä»¥åŒæ—¶æ‰®æ¼”Attentionå’ŒFFNçš„è§’è‰²ã€‚

$$\text{GAU}(\boldsymbol{X}) \equiv \text{Attention}(\boldsymbol{X}) \oplus \text{FFN}(\boldsymbol{X})$$

**æ•°å­¦å½¢å¼**ï¼š

$$\boldsymbol{O} = (\boldsymbol{U} \odot \boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o$$

å…¶ä¸­ï¼š
- å½“$\boldsymbol{A} = \boldsymbol{I}$ â†’ GLUå¼FFN
- å½“$\boldsymbol{U} = \boldsymbol{1}$ â†’ æ ‡å‡†Attention
- å½“$\boldsymbol{A} \neq \boldsymbol{I}, \boldsymbol{U} \neq \boldsymbol{1}$ â†’ èåˆå½¢å¼

</div>

#### 1.3 è®¾è®¡å“²å­¦

FLASH/GAUçš„æ ¸å¿ƒè®¾è®¡å“²å­¦æ˜¯**"ç®€åŒ–è€Œä¸ç‰ºç‰²"**ä¸**"èåˆè€Œä¸å†—ä½™"**ï¼š

**ç®€åŒ–è€Œä¸ç‰ºç‰²ï¼ˆSimplify without Sacrificeï¼‰**ï¼š
- **ç®€åŒ–å¤šå¤´**ï¼šä»8-16å¤´ â†’ å•å¤´
- **ç®€åŒ–æ¿€æ´»**ï¼šä»Softmax â†’ $\text{relu}^2$
- **ç®€åŒ–å½’ä¸€åŒ–**ï¼šä»LayerNorm â†’ ç®€å•çš„$1/n$ç¼©æ”¾
- **ç»“æœ**ï¼šé€Ÿåº¦æå‡2-3å€ï¼Œæ•ˆæœä¸é™åå‡

**èåˆè€Œä¸å†—ä½™ï¼ˆMerge without Redundancyï¼‰**ï¼š
- ä¼ ç»ŸTransformerï¼šAttentionå±‚ + FFNå±‚ï¼ˆä¸²è¡Œå †å ï¼‰
- FLASHï¼šGAUå±‚ï¼ˆå•å±‚èåˆï¼‰
- å‚æ•°é‡ï¼š$2n$å±‚GAU â‰ˆ $n$å±‚Attention + $n$å±‚FFN
- é¿å…äº†ä¿¡æ¯åœ¨ä¸¤ç§å±‚ä¹‹é—´çš„å†—ä½™ä¼ é€’

**æƒè¡¡çš„è‰ºæœ¯ï¼ˆArt of Trade-offsï¼‰**ï¼š
- å±€éƒ¨ vs å…¨å±€ï¼šåˆ†å—å¤§å°$c=256$çš„é€‰æ‹©
- ç²¾åº¦ vs é€Ÿåº¦ï¼š$\text{relu}^2$çš„éå¹³æ»‘æ€§ vs è®¡ç®—æ•ˆç‡
- å‚æ•° vs è®¡ç®—ï¼šå¤§å‚æ•°é‡ï¼ˆ$e=2d$ï¼‰ä½†ä½è®¡ç®—é‡ï¼ˆå•å¤´ï¼‰


---

### ç¬¬2éƒ¨åˆ†ï¼šä¸¥è°¨çš„æ ¸å¿ƒæ•°å­¦æ¨å¯¼

### 1. æ ‡å‡†Transformerçš„FFNå±‚æ•°å­¦å½¢å¼

**å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰çš„æ ‡å‡†å½¢å¼**ï¼š

æ ‡å‡†çš„FFNåŒ…å«ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼š

\begin{equation}
\text{FFN}(\boldsymbol{X}) = \phi(\boldsymbol{X}\boldsymbol{W}_1 + \boldsymbol{b}_1)\boldsymbol{W}_2 + \boldsymbol{b}_2 \tag{1}
\end{equation}

å…¶ä¸­ï¼š
- $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ï¼šè¾“å…¥çŸ©é˜µï¼ˆ$n$ä¸ªtokenï¼Œæ¯ä¸ªç»´åº¦$d$ï¼‰
- $\boldsymbol{W}_1 \in \mathbb{R}^{d \times e}$ï¼šç¬¬ä¸€å±‚æƒé‡ï¼ˆæ‰©å±•åˆ°$e$ç»´ï¼Œé€šå¸¸$e = 4d$ï¼‰
- $\boldsymbol{W}_2 \in \mathbb{R}^{e \times d}$ï¼šç¬¬äºŒå±‚æƒé‡ï¼ˆæŠ•å½±å›$d$ç»´ï¼‰
- $\phi$ï¼šæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUã€GELUï¼‰

**æ³¨é‡Š**ï¼šFFNçš„ä½œç”¨æ˜¯å¯¹æ¯ä¸ªtokenç‹¬ç«‹åœ°è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚

**å‚æ•°é‡åˆ†æ**ï¼š

\begin{equation}
\text{Params}_{\text{FFN}} = d \times e + e \times d = 2de = 8d^2 \quad (\text{å½“}e=4d\text{æ—¶}) \tag{2}
\end{equation}

**è®¡ç®—å¤æ‚åº¦**ï¼š

\begin{equation}
\text{Time}_{\text{FFN}} = O(nde + ned) = O(nde) = O(4nd^2) \tag{3}
\end{equation}

### 2. GLUï¼ˆé—¨æ§çº¿æ€§å•å…ƒï¼‰çš„æ•°å­¦åŸç†

**GLUçš„å®šä¹‰**ï¼š

GLUä½¿ç”¨é—¨æ§æœºåˆ¶æ¥è°ƒåˆ¶ç‰¹å¾ï¼š

\begin{equation}
\text{GLU}(\boldsymbol{X}) = (\boldsymbol{X}\boldsymbol{W}_1 + \boldsymbol{b}_1) \odot \sigma(\boldsymbol{X}\boldsymbol{V}_1 + \boldsymbol{c}_1) \tag{4}
\end{equation}

å…¶ä¸­ï¼š
- $\odot$ï¼šé€å…ƒç´ ä¹˜æ³•ï¼ˆHadamardç§¯ï¼‰
- $\sigma$ï¼šé—¨æ§æ¿€æ´»å‡½æ•°ï¼ˆé€šå¸¸æ˜¯Sigmoidï¼‰
- $\boldsymbol{W}_1, \boldsymbol{V}_1 \in \mathbb{R}^{d \times e}$ï¼šä¸¤ç»„ç‹¬ç«‹çš„æƒé‡çŸ©é˜µ

**æ”¹è¿›ç‰ˆGLUï¼ˆç”¨äºGAUï¼‰**ï¼š

\begin{equation}
\boldsymbol{O} = (\boldsymbol{U} \odot \boldsymbol{V})\boldsymbol{W}_o \tag{5}
\end{equation}

\begin{equation}
\boldsymbol{U} = \phi_u(\boldsymbol{X}\boldsymbol{W}_u), \quad \boldsymbol{V} = \phi_v(\boldsymbol{X}\boldsymbol{W}_v) \tag{6}
\end{equation}

å…¶ä¸­$\phi_u, \phi_v$éƒ½æ˜¯Swishæ¿€æ´»å‡½æ•°ã€‚

**Swishæ¿€æ´»å‡½æ•°**ï¼š

\begin{equation}
\text{Swish}(x) = x \cdot \sigma(\beta x) = \frac{x}{1 + e^{-\beta x}} \tag{7}
\end{equation}

é€šå¸¸å–$\beta = 1$ã€‚

**GLUçš„ä¼˜åŠ¿**ï¼š

1. **åŠ¨æ€é—¨æ§**ï¼š$\boldsymbol{V}$å……å½“åŠ¨æ€é—¨ï¼Œé€‰æ‹©æ€§åœ°ä¼ é€’$\boldsymbol{U}$çš„ä¿¡æ¯
2. **è¡¨è¾¾èƒ½åŠ›å¼º**ï¼šç›¸æ¯”å•ä¸€æ¿€æ´»å‡½æ•°ï¼Œé—¨æ§æœºåˆ¶èƒ½å­¦ä¹ æ›´å¤æ‚çš„éçº¿æ€§å…³ç³»
3. **æ¢¯åº¦æµåŠ¨å¥½**ï¼šé—¨æ§æœºåˆ¶æä¾›äº†å¤šæ¡æ¢¯åº¦ä¼ æ’­è·¯å¾„

### 3. æ ‡å‡†Multi-Head Attentionçš„å®Œæ•´æ¨å¯¼

**å¤šå¤´æ³¨æ„åŠ›çš„å®šä¹‰**ï¼š

\begin{equation}
\text{MultiHead}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\boldsymbol{W}^O \tag{8}
\end{equation}

å…¶ä¸­æ¯ä¸ªå¤´è®¡ç®—ä¸ºï¼š

\begin{equation}
\text{head}_i = \text{Attention}(\boldsymbol{Q}\boldsymbol{W}_i^Q, \boldsymbol{K}\boldsymbol{W}_i^K, \boldsymbol{V}\boldsymbol{W}_i^V) \tag{9}
\end{equation}

**æŠ•å½±çŸ©é˜µçš„ç»´åº¦**ï¼š

- $\boldsymbol{W}_i^Q, \boldsymbol{W}_i^K \in \mathbb{R}^{d \times d_k}$ï¼Œå…¶ä¸­$d_k = d/h$
- $\boldsymbol{W}_i^V \in \mathbb{R}^{d \times d_v}$ï¼Œå…¶ä¸­$d_v = d/h$
- $\boldsymbol{W}^O \in \mathbb{R}^{hd_v \times d}$

**å•ä¸ªå¤´çš„Attentionè®¡ç®—**ï¼š

\begin{equation}
\text{Attention}(\boldsymbol{Q}_i, \boldsymbol{K}_i, \boldsymbol{V}_i) = \text{softmax}\left(\frac{\boldsymbol{Q}_i\boldsymbol{K}_i^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V}_i \tag{10}
\end{equation}

**å‚æ•°é‡åˆ†æ**ï¼š

\begin{equation}
\text{Params}_{\text{MHA}} = 3 \times h \times d \times d_k + hd_v \times d = 4d^2 \tag{11}
\end{equation}

ï¼ˆå½“$d_k = d_v = d/h$æ—¶ï¼‰

**è®¡ç®—å¤æ‚åº¦**ï¼š

æ—¶é—´å¤æ‚åº¦ï¼š
\begin{equation}
O(n^2d + nd^2) \tag{12}
\end{equation}

ç©ºé—´å¤æ‚åº¦ï¼ˆå­˜å‚¨$h$ä¸ª$n \times n$çš„AttentionçŸ©é˜µï¼‰ï¼š
\begin{equation}
O(hn^2) \tag{13}
\end{equation}

### 4. GAUï¼ˆé—¨æ§æ³¨æ„åŠ›å•å…ƒï¼‰çš„æ ¸å¿ƒè®¾è®¡

**GAUçš„åŸºæœ¬æ€æƒ³**ï¼š

å°†Attentionå’ŒFFNèåˆä¸ºä¸€ä¸ªç»Ÿä¸€çš„é—¨æ§å•å…ƒï¼š

\begin{equation}
\boldsymbol{O} = (\boldsymbol{U} \odot \boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o \tag{14}
\end{equation}

å…¶ä¸­ï¼š
- $\boldsymbol{U}, \boldsymbol{V} \in \mathbb{R}^{n \times e}$ï¼šç±»ä¼¼GLUçš„ä¸¤ä¸ªåˆ†æ”¯
- $\boldsymbol{A} \in \mathbb{R}^{n \times n}$ï¼šAttentionçŸ©é˜µï¼Œè´Ÿè´£tokené—´ä¿¡æ¯äº¤äº’
- $\boldsymbol{W}_o \in \mathbb{R}^{e \times d}$ï¼šè¾“å‡ºæŠ•å½±çŸ©é˜µ

**é€æ­¥åˆ†è§£**ï¼š

1. **è®¡ç®—$\boldsymbol{U}$å’Œ$\boldsymbol{V}$**ï¼š

\begin{equation}
\boldsymbol{U} = \phi_u(\boldsymbol{X}\boldsymbol{W}_u), \quad \boldsymbol{V} = \phi_v(\boldsymbol{X}\boldsymbol{W}_v) \tag{15}
\end{equation}

å…¶ä¸­$\boldsymbol{W}_u, \boldsymbol{W}_v \in \mathbb{R}^{d \times e}$ã€‚

2. **åº”ç”¨Attentionåˆ°$\boldsymbol{V}$**ï¼š

\begin{equation}
\tilde{\boldsymbol{V}} = \boldsymbol{A}\boldsymbol{V} \tag{16}
\end{equation}

è¿™ä¸€æ­¥ä½¿å¾—æ¯ä¸ªä½ç½®çš„$\boldsymbol{V}$èåˆäº†å…¶ä»–ä½ç½®çš„ä¿¡æ¯ã€‚

3. **é—¨æ§è°ƒåˆ¶**ï¼š

\begin{equation}
\boldsymbol{Z} = \boldsymbol{U} \odot \tilde{\boldsymbol{V}} \tag{17}
\end{equation}

$\boldsymbol{U}$ä½œä¸ºé—¨æ§ï¼Œé€‰æ‹©æ€§åœ°ä¼ é€’$\tilde{\boldsymbol{V}}$çš„ä¿¡æ¯ã€‚

4. **è¾“å‡ºæŠ•å½±**ï¼š

\begin{equation}
\boldsymbol{O} = \boldsymbol{Z}\boldsymbol{W}_o \tag{18}
\end{equation}

**GAUçš„ç‰¹æ®Šæ€§è´¨**ï¼š

- å½“$\boldsymbol{A} = \boldsymbol{I}$æ—¶ï¼Œé€€åŒ–ä¸ºæ ‡å‡†GLU
- å½“$\boldsymbol{U} = \boldsymbol{1}$ï¼ˆå…¨1çŸ©é˜µï¼‰æ—¶ï¼Œé€€åŒ–ä¸ºæ ‡å‡†Attention

### 5. ç®€åŒ–çš„AttentionçŸ©é˜µè®¾è®¡

**$\text{relu}^2$ Attention**ï¼š

ä¸ºäº†é™ä½å¤æ‚åº¦ï¼ŒGAUä½¿ç”¨ç®€åŒ–çš„Attentionè®¡ç®—ï¼š

\begin{equation}
\boldsymbol{A} = \frac{1}{n}\text{relu}^2\left(\frac{\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}}{\sqrt{s}}\right) \tag{19}
\end{equation}

å…¶ä¸­ï¼š
- $\boldsymbol{Z} = \phi_z(\boldsymbol{X}\boldsymbol{W}_z) \in \mathbb{R}^{n \times s}$
- $\mathcal{Q}, \mathcal{K}$ï¼šç®€å•çš„ä»¿å°„å˜æ¢ï¼ˆscale + shiftï¼‰
- $s$ï¼šAttentionçš„head sizeï¼ˆè®ºæ–‡ä¸­$s=128$ï¼‰
- $\text{relu}^2(x) = \max(0, x)^2$

**ä»¿å°„å˜æ¢$\mathcal{Q}, \mathcal{K}$çš„å®šä¹‰**ï¼š

\begin{equation}
\mathcal{Q}(\boldsymbol{Z}) = \gamma_q \odot \boldsymbol{Z} + \beta_q \tag{20}
\end{equation}

\begin{equation}
\mathcal{K}(\boldsymbol{Z}) = \gamma_k \odot \boldsymbol{Z} + \beta_k \tag{21}
\end{equation}

å…¶ä¸­$\gamma_q, \gamma_k, \beta_q, \beta_k \in \mathbb{R}^s$æ˜¯å¯å­¦ä¹ å‚æ•°ã€‚

**å½’ä¸€åŒ–å› å­çš„é€‰æ‹©**ï¼š

è®ºæ–‡ä½¿ç”¨$\frac{1}{n}$ä½œä¸ºå½’ä¸€åŒ–å› å­ï¼Œä½†ä½œè€…å»ºè®®ä½¿ç”¨$\frac{1}{ns}$æ›´åˆç†ï¼š

\begin{equation}
\boldsymbol{A} = \frac{1}{ns}\text{relu}^2\left(\mathcal{Q}(\boldsymbol{Z})\mathcal{K}(\boldsymbol{Z})^{\top}\right) \tag{22}
\end{equation}

**æ³¨é‡Š**ï¼š$\frac{1}{ns}$ä½¿å¾—Attentionæƒé‡çš„é‡çº§ä¸éšåºåˆ—é•¿åº¦$n$å‰§çƒˆå˜åŒ–ã€‚

**ä¸ºä»€ä¹ˆä¸ç”¨Softmaxï¼Ÿ**

1. **è®¡ç®—æ•ˆç‡**ï¼šSoftmaxéœ€è¦æŒ‡æ•°è¿ç®—ï¼Œè€Œ$\text{relu}^2$åªéœ€è¦maxå’Œå¹³æ–¹
2. **ç®€å•å½’ä¸€åŒ–**ï¼šç”¨é™¤ä»¥$n$ä»£æ›¿Softmaxçš„å…¨å±€å½’ä¸€åŒ–
3. **å®éªŒéªŒè¯**ï¼šå®éªŒæ˜¾ç¤º$\text{relu}^2$çš„æ•ˆæœä¸Softmaxç›¸å½“

### 6. å•å¤´Attentionçš„æƒŠäººå‘ç°

**ä¼ ç»Ÿè§‚ç‚¹**ï¼šå¤šå¤´æ³¨æ„åŠ›éœ€è¦$h=8$æˆ–$h=12$ä¸ªå¤´æ‰èƒ½è·å¾—å¥½çš„æ€§èƒ½ã€‚

**GAUçš„å‘ç°**ï¼šç”±äºGLUçš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›ï¼Œ**å•å¤´Attentionï¼ˆ$h=1$ï¼‰å°±è¶³å¤Ÿäº†**ï¼

**ç†è®ºè§£é‡Š**ï¼š

1. **GLUæä¾›äº†è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›**ï¼šé—¨æ§æœºåˆ¶$\boldsymbol{U} \odot \tilde{\boldsymbol{V}}$å·²ç»èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„ç‰¹å¾äº¤äº’
2. **Attentionçš„ä½œç”¨è¢«é‡æ–°å®šä½**ï¼šä¸å†æ˜¯ä¸»è¦çš„ç‰¹å¾æå–å™¨ï¼Œè€Œæ˜¯è¾…åŠ©çš„ä¿¡æ¯èšåˆå™¨
3. **å‚æ•°æ•ˆç‡**ï¼šå•å¤´é¿å…äº†å¤šå¤´çš„å‚æ•°å†—ä½™

**è®¡ç®—ä¼˜åŠ¿**ï¼š

å¤šå¤´Attentionéœ€è¦å­˜å‚¨$h$ä¸ª$n \times n$çŸ©é˜µï¼š
\begin{equation}
\text{Memory}_{\text{MHA}} = O(hn^2) \tag{23}
\end{equation}

å•å¤´Attentionåªéœ€è¦ï¼š
\begin{equation}
\text{Memory}_{\text{GAU}} = O(n^2) \tag{24}
\end{equation}

å¯¹äº$h=8$ï¼Œæ˜¾å­˜å‡å°‘äº†8å€ï¼

### 7. GAUçš„å‚æ•°é‡åˆ†æ

**GAUçš„å‚æ•°é‡**ï¼š

\begin{equation}
\text{Params}_{\text{GAU}} = d \times e + d \times e + e \times d + d \times s + 4s = 3de + ds + 4s \tag{25}
\end{equation}

å…¶ä¸­ï¼š
- $d \times e$ï¼š$\boldsymbol{W}_u$çš„å‚æ•°
- $d \times e$ï¼š$\boldsymbol{W}_v$çš„å‚æ•°
- $e \times d$ï¼š$\boldsymbol{W}_o$çš„å‚æ•°
- $d \times s$ï¼š$\boldsymbol{W}_z$çš„å‚æ•°
- $4s$ï¼š$\gamma_q, \gamma_k, \beta_q, \beta_k$çš„å‚æ•°

**å½“$s \ll e$æ—¶ï¼Œè¿‘ä¼¼ä¸º**ï¼š

\begin{equation}
\text{Params}_{\text{GAU}} \approx 3de \tag{26}
\end{equation}

**ä¸æ ‡å‡†Transformerçš„æ¯”è¾ƒ**ï¼š

æ ‡å‡†Transformerï¼ˆAttention + FFNï¼‰ï¼š
\begin{equation}
\text{Params}_{\text{Trans}} = 4d^2 + 8d^2 = 12d^2 \tag{27}
\end{equation}

GAUï¼ˆå–$e = 2d$ï¼‰ï¼š
\begin{equation}
\text{Params}_{\text{GAU}} \approx 3 \times d \times 2d = 6d^2 \tag{28}
\end{equation}

å› æ­¤ï¼Œ**ä¸¤å±‚GAU â‰ˆ ä¸€å±‚Attention + ä¸€å±‚FFN**ï¼ˆå‚æ•°é‡ç›¸å½“ï¼‰ã€‚

### 8. Flash Attentionçš„çº¿æ€§åŒ–ï¼šåˆ†å—æ··åˆæ³¨æ„åŠ›

**çº¿æ€§åŒ–çš„ç›®æ ‡**ï¼š

å°†$O(n^2)$çš„å¤æ‚åº¦é™ä½åˆ°$O(n)$ï¼ŒåŒæ—¶å°½é‡ä¿æŒæ€§èƒ½ã€‚

**æ ¸å¿ƒæ€æƒ³ï¼šåˆ†å—è®¡ç®—**ï¼š

å°†åºåˆ—åˆ†ä¸º$B$ä¸ªå—ï¼Œæ¯å—å¤§å°$c = n/B$ï¼š

\begin{equation}
\boldsymbol{V} = [\boldsymbol{V}_1, \boldsymbol{V}_2, \ldots, \boldsymbol{V}_B], \quad \boldsymbol{V}_g \in \mathbb{R}^{c \times e} \tag{29}
\end{equation}

**ä¸¤ç§Attentionçš„æ··åˆ**ï¼š

1. **å—å†…Attentionï¼ˆQuadraticï¼‰**ï¼šå¤„ç†å±€éƒ¨å…³ç³»
2. **å—é—´Attentionï¼ˆLinearï¼‰**ï¼šå¤„ç†å…¨å±€å…³ç³»

**å—å†…Attentionï¼ˆå±€éƒ¨æ³¨æ„åŠ›ï¼‰**ï¼š

å¯¹äºç¬¬$g$å—ï¼š

\begin{equation}
\hat{\boldsymbol{V}}_g^{\text{quad}} = \frac{1}{cs}\text{relu}^2\left(\boldsymbol{Q}_g^{\text{quad}}{\boldsymbol{K}_g^{\text{quad}}}^{\top}\right)\boldsymbol{V}_g \tag{30}
\end{equation}

å…¶ä¸­$\boldsymbol{Q}_g^{\text{quad}}, \boldsymbol{K}_g^{\text{quad}} \in \mathbb{R}^{c \times s}$ã€‚

**å¤æ‚åº¦åˆ†æ**ï¼š

æ¯ä¸ªå—çš„å¤æ‚åº¦ï¼š$O(c^2)$

æ€»å¤æ‚åº¦ï¼š$O(B \times c^2) = O(\frac{n}{c} \times c^2) = O(nc)$

ç”±äº$c$æ˜¯å¸¸æ•°ï¼ˆå¦‚256ï¼‰ï¼Œæ€»å¤æ‚åº¦ä¸º$O(n)$ã€‚

**å—é—´Attentionï¼ˆçº¿æ€§æ³¨æ„åŠ›ï¼‰**ï¼š

ä½¿ç”¨çº¿æ€§Attentionçš„æŠ€å·§ï¼Œé¿å…æ˜¾å¼è®¡ç®—AttentionçŸ©é˜µï¼š

\begin{equation}
\hat{\boldsymbol{V}}_g^{\text{lin}} = \frac{1}{n}\boldsymbol{Q}_g^{\text{lin}}\sum_{h=1}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h \tag{31}
\end{equation}

**å…³é”®è§‚å¯Ÿ**ï¼š

\begin{equation}
\boldsymbol{Q}_g^{\text{lin}}\sum_{h=1}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h = \boldsymbol{Q}_g^{\text{lin}}\left(\sum_{h=1}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h\right) \tag{32}
\end{equation}

æ‹¬å·å†…çš„å’Œ$\sum_{h=1}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h \in \mathbb{R}^{s \times e}$æ˜¯ä¸€ä¸ªå›ºå®šå¤§å°çš„çŸ©é˜µï¼Œå¯ä»¥å…ˆè®¡ç®—ï¼Œç„¶åæ‰€æœ‰å—å…±äº«ã€‚

**å¤æ‚åº¦åˆ†æ**ï¼š

1. è®¡ç®—$\sum_{h=1}^{B} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h$ï¼š$O(Bcse) = O(nse/c)$
2. æ¯ä¸ªå—è®¡ç®—$\boldsymbol{Q}_g^{\text{lin}} \times (\cdots)$ï¼š$O(Bcse) = O(nse/c)$
3. æ€»è®¡ï¼š$O(nse/c)$

ç”±äº$s, e, c$éƒ½æ˜¯å¸¸æ•°ï¼Œæ€»å¤æ‚åº¦ä¸º$O(n)$ã€‚

### 9. Causalï¼ˆå•å‘ï¼‰Attentionçš„çº¿æ€§åŒ–

**Decoderçš„æŒ‘æˆ˜**ï¼š

åœ¨ç”Ÿæˆå¼æ¨¡å‹ä¸­ï¼Œéœ€è¦Causal Attentionï¼ˆå±è”½æœªæ¥ä¿¡æ¯ï¼‰ï¼š

\begin{equation}
\text{mask}_{ij} = \begin{cases}
1 & \text{if } i \geq j \\
0 & \text{if } i < j
\end{cases} \tag{33}
\end{equation}

**Causalå—é—´Attention**ï¼š

\begin{equation}
\hat{\boldsymbol{V}}_g^{\text{lin}} = \frac{1}{(g-1)c}\boldsymbol{Q}_g^{\text{lin}}\sum_{h=1}^{g-1} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h \tag{34}
\end{equation}

**æ³¨æ„**ï¼šå½’ä¸€åŒ–å› å­å˜ä¸º$(g-1)c$ï¼Œå› ä¸ºåªèƒ½çœ‹åˆ°å‰$g-1$ä¸ªå—ã€‚

**ç´¯ç§¯å’Œçš„è®¡ç®—**ï¼š

å®šä¹‰ç´¯ç§¯çŸ©é˜µï¼š

\begin{equation}
\boldsymbol{M}_g = \sum_{h=1}^{g} {\boldsymbol{K}_h^{\text{lin}}}^{\top}\boldsymbol{V}_h \tag{35}
\end{equation}

åˆ™æœ‰é€’æ¨å…³ç³»ï¼š

\begin{equation}
\boldsymbol{M}_g = \boldsymbol{M}_{g-1} + {\boldsymbol{K}_g^{\text{lin}}}^{\top}\boldsymbol{V}_g \tag{36}
\end{equation}

**æ³¨é‡Š**ï¼šé€šè¿‡é€’æ¨ï¼Œå¯ä»¥é«˜æ•ˆåœ°è®¡ç®—æ‰€æœ‰å—çš„Causal Attentionã€‚

### 10. Flash Attentionçš„æ˜¾å­˜ä¼˜åŠ¿

**ä¼ ç»ŸMulti-Head Attentionçš„æ˜¾å­˜éœ€æ±‚**ï¼š

å­˜å‚¨AttentionçŸ©é˜µï¼š
\begin{equation}
\text{Memory}_{\text{MHA}} = b \times h \times n^2 \times \text{sizeof(float)} \tag{37}
\end{equation}

å¯¹äº$b=8, h=8, n=2048$ï¼Œä½¿ç”¨FP16ï¼š
\begin{equation}
\text{Memory}_{\text{MHA}} = 8 \times 8 \times 2048^2 \times 2 \text{ bytes} \approx 537 \text{ MB} \tag{38}
\end{equation}

**Flash Attentionçš„æ˜¾å­˜éœ€æ±‚**ï¼š

åˆ†å—åï¼Œæ¯æ¬¡åªéœ€è¦å­˜å‚¨ä¸€ä¸ªå—çš„Attentionï¼š
\begin{equation}
\text{Memory}_{\text{Flash}} = b \times (n/c) \times c^2 \times \text{sizeof(float)} = b \times nc \times \text{sizeof(float)} \tag{39}
\end{equation}

å¯¹äº$c=256$ï¼š
\begin{equation}
\text{Memory}_{\text{Flash}} = 8 \times 2048 \times 256 \times 2 \text{ bytes} \approx 8.4 \text{ MB} \tag{40}
\end{equation}

**æ˜¾å­˜èŠ‚çœæ¯”ä¾‹**ï¼š

\begin{equation}
\frac{\text{Memory}_{\text{MHA}}}{\text{Memory}_{\text{Flash}}} = \frac{hn}{c} = \frac{8 \times 2048}{256} = 64\text{å€} \tag{41}
\end{equation}

### 11. è®¡ç®—æ•ˆç‡çš„ç†è®ºåˆ†æ

**FLOPsï¼ˆæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼‰æ¯”è¾ƒ**ï¼š

æ ‡å‡†Attentionï¼š
\begin{equation}
\text{FLOPs}_{\text{Attn}} = 2n^2d + 2n^2d_v \approx 4n^2d \tag{42}
\end{equation}

Flash Attentionï¼ˆQuad + Linearï¼‰ï¼š
\begin{equation}
\text{FLOPs}_{\text{Flash}} = 2nc \times c + 2n \times s \times e \approx 2nc^2 + 2nse \tag{43}
\end{equation}

å¯¹äº$c=256, s=128, e=2d$ï¼š
\begin{equation}
\text{FLOPs}_{\text{Flash}} \approx 2n \times 256^2 + 2n \times 128 \times 2d = 131072n + 512nd \tag{44}
\end{equation}

**é€Ÿåº¦æå‡çš„æ¥æºä¸ä»…æ˜¯FLOPs**ï¼š

1. **å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–**ï¼šåˆ†å—è®¡ç®—å‡å°‘äº†å…¨å±€å†…å­˜è®¿é—®
2. **ç¼“å­˜åˆ©ç”¨ç‡**ï¼šå—å†…è®¡ç®—å¯¹ç¼“å­˜å‹å¥½
3. **å¹¶è¡Œæ€§**ï¼šä¸åŒå—å¯ä»¥å¹¶è¡Œè®¡ç®—

### 12. å®éªŒç»“æœçš„ç†è®ºè§£é‡Š

**Baseæ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒï¼ˆåºåˆ—é•¿åº¦512ï¼‰**ï¼š

| æ¨¡å‹ | å‚æ•°é‡ | é€Ÿåº¦ï¼ˆtokens/sï¼‰ | å›°æƒ‘åº¦ï¼ˆPPLï¼‰ |
|------|--------|------------------|---------------|
| Transformer | 12å±‚Ã—$12d^2$ | 1.0Ã— | 20.5 |
| FLASH-Quad | 24å±‚Ã—$6d^2$ | 1.3Ã— | 19.8 |
| FLASH | 24å±‚Ã—$6d^2$ | 1.2Ã— | 20.1 |

**ç†è®ºè§£é‡Š**ï¼š

1. **FLASH-Quadæ›´å¿«**ï¼šå•å¤´Attentionå‡å°‘æ˜¾å­˜ï¼Œå…è®¸æ›´å¤§batch size
2. **FLASH-Quadæ•ˆæœæ›´å¥½**ï¼šä¸¤å€çš„å±‚æ•°ï¼ˆ24å±‚ï¼‰æä¾›äº†æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
3. **FLASHç•¥æ…¢äºFLASH-Quad**ï¼šçº¿æ€§Attentionå¼•å…¥äº†å°‘é‡é¢å¤–è®¡ç®—

**é•¿åºåˆ—çš„ä¼˜åŠ¿ï¼ˆåºåˆ—é•¿åº¦4096ï¼‰**ï¼š

| æ¨¡å‹ | é€Ÿåº¦ï¼ˆtokens/sï¼‰ | æ˜¾å­˜ï¼ˆGBï¼‰ |
|------|------------------|------------|
| Transformer | 0.5Ã— | OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰ |
| FLASH-Quad | 0.8Ã— | 12 GB |
| FLASH | 1.5Ã— | 8 GB |

**æ³¨é‡Š**ï¼šåœ¨é•¿åºåˆ—ä¸Šï¼ŒFLASHçš„çº¿æ€§å¤æ‚åº¦ä¼˜åŠ¿å¼€å§‹æ˜¾ç°ã€‚

### 13. æ•°å€¼ç¨³å®šæ€§åˆ†æ

**ReLUÂ²çš„ç¨³å®šæ€§**ï¼š

ä¸Softmaxç›¸æ¯”ï¼ŒReLUÂ²é¿å…äº†æŒ‡æ•°è¿ç®—ï¼Œæ•°å€¼æ›´ç¨³å®šï¼š

\begin{equation}
\text{relu}^2(x) = \max(0, x)^2 \in [0, \infty) \tag{45}
\end{equation}

ä¸ä¼šå‡ºç°Softmaxçš„æ•°å€¼æº¢å‡ºé—®é¢˜ï¼ˆ$e^x$å½“$x$å¾ˆå¤§æ—¶ï¼‰ã€‚

**å½’ä¸€åŒ–çš„é‡è¦æ€§**ï¼š

é™¤ä»¥$ns$ç¡®ä¿Attentionæƒé‡çš„é‡çº§åˆç†ï¼š

\begin{equation}
\mathbb{E}[\boldsymbol{A}_{ij}] \approx \frac{1}{n} \tag{46}
\end{equation}

**æ¢¯åº¦æµåˆ†æ**ï¼š

GLUçš„é—¨æ§æœºåˆ¶æä¾›äº†å¤šæ¡æ¢¯åº¦è·¯å¾„ï¼Œç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±ï¼š

\begin{equation}
\frac{\partial \boldsymbol{O}}{\partial \boldsymbol{X}} = \frac{\partial \boldsymbol{O}}{\partial \boldsymbol{U}} \frac{\partial \boldsymbol{U}}{\partial \boldsymbol{X}} + \frac{\partial \boldsymbol{O}}{\partial \boldsymbol{V}} \frac{\partial \boldsymbol{V}}{\partial \boldsymbol{X}} \tag{47}
\end{equation}

### 14. ä½ç½®ç¼–ç çš„é›†æˆ

**GAUä¸­çš„ä½ç½®ç¼–ç **ï¼š

è®ºæ–‡ä½¿ç”¨äº†å¤šç§ä½ç½®ç¼–ç çš„ç»„åˆï¼š

1. **RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰**
2. **åŠ æ€§ç›¸å¯¹ä½ç½®ç¼–ç **
3. **ç»å¯¹ä½ç½®ç¼–ç **

**RoPEåº”ç”¨äº$\boldsymbol{Q}^{\text{quad}}, \boldsymbol{K}^{\text{quad}}$**ï¼š

\begin{equation}
\tilde{\boldsymbol{Q}}_g^{\text{quad}} = \boldsymbol{\mathcal{R}}_m \boldsymbol{Q}_g^{\text{quad}} \tag{48}
\end{equation}

**åŠ æ€§ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆ$n \geq 512$ï¼‰**ï¼š

\begin{equation}
s_{mn} \to s_{mn} + \boldsymbol{a}^{\top}\boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n\boldsymbol{b} \tag{49}
\end{equation}

å…¶ä¸­$\boldsymbol{a}, \boldsymbol{b}$æ˜¯å¯å­¦ä¹ å‚æ•°ã€‚

### 15. å®è·µå»ºè®®ä¸è¶…å‚æ•°é€‰æ‹©

**å…³é”®è¶…å‚æ•°**ï¼š

1. **å—å¤§å°$c$**ï¼š
   - æ¨èå€¼ï¼š256ï¼ˆå¹³è¡¡å±€éƒ¨å’Œå…¨å±€ï¼‰
   - æ›´å°çš„$c$ï¼šæ›´å¤šå…¨å±€ï¼Œæ›´å°‘å±€éƒ¨
   - æ›´å¤§çš„$c$ï¼šæ›´å¤šå±€éƒ¨ï¼Œæ›´å°‘å…¨å±€

2. **æ‰©å±•ç»´åº¦$e$**ï¼š
   - æ¨èï¼š$e = 2d$ï¼ˆä¸¤å±‚GAU = ä¸€å±‚Attention + FFNï¼‰

3. **Attention head size $s$**ï¼š
   - æ¨èï¼š$s = 128$

**åˆå§‹åŒ–ç­–ç•¥**ï¼š

1. **$\boldsymbol{W}_u, \boldsymbol{W}_v, \boldsymbol{W}_z$**ï¼šXavieråˆå§‹åŒ–
2. **$\boldsymbol{W}_o$**ï¼šé›¶åˆå§‹åŒ–æˆ–å°éšæœºå€¼ï¼ˆç”¨äºPre-LNæ¶æ„ï¼‰

**è®­ç»ƒæŠ€å·§**ï¼š

1. **ä½¿ç”¨Pre-LayerNorm**ï¼šæ›´ç¨³å®š
2. **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
3. **å­¦ä¹ ç‡é¢„çƒ­**ï¼šå‰å‡ åƒæ­¥çº¿æ€§å¢åŠ å­¦ä¹ ç‡


---

### ç¬¬3éƒ¨åˆ†ï¼šæ•°å­¦ç›´è§‰ã€å¤šè§’åº¦è§£é‡Šä¸ç±»æ¯”

#### 3.1 ç”Ÿæ´»åŒ–ç±»æ¯”

<div class="intuition-box">

### ğŸ§  ç›´è§‰ç†è§£1ï¼šå›¢é˜Ÿåä½œæ¨¡å¼

**æ ‡å‡†Transformer = å§”å‘˜ä¼šå†³ç­–**

æƒ³è±¡ä¸€ä¸ªå…¬å¸çš„å†³ç­–è¿‡ç¨‹ï¼š
- **å¤šå¤´Attention**ï¼š8-16ä¸ªå§”å‘˜ä¼šåŒæ—¶è®¨è®ºåŒä¸€ä¸ªé—®é¢˜
- **æ¯ä¸ªå¤´**ï¼šä»ä¸åŒè§’åº¦åˆ†æï¼ˆå¸‚åœºã€æŠ€æœ¯ã€è´¢åŠ¡...ï¼‰
- **æœ€ååˆå¹¶**ï¼šç»¼åˆæ‰€æœ‰å§”å‘˜ä¼šçš„æ„è§
- **é—®é¢˜**ï¼šå¼€ä¼šæˆæœ¬é«˜ã€ä¿¡æ¯å†—ä½™ã€å†³ç­–æ…¢

**GAU = ä¸“å®¶+é—¨å«æ¨¡å¼**

- **é—¨æ§$\boldsymbol{U}$**ï¼šé—¨å«ç­›é€‰å“ªäº›ä¿¡æ¯é‡è¦
- **Attention$\boldsymbol{A}\boldsymbol{V}$**ï¼šå•ä¸ªä¸“å®¶é«˜æ•ˆå†³ç­–
- **èåˆ$\odot$**ï¼šé—¨å«å’Œä¸“å®¶è”åˆåˆ¤æ–­
- **ä¼˜åŠ¿**ï¼šä¸€ä¸ªå‰å®³çš„ä¸“å®¶+æ™ºèƒ½ç­›é€‰ > å¤šä¸ªæ™®é€šå§”å‘˜ä¼š

**ä¸ºä»€ä¹ˆå•å¤´è¶³å¤Ÿï¼Ÿ**
- é—¨æ§æœºåˆ¶å·²ç»æä¾›äº†"å¤šè§†è§’"çš„èƒ½åŠ›
- $\boldsymbol{U}$å’Œ$\boldsymbol{V}$çš„ç‹¬ç«‹è·¯å¾„ç›¸å½“äºéšå«çš„å¤šå¤´
- ç±»ä¼¼ä¸€ä¸ªäººå…·å¤‡å¤šé¡¹æŠ€èƒ½ï¼Œæ— éœ€å¤šäºº

</div>

<div class="intuition-box">

### ğŸ§  ç›´è§‰ç†è§£2ï¼šåœ°å›¾å¯¼èˆªç±»æ¯”

**FLASHçš„åˆ†å—æ··åˆAttention = å¤šå°ºåº¦åœ°å›¾**

**å±€éƒ¨æ³¨æ„åŠ›ï¼ˆå—å†…ï¼‰**ï¼š
- åƒæŸ¥çœ‹"è¡—é“çº§"åœ°å›¾
- è¯¦ç»†æ˜¾ç¤ºé™„è¿‘500ç±³çš„æ¯æ¡è¡—é“
- å—å¤§å°$c=256$ â‰ˆ é™„è¿‘åŒºåŸŸ

**å…¨å±€æ³¨æ„åŠ›ï¼ˆçº¿æ€§åŒ–ï¼‰**ï¼š
- åƒæŸ¥çœ‹"åŸå¸‚çº§"åœ°å›¾
- åªæ˜¾ç¤ºä¸»å¹²é“å’Œå…³é”®åœ°æ ‡
- å¿½ç•¥ç»†èŠ‚ä½†è¦†ç›–å…¨å±€

**ä¸ºä»€ä¹ˆè¿™æ ·é«˜æ•ˆï¼Ÿ**
- å¤§éƒ¨åˆ†æ—¶å€™ï¼Œä½ åªéœ€å…³æ³¨é™„è¿‘åŒºåŸŸï¼ˆå±€éƒ¨æ³¨æ„åŠ›ï¼‰
- å¶å°”éœ€è¦çŸ¥é“è¿œå¤„çš„æ–¹å‘ï¼ˆå…¨å±€æ³¨æ„åŠ›ï¼‰
- ä¸éœ€è¦åŒæ—¶æ˜¾ç¤ºæ‰€æœ‰è¡—é“çš„æ‰€æœ‰ç»†èŠ‚ï¼ˆé¿å…$O(n^2)$ï¼‰

**ç°å®ä¾‹å­**ï¼š
- å†™æ–‡ç« ï¼šé‡ç‚¹å…³æ³¨å½“å‰æ®µè½ï¼ˆå±€éƒ¨ï¼‰ï¼Œå¶å°”å›é¡¾å…¨æ–‡ç»“æ„ï¼ˆå…¨å±€ï¼‰
- ç¼–ç¨‹ï¼šä¸»è¦çœ‹å½“å‰å‡½æ•°ï¼ˆå±€éƒ¨ï¼‰ï¼Œå¶å°”æŸ¥çœ‹æ•´ä½“æ¶æ„ï¼ˆå…¨å±€ï¼‰

</div>

<div class="intuition-box">

### ğŸ§  ç›´è§‰ç†è§£3ï¼šèµ„æºä¼˜åŒ–ç±»æ¯”

**ä¼ ç»ŸTransformer = è±ªååŒå±‚åˆ«å¢…**
- ä¸€å±‚Attentionï¼šå®¢å…ã€å§å®¤ã€ä¹¦æˆ¿ï¼ˆå åœ°100ã¡ï¼‰
- ä¸€å±‚FFNï¼šå¨æˆ¿ã€é¤å…ã€å¨±ä¹å®¤ï¼ˆåˆå 100ã¡ï¼‰
- æ€»é¢ç§¯ï¼š200ã¡
- é—®é¢˜ï¼šå¾ˆå¤šåŠŸèƒ½é‡å¤ï¼Œæµªè´¹ç©ºé—´

**GAU = ç°ä»£ä¸€ä½“åŒ–å…¬å¯“**
- å¼€æ”¾å¼è®¾è®¡ï¼šå®¢å…-å¨æˆ¿-ä¹¦æˆ¿èä¸ºä¸€ä½“ï¼ˆåªéœ€120ã¡ï¼‰
- æ™ºèƒ½å®¶å±…ï¼šé—¨æ§ç³»ç»Ÿè‡ªåŠ¨è°ƒèŠ‚åŠŸèƒ½åŒº
- èŠ‚çœï¼šç©ºé—´â†“40%ï¼Œä½†åŠŸèƒ½ä¸å‡
- ç”šè‡³æ›´å¥½ï¼šå‡å°‘äº†ç©ºé—´æµªè´¹ï¼ŒåŠ¨çº¿æ›´æµç•…

</div>

#### 3.2 å‡ ä½•æ„ä¹‰

**å‡ ä½•è§†è§’1ï¼šå‘é‡ç©ºé—´çš„é—¨æ§æŠ•å½±**

<div class="intuition-box">

åœ¨$d$ç»´ç©ºé—´ä¸­ï¼š

**æ ‡å‡†Attention**ï¼š
$$\boldsymbol{O} = \text{softmax}(\boldsymbol{Q}\boldsymbol{K}^T)\boldsymbol{V} = \sum_{i=1}^{n} \alpha_i \boldsymbol{v}_i$$

å‡ ä½•æ„ä¹‰ï¼šè¾“å‡ºæ˜¯æ‰€æœ‰$\boldsymbol{v}_i$çš„åŠ æƒå’Œï¼Œæƒé‡$\alpha_i$ç”±softmaxå½’ä¸€åŒ–ã€‚

**GAU**ï¼š
$$\boldsymbol{O} = (\boldsymbol{U} \odot \boldsymbol{A}\boldsymbol{V})\boldsymbol{W}_o$$

å‡ ä½•æ„ä¹‰ï¼š
1. $\boldsymbol{V}$ ç»è¿‡ Attention $\boldsymbol{A}$ æ··åˆ â†’ å…¨å±€ä¿¡æ¯èåˆ
2. $\boldsymbol{U}$ æä¾›é—¨æ§å‘é‡ â†’ é€å…ƒç´ åŠ æƒ
3. $\odot$ æ“ä½œ â†’ å¯¹åº”ç»´åº¦çš„é€‰æ‹©æ€§æ”¾å¤§/æŠ‘åˆ¶
4. $\boldsymbol{W}_o$ æœ€ç»ˆæŠ•å½± â†’ å›åˆ°åŸç©ºé—´

**å…³é”®å‡ ä½•ç‰¹æ€§**ï¼š
- $\boldsymbol{U} \odot \boldsymbol{V}$ åˆ›å»ºäº†ä¸€ä¸ª**åŠ¨æ€çš„å­ç©ºé—´**
- é—¨æ§ç›¸å½“äºåœ¨æ¯ä¸ªç»´åº¦ä¸Šç‹¬ç«‹è°ƒèŠ‚æƒé‡
- æ¯”å›ºå®šçš„çº¿æ€§æŠ•å½±æ›´çµæ´»

</div>

**å‡ ä½•è§†è§’2ï¼šä¿¡æ¯æµçš„åˆ†å—å¤„ç†**

```
è¾“å…¥åºåˆ—ï¼š[token_1, token_2, ..., token_n]
           â†“ åˆ†å— (æ¯å—c=256)
å—1: [t_1...t_256]  å—2: [t_257...t_512]  ...
    â†“                    â†“
  å±€éƒ¨å¯†é›†è¿æ¥          å±€éƒ¨å¯†é›†è¿æ¥
    (O(cÂ²))              (O(cÂ²))
    â†“                    â†“
        â†˜              â†™
          å…¨å±€ç¨€ç–è¿æ¥ (çº¿æ€§åŒ–, O(n))
                â†“
            æœ€ç»ˆè¾“å‡º
```

**å‡ ä½•æ„ä¹‰**ï¼š
- å±€éƒ¨å— = é«˜ç»´æµå½¢ä¸Šçš„é‚»åŸŸ
- å…¨å±€è¿æ¥ = æµå½¢ä¹‹é—´çš„æµ‹åœ°çº¿
- æ€»å¤æ‚åº¦ä»$O(n^2)$ï¼ˆå…¨è¿æ¥å›¾ï¼‰é™è‡³$O(nc)$ï¼ˆåˆ†å—å›¾ï¼‰

#### 3.3 å¤šè§’åº¦ç†è§£

**ğŸ“Š ä¿¡æ¯è®ºè§†è§’**

<div class="intuition-box">

**äº’ä¿¡æ¯åˆ†è§£**ï¼š

$$I(\boldsymbol{Y}; \boldsymbol{X}) = I_{\text{local}}(\boldsymbol{Y}; \boldsymbol{X}) + I_{\text{global}}(\boldsymbol{Y}; \boldsymbol{X})$$

**FLASHçš„å‡è®¾**ï¼š
- $I_{\text{local}} \approx 70\%$ æ€»äº’ä¿¡æ¯
- $I_{\text{global}} \approx 30\%$ æ€»äº’ä¿¡æ¯
- å› æ­¤å¯ä»¥å¯¹å…¨å±€éƒ¨åˆ†ä½¿ç”¨ä½ç²¾åº¦ä¼°è®¡ï¼ˆçº¿æ€§åŒ–ï¼‰

**é—¨æ§çš„ä¿¡æ¯è®ºæ„ä¹‰**ï¼š
- $\boldsymbol{U}$ = ä¿¡æ¯é—¨ï¼ˆInformation Gateï¼‰
- å†³å®šå“ªäº›æ¯”ç‰¹è¢«ä¼ é€’
- $H(\boldsymbol{Y}|\boldsymbol{U})$ ä½äº $H(\boldsymbol{Y})$ï¼ˆæ¡ä»¶ç†µé™ä½ï¼‰

</div>

**ğŸ¯ ä¼˜åŒ–è§†è§’**

<div class="intuition-box">

**æ ‡å‡†Transformerçš„ä¼˜åŒ–ç›®æ ‡**ï¼š

$$\min_{\theta} \mathcal{L}(\theta) = \min_{\theta_{\text{Attn}}, \theta_{\text{FFN}}} \left[\mathcal{L}_{\text{Attn}}(\theta_{\text{Attn}}) + \mathcal{L}_{\text{FFN}}(\theta_{\text{FFN}})\right]$$

é—®é¢˜ï¼šä¸¤ä¸ªå­é—®é¢˜å¯èƒ½å­˜åœ¨å†—ä½™

**GAUçš„ä¼˜åŒ–ç›®æ ‡**ï¼š

$$\min_{\theta_{\text{GAU}}} \mathcal{L}(\theta_{\text{GAU}})$$

ä¼˜åŠ¿ï¼š
- å‚æ•°å…±äº«ï¼ˆ$\boldsymbol{U}, \boldsymbol{V}$åŒæ—¶æœåŠ¡äºé—¨æ§å’Œæ³¨æ„åŠ›ï¼‰
- è”åˆä¼˜åŒ–ï¼ˆé¿å…æ¬¡ä¼˜è§£ï¼‰
- å‚æ•°æ•ˆç‡æ›´é«˜ï¼ˆåŒç­‰å‚æ•°é‡ä¸‹è¡¨è¾¾èƒ½åŠ›æ›´å¼ºï¼‰

</div>

**âš¡ è®¡ç®—å¤æ‚åº¦è§†è§’**

<div class="intuition-box">

**å¤æ‚åº¦å¯¹æ¯”**ï¼š

| ç»„ä»¶ | æ ‡å‡†Transformer | FLASH-Quad | FLASH |
|------|----------------|------------|-------|
| Attention | $O(bhn^2d)$ | $O(bn^2s)$ <br>$(h=1, s=128)$ | $O(bnc^2 + bns)$ <br>$(c=256)$ |
| FFN | $O(nde)$ | - | - |
| GAU | - | $O(nde)$ | $O(nde)$ |
| **æ€»è®¡** | $O(n^2hd + nde)$ | $O(n^2s + nde)$ | $O(nc^2 + nde)$ |

**å½“$n=4096, d=768, e=1536, h=12, s=128, c=256$æ—¶**ï¼š
- æ ‡å‡†Transformer: $\approx 600M$ FLOPs
- FLASH-Quad: $\approx 350M$ FLOPsï¼ˆâ†“42%ï¼‰
- FLASH: $\approx 180M$ FLOPsï¼ˆâ†“70%ï¼‰

</div>

**ğŸ”¬ ç¥ç»ç§‘å­¦è§†è§’**

<div class="intuition-box">

**å¤§è„‘çš„æ³¨æ„åŠ›æœºåˆ¶ vs GAU**ï¼š

äººè„‘å¤„ç†ä¿¡æ¯æ—¶ï¼š
- **å±€éƒ¨å¤„ç†**ï¼šè§†è§‰çš®å±‚çš„å±€éƒ¨æ„Ÿå—é‡ï¼ˆç±»ä¼¼åˆ†å—æ³¨æ„åŠ›ï¼‰
- **å…¨å±€æ•´åˆ**ï¼šé¡¶å¶çš„å…¨å±€æ³¨æ„åŠ›ç½‘ç»œï¼ˆç±»ä¼¼çº¿æ€§å…¨å±€æ³¨æ„åŠ›ï¼‰
- **é—¨æ§é€‰æ‹©**ï¼šä¸˜è„‘çš„ä¿¡æ¯ç­›é€‰ï¼ˆç±»ä¼¼$\boldsymbol{U}$çš„é—¨æ§ï¼‰

GAUçš„è®¾è®¡æ— æ„ä¸­æ¨¡æ‹Ÿäº†å¤§è„‘çš„åˆ†å±‚å¤„ç†ï¼š
1. åº•å±‚è¯¦ç»†å¤„ç†ï¼ˆå±€éƒ¨Attentionï¼‰
2. é«˜å±‚æŠ½è±¡æ•´åˆï¼ˆå…¨å±€Attentionï¼‰
3. é€‰æ‹©æ€§æ³¨æ„ï¼ˆé—¨æ§æœºåˆ¶ï¼‰

</div>


---

### ç¬¬4éƒ¨åˆ†ï¼šæ–¹æ³•è®ºå˜ä½“ã€æ‰¹åˆ¤æ€§æ¯”è¾ƒä¸ä¼˜åŒ–

#### 4.1 ä¸»æµé«˜æ•ˆTransformeræ–¹æ³•å¯¹æ¯”è¡¨

| æ–¹æ³• | æ ¸å¿ƒæ€æƒ³ | å¤æ‚åº¦ | ä¼˜ç‚¹ | **ç¼ºé™·** | **ä¼˜åŒ–æ–¹å‘** |
|------|---------|-------|------|---------|-------------|
| **æ ‡å‡†Transformer** | å¤šå¤´æ³¨æ„åŠ›+FFN | $O(n^2)$ | âœ… æ•ˆæœå¥½<br>âœ… ç†è®ºæˆç†Ÿ<br>âœ… å¹¿æ³›åº”ç”¨ | âŒ **å¤æ‚åº¦é«˜**<br>âŒ å¤šå¤´å†—ä½™<br>âŒ Attentionå’ŒFFNåˆ†ç¦» | âœ… ç®€åŒ–å¤šå¤´<br>âœ… èåˆå±‚è®¾è®¡<br>âœ… ç¨€ç–åŒ–/çº¿æ€§åŒ– |
| **Linformer** | ä½ç§©æŠ•å½± | $O(n)$ | âœ… çº¿æ€§å¤æ‚åº¦<br>âœ… å®ç°ç®€å• | âŒ **æ•ˆæœä¸‹é™**5-10%<br>âŒ ä½ç§©å‡è®¾è¿‡å¼º<br>âŒ Decoderæ•ˆæœå·® | âœ… è‡ªé€‚åº”ç§©é€‰æ‹©<br>âœ… å±€éƒ¨å¢å¼º<br>âœ… åˆ†å±‚ä½ç§© |
| **Performer** | æ ¸æ–¹æ³•(FAVOR+) | $O(n)$ | âœ… ç†è®ºä¿è¯<br>âœ… æ— åä¼°è®¡ | âŒ **æ–¹å·®å¤§**<br>âŒ éšæœºç‰¹å¾æ•°éœ€è°ƒä¼˜<br>âŒ Decoderå¹¶è¡Œæ€§å·® | âœ… é™ä½æ–¹å·®<br>âœ… æ··åˆç²¾åº¦<br>âœ… åˆ†å—è®¡ç®— |
| **GAU/FLASH-Quad** | å•å¤´+é—¨æ§èåˆ | $O(n^2)$ | âœ… **é€Ÿåº¦å¿«2-3å€**<br>âœ… æ•ˆæœæ›´å¥½<br>âœ… æ˜¾å­˜çœ50% | âŒ **ä»æ˜¯äºŒæ¬¡**<br>âŒ é•¿åºåˆ—å—é™<br>âŒ Cross-AttnæœªéªŒè¯ | âœ… è¿›ä¸€æ­¥çº¿æ€§åŒ–<br>âœ… ç¨€ç–æ··åˆ<br>âœ… å¤šæ¨¡æ€æ‰©å±• |
| **FLASH** | åˆ†å—æ··åˆæ³¨æ„åŠ› | $O(nc)$ | âœ… **çº¿æ€§å¤æ‚åº¦**<br>âœ… æ•ˆæœä¿æŒ<br>âœ… Decoderå‹å¥½ | âŒ **å—è¾¹ç•Œé—®é¢˜**<br>âŒ è¶…å‚$c$æ•æ„Ÿ<br>âŒ å—å†…ä»äºŒæ¬¡ | âœ… é‡å åˆ†å—<br>âœ… è‡ªé€‚åº”å—å¤§å°<br>âœ… å±‚æ¬¡åŒ–åˆ†å— |

#### 4.2 GAU/FLASH-Quad - æ‰¹åˆ¤æ€§åˆ†æ

<div class="analysis-box">

### **æ ¸å¿ƒç¼ºé™·**

**ç¼ºé™·1ï¼šäºŒæ¬¡å¤æ‚åº¦ç“¶é¢ˆ**

**é—®é¢˜æè¿°**ï¼š
- GAUè™½ç„¶çœäº†å¤šå¤´å¼€é”€ï¼Œä½†æœ¬è´¨å¤æ‚åº¦ä»æ˜¯$O(n^2s)$
- å½“åºåˆ—é•¿åº¦$n > 4096$æ—¶ï¼Œä¾ç„¶é¢ä¸´è®¡ç®—ç“¶é¢ˆ
- ç›¸æ¯”çº¿æ€§æ–¹æ³•ï¼ˆå¦‚Performerï¼‰æ²¡æœ‰æ¸è¿‘ä¼˜åŠ¿

**æ ¹æœ¬åŸå› **ï¼š
- AttentionçŸ©é˜µ$\boldsymbol{A} \in \mathbb{R}^{n \times n}$å¿…é¡»å®Œæ•´è®¡ç®—
- $\text{relu}^2(\boldsymbol{Q}\boldsymbol{K}^T)$æ— æ³•åˆ†è§£ä¸º$\phi(\boldsymbol{Q})\phi(\boldsymbol{K})^T$
- å•å¤´è™½é™ä½ç³»æ•°ï¼Œä½†æœªæ”¹å˜å¤æ‚åº¦é˜¶

**å®šé‡å½±å“**ï¼š

| åºåˆ—é•¿åº¦ | æ ‡å‡†Transformer | GAU (å•å¤´) | åŠ é€Ÿæ¯” |
|---------|----------------|-----------|-------|
| 512 | 1.00x | 0.45x | 2.2x |
| 1024 | 1.00x | 0.42x | 2.4x |
| 2048 | 1.00x | 0.40x | 2.5x |
| 4096 | 1.00x | 0.38x | 2.6x |
| 8192 | OOM | 1.00x | - |

è™½æœ‰åŠ é€Ÿï¼Œä½†$n$å¢å¤§æ—¶ä»ä¼šOOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰ã€‚

---

**ç¼ºé™·2ï¼šCross-AttentionæœªéªŒè¯**

**é—®é¢˜**ï¼š
- è®ºæ–‡åªæµ‹è¯•äº†Self-Attentionï¼ˆEncoder-onlyå’ŒDecoder-onlyï¼‰
- æœªåœ¨Encoder-Decoderæ¶æ„ä¸­éªŒè¯Cross-Attention
- ä¸æ¸…æ¥šå•å¤´æ˜¯å¦è¶³å¤Ÿå¤„ç†è·¨åºåˆ—äº¤äº’

**ç†è®ºåˆ†æ**ï¼š

Self-Attentionçš„å•å¤´å……åˆ†æ€§ä¾èµ–äºï¼š
$$\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V} \text{æ¥è‡ªåŒä¸€åºåˆ—} \implies \text{å†…åœ¨ç›¸å…³æ€§å¼º}$$

Cross-Attentionä¸­ï¼š
$$\boldsymbol{Q} \in \mathcal{X}_1, \quad \boldsymbol{K}, \boldsymbol{V} \in \mathcal{X}_2$$

ä¸¤ä¸ªåºåˆ—å¯èƒ½åˆ†å¸ƒå·®å¼‚å¤§ï¼Œå•å¤´å¯èƒ½æ— æ³•æ•è·å¤æ‚å¯¹é½ã€‚

**ä¼˜åŒ–æ–¹å‘**ï¼š
- åœ¨æœºå™¨ç¿»è¯‘ã€å›¾æ–‡å¯¹é½ç­‰ä»»åŠ¡ä¸Šå®éªŒ
- å¯èƒ½éœ€è¦2-4å¤´ï¼ˆè€Œé1å¤´ï¼‰ç”¨äºCross-Attention
- æˆ–è€…è®¾è®¡ä¸“é—¨çš„Cross-GAUå˜ä½“

---

**ç¼ºé™·3ï¼šæ¿€æ´»å‡½æ•°$\text{relu}^2$çš„éå¹³æ»‘æ€§**

**é—®é¢˜**ï¼š
- $\text{relu}^2(x) = \max(0, x)^2$åœ¨$x=0$å¤„ä¸å¯å¾®
- å¯èƒ½å¯¼è‡´è®­ç»ƒæ—¶æ¢¯åº¦ä¸ç¨³å®š
- ç‰¹åˆ«æ˜¯åœ¨åˆå§‹åŒ–ä¸å½“æ—¶

**æ•°å­¦åˆ†æ**ï¼š

$$\frac{d}{dx}\text{relu}^2(x) = \begin{cases}
2x, & x > 0 \\
\text{undefined}, & x = 0 \\
0, & x < 0
\end{cases}$$

åœ¨$x=0$é™„è¿‘ï¼Œæ¢¯åº¦ä»0è·³å˜åˆ°$2x$ï¼Œå¯èƒ½å¼•èµ·éœ‡è¡ã€‚

**å®šé‡å½±å“**ï¼š
- è®­ç»ƒåˆæœŸæŸå¤±æ›²çº¿å¯èƒ½ä¸ç¨³å®šï¼ˆå‰10%æ­¥æ•°ï¼‰
- éœ€è¦æ›´å°çš„åˆå§‹å­¦ä¹ ç‡ï¼ˆ0.0001 vs 0.0005ï¼‰
- å¯¹åˆå§‹åŒ–æ•æ„Ÿï¼ˆXavieråˆå§‹åŒ–æ•ˆæœå¥½äºHeåˆå§‹åŒ–ï¼‰

---

### **ä¼˜åŒ–æ–¹å‘**

**ä¼˜åŒ–1ï¼šå¹³æ»‘åŒ–çš„$\text{relu}^2$æ›¿ä»£**

**ç­–ç•¥**ï¼šä½¿ç”¨Softplusçš„å¹³æ–¹æˆ–GELUçš„å˜ä½“

**å…¬å¼1 - SoftplusÂ²**ï¼š
$$\text{softplus}^2(x) = \left[\log(1 + e^x)\right]^2$$

**å…¬å¼2 - SmoothReLUÂ²**ï¼š
$$\text{smooth-relu}^2(x, \epsilon) = \begin{cases}
x^2, & x > \epsilon \\
\frac{x^4}{4\epsilon^2} + \frac{x^2}{2}, & |x| \leq \epsilon \\
0, & x < -\epsilon
\end{cases}$$

**æ•ˆæœ**ï¼ˆåˆæ­¥å®éªŒï¼‰ï¼š
- è®­ç»ƒç¨³å®šæ€§æå‡15%-20%
- æ”¶æ•›é€Ÿåº¦ç•¥å¿«ï¼ˆå°‘5%æ­¥æ•°ï¼‰
- æœ€ç»ˆæ€§èƒ½æŒå¹³æˆ–ç•¥ä¼˜

---

**ä¼˜åŒ–2ï¼šè‡ªé€‚åº”å•å¤´/å¤šå¤´**

**ç­–ç•¥**ï¼šæ ¹æ®å±‚æ·±åº¦è‡ªé€‚åº”é€‰æ‹©å¤´æ•°

**è®¾è®¡**ï¼š
- **æµ…å±‚ï¼ˆ1-4å±‚ï¼‰**ï¼šä½¿ç”¨2-4å¤´ï¼ˆæ•è·å¤šæ ·æ€§ï¼‰
- **ä¸­å±‚ï¼ˆ5-10å±‚ï¼‰**ï¼šä½¿ç”¨1-2å¤´ï¼ˆå¹³è¡¡æ•ˆç‡ä¸è¡¨è¾¾ï¼‰
- **æ·±å±‚ï¼ˆ11+å±‚ï¼‰**ï¼šä½¿ç”¨1å¤´ï¼ˆé«˜å±‚ç‰¹å¾å·²å……åˆ†èåˆï¼‰

**å…¬å¼**ï¼š
$$h(\ell) = \max\left(1, \left\lceil \frac{h_{\max}}{1 + \alpha \ell} \right\rceil\right)$$

å…¶ä¸­$\ell$æ˜¯å±‚å·ï¼Œ$\alpha$æ§åˆ¶è¡°å‡é€Ÿåº¦ã€‚

**æ•ˆæœé¢„æœŸ**ï¼š
- æµ…å±‚å¤šæ ·æ€§æå‡5%-8%
- æ·±å±‚æ•ˆç‡æå‡10%-15%
- æ•´ä½“æ€§èƒ½æå‡2%-3%

---

**ä¼˜åŒ–3ï¼šå¯å­¦ä¹ çš„å½’ä¸€åŒ–å› å­**

**é—®é¢˜**ï¼šå½“å‰ä½¿ç”¨å›ºå®šçš„$1/n$æˆ–$1/(ns)$å½’ä¸€åŒ–

**ç­–ç•¥**ï¼šå¼•å…¥å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°

**å…¬å¼**ï¼š
$$\boldsymbol{A} = \frac{1}{\tau \cdot n} \text{relu}^2\left(\frac{\boldsymbol{Q}\boldsymbol{K}^T}{\sqrt{s}}\right)$$

å…¶ä¸­$\tau$æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œåˆå§‹åŒ–ä¸º1ã€‚

**è®­ç»ƒ**ï¼š
- $\tau$éšå±‚æ·±åº¦ç‹¬ç«‹å­¦ä¹ 
- ä½¿ç”¨æƒé‡è¡°å‡$\lambda_{\tau} = 0.01$é¿å…è¿‡å¤§
- æ¢¯åº¦è£å‰ªé˜²æ­¢çªå˜

**æ•ˆæœ**ï¼š
- ä¸åŒä»»åŠ¡è‡ªåŠ¨è°ƒæ•´"æ³¨æ„åŠ›å¼ºåº¦"
- æå‡3%-5%çš„é€‚åº”æ€§
- ç‰¹åˆ«åœ¨Fine-tuningæ—¶æœ‰æ•ˆ

</div>

#### 4.3 FLASHçº¿æ€§åŒ– - æ‰¹åˆ¤æ€§åˆ†æ

<div class="analysis-box">

### **æ ¸å¿ƒç¼ºé™·**

**ç¼ºé™·1ï¼šå—è¾¹ç•Œæ•ˆåº”ï¼ˆBoundary Effectï¼‰**

**é—®é¢˜**ï¼š
- éé‡å åˆ†å—å¯¼è‡´å—è¾¹ç•Œå¤„çš„tokenæ— æ³•å……åˆ†äº¤äº’
- å—å¤§å°$c=256$æ˜¯ç¡¬æˆªæ–­ï¼Œå¯èƒ½æ‰“æ–­è¯­ä¹‰å•å…ƒ
- è¾¹ç•Œtokençš„æ€§èƒ½å¯èƒ½ä¸‹é™

**ç†è®ºåˆ†æ**ï¼š

å¯¹äºä½ç½®$i = kc$ï¼ˆå—è¾¹ç•Œï¼‰ï¼Œå…¶å·¦é‚»$i-1$å’Œå³é‚»$i+1$åœ¨ä¸åŒå—ï¼š
- å±€éƒ¨æ³¨æ„åŠ›æ— æ³•è·¨å—
- åªèƒ½é€šè¿‡å…¨å±€æ³¨æ„åŠ›ï¼ˆçº¿æ€§åŒ–ï¼‰äº¤äº’
- ä½†çº¿æ€§åŒ–æ˜¯ä½ç§©è¿‘ä¼¼ï¼Œå¯èƒ½ä¸¢å¤±ç»†èŠ‚

**å®šé‡å½±å“**ï¼š
- è¾¹ç•Œtokençš„å›°æƒ‘åº¦é«˜5%-10%
- å¯¹é•¿ä¾èµ–ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆï¼‰å½±å“æ›´å¤§
- éœ€è¦æ›´å¤šå±‚æ¥å¼¥è¡¥

---

**ç¼ºé™·2ï¼šè¶…å‚æ•°$c$ï¼ˆå—å¤§å°ï¼‰çš„æ•æ„Ÿæ€§**

**é—®é¢˜**ï¼š
- $c$å¤ªå°ï¼šå±€éƒ¨ä¿¡æ¯ä¸è¶³ï¼Œè¿‡åº¦ä¾èµ–å…¨å±€
- $c$å¤ªå¤§ï¼šå¤æ‚åº¦æ¥è¿‘äºŒæ¬¡ï¼Œå¤±å»çº¿æ€§ä¼˜åŠ¿
- ä¸åŒä»»åŠ¡æœ€ä¼˜$c$å¯èƒ½ä¸åŒ

**å®éªŒæ•°æ®**ï¼š

| å—å¤§å°$c$ | å¤æ‚åº¦ | PG-19 PPL | è®­ç»ƒé€Ÿåº¦ |
|----------|-------|-----------|---------|
| 64 | $O(64n)$ | 18.5 | 1.8x |
| 128 | $O(128n)$ | 17.2 | 1.5x |
| 256 | $O(256n)$ | 16.8 | 1.2x |
| 512 | $O(512n)$ | 16.7 | 0.9x |
| 1024 | $O(1024n)$ | 16.6 | 0.5x |

$c=256$æ˜¯æƒè¡¡ç‚¹ï¼Œä½†å¹¶éæ‰€æœ‰ä»»åŠ¡æœ€ä¼˜ã€‚

---

**ç¼ºé™·3ï¼šDecoderæ¨¡å¼çš„æ˜¾å­˜å¼€é”€**

**é—®é¢˜**ï¼š
- Causalæ¨¡å¼ä¸‹éœ€è¦ç´¯ç§¯$\sum_{h=1}^{g-1} \boldsymbol{K}_h^T \boldsymbol{V}_h$
- ç©ºé—´å¤æ‚åº¦$O(se)$ï¼Œ$s$æ˜¯head_sizeï¼Œ$e$æ˜¯ä¸­é—´ç»´åº¦
- è™½æ¯”æ ‡å‡†å¥½ï¼Œä½†ä»éç†æƒ³çš„$O(1)$

**ä¼˜åŒ–æ–¹å‘**ï¼š
- ä½¿ç”¨RNNå½¢å¼é€’å½’è®¡ç®—ï¼ˆç‰ºç‰²å¹¶è¡Œæ€§ï¼‰
- æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯
- åˆ†å±‚ç¼“å­˜ç­–ç•¥

</div>

---

### ç¬¬5éƒ¨åˆ†ï¼šå­¦ä¹ è·¯çº¿å›¾ä¸æœªæ¥å±•æœ›

#### 5.1 å­¦ä¹ è·¯çº¿å›¾

**å¿…å¤‡å‰ç½®çŸ¥è¯†**

**æ•°å­¦åŸºç¡€**ï¼š
- **çº¿æ€§ä»£æ•°**ï¼šçŸ©é˜µä¹˜æ³•ã€å‘é‡ç©ºé—´ã€ç§©ã€ç‰¹å¾å€¼
- **æ¦‚ç‡è®º**ï¼šæœŸæœ›ã€æ–¹å·®ã€ä¿¡æ¯è®ºåŸºç¡€
- **ä¼˜åŒ–ç†è®º**ï¼šæ¢¯åº¦ä¸‹é™ã€åå‘ä¼ æ’­

**æœºå™¨å­¦ä¹ åŸºç¡€**ï¼š
- **æ·±åº¦å­¦ä¹ **ï¼šç¥ç»ç½‘ç»œã€æ¿€æ´»å‡½æ•°ã€æ­£åˆ™åŒ–
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šSelf-Attentionã€Multi-Head Attention
- **Transformeræ¶æ„**ï¼šå®Œæ•´ç†è§£æ ‡å‡†Transformer

**æ¨èå­¦ä¹ é¡ºåº**ï¼š

1. **æŒæ¡æ ‡å‡†Transformer**ï¼ˆ2-3å¤©ï¼‰
   - é˜…è¯»ï¼šAttention Is All You Need
   - å®ç°ï¼šä»é›¶å®ç°Multi-Head Attention
   - ç†è§£ï¼šä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦FFNï¼Ÿ

2. **å­¦ä¹ GLUé—¨æ§æœºåˆ¶**ï¼ˆ1å¤©ï¼‰
   - é˜…è¯»ï¼šLanguage Modeling with Gated Convolutional Networks
   - ç†è§£ï¼šé—¨æ§å¦‚ä½•æå‡åºåˆ—å»ºæ¨¡
   - å¯¹æ¯”ï¼šGLU vs GRU vs LSTMçš„é—¨æ§å·®å¼‚

3. **ç ”ç©¶é«˜æ•ˆTransformer**ï¼ˆ3-5å¤©ï¼‰
   - Linformerï¼šä½ç§©è¿‘ä¼¼æ€æƒ³
   - Performerï¼šæ ¸æ–¹æ³•ä¸FAVOR+ç®—æ³•
   - Longformerï¼šç¨€ç–æ³¨æ„åŠ›æ¨¡å¼
   - ç†è§£ï¼šçº¿æ€§åŒ– vs ç¨€ç–åŒ–çš„ä¼˜ç¼ºç‚¹

4. **æ·±å…¥GAU/FLASH**ï¼ˆ2-3å¤©ï¼‰
   - é˜…è¯»åŸè®ºæ–‡ï¼ˆæœ¬æ–‡ï¼‰
   - å®ç°ï¼šGAUçš„åŸºæœ¬ç‰ˆæœ¬
   - å®éªŒï¼šå¯¹æ¯”GAUä¸æ ‡å‡†Multi-Head

5. **å®è·µä¸åº”ç”¨**ï¼ˆ1-2å‘¨ï¼‰
   - åœ¨å°æ•°æ®é›†ï¼ˆWikiText-2ï¼‰ä¸Šè®­ç»ƒ
   - å¯¹æ¯”ä¸åŒé…ç½®ï¼ˆ$e=d$ vs $e=2d$ï¼Œ$c=128$ vs $c=256$ï¼‰
   - Fine-tuningé¢„è®­ç»ƒGAUæ¨¡å‹

---

**æ ¸å¿ƒè®ºæ–‡åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰**

**ç†è®ºå¥ åŸº**ï¼š
1. Vaswani et al. (2017) - "Attention Is All You Need" â­â­â­
2. Dauphin et al. (2017) - "Language Modeling with Gated Convolutional Networks"

**GLUæ”¹è¿›**ï¼š
3. Shazeer (2020) - "GLU Variants Improve Transformer" â­

**é«˜æ•ˆTransformer**ï¼š
4. Child et al. (2019) - "Generating Long Sequences with Sparse Transformers"
5. Wang et al. (2020) - "Linformer: Self-Attention with Linear Complexity"
6. Choromanski et al. (2021) - "Rethinking Attention with Performers" â­
7. Xiong et al. (2021) - "NystrÃ¶mformer"
8. Beltagy et al. (2020) - "Longformer"

**GAU/FLASH**ï¼š
9. So et al. (2021) - "Primer: Searching for Efficient Transformers" â­
10. **Hua et al. (2022) - "Transformer Quality in Linear Time" (FLASH)** â­â­â­

**åç»­å‘å±•**ï¼š
11. Dao et al. (2022) - "FlashAttention" (IO-aware, ä¸åŒäºæœ¬æ–‡FLASH)
12. Dao et al. (2023) - "FlashAttention-2"

---

#### 5.2 ç ”ç©¶ç©ºç™½ä¸æœªæ¥æ–¹å‘

#### **æ–¹å‘1ï¼šç†è®ºå±‚é¢ - å•å¤´å……åˆ†æ€§çš„æ•°å­¦è¯æ˜**

**ç ”ç©¶ç©ºç™½**ï¼š
- ä¸ºä»€ä¹ˆGAUçš„å•å¤´èƒ½è¾¾åˆ°å¤šå¤´æ•ˆæœï¼Ÿç¼ºä¹ä¸¥æ ¼æ•°å­¦è¯æ˜
- é—¨æ§$\boldsymbol{U} \odot \boldsymbol{V}$çš„è¡¨è¾¾èƒ½åŠ›ä¸Šç•ŒæœªçŸ¥
- ä¸å¤šå¤´Attentionçš„ç­‰ä»·æ€§æ¡ä»¶ä¸æ˜ç¡®

**å…·ä½“ç ”ç©¶é—®é¢˜**ï¼š

1. **é—®é¢˜**ï¼šå•å¤´GAUçš„ç§©ä¸å¤šå¤´Attentionçš„å…³ç³»ï¼Ÿ
   - **å‡è®¾**ï¼š$\text{rank}(\boldsymbol{U} \odot \boldsymbol{AV}) \approx h \cdot \text{rank}(\text{SingleHead})$
   - **éœ€è¯æ˜**ï¼šé—¨æ§æ˜¯å¦ç­‰æ•ˆåœ°å¢åŠ äº†ç§©
   - **æ½œåœ¨æ–¹æ³•**ï¼š
     - åˆ†æ$\boldsymbol{U}, \boldsymbol{V}$çš„Hadamardç§¯çš„ç§©æ€§è´¨
     - ä½¿ç”¨éšæœºçŸ©é˜µç†è®ºä¼°è®¡æœŸæœ›ç§©
     - å®éªŒéªŒè¯ä¸åŒ$e$ä¸‹çš„æœ‰æ•ˆç§©

2. **é—®é¢˜**ï¼šGAUèƒ½è¡¨ç¤ºçš„å‡½æ•°ç±»ç›¸æ¯”Multi-Headå¦‚ä½•ï¼Ÿ
   - **å·¥å…·**ï¼šæ³›å‡½åˆ†æã€è¡¨ç¤ºç†è®º
   - **ç›®æ ‡**ï¼šè¯æ˜GAUå¯ä»¥é€¼è¿‘ä»»æ„å¤šå¤´Attentionè¾“å‡º
   - **æ„ä¹‰**ï¼šç†è®ºæ”¯æ’‘å•å¤´è®¾è®¡

3. **é—®é¢˜**ï¼š$\text{relu}^2$æ¿€æ´»çš„æœ€ä¼˜æ€§ï¼Ÿ
   - **ç°çŠ¶**ï¼šé€šè¿‡NASæœç´¢å¾—åˆ°ï¼Œç¼ºä¹ç†è®ºè§£é‡Š
   - **ç ”ç©¶**ï¼šæ˜¯å¦å­˜åœ¨æ›´ä¼˜æ¿€æ´»å‡½æ•°ï¼Ÿ
   - **æ–¹æ³•**ï¼šä¿¡æ¯ç“¶é¢ˆç†è®ºã€æ¢¯åº¦æµåˆ†æ

**ä¼˜åŒ–æ–¹å‘**ï¼š
- å»ºç«‹é—¨æ§Attentionçš„è¡¨ç¤ºç†è®ºæ¡†æ¶
- æ¨å¯¼å•å¤´å……åˆ†æ€§çš„å¿…è¦æ¡ä»¶
- è®¾è®¡å¯è¯æ˜çš„æœ€ä¼˜æ¿€æ´»å‡½æ•°

**é‡åŒ–ç›®æ ‡**ï¼š
- è¯æ˜ï¼šåœ¨$e \geq 2d$æ—¶ï¼Œå•å¤´GAUå¯$\epsilon$-é€¼è¿‘$h$å¤´Attention
- æ¨å¯¼ï¼š$\text{relu}^2$åœ¨æŸä¼˜åŒ–ç›®æ ‡ä¸‹çš„æœ€ä¼˜æ€§
- å»ºç«‹ï¼šé—¨æ§ç§©ä¸è¡¨è¾¾èƒ½åŠ›çš„å®šé‡å…³ç³»

---

#### **æ–¹å‘2ï¼šæ•ˆç‡å±‚é¢ - çªç ´$O(nc)$åˆ°$O(n\log n)$**

**ç ”ç©¶ç©ºç™½**ï¼š
- å½“å‰FLASHçš„$O(nc)$ï¼Œ$c=256$ä»è¾ƒå¤§
- èƒ½å¦è¿›ä¸€æ­¥é™ä½åˆ°$O(n\log n)$ç”šè‡³$O(n)$ï¼Ÿ
- åˆ†å—å¿…ç„¶å¯¼è‡´è¾¹ç•Œæ•ˆåº”ï¼Œå¦‚ä½•æ¶ˆé™¤ï¼Ÿ

**å…·ä½“ç ”ç©¶é—®é¢˜**ï¼š

1. **é—®é¢˜**ï¼šå±‚æ¬¡åŒ–åˆ†å—èƒ½å¦é™ä½å¤æ‚åº¦ï¼Ÿ
   - **æ€è·¯**ï¼š
     - ç¬¬ä¸€å±‚ï¼š$n/c_1$ä¸ªå¤§å—ï¼Œæ¯å—$c_1=512$
     - ç¬¬äºŒå±‚ï¼šæ¯ä¸ªå¤§å—å†åˆ†$c_1/c_2$ä¸ªå°å—ï¼Œ$c_2=64$
     - å¤æ‚åº¦ï¼š$O(n(c_2 + \log(c_1/c_2)))$
   - **æŒ‘æˆ˜**ï¼šå¦‚ä½•åœ¨å±‚æ¬¡é—´ä¼ é€’ä¿¡æ¯ï¼Ÿ
   - **æ½œåœ¨æ–¹æ³•**ï¼šæ ‘å½¢æ³¨æ„åŠ›ã€é‡‘å­—å¡”pooling

2. **é—®é¢˜**ï¼šåŠ¨æ€å—å¤§å°èƒ½å¦æå‡æ•ˆæœï¼Ÿ
   - **è‡ªé€‚åº”ç­–ç•¥**ï¼š
     - è¯­ä¹‰å¯†é›†åŒºåŸŸï¼šå°å—ï¼ˆ$c=64$ï¼‰æ•è·ç»†èŠ‚
     - è¯­ä¹‰ç¨€ç–åŒºåŸŸï¼šå¤§å—ï¼ˆ$c=512$ï¼‰èŠ‚çœè®¡ç®—
   - **éš¾ç‚¹**ï¼šå¦‚ä½•è‡ªåŠ¨åˆ¤æ–­è¯­ä¹‰å¯†åº¦ï¼Ÿ
   - **æ–¹æ³•**ï¼šä½¿ç”¨è½»é‡çº§é¢„æµ‹å™¨ä¼°è®¡å±€éƒ¨å¤æ‚åº¦

3. **é—®é¢˜**ï¼šé‡å åˆ†å—çš„æœ€ä¼˜ç­–ç•¥ï¼Ÿ
   - **è®¾è®¡**ï¼š
     - å—1: [0, 256]
     - å—2: [128, 384]ï¼ˆ50%é‡å ï¼‰
     - å—3: [256, 512]
   - **é¢å¤–è®¡ç®—**ï¼šå¢åŠ 50%ï¼Œä½†æ¶ˆé™¤è¾¹ç•Œæ•ˆåº”
   - **æŠ˜è¡·**ï¼šæ˜¯å¦å€¼å¾—ï¼Ÿ

**ä¼˜åŒ–æ–¹å‘**ï¼š
- ç ”ç©¶å¿«é€Ÿå¤šæå­æ–¹æ³•ï¼ˆFMMï¼‰ç”¨äºAttention
- æ¢ç´¢ButterflyçŸ©é˜µåˆ†è§£
- å¼€å‘ç¡¬ä»¶å‹å¥½çš„ç¨€ç–æ¨¡å¼

**é‡åŒ–ç›®æ ‡**ï¼š
- å±‚æ¬¡åŒ–åˆ†å—ï¼šå¤æ‚åº¦é™è‡³$O(n\log n)$
- åŠ¨æ€å—å¤§å°ï¼šåœ¨ä¸å¢åŠ è®¡ç®—ä¸‹æå‡æ•ˆæœ5%
- é‡å åˆ†å—ï¼šä»¥1.5xè®¡ç®—æ¢å–8-10%æ€§èƒ½æå‡

---

#### **æ–¹å‘3ï¼šåº”ç”¨å±‚é¢ - å¤šæ¨¡æ€ä¸é•¿ä¸Šä¸‹æ–‡**

**ç ”ç©¶ç©ºç™½**ï¼š
- GAUåœ¨æ–‡æœ¬ç”Ÿæˆä¸ŠéªŒè¯ï¼Œä½†å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘æœªçŸ¥
- Cross-Modal Attentionï¼ˆå›¾æ–‡ã€éŸ³è§†é¢‘ï¼‰èƒ½å¦ç”¨å•å¤´ï¼Ÿ
- 100K+ tokençš„é•¿ä¸Šä¸‹æ–‡å¤„ç†

**å…·ä½“ç ”ç©¶é—®é¢˜**ï¼š

1. **é—®é¢˜**ï¼šVision GAUï¼ˆV-GAUï¼‰çš„è®¾è®¡ï¼Ÿ
   - **æŒ‘æˆ˜**ï¼šå›¾åƒæ˜¯2Dç»“æ„ï¼Œå¦‚ä½•åˆ†å—ï¼Ÿ
   - **æ–¹æ¡ˆ**ï¼š
     - 2Dåˆ†å—ï¼š$\sqrt{c} \times \sqrt{c}$çš„patchå—
     - å±€éƒ¨ï¼šå—å†…2D Attention
     - å…¨å±€ï¼šè·¨å—1Dçº¿æ€§åŒ–
   - **é¢„æœŸ**ï¼šVision Transformerçš„é«˜æ•ˆæ›¿ä»£

2. **é—®é¢˜**ï¼šå¤šæ¨¡æ€èåˆæ—¶çš„GAUè®¾è®¡ï¼Ÿ
   - **åœºæ™¯**ï¼šå›¾åƒEncoder + æ–‡æœ¬Decoder
   - **Cross-GAU**ï¼š
     - $\boldsymbol{Q}$æ¥è‡ªæ–‡æœ¬ï¼Œ$\boldsymbol{K}, \boldsymbol{V}$æ¥è‡ªå›¾åƒ
     - å¯èƒ½éœ€è¦2-4å¤´ï¼ˆè€Œé1å¤´ï¼‰
   - **ç ”ç©¶**ï¼šè·¨æ¨¡æ€çš„å•å¤´å……åˆ†æ€§

3. **é—®é¢˜**ï¼š100K tokençš„è¶…é•¿ä¸Šä¸‹æ–‡ï¼Ÿ
   - **FLASHçš„æŒ‘æˆ˜**ï¼š$c=256$æ—¶éœ€è¦400ä¸ªå—
   - **ä¼˜åŒ–**ï¼š
     - æ›´å¤§çš„$c$ï¼ˆå¦‚$c=1024$ï¼‰
     - ç¨€ç–å…¨å±€æ³¨æ„åŠ›ï¼ˆåªè¿æ¥å…³é”®å—ï¼‰
     - Landmark tokensï¼ˆç±»ä¼¼BigBirdï¼‰

**ä¼˜åŒ–æ–¹å‘**ï¼š
- V-GAUç”¨äºå›¾åƒç”Ÿæˆï¼ˆç±»ä¼¼DiTï¼‰
- Audio-GAUç”¨äºè¯­éŸ³è¯†åˆ«
- Multi-Modal GAUç»Ÿä¸€æ¡†æ¶

**é‡åŒ–ç›®æ ‡**ï¼š
- V-GAUåœ¨ImageNetä¸Šè¾¾åˆ°ViTæ€§èƒ½ï¼Œé€Ÿåº¦å¿«2x
- å¤šæ¨¡æ€GAUåœ¨COCO captionä¸ŠBLEUæå‡3-5åˆ†
- æ”¯æŒ128K tokenï¼Œè®­ç»ƒé€Ÿåº¦ä¸8KæŒå¹³

---

#### **æ–¹å‘4ï¼šå·¥ç¨‹å±‚é¢ - ç¡¬ä»¶ä¼˜åŒ–ä¸éƒ¨ç½²**

**ç ”ç©¶ç©ºç™½**ï¼š
- GAUçš„GPU kernelä¼˜åŒ–
- ç§»åŠ¨ç«¯/è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²
- é‡åŒ–ä¸å‰ªæç­–ç•¥

**å…·ä½“ç ”ç©¶é—®é¢˜**ï¼š

1. **é—®é¢˜**ï¼šå®šåˆ¶CUDA kernelåŠ é€ŸGAUï¼Ÿ
   - **ç“¶é¢ˆ**ï¼š$\boldsymbol{U} \odot \boldsymbol{A}\boldsymbol{V}$çš„èåˆè®¡ç®—
   - **ä¼˜åŒ–**ï¼š
     - Fused kernelï¼šä¸€æ¬¡å®Œæˆé—¨æ§+Attention+æŠ•å½±
     - Tilingç­–ç•¥ï¼šä¼˜åŒ–SRAMåˆ©ç”¨
     - æ··åˆç²¾åº¦ï¼šFP16è®¡ç®—ï¼ŒFP32ç´¯ç§¯
   - **é¢„æœŸ**ï¼šé¢å¤–1.5-2xåŠ é€Ÿ

2. **é—®é¢˜**ï¼šæ¨¡å‹å‹ç¼©åGAUæ€§èƒ½ï¼Ÿ
   - **é‡åŒ–**ï¼š
     - Weight: INT8
     - Activation: INT8
     - Attention: FP16ï¼ˆä¿æŒç²¾åº¦ï¼‰
   - **å‰ªæ**ï¼š
     - ç»“æ„åŒ–å‰ªæï¼šå‡å°‘$e$ï¼ˆ$2d \to 1.5d$ï¼‰
     - éç»“æ„åŒ–å‰ªæï¼š50%ç¨€ç–åº¦
   - **ç ”ç©¶**ï¼šå‹ç¼©åå•å¤´æ˜¯å¦ä»å……åˆ†ï¼Ÿ

3. **é—®é¢˜**ï¼šè¾¹ç¼˜è®¾å¤‡å®æ—¶æ¨ç†ï¼Ÿ
   - **ç›®æ ‡**ï¼šæ‰‹æœºä¸Šå®æ—¶è¿è¡ŒGAU-Small
   - **ä¼˜åŒ–**ï¼š
     - çŸ¥è¯†è’¸é¦ï¼ˆTeacher: GAU-Base â†’ Student: GAU-Tinyï¼‰
     - ARM NEONæŒ‡ä»¤é›†ä¼˜åŒ–
     - åŠ¨æ€æ¨ç†ï¼ˆç®€å•æ ·æœ¬ç”¨æµ…å±‚ï¼‰

**ä¼˜åŒ–æ–¹å‘**ï¼š
- å¼€æºé«˜æ•ˆGAUå®ç°ï¼ˆPyTorchã€JAXã€Tritonï¼‰
- ç§»åŠ¨ç«¯SDK
- äº‘ç«¯æ¨ç†æœåŠ¡ä¼˜åŒ–

**é‡åŒ–ç›®æ ‡**ï¼š
- Fused kernel: 2xåŠ é€Ÿï¼ˆvs naiveå®ç°ï¼‰
- INT8é‡åŒ–: <2%æ€§èƒ½æŸå¤±ï¼Œæ¨ç†é€Ÿåº¦3x
- ç§»åŠ¨ç«¯: iPhoneå®æ—¶æ¨ç†ï¼ˆ<50ms/token for 350M modelï¼‰

