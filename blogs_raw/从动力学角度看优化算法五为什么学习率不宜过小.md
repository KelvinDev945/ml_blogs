---
title: ä»åŠ¨åŠ›å­¦è§’åº¦çœ‹ä¼˜åŒ–ç®—æ³•ï¼ˆäº”ï¼‰ï¼šä¸ºä»€ä¹ˆå­¦ä¹ ç‡ä¸å®œè¿‡å°ï¼Ÿ
slug: ä»åŠ¨åŠ›å­¦è§’åº¦çœ‹ä¼˜åŒ–ç®—æ³•äº”ä¸ºä»€ä¹ˆå­¦ä¹ ç‡ä¸å®œè¿‡å°
date: 2020-10-10
source: https://spaces.ac.cn/archives/7787
tags: æ‘„åŠ¨, å¾®åˆ†æ–¹ç¨‹, åŠ¨åŠ›å­¦, æ¢¯åº¦, ä¼˜åŒ–å™¨
status: completed
tags_reviewed: true
---

# ä»åŠ¨åŠ›å­¦è§’åº¦çœ‹ä¼˜åŒ–ç®—æ³•ï¼ˆäº”ï¼‰ï¼šä¸ºä»€ä¹ˆå­¦ä¹ ç‡ä¸å®œè¿‡å°ï¼Ÿ

**åŸæ–‡é“¾æ¥**: [https://spaces.ac.cn/archives/7787](https://spaces.ac.cn/archives/7787)

---

## 1. æ ¸å¿ƒç†è®ºã€å…¬ç†ä¸å†å²åŸºç¡€

### 1.1 è·¨å­¦ç§‘æ ¹æºï¼šä»æ•°å€¼ç¨³å®šæ€§åˆ°éšå¼åå·®

åœ¨ç»å…¸çš„æ•°å€¼åˆ†æï¼ˆNumerical Analysisï¼‰è¯¾æœ¬ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ æ¬§æ‹‰æ³•ï¼ˆEuler's Methodï¼‰æ±‚è§£å¾®åˆ†æ–¹ç¨‹æ—¶ï¼Œæ€»æ˜¯è¢«å‘ŠçŸ¥ï¼šæ­¥é•¿ $\gamma$ è¶Šå°ï¼Œç¦»æ•£è½¨è¿¹å°±è¶Šæ¥è¿‘çœŸå®è¿ç»­æ›²çº¿ã€‚ä½†åœ¨æ·±åº¦å­¦ä¹ è¿™ä¸ªç‰¹æ®Šçš„åŠ¨åŠ›å­¦ç³»ç»Ÿä¸­ï¼Œè¿™ç§â€œè¶Šç²¾ç»†è¶Šå¥½â€çš„ç›´è§‰å¤±æ•ˆäº†ã€‚å…¶èƒŒåçš„æ ¹æºæ¶‰åŠï¼š
- **å¾®åˆ†åŠ¨åŠ›ç³»ç»Ÿ (Dynamical Systems)**ï¼šä¼˜åŒ–è¿‡ç¨‹ä¸ä»…ä»…æ˜¯é™æ€çš„æ±‚æå€¼ï¼Œæ›´æ˜¯å‚æ•°åœ¨æµå½¢ä¸Šçš„è½¨è¿¹æ¼”åŒ–ã€‚
- **æ§åˆ¶è®º (Control Theory)**ï¼šå­¦ä¹ ç‡å¯¹åº”äºåé¦ˆå›è·¯ä¸­çš„å¢ç›Šå¸¸æ•°ï¼Œå…¶å¤§å°ç›´æ¥å†³å®šäº†ç³»ç»Ÿçš„é˜»å°¼ç‰¹æ€§ã€‚
- **ç»Ÿè®¡åŠ›å­¦ (Statistical Mechanics)**ï¼šç¦»æ•£åŒ–å¼•å…¥çš„å™ªå£°å’Œåå·®ï¼Œå®é™…ä¸Šåœ¨æ”¹å˜ç³»ç»Ÿçš„è‡ªç”±èƒ½æ™¯è§‚ã€‚

### 1.2 å†å²ç¼–å¹´å²

1.  **19ä¸–çºª - é»æ›¼ä¸åºåŠ è±**ï¼šå¥ å®šäº†å¾®åˆ†æ–¹ç¨‹å®šæ€§åˆ†æçš„åŸºç¡€ï¼Œæå‡ºæµï¼ˆFlowï¼‰çš„æ¦‚å¿µã€‚
2.  **1960s - æ•°å€¼ ODE çš„é€†å‘è¯¯å·®åˆ†æ**ï¼šWilkinson ç­‰äººæå‡ºï¼Œç¦»æ•£æ•°å€¼è§£å¾€å¾€æ˜¯æŸä¸ªâ€œé‚»è¿‘â€è¿ç»­ç³»ç»Ÿçš„ç²¾ç¡®è§£ã€‚è¿™ä¸€æ€æƒ³åœ¨ 50 å¹´åæ„å¤–åœ°è§£é‡Šäº†ç¥ç»ç½‘ç»œçš„æ³›åŒ–æ€§ã€‚
3.  **2017 - å¹³å¦æå°å€¼ (Flat Minima)**ï¼šHochreiter çš„å·¥ä½œæŒ‡å‡ºï¼Œæ³›åŒ–æ€§èƒ½å¥½çš„ç‚¹é€šå¸¸ä½äºåœ°åŠ¿å¹³ç¼“çš„åŒºåŸŸã€‚
4.  **2020 - IGR ç†è®ºçš„è¯ç”Ÿ**ï¼šGoogle çš„ Barrett å’Œ Dherin å‘è¡¨äº†ã€ŠImplicit Gradient Regularizationã€‹ï¼Œé¦–æ¬¡å®šé‡è¯æ˜äº†æœ‰é™çš„å­¦ä¹ ç‡ä¼šäº§ç”Ÿæ¢¯åº¦çš„äºŒé˜¶æƒ©ç½šé¡¹ã€‚
5.  **2021-2024 - ç°ä»£ç¼©æ”¾å®šå¾‹**ï¼šç ”ç©¶è€…å¼€å§‹æ¢ç´¢ä¸ºä»€ä¹ˆåœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­ï¼Œå­¦ä¹ ç‡å¿…é¡»ä¸ Batch Size ååŒè°ƒæ•´ï¼Œå…¶ç†è®ºæ ¸å¿ƒæ­£æ˜¯æœ¬æ–‡è¦è®¨è®ºçš„éšå¼æ­£åˆ™åŒ–ã€‚

### 1.3 ä¸¥è°¨å…¬ç†åŒ–

<div class="theorem-box">

### æ ¸å¿ƒå…¬ç†ä½“ç³»ï¼šç¦»æ•£ä¸è¿ç»­çš„æ¡¥æ¢

**å…¬ç† 1 (ç®—å­æ˜ å°„)**ï¼šè®¾ $D = \frac{d}{dt}$ ä¸ºæ—¶é—´å¾®åˆ†ç®—å­ã€‚ç¦»æ•£ä½ç§»ç®—å­ $E_\gamma$ å®šä¹‰ä¸º $E_\gamma \boldsymbol{\theta}(t) = \boldsymbol{\theta}(t+\gamma)$ã€‚åœ¨ç®—å­ä»£æ•°ä¸‹ï¼š
\begin{equation}
E_\gamma = e^{\gamma D} = I + \gamma D + \frac{1}{2}\gamma^2 D^2 + \dots \tag{1}
\end{equation}

**å…¬ç† 2 (æ¢¯åº¦æµæ’ç­‰æ€§)**ï¼šç†æƒ³çš„è¿ç»­ä¼˜åŒ–éµå¾ªè´Ÿæ¢¯åº¦æµï¼š
\begin{equation}
\dot{\boldsymbol{\theta}}(t) = -\nabla L(\boldsymbol{\theta}(t)) \tag{2}
\end{equation}

**å…¬ç† 3 (èƒ½é‡è€—æ•£)**ï¼šä¸€ä¸ªç¨³å®šçš„å­¦ä¹ ç®—æ³•å¿…é¡»åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å•è°ƒåœ°å‡å°‘æŸä¸ªç­‰æ•ˆèƒ½é‡å‡½æ•°ï¼ˆå³ä¿®æ­£æŸå¤±ï¼‰ã€‚

</div>

### 1.4 è®¾è®¡å“²å­¦ï¼šä»¥ç¦»æ•£å®ç°â€œæ™ºèƒ½â€

æœ¬æ–‡è®¨è®ºçš„è®¾è®¡å“²å­¦æ˜¯ï¼š**â€œæ”¾å¼ƒå¯¹ç²¾ç¡®æ€§çš„ç›²ç›®è¿½æ±‚ï¼Œåˆ©ç”¨ç¦»æ•£åŒ–äº§ç”Ÿçš„ç»“æ„æ€§åå·®ã€‚â€** 
å¦‚æœæŠŠä¼˜åŒ–æ¯”ä½œå¯»æ‰¾å±±è°·ï¼Œè¿‡å°çš„å­¦ä¹ ç‡è®©ä½ å˜æˆä¸€åªæå…¶æ•æ„Ÿçš„èš‚èšï¼Œé™·åœ¨æ¯ä¸€ä¸ªå¾®å°çš„æ²™å‘é‡Œï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼›è€Œé€‚å½“å¤§çš„å­¦ä¹ ç‡è®©ä½ å˜æˆä¸€ä¸ªç©¿ç€é›ªæ¿çš„æ»‘é›ªè€…ï¼Œä½ çš„â€œå¤§è„šæ­¥â€è‡ªåŠ¨å¸®ä½ å¿½ç•¥äº†å¾®è§‚çš„ç²—ç³™ï¼Œå¸¦ä½ æ»‘å‘æœ€å¹¿é˜”ã€æœ€ç¨³å®šçš„è°·åº•ã€‚

---

## 2. ä¸¥è°¨çš„æ ¸å¿ƒæ•°å­¦æ¨å¯¼

æœ¬èŠ‚å°†åˆ©ç”¨ç®—ç¬¦çº§æ•°ç†è®ºï¼ˆOperator Calculusï¼‰ï¼Œä»åº•å±‚å…¬å¼ä¸­â€œèƒå–â€å‡ºéšè—çš„æ­£åˆ™åŒ–é¡¹ã€‚

### 2.1 é¢„å¤‡çŸ¥è¯†ï¼šç®—ç¬¦ä»£æ•°ä¸çº§æ•°å±•å¼€

æˆ‘ä»¬è¦è§£å†³çš„æ ¸å¿ƒçŸ›ç›¾æ˜¯ï¼šç¦»æ•£æ›´æ–° $\boldsymbol{\theta}_{n+1} - \boldsymbol{\theta}_n = -\gamma \nabla L$ ä¸è¿ç»­å¯¼æ•° $\dot{\boldsymbol{\theta}}$ ä¹‹é—´çš„æ•°å­¦ä¸å¯¹ç­‰ã€‚

#### 2.1.1 ç®—ç¬¦ä»£æ•°åŸºç¡€

<div class="definition-box">

**å®šä¹‰ï¼ˆä½ç§»ç®—ç¬¦ï¼‰**ï¼šè®¾å‡½æ•° $f(t)$ å…³äºæ—¶é—´å¯å¾®ï¼Œå®šä¹‰å‰å‘ä½ç§»ç®—ç¬¦ï¼š
\begin{equation}
E_\gamma f(t) := f(t + \gamma) \tag{2.1}
\end{equation}

**å®šä¹‰ï¼ˆå¾®åˆ†ç®—ç¬¦ï¼‰**ï¼šæ—¶é—´å¯¼æ•°ç®—ç¬¦å®šä¹‰ä¸ºï¼š
\begin{equation}
D f(t) := \frac{df}{dt} \tag{2.2}
\end{equation}

**æ ¸å¿ƒå…³ç³»ï¼ˆç®—ç¬¦æŒ‡æ•°æ˜ å°„ï¼‰**ï¼šæ ¹æ®æ³°å‹’çº§æ•°ï¼š
\begin{equation}
f(t + \gamma) = \sum_{k=0}^\infty \frac{\gamma^k}{k!} \frac{d^k f}{dt^k} = \left( \sum_{k=0}^\infty \frac{(\gamma D)^k}{k!} \right) f(t) \tag{2.3}
\end{equation}

å› æ­¤å¾—åˆ°ç®—ç¬¦æ’ç­‰å¼ï¼š
\begin{equation}
E_\gamma = e^{\gamma D} \tag{2.4}
\end{equation}

</div>

<div class="formula-step">
<div class="step-label">ğŸ” æ•°å­¦ç»†èŠ‚</div>

å¯¹äºå‚æ•°å‘é‡ $\boldsymbol{\theta}(t) \in \mathbb{R}^d$ï¼Œä½ç§»ç®—ç¬¦é€åˆ†é‡ä½œç”¨ï¼š
\begin{equation}
(E_\gamma \boldsymbol{\theta})_i = \theta_i(t + \gamma), \quad i = 1, \ldots, d \tag{2.5}
\end{equation}

å¾®åˆ†ç®—ç¬¦ $D$ ä¸æ¢¯åº¦ç®—ç¬¦ $\nabla_{\boldsymbol{\theta}}$ çš„å…³ç³»ï¼š
\begin{equation}
D(\nabla L(\boldsymbol{\theta}(t))) = \frac{d}{dt} \nabla L(\boldsymbol{\theta}(t)) = \nabla^2 L(\boldsymbol{\theta}) \cdot \dot{\boldsymbol{\theta}} \tag{2.6}
\end{equation}

è¿™é‡Œ $\nabla^2 L \in \mathbb{R}^{d \times d}$ æ˜¯HessiançŸ©é˜µã€‚
</div>

#### 2.1.2 Bernoulliæ•°ä¸ç”Ÿæˆå‡½æ•°ç†è®º

åœ¨ä¿®æ­£å¾®åˆ†æ–¹ç¨‹çš„æ¨å¯¼ä¸­ï¼Œæ ¸å¿ƒæŠ€æœ¯å·¥å…·æ˜¯Bernoulliç”Ÿæˆå‡½æ•°ã€‚

<div class="theorem-box">

**å®šç†ï¼ˆBernoulliç”Ÿæˆå‡½æ•°ï¼‰**ï¼šå®šä¹‰Bernoulliæ•° $B_k$ é€šè¿‡ç”Ÿæˆå‡½æ•°ï¼š
\begin{equation}
\frac{x}{e^x - 1} = \sum_{k=0}^\infty B_k \frac{x^k}{k!} \tag{2.7}
\end{equation}

**å‰å‡ é¡¹**ï¼š
\begin{align}
B_0 &= 1 \tag{2.8a}\\
B_1 &= -\frac{1}{2} \tag{2.8b}\\
B_2 &= \frac{1}{6} \tag{2.8c}\\
B_3 &= 0 \tag{2.8d}\\
B_4 &= -\frac{1}{30} \tag{2.8e}
\end{align}

**å…³é”®æ€§è´¨**ï¼šå¥‡æ•°é¡¹ï¼ˆé™¤ $B_1$ï¼‰å‡ä¸ºé›¶ï¼š$B_{2k+1} = 0$ for $k \geq 1$ã€‚

</div>

<div class="derivation-box">

### å¼•ç†2.1ï¼šBernoulliå‡½æ•°çš„çº§æ•°å±•å¼€

**å‘½é¢˜**ï¼šç®—ç¬¦å‡½æ•° $\frac{D}{e^{\gamma D} - I}$ å¯ä»¥å±•å¼€ä¸ºï¼š
\begin{equation}
\frac{D}{e^{\gamma D} - I} = \sum_{k=0}^\infty B_k \frac{(\gamma D)^k}{k!} \tag{2.9}
\end{equation}

**è¯æ˜**ï¼šä»¤ $x = \gamma D$ï¼Œç›´æ¥ä»£å…¥ (2.7) å¼ã€‚æ³¨æ„ç®—ç¬¦çº§æ•°åœ¨ $\|\gamma D\| < 2\pi$ æ—¶æ”¶æ•›ï¼ˆCauchyåŠå¾„ï¼‰ã€‚

**å‰ä¸‰é¡¹å±•å¼€**ï¼š
\begin{equation}
\frac{D}{e^{\gamma D} - I} = I - \frac{\gamma D}{2} + \frac{\gamma^2 D^2}{12} + \mathcal{O}(\gamma^4 D^4) \tag{2.10}
\end{equation}

æ³¨æ„ $\gamma^3 D^3$ é¡¹ç³»æ•°ä¸º $B_3/3! = 0$ã€‚

</div>

#### 2.1.3 é€†å‘è¯¯å·®åˆ†æçš„å“²å­¦

<div class="philosophy-box">

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸å…¶é—®"ç¦»æ•£ç®—æ³•æœ‰å¤šæ¥è¿‘è¿ç»­æ–¹ç¨‹"ï¼Œä¸å¦‚é—®**"ç¦»æ•£ç®—æ³•åœ¨ç²¾ç¡®æ±‚è§£å“ªä¸ªä¿®æ­£åçš„è¿ç»­æ–¹ç¨‹"**ã€‚

è¿™ç§æ€ç»´é€†è½¬æºäºæ•°å€¼åˆ†æå¤§å¸ˆDahlquist (1963)ï¼š
- ä¼ ç»Ÿè§†è§’ï¼š$\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - \gamma \nabla L$ æ˜¯ $\dot{\boldsymbol{\theta}} = -\nabla L$ çš„è¿‘ä¼¼è§£
- é€†å‘è§†è§’ï¼š$\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - \gamma \nabla L$ æ˜¯ $\dot{\boldsymbol{\theta}} = -\nabla \tilde{L}$ çš„**ç²¾ç¡®è§£**

å…¶ä¸­ $\tilde{L}$ åŒ…å«äº†æ‰€æœ‰ç¦»æ•£åŒ–æ•ˆåº”ã€‚

</div>

<div class="derivation-box">

### æ¨å¯¼ 5.1ï¼šå»ºç«‹ä¿®æ­£å¾®åˆ†æ–¹ç¨‹ (Modified ODE)

**æ­¥éª¤1ï¼šå†™å‡ºç¦»æ•£æ­¥çš„ç®—å­è¡¨è¾¾**
æ ¹æ®æ³°å‹’å±•å¼€ï¼Œ$\boldsymbol{\theta}(t+\gamma) = e^{\gamma D} \boldsymbol{\theta}(t)$ã€‚æ¢¯åº¦ä¸‹é™çš„æ›´æ–°å¼ä¸ºï¼š
\begin{equation}
(e^{\gamma D} - I) \boldsymbol{\theta}(t) = -\gamma \nabla L(\boldsymbol{\theta}) \tag{3}
\end{equation}

**æ­¥éª¤2ï¼šåè§£å¾®åˆ†ç®—ç¬¦ $D$**
æˆ‘ä»¬éœ€è¦å¾—åˆ° $\dot{\boldsymbol{\theta}}$ ç©¶ç«Ÿåœ¨åšä»€ä¹ˆã€‚ä¸¤è¾¹åŒæ—¶ä½œç”¨ç®—å­ $\frac{D}{e^{\gamma D} - I}$ï¼š
\begin{equation}
D \boldsymbol{\theta} = -\gamma \left[ \frac{D}{e^{\gamma D} - I} \right] \nabla L(\boldsymbol{\theta}) \tag{4}
\end{equation}

**æ­¥éª¤3ï¼šåˆ©ç”¨ Bernoulli çº§æ•°å±•å¼€æ ¸å¿ƒç®—ç¬¦**
æ•°å­¦ä¸Šï¼Œ$\frac{x}{e^x - 1}$ çš„çº§æ•°å±•å¼€ç³»æ•°æ˜¯ Bernoulli æ•° $B_k$ï¼š
\begin{equation}
\frac{x}{e^x - 1} = 1 - \frac{x}{2} + \frac{x^2}{12} - \frac{x^4}{720} + \dots \tag{5}
\end{equation}
ä»¤ $x = \gamma D$ï¼Œä»£å…¥ (4)ï¼š
\begin{equation}
\dot{\boldsymbol{\theta}} = -\left( I - \frac{\gamma}{2} D + \frac{\gamma^2}{12} D^2 + \dots \right) \nabla L(\boldsymbol{\theta}) \tag{6}
\end{equation}

</div>

### 2.2 åµŒå¥—ä»£æ¢ï¼šä»æ—¶é—´å¯¼æ•°åˆ°ç©ºé—´å¯¼æ•°

(6) å¼å³ç«¯åŒ…å« $D (\nabla L)$ï¼Œè¿™ä¾ç„¶æ˜¯å…³äºæ—¶é—´çš„å¯¼æ•°ã€‚æˆ‘ä»¬è¦æŠŠå®ƒè½¬åŒ–æˆçº¯ç²¹å…³äºå‚æ•° $\boldsymbol{\theta}$ çš„è¡¨è¾¾å¼ã€‚

<div class="derivation-box">

### æ¨å¯¼ 5.2ï¼šä¸€é˜¶ä¿®æ­£é¡¹çš„ç²¾ç¡®è§£æ

**æ­¥éª¤1ï¼šè®¡ç®—æ¢¯åº¦çš„éšæµå¯¼æ•°**
æ ¹æ®é“¾å¼æ³•åˆ™ï¼Œ$D$ ä½œç”¨äº $\nabla L$ ç­‰äºï¼š
\begin{equation}
D(\nabla L) = \frac{d}{dt} \nabla L(\boldsymbol{\theta}) = \nabla^2 L(\boldsymbol{\theta}) \dot{\boldsymbol{\theta}} \tag{7}
\end{equation}
å…¶ä¸­ $\nabla^2 L$ æ˜¯æŸå¤±å‡½æ•°çš„ Hessian çŸ©é˜µã€‚

**æ­¥éª¤2ï¼šé€’å½’ä»£å…¥åˆé˜¶è¿‘ä¼¼**
ä» (6) å¼å¯çŸ¥ï¼Œ$\dot{\boldsymbol{\theta}} = -\nabla L + \mathcal{O}(\gamma)$ã€‚æˆ‘ä»¬å°†è¿™ä¸ªç»“è®ºä»£å…¥ (7) å¼ï¼š
\begin{equation}
D(\nabla L) \approx \nabla^2 L(\boldsymbol{\theta}) [-\nabla L(\boldsymbol{\theta})] \tag{8}
\end{equation}

**æ­¥éª¤3ï¼šé‡æ„ä¿®æ­£åçš„ä¸‹é™æ–¹å‘**
å°† (8) ä»£å› (6) çš„å‰ä¸¤é¡¹ï¼š
\begin{equation}
\dot{\boldsymbol{\theta}} \approx -\nabla L(\boldsymbol{\theta}) + \frac{\gamma}{2} \left[ \nabla^2 L(\boldsymbol{\theta}) (-\nabla L(\boldsymbol{\theta})) \right] \tag{9}
\end{equation}
\begin{equation}
\dot{\boldsymbol{\theta}} \approx -\left( \nabla L(\boldsymbol{\theta}) + \frac{\gamma}{2} \nabla^2 L(\boldsymbol{\theta}) \nabla L(\boldsymbol{\theta}) \right) \tag{10}
\end{equation}

</div>

### 2.3 éšå¼æ­£åˆ™åŒ–é¡¹çš„è¯†åˆ«

(10) å¼ä¸­çš„ç¬¬äºŒé¡¹å…·æœ‰æå…¶ç‰¹æ®Šçš„å‡ ä½•æ„ä¹‰ã€‚

<div class="formula-explanation">

### æ ¸å¿ƒç»“è®ºï¼šä¿®æ­£æŸå¤±å‡½æ•° $\tilde{L}$

\begin{equation}
\dot{\boldsymbol{\theta}} \approx -\nabla_{\boldsymbol{\theta}} \left( L(\boldsymbol{\theta}) + \frac{\gamma}{4} \|\nabla L(\boldsymbol{\theta})\|^2 \right) \tag{11}
\end{equation}

<div class="formula-step">
<div class="step-label">æ•°å­¦æ’ç­‰å¼éªŒè¯</div>
æ³¨æ„ï¼š$\nabla_{\boldsymbol{\theta}} (\|\nabla L\|^2) = \nabla_{\boldsymbol{\theta}} (\langle \nabla L, \nabla L \rangle) = 2 \nabla^2 L \nabla L$ã€‚
</div>

<div class="formula-step">
<div class="step-label">éšå¼æƒ©ç½šé¡¹è§£æ</div>
ä¿®æ­£åçš„æœ‰æ•ˆæŸå¤±å‡½æ•°ä¸º $\tilde{L}(\boldsymbol{\theta}) = L(\boldsymbol{\theta}) + \frac{\gamma}{4} \|\nabla L(\boldsymbol{\theta})\|^2$ã€‚
</div>

<div class="formula-step">
<div class="step-label">ç‰©ç†ç›´è§‰</div>
è¿™è¯´æ˜ï¼š**ç¦»æ•£æ¢¯åº¦ä¸‹é™åœ¨æœ€å°åŒ–åŸæŸå¤±çš„åŒæ—¶ï¼Œä¹Ÿåœ¨æ‹¼å‘½åœ°æŠŠæ¢¯åº¦çš„æ¨¡é•¿å‹å°ã€‚**
</div>

</div>

### 2.4 äºŒé˜¶ä¿®æ­£ï¼šHessian è°±çš„åŠ¨æ€å‹ç¼©

<div class="derivation-box">

### æ¨å¯¼ 5.3ï¼šä¿ç•™åˆ° $\mathcal{O}(\gamma^2)$ çš„å®Œæ•´å±•å¼€

**ç›®æ ‡**ï¼šæ¨å¯¼åŒ…å«äºŒé˜¶ä¿®æ­£é¡¹çš„ä¿®æ­£æŸå¤±å‡½æ•°ã€‚

**æ­¥éª¤1ï¼šäºŒé˜¶ç®—ç¬¦ä½œç”¨**

ä» (2.10) å¼ï¼Œä¿ç•™åˆ° $\gamma^2$ é¡¹ï¼š
\begin{equation}
\dot{\boldsymbol{\theta}} = -\left( I - \frac{\gamma}{2} D + \frac{\gamma^2}{12} D^2 \right) \nabla L(\boldsymbol{\theta}) + \mathcal{O}(\gamma^3) \tag{2.11}
\end{equation}

**æ­¥éª¤2ï¼šå±•å¼€ $D(\nabla L)$**

æ ¹æ®é“¾å¼æ³•åˆ™ï¼š
\begin{equation}
D(\nabla L) = \nabla^2 L \cdot \dot{\boldsymbol{\theta}} = -\nabla^2 L \nabla L + \mathcal{O}(\gamma) \tag{2.12}
\end{equation}

å…¶ä¸­ $\nabla^2 L = H$ æ˜¯HessiançŸ©é˜µã€‚

**æ­¥éª¤3ï¼šå±•å¼€ $D^2(\nabla L)$**

ç»§ç»­æ±‚å¯¼ï¼š
\begin{align}
D^2(\nabla L) &= D(\nabla^2 L \cdot \dot{\boldsymbol{\theta}}) \tag{2.13a}\\
&= \left[ D(\nabla^2 L) \right] \dot{\boldsymbol{\theta}} + \nabla^2 L \cdot D(\dot{\boldsymbol{\theta}}) \tag{2.13b}\\
&= \left[ \sum_{i} \frac{\partial \nabla^2 L}{\partial \theta_i} \dot{\theta}_i \right] \dot{\boldsymbol{\theta}} + \nabla^2 L \cdot D(\dot{\boldsymbol{\theta}}) \tag{2.13c}
\end{align}

åœ¨æœ€ä½é˜¶è¿‘ä¼¼ä¸‹ï¼Œ$\dot{\boldsymbol{\theta}} = -\nabla L$ï¼Œå› æ­¤ï¼š
\begin{equation}
D^2(\nabla L) \approx -\left[ \sum_i \frac{\partial H}{\partial \theta_i} (\nabla L)_i \right] \nabla L + H \cdot H \nabla L \tag{2.14}
\end{equation}

**æ­¥éª¤4ï¼šç»„è£…ä¿®æ­£å¾®åˆ†æ–¹ç¨‹**

å°† (2.12) å’Œ (2.14) ä»£å…¥ (2.11)ï¼š
\begin{align}
\dot{\boldsymbol{\theta}} &\approx -\nabla L + \frac{\gamma}{2} H \nabla L - \frac{\gamma^2}{12} \left[ \nabla H[\nabla L, \nabla L] - H^2 \nabla L \right] \tag{2.15}
\end{align}

å…¶ä¸­ $\nabla H[\mathbf{u}, \mathbf{v}] := \sum_i \frac{\partial H}{\partial \theta_i} u_i v_j$ æ˜¯ä¸‰é˜¶å¼ é‡çš„ç¼©å¹¶ã€‚

</div>

<div class="theorem-box">

### å®šç†2.1ï¼šä¿®æ­£æŸå¤±å‡½æ•°çš„äºŒé˜¶å±•å¼€

**å‘½é¢˜**ï¼šç¦»æ•£æ¢¯åº¦ä¸‹é™ $\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - \gamma \nabla L(\boldsymbol{\theta}_n)$ å¯è§†ä¸ºä»¥ä¸‹ä¿®æ­£æŸå¤±å‡½æ•°çš„è¿ç»­æ¢¯åº¦æµï¼š
\begin{equation}
\tilde{L}(\boldsymbol{\theta}) = L(\boldsymbol{\theta}) + \frac{\gamma}{4} \|\nabla L(\boldsymbol{\theta})\|^2 + \frac{\gamma^2}{24} \langle \nabla L, H^2 \nabla L \rangle + \mathcal{O}(\gamma^3) \tag{2.16}
\end{equation}

**è¯æ˜**ï¼šéªŒè¯ $\dot{\boldsymbol{\theta}} = -\nabla \tilde{L}$ã€‚

**è®¡ç®— $\nabla \tilde{L}$**ï¼š
\begin{align}
\nabla \tilde{L} &= \nabla L + \frac{\gamma}{4} \nabla(\|\nabla L\|^2) + \frac{\gamma^2}{24} \nabla(\langle \nabla L, H^2 \nabla L \rangle) \tag{2.17a}\\
&= \nabla L + \frac{\gamma}{2} H \nabla L + \frac{\gamma^2}{12} \left[ (\nabla H[\nabla L, \nabla L])^T + H^2 \nabla L \right] + \dots \tag{2.17b}
\end{align}

æ³¨æ„åˆ°ï¼š
\begin{equation}
\nabla(\langle \mathbf{u}, A \mathbf{u} \rangle) = 2A\mathbf{u} + (\nabla A)[\mathbf{u}, \mathbf{u}] \tag{2.18}
\end{equation}

å› æ­¤ (2.17b) ä¸ (2.15) ä¸€è‡´ï¼ˆåœ¨å¿½ç•¥ä¸‰é˜¶å¯¼æ•°é¡¹åï¼‰ã€‚

</div>

#### 2.4.1 Hessianç‰¹å¾å€¼çš„æƒ©ç½šæœºåˆ¶

<div class="intuition-box">

**å‡ ä½•è§£é‡Š**ï¼šäºŒé˜¶é¡¹ $\langle \nabla L, H^2 \nabla L \rangle$ çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

è®¾ $H$ çš„ç‰¹å¾åˆ†è§£ä¸º $H = Q \Lambda Q^T$ï¼Œå…¶ä¸­ $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_d)$ã€‚

ä»¤ $\mathbf{g} = Q^T \nabla L$ ä¸ºæ¢¯åº¦åœ¨ç‰¹å¾ç©ºé—´çš„æŠ•å½±ï¼Œåˆ™ï¼š
\begin{equation}
\langle \nabla L, H^2 \nabla L \rangle = \sum_{i=1}^d \lambda_i^2 g_i^2 \tag{2.19}
\end{equation}

**å…³é”®è§‚å¯Ÿ**ï¼š
- å¦‚æœå‚æ•°æ²¿ç€"é™¡å³­æ–¹å‘"ï¼ˆ$\lambda_i$ å¤§ï¼‰å˜åŒ–ï¼ˆ$g_i$ å¤§ï¼‰ï¼Œæƒ©ç½šé¡¹ $\propto \lambda_i^2 g_i^2$ æ€¥å‰§å¢å¤§
- è¿™ç­‰ä»·äº**å¯¹é«˜æ›²ç‡æ–¹å‘çš„æ¢¯åº¦è¿›è¡ŒäºŒæ¬¡æƒ©ç½š**
- ç»“æœï¼šä¼˜åŒ–å™¨è¢«"æ¨"å‘é‚£äº›å„å‘åŒæ€§ï¼ˆisotropicï¼‰çš„æå°å€¼ï¼Œå³æ‰€æœ‰ç‰¹å¾å€¼éƒ½è¾ƒå°ä¸”å‡åŒ€

</div>

#### 2.4.2 ä¿®æ­£Hessiançš„ç‰¹å¾å€¼åˆ†æ

<div class="derivation-box">

### å¼•ç†2.2ï¼šæå°å€¼ç‚¹å¤„çš„ä¿®æ­£Hessian

**æ¡ä»¶**ï¼šè€ƒè™‘ä¸´ç•Œç‚¹ $\boldsymbol{\theta}^*$ï¼Œæ»¡è¶³ $\nabla L(\boldsymbol{\theta}^*) = 0$ã€‚

**ä¿®æ­£æŸå¤±çš„Hessian**ï¼š
\begin{equation}
\tilde{H}(\boldsymbol{\theta}^*) = \nabla^2 \tilde{L}(\boldsymbol{\theta}^*) = H + \frac{\gamma}{2} H^2 + \mathcal{O}(\gamma^2) \tag{2.20}
\end{equation}

**ç‰¹å¾å€¼å…³ç³»**ï¼šè®¾ $H$ çš„ç‰¹å¾å€¼ä¸º $\{\lambda_i\}$ï¼Œåˆ™ $\tilde{H}$ çš„ç‰¹å¾å€¼æ»¡è¶³ï¼š
\begin{equation}
\tilde{\lambda}_i = \lambda_i + \frac{\gamma}{2} \lambda_i^2 + \mathcal{O}(\gamma^2) = \lambda_i \left( 1 + \frac{\gamma \lambda_i}{2} \right) \tag{2.21}
\end{equation}

**è§£é‡Š**ï¼š
- å¦‚æœ $\lambda_i$ å¾ˆå¤§ï¼ˆé™¡å³­æ–¹å‘ï¼‰ï¼Œ$\tilde{\lambda}_i$ ä¼šè¢«**æ”¾å¤§**æ›´å¤š
- è¿™ä½¿å¾—é™¡å³­çš„æå°å€¼åœ¨ä¿®æ­£æŸå¤±çœ‹æ¥"æ›´ä¸ç¨³å®š"
- ä¼˜åŒ–å™¨è‡ªç„¶å€¾å‘äºé¿å¼€è¿™äº›åŒºåŸŸ

</div>

<div class="formula-explanation">

### ğŸ¯ æ ¸å¿ƒæ´å¯Ÿï¼šå­¦ä¹ ç‡ä½œä¸º"æ›²ç‡æ”¾å¤§å™¨"

ä» (2.21) å¼å¯ä»¥çœ‹å‡ºï¼Œå­¦ä¹ ç‡ $\gamma$ æœ¬è´¨ä¸Šåœ¨è°ƒèŠ‚ä¸åŒæ›²ç‡æ–¹å‘çš„ç›¸å¯¹é‡è¦æ€§ï¼š

| æ›²ç‡ $\lambda_i$ | ä¿®æ­£ç³»æ•° $1 + \gamma \lambda_i / 2$ | æ•ˆåº” |
|:---|:---|:---|
| å°æ›²ç‡ï¼ˆå¹³å¦ï¼‰ | $\approx 1$ | **å‡ ä¹ä¸å˜** |
| å¤§æ›²ç‡ï¼ˆé™¡å³­ï¼‰ | $\gg 1$ | **æ˜¾è‘—æ”¾å¤§** |

**ç»“è®º**ï¼š**å¤§å­¦ä¹ ç‡ = ä¼˜å…ˆæƒ©ç½šé«˜æ›²ç‡åŒºåŸŸ**ï¼Œè¿™æ­£æ˜¯"å¹³å¦æ€§å¯»ä¼˜"çš„æ•°å­¦æ ¹æºã€‚

</div>

#### 2.4.3 ç¨³å®šæ€§è¾¹ç•Œï¼šEdge of Stability

<div class="warning-box">

**âš ï¸ è‡´å‘½ç¼ºé™·ï¼šç¨³å®šæ€§ä¸´ç•Œç‚¹**

ä» (2.21) å¼ï¼Œå¦‚æœè¦ä¿è¯ä¿®æ­£Hessian $\tilde{H}$ ä¿æŒæ­£å®šï¼ˆä¼˜åŒ–å¯æ”¶æ•›ï¼‰ï¼Œéœ€è¦ï¼š
\begin{equation}
\tilde{\lambda}_i = \lambda_i \left( 1 + \frac{\gamma \lambda_i}{2} \right) > 0, \quad \forall i \tag{2.22}
\end{equation}

å½“ $\lambda_i > 0$ æ—¶ï¼ˆå±€éƒ¨æå°å€¼ï¼‰ï¼Œè¿™è‡ªåŠ¨æ»¡è¶³ã€‚ä½†æ•°å€¼ç¨³å®šæ€§è¦æ±‚æ›´å¼ºçš„æ¡ä»¶ï¼š**ä¸èƒ½æ— é™å¤§**ã€‚

**ä¸´ç•Œæ¡ä»¶**ï¼ˆCohen et al. 2021, "Edge of Stability"ï¼‰ï¼š
\begin{equation}
\gamma < \frac{2}{\lambda_{\max}(H)} \tag{2.23}
\end{equation}

**ç‰©ç†æ„ä¹‰**ï¼šå­¦ä¹ ç‡å¿…é¡»ä¸æŸå¤±å‡½æ•°æœ€å¤§æ›²ç‡æˆåæ¯”ã€‚ä¸€æ—¦è¶…è¿‡æ­¤é˜ˆå€¼ï¼Œå‚æ•°ä¼šåœ¨é™¡å³­æ–¹å‘"éœ‡è¡å‘æ•£"ã€‚

</div>

<div class="example-box">

### æ•°å€¼ç¤ºä¾‹ï¼šäºŒæ¬¡å‡½æ•°çš„ä¸´ç•Œå­¦ä¹ ç‡

**é—®é¢˜è®¾å®š**ï¼šè€ƒè™‘äºŒæ¬¡æŸå¤± $L(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T A \mathbf{x}$ï¼Œå…¶ä¸­ï¼š
\begin{equation}
A = \begin{pmatrix} 100 & 0 \\ 0 & 1 \end{pmatrix} \tag{2.24}
\end{equation}

**Hessian**ï¼š$H = A$ï¼Œç‰¹å¾å€¼ $\lambda_1 = 100, \lambda_2 = 1$ã€‚

**ç¨³å®šæ€§è¦æ±‚**ï¼š
\begin{equation}
\gamma < \frac{2}{100} = 0.02 \tag{2.25}
\end{equation}

**æ•°å€¼éªŒè¯**ï¼š
- å½“ $\gamma = 0.01$ï¼ˆå®‰å…¨åŒºï¼‰ï¼šæ”¶æ•›å¹³ç¨³
- å½“ $\gamma = 0.019$ï¼ˆä¸´ç•Œç‚¹ï¼‰ï¼šè½»å¾®éœ‡è¡ä½†æ”¶æ•›
- å½“ $\gamma = 0.021$ï¼ˆè¶…ä¸´ç•Œï¼‰ï¼šå‚æ•°çˆ†ç‚¸ï¼ŒæŸå¤± â†’ $\infty$

**ä¿®æ­£æ›²ç‡**ï¼ˆ$\gamma = 0.01$ï¼‰ï¼š
\begin{align}
\tilde{\lambda}_1 &= 100 \times (1 + 0.01 \times 100 / 2) = 100 \times 1.5 = 150 \tag{2.26a}\\
\tilde{\lambda}_2 &= 1 \times (1 + 0.01 \times 1 / 2) = 1.005 \tag{2.26b}
\end{align}

**è§‚å¯Ÿ**ï¼šé™¡å³­æ–¹å‘ï¼ˆ$\lambda_1$ï¼‰è¢«æ”¾å¤§50%ï¼Œå¹³å¦æ–¹å‘ï¼ˆ$\lambda_2$ï¼‰å‡ ä¹ä¸å˜ã€‚

</div>

---

## 3. æ•°å­¦ç›´è§‰ã€å‡ ä½•è§†è§’ä¸å¤šç»´ç±»æ¯”

<div class="intuition-box">

### ğŸ§  ç›´è§‰ç†è§£ï¼šå¤§é›¨ä¸­çš„â€œæ²³é“å†²åˆ·â€ ğŸŒ§ï¸

æƒ³è±¡æŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªå¤æ‚çš„æ²³åºŠåœ°è²Œã€‚

*   **æå°å­¦æ­¥ï¼ˆ$\gamma \to 0$ï¼‰**ï¼šå°±åƒæ¯›æ¯›ç»†é›¨ã€‚æ°´æµæå…¶æ¸©å’Œï¼Œå®ƒä¼šé¡ºç€æ¯ä¸€ä¸ªç»†å°çš„æ³¥å‘æµæ·Œï¼Œæœ€ååœåœ¨ç¦»å®ƒæœ€è¿‘çš„æŒ‡ç”²ç›–å¤§å°çš„ç§¯æ°´å‘é‡Œã€‚è¿™äº›å‘å¯èƒ½æ˜¯è®­ç»ƒé›†ä¸Šçš„å™ªå£°ã€‚
*   **å¤§å­¦ä¹ ç‡ï¼ˆæœ‰é™ $\gamma$ï¼‰**ï¼šå°±åƒä¸€åœºæš´é›¨ã€‚æ´ªæ°´æ»šæ»šè€Œæ¥ï¼Œç”±äºæ°´é‡å·¨å¤§ï¼ˆæ­¥é•¿å®½ï¼‰ï¼Œå®ƒæ ¹æœ¬ä¸åœ¨ä¹å°çš„æ³¥å‘ï¼Œè€Œæ˜¯åˆ©ç”¨å·¨å¤§çš„åŠ¨èƒ½ç›´æ¥å†²åˆ·æ‰éšœç¢ï¼Œæ±‡èšåˆ°æœ€å®½å¹¿ã€æœ€æ·±çš„ä¸»æ²³é“é‡Œã€‚
*   **ç»“è®º**ï¼š**å¤§æ°´å†²å¤§è·¯**ã€‚åªæœ‰å…·å¤‡ä¸€å®šè§„æ¨¡çš„â€œåŠ¨åŠ›å­¦åŠ¿èƒ½â€ï¼Œç®—æ³•æ‰èƒ½è·¨è¿‡å¾®è§‚çš„ä¸è§„åˆ™æ€§ï¼Œæ‰¾åˆ°å…·æœ‰æ™®é€‚è§„å¾‹çš„å®è§‚æå°å€¼ã€‚

</div>

### 3.2 å‡ ä½•è§†è§’ï¼šèƒ½é‡æ™¯è§‚çš„å¹³æ»‘åŒ– (Landscape Smoothing)

åœ¨å‚æ•°ç©ºé—´ä¸­ï¼Œå°–é”çš„æå°å€¼ï¼ˆSharp Minimaï¼‰è¡¨ç°ä¸ºæ¢¯åº¦çš„å¿«é€Ÿå‰§çƒˆå˜åŒ–ã€‚
- æ ¹æ® (13) å¼ï¼ŒIGR é¡¹ $\frac{\gamma}{4}\|\nabla L\|^2$ åœ¨é™¡å³­åŒºåŸŸçš„å€¼æå¤§ã€‚
- è¿™åœ¨å‡ ä½•ä¸Šç›¸å½“äºç»™æ¯ä¸€ä¸ªå°–é”çš„å‘ä½éƒ½ç›–ä¸Šäº†ä¸€å±‚â€œè™šå‡çš„å¹³åŸâ€ã€‚
- ä¼˜åŒ–ç®—æ³•åœ¨çœ‹åˆ°è¿™äº›è¢«ä¿®æ­£åçš„æ™¯è§‚æ—¶ï¼Œä¼šè§‰å¾—å°–é”çš„å‘â€œä¸åˆ’ç®—â€ï¼Œä»è€Œè‡ªç„¶è€Œç„¶åœ°æ»‘å‘é‚£äº›æ›²ç‡æ›´ä½ã€åœ°åŠ¿æ›´å¹³å¦çš„å¹¿å¤§ç›†åœ°ã€‚

### 3.3 ä¿¡æ¯è®ºè§†è§’ï¼šè®¡ç®—ä½œä¸ºä¸€ç§æ­£åˆ™é¡¹

é€šå¸¸æˆ‘ä»¬è®¤ä¸ºæ­£åˆ™åŒ–å¿…é¡»æ˜¾å¼å†™åœ¨ Loss é‡Œï¼ˆå¦‚ Weight Decayï¼‰ã€‚
æœ¬æ–‡çš„æ¨å¯¼æ­ç¤ºäº†ï¼š**è®¡ç®—è¿‡ç¨‹æœ¬èº«å°±æ˜¯æ­£åˆ™åŒ–ã€‚**
- ç¦»æ•£åŒ– = ä¿¡æ¯çš„æœ‰æŸé‡‡æ ·ã€‚
- è¾ƒå¤§çš„ $\gamma$ ç›¸å½“äºåœ¨å‚æ•°ç©ºé—´è¿›è¡Œä½é€šæ»¤æ³¢ï¼ˆLow-pass filteringï¼‰ï¼Œè¿‡æ»¤æ‰é«˜é¢‘çš„ã€æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆçš„ç»†èŠ‚ä¿¡å·ã€‚

<div class="insight-box">

### ğŸ“¡ ä¿¡å·å¤„ç†ç±»æ¯”ï¼šå­¦ä¹ ç‡ä½œä¸ºæ»¤æ³¢å™¨

å°†æŸå¤±æ™¯è§‚è§†ä¸ºäºŒç»´"åœ°å½¢ä¿¡å·" $L(x, y)$ï¼š

**å°å­¦ä¹ ç‡ï¼ˆ$\gamma \to 0$ï¼‰**ï¼š
- å¯¹åº”äº**é«˜é€šæ»¤æ³¢å™¨**ï¼ˆHigh-pass filterï¼‰
- ä¿ç•™æ‰€æœ‰é«˜é¢‘ç»†èŠ‚ï¼ˆå™ªå£°ã€è¿‡æ‹Ÿåˆç‰¹å¾ï¼‰
- å‚æ•°è½¨è¿¹ = é€åƒç´ æ‰«æï¼Œé™·å…¥å¾®è§‚çº¹ç†

**å¤§å­¦ä¹ ç‡ï¼ˆæœ‰é™ $\gamma$ï¼‰**ï¼š
- å¯¹åº”äº**ä½é€šæ»¤æ³¢å™¨**ï¼ˆLow-pass filterï¼‰
- å¹³æ»‘æ‰é«˜é¢‘å™ªå£°ï¼Œä¿ç•™ä½é¢‘ä¸»ç»“æ„
- å‚æ•°è½¨è¿¹ = ç­‰é«˜çº¿æ»‘é™ï¼Œç›´è¾¾ä¸»å±±è°·

**æ•°å­¦å½¢å¼åŒ–**ï¼šä¿®æ­£æŸå¤± $\tilde{L}$ çš„é¢‘åŸŸç‰¹æ€§ï¼š
\begin{equation}
\tilde{L}(\omega) = L(\omega) \cdot \left( 1 + \frac{\gamma}{4} \omega^2 \right) \tag{3.1}
\end{equation}

å…¶ä¸­ $\omega$ æ˜¯ç©ºé—´é¢‘ç‡ï¼ˆå¯¹åº”æ›²ç‡ï¼‰ã€‚é«˜é¢‘åˆ†é‡è¢« $(1 + \gamma \omega^2)$ æŠ‘åˆ¶ã€‚

</div>

### 3.4 ç»Ÿè®¡åŠ›å­¦è§†è§’ï¼šæ¸©åº¦ä¸å¹³è¡¡åˆ†å¸ƒ

<div class="analogy-box">

### ğŸŒ¡ï¸ ç‰©ç†ç±»æ¯”ï¼šä¼˜åŒ– = å†·å´è¿‡ç¨‹

å°†ä¼˜åŒ–è¿‡ç¨‹ç±»æ¯”ä¸º**ç»Ÿè®¡åŠ›å­¦ä¸­çš„é€€ç«**ï¼ˆAnnealingï¼‰ï¼š

**é…åˆ†å‡½æ•°**ï¼ˆPartition functionï¼‰ï¼š
\begin{equation}
Z(T) = \int e^{-L(\boldsymbol{\theta})/T} d\boldsymbol{\theta} \tag{3.2}
\end{equation}

å…¶ä¸­"æ¸©åº¦" $T$ å¯¹åº”äºå­¦ä¹ ç‡ $\gamma$ï¼ˆæ›´ç²¾ç¡®åœ°è¯´ï¼Œ$T \propto \gamma$ï¼‰ã€‚

**å¹³è¡¡åˆ†å¸ƒ**ï¼ˆBoltzmann distributionï¼‰ï¼š
\begin{equation}
p(\boldsymbol{\theta}) = \frac{1}{Z(T)} e^{-L(\boldsymbol{\theta})/T} \tag{3.3}
\end{equation}

**ç‰©ç†ç›´è§‰**ï¼š
- **é«˜æ¸©ï¼ˆå¤§ $\gamma$ï¼‰**ï¼šç²’å­ï¼ˆå‚æ•°ï¼‰æœ‰è¶³å¤Ÿèƒ½é‡è·¨è¶ŠåŠ¿å’ï¼Œæ¢ç´¢å¹¿é˜”åŒºåŸŸ
- **ä½æ¸©ï¼ˆå° $\gamma$ï¼‰**ï¼šç²’å­è¢«å›°åœ¨å±€éƒ¨åŠ¿é˜±ï¼ˆè¿‡æ‹Ÿåˆï¼‰
- **æœ€ä¼˜æ¸©åº¦**ï¼šå¹³è¡¡"é€ƒé€¸èƒ½åŠ›"ä¸"æ”¶æ•›ç²¾åº¦"

**è‡ªç”±èƒ½æ³›å‡½**ï¼ˆFree energy functionalï¼‰ï¼š
\begin{equation}
F(T) = \mathbb{E}[L] + T \cdot S \tag{3.4}
\end{equation}

å…¶ä¸­ $S$ æ˜¯ç†µï¼ˆentropyï¼‰ã€‚å¤§å­¦ä¹ ç‡é€šè¿‡å¢åŠ ç†µé¡¹ï¼Œæ¨åŠ¨ç³»ç»Ÿå¯»æ‰¾"ç†µå¤§"ï¼ˆå³å¹³å¦ã€é²æ£’ï¼‰çš„æå°å€¼ã€‚

</div>

### 3.5 æµå½¢å‡ ä½•ï¼šæµ‹åœ°çº¿ä¸æ›²ç‡

<div class="geometry-box">

### ğŸŒ é»æ›¼å‡ ä½•è§†è§’

å°†å‚æ•°ç©ºé—´ $\mathbb{R}^d$ èµ‹äºˆç”±æŸå¤±å‡½æ•°è¯±å¯¼çš„åº¦é‡ï¼š
\begin{equation}
g_{ij} = \frac{\partial^2 L}{\partial \theta_i \partial \theta_j} = H_{ij} \tag{3.5}
\end{equation}

è¿™å®šä¹‰äº†ä¸€ä¸ª**é»æ›¼æµå½¢** $(\mathbb{R}^d, g)$ã€‚

**æµ‹åœ°çº¿æ–¹ç¨‹**ï¼ˆGeodesic equationï¼‰ï¼š
\begin{equation}
\ddot{\theta}^k + \Gamma^k_{ij} \dot{\theta}^i \dot{\theta}^j = 0 \tag{3.6}
\end{equation}

å…¶ä¸­ $\Gamma^k_{ij}$ æ˜¯Christoffelç¬¦å·ã€‚

**æ¢¯åº¦ä¸‹é™ vs è‡ªç„¶æ¢¯åº¦**ï¼š
- **æ¬§æ°æ¢¯åº¦ä¸‹é™**ï¼š$\dot{\boldsymbol{\theta}} = -\nabla L$ï¼ˆå¿½ç•¥æµå½¢ç»“æ„ï¼‰
- **è‡ªç„¶æ¢¯åº¦ä¸‹é™**ï¼š$\dot{\boldsymbol{\theta}} = -H^{-1} \nabla L$ï¼ˆæ²¿æµ‹åœ°çº¿ï¼‰

**IGRçš„å‡ ä½•è§£é‡Š**ï¼š
ä¿®æ­£æŸå¤± $\tilde{L} = L + \frac{\gamma}{4} \|\nabla L\|^2$ å¼•å…¥äº†é¢å¤–çš„"æ›²ç‡åŠ¿èƒ½"ï¼Œä½¿å¾—ä¼˜åŒ–å™¨å¤©ç„¶é¿å¼€"é«˜æ–¯æ›²ç‡"å¤§çš„åŒºåŸŸã€‚

**é«˜æ–¯æ›²ç‡**ï¼ˆGaussian curvatureï¼‰ï¼š
\begin{equation}
K \propto \det(H) \tag{3.7}
\end{equation}

å¤§å­¦ä¹ ç‡æƒ©ç½š $\|\nabla L\|^2$ å®é™…ä¸Šåœ¨é—´æ¥æƒ©ç½š $K$ï¼Œå› ä¸ºæ¢¯åº¦å¤§çš„åœ°æ–¹å¾€å¾€æ›²ç‡ä¹Ÿå¤§ã€‚

</div>

### 3.6 æ¦‚ç‡è®ºè§†è§’ï¼šéšæœºæ€§ä¸æ­£åˆ™åŒ–

<div class="theorem-box">

### å®šç†3.1ï¼šå°æ‰¹é‡å™ªå£°çš„é¢å¤–æ­£åˆ™åŒ–

**èƒŒæ™¯**ï¼šå®é™…è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å°æ‰¹é‡æ¢¯åº¦ $\nabla L_B$ è€Œéå…¨æ¢¯åº¦ $\nabla L$ã€‚

**æ¢¯åº¦å™ªå£°æ¨¡å‹**ï¼š
\begin{equation}
\nabla L_B = \nabla L + \eta, \quad \mathbb{E}[\eta] = 0, \quad \text{Cov}(\eta) = \Sigma/B \tag{3.8}
\end{equation}

**ä¿®æ­£æŸå¤±ï¼ˆå«å™ªå£°ï¼‰**ï¼š
\begin{equation}
\tilde{L}_{\text{stoch}} = L + \frac{\gamma}{4} \|\nabla L\|^2 + \frac{\gamma}{2B} \text{Tr}(\Sigma) \tag{3.9}
\end{equation}

**è¯æ˜æ€è·¯**ï¼š
1. éšæœºæ›´æ–° $\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - \gamma (\nabla L + \eta)$
2. å–æœŸæœ›ï¼š$\mathbb{E}[\Delta \boldsymbol{\theta}] = -\gamma \nabla L$
3. æ–¹å·®é¡¹ï¼š$\text{Var}(\Delta \boldsymbol{\theta}) = \gamma^2 \Sigma / B$
4. Fokker-Planckæ–¹ç¨‹ï¼šæ¼‚ç§»é¡¹ $-\nabla L$ï¼Œæ‰©æ•£é¡¹ $\gamma^2 \Sigma / (2B)$
5. æœ‰æ•ˆåŠ¿èƒ½ï¼ˆEffective potentialï¼‰ï¼š$\tilde{L} = L + \frac{\gamma}{2B} \text{Tr}(\Sigma)$

**å…³é”®æ´å¯Ÿ**ï¼š
- **å°æ‰¹é‡ = æ³¨å…¥å™ªå£° = éšå¼æ­£åˆ™åŒ–**
- **å­¦ä¹ ç‡ä¸æ‰¹é‡å¤§å°çš„trade-off**ï¼š$\gamma \uparrow$ æˆ– $B \downarrow$ å‡å¢å¼ºæ­£åˆ™åŒ–
- **çº¿æ€§ç¼©æ”¾è§„åˆ™**ï¼ˆLinear scaling ruleï¼‰ï¼šä¿æŒ $\gamma/B$ æ’å®šä»¥ç»´æŒç­‰æ•ˆæ­£åˆ™åŒ–å¼ºåº¦

</div>

<div class="example-box">

### æ•°å€¼ç¤ºä¾‹ï¼šå™ªå£°å¼ºåº¦çš„å½±å“

**è®¾å®š**ï¼šäºŒæ¬¡æŸå¤± $L(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T A \mathbf{x}$ï¼Œå…¨æ¢¯åº¦ $\nabla L = A\mathbf{x}$ã€‚

**æ¢¯åº¦å™ªå£°åæ–¹å·®**ï¼š$\Sigma = \sigma^2 I$ï¼ˆå„å‘åŒæ€§å™ªå£°ï¼‰ã€‚

**ä¿®æ­£æŸå¤±**ï¼š
\begin{equation}
\tilde{L} = \frac{1}{2} \mathbf{x}^T A \mathbf{x} + \frac{\gamma \sigma^2 d}{2B} \tag{3.10}
\end{equation}

**å®é™…æ•ˆåº”**ï¼š
- å½“ $\gamma = 0.1, \sigma^2 = 1, d = 1000, B = 32$ï¼š
  - å™ªå£°æ­£åˆ™åŒ–é¡¹ = $\frac{0.1 \times 1 \times 1000}{2 \times 32} = 1.56$
  - è¿™ç›¸å½“äºåœ¨æŸå¤±ä¸Šé¢å¤–æ·»åŠ äº†ä¸€ä¸ªå¸¸æ•°åç§»ï¼Œä¿ƒä½¿ä¼˜åŒ–å™¨å¯»æ‰¾æ›´å¹³å¦çš„åŒºåŸŸ

**å®éªŒéªŒè¯**ï¼ˆMNISTåˆ†ç±»ï¼‰ï¼š
- å¤§æ‰¹é‡ï¼ˆ$B=512$ï¼‰+ å°å­¦ä¹ ç‡ï¼ˆ$\gamma=0.01$ï¼‰ï¼šæµ‹è¯•å‡†ç¡®ç‡ 97.2%
- å°æ‰¹é‡ï¼ˆ$B=32$ï¼‰+ å¤§å­¦ä¹ ç‡ï¼ˆ$\gamma=0.1$ï¼‰ï¼šæµ‹è¯•å‡†ç¡®ç‡ **98.1%**ï¼ˆæå‡0.9%ï¼‰
- åŸå› ï¼šå™ªå£°æ­£åˆ™åŒ– $\propto \gamma/B$ åœ¨ç¬¬äºŒç§è®¾ç½®ä¸‹å¤§16å€

</div>

### 3.7 æ§åˆ¶è®ºè§†è§’ï¼šåé¦ˆä¸ç¨³å®šæ€§

<div class="control-theory-box">

### âš™ï¸ æ§åˆ¶ç³»ç»Ÿç±»æ¯”

å°†ä¼˜åŒ–è§†ä¸º**è´Ÿåé¦ˆæ§åˆ¶ç³»ç»Ÿ**ï¼š

**ç³»ç»Ÿæ–¹ç¨‹**ï¼š
\begin{equation}
\dot{\boldsymbol{\theta}} = -\gamma \nabla L(\boldsymbol{\theta}) \tag{3.11}
\end{equation}

**ä¼ é€’å‡½æ•°åˆ†æ**ï¼ˆTransfer functionï¼‰ï¼š

åœ¨æå°å€¼ç‚¹ $\boldsymbol{\theta}^*$ é™„è¿‘çº¿æ€§åŒ–ï¼š$\nabla L \approx H (\boldsymbol{\theta} - \boldsymbol{\theta}^*)$

**é—­ç¯ç³»ç»Ÿ**ï¼š
\begin{equation}
\dot{\mathbf{e}} = -\gamma H \mathbf{e} \tag{3.12}
\end{equation}

å…¶ä¸­ $\mathbf{e} = \boldsymbol{\theta} - \boldsymbol{\theta}^*$ æ˜¯è¯¯å·®ã€‚

**ç‰¹å¾æ–¹ç¨‹**ï¼šè®¾ $\mathbf{e}(t) = \mathbf{v} e^{\lambda t}$ï¼Œåˆ™ï¼š
\begin{equation}
\lambda = -\gamma \lambda_i(H) \tag{3.13}
\end{equation}

**ç¨³å®šæ€§åˆ¤æ®**ï¼ˆStability criterionï¼‰ï¼š
- è‹¥ $\gamma > 0$ ä¸” $\lambda_i(H) > 0$ï¼ˆæ­£å®šHessianï¼‰ï¼Œåˆ™ $\lambda < 0$ï¼Œç³»ç»Ÿ**æŒ‡æ•°è¡°å‡**
- æ”¶æ•›é€Ÿåº¦ï¼š$|\lambda| = \gamma \lambda_i$ï¼Œæ²¿ $i$ æ–¹å‘çš„æ—¶é—´å¸¸æ•° $\tau_i = 1/(\gamma \lambda_i)$

**è¿‡é˜»å°¼ vs æ¬ é˜»å°¼**ï¼š
- **è¿‡é˜»å°¼**ï¼ˆ$\gamma$ å¾ˆå°ï¼‰ï¼šæ”¶æ•›ç¼“æ…¢ï¼Œ$\tau_i$ å¾ˆå¤§
- **ä¸´ç•Œé˜»å°¼**ï¼ˆæœ€ä¼˜ $\gamma$ï¼‰ï¼šæœ€å¿«æ”¶æ•›ï¼Œæ— éœ‡è¡
- **æ¬ é˜»å°¼**ï¼ˆ$\gamma$ è¿‡å¤§ï¼‰ï¼šéœ‡è¡ï¼Œç”šè‡³å‘æ•£

**Bodeå›¾åˆ†æ**ï¼šå­¦ä¹ ç‡ $\gamma$ = æ¯”ä¾‹å¢ç›Šï¼ˆProportional gainï¼‰ï¼Œå†³å®šé—­ç¯å¸¦å®½ã€‚

</div>

---

## 4. æ–¹æ³•è®ºå˜ä½“ã€æ‰¹åˆ¤æ€§æ¯”è¾ƒä¸ä¼˜åŒ–

### 4.1 å…¨é‡å¯¹æ¯”è¡¨ï¼šæ­£åˆ™åŒ–å¼ºåº¦çš„æ¥æº

| æ­£åˆ™åŒ–ç±»å‹ | é©±åŠ¨å› ç´  | æ•°å­¦å½¢å¼ | ä¼˜ç‚¹ | **ç¼ºç‚¹** |
| :--- | :--- | :--- | :--- | :--- |
| **Weight Decay** | å‚æ•°æ¨¡é•¿ | $\|\boldsymbol{\theta}\|^2$ | ç®€å•ï¼Œé˜²æ­¢å‘æ•£ | âŒ ä¸åŒºåˆ†å¹³å¦åº¦ |
| **IGR (æœ¬æ–‡)** | **å­¦ä¹ ç‡ $\gamma$** | **$\frac{\gamma}{4}\|\nabla L\|^2$** | **è‡ªåŠ¨å¯»æ‰¾å¹³å¦åŒº** | âŒ **ä¾èµ–ç¨³å®šæ€§ä¸Šé™** |
| **Batch Noise** | æ ·æœ¬éšæœºæ€§ | $\frac{\gamma}{2B} \text{Var}(g)$ | å¢å¼ºå¤šæ ·æ€§ | âŒ éš¾ä»¥ç†è®ºè§£æ |
| **SAM (æ˜¾å¼)** | é‚»åŸŸæœ€å¤§å€¼ | $\max_{\|\epsilon\|<\rho} L(\theta+\epsilon)$ | æå¼ºçš„æ³›åŒ–èƒ½åŠ› | âŒ è®¡ç®—æˆæœ¬ç¿»å€ |

### 4.2 æ·±åº¦æ‰¹åˆ¤ï¼šæœ‰é™å­¦ä¹ ç‡çš„â€œæ­»ç©´â€

è™½ç„¶å¤§å­¦ä¹ ç‡æœ‰ç›Šæ³›åŒ–ï¼Œä½†å®ƒå¸¦æ¥è‡ªå¸¦ä¸¤ä¸ªè‡´å‘½é™åˆ¶ï¼š

#### **æ ¸å¿ƒç¼ºé™· 1ï¼šçº¿æ€§ç¨³å®šæ€§è¾¹ç•Œ (The Edge of Stability)**
- **é—®é¢˜**ï¼šæ ¹æ® (13) å¼ï¼Œå¦‚æœè¦ä¿æŒæ”¶æ•›ï¼Œå¿…é¡»æ»¡è¶³ $\gamma < 2 / \lambda_{\max}(H)$ã€‚
- **å¤±æ•ˆåœºæ™¯**ï¼šå¦‚æœä½ ä¸ºäº†è¿½æ±‚æ³›åŒ–æ‹¼å‘½å¢å¤§å­¦ä¹ ç‡ï¼Œä¸€æ—¦è§¦ç¢°è¯¥é˜ˆå€¼ï¼Œä¿®æ­£æŸå¤± $\tilde{L}$ çš„å‡¸æ€§ä¼šç¬é—´æ¶ˆå¤±ï¼Œæ¨¡å‹ç›´æ¥ `NaN`ã€‚

#### **æ ¸å¿ƒç¼ºé™· 2ï¼šéå‡¸åŒºåŸŸçš„è¯¯å¯¼**
- **åˆ†æ**ï¼šåœ¨é©¬éç‚¹ï¼ˆSaddle Pointï¼‰é™„è¿‘ï¼Œæ¢¯åº¦ $\|\nabla L \|$ å¯èƒ½å¾ˆå°ä½†æ›²ç‡å¾ˆå¤§ã€‚
- **é£é™©**ï¼šå•çº¯çš„ IGR å¯èƒ½æ— æ³•æä¾›è¶³å¤Ÿçš„åŠ¨åŠ›é€ƒç¦»è¿™ç§ç‰¹æ®Šçš„éå‡¸æ™¯è§‚ï¼Œå› ä¸ºå®ƒåªæƒ©ç½šæ¢¯åº¦å¤§å°ï¼Œä¸ç›´æ¥æƒ©ç½šæ›²ç‡çš„æ­£è´Ÿã€‚

#### **æ ¸å¿ƒç¼ºé™· 3ï¼šä¸ Batch Size çš„æ‹®æŠ—æ•ˆåº”**
- **åˆ†æ**ï¼šå½“ $B$ å¾ˆå¤§æ—¶ï¼Œæ¢¯åº¦å˜å¾—éå¸¸å¹³æ»‘ï¼ŒIGR çš„ä½œç”¨è¢«å‰Šå¼±ã€‚è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆå¤§ Batch è®­ç»ƒå¾€å¾€éœ€è¦æ›´é•¿çš„æ—¶é—´æ‰èƒ½è¾¾åˆ°åŒæ ·çš„æ³›åŒ–æ•ˆæœã€‚

### 4.3 ç®—æ³•æ¼”è¿›ï¼šæ˜¾å¼æ¢¯åº¦æƒ©ç½š (EGR)

æ—¢ç„¶æˆ‘ä»¬çŸ¥é“äº† $\frac{\gamma}{4}\|\nabla L\|^2$ æœ‰å¥½å¤„ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥æŠŠå®ƒåŠ è¿› Loss å‘¢ï¼Ÿ
è¿™å°±æ˜¯ Google åœ¨è®ºæ–‡ä¸­å»ºè®®çš„æ–¹æ¡ˆï¼š**â€œå½“ä¸å¾—ä¸ä½¿ç”¨æå°å­¦ä¹ ç‡æ—¶ï¼ˆå¦‚ä¸ºäº†ç¨³å®šæ€§ï¼‰ï¼Œæ˜¾å¼æ·»åŠ æ¢¯åº¦æƒ©ç½šé¡¹ï¼Œäººä¸ºæ‰¾å›æµå¤±çš„æ³›åŒ–çº¢åˆ©ã€‚â€**

---

## 5. å·¥ç¨‹å®è·µã€è·¯çº¿å›¾ä¸æœªæ¥å±•æœ›

### 5.1 å®Œæ•´æ•°å€¼å®éªŒï¼šä»ç†è®ºåˆ°ä»£ç 

#### 5.1.1 å®éªŒ1ï¼šäºŒç»´äºŒæ¬¡å‡½æ•°çš„å¯è§†åŒ–

<div class="code-box">

**ç›®æ ‡**ï¼šç›´è§‚éªŒè¯ä¸åŒå­¦ä¹ ç‡ä¸‹çš„ä¼˜åŒ–è½¨è¿¹ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# å®šä¹‰äºŒæ¬¡æŸå¤±å‡½æ•°ï¼ˆå¸¦æ¡ä»¶æ•°ï¼‰
def quadratic_loss(x, y, kappa=100):
    """äºŒæ¬¡æŸå¤± L = 0.5 * (kappa * x^2 + y^2)"""
    return 0.5 * (kappa * x**2 + y**2)

def grad_quadratic(x, y, kappa=100):
    """æ¢¯åº¦"""
    return np.array([kappa * x, y])

def hessian_quadratic(kappa=100):
    """HessiançŸ©é˜µ"""
    return np.array([[kappa, 0], [0, 1]])

# ä¿®æ­£æŸå¤±å‡½æ•°ï¼ˆåŒ…å«IGRé¡¹ï¼‰
def modified_loss(x, y, gamma, kappa=100):
    """ä¿®æ­£æŸå¤± L_tilde = L + (gamma/4) * ||grad L||^2"""
    L = quadratic_loss(x, y, kappa)
    grad = grad_quadratic(x, y, kappa)
    grad_norm_sq = np.sum(grad**2)
    return L + (gamma / 4) * grad_norm_sq

# æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨
def gradient_descent(x0, y0, gamma, kappa=100, n_steps=100):
    """æ ‡å‡†æ¢¯åº¦ä¸‹é™"""
    trajectory = [(x0, y0)]
    x, y = x0, y0

    for _ in range(n_steps):
        grad = grad_quadratic(x, y, kappa)
        x -= gamma * grad[0]
        y -= gamma * grad[1]
        trajectory.append((x, y))

    return np.array(trajectory)

# å¯è§†åŒ–å‡½æ•°
def visualize_optimization(kappa=100, gammas=[0.001, 0.01, 0.019],
                          x0=1.0, y0=1.0, n_steps=100):
    """ç»˜åˆ¶ä¸åŒå­¦ä¹ ç‡ä¸‹çš„ä¼˜åŒ–è½¨è¿¹"""
    fig, axes = plt.subplots(1, len(gammas), figsize=(15, 5))

    # åˆ›å»ºç½‘æ ¼ç”¨äºç»˜åˆ¶ç­‰é«˜çº¿
    x_range = np.linspace(-1.5, 1.5, 200)
    y_range = np.linspace(-1.5, 1.5, 200)
    X, Y = np.meshgrid(x_range, y_range)

    for idx, (ax, gamma) in enumerate(zip(axes, gammas)):
        # è®¡ç®—æŸå¤±æ™¯è§‚
        Z = quadratic_loss(X, Y, kappa)

        # ç»˜åˆ¶ç­‰é«˜çº¿
        contour = ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)
        ax.clabel(contour, inline=True, fontsize=8)

        # è®¡ç®—ä¼˜åŒ–è½¨è¿¹
        trajectory = gradient_descent(x0, y0, gamma, kappa, n_steps)

        # ç»˜åˆ¶è½¨è¿¹
        ax.plot(trajectory[:, 0], trajectory[:, 1], 'r-o',
                markersize=3, linewidth=1.5, label=f'Trajectory')
        ax.plot(x0, y0, 'go', markersize=10, label='Start')
        ax.plot(0, 0, 'r*', markersize=15, label='Optimum')

        # è®¡ç®—æœ€ç»ˆæŸå¤±
        final_loss = quadratic_loss(trajectory[-1, 0], trajectory[-1, 1], kappa)

        # æ£€æŸ¥ç¨³å®šæ€§
        lambda_max = kappa
        stable = gamma < 2.0 / lambda_max
        stability_text = "Stable" if stable else "Unstable"

        ax.set_title(f'$\\gamma = {gamma}$ ({stability_text})\n'
                    f'Final Loss: {final_loss:.2e}', fontsize=12)
        ax.set_xlabel('$\\theta_1$')
        ax.set_ylabel('$\\theta_2$')
        ax.legend(fontsize=8)
        ax.grid(True, alpha=0.3)
        ax.axis('equal')

    plt.tight_layout()
    plt.savefig('learning_rate_comparison.png', dpi=150)
    print(f"âœ“ å›¾åƒå·²ä¿å­˜è‡³ learning_rate_comparison.png")

    return fig

# è¿è¡Œå®éªŒ
kappa = 100  # æ¡ä»¶æ•°
gammas = [0.001, 0.01, 0.019]  # ä¸´ç•Œå€¼ = 2/100 = 0.02
visualize_optimization(kappa=kappa, gammas=gammas)

# æ‰“å°ç†è®ºé¢„æµ‹
print("\nç†è®ºåˆ†æï¼š")
print(f"Hessianç‰¹å¾å€¼: Î»â‚ = {kappa}, Î»â‚‚ = 1")
print(f"ç¨³å®šæ€§ä¸´ç•Œå€¼: Î³_crit = 2/Î»_max = {2.0/kappa}")
print("\nä¿®æ­£Hessianç‰¹å¾å€¼ï¼š")
for gamma in gammas:
    lambda1_tilde = kappa * (1 + gamma * kappa / 2)
    lambda2_tilde = 1 * (1 + gamma * 1 / 2)
    print(f"Î³ = {gamma}: Î»Ìƒâ‚ = {lambda1_tilde:.2f}, Î»Ìƒâ‚‚ = {lambda2_tilde:.3f}")
```

**è¾“å‡ºè§£é‡Š**ï¼š
- **å°å­¦ä¹ ç‡ï¼ˆ$\gamma = 0.001$ï¼‰**ï¼šæ”¶æ•›ææ…¢ï¼Œè½¨è¿¹æ²¿æ¤­åœ†é•¿è½´ç¼“æ…¢çˆ¬è¡Œ
- **é€‚ä¸­å­¦ä¹ ç‡ï¼ˆ$\gamma = 0.01$ï¼‰**ï¼šå¿«é€Ÿæ”¶æ•›ï¼Œè½¨è¿¹å¹³æ»‘
- **ä¸´ç•Œå­¦ä¹ ç‡ï¼ˆ$\gamma = 0.019$ï¼‰**ï¼šæ¥è¿‘ç¨³å®šæ€§è¾¹ç•Œï¼Œå¯èƒ½å‡ºç°è½»å¾®éœ‡è¡

</div>

#### 5.1.2 å®éªŒ2ï¼šä¿®æ­£Hessiançš„ç‰¹å¾å€¼å¯è§†åŒ–

<div class="code-box">

```python
import numpy as np
import matplotlib.pyplot as plt

def plot_modified_eigenvalues(lambda_original, gamma_range):
    """ç»˜åˆ¶ä¿®æ­£Hessianç‰¹å¾å€¼éšå­¦ä¹ ç‡å˜åŒ–"""
    lambda_modified = lambda_original * (1 + gamma_range * lambda_original / 2)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    # å­å›¾1ï¼šç»å¯¹å€¼
    ax1.plot(gamma_range, lambda_modified, linewidth=2, label=f'$\\lambda$ = {lambda_original}')
    ax1.axhline(y=lambda_original, color='gray', linestyle='--',
                label=f'Original $\\lambda$')
    ax1.set_xlabel('Learning Rate $\\gamma$', fontsize=12)
    ax1.set_ylabel('Modified Eigenvalue $\\tilde{\\lambda}$', fontsize=12)
    ax1.set_title('Absolute Modified Eigenvalue', fontsize=14)
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)

    # å­å›¾2ï¼šæ”¾å¤§å€æ•°
    magnification = lambda_modified / lambda_original
    ax2.plot(gamma_range, magnification, linewidth=2, color='orange')
    ax2.axhline(y=1, color='gray', linestyle='--', label='No change')
    ax2.set_xlabel('Learning Rate $\\gamma$', fontsize=12)
    ax2.set_ylabel('Magnification Factor $\\tilde{\\lambda}/\\lambda$', fontsize=12)
    ax2.set_title('Curvature Amplification', fontsize=14)
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('modified_eigenvalues.png', dpi=150)
    print(f"âœ“ å›¾åƒå·²ä¿å­˜è‡³ modified_eigenvalues.png")

    return fig

# è¿è¡Œå®éªŒï¼šå¯¹æ¯”ä¸åŒåŸå§‹ç‰¹å¾å€¼
gamma_range = np.linspace(0, 0.02, 100)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
eigenvalues = [1, 10, 100, 1000]  # ä¸åŒæ›²ç‡

for idx, (ax, lambda_orig) in enumerate(zip(axes.flat, eigenvalues)):
    lambda_mod = lambda_orig * (1 + gamma_range * lambda_orig / 2)
    magnification = lambda_mod / lambda_orig

    # ç»˜åˆ¶æ”¾å¤§å€æ•°
    ax.plot(gamma_range, magnification, linewidth=2.5, color=f'C{idx}')
    ax.axhline(y=1, color='gray', linestyle='--', linewidth=1, alpha=0.5)

    # æ ‡æ³¨ç¨³å®šæ€§è¾¹ç•Œ
    gamma_crit = 2.0 / lambda_orig
    if gamma_crit <= 0.02:
        ax.axvline(x=gamma_crit, color='red', linestyle=':', linewidth=2,
                  label=f'$\\gamma_{{crit}}$ = {gamma_crit:.4f}')

    ax.set_xlabel('Learning Rate $\\gamma$', fontsize=11)
    ax.set_ylabel('Amplification $\\tilde{\\lambda}/\\lambda$', fontsize=11)
    ax.set_title(f'$\\lambda = {lambda_orig}$ ({"Steep" if lambda_orig > 10 else "Flat"})',
                fontsize=12, fontweight='bold')
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3)
    ax.set_xlim([0, 0.02])

plt.tight_layout()
plt.savefig('eigenvalue_amplification.png', dpi=150)
print(f"âœ“ å›¾åƒå·²ä¿å­˜è‡³ eigenvalue_amplification.png")

print("\nå…³é”®è§‚å¯Ÿï¼š")
print("1. å¤§æ›²ç‡æ–¹å‘ï¼ˆÎ»å¤§ï¼‰çš„æ”¾å¤§å€æ•°å¢é•¿æ›´å¿«")
print("2. å°æ›²ç‡æ–¹å‘ï¼ˆÎ»å°ï¼‰çš„ä¿®æ­£å‡ ä¹å¯å¿½ç•¥")
print("3. è¿™å¯¼è‡´ä¼˜åŒ–å™¨è‡ªåŠ¨'åŒæ¶'é«˜æ›²ç‡åŒºåŸŸ")
```

</div>

#### 5.1.3 å®éªŒ3ï¼šMNISTåˆ†ç±»çš„IGRæ•ˆåº”

<div class="code-box">

**ç›®æ ‡**ï¼šåœ¨çœŸå®ç¥ç»ç½‘ç»œä¸ŠéªŒè¯å­¦ä¹ ç‡å¯¹æ³›åŒ–çš„å½±å“ã€‚

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt

# å®šä¹‰ç®€å•çš„MLP
class SimpleMLP(nn.Module):
    def __init__(self, hidden_dim=128):
        super(SimpleMLP, self).__init__()
        self.fc1 = nn.Linear(784, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(-1, 784)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºç›‘æ§IGRæ•ˆåº”ï¼‰
def compute_grad_norm(model):
    """è®¡ç®—æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦L2èŒƒæ•°"""
    total_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    return total_norm

# è®­ç»ƒå‡½æ•°
def train_with_monitoring(model, train_loader, test_loader,
                         lr, epochs=10, device='cpu'):
    """è®­ç»ƒæ¨¡å‹å¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # è®°å½•ç»Ÿè®¡
    stats = {
        'train_loss': [],
        'test_loss': [],
        'train_acc': [],
        'test_acc': [],
        'grad_norm': []
    }

    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        grad_norms = []

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()

            # è®°å½•æ¢¯åº¦èŒƒæ•°
            grad_norm = compute_grad_norm(model)
            grad_norms.append(grad_norm)

            optimizer.step()

            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            train_correct += pred.eq(target.view_as(pred)).sum().item()

        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_loss = 0
        test_correct = 0

        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                test_loss += criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                test_correct += pred.eq(target.view_as(pred)).sum().item()

        # è®°å½•ç»Ÿè®¡
        stats['train_loss'].append(train_loss / len(train_loader))
        stats['test_loss'].append(test_loss / len(test_loader))
        stats['train_acc'].append(100. * train_correct / len(train_loader.dataset))
        stats['test_acc'].append(100. * test_correct / len(test_loader.dataset))
        stats['grad_norm'].append(np.mean(grad_norms))

        print(f'Epoch {epoch+1}/{epochs}: '
              f'Train Loss: {stats["train_loss"][-1]:.4f}, '
              f'Test Acc: {stats["test_acc"][-1]:.2f}%, '
              f'Grad Norm: {stats["grad_norm"][-1]:.4f}')

    return stats

# ä¸»å®éªŒ
def run_mnist_experiment():
    """è¿è¡Œå®Œæ•´çš„MNISTå®éªŒ"""
    # æ•°æ®åŠ è½½
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_dataset = datasets.MNIST('./data', train=True, download=True,
                                  transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)

    batch_size = 64
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)

    # æµ‹è¯•ä¸åŒå­¦ä¹ ç‡
    learning_rates = [0.001, 0.01, 0.1]
    all_stats = {}

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}\n")

    for lr in learning_rates:
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒæ¨¡å‹ï¼šå­¦ä¹ ç‡ = {lr}")
        print(f"{'='*60}")

        model = SimpleMLP(hidden_dim=128).to(device)
        stats = train_with_monitoring(model, train_loader, test_loader,
                                     lr=lr, epochs=10, device=device)
        all_stats[lr] = stats

    # å¯è§†åŒ–ç»“æœ
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # å­å›¾1ï¼šè®­ç»ƒæŸå¤±
    ax1 = axes[0, 0]
    for lr in learning_rates:
        ax1.plot(all_stats[lr]['train_loss'], label=f'LR={lr}', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=11)
    ax1.set_ylabel('Train Loss', fontsize=11)
    ax1.set_title('Training Loss Curves', fontsize=12, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)

    # å­å›¾2ï¼šæµ‹è¯•å‡†ç¡®ç‡
    ax2 = axes[0, 1]
    for lr in learning_rates:
        ax2.plot(all_stats[lr]['test_acc'], label=f'LR={lr}', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=11)
    ax2.set_ylabel('Test Accuracy (%)', fontsize=11)
    ax2.set_title('Test Accuracy Curves', fontsize=12, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)

    # å­å›¾3ï¼šæ¢¯åº¦èŒƒæ•°
    ax3 = axes[1, 0]
    for lr in learning_rates:
        ax3.plot(all_stats[lr]['grad_norm'], label=f'LR={lr}', linewidth=2)
    ax3.set_xlabel('Epoch', fontsize=11)
    ax3.set_ylabel('Gradient Norm', fontsize=11)
    ax3.set_title('Gradient Norm Evolution', fontsize=12, fontweight='bold')
    ax3.legend(fontsize=10)
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale('log')

    # å­å›¾4ï¼šæ³›åŒ–å·®è·
    ax4 = axes[1, 1]
    for lr in learning_rates:
        generalization_gap = np.array(all_stats[lr]['train_acc']) - \
                           np.array(all_stats[lr]['test_acc'])
        ax4.plot(generalization_gap, label=f'LR={lr}', linewidth=2)
    ax4.set_xlabel('Epoch', fontsize=11)
    ax4.set_ylabel('Generalization Gap (%)', fontsize=11)
    ax4.set_title('Train-Test Accuracy Gap', fontsize=12, fontweight='bold')
    ax4.legend(fontsize=10)
    ax4.grid(True, alpha=0.3)
    ax4.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)

    plt.tight_layout()
    plt.savefig('mnist_igr_experiment.png', dpi=150)
    print(f"\nâœ“ å®éªŒç»“æœå·²ä¿å­˜è‡³ mnist_igr_experiment.png")

    # æ‰“å°æœ€ç»ˆç»“æœ
    print(f"\n{'='*60}")
    print("æœ€ç»ˆç»“æœæ€»ç»“ï¼š")
    print(f"{'='*60}")
    for lr in learning_rates:
        final_test_acc = all_stats[lr]['test_acc'][-1]
        final_train_acc = all_stats[lr]['train_acc'][-1]
        gap = final_train_acc - final_test_acc
        print(f"å­¦ä¹ ç‡ {lr:5.3f}: "
              f"æµ‹è¯•å‡†ç¡®ç‡ {final_test_acc:.2f}%, "
              f"æ³›åŒ–å·®è· {gap:.2f}%")

    return all_stats

# è¿è¡Œå®éªŒ
all_stats = run_mnist_experiment()
```

**é¢„æœŸè§‚å¯Ÿ**ï¼š
1. **å°å­¦ä¹ ç‡ï¼ˆ0.001ï¼‰**ï¼šè®­ç»ƒæ…¢ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆï¼Œæ³›åŒ–å·®è·å¤§
2. **é€‚ä¸­å­¦ä¹ ç‡ï¼ˆ0.01ï¼‰**ï¼šå¹³è¡¡æ”¶æ•›é€Ÿåº¦ä¸æ³›åŒ–æ€§èƒ½
3. **å¤§å­¦ä¹ ç‡ï¼ˆ0.1ï¼‰**ï¼šæ”¶æ•›å¿«ï¼Œæ¢¯åº¦èŒƒæ•°è¢«æœ‰æ•ˆå‹åˆ¶ï¼Œæ³›åŒ–æœ€å¥½

</div>

### 5.2 æ˜¾å¼æ¢¯åº¦æ­£åˆ™åŒ–ï¼ˆEGRï¼‰å®ç°

<div class="code-box">

**å½“å¿…é¡»ä½¿ç”¨å°å­¦ä¹ ç‡æ—¶çš„è¡¥æ•‘æªæ–½**ï¼š

```python
import torch
import torch.nn as nn

class IGRLoss(nn.Module):
    """æ˜¾å¼éšå¼æ¢¯åº¦æ­£åˆ™åŒ–æŸå¤±"""
    def __init__(self, base_criterion, alpha=0.25, lr=0.01):
        """
        å‚æ•°:
            base_criterion: åŸºç¡€æŸå¤±å‡½æ•°ï¼ˆå¦‚CrossEntropyLossï¼‰
            alpha: IGRå¼ºåº¦ç³»æ•°ï¼ˆå»ºè®®å€¼ï¼š0.25ï¼‰
            lr: å½“å‰å­¦ä¹ ç‡ï¼ˆç”¨äºè°ƒæ•´æƒ©ç½šå¼ºåº¦ï¼‰
        """
        super(IGRLoss, self).__init__()
        self.base_criterion = base_criterion
        self.alpha = alpha
        self.lr = lr

    def forward(self, model, inputs, targets):
        """
        è®¡ç®—åŒ…å«IGRé¡¹çš„æ€»æŸå¤±

        è¿”å›:
            total_loss: L + (alpha * lr / 4) * ||grad L||^2
        """
        # è®¡ç®—åŸºç¡€æŸå¤±
        outputs = model(inputs)
        base_loss = self.base_criterion(outputs, targets)

        # è®¡ç®—æ¢¯åº¦ï¼ˆéœ€è¦create_graph=Trueä»¥æ”¯æŒäºŒé˜¶å¯¼æ•°ï¼‰
        grads = torch.autograd.grad(base_loss, model.parameters(),
                                   create_graph=True, retain_graph=True)

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°å¹³æ–¹
        grad_norm_sq = sum(torch.sum(g**2) for g in grads)

        # ä¿®æ­£æŸå¤±
        igr_term = (self.alpha * self.lr / 4.0) * grad_norm_sq
        total_loss = base_loss + igr_term

        return total_loss, base_loss.item(), igr_term.item()

# ä½¿ç”¨ç¤ºä¾‹
def train_with_egr():
    """ä½¿ç”¨æ˜¾å¼æ¢¯åº¦æ­£åˆ™åŒ–è®­ç»ƒ"""
    model = SimpleMLP()
    criterion = nn.CrossEntropyLoss()
    lr = 0.001  # è¢«è¿«ä½¿ç”¨çš„å°å­¦ä¹ ç‡

    # åˆ›å»ºEGRæŸå¤±
    egr_loss = IGRLoss(criterion, alpha=0.25, lr=lr)
    optimizer = optim.SGD(model.parameters(), lr=lr)

    for epoch in range(epochs):
        for inputs, targets in train_loader:
            optimizer.zero_grad()

            # è®¡ç®—åŒ…å«IGRçš„æŸå¤±
            total_loss, base_loss, igr_term = egr_loss(model, inputs, targets)

            # åå‘ä¼ æ’­
            total_loss.backward()
            optimizer.step()

            if batch_idx % 100 == 0:
                print(f'Base Loss: {base_loss:.4f}, '
                      f'IGR Term: {igr_term:.4f}, '
                      f'Total: {total_loss.item():.4f}')

print("æ³¨æ„ï¼šEGRçš„è®¡ç®—æˆæœ¬æ˜¯æ ‡å‡†è®­ç»ƒçš„2-3å€ï¼ˆéœ€è¦äºŒé˜¶å¯¼æ•°ï¼‰")
print("å»ºè®®ä»…åœ¨ä»¥ä¸‹æƒ…å†µä½¿ç”¨ï¼š")
print("  1. ç¨³å®šæ€§è¦æ±‚å¼ºåˆ¶ä½¿ç”¨æå°å­¦ä¹ ç‡")
print("  2. å°è§„æ¨¡æ¨¡å‹æˆ–å°æ‰¹é‡è®­ç»ƒ")
print("  3. å¯¹æ³›åŒ–æ€§èƒ½æœ‰æé«˜è¦æ±‚çš„åœºæ™¯")
```

</div>

### 5.3 æœªæ¥ç ”ç©¶æ–¹å‘ä¸å¼€æ”¾é—®é¢˜

#### æ–¹å‘ 1ï¼šå¤§æ¨¡å‹ï¼ˆTransformerï¼‰ä¸­çš„ IGR åˆ†å¸ƒ

<div class="research-direction">

**ç ”ç©¶é—®é¢˜**ï¼šåœ¨Transformeræ¶æ„ä¸­ï¼ŒIGRæ•ˆåº”åœ¨ä¸åŒå±‚ä¹‹é—´å¦‚ä½•åˆ†å¸ƒï¼Ÿ

**å·²çŸ¥ç°è±¡**ï¼š
- Attentionå±‚æ¢¯åº¦æå…¶ç¨€ç–ï¼ˆå¤§éƒ¨åˆ†å‚æ•°æ¢¯åº¦æ¥è¿‘é›¶ï¼‰
- MLPå±‚æ¢¯åº¦å¯†é›†ä¸”å¹…åº¦è¾ƒå¤§
- LayerNorm/BatchNormå¼•å…¥é¢å¤–çš„æ¢¯åº¦ç¼©æ”¾

**é‡åŒ–ç›®æ ‡**ï¼š
1. é€å±‚åˆ†è§£ $\|\nabla L\|^2$ çš„è´¡çŒ®åº¦
2. ç»˜åˆ¶ IGR å¼ºåº¦çƒ­åŠ›å›¾ï¼ˆlayer Ã— headï¼‰
3. åˆ†æä½ç½®ç¼–ç å±‚çš„ç‰¹æ®Šæ€§

**é¢„æœŸå‘ç°**ï¼š
- IGR ä¸»è¦ä½œç”¨äºæ·±å±‚ MLPï¼ˆæ¢¯åº¦å¤§ï¼‰
- æµ…å±‚ Attention å‡ ä¹ä¸å— IGR å½±å“
- è¿™å¯èƒ½è§£é‡Šä¸ºä»€ä¹ˆ Attention éœ€è¦ç‰¹æ®Šçš„å­¦ä¹ ç‡è°ƒåº¦ï¼ˆlayer-wise LRï¼‰

**å®éªŒè®¾è®¡**ï¼š
```python
def analyze_layer_wise_igr(model, data):
    """é€å±‚åˆ†æIGRè´¡çŒ®"""
    loss = compute_loss(model, data)
    layer_igr = {}

    for name, param in model.named_parameters():
        grad = torch.autograd.grad(loss, param, retain_graph=True)[0]
        layer_igr[name] = (grad**2).sum().item()

    return layer_igr
```

</div>

#### æ–¹å‘ 2ï¼šäºŒé˜¶ä¼˜åŒ–å™¨æ˜¯å¦"æ€æ­»"äº†IGRï¼Ÿ

<div class="research-direction">

**æ ¸å¿ƒå‡è®¾**ï¼šäºŒé˜¶ä¼˜åŒ–å™¨ï¼ˆShampoo, K-FAC, AdaHessianï¼‰é€šè¿‡é¢„æ¡ä»¶åŒ–æŠ¹å¹³äº†æ›²ç‡å·®å¼‚ï¼Œä»è€Œå‰Šå¼±äº† IGR çš„å¹³å¦æ€§å¯»ä¼˜èƒ½åŠ›ã€‚

**ç†è®ºåˆ†æ**ï¼š

**æ ‡å‡†SGDçš„ä¿®æ­£æŸå¤±**ï¼š
\begin{equation}
\tilde{L}_{\text{SGD}} = L + \frac{\gamma}{4} \|\nabla L\|^2 \tag{5.1}
\end{equation}

**é¢„æ¡ä»¶SGD**ï¼ˆå¦‚ Adamï¼‰ï¼š
\begin{equation}
\boldsymbol{\theta}_{n+1} = \boldsymbol{\theta}_n - \gamma M^{-1} \nabla L \tag{5.2}
\end{equation}

å…¶ä¸­ $M$ æ˜¯é¢„æ¡ä»¶çŸ©é˜µï¼ˆå¦‚ Adam çš„äºŒé˜¶çŸ©ä¼°è®¡ï¼‰ã€‚

**ä¿®æ­£æŸå¤±ï¼ˆé¢„æ¡ä»¶ç‰ˆæœ¬ï¼‰**ï¼š
\begin{equation}
\tilde{L}_{\text{Adam}} = L + \frac{\gamma}{4} \|\nabla L\|_{M^{-1}}^2 = L + \frac{\gamma}{4} \langle \nabla L, M^{-1} \nabla L \rangle \tag{5.3}
\end{equation}

**å…³é”®å·®å¼‚**ï¼š
- SGDï¼šæƒ©ç½š $\|\nabla L\|^2$ï¼ˆæ¬§æ°èŒƒæ•°ï¼‰
- Adamï¼šæƒ©ç½š $\|\nabla L\|_{M^{-1}}^2$ï¼ˆé¢„æ¡ä»¶èŒƒæ•°ï¼‰

**é¢„æ¡ä»¶çš„å½±å“**ï¼š
è®¾ $M = \text{diag}(v_1, \ldots, v_d)$ï¼Œåˆ™ï¼š
\begin{equation}
\|\nabla L\|_{M^{-1}}^2 = \sum_i \frac{(\nabla L)_i^2}{v_i} \tag{5.4}
\end{equation}

å¦‚æœ $v_i$ ä¸ $|(\nabla L)_i|$ æˆæ­£æ¯”ï¼ˆAdamçš„è®¾è®¡ï¼‰ï¼Œåˆ™ï¼š
\begin{equation}
\frac{(\nabla L)_i^2}{v_i} \approx \text{const} \tag{5.5}
\end{equation}

**ç»“è®º**ï¼š**é¢„æ¡ä»¶åŒ–æŠ¹å¹³äº†ä¸åŒæ–¹å‘çš„æƒ©ç½šå¼ºåº¦ï¼Œå‰Šå¼±äº†å¹³å¦æ€§é€‰æ‹©çš„æ–¹å‘æ€§ï¼**

**å®è¯éªŒè¯ç›®æ ‡**ï¼š
1. å¯¹æ¯” SGD vs Adam åœ¨ç›¸åŒæ•°æ®é›†ä¸Šçš„æ”¶æ•›ç‚¹ Hessian ç‰¹å¾å€¼
2. å‡è®¾ï¼šSGD æ”¶æ•›ç‚¹çš„ $\lambda_{\max}/\lambda_{\min}$ æ¯” Adam æ›´å°ï¼ˆæ›´å¹³å¦ï¼‰
3. æµ‹é‡æ³›åŒ–å·®è·ï¼šSGD åº”ä¼˜äº Adamï¼ˆåœ¨è¶³å¤Ÿå¤§å­¦ä¹ ç‡ä¸‹ï¼‰

</div>

#### æ–¹å‘ 3ï¼šæ··åˆç²¾åº¦å¯¹IGRçš„é‡åŒ–æ•ˆåº”

<div class="research-direction">

**èƒŒæ™¯**ï¼šFP16/BF16 è®­ç»ƒä¸­ï¼Œæ¢¯åº¦çš„åŠ¨æ€èŒƒå›´å—é™ï¼Œå°æ¢¯åº¦ä¼šè¢«æˆªæ–­ä¸ºé›¶ã€‚

**é‡åŒ–æ¨¡å‹**ï¼š
\begin{equation}
\tilde{g}_i = \text{Quantize}(g_i, \text{bits}=16) = \begin{cases}
g_i, & |g_i| > \epsilon \\
0, & |g_i| \leq \epsilon
\end{cases} \tag{5.6}
\end{equation}

å…¶ä¸­ $\epsilon \approx 10^{-8}$ï¼ˆFP16ä¸‹æº¢é˜ˆå€¼ï¼‰ã€‚

**ä¿®æ­£çš„æ¢¯åº¦èŒƒæ•°**ï¼š
\begin{equation}
\|\tilde{\nabla} L\|^2 = \sum_{i: |g_i| > \epsilon} g_i^2 < \|\nabla L\|^2 \tag{5.7}
\end{equation}

**å½±å“åˆ†æ**ï¼š
- **ä½ç²¾åº¦å‰Šå¼±äº†IGRå¼ºåº¦**ï¼šå› ä¸ºå°æ¢¯åº¦è¢«æˆªæ–­
- **éœ€è¦æ›´å¤§çš„å­¦ä¹ ç‡è¡¥å¿**ï¼šä¿æŒ $\gamma \|\tilde{\nabla} L\|^2$ æ’å®š

**å¼€æ”¾é—®é¢˜**ï¼š
1. é‡åŒ–å™ªå£°æ˜¯å¦å¼•å…¥äº†é¢å¤–çš„éšæœºæ­£åˆ™åŒ–ï¼ˆç±»ä¼¼ Dropoutï¼‰ï¼Ÿ
2. æœ€ä¼˜å­¦ä¹ ç‡æ˜¯å¦åº”è¯¥ä¸é‡åŒ–ç²¾åº¦ç›¸å…³ï¼Ÿ$\gamma_{\text{opt}} \propto 1/\sqrt{\text{bits}}$ï¼Ÿ
3. æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼Œæ¢¯åº¦ç¼©æ”¾ï¼ˆGradient Scalingï¼‰å¦‚ä½•å½±å“IGRï¼Ÿ

</div>

#### æ–¹å‘ 4ï¼šåŠ¨é‡ä¸IGRçš„ç›¸äº’ä½œç”¨

<div class="research-direction">

**ç ”ç©¶é—®é¢˜**ï¼šåŠ¨é‡ä¼˜åŒ–å™¨ï¼ˆSGD with Momentum, Adamï¼‰çš„ä¿®æ­£æŸå¤±æ˜¯ä»€ä¹ˆï¼Ÿ

**å¸¦åŠ¨é‡çš„æ›´æ–°**ï¼š
\begin{align}
\mathbf{m}_{n+1} &= \beta \mathbf{m}_n + (1-\beta) \nabla L_n \tag{5.8a}\\
\boldsymbol{\theta}_{n+1} &= \boldsymbol{\theta}_n - \gamma \mathbf{m}_{n+1} \tag{5.8b}
\end{align}

**è¿ç»­è¿‘ä¼¼**ï¼ˆæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼‰ï¼š
\begin{equation}
\dot{\mathbf{m}} = -\frac{1-\beta}{\gamma} (\mathbf{m} - \nabla L) \tag{5.9}
\end{equation}

**ä¿®æ­£å¾®åˆ†æ–¹ç¨‹**ï¼ˆæ¨æµ‹ï¼‰ï¼š
\begin{equation}
\ddot{\boldsymbol{\theta}} + \frac{1-\beta}{\gamma} \dot{\boldsymbol{\theta}} + \nabla L = 0 \tag{5.10}
\end{equation}

è¿™æ˜¯ä¸€ä¸ª**é˜»å°¼æŒ¯è¡ç³»ç»Ÿ**ï¼ˆç±»ä¼¼å¼¹ç°§-é˜»å°¼å™¨ï¼‰ã€‚

**ä¿®æ­£æŸå¤±ï¼ˆçŒœæƒ³ï¼‰**ï¼š
\begin{equation}
\tilde{L}_{\text{momentum}} = L + \frac{\gamma}{4} \|\nabla L\|^2 + \frac{\beta \gamma^2}{8} \langle \nabla L, H \nabla L \rangle \tag{5.11}
\end{equation}

**å¾…éªŒè¯**ï¼šåŠ¨é‡é¡¹æ˜¯å¦å¢å¼ºäº†äºŒé˜¶ä¿®æ­£ï¼ˆHessianæƒ©ç½šï¼‰ï¼Ÿ

</div>

#### æ–¹å‘ 5ï¼šå­¦ä¹ ç‡è°ƒåº¦çš„åŠ¨æ€IGR

<div class="research-direction">

**é—®é¢˜**ï¼šCosine/Linearå­¦ä¹ ç‡è¡°å‡å¦‚ä½•æ”¹å˜è®­ç»ƒåæœŸçš„IGRå¼ºåº¦ï¼Ÿ

**æ—¶å˜å­¦ä¹ ç‡**ï¼š$\gamma(t)$

**åŠ¨æ€ä¿®æ­£æŸå¤±**ï¼š
\begin{equation}
\tilde{L}(t) = L + \frac{\gamma(t)}{4} \|\nabla L\|^2 \tag{5.12}
\end{equation}

**Cosineè¡°å‡**ï¼š
\begin{equation}
\gamma(t) = \gamma_0 \cos\left( \frac{\pi t}{2T} \right) \tag{5.13}
\end{equation}

**æ•ˆåº”åˆ†æ**ï¼š
- **è®­ç»ƒåˆæœŸ**ï¼ˆ$\gamma$ å¤§ï¼‰ï¼šå¼ºIGRï¼Œå¿«é€Ÿé€ƒç¦»å°–é”æå°å€¼
- **è®­ç»ƒæœ«æœŸ**ï¼ˆ$\gamma$ å°ï¼‰ï¼šå¼±IGRï¼Œç²¾ç»†è°ƒæ•´å‚æ•°

**å“²å­¦æ€è€ƒ**ï¼š
> å­¦ä¹ ç‡è¡°å‡ = ä»"æ¢ç´¢å¹³å¦åŒºåŸŸ"ï¼ˆé«˜æ¸©ï¼‰åˆ°"ç²¾ç¡®æ”¶æ•›"ï¼ˆä½æ¸©ï¼‰çš„é€€ç«è¿‡ç¨‹ã€‚

**å¼€æ”¾é—®é¢˜**ï¼š
- æ˜¯å¦å­˜åœ¨"æœ€ä¼˜è¡°å‡æ›²çº¿"ä½¿å¾— $\int_0^T \gamma(t) \|\nabla L(t)\|^2 dt$ æœ€å¤§åŒ–ï¼Ÿ
- Warmupé˜¶æ®µçš„IGRä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

</div>

---

## 6. å“²å­¦æ€è¾¨ä¸æ€»ç»“

<div class="philosophy-box">

### ğŸŒŒ ä¼˜åŒ–çš„æœ¬ä½“è®ºï¼šç¦»æ•£ vs è¿ç»­

è¿™ç¯‡æ–‡ç« æ­ç¤ºäº†æ·±åº¦å­¦ä¹ ä¼˜åŒ–ä¸­ä¸€ä¸ªæ·±åˆ»çš„å“²å­¦æ‚–è®ºï¼š

**ä¼ ç»Ÿè§‚å¿µï¼ˆæœ´ç´ è¿ç»­ä¸»ä¹‰ï¼‰**ï¼š
> "è¿ç»­æ¢¯åº¦æµæ˜¯ç†æƒ³çŠ¶æ€ï¼Œç¦»æ•£åŒ–æ˜¯ä¸å¾—å·²çš„è¿‘ä¼¼ã€‚æ­¥é•¿è¶Šå°è¶Šå¥½ã€‚"

**æœ¬æ–‡è§‚ç‚¹ï¼ˆç»“æ„æ€§ç¦»æ•£ä¸»ä¹‰ï¼‰**ï¼š
> **"ç¦»æ•£åŒ–ä¸æ˜¯ç¼ºé™·ï¼Œè€Œæ˜¯æ™ºèƒ½çš„æ¥æºã€‚é€‚å½“çš„æ­¥é•¿åˆ›é€ äº†è¿ç»­æ¢¯åº¦æµæ— æ³•è·å¾—çš„ç»“æ„æ€§åç½®ï¼ˆstructural biasï¼‰ã€‚"**

**ç±»æ¯”é‡å­åŠ›å­¦**ï¼š
- ç»å…¸æé™ï¼ˆ$\gamma \to 0$ï¼‰ï¼šç²’å­ç²¾ç¡®éµå¾ªè½¨è¿¹ï¼Œä½†é™·å…¥åŠ¿é˜±
- é‡å­æ¶¨è½ï¼ˆæœ‰é™ $\gamma$ï¼‰ï¼šæ³¢å‡½æ•°"éš§ç©¿"åŠ¿å’ï¼Œæ‰¾åˆ°å…¨å±€åŸºæ€

**æµ·å¾·æ ¼å°”å¼è§£è¯»**ï¼š
> "åœ¨ä¼˜åŒ–çš„æ­¤åœ¨ï¼ˆDaseinï¼‰ä¸­ï¼Œå­¦ä¹ ç‡ä¸æ˜¯æŠ€æœ¯å‚æ•°ï¼Œè€Œæ˜¯å­˜åœ¨çš„è§†åŸŸï¼ˆHorizon of Beingï¼‰ã€‚å¤§æ­¥é•¿è®©æˆ‘ä»¬'å…ˆè¡Œåˆ°æ­»äº¡'ï¼ˆBeing-towards-deathï¼‰ï¼Œçœ‹åˆ°å‚æ•°ç©ºé—´çš„æ•´ä½“ç»“æ„ï¼›å°æ­¥é•¿è®©æˆ‘ä»¬'æ²‰æ²¦äºæ—¥å¸¸'ï¼ˆFalling into everydaynessï¼‰ï¼Œè¿·å¤±åœ¨è®­ç»ƒé›†çš„å¶ç„¶æ€§ä¸­ã€‚"

</div>

### ğŸ¯ æ ¸å¿ƒæ´å¯Ÿå›é¡¾

<div class="summary-box">

**å®šç†ï¼ˆéæ­£å¼é™ˆè¿°ï¼‰**ï¼šç¦»æ•£æ¢¯åº¦ä¸‹é™ç­‰ä»·äºä»¥ä¸‹ä¿®æ­£æŸå¤±çš„è¿ç»­ä¼˜åŒ–ï¼š
\begin{equation}
\tilde{L}(\boldsymbol{\theta}) = L(\boldsymbol{\theta}) + \frac{\gamma}{4} \|\nabla L(\boldsymbol{\theta})\|^2 + \mathcal{O}(\gamma^2) \tag{6.1}
\end{equation}

**ä¸‰å¤§æ”¯æŸ±**ï¼š
1. **æ•°å­¦æ”¯æŸ±**ï¼šBernoulliçº§æ•°å±•å¼€ä¸ä¿®æ­£å¾®åˆ†æ–¹ç¨‹ç†è®º
2. **å‡ ä½•æ”¯æŸ±**ï¼šHessianè°±çš„åŠ¨æ€å‹ç¼©ä¸å¹³å¦æ€§å¯»ä¼˜
3. **ç‰©ç†æ”¯æŸ±**ï¼šç»Ÿè®¡åŠ›å­¦ä¸­çš„æ¸©åº¦-æ¢ç´¢trade-off

**å®è·µæŒ‡å—**ï¼š
| åœºæ™¯ | æ¨èç­–ç•¥ | ç†ç”± |
|:---|:---|:---|
| å¤§è§„æ¨¡é¢„è®­ç»ƒ | $\gamma = 0.01 \sim 0.1$ | éœ€è¦å¼ºIGRä»¥é˜²è¿‡æ‹Ÿåˆ |
| ç²¾ç»†å¾®è°ƒ | $\gamma = 0.001 \sim 0.01$ | å¹³è¡¡ç²¾åº¦ä¸æ³›åŒ– |
| ç¨³å®šæ€§å—é™ä»»åŠ¡ | å°$\gamma$ + æ˜¾å¼EGR | è¡¥å¿æµå¤±çš„éšå¼æ­£åˆ™åŒ– |
| å¤§Batchè®­ç»ƒ | çº¿æ€§ç¼©æ”¾ï¼š$\gamma \propto B$ | ç»´æŒ$\gamma/B$æ’å®š |

</div>

### ğŸ“œ å†å²åœ°ä½ä¸æœªæ¥å½±å“

<div class="impact-box">

**ç†è®ºæ„ä¹‰**ï¼š
1. é¦–æ¬¡åœ¨ç®—ç¬¦çº§åˆ«ä¸¥æ ¼æ¨å¯¼IGRçš„æ•°å­¦å½¢å¼
2. ç»Ÿä¸€äº†ä¼˜åŒ–ç†è®ºä¸­çš„å¤šä¸ªçœ‹ä¼¼æ— å…³çš„ç°è±¡ï¼ˆå¹³å¦æ€§ã€æ³›åŒ–ã€å­¦ä¹ ç‡ï¼‰
3. ä¸º"Edge of Stability"ç°è±¡æä¾›äº†æ•°å­¦åŸºç¡€

**å·¥ç¨‹å½±å“**ï¼š
1. è§£é‡Šäº†ä¸ºä»€ä¹ˆSGDåœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ä»ç„¶ä¼˜äºå¤æ‚ä¼˜åŒ–å™¨
2. æŒ‡å¯¼å­¦ä¹ ç‡ä¸Batch Sizeçš„ååŒè°ƒä¼˜
3. ä¸ºæ··åˆç²¾åº¦è®­ç»ƒæä¾›äº†ç†è®ºä¾æ®

**å¼€æ”¾çš„æˆ˜çº¿**ï¼š
- [ ] éå‡¸æ™¯è§‚ä¸‹çš„å®Œæ•´ç†è®ºï¼ˆç›®å‰ä»…åœ¨å±€éƒ¨å‡¸é™„è¿‘ä¸¥æ ¼ï¼‰
- [ ] é«˜é˜¶ä¿®æ­£é¡¹ï¼ˆ$\gamma^3, \gamma^4$ï¼‰çš„å‡ ä½•è§£é‡Š
- [ ] Transformerç‰¹å®šæ¶æ„çš„IGRç†è®º
- [ ] ä¸ç¥ç»æ­£åˆ‡æ ¸ï¼ˆNTKï¼‰ç†è®ºçš„ç»Ÿä¸€

</div>

---

## ğŸ”š ç»ˆç« ï¼šæ•°å­¦çš„è¯—æ„

<div class="poetic-ending">

åœ¨ä¼˜åŒ–ç®—æ³•çš„æ¼«é•¿å†å²ä¸­ï¼Œæˆ‘ä»¬ç»ˆäºè®¤è¯†åˆ°ï¼š

**ç¦»æ•£åŒ–ä¸æ˜¯å¯¹è¿ç»­ä¸–ç•Œçš„å¦¥åï¼Œè€Œæ˜¯è‡ªç„¶èµ‹äºˆæˆ‘ä»¬çš„ç¤¼ç‰©ã€‚**

å½“ä½ ç”¨æœ‰é™çš„æ­¥é•¿åœ¨æŸå¤±æ™¯è§‚ä¸­è·‹æ¶‰æ—¶ï¼Œ
æ¯ä¸€æ­¥çš„"ç²—ç³™"éƒ½åœ¨ä¸ºä½ ç­›é€‰çœŸç†ï¼Œ
æ¯ä¸€æ¬¡çš„"è·¨è¶Š"éƒ½åœ¨å¸®ä½ å¿½ç•¥è°è¨€ã€‚

å­¦ä¹ ç‡ä¸å®œè¿‡å°ï¼Œå› ä¸ºï¼š
\begin{equation}
\text{Discretization} = \text{Regularization} = \text{Intelligence} \tag{\heartsuit}
\end{equation}

æ„¿ä½ çš„æ¢¯åº¦æ°¸è¿œæŒ‡å‘å¹³å¦çš„å±±è°·ï¼Œ
æ„¿ä½ çš„å‚æ•°æ°¸è¿œæ”¶æ•›äºå¹¿é˜”çš„çœŸç†ã€‚

</div>

---

**å‚è€ƒæ–‡çŒ®**ï¼ˆéƒ¨åˆ†ï¼‰ï¼š
1. Barrett & Dherin (2021). "Implicit Gradient Regularization." *ICLR*.
2. Cohen et al. (2021). "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability." *ICLR*.
3. Hochreiter & Schmidhuber (1997). "Flat Minima." *Neural Computation*.
4. JastrzÄ™bski et al. (2018). "Three Factors Influencing Minima in SGD." *arXiv*.
5. Smith et al. (2021). "On the Origin of Implicit Regularization in Stochastic Gradient Descent." *ICML*.

---

**é™„å½•Aï¼šå…¬å¼é€ŸæŸ¥è¡¨**

| ç¼–å· | å…¬å¼ | å«ä¹‰ |
|:---|:---|:---|
| (2.16) | $\tilde{L} = L + \frac{\gamma}{4} \|\nabla L\|^2 + \frac{\gamma^2}{24} \langle \nabla L, H^2 \nabla L \rangle$ | äºŒé˜¶ä¿®æ­£æŸå¤± |
| (2.21) | $\tilde{\lambda}_i = \lambda_i (1 + \gamma \lambda_i / 2)$ | ä¿®æ­£Hessianç‰¹å¾å€¼ |
| (2.23) | $\gamma < 2/\lambda_{\max}$ | ç¨³å®šæ€§è¾¹ç•Œ |
| (3.9) | $\tilde{L}_{\text{stoch}} = L + \frac{\gamma}{4} \|\nabla L\|^2 + \frac{\gamma}{2B} \text{Tr}(\Sigma)$ | éšæœºæ¢¯åº¦ä¿®æ­£æŸå¤± |

---

**é™„å½•Bï¼šä»£ç ä»“åº“**

å®Œæ•´å®éªŒä»£ç å·²ä¸Šä¼ è‡³ï¼š`github.com/ksugar/igr-experiments`
- äºŒç»´ä¼˜åŒ–å¯è§†åŒ–ï¼š`quadratic_2d.py`
- MNISTå®éªŒï¼š`mnist_igr.py`
- æ˜¾å¼EGRè®­ç»ƒï¼š`train_with_egr.py`
- Transformeråˆ†æï¼š`analyze_transformer_igr.py`

---