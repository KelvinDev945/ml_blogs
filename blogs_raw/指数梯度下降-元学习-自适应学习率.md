---
title: 指数梯度下降 + 元学习 = 自适应学习率
slug: 指数梯度下降-元学习-自适应学习率
date: 2022-03-03
tags: 优化, 梯度, 优化器, 生成模型, attention
status: completed
---

# 指数梯度下降 + 元学习 = 自适应学习率

**原文链接**: [https://spaces.ac.cn/archives/8968](https://spaces.ac.cn/archives/8968)

**发布日期**: 

---

前两天刷到了Google的一篇论文[《Step-size Adaptation Using Exponentiated Gradient Updates》](https://papers.cool/arxiv/2202.00145)，在其中学到了一些新的概念，所以在此记录分享一下。主要的内容有两个，一是非负优化的指数梯度下降，二是基于元学习思想的学习率调整算法，两者都颇有意思，有兴趣的读者也可以了解一下。

## 指数梯度下降 #

梯度下降大家可能听说得多了，指的是对于无约束函数$\mathcal{L}(\boldsymbol{\theta})$的最小化，我们用如下格式进行更新：  
\begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\end{equation}  
其中$\eta$是学习率。然而很多任务并非总是无约束的，对于最简单的非负约束，我们可以改为如下格式更新：  
\begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t \odot \exp\left(- \eta\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\right)\label{eq:egd}\end{equation}  
这里的$\odot$是逐位对应相乘（Hadamard积）。容易看到，只要初始化的$\boldsymbol{\theta}_0$是非负的，那么在整个更新过程中$\boldsymbol{\theta}_t$都会保持非负，这就是用于非负约束优化的“指数梯度下降”。

怎么理解这个“指数梯度下降”呢？也不难，转化为无约束的情形进行推导就行了。如果$\boldsymbol{\theta}$是非负的，那么$\boldsymbol{\varphi}=\log\boldsymbol{\theta}$就是可正可负的了，因此可以设$\boldsymbol{\theta}=e^{\boldsymbol{\varphi}}$转化为关于$\boldsymbol{\varphi}$的无约束优化问题，继而就可以用梯度下降解决：  
\begin{equation}\boldsymbol{\varphi}_{t+1} = \boldsymbol{\varphi}_t - \eta\nabla_{\boldsymbol{\varphi}}\mathcal{L}(e^{\boldsymbol{\varphi}_t}) = \boldsymbol{\varphi}_t - \eta e^{\boldsymbol{\varphi}_t}\odot\nabla_{e^{\boldsymbol{\varphi}}}\mathcal{L}(e^{\boldsymbol{\varphi}_t})\end{equation}  
我们认为梯度的$e^{\boldsymbol{\varphi}_t}\odot$这部分只起到了调节学习率的作用，所以它不是本质重要的，我们将它舍去得到  
\begin{equation}\boldsymbol{\varphi}_{t+1} = \boldsymbol{\varphi}_t - \eta \nabla_{e^{\boldsymbol{\varphi}}}\mathcal{L}(e^{\boldsymbol{\varphi}_t})\end{equation}  
两边取指数得  
\begin{equation}e^{\boldsymbol{\varphi}_{t+1}} = e^{\boldsymbol{\varphi}_t}\odot\exp\left( - \eta \nabla_{e^{\boldsymbol{\varphi}}}\mathcal{L}(e^{\boldsymbol{\varphi}_t})\right)\end{equation}  
换回$\boldsymbol{\theta}=e^{\boldsymbol{\varphi}}$就得到式$\eqref{eq:egd}$。

## 元学习调学习率 #

对于元学习（Meta Learning），可能多数读者都跟笔者一样听得多，但几乎没接触过。简单来说，普通机器学习跟元学习的关系，就像是数学中“函数”跟“泛函”的关系，泛函是“函数的函数”，元学习则是“学习如何学习（Learning How to Learn）”，也就是说它是关于“学习”本身的方法论，比如接下来要介绍的，就是“用梯度下降去调整梯度下降”。

我们从一般的梯度下降出发，记目标函数$\mathcal{L}$的梯度为$\boldsymbol{g}$，那么更新公式为  
\begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\boldsymbol{g}_t\end{equation}  
我们希望给每个分量都调节一下学习率，所以我们引入跟参数一样大小的非负变量$\boldsymbol{\nu}$，修改更新公式为  
\begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\boldsymbol{\nu}_{t+1}\odot\boldsymbol{g}_t\label{eq:update}\end{equation}  
那么，$\boldsymbol{\nu}$要按照什么规则迭代呢？记住我们最终的目的是最小化$\mathcal{L}$，所以$\boldsymbol{\nu}$的更新规则应该也要是梯度下降，而这里$\boldsymbol{\nu}$要求是非负的，所以我们用指数梯度下降：  
\begin{equation}\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot\exp\left(- \gamma\nabla_{\boldsymbol{\nu}_t}\mathcal{L}\right)\label{eq:update-nu}\end{equation}  
注意$\mathcal{L}$本来只是$\boldsymbol{\theta}$的函数，但根据$\eqref{eq:update}$，在$t$时刻我们有$\boldsymbol{\theta}_t = \boldsymbol{\theta}_{t-1} - \eta\boldsymbol{\nu}_t\odot\boldsymbol{g}_{t-1}$，所以根据链式法则有  
\begin{equation}\nabla_{\boldsymbol{\nu}_t}\mathcal{L} = -\eta\boldsymbol{g}_{t-1} \odot\nabla_{\boldsymbol{\theta}_t}\mathcal{L}= -\eta\boldsymbol{g}_{t-1} \odot\boldsymbol{g}_t\end{equation}  
代入到$\nu$的更新公式$\eqref{eq:update-nu}$，得到  
\begin{equation}\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot\exp\left( \gamma\eta\boldsymbol{g}_{t-1} \odot\boldsymbol{g}_t\right)\end{equation}  
将$\gamma\eta$合成一个参数$\gamma$，于是整个模型的更新公式是：  
\begin{equation}\begin{aligned}&\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot\exp\left( \gamma\boldsymbol{g}_{t-1} \odot\boldsymbol{g}_t\right) \\\  
&\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\boldsymbol{\nu}_{t+1}\odot\boldsymbol{g}_t\end{aligned}\end{equation}  
如果$\boldsymbol{\nu}$初始化为全1，那么将有  
\begin{equation}\boldsymbol{\nu}_{t+1} = \exp\left(\gamma\sum_{k=1}^t\boldsymbol{g}_{k-1} \odot\boldsymbol{g}_k\right)\end{equation}  
可以看到，该方法的学习率调节思路是：如果某分量相邻两步的梯度经常同号，那么对应项的累加结果就是正的，意味着我们可以适当扩大一下学习率；如果相邻两步的梯度经常异号，那么对应项的累加结果很可能是负的，意味着我们可以适当缩小一下学习率。

注意这跟Adam调学习率的思想是不一样的，Adam调节学习率的思想是如果某个分量的梯度长时间很小，那么就意味着该参数可能没学好，所以尝试放大它的学习率。两者也算是各有各的道理吧。

## 简单做个小结 #

本文主要对“指数梯度下降”和“元学习调学习率”两个概念做了简单笔记，“指数梯度下降”是非负约束优化的一个简单有效的方案，而“元学习调学习率”则是元学习的一个简单易懂的应用。其中在介绍“元学习调学习率”时笔者做了一些简化，相比原论文的形式更为简单一些，但思想是一致的。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/8968>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 03, 2022). 《指数梯度下降 + 元学习 = 自适应学习率 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/8968>

@online{kexuefm-8968,  
title={指数梯度下降 + 元学习 = 自适应学习率},  
author={苏剑林},  
year={2022},  
month={Mar},  
url={\url{https://spaces.ac.cn/archives/8968}},  
} 


---

## 公式推导与注释

### 1. 指数梯度下降的理论基础

#### 1.1 非负约束优化问题

对于参数空间受限于非负象限的优化问题：
\begin{equation}\min_{\boldsymbol{\theta} \geq \boldsymbol{0}} \mathcal{L}(\boldsymbol{\theta})\tag{1}\end{equation}

**数学直觉**：许多实际问题具有非负约束，如概率分布、注意力权重、资源分配等。

标准梯度下降无法保证非负性：
\begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\tag{2}\end{equation}

即使$\boldsymbol{\theta}_t \geq \boldsymbol{0}$，也可能产生$\boldsymbol{\theta}_{t+1}$的某些分量为负。

#### 1.2 指数梯度下降更新规则

指数梯度下降（EGD）定义为：
\begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t \odot \exp\left(- \eta\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\right)\tag{3}\end{equation}

**关键性质**：
1. **非负性保持**：$\boldsymbol{\theta}_0 > \boldsymbol{0} \Rightarrow \boldsymbol{\theta}_t > \boldsymbol{0}, \forall t$
2. **乘性更新**：相对变化与梯度成正比
3. **尺度不变性**：对参数缩放具有良好性质

分量形式：
\begin{equation}\theta_{t+1,i} = \theta_{t,i} \exp\left(- \eta \frac{\partial \mathcal{L}}{\partial \theta_{t,i}}\right)\tag{4}\end{equation}

#### 1.3 从对数空间的角度理解

引入对数参数化：
\begin{equation}\boldsymbol{\varphi} = \log \boldsymbol{\theta} \quad \Leftrightarrow \quad \boldsymbol{\theta} = \exp(\boldsymbol{\varphi})\tag{5}\end{equation}

**数学直觉**：对数变换将非负约束$\boldsymbol{\theta} > \boldsymbol{0}$转化为无约束问题$\boldsymbol{\varphi} \in \mathbb{R}^d$。

链式法则给出梯度关系：
\begin{equation}\nabla_{\boldsymbol{\varphi}}\mathcal{L}(e^{\boldsymbol{\varphi}}) = e^{\boldsymbol{\varphi}} \odot \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\tag{6}\end{equation}

在对数空间进行标准梯度下降：
\begin{equation}\boldsymbol{\varphi}_{t+1} = \boldsymbol{\varphi}_t - \eta \nabla_{\boldsymbol{\varphi}}\mathcal{L}(e^{\boldsymbol{\varphi}_t})\tag{7}\end{equation}

代入式(6)：
\begin{equation}\boldsymbol{\varphi}_{t+1} = \boldsymbol{\varphi}_t - \eta e^{\boldsymbol{\varphi}_t} \odot \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\tag{8}\end{equation}

**预条件化解释**：$e^{\boldsymbol{\varphi}_t}$项可视为自适应学习率。忽略此项得到：
\begin{equation}\boldsymbol{\varphi}_{t+1} = \boldsymbol{\varphi}_t - \eta \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\tag{9}\end{equation}

两边取指数：
\begin{equation}e^{\boldsymbol{\varphi}_{t+1}} = e^{\boldsymbol{\varphi}_t} \odot \exp\left(- \eta \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)\right)\tag{10}\end{equation}

恢复原参数$\boldsymbol{\theta} = e^{\boldsymbol{\varphi}}$即得式(3)。

### 2. 镜像下降与Bregman散度

#### 2.1 Bregman散度

给定严格凸函数$\psi: \mathbb{R}^d \to \mathbb{R}$，Bregman散度定义为：
\begin{equation}D_\psi(\boldsymbol{\theta}_1, \boldsymbol{\theta}_2) = \psi(\boldsymbol{\theta}_1) - \psi(\boldsymbol{\theta}_2) - \nabla\psi(\boldsymbol{\theta}_2)^{\top}(\boldsymbol{\theta}_1 - \boldsymbol{\theta}_2)\tag{11}\end{equation}

**几何意义**：函数值与其线性近似之差。

**性质**：
1. $D_\psi(\boldsymbol{\theta}_1, \boldsymbol{\theta}_2) \geq 0$（凸性保证）
2. $D_\psi(\boldsymbol{\theta}_1, \boldsymbol{\theta}_2) = 0 \Leftrightarrow \boldsymbol{\theta}_1 = \boldsymbol{\theta}_2$（严格凸保证）
3. 一般不对称：$D_\psi(\boldsymbol{\theta}_1, \boldsymbol{\theta}_2) \neq D_\psi(\boldsymbol{\theta}_2, \boldsymbol{\theta}_1)$

#### 2.2 镜像下降框架

镜像下降更新规则：
\begin{equation}\boldsymbol{\theta}_{t+1} = \arg\min_{\boldsymbol{\theta}} \left\{\eta \nabla\mathcal{L}(\boldsymbol{\theta}_t)^{\top}\boldsymbol{\theta} + D_\psi(\boldsymbol{\theta}, \boldsymbol{\theta}_t)\right\}\tag{12}\end{equation}

一阶最优性条件：
\begin{equation}\eta \nabla\mathcal{L}(\boldsymbol{\theta}_t) + \nabla\psi(\boldsymbol{\theta}_{t+1}) - \nabla\psi(\boldsymbol{\theta}_t) = \boldsymbol{0}\tag{13}\end{equation}

即：
\begin{equation}\nabla\psi(\boldsymbol{\theta}_{t+1}) = \nabla\psi(\boldsymbol{\theta}_t) - \eta \nabla\mathcal{L}(\boldsymbol{\theta}_t)\tag{14}\end{equation}

#### 2.3 负熵作为镜像函数

对于非负单纯形$\Delta_d = \{\boldsymbol{\theta} \geq \boldsymbol{0}: \sum_i \theta_i = 1\}$，选择负熵：
\begin{equation}\psi(\boldsymbol{\theta}) = \sum_{i=1}^d \theta_i \log \theta_i\tag{15}\end{equation}

梯度为：
\begin{equation}\nabla\psi(\boldsymbol{\theta}) = \boldsymbol{1} + \log \boldsymbol{\theta}\tag{16}\end{equation}

代入式(14)：
\begin{equation}\boldsymbol{1} + \log \boldsymbol{\theta}_{t+1} = \boldsymbol{1} + \log \boldsymbol{\theta}_t - \eta \nabla\mathcal{L}(\boldsymbol{\theta}_t)\tag{17}\end{equation}

化简得：
\begin{equation}\log \boldsymbol{\theta}_{t+1} = \log \boldsymbol{\theta}_t - \eta \nabla\mathcal{L}(\boldsymbol{\theta}_t)\tag{18}\end{equation}

两边取指数：
\begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t \odot \exp(-\eta \nabla\mathcal{L}(\boldsymbol{\theta}_t))\tag{19}\end{equation}

**重要结论**：指数梯度下降是以负熵为镜像函数的镜像下降。

#### 2.4 KL散度与负熵Bregman散度

负熵的Bregman散度正是KL散度：
\begin{equation}\begin{aligned}
D_\psi(\boldsymbol{p}, \boldsymbol{q}) &= \sum_i p_i \log p_i - \sum_i q_i \log q_i - \sum_i (1 + \log q_i)(p_i - q_i)\\
&= \sum_i p_i \log p_i - \sum_i p_i \log q_i\\
&= \sum_i p_i \log \frac{p_i}{q_i}\\
&= \text{KL}(\boldsymbol{p} \| \boldsymbol{q})
\end{aligned}\tag{20}\end{equation}

### 3. 在线学习理论与遗憾界

#### 3.1 在线凸优化设置

在线学习协议：
1. 第$t$轮，学习者选择$\boldsymbol{\theta}_t \in \mathcal{K}$
2. 环境揭示凸损失函数$\mathcal{L}_t: \mathcal{K} \to \mathbb{R}$
3. 学习者遭受损失$\mathcal{L}_t(\boldsymbol{\theta}_t)$

**遗憾（Regret）**定义为累积损失与最优固定策略的差距：
\begin{equation}R_T = \sum_{t=1}^T \mathcal{L}_t(\boldsymbol{\theta}_t) - \min_{\boldsymbol{\theta}^* \in \mathcal{K}} \sum_{t=1}^T \mathcal{L}_t(\boldsymbol{\theta}^*)\tag{21}\end{equation}

**目标**：设计算法使$R_T = o(T)$（次线性遗憾）。

#### 3.2 指数梯度下降的遗憾界

**定理1**（EGD遗憾界）：设$\mathcal{K} = \Delta_d$（概率单纯形），损失函数$\mathcal{L}_t$凸且$\|\nabla\mathcal{L}_t\|_\infty \leq G$。指数梯度下降（学习率$\eta = \sqrt{\frac{2\log d}{GT}}$）保证：
\begin{equation}R_T \leq \sqrt{2GT \log d}\tag{22}\end{equation}

**证明**：利用KL散度分析框架。对任意$\boldsymbol{\theta}^* \in \Delta_d$：

**步骤1**：凸函数性质
\begin{equation}\mathcal{L}_t(\boldsymbol{\theta}_t) - \mathcal{L}_t(\boldsymbol{\theta}^*) \leq \nabla\mathcal{L}_t(\boldsymbol{\theta}_t)^{\top}(\boldsymbol{\theta}_t - \boldsymbol{\theta}^*)\tag{23}\end{equation}

**步骤2**：镜像下降基本不等式
\begin{equation}\nabla\mathcal{L}_t(\boldsymbol{\theta}_t)^{\top}(\boldsymbol{\theta}_t - \boldsymbol{\theta}^*) \leq \frac{1}{\eta}\left[D_\psi(\boldsymbol{\theta}^*, \boldsymbol{\theta}_t) - D_\psi(\boldsymbol{\theta}^*, \boldsymbol{\theta}_{t+1})\right] + \frac{\eta}{2}\|\nabla\mathcal{L}_t(\boldsymbol{\theta}_t)\|^2\tag{24}\end{equation}

其中$\|\cdot\|$是关于$\nabla^2\psi$的对偶范数。

**步骤3**：对$t$求和（望远镜和）
\begin{equation}\sum_{t=1}^T \nabla\mathcal{L}_t(\boldsymbol{\theta}_t)^{\top}(\boldsymbol{\theta}_t - \boldsymbol{\theta}^*) \leq \frac{D_\psi(\boldsymbol{\theta}^*, \boldsymbol{\theta}_1)}{\eta} + \frac{\eta}{2}\sum_{t=1}^T \|\nabla\mathcal{L}_t(\boldsymbol{\theta}_t)\|^2\tag{25}\end{equation}

**步骤4**：界定各项

负熵散度有界：
\begin{equation}D_\psi(\boldsymbol{\theta}^*, \boldsymbol{\theta}_1) = \text{KL}(\boldsymbol{\theta}^* \| \boldsymbol{\theta}_1) \leq \log d\tag{26}\end{equation}

（当$\boldsymbol{\theta}_1 = (1/d, \ldots, 1/d)$均匀初始化时取等）

梯度平方和：
\begin{equation}\sum_{t=1}^T \|\nabla\mathcal{L}_t(\boldsymbol{\theta}_t)\|^2 \leq TG^2\tag{27}\end{equation}

**步骤5**：代入并优化学习率
\begin{equation}R_T \leq \frac{\log d}{\eta} + \frac{\eta TG^2}{2}\tag{28}\end{equation}

选择$\eta = \sqrt{\frac{2\log d}{TG^2}}$最小化右端：
\begin{equation}R_T \leq \sqrt{2TG^2 \log d} = \sqrt{2GT\log d}\tag{29}\end{equation}

**数学直觉**：$O(\sqrt{T})$遗憾是最优的，KL散度的对数上界提供了维度依赖。

### 4. 元学习的数学框架

#### 4.1 双层优化视角

元学习目标：学习如何学习，即优化优化器本身。

**外层优化**：优化器参数（如学习率）
\begin{equation}\min_{\boldsymbol{\nu}} \mathbb{E}_{\text{task}} \left[\mathcal{L}_{\text{val}}(\boldsymbol{\theta}^*(\boldsymbol{\nu}))\right]\tag{30}\end{equation}

**内层优化**：模型参数
\begin{equation}\boldsymbol{\theta}^*(\boldsymbol{\nu}) = \arg\min_{\boldsymbol{\theta}} \mathcal{L}_{\text{train}}(\boldsymbol{\theta}; \boldsymbol{\nu})\tag{31}\end{equation}

#### 4.2 自适应学习率的元学习推导

目标函数：
\begin{equation}\mathcal{L}(\boldsymbol{\theta})\tag{32}\end{equation}

引入分量级学习率$\boldsymbol{\nu} \in \mathbb{R}_{++}^d$：
\begin{equation}\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \boldsymbol{\nu}_{t+1} \odot \boldsymbol{g}_t\tag{33}\end{equation}

其中$\boldsymbol{g}_t = \nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}_t)$。

**元学习原则**：$\boldsymbol{\nu}$的更新也应最小化$\mathcal{L}$。

#### 4.3 学习率的梯度计算

链式法则：
\begin{equation}\nabla_{\boldsymbol{\nu}_t} \mathcal{L}(\boldsymbol{\theta}_t) = \nabla_{\boldsymbol{\nu}_t} \boldsymbol{\theta}_t \cdot \nabla_{\boldsymbol{\theta}_t} \mathcal{L}(\boldsymbol{\theta}_t)\tag{34}\end{equation}

由式(33)：
\begin{equation}\frac{\partial \theta_{t,i}}{\partial \nu_{t,j}} = -\eta g_{t-1,i} \delta_{ij}\tag{35}\end{equation}

矩阵形式：
\begin{equation}\nabla_{\boldsymbol{\nu}_t} \boldsymbol{\theta}_t = -\eta \text{diag}(\boldsymbol{g}_{t-1})\tag{36}\end{equation}

代入式(34)：
\begin{equation}\nabla_{\boldsymbol{\nu}_t} \mathcal{L} = -\eta \text{diag}(\boldsymbol{g}_{t-1}) \boldsymbol{g}_t = -\eta \boldsymbol{g}_{t-1} \odot \boldsymbol{g}_t\tag{37}\end{equation}

#### 4.4 指数梯度更新学习率

由于$\boldsymbol{\nu} > \boldsymbol{0}$，使用指数梯度下降：
\begin{equation}\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot \exp\left(-\gamma \nabla_{\boldsymbol{\nu}_t} \mathcal{L}\right)\tag{38}\end{equation}

代入式(37)：
\begin{equation}\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot \exp\left(\gamma \eta \boldsymbol{g}_{t-1} \odot \boldsymbol{g}_t\right)\tag{39}\end{equation}

定义$\gamma' = \gamma \eta$，合并超参数：
\begin{equation}\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot \exp\left(\gamma' \boldsymbol{g}_{t-1} \odot \boldsymbol{g}_t\right)\tag{40}\end{equation}

**完整算法**：
\begin{equation}\begin{cases}
\boldsymbol{\nu}_{t+1} = \boldsymbol{\nu}_t \odot \exp\left(\gamma \boldsymbol{g}_{t-1} \odot \boldsymbol{g}_t\right)\\
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \boldsymbol{\nu}_{t+1} \odot \boldsymbol{g}_t
\end{cases}\tag{41}\end{equation}

#### 4.5 累积形式与直觉

初始化$\boldsymbol{\nu}_0 = \boldsymbol{1}$，递推展开：
\begin{equation}\boldsymbol{\nu}_t = \exp\left(\gamma \sum_{k=1}^{t-1} \boldsymbol{g}_k \odot \boldsymbol{g}_{k+1}\right)\tag{42}\end{equation}

**数学直觉**：
1. **同号梯度**：若$g_{k,i} \cdot g_{k+1,i} > 0$（方向一致），累积项增大，$\nu_{t,i}$增大，加速该方向
2. **异号梯度**：若$g_{k,i} \cdot g_{k+1,i} < 0$（方向振荡），累积项减小，$\nu_{t,i}$减小，抑制振荡
3. **自适应性**：每个分量独立调整，实现分量级学习率自适应

### 5. 与Adam的比较分析

#### 5.1 Adam的学习率调整机制

Adam更新规则：
\begin{equation}\begin{aligned}
\boldsymbol{m}_t &= \beta_1 \boldsymbol{m}_{t-1} + (1-\beta_1) \boldsymbol{g}_t\\
\boldsymbol{v}_t &= \beta_2 \boldsymbol{v}_{t-1} + (1-\beta_2) \boldsymbol{g}_t^2\\
\boldsymbol{\theta}_{t+1} &= \boldsymbol{\theta}_t - \eta \frac{\boldsymbol{m}_t}{\sqrt{\boldsymbol{v}_t} + \epsilon}
\end{aligned}\tag{43}\end{equation}

有效学习率：
\begin{equation}\boldsymbol{\nu}_t^{\text{Adam}} = \frac{\eta}{\sqrt{\boldsymbol{v}_t} + \epsilon}\tag{44}\end{equation}

**Adam原则**：梯度小的分量需要更大学习率（可能未充分学习）。

#### 5.2 元学习方法的原则

元学习有效学习率（从式(42)）：
\begin{equation}\boldsymbol{\nu}_t^{\text{Meta}} = \eta \exp\left(\gamma \sum_{k=1}^{t-1} \boldsymbol{g}_k \odot \boldsymbol{g}_{k+1}\right)\tag{45}\end{equation}

**元学习原则**：梯度方向一致的分量需要更大学习率（稳定优化方向）。

#### 5.3 定量比较

考虑单个分量$i$：

**场景1：持续小梯度**
- Adam：$v_{t,i}$小 → $\nu_{t,i}^{\text{Adam}}$大
- 元学习：若$g_{k,i}$同号但小，累积慢 → $\nu_{t,i}^{\text{Meta}}$增长慢

**场景2：大梯度稳定方向**
- Adam：$v_{t,i}$大 → $\nu_{t,i}^{\text{Adam}}$小
- 元学习：$g_{k,i} \cdot g_{k+1,i}$持续大正值 → $\nu_{t,i}^{\text{Meta}}$指数增长

**场景3：梯度振荡**
- Adam：$v_{t,i}$取决于$\beta_2$，相对稳定
- 元学习：$g_{k,i} \cdot g_{k+1,i}$正负交替 → $\nu_{t,i}^{\text{Meta}}$被抑制

**结论**：两种方法基于不同启发式，适用于不同优化景观。

### 6. 收敛性分析

#### 6.1 凸情况的收敛率

**定理2**（元学习EGD收敛）：设$\mathcal{L}$为$L$-Lipschitz凸函数，学习率$\eta = O(1/\sqrt{T})$，$\gamma = O(1)$。则元学习指数梯度下降满足：
\begin{equation}\frac{1}{T}\sum_{t=1}^T \mathcal{L}(\boldsymbol{\theta}_t) - \mathcal{L}(\boldsymbol{\theta}^*) = O\left(\frac{\log T}{\sqrt{T}}\right)\tag{46}\end{equation}

**证明思路**：
1. 利用凸性：$\mathcal{L}(\boldsymbol{\theta}_t) - \mathcal{L}(\boldsymbol{\theta}^*) \leq \boldsymbol{g}_t^{\top}(\boldsymbol{\theta}_t - \boldsymbol{\theta}^*)$
2. 分析$\|\boldsymbol{\theta}_t - \boldsymbol{\theta}^*\|^2$的递推
3. $\boldsymbol{\nu}_t$的指数增长需要额外$\log T$因子控制

#### 6.2 强凸情况

**定理3**：若$\mathcal{L}$为$\mu$-强凸，则存在$\eta, \gamma$使得：
\begin{equation}\mathbb{E}[\|\boldsymbol{\theta}_T - \boldsymbol{\theta}^*\|^2] \leq \left(1 - \frac{\mu\eta}{2}\right)^T \|\boldsymbol{\theta}_0 - \boldsymbol{\theta}^*\|^2 + O\left(\frac{\eta^2 \sigma^2}{\mu}\right)\tag{47}\end{equation}

其中$\sigma^2$是梯度方差上界。

**收敛速率**：线性收敛 + 方差主导的残差。

### 7. 数值稳定性分析

#### 7.1 指数溢出问题

式(40)中的指数运算可能导致数值溢出：
\begin{equation}\nu_{t+1,i} = \nu_{t,i} \exp(\gamma g_{t-1,i} g_{t,i})\tag{48}\end{equation}

**问题**：若$\gamma g_{t-1,i} g_{t,i}$大，$\exp(\cdot)$可能超出浮点表示范围。

**解决方案1：对数空间计算**
\begin{equation}\log \nu_{t+1,i} = \log \nu_{t,i} + \gamma g_{t-1,i} g_{t,i}\tag{49}\end{equation}

使用时：
\begin{equation}\theta_{t+1,i} = \theta_{t,i} - \eta \exp(\log \nu_{t+1,i}) g_{t,i}\tag{50}\end{equation}

**解决方案2：裁剪机制**
\begin{equation}\nu_{t+1,i} = \nu_{t,i} \exp\left(\gamma \cdot \text{clip}(g_{t-1,i} g_{t,i}, -c, c)\right)\tag{51}\end{equation}

其中$c > 0$是裁剪阈值。

#### 7.2 学习率爆炸防护

限制$\boldsymbol{\nu}_t$的范围：
\begin{equation}\nu_{t,i} \in [\nu_{\min}, \nu_{\max}]\tag{52}\end{equation}

投影操作：
\begin{equation}\nu_{t+1,i} = \text{clip}\left(\nu_{t,i} \exp(\gamma g_{t-1,i} g_{t,i}), \nu_{\min}, \nu_{\max}\right)\tag{53}\end{equation}

典型值：$\nu_{\min} = 10^{-3}, \nu_{\max} = 10^3$。

#### 7.3 梯度噪声的影响

随机梯度情况：$\tilde{\boldsymbol{g}}_t = \boldsymbol{g}_t + \boldsymbol{\xi}_t$，其中$\mathbb{E}[\boldsymbol{\xi}_t] = \boldsymbol{0}$。

噪声对学习率更新的影响：
\begin{equation}\mathbb{E}[\tilde{g}_{t-1,i} \tilde{g}_{t,i}] = g_{t-1,i} g_{t,i} + \mathbb{E}[\xi_{t-1,i} \xi_{t,i}]\tag{54}\end{equation}

若噪声独立：$\mathbb{E}[\xi_{t-1,i} \xi_{t,i}] = 0$，方法对噪声鲁棒。

但方差影响：
\begin{equation}\text{Var}(\tilde{g}_{t-1,i} \tilde{g}_{t,i}) \approx g_{t-1,i}^2 \sigma_{t,i}^2 + g_{t,i}^2 \sigma_{t-1,i}^2\tag{55}\end{equation}

**建议**：在噪声大的场景减小$\gamma$。

### 8. 实际算法实现

#### 8.1 完整伪代码

```
输入：初始参数 θ₀，基础学习率 η，元学习率 γ
      最大迭代数 T
初始化：ν₀ = 1（全1向量）
       g₀ = 0

for t = 1 to T do
    # 计算梯度
    g_t = ∇_θ L(θ_{t-1})

    # 更新自适应学习率
    ν_t = ν_{t-1} ⊙ exp(γ · g_{t-1} ⊙ g_t)

    # 数值稳定性：裁剪学习率
    ν_t = clip(ν_t, ν_min, ν_max)

    # 更新参数
    θ_t = θ_{t-1} - η · ν_t ⊙ g_t

end for
返回：θ_T
```

#### 8.2 PyTorch实现示例

```python
import torch

class MetaEGD(torch.optim.Optimizer):
    def __init__(self, params, lr=1e-3, meta_lr=0.1,
                 nu_min=1e-3, nu_max=1e3):
        defaults = dict(lr=lr, meta_lr=meta_lr,
                       nu_min=nu_min, nu_max=nu_max)
        super(MetaEGD, self).__init__(params, defaults)

    def step(self, closure=None):
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad.data
                state = self.state[p]

                # 状态初始化
                if len(state) == 0:
                    state['nu'] = torch.ones_like(p.data)
                    state['prev_grad'] = torch.zeros_like(p.data)

                # 获取状态
                nu = state['nu']
                prev_grad = state['prev_grad']

                # 更新自适应学习率
                nu = nu * torch.exp(
                    group['meta_lr'] * prev_grad * grad
                )

                # 裁剪
                nu = torch.clamp(nu, group['nu_min'], group['nu_max'])

                # 更新参数
                p.data.add_(grad, alpha=-group['lr']*nu)

                # 保存状态
                state['nu'] = nu
                state['prev_grad'] = grad.clone()

        return loss
```

### 9. 计算复杂度分析

#### 9.1 时间复杂度

每次迭代的操作：
1. 梯度计算：$O(M)$（$M$为模型复杂度）
2. 逐元素乘法 $\boldsymbol{g}_{t-1} \odot \boldsymbol{g}_t$：$O(d)$
3. 指数运算：$O(d)$
4. 参数更新：$O(d)$

**总复杂度**：$O(M + d)$，与标准SGD相同量级。

#### 9.2 空间复杂度

需要额外存储：
1. $\boldsymbol{\nu}_t$：$O(d)$
2. $\boldsymbol{g}_{t-1}$：$O(d)$

**总额外空间**：$O(d)$

与Adam（存储$\boldsymbol{m}_t, \boldsymbol{v}_t$，共$2d$）相比更节省。

### 10. 实践建议与超参数调优

#### 10.1 超参数选择指南

**基础学习率 $\eta$**：
- 初始尝试：$\eta \in [10^{-4}, 10^{-2}]$
- 与标准SGD相比可略小（因为$\boldsymbol{\nu}_t$会放大）

**元学习率 $\gamma$**：
- 推荐范围：$\gamma \in [0.01, 0.5]$
- 梯度噪声大时减小$\gamma$
- 损失landscape平滑时可增大$\gamma$

**裁剪范围**：
- $\nu_{\min} = 10^{-3}, \nu_{\max} = 10^{3}$（默认）
- 训练不稳定时收紧范围

#### 10.2 适用场景

**推荐使用**：
1. 稀疏梯度问题（NLP、推荐系统）
2. 不同参数有不同优化难度
3. 梯度方向相对稳定

**不推荐使用**：
1. 极高维问题（存储$\boldsymbol{g}_{t-1}$）
2. 梯度噪声极大
3. 需要快速收敛的场景（warmup慢）

#### 10.3 调试技巧

**监控指标**：
1. $\|\boldsymbol{\nu}_t\|_\infty$：检查是否触及边界
2. $\text{sign}(\boldsymbol{g}_{t-1} \odot \boldsymbol{g}_t)$的正负比例
3. 有效学习率分布：$\eta \boldsymbol{\nu}_t$

**常见问题**：
- **学习率爆炸**：减小$\gamma$或收紧$\nu_{\max}$
- **收敛慢**：增大$\eta$或$\gamma$
- **损失振荡**：减小$\gamma$（过度放大振荡方向）

### 11. 理论扩展与变体

#### 11.1 带动量的变体

结合Nesterov动量：
\begin{equation}\begin{aligned}
\boldsymbol{v}_t &= \beta \boldsymbol{v}_{t-1} + \boldsymbol{g}_t\\
\boldsymbol{\nu}_{t+1} &= \boldsymbol{\nu}_t \odot \exp(\gamma \boldsymbol{v}_{t-1} \odot \boldsymbol{v}_t)\\
\boldsymbol{\theta}_{t+1} &= \boldsymbol{\theta}_t - \eta \boldsymbol{\nu}_{t+1} \odot \boldsymbol{v}_t
\end{aligned}\tag{56}\end{equation}

**优势**：动量平滑梯度估计，减少噪声影响。

#### 11.2 自适应元学习率

使$\gamma$也自适应：
\begin{equation}\gamma_t = \gamma_0 \cdot \exp\left(-\alpha \cdot \frac{t}{T}\right)\tag{57}\end{equation}

**动机**：训练后期梯度更稳定，可减小元学习率避免过拟合。

#### 11.3 分组学习率

将参数分组$\{\boldsymbol{\theta}^{(1)}, \ldots, \boldsymbol{\theta}^{(K)}\}$，每组独立$\boldsymbol{\nu}^{(k)}$：
\begin{equation}\nu_{t+1}^{(k)} = \nu_t^{(k)} \odot \exp\left(\gamma_k \boldsymbol{g}_{t-1}^{(k)} \odot \boldsymbol{g}_t^{(k)}\right)\tag{58}\end{equation}

**应用**：神经网络不同层使用不同$\gamma_k$。

### 12. 实验建议与基准测试

#### 12.1 玩具问题验证

**二次碗测试**：
\begin{equation}\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{2}\boldsymbol{\theta}^{\top} Q \boldsymbol{\theta}\tag{59}\end{equation}

其中$Q = \text{diag}(1, 10, 100)$（不同曲率）。

**期望行为**：$\nu_{t,3} > \nu_{t,2} > \nu_{t,1}$（高曲率方向更大学习率）。

#### 12.2 标准基准

1. **MNIST/CIFAR-10**：小规模视觉任务
2. **Penn TreeBank**：语言建模
3. **IMDB情感分析**：稀疏特征场景

**对比优化器**：SGD, Adam, RMSProp, AdaGrad。

**评估指标**：
- 训练损失曲线
- 验证准确率
- 收敛步数（达到目标损失的迭代次数）

### 总结

本文详细推导了指数梯度下降与元学习自适应学习率的数学基础：

1. **指数梯度下降**：通过对数空间变换和镜像下降框架，建立了非负约束优化的理论基础
2. **KL散度联系**：证明了负熵Bregman散度即KL散度，连接信息几何与优化
3. **在线学习理论**：给出了$O(\sqrt{T\log d})$遗憾界
4. **元学习推导**：从双层优化出发，严格推导了自适应学习率更新规则
5. **数值稳定性**：提供了对数空间计算和裁剪机制
6. **实践指导**：包含完整实现、超参数建议和调试技巧

核心洞察：梯度方向的一致性（$\boldsymbol{g}_{t-1} \odot \boldsymbol{g}_t$）可作为学习率调整的信号，这与Adam基于梯度大小的策略形成互补。

