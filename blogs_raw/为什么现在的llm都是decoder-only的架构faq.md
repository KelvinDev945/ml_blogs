---
title: 《为什么现在的LLM都是Decoder-only的架构？》FAQ
slug: 为什么现在的llm都是decoder-only的架构faq
date: 
source: https://spaces.ac.cn/archives/9547
tags: 问答, 语言模型, 文本生成, attention, 生成模型
status: pending
---

# 《为什么现在的LLM都是Decoder-only的架构？》FAQ

**原文链接**: [https://spaces.ac.cn/archives/9547](https://spaces.ac.cn/archives/9547)

**发布日期**: 

---

上周笔者写了[《为什么现在的LLM都是Decoder-only的架构？》](/archives/9529)，总结了一下我在这个问题上的一些实验结论和猜测。果然是热点问题流量大，paperweekly的转发没多久阅读量就破万了，知乎上点赞数也不少。在几个平台上，陆陆续续收到了读者的一些意见或者疑问，总结了其中一些有代表性的问题，做成了本篇FAQ，希望能进一步帮助大家解决疑惑。

## 回顾 #

在[《为什么现在的LLM都是Decoder-only的架构？》](/archives/9529)中，笔者对GPT和UniLM两种架构做了对比实验，然后结合以往的研究经历，猜测了如下结论：

> 1、输入部分的注意力改为双向不会带来收益，Encoder-Decoder架构的优势很可能只是源于参数翻倍；
> 
> 2、双向注意力没有带来收益，可能是因为双向注意力的低秩问题导致效果下降。

所以，基于这两点推测，我们得到结论：

> 在同等参数量、同等推理成本下，Decoder-only架构是最优选择。

相关实验和思考的细节，请读者移步阅读原文，这里就不重复了。

## 问答 #

这里对读者的部分疑惑给出自己的答案。

> **问题1：** $n \gg d$似乎不成立？

**答：** $n$是序列长度，$d$是head_size不是hidden_size，在多头注意力中，head_size = hidden_size / heads，比如BERT base中head_size = 768 / 12 = 64，而预训练长度$n$一般为512，所以$n \gg d$大致上都是成立的。

> **问题2：** BERT和初代GPT参数量一样，为什么BERT在理解任务上更好呢？

**答：** BERT和GPT不仅架构不一样，预训练任务也不一样，无法公平比较。原文最后笔者已经给出了一个利用GPT的思想改进BERT的思路，并且初步的实验显示它很可能会优于BERT，那个实验才是严格控制变量的。

> **问题3：** “双向注意力的低秩问题带来的效果下降”这看起来像一个bug。现在工业界绝大多数模型都是双向注意力，波及范围也太广了吧？

**答：** 我们并没有说“双向注意力在任何任务上都非常糟糕”之类的结论，“现在工业界绝大多数模型都是双向注意力”这个现象其实跟原文的结论并不冲突。我们在原文的实验结论是“在生成任务上的Encoder引入双向注意力似乎不会带来收益”，结论的条件是很明确的——“在生成任务的Encoder”。

> **问题4：** 不是吧…decorder模型更适合对话模型而已，在谷歌内部，基于llm的encorder模型，decorder模型和encorder-decorder模型都有，适用场景不同，其他两个在其他任务上效果更好

**答：** 这个问题的回答跟上一个问题类似，“decorder模型和encorder-decorder模型都有”的现象，跟原文结论不矛盾。我们只是初步推测“在生成任务上的Encoder引入双向注意力似乎不会带来收益”，并没有说Encoder带来的参数翻倍不会带来收益。

> **问题5：** 你的结论跟T5、UL2的结论似乎矛盾？

**答：** 首先，原文的结论跟UL2的并不矛盾，原文推测“在同等参数量、同等推理成本下，Decoder-only架构是最优选择”，UL2的结论是Encoder-Decoder效果更好，但Encoder-Decoder和Decoder-only不是同等参数量的。其次，原文的结论跟T5中的实验结果（Table 2）确实有些冲突，然而，我对T5的实验结果也存疑：

> 1、该表格中的decoder-only与unilm是否真的做到了严格的控制变量，因为两者相差实在太大了，感觉这个差距是不合理的，即纵然decoder-only可能不如unilm，但差距应该不至于那么大；
> 
> 2、本文中比较的是同样的任务和数据前提下，用unilm和decoder-only分别从零训练，对比训练结果（直接对比预训练的结果，不微调到其他任务上）；而T5论文比较的是各种任务预训练后，再在下游任务微调的结果。两者流程不一样，是否可能产生结果上的差异？

> **问题6：** 最后的实验loss下降更快能说明模型效果更好吗？

**答：** 在目前笔者训练的步数来看，正反混合注意力表现一直更好，只能猜测后面这个趋势也一直保持，这是目前我能做到的实验上限了。期待有兴趣有条件的读者能进一步实验来肯定或者否定该结论。

> **问题7：** 关于您说的“GPT跟UniLM相比才算是严格控制变量”，我觉得不太准确。Google UL2 论文指出，对于 pre-trained language model， 模型架构与预训练任务都对模型质量起关键作用。

**答：** 本文的UniLM和GPT，指的是只有Attention Mask不一致的两个模型架构，在做对比实验的时候，除了Attention Mask不一致外，其他所有细节都是对齐的。

> **问题8：** 会不会还有一个原因，下三角或上三角mask更能够把位置编码的信息处理得更好？

**答：** 这确实是一个很新颖的观点，我没有从这个角度思考过。但事实上，三角形mask除了带来秩的提升外，确确实实也带来了位置识别上的优势，它打破了transformer的置换不变性，直接引入了从左往右的序，所以甚至不加位置编码都行。也许两者都是起作用的原因。

## 小结 #

本文对上一篇文章部分读者提出的一些疑问做了回答。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/9547>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 20, 2023). 《《为什么现在的LLM都是Decoder-only的架构？》FAQ 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/9547>

@online{kexuefm-9547,  
title={《为什么现在的LLM都是Decoder-only的架构？》FAQ},  
author={苏剑林},  
year={2023},  
month={Mar},  
url={\url{https://spaces.ac.cn/archives/9547}},  
} 


---

## 公式推导与注释

TODO: 添加详细的数学公式推导和注释

