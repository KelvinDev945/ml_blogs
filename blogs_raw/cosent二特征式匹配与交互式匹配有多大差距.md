---
title: CoSENTï¼ˆäºŒï¼‰ï¼šç‰¹å¾å¼åŒ¹é…ä¸äº¤äº’å¼åŒ¹é…æœ‰å¤šå¤§å·®è·ï¼Ÿ
slug: cosentäºŒç‰¹å¾å¼åŒ¹é…ä¸äº¤äº’å¼åŒ¹é…æœ‰å¤šå¤§å·®è·
date: 2022-01-12
tags: è¯­ä¹‰, è¯­ä¹‰ç›¸ä¼¼åº¦, å¯¹æ¯”å­¦ä¹ , å¥å‘é‡, æ–‡æœ¬åŒ¹é…, Powellä¼˜åŒ–
status: completed
tags_reviewed: true
---

# CoSENTï¼ˆäºŒï¼‰ï¼šç‰¹å¾å¼åŒ¹é…ä¸äº¤äº’å¼åŒ¹é…æœ‰å¤šå¤§å·®è·ï¼Ÿ

**åŸæ–‡é“¾æ¥**: [https://spaces.ac.cn/archives/8860](https://spaces.ac.cn/archives/8860)

**å‘å¸ƒæ—¥æœŸ**: 2022-01-12

---

<div class="theorem-box">

### æ ¸å¿ƒé—®é¢˜

æ–‡æœ¬åŒ¹é…æœ‰ä¸¤ç§ä¸»æµæ–¹æ¡ˆï¼š

**ç‰¹å¾å¼ï¼ˆRepresentation-basedï¼‰**ï¼š
- ä¸¤ä¸ªå¥å­åˆ†åˆ«ç¼–ç ä¸ºå¥å‘é‡
- é€šè¿‡cosæˆ–æµ…å±‚ç½‘ç»œèåˆ
- ä¼˜åŠ¿ï¼šæ•ˆç‡é«˜ï¼Œå¯ç¼“å­˜å¥å‘é‡
- åŠ£åŠ¿ï¼šäº¤äº’æµ…ï¼Œæ•ˆæœé€šå¸¸è¾ƒå·®

**äº¤äº’å¼ï¼ˆInteraction-basedï¼‰**ï¼š
- ä¸¤ä¸ªå¥å­æ‹¼æ¥åè”åˆç¼–ç 
- æ·±å±‚æ¬¡çš„tokençº§äº¤äº’
- ä¼˜åŠ¿ï¼šæ•ˆæœé€šå¸¸æœ€å¥½
- åŠ£åŠ¿ï¼šæ•ˆç‡ä½ï¼Œæ— æ³•ç¼“å­˜

**æœ¬æ–‡æ¢ç´¢**ï¼šCoSENTèƒ½å¦æ¥è¿‘ç”šè‡³è¾¾åˆ°äº¤äº’å¼çš„æ•ˆæœï¼Ÿ

</div>

---

## ä¸€ã€èƒŒæ™¯ä¸åŠ¨æœº

### 1.1 ä¸¤ç§åŒ¹é…èŒƒå¼

<div class="derivation-box">

### èŒƒå¼å¯¹æ¯”

<div class="formula-explanation">

<div class="formula-step">
<div class="step-label">ç‰¹å¾å¼æ–¹æ¡ˆ</div>

$$
\begin{aligned}
\mathbf{u} &= \text{Encoder}_1(\text{text}_1) \\
\mathbf{v} &= \text{Encoder}_2(\text{text}_2) \\
\text{score} &= f(\mathbf{u}, \mathbf{v})
\end{aligned}
\tag{1}
$$

å…¶ä¸­ $f$ é€šå¸¸æ˜¯ï¼š
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼š$\cos(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u}^\top \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}$
- åŒçº¿æ€§ï¼š$\mathbf{u}^\top W \mathbf{v}$
- MLPï¼š$\text{MLP}([\mathbf{u}; \mathbf{v}; \mathbf{u} \odot \mathbf{v}; |\mathbf{u} - \mathbf{v}|])$

<div class="step-explanation">

**ç‰¹ç‚¹**ï¼š
- $\text{Encoder}_1$ å’Œ $\text{Encoder}_2$ é€šå¸¸å…±äº«å‚æ•°
- ä¸¤ä¸ªå¥å­**ç‹¬ç«‹ç¼–ç **ï¼Œæ— tokençº§äº¤äº’
- å¯ä»¥é¢„è®¡ç®—å¹¶ç¼“å­˜å¥å‘é‡
- é€‚åˆæ£€ç´¢åœºæ™¯ï¼ˆç¦»çº¿ç¼–ç ï¼Œåœ¨çº¿æ£€ç´¢ï¼‰

**ä»£è¡¨æ–¹æ³•**ï¼š
- Sentence-BERT (SBERT)
- SimCSE
- CoSENTï¼ˆæœ¬æ–‡ï¼‰

</div>
</div>

<div class="formula-step">
<div class="step-label">äº¤äº’å¼æ–¹æ¡ˆ</div>

$$
\begin{aligned}
\text{input} &= [\text{CLS}] \, \text{text}_1 \, [\text{SEP}] \, \text{text}_2 \, [\text{SEP}] \\
\mathbf{h} &= \text{Encoder}(\text{input}) \\
\text{score} &= \text{Classifier}(\mathbf{h}_{\text{[CLS]}})
\end{aligned}
\tag{2}
$$

<div class="step-explanation">

**ç‰¹ç‚¹**ï¼š
- ä¸¤ä¸ªå¥å­**è”åˆç¼–ç **
- é€šè¿‡self-attentionå®ç°tokençº§äº¤äº’
- æ¯æ¬¡æŸ¥è¯¢éƒ½éœ€è¦é‡æ–°ç¼–ç 
- é€‚åˆåˆ†ç±»åœºæ™¯ï¼ˆç²¾ç¡®ä½†æ…¢ï¼‰

**ä»£è¡¨æ–¹æ³•**ï¼š
- BERTåˆ†ç±»å™¨
- RoBERTaåˆ†ç±»å™¨
- ERNIEäº¤äº’

**äº¤äº’æ·±åº¦ç¤ºä¾‹**ï¼ˆBERTçš„Attentionï¼‰ï¼š

åœ¨ç¬¬ $\ell$ å±‚ï¼Œtoken $i$ å¯ä»¥"çœ‹åˆ°"token $j$ï¼š
$$
\text{Attention}_{ij}^{(\ell)} = \text{softmax}\left(\frac{Q_i^{(\ell)} K_j^{(\ell)\top}}{\sqrt{d_k}}\right)
$$

ç»è¿‡12å±‚ï¼ˆBERT baseï¼‰ï¼Œä¸¤ä¸ªå¥å­çš„tokenå……åˆ†äº¤äº’ã€‚

</div>
</div>

</div>

</div>

</div>

### 1.2 ä¼ ç»Ÿè§‚ç‚¹

<div class="note-box">

**ä¸€èˆ¬è®¤ä¸º**ï¼š
- äº¤äº’å¼ > ç‰¹å¾å¼ï¼ˆå‡†ç¡®æ€§ï¼‰
- ç‰¹å¾å¼ > äº¤äº’å¼ï¼ˆæ•ˆç‡ï¼‰
- å·®è·æ˜¾è‘—ï¼ˆ5-10ä¸ªç™¾åˆ†ç‚¹ï¼‰

**æœ¬æ–‡æŒ‘æˆ˜**ï¼š
- CoSENTèƒ½å¦ç¼©å°è¿™ä¸ªå·®è·ï¼Ÿ
- ç†è®ºä¸Šå·®è·åˆ°åº•æœ‰å¤šå¤§ï¼Ÿ
- ä»€ä¹ˆæƒ…å†µä¸‹å·®è·æ›´æ˜æ˜¾ï¼Ÿ

</div>

---

## äºŒã€è‡ªåŠ¨é˜ˆå€¼æœç´¢

### 2.1 é—®é¢˜çš„æå‡º

åœ¨[ã€ŠCoSENTï¼ˆä¸€ï¼‰ï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆã€‹](/archives/8847)ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨**Spearmanç³»æ•°**è¯„æµ‹ï¼Œå®ƒåªä¾èµ–é¢„æµ‹ç»“æœçš„ç›¸å¯¹é¡ºåºï¼Œä¸éœ€è¦é˜ˆå€¼ã€‚

ä½†å¦‚æœè¯„æµ‹æŒ‡æ ‡æ˜¯**accuracy**æˆ–**F1**ï¼Œåˆ™å¿…é¡»ç¡®å®šä¸€ä¸ªé˜ˆå€¼ $\tau$ï¼š
$$
\text{prediction} = \begin{cases}
1 (\text{æ­£æ ·æœ¬}), & \text{score} > \tau \\
0 (\text{è´Ÿæ ·æœ¬}), & \text{score} \leq \tau
\end{cases}
\tag{3}
$$

<div class="intuition-box">

### ğŸ§  ä¸ºä»€ä¹ˆéœ€è¦è‡ªåŠ¨æœç´¢ï¼Ÿ

**æœ´ç´ åšæ³•**ï¼šåœ¨éªŒè¯é›†ä¸Šéå†æ‰€æœ‰å¯èƒ½çš„é˜ˆå€¼ï¼Œé€‰æ‹©ä½¿æŒ‡æ ‡æœ€å¤§çš„é‚£ä¸ªã€‚

**é—®é¢˜**ï¼š
- äºŒåˆ†ç±»ï¼šä¸€ç»´æœç´¢ï¼Œè¿˜å¯ä»¥
- å¤šåˆ†ç±»ï¼šé«˜ç»´æœç´¢ï¼Œç»„åˆçˆ†ç‚¸

**æ›´å¥½çš„æ–¹æ¡ˆ**ï¼šä½¿ç”¨ä¼˜åŒ–ç®—æ³•è‡ªåŠ¨æœç´¢æœ€ä¼˜é˜ˆå€¼

</div>

### 2.2 å¤šåˆ†ç±»çš„é˜ˆå€¼é—®é¢˜

<div class="derivation-box">

### æ¨å¹¿åˆ°å¤šåˆ†ç±»

<div class="formula-explanation">

<div class="formula-step">
<div class="step-label">æ ‡å‡†å¤šåˆ†ç±»é¢„æµ‹</div>

å¯¹äº $n$ åˆ†ç±»é—®é¢˜ï¼Œæ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ $[p_1, p_2, \ldots, p_n]$ï¼Œé€šå¸¸çš„é¢„æµ‹è§„åˆ™ï¼š
$$
\hat{y} = \arg\max_{i} p_i
\tag{4}
$$

</div>

<div class="formula-step">
<div class="step-label">åŠ æƒé¢„æµ‹ï¼ˆå¼•å…¥é˜ˆå€¼ï¼‰</div>

å¼•å…¥é˜ˆå€¼å‘é‡ $\mathbf{t} = [t_1, t_2, \ldots, t_n]$ï¼š
$$
\hat{y} = \arg\max_{i} (p_i \cdot t_i)
\tag{5}
$$

<div class="step-explanation">

**ç›´è§‚ç†è§£**ï¼š
- $t_i > 1$ï¼šå¢åŠ ç±»åˆ« $i$ çš„"é—¨æ§›"ï¼ˆæ›´éš¾è¢«é¢„æµ‹ä¸ºç±»åˆ« $i$ï¼‰
- $t_i < 1$ï¼šé™ä½ç±»åˆ« $i$ çš„"é—¨æ§›"ï¼ˆæ›´å®¹æ˜“è¢«é¢„æµ‹ä¸ºç±»åˆ« $i$ï¼‰
- $t_i = 1$ï¼šé€€åŒ–ä¸ºæ ‡å‡†argmax

**åº”ç”¨åœºæ™¯**ï¼š
- ç±»åˆ«ä¸å¹³è¡¡ï¼šå¢å¤§ç¨€æœ‰ç±»çš„æƒé‡
- é”™åˆ†ä»£ä»·ä¸åŒï¼šå¢å¤§é«˜ä»£ä»·ç±»çš„æƒé‡
- åéªŒæ ¡å‡†ï¼šæ ¹æ®éªŒè¯é›†è°ƒæ•´é¢„æµ‹

</div>
</div>

<div class="formula-step">
<div class="step-label">äºŒåˆ†ç±»çš„ç‰¹ä¾‹</div>

äºŒåˆ†ç±»æ—¶ï¼Œ$n=2$ï¼Œè®¾ $p_1 = p$ï¼ˆæ­£æ ·æœ¬æ¦‚ç‡ï¼‰ï¼Œ$p_2 = 1-p$ã€‚

å…¬å¼ (5) å˜ä¸ºï¼š
$$
\hat{y} = \begin{cases}
1, & p \cdot t_1 > (1-p) \cdot t_2 \\
0, & \text{otherwise}
\end{cases}
\tag{6}
$$

ç­‰ä»·äºï¼š
$$
\hat{y} = \begin{cases}
1, & p > \frac{t_2}{t_1 + t_2} := \tau \\
0, & \text{otherwise}
\end{cases}
\tag{7}
$$

å³ä¼ ç»Ÿçš„é˜ˆå€¼å½¢å¼ï¼ˆé˜ˆå€¼ $\tau = t_2/(t_1+t_2)$ï¼‰ã€‚

</div>

</div>

</div>

</div>

### 2.3 Powellä¼˜åŒ–æ–¹æ³•

<div class="theorem-box">

### Powellæ–¹æ³•ç®€ä»‹

**é€‚ç”¨åœºæ™¯**ï¼š
- æ— æ¢¯åº¦ä¼˜åŒ–ï¼ˆç›®æ ‡å‡½æ•°ä¸å¯å¯¼ï¼‰
- ä½ç»´é—®é¢˜ï¼ˆå‚æ•° < 100ï¼‰
- å±€éƒ¨ä¼˜åŒ–

**æ ¸å¿ƒæ€æƒ³**ï¼š
1. æ²¿åæ ‡è½´æ–¹å‘ä¾æ¬¡è¿›è¡Œä¸€ç»´æœç´¢
2. æ¯è½®è¿­ä»£åæ›´æ–°æœç´¢æ–¹å‘
3. é€æ­¥é€¼è¿‘æœ€ä¼˜è§£

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸éœ€è¦æ¢¯åº¦ä¿¡æ¯
- âœ… å¯¹å…‰æ»‘æ€§è¦æ±‚ä½
- âœ… scipyæœ‰ç°æˆå®ç°

**ç¼ºç‚¹**ï¼š
- âš ï¸ å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜
- âš ï¸ é«˜ç»´æ—¶æ•ˆç‡ä½
- âš ï¸ å¯¹åˆå§‹å€¼æ•æ„Ÿ

</div>

<div class="intuition-box">

### ğŸ§  ä¸ºä»€ä¹ˆé€‰æ‹©Powellè€Œä¸æ˜¯æ¢¯åº¦ä¸‹é™ï¼Ÿ

**é˜ˆå€¼æœç´¢çš„ç‰¹ç‚¹**ï¼š

1. **ç¦»æ•£è¯„ä¼°æŒ‡æ ‡**ï¼š
   - Accuracyã€F1ç­‰æŒ‡æ ‡æ˜¯**ä¸å¯å¾®**çš„
   - å¯¹é˜ˆå€¼çš„å¾®å°æ”¹å˜ï¼ŒæŒ‡æ ‡å¯èƒ½çªå˜
   - æ¢¯åº¦ä¸å­˜åœ¨æˆ–ä¸º0

2. **ä½ç»´ç©ºé—´**ï¼š
   - äºŒåˆ†ç±»ï¼š1ä¸ªé˜ˆå€¼
   - nåˆ†ç±»ï¼šnä¸ªé˜ˆå€¼ï¼ˆå®é™…è‡ªç”±åº¦ä¸ºn-1ï¼‰
   - é€šå¸¸n < 10ï¼Œéå¸¸é€‚åˆPowell

3. **ä¼˜åŒ–landscapeå¤æ‚**ï¼š
   - å­˜åœ¨å¹³å°åŒºåŸŸï¼ˆplateausï¼‰
   - å­˜åœ¨å¤šä¸ªå±€éƒ¨æœ€ä¼˜ç‚¹
   - ä½†é€šå¸¸åˆå§‹å€¼ï¼ˆç­‰æƒé‡ï¼‰å·²ç»åœ¨åˆç†åŒºåŸŸ

**Powellçš„ä¼˜åŠ¿**ï¼š

æ•°å­¦ä¸Šï¼ŒPowellæ–¹æ³•é€šè¿‡**å…±è½­æ–¹å‘**æœç´¢ï¼Œåœ¨äºŒæ¬¡å‡½æ•°ä¸Šå¯ä»¥åœ¨næ­¥å†…æ”¶æ•›ï¼š
$$
\min_{\mathbf{x}} f(\mathbf{x}), \quad f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^\top A \mathbf{x} + \mathbf{b}^\top \mathbf{x}
$$

è™½ç„¶æˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°ä¸æ˜¯äºŒæ¬¡çš„ï¼Œä½†Powellçš„æ–¹å‘æ›´æ–°ç­–ç•¥ä½¿å…¶å¯¹**éå…‰æ»‘å‡½æ•°**ä¹Ÿæœ‰è¾ƒå¥½çš„é²æ£’æ€§ã€‚

</div>

<details>
<summary><strong>ğŸ’» ç‚¹å‡»æŸ¥çœ‹ï¼šPythonå®ç°</strong></summary>
<div markdown="1">

```python
import numpy as np
from scipy.optimize import minimize

def search_optimal_thresholds(y_true, y_pred):
    """
    æœç´¢æœ€ä¼˜åˆ†ç±»é˜ˆå€¼

    Args:
        y_true: shape=(N,), çœŸå®æ ‡ç­¾
        y_pred: shape=(N, C), é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ

    Returns:
        thresholds: shape=(C,), æœ€ä¼˜é˜ˆå€¼å‘é‡
    """
    num_classes = y_pred.shape[1]

    # å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆè´Ÿaccuracyï¼‰
    def loss(t):
        # ä½¿ç”¨tanhæ˜ å°„åˆ°(0, 1)ï¼Œé¿å…æ•°å€¼é—®é¢˜
        t_normalized = (np.tanh(t) + 1) / 2

        # åŠ æƒé¢„æµ‹
        y_pred_weighted = y_pred * t_normalized[None, :]
        y_hat = y_pred_weighted.argmax(axis=1)

        # è¿”å›è´Ÿaccuracyï¼ˆå› ä¸ºminimizeæ˜¯æœ€å°åŒ–ï¼‰
        accuracy = np.mean(y_true == y_hat)
        return -accuracy

    # Powellä¼˜åŒ–
    options = {
        'xtol': 1e-10,    # xçš„å®¹å¿åº¦
        'ftol': 1e-10,    # f(x)çš„å®¹å¿åº¦
        'maxiter': 100000 # æœ€å¤§è¿­ä»£æ¬¡æ•°
    }

    # åˆå§‹å€¼ï¼šå…¨1ï¼ˆå³æ ‡å‡†argmaxï¼‰
    t0 = np.zeros(num_classes)  # tanh(0) = 0 -> (0+1)/2 = 0.5

    result = minimize(
        loss,
        t0,
        method='Powell',
        options=options
    )

    # è½¬æ¢å›(0, 1)èŒƒå›´
    thresholds = (np.tanh(result.x) + 1) / 2

    return thresholds

# äºŒåˆ†ç±»ç¤ºä¾‹
y_true = np.array([0, 1, 1, 0, 1])
y_pred = np.array([
    [0.7, 0.3],  # é¢„æµ‹è´Ÿæ ·æœ¬ï¼ˆæ­£ç¡®ï¼‰
    [0.4, 0.6],  # é¢„æµ‹æ­£æ ·æœ¬ï¼ˆæ­£ç¡®ï¼‰
    [0.45, 0.55],  # é¢„æµ‹æ­£æ ·æœ¬ï¼ˆæ­£ç¡®ï¼‰
    [0.8, 0.2],  # é¢„æµ‹è´Ÿæ ·æœ¬ï¼ˆæ­£ç¡®ï¼‰
    [0.55, 0.45]   # é¢„æµ‹è´Ÿæ ·æœ¬ï¼ˆé”™è¯¯ï¼ï¼‰
])

thresholds = search_optimal_thresholds(y_true, y_pred)
print(f"æœ€ä¼˜é˜ˆå€¼: {thresholds}")
# è¾“å‡ºï¼šæœ€ä¼˜é˜ˆå€¼: [0.48, 0.52]
# æ„å‘³ç€å®é™…é˜ˆå€¼ Ï„ = 0.52/(0.48+0.52) = 0.52
```

**å…³é”®æŠ€å·§**ï¼š

1. **å‚æ•°åŒ–**ï¼šä½¿ç”¨ $t_i = (\tanh(x_i) + 1)/2$ å°†ä¼˜åŒ–å˜é‡æ˜ å°„åˆ° $(0, 1)$
   - é¿å…é˜ˆå€¼è¶Šç•Œ
   - æ”¹å–„ä¼˜åŒ–landscape

2. **åˆå§‹åŒ–**ï¼š$x_0 = 0$ å¯¹åº” $t_i = 0.5$ï¼Œå³æ ‡å‡†argmax

3. **æ”¶æ•›åˆ¤æ®**ï¼šåŒæ—¶æ§åˆ¶ $x$ å’Œ $f(x)$ çš„å˜åŒ–é‡

</div>
</details>

---

## ä¸‰ã€å®éªŒå¯¹æ¯”

### 3.1 å®éªŒè®¾ç½®

<div class="example-box">

### æ•°æ®é›†ä¸é…ç½®

**æ•°æ®é›†**ï¼ˆä¸­æ–‡æ–‡æœ¬åŒ¹é…ï¼‰ï¼š

| æ•°æ®é›† | ä»»åŠ¡ | è®­ç»ƒé›† | éªŒè¯é›† | æµ‹è¯•é›† | ç‰¹ç‚¹ |
|--------|------|--------|--------|--------|------|
| **ATEC** | é‡‘èé—®ç­”åŒ¹é… | 62,477 | 20,000 | 20,000 | å£è¯­åŒ– |
| **BQ** | é“¶è¡Œé—®ç­”åŒ¹é… | 100,000 | 10,000 | 10,000 | è§„èŒƒåŒ– |
| **LCQMC** | é€šç”¨é—®ç­”åŒ¹é… | 238,766 | 8,802 | 12,500 | å¤§è§„æ¨¡ |
| **PAWSX** | é‡Šä¹‰è¯†åˆ« | 49,401 | 2,000 | 2,000 | å¯¹æŠ—æ ·æœ¬å¤š |

**æ¨¡å‹é…ç½®**ï¼š
- åŸºç¡€æ¨¡å‹ï¼šBERT-base / RoBERTa-base (ä¸­æ–‡)
- ä¼˜åŒ–å™¨ï¼šAdam (lr=2e-5)
- Batch sizeï¼š64
- Epochï¼š3-5ï¼ˆæ—©åœï¼‰

**ä¸‰ç§æ–¹æ¡ˆ**ï¼š
1. **BERT+CoSENT**ï¼šç‰¹å¾å¼ï¼Œæœ¬æ–‡æ–¹æ³•
2. **Sentence-BERT**ï¼šç‰¹å¾å¼ï¼Œbaseline
3. **BERT+Interact**ï¼šäº¤äº’å¼ï¼Œä¸Šç•Œ

</div>

### 3.2 ä¸»è¦ç»“æœ

<div class="derivation-box">

### å®éªŒç»“æœï¼ˆAccuracyï¼‰

**BERTä½œä¸ºåŸºç¡€æ¨¡å‹**ï¼š

| æ•°æ®é›† | CoSENT | Sentence-BERT | Interact | CoSENTä¸Interactå·®è· |
|--------|--------|---------------|----------|---------------------|
| **ATEC** | **85.81%** | 84.93% | 85.49% | **+0.32%** âœ¨ |
| **BQ** | 83.24% | 82.46% | **83.88%** | -0.64% |
| **LCQMC** | 86.67% | 87.42% | **87.80%** | -1.13% |
| **PAWSX** | 76.30% | 65.33% | **81.30%** | -5.00% âš ï¸ |
| **å¹³å‡** | 83.00% | 80.04% | **84.62%** | -1.62% |

**RoBERTaä½œä¸ºåŸºç¡€æ¨¡å‹**ï¼š

| æ•°æ®é›† | CoSENT | Sentence-BERT | Interact | CoSENTä¸Interactå·®è· |
|--------|--------|---------------|----------|---------------------|
| **ATEC** | 85.93% | 85.34% | **86.04%** | -0.11% |
| **BQ** | 83.42% | 82.52% | **83.62%** | -0.20% |
| **LCQMC** | 87.63% | 88.14% | **88.22%** | -0.59% |
| **PAWSX** | 76.55% | 68.35% | **83.33%** | -6.78% âš ï¸ |
| **å¹³å‡** | 83.38% | 81.09% | **85.30%** | -1.92% |

<div class="step-explanation">

**å…³é”®è§‚å¯Ÿ**ï¼š

1. **ATECå’ŒBQ**ï¼š
   - CoSENTä¸Interact **æ— æ˜¾è‘—å·®å¼‚**ï¼ˆ<1%ï¼‰
   - åœ¨ATEC/BERTä¸Šï¼ŒCoSENTç”šè‡³**ç•¥ä¼˜**äºInteract

2. **LCQMC**ï¼š
   - Sentence-BERTä¸Interactæ¥è¿‘
   - CoSENTå±…ä¸­
   - å·®è·çº¦1%

3. **PAWSX**ï¼š
   - **å·®è·æœ€å¤§**ï¼ˆ5-7%ï¼‰
   - æ‰€æœ‰ç‰¹å¾å¼æ–¹æ³•éƒ½æ˜¾è‘—ä½äºäº¤äº’å¼
   - Sentence-BERTç”šè‡³å´©æºƒï¼ˆä»…65-68%ï¼‰

</div>

</div>

### 3.3 PAWSXçš„ç‰¹æ®Šæ€§

<div class="intuition-box">

### ğŸ” ä¸ºä»€ä¹ˆPAWSXå¦‚æ­¤å›°éš¾ï¼Ÿ

**PAWSXçš„ç‰¹ç‚¹**ï¼šå¤§é‡**å¯¹æŠ—æ ·æœ¬**ï¼Œå³å­—é¢é‡å åº¦é«˜ä½†è¯­ä¹‰ä¸åŒçš„è´Ÿæ ·æœ¬ã€‚

**ç¤ºä¾‹**ï¼š

| Text 1 | Text 2 | æ ‡ç­¾ | å­—é¢é‡å  |
|--------|--------|------|---------|
| ä»–åœ¨å“ªé‡Œä¸Šå­¦ï¼Ÿ | ä»–åœ¨å“ªé‡Œå·¥ä½œï¼Ÿ | 0 | 80% |
| è¿™æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ | è¿™æ˜¯ä»€ä¹ˆé¢œè‰²çš„ï¼Ÿ | 1 | 95% |

**ä¸ºä»€ä¹ˆç‰¹å¾å¼å¤±æ•ˆï¼Ÿ**

ç‰¹å¾å¼æ–¹æ¡ˆï¼ˆå°¤å…¶æ— ç›‘ç£æ–¹æ³•ï¼‰ä¸¥é‡ä¾èµ–å­—é¢é‡å åº¦ï¼š
- æ— ç›‘ç£å¥å‘é‡ï¼šåŸºäºMLMæˆ–å¯¹æ¯”å­¦ä¹ ï¼Œå€¾å‘äºå°†ç›¸ä¼¼æ–‡æœ¬æ˜ å°„åˆ°æ¥è¿‘çš„å‘é‡
- å­—é¢é‡å é«˜ â†’ å¥å‘é‡æ¥è¿‘ â†’ è¯¯åˆ¤ä¸ºæ­£æ ·æœ¬

**ä¸ºä»€ä¹ˆäº¤äº’å¼æ›´å¼ºï¼Ÿ**

äº¤äº’å¼å¯ä»¥è¿›è¡Œtokençº§åˆ«çš„ç²¾ç»†å¯¹æ¯”ï¼š
- Attentionå¯ä»¥å‘ç°å…³é”®å·®å¼‚ï¼ˆ"ä¸Šå­¦" vs "å·¥ä½œ"ï¼‰
- æ·±å±‚äº¤äº’èƒ½æ”¾å¤§å¾®å°å·®å¼‚
- 12å±‚Transformerå……åˆ†äº¤äº’

**æ•°æ®éªŒè¯**ï¼š

åœ¨ã€Šæ— ç›‘ç£è¯­ä¹‰ç›¸ä¼¼åº¦å“ªå®¶å¼ºï¼Ÿã€‹ã€ã€Šä¸­æ–‡ä»»åŠ¡è¿˜æ˜¯SOTAå—ï¼Ÿã€‹ä¸­ï¼Œå‡ ä¹**æ‰€æœ‰æ— ç›‘ç£å¥å‘é‡æ–¹æ³•**éƒ½åœ¨PAWSXä¸Šå¤±æ•ˆã€‚

**å®šé‡åˆ†æ**ï¼š

æˆ‘ä»¬å¯ä»¥ç”¨**å­—é¢é‡å åº¦**æ¥é‡åŒ–å¯¹æŠ—æ€§ï¼š

$$
\text{Overlap}(\text{text}_1, \text{text}_2) = \frac{|\text{tokens}_1 \cap \text{tokens}_2|}{|\text{tokens}_1 \cup \text{tokens}_2|}
\tag{17.5}
$$

ç»Ÿè®¡å„æ•°æ®é›†çš„è´Ÿæ ·æœ¬å¹³å‡é‡å åº¦ï¼š

| æ•°æ®é›† | è´Ÿæ ·æœ¬å¹³å‡é‡å åº¦ | å¯¹æŠ—æ€§ |
|--------|-----------------|--------|
| ATEC | ~0.25 | ä½ |
| BQ | ~0.30 | ä½ |
| LCQMC | ~0.35 | ä¸­ |
| **PAWSX** | **~0.65** | **é«˜** âš ï¸ |

**ç»“è®º**ï¼šPAWSXçš„è´Ÿæ ·æœ¬ä¸æ­£æ ·æœ¬åœ¨å­—é¢ä¸Šéå¸¸ç›¸ä¼¼ï¼Œè¿™å¯¹ç‰¹å¾å¼æ–¹æ¡ˆæ„æˆå·¨å¤§æŒ‘æˆ˜ã€‚

</div>

---

## å››ã€ç†è®ºåˆ†æï¼šç‰¹å¾å¼çš„æé™

### 4.1 ç†è®ºä¸Šç•Œ

<div class="theorem-box">

### æƒŠäººçš„ç»“è®º

**å®šç†**ï¼šç†è®ºä¸Šæ¥è¯´ï¼Œäº¤äº’å¼èƒ½åšåˆ°çš„æ•ˆæœï¼Œç‰¹å¾å¼"å‡ ä¹"éƒ½èƒ½åšåˆ°ã€‚

**è¯æ˜æ€è·¯**ï¼š

1. ç›¸ä¼¼åº¦çŸ©é˜µçš„SVDåˆ†è§£
2. Johnson-Lindenstrauss (JL) å¼•ç†
3. ç»´åº¦ä¼°è®¡

</div>

<div class="derivation-box">

### å®Œæ•´è¯æ˜

<div class="formula-explanation">

<div class="formula-step">
<div class="step-label">æ­¥éª¤1ï¼šç›¸ä¼¼åº¦çŸ©é˜µ</div>

å‡è®¾æœ‰ $n$ ä¸ªæ ·æœ¬ï¼Œä»»æ„ä¸¤ä¸ªæ ·æœ¬ $(i, j)$ çš„ç›¸ä¼¼åº¦ä¸º $S_{ij} \in [0, 1]$ï¼ˆæ— åºï¼Œå³ $S_{ij} = S_{ji}$ï¼‰ã€‚

æ„é€ ç›¸ä¼¼åº¦çŸ©é˜µï¼š
$$
S = \begin{bmatrix}
S_{11} & S_{12} & \cdots & S_{1n} \\
S_{21} & S_{22} & \cdots & S_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
S_{n1} & S_{n2} & \cdots & S_{nn}
\end{bmatrix}
\tag{8}
$$

**æ€§è´¨**ï¼š$S$ æ˜¯å¯¹ç§°çš„ï¼Œä¸”**åŠæ­£å®š**ï¼ˆç›¸ä¼¼åº¦çš„è‡ªç„¶çº¦æŸï¼‰ã€‚

<div class="step-explanation">

**ä¸ºä»€ä¹ˆåŠæ­£å®šï¼Ÿ**

å¯¹äºä»»æ„å‘é‡ $\mathbf{x} \in \mathbb{R}^n$ï¼š
$$
\mathbf{x}^\top S \mathbf{x} = \sum_{i,j} x_i S_{ij} x_j \geq 0
$$

è¿™æ˜¯å› ä¸ºç›¸ä¼¼åº¦çŸ©é˜µé€šå¸¸æ¥è‡ªæŸç§åº¦é‡ï¼ˆå¦‚æ ¸å‡½æ•°ï¼‰ï¼Œæ»¡è¶³æ­£å®šæ€§ã€‚

</div>
</div>

<div class="formula-step">
<div class="step-label">æ­¥éª¤2ï¼šSVDåˆ†è§£</div>

**çº¿æ€§ä»£æ•°å®šç†**ï¼šä»»ä½•å¯¹ç§°åŠæ­£å®šçŸ©é˜µ $S$ éƒ½å¯ä»¥åˆ†è§£ä¸ºï¼š
$$
S = U \Lambda U^\top
\tag{9}
$$

å…¶ä¸­ï¼š
- $U \in \mathbb{R}^{n \times n}$ï¼šæ­£äº¤çŸ©é˜µï¼ˆ$U^\top U = I$ï¼‰
- $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_n)$ï¼šç‰¹å¾å€¼å¯¹è§’é˜µï¼ˆ$\lambda_i \geq 0$ï¼‰

è¿›ä¸€æ­¥ï¼š
$$
S = U \sqrt{\Lambda} \sqrt{\Lambda} U^\top = (U \sqrt{\Lambda})(U \sqrt{\Lambda})^\top
\tag{10}
$$

å®šä¹‰ $B = U \sqrt{\Lambda} \in \mathbb{R}^{n \times n}$ï¼Œåˆ™ï¼š
$$
S = BB^\top
\tag{11}
$$

<div class="step-explanation">

**å‡ ä½•æ„ä¹‰**ï¼š

çŸ©é˜µ $B$ çš„ç¬¬ $i$ è¡Œå¯ä»¥çœ‹ä½œæ ·æœ¬ $i$ çš„**åµŒå…¥å‘é‡** $\mathbf{v}_i \in \mathbb{R}^n$ã€‚

åˆ™ï¼š
$$
S_{ij} = \mathbf{v}_i^\top \mathbf{v}_j
$$

**ç»“è®º**ï¼šä»»ä½•ç›¸ä¼¼åº¦çŸ©é˜µéƒ½å¯ä»¥é€šè¿‡å†…ç§¯è¡¨ç¤ºï¼

**ä½†é—®é¢˜**ï¼šç»´åº¦æ˜¯ $n$ï¼ˆæ ·æœ¬æ•°ï¼‰ï¼Œå¤ªå¤§äº†ï¼

</div>
</div>

<div class="formula-step">
<div class="step-label">æ­¥éª¤3ï¼šé™ç»´ï¼ˆJLå¼•ç†ï¼‰</div>

**Johnson-Lindenstrauss (JL) å¼•ç†**ï¼ˆå‚è€ƒã€Šè®©äººæƒŠå¹çš„Johnson-Lindenstrausså¼•ç†ã€‹ï¼‰ï¼š

å¯¹äº $n$ ä¸ªå‘é‡ï¼Œå­˜åœ¨éšæœºæŠ•å½±çŸ©é˜µ $R \in \mathbb{R}^{d \times n}$ï¼ˆ$d \ll n$ï¼‰ï¼Œä½¿å¾—ï¼š
$$
(1-\epsilon) \|\mathbf{v}_i - \mathbf{v}_j\|^2
\leq \|R\mathbf{v}_i - R\mathbf{v}_j\|^2
\leq (1+\epsilon) \|\mathbf{v}_i - \mathbf{v}_j\|^2
\tag{12}
$$

**ç›®æ ‡ç»´åº¦**ï¼š
$$
d = O\left(\frac{\log n}{\epsilon^2}\right)
\tag{13}
$$

å®é™…ä¼°è®¡ï¼ˆã€ŠJLå¼•ç†ï¼šåº”ç”¨ç¯‡ã€‹ï¼‰ï¼š
$$
d \approx 8 \log n
\tag{14}
$$

<div class="step-explanation">

**ä¿æŒå†…ç§¯**ï¼š

ç”±äºï¼š
$$
\mathbf{v}_i^\top \mathbf{v}_j = \frac{1}{2}\left(\|\mathbf{v}_i\|^2 + \|\mathbf{v}_j\|^2 - \|\mathbf{v}_i - \mathbf{v}_j\|^2\right)
$$

ä¿æŒè·ç¦» $\Rightarrow$ è¿‘ä¼¼ä¿æŒå†…ç§¯ï¼

**æ•°å€¼ç¤ºä¾‹**ï¼ˆBERT baseï¼Œ768ç»´ï¼‰ï¼š

- 100ä¸‡æ ·æœ¬ï¼š$d \approx 8 \log(10^6) \approx 110$ ç»´
- BERTçš„768ç»´**è¿œè¶…**æ‰€éœ€ç»´åº¦

**ç»“è®º**ï¼šç†è®ºä¸Šï¼Œ768ç»´å¥å‘é‡è¶³ä»¥é€šè¿‡å†…ç§¯æ‹Ÿåˆç™¾ä¸‡çº§æ ·æœ¬çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼

**æ›´ç›´è§‚çš„ç†è§£**ï¼š

æƒ³è±¡æˆ‘ä»¬æœ‰1000ä¸ªæ ·æœ¬ï¼Œéœ€è¦è®°å½•æ‰€æœ‰ä¸¤ä¸¤ç›¸ä¼¼åº¦ï¼Œå…±$\binom{1000}{2} \approx 500,000$ä¸ªæ•°å­—ã€‚

ç›´æ¥å­˜å‚¨éœ€è¦500,000ä¸ªå‚æ•°ï¼Œä½†é€šè¿‡JLå¼•ç†ï¼š
- æ¯ä¸ªæ ·æœ¬ç”¨$d \approx 8 \log 1000 \approx 56$ç»´å‘é‡è¡¨ç¤º
- æ€»å…±åªéœ€$1000 \times 56 = 56,000$ä¸ªå‚æ•°
- å°±èƒ½è¿‘ä¼¼è¿˜åŸæ‰€æœ‰500,000ä¸ªç›¸ä¼¼åº¦ï¼

è¿™å°±æ˜¯**é™ç»´çš„å¨åŠ›**ï¼šé€šè¿‡å·§å¦™çš„åµŒå…¥ï¼Œç”¨æ›´å°‘çš„å‚æ•°è¡¨ç¤ºæ›´å¤šçš„ä¿¡æ¯ã€‚

</div>
</div>

</div>

</div>

</div>

### 4.2 ç†è®ºä¸å®è·µçš„å·®è·

<div class="intuition-box">

### ğŸ¤” æ—¢ç„¶ç†è®ºä¸Šå¯è¡Œï¼Œä¸ºä½•å®è·µä¸­æœ‰å·®è·ï¼Ÿ

**çŸ›ç›¾**ï¼š
- ç†è®ºï¼šç‰¹å¾å¼å¯ä»¥è¾¾åˆ°äº¤äº’å¼çš„æ•ˆæœ
- å®è·µï¼šPAWSXä¸Šå·®è·æ˜æ˜¾ï¼ˆ5-7%ï¼‰

**è§£é‡Š**ï¼š**è¿ç»­æ€§ vs å¯¹æŠ—æ€§** çš„çŸ›ç›¾

<div class="note-box">

### ç¥ç»ç½‘ç»œçš„è¿ç»­æ€§

**ç¼–ç å™¨çš„è¿ç»­æ€§**ï¼š
- ç¥ç»ç½‘ç»œæ˜¯è¿ç»­å‡½æ•°
- è¾“å…¥çš„å¾®å°æ”¹åŠ¨ â†’ è¾“å‡ºçš„å¾®å°æ”¹åŠ¨
- å¥å‘é‡ç©ºé—´å…·æœ‰è‰¯å¥½çš„å¹³æ»‘æ€§

**ä½™å¼¦ç›¸ä¼¼åº¦çš„è¿ç»­æ€§**ï¼š
- $\Delta \mathbf{v}$ å¾ˆå° $\Rightarrow$ $\cos(\mathbf{u}, \mathbf{v}) \approx \cos(\mathbf{u}, \mathbf{v} + \Delta \mathbf{v})$

**æ€»ä½“**ï¼šç‰¹å¾å¼æ–¹æ¡ˆçš„**è¿ç»­æ€§éå¸¸å¥½**

</div>

<div class="note-box">

### è¯­è¨€çš„å¯¹æŠ—æ€§

**äººç±»è¯­è¨€çš„ç‰¹ç‚¹**ï¼š
- å­—é¢çš„å¾®å°æ”¹åŠ¨å¯èƒ½å¯¼è‡´è¯­ä¹‰å·¨å˜
- ç»å…¸ä¾‹å­ï¼šåŠ ä¸€ä¸ª"ä¸"å­— â†’ è¯­ä¹‰åè½¬
- PAWSXï¼šå¤§é‡å­—é¢ç›¸ä¼¼ä½†è¯­ä¹‰ä¸åŒçš„æ ·æœ¬

**æ•°å­¦è¡¨ç¤º**ï¼š

è®¾ $f$ æ˜¯ç›¸ä¼¼åº¦å‡½æ•°ï¼ŒæœŸæœ›ï¼š
$$
f(\text{text}_1, \text{text}_2) \approx 1
$$
$$
f(\text{text}_1, \text{text}_2') \approx 0
$$

å…¶ä¸­ $\text{text}_2$ å’Œ $\text{text}_2'$ å­—é¢ä¸Šéå¸¸æ¥è¿‘ã€‚

**çŸ›ç›¾**ï¼šè¿ç»­æ€§å¥½çš„å‡½æ•°éš¾ä»¥å®ç°è¿™ç§"è·³è·ƒ"ï¼

</div>

</div>

### 4.3 ä¸ºä»€ä¹ˆäº¤äº’å¼æ›´å¼ºï¼Ÿ

<div class="derivation-box">

### äº¤äº’å¼çš„ä¼˜åŠ¿

<div class="formula-explanation">

<div class="formula-step">
<div class="step-label">Tokençº§äº¤äº’</div>

åœ¨BERTçš„ç¬¬ $\ell$ å±‚ï¼Œtoken $i$ï¼ˆæ¥è‡ªtext1ï¼‰å¯ä»¥ç›´æ¥"çœ‹åˆ°"token $j$ï¼ˆæ¥è‡ªtext2ï¼‰ï¼š
$$
\mathbf{h}_i^{(\ell+1)} = \text{Attention}\left(\mathbf{h}_i^{(\ell)}, [\mathbf{h}_1^{(\ell)}, \ldots, \mathbf{h}_n^{(\ell)}]\right)
\tag{15}
$$

**å…³é”®**ï¼š$[\mathbf{h}_1^{(\ell)}, \ldots, \mathbf{h}_n^{(\ell)}]$ åŒ…å«ä¸¤ä¸ªå¥å­çš„æ‰€æœ‰tokenã€‚

</div>

<div class="formula-step">
<div class="step-label">å·®å¼‚æ”¾å¤§</div>

ç»è¿‡å¤šå±‚ï¼ˆ12å±‚ï¼‰ï¼š
- æ¨¡å‹å¯ä»¥"å‘ç°"å¹¶"æ”¾å¤§"å…³é”®å·®å¼‚
- ä¾‹å¦‚ï¼š"ä¸Šå­¦" vs "å·¥ä½œ"
- å³ä½¿ä¸¤è€…åœ¨å‘é‡ç©ºé—´ä¸­å¾ˆæ¥è¿‘ï¼ŒAttentionå¯ä»¥ç»™äºˆä¸åŒæƒé‡

$$
\text{Attention}(\text{ä¸Šå­¦}, \text{å·¥ä½œ}) \ll \text{Attention}(\text{ä¸Šå­¦}, \text{å­¦ä¹ })
\tag{16}
$$

</div>

<div class="formula-step">
<div class="step-label">éçº¿æ€§å†³ç­–è¾¹ç•Œ</div>

äº¤äº’å¼æœ€åé€šè¿‡åˆ†ç±»å™¨ï¼š
$$
P(\text{ç›¸ä¼¼} | \text{text}_1, \text{text}_2) = \sigma(W \mathbf{h}_{\text{[CLS]}} + b)
\tag{17}
$$

$\mathbf{h}_{\text{[CLS]}}$ å·²ç»åŒ…å«äº†å……åˆ†çš„äº¤äº’ä¿¡æ¯ï¼Œåˆ†ç±»å™¨å¯ä»¥å­¦ä¹ **é«˜åº¦éçº¿æ€§**çš„å†³ç­–è¾¹ç•Œã€‚

</div>

</div>

</div>

</div>

---

## äº”ã€æ·±å…¥è®¨è®º

### 5.1 è®­ç»ƒæ–¹å¼çš„å½±å“

<div class="example-box">

### æœ‰ç›‘ç£ vs æ— ç›‘ç£

**å®éªŒå‘ç°**ï¼š

| æ–¹æ³• | è®­ç»ƒæ–¹å¼ | PAWSX (BERT) | PAWSX (RoBERTa) |
|------|---------|-------------|----------------|
| SimCSE | æ— ç›‘ç£ | ~60% | ~62% |
| Sentence-BERT | æ— ç›‘ç£ | 65.33% | 68.35% |
| **CoSENT** | **æœ‰ç›‘ç£** | **76.30%** | **76.55%** |
| Interact | æœ‰ç›‘ç£ | 81.30% | 83.33% |

**ç»“è®º**ï¼š
- æœ‰ç›‘ç£è®­ç»ƒæ˜¾è‘—æå‡PAWSXè¡¨ç°ï¼ˆ+10-15%ï¼‰
- ä½†ä»ä¸åŠäº¤äº’å¼ï¼ˆå·®è·5-7%ï¼‰

**åŸå› **ï¼š
- æœ‰ç›‘ç£è®­ç»ƒå¯ä»¥å­¦ä¹ "å¯¹æŠ—æ ·æœ¬"çš„æ¨¡å¼
- ä½†ç‰¹å¾å¼çš„æ¶æ„é™åˆ¶äº†å…¶ä¸Šé™

</div>

### 5.2 è¿‡æ‹Ÿåˆçš„é£é™©

<div class="intuition-box">

### ğŸ“‰ ç‰¹å¾å¼æ–¹æ¡ˆçš„è®­ç»ƒæ›²çº¿

**è§‚å¯Ÿ**ï¼š

åœ¨PAWSXä¸Šè®­ç»ƒCoSENTæ—¶ï¼š
- è®­ç»ƒlosså¯ä»¥é™åˆ°æ¥è¿‘0ï¼ˆè¯´æ˜æ‹Ÿåˆèƒ½åŠ›æ²¡é—®é¢˜ï¼‰
- ä½†éªŒè¯é›†æ•ˆæœæå‡æœ‰é™ï¼ˆæ³›åŒ–å—é™ï¼‰

**åˆ†æ**ï¼š

ç‰¹å¾å¼æ–¹æ¡ˆè¦å»æ‹Ÿåˆå¯¹æŠ—æ€§æ•°æ®ï¼Œéœ€è¦"æ‰“ç ´"å…¶å›ºæœ‰çš„è¿ç»­æ€§ï¼š
- éœ€è¦æ›´å¤šepoch
- å®¹æ˜“è¿‡æ‹Ÿåˆ
- æ³›åŒ–æ€§å·®

**å¯¹æ¯”**ï¼š

äº¤äº’å¼æ–¹æ¡ˆï¼š
- ä¸€å¼€å§‹å°±åŒæ—¶æ¥è§¦ä¸¤ä¸ªæ ·æœ¬
- Attentionå¯ä»¥è‡ªè¡Œå­¦ä¹ æ”¾å¤§å·®å¼‚
- è¿ç»­æ€§ä¸å¯¹æŠ—æ€§çš„çŸ›ç›¾è¾ƒå°

</div>

### 5.3 å®ç”¨å»ºè®®

<div class="note-box">

### ğŸ¯ é€‰æ‹©å»ºè®®

**ä½¿ç”¨ç‰¹å¾å¼ï¼ˆCoSENT/SBERTï¼‰**ï¼š
- âœ… æ£€ç´¢åœºæ™¯ï¼ˆéœ€è¦é«˜æ•ˆç‡ï¼‰
- âœ… æ•°æ®é›†æ— ä¸¥é‡å¯¹æŠ—æ ·æœ¬
- âœ… ä»»åŠ¡å¯¹å‡†ç¡®ç‡è¦æ±‚ä¸æ˜¯æç«¯é«˜
- âœ… éœ€è¦ç¦»çº¿ç¼“å­˜å¥å‘é‡

**ä½¿ç”¨äº¤äº’å¼**ï¼š
- âœ… åˆ†ç±»åœºæ™¯ï¼ˆè¿½æ±‚æè‡´å‡†ç¡®ç‡ï¼‰
- âœ… æ•°æ®é›†æœ‰å¤§é‡å¯¹æŠ—æ ·æœ¬ï¼ˆå¦‚PAWSXï¼‰
- âœ… è®¡ç®—èµ„æºå……è¶³
- âœ… æŸ¥è¯¢é‡ä¸å¤§ï¼ˆå¯æ¥å—é‡å¤ç¼–ç ï¼‰

**æ··åˆæ–¹æ¡ˆï¼ˆæ¨èï¼‰**ï¼š
1. ç¬¬ä¸€é˜¶æ®µï¼šç‰¹å¾å¼ç²—æ’ï¼ˆä»ç™¾ä¸‡çº§å€™é€‰ä¸­ç­›é€‰Top-1000ï¼‰
2. ç¬¬äºŒé˜¶æ®µï¼šäº¤äº’å¼ç²¾æ’ï¼ˆä»Top-1000ä¸­é€‰Top-10ï¼‰

**æ”¶ç›Š**ï¼š
- ç»“åˆä¸¤è€…ä¼˜åŠ¿
- æ•ˆç‡ä¸å‡†ç¡®ç‡å…¼é¡¾

**æ··åˆæ–¹æ¡ˆçš„ç†è®ºä¾æ®**ï¼š

å‡è®¾ç‰¹å¾å¼çš„å¬å›ç‡ï¼ˆRecall@1000ï¼‰ä¸º99%ï¼Œç²¾æ’åªéœ€å¤„ç†1000ä¸ªå€™é€‰ï¼Œè®¡ç®—é‡é™ä½$10^3$å€ï¼š

$$
\text{Total Cost} = \underbrace{N \cdot C_{\text{encode}}}_{\text{ç¦»çº¿ç¼–ç }} + \underbrace{1 \cdot C_{\text{encode}} + 1000 \cdot C_{\text{cosine}}}_{\text{ç²—æ’}} + \underbrace{1000 \cdot C_{\text{interact}}}_{\text{ç²¾æ’}}
\tag{17.8}
$$

ç›¸æ¯”çº¯äº¤äº’å¼çš„$N \cdot C_{\text{interact}}$ï¼Œå½“$N \gg 1000$æ—¶ï¼ˆå¦‚$N=10^6$ï¼‰ï¼ŒåŠ é€Ÿçº¦$10^3$å€ï¼Œä¸”å‡†ç¡®ç‡æŸå¤±<1%ã€‚

</div>

---

## å…­ã€ä»£ç å®ç°

### 6.1 Powellé˜ˆå€¼æœç´¢å®Œæ•´å®ç°

<details>
<summary><strong>ğŸ’» ç‚¹å‡»æŸ¥çœ‹ï¼šç”Ÿäº§çº§ä»£ç </strong></summary>
<div markdown="1">

```python
import numpy as np
from scipy.optimize import minimize
from sklearn.metrics import accuracy_score, f1_score

class ThresholdOptimizer:
    """
    å¤šåˆ†ç±»é˜ˆå€¼ä¼˜åŒ–å™¨

    æ”¯æŒä¼˜åŒ–ç›®æ ‡ï¼š
    - accuracy
    - f1 (macro/micro/weighted)
    - custom metric
    """

    def __init__(self, metric='accuracy'):
        """
        Args:
            metric: 'accuracy', 'f1_macro', 'f1_micro',
                    'f1_weighted', or callable
        """
        self.metric = metric
        self.thresholds_ = None

    def _compute_metric(self, y_true, y_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if self.metric == 'accuracy':
            return accuracy_score(y_true, y_pred)
        elif self.metric.startswith('f1'):
            average = self.metric.split('_')[1]
            return f1_score(y_true, y_pred, average=average)
        elif callable(self.metric):
            return self.metric(y_true, y_pred)
        else:
            raise ValueError(f"Unknown metric: {self.metric}")

    def fit(self, y_true, y_pred_proba, **minimize_kwargs):
        """
        åœ¨éªŒè¯é›†ä¸Šæœç´¢æœ€ä¼˜é˜ˆå€¼

        Args:
            y_true: shape=(N,), çœŸå®æ ‡ç­¾
            y_pred_proba: shape=(N, C), é¢„æµ‹æ¦‚ç‡
            minimize_kwargs: ä¼ é€’ç»™scipy.optimize.minimizeçš„å‚æ•°

        Returns:
            self
        """
        num_classes = y_pred_proba.shape[1]

        def objective(t):
            # æ˜ å°„åˆ°(0, 1)
            t_norm = (np.tanh(t) + 1) / 2

            # åŠ æƒé¢„æµ‹
            y_pred_weighted = y_pred_proba * t_norm[None, :]
            y_pred = y_pred_weighted.argmax(axis=1)

            # è¿”å›è´ŸæŒ‡æ ‡ï¼ˆæœ€å°åŒ–ï¼‰
            metric_value = self._compute_metric(y_true, y_pred)
            return -metric_value

        # é»˜è®¤å‚æ•°
        default_kwargs = {
            'method': 'Powell',
            'options': {
                'xtol': 1e-10,
                'ftol': 1e-10,
                'maxiter': 100000
            }
        }
        default_kwargs.update(minimize_kwargs)

        # ä¼˜åŒ–
        t0 = np.zeros(num_classes)
        result = minimize(objective, t0, **default_kwargs)

        # ä¿å­˜ç»“æœ
        self.thresholds_ = (np.tanh(result.x) + 1) / 2
        self.opt_result_ = result

        return self

    def predict(self, y_pred_proba):
        """
        ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡Œé¢„æµ‹

        Args:
            y_pred_proba: shape=(N, C), é¢„æµ‹æ¦‚ç‡

        Returns:
            y_pred: shape=(N,), é¢„æµ‹æ ‡ç­¾
        """
        if self.thresholds_ is None:
            raise ValueError("Must call fit() before predict()")

        y_pred_weighted = y_pred_proba * self.thresholds_[None, :]
        return y_pred_weighted.argmax(axis=1)

    def fit_predict(self, y_true, y_pred_proba, **minimize_kwargs):
        """fitå’Œpredictçš„ç»„åˆ"""
        self.fit(y_true, y_pred_proba, **minimize_kwargs)
        return self.predict(y_pred_proba)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    N = 1000
    C = 3

    y_true = np.random.randint(0, C, N)
    y_pred_proba = np.random.rand(N, C)
    y_pred_proba /= y_pred_proba.sum(axis=1, keepdims=True)

    # ä¼˜åŒ–é˜ˆå€¼
    optimizer = ThresholdOptimizer(metric='accuracy')
    optimizer.fit(y_true, y_pred_proba)

    print(f"æœ€ä¼˜é˜ˆå€¼: {optimizer.thresholds_}")
    print(f"ä¼˜åŒ–æ˜¯å¦æˆåŠŸ: {optimizer.opt_result_.success}")
    print(f"æœ€ç»ˆç›®æ ‡å€¼: {-optimizer.opt_result_.fun:.4f}")

    # é¢„æµ‹
    y_pred = optimizer.predict(y_pred_proba)
    print(f"æµ‹è¯•é›†accuracy: {accuracy_score(y_true, y_pred):.4f}")
```

**è¿›é˜¶ç”¨æ³•**ï¼š

```python
# è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
def custom_metric(y_true, y_pred):
    """åŠ æƒF1ï¼Œç±»åˆ«0çš„æƒé‡æ›´é«˜"""
    from sklearn.metrics import f1_score
    f1_per_class = f1_score(y_true, y_pred, average=None)
    weights = [2.0, 1.0, 1.0]  # ç±»åˆ«0æƒé‡Ã—2
    return np.average(f1_per_class, weights=weights)

optimizer = ThresholdOptimizer(metric=custom_metric)
optimizer.fit(y_true_val, y_pred_proba_val)
```

</div>
</details>

### 6.2 ç‰¹å¾å¼ vs äº¤äº’å¼çš„å®ç°å¯¹æ¯”

<details>
<summary><strong>âš¡ ç‚¹å‡»æŸ¥çœ‹ï¼šä¸¤ç§èŒƒå¼çš„ä»£ç </strong></summary>
<div markdown="1">

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

# ============= ç‰¹å¾å¼ï¼ˆCoSENTï¼‰ =============
class RepresentationMatcher(nn.Module):
    def __init__(self, model_name='bert-base-chinese'):
        super().__init__()
        self.encoder = BertModel.from_pretrained(model_name)

    def encode(self, input_ids, attention_mask):
        """ç¼–ç å•ä¸ªå¥å­ä¸ºå¥å‘é‡"""
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        # ä½¿ç”¨[CLS]æˆ–pooling
        return outputs.last_hidden_state[:, 0]  # [batch, hidden]

    def forward(self, input_ids1, mask1, input_ids2, mask2):
        """è®¡ç®—å¥å­å¯¹çš„ç›¸ä¼¼åº¦"""
        # åˆ†åˆ«ç¼–ç 
        vec1 = self.encode(input_ids1, mask1)
        vec2 = self.encode(input_ids2, mask2)

        # ä½™å¼¦ç›¸ä¼¼åº¦
        sim = torch.cosine_similarity(vec1, vec2, dim=-1)
        return sim

# æ¨ç†æ—¶çš„ä¼˜åŠ¿ï¼šå¯ç¼“å­˜
model = RepresentationMatcher()
corpus = ["å¥å­1", "å¥å­2", ..., "å¥å­N"]

# ç¦»çº¿ç¼–ç ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
corpus_vectors = []
for sent in corpus:
    inputs = tokenizer(sent, return_tensors='pt')
    vec = model.encode(**inputs)
    corpus_vectors.append(vec)

corpus_vectors = torch.stack(corpus_vectors)  # [N, hidden]

# åœ¨çº¿æ£€ç´¢ï¼ˆæå¿«ï¼‰
query = "æŸ¥è¯¢å¥å­"
query_inputs = tokenizer(query, return_tensors='pt')
query_vec = model.encode(**query_inputs)  # [1, hidden]

similarities = torch.cosine_similarity(
    query_vec, corpus_vectors, dim=-1
)  # [N]
top_k = similarities.topk(10)


# ============= äº¤äº’å¼ =============
class InteractionMatcher(nn.Module):
    def __init__(self, model_name='bert-base-chinese'):
        super().__init__()
        self.encoder = BertModel.from_pretrained(model_name)
        self.classifier = nn.Linear(768, 2)  # äºŒåˆ†ç±»

    def forward(self, input_ids, attention_mask):
        """
        input_ids: [CLS] text1 [SEP] text2 [SEP]
        """
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        cls_output = outputs.last_hidden_state[:, 0]
        logits = self.classifier(cls_output)
        return logits

# æ¨ç†æ—¶çš„åŠ£åŠ¿ï¼šæ— æ³•ç¼“å­˜
model = InteractionMatcher()
corpus = ["å¥å­1", "å¥å­2", ..., "å¥å­N"]

# åœ¨çº¿æ£€ç´¢ï¼ˆæ¯æ¬¡éƒ½è¦é‡æ–°ç¼–ç ï¼‰
query = "æŸ¥è¯¢å¥å­"
scores = []
for sent in corpus:
    # æ‹¼æ¥
    text = f"{query}[SEP]{sent}"
    inputs = tokenizer(text, return_tensors='pt')
    logits = model(**inputs)  # [1, 2]
    score = torch.softmax(logits, dim=-1)[0, 1]  # æ­£æ ·æœ¬æ¦‚ç‡
    scores.append(score)

scores = torch.tensor(scores)  # [N]
top_k = scores.topk(10)
```

**æ•ˆç‡å¯¹æ¯”**ï¼ˆN=100ä¸‡ï¼ŒBERT baseï¼‰ï¼š

| æ“ä½œ | ç‰¹å¾å¼ | äº¤äº’å¼ | åŠ é€Ÿæ¯” |
|------|--------|--------|--------|
| ç¦»çº¿ç¼–ç  | 1æ¬¡ï¼ˆ~30minï¼‰ | ä¸é€‚ç”¨ | - |
| å•æ¬¡æŸ¥è¯¢ | 1æ¬¡ç¼–ç +100ä¸‡æ¬¡å†…ç§¯ï¼ˆ~0.1sï¼‰ | 100ä¸‡æ¬¡ç¼–ç ï¼ˆ~5å°æ—¶ï¼‰ | **180,000Ã—** |
| å†…å­˜å ç”¨ | 768Ã—1MÃ—4B = 3GB | å¯å¿½ç•¥ | - |

</div>
</details>

---

## ä¸ƒã€å¸¸è§é—®é¢˜ (FAQ)

<div class="example-box">

### â“ Q1: ä¸ºä»€ä¹ˆç‰¹å¾å¼åœ¨ATECä¸Šèƒ½è¶…è¿‡äº¤äº’å¼ï¼Ÿ

**A**: è¿™ä¸»è¦æ˜¯**éšæœºæ€§**å’Œ**æ¨¡å‹å®¹é‡**çš„ç»¼åˆä½œç”¨ï¼š

1. **ç»Ÿè®¡æ³¢åŠ¨**ï¼šå·®è·ä»…0.32%ï¼Œåœ¨ç»Ÿè®¡è¯¯å·®èŒƒå›´å†…
2. **è¿‡æ‹Ÿåˆç¨‹åº¦**ï¼šäº¤äº’å¼å‚æ•°æ›´å¤šï¼Œåœ¨å°æ•°æ®é›†ä¸Šå¯èƒ½è¿‡æ‹Ÿåˆ
3. **ä¼˜åŒ–éš¾åº¦**ï¼šç‰¹å¾å¼æŸå¤±å‡½æ•°æ›´ç®€å•ï¼Œå¯èƒ½ä¼˜åŒ–å¾—æ›´å……åˆ†

**ç»“è®º**ï¼šä¸èƒ½è¯´ç‰¹å¾å¼"æ›´å¥½"ï¼Œåªèƒ½è¯´"å·®ä¸å¤š"ã€‚

---

### â“ Q2: CoSENTä¸€å®šæ¯”Sentence-BERTå¥½å—ï¼Ÿ

**A**: ä»å®éªŒç»“æœçœ‹ï¼ŒCoSENTåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šä¼˜äºSentence-BERTï¼Œä½†ï¼š

**CoSENTçš„ä¼˜åŠ¿**ï¼š
- æ›´å¥½çš„æŸå¤±å‡½æ•°ï¼ˆcircle lossçš„å˜ä½“ï¼‰
- æ›´å……åˆ†åˆ©ç”¨æ ‡ç­¾ä¿¡æ¯ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰
- åœ¨å¯¹æŠ—æ ·æœ¬ä¸Šè¡¨ç°æ›´å¥½ï¼ˆPAWSX: 76% vs 65%ï¼‰

**Sentence-BERTçš„ä¼˜åŠ¿**ï¼š
- æ›´ç®€å•ï¼Œæ›´å®¹æ˜“å®ç°
- æ— ç›‘ç£ç‰ˆæœ¬å¯ç”¨ï¼ˆSimCSEç­‰ï¼‰
- æŸäº›æ•°æ®é›†ä¸Šä¹Ÿä¸å·®ï¼ˆLCQMC: 87.42% vs 86.67%ï¼‰

**å»ºè®®**ï¼šä¼˜å…ˆå°è¯•CoSENTï¼Œä½†ä¿ç•™Sentence-BERTä½œä¸ºbaselineã€‚

---

### â“ Q3: 768ç»´çœŸçš„å¤Ÿç”¨å—ï¼Ÿä¸ºä»€ä¹ˆä¸ç”¨æ›´é«˜ç»´ï¼Ÿ

**A**: ä»JLå¼•ç†çš„åˆ†æçœ‹ï¼Œ768ç»´**ç†è®ºä¸Š**è¶³å¤Ÿï¼Œä½†ï¼š

**å®è·µä¸­çš„è€ƒè™‘**ï¼š

1. **ç»´åº¦ vs è¡¨è¾¾èƒ½åŠ›**ï¼š
   - 768ç»´ï¼šç™¾ä¸‡çº§æ ·æœ¬è¶³å¤Ÿ
   - 1024ç»´ï¼šå¯èƒ½ç•¥å¥½ï¼Œä½†æå‡æœ‰é™
   - >2048ç»´ï¼šè¿‡æ‹Ÿåˆé£é™©å¢åŠ 

2. **è®¡ç®— vs æ•ˆæœçš„æƒè¡¡**ï¼š
   - æ›´é«˜ç»´ â†’ æ›´æ…¢çš„æ¨ç†
   - æ›´é«˜ç»´ â†’ æ›´å¤šçš„å†…å­˜å ç”¨
   - æ”¶ç›Šé€’å‡

**ç»éªŒæ³•åˆ™**ï¼š
- BERT-base (768ç»´)ï¼šé€‚åˆå¤§å¤šæ•°åœºæ™¯
- BERT-large (1024ç»´)ï¼šè¿½æ±‚æè‡´æ•ˆæœ
- è‡ªå®šä¹‰ç»´åº¦ï¼šé€šè¿‡æŠ•å½±å±‚è°ƒæ•´ï¼ˆå¦‚é™ç»´åˆ°256ï¼‰

---

### â“ Q4: æ··åˆæ–¹æ¡ˆä¸­ï¼Œç²—æ’åº”è¯¥å–Top-Kï¼ŒKåº”è¯¥å¤šå¤§ï¼Ÿ

**A**: Kçš„é€‰æ‹©æ˜¯**å¬å›ç‡ vs è®¡ç®—é‡**çš„æƒè¡¡ï¼š

**ç†è®ºåˆ†æ**ï¼š

å‡è®¾ç²—æ’çš„å‡†ç¡®ç‡ä¸º$p$ï¼Œåˆ™Recall@Kçº¦ä¸ºï¼š
$$
\text{Recall@K} \approx 1 - (1-p)^K
$$

| K | Recall@K ($p=0.001$) | Recall@K ($p=0.01$) | ç²¾æ’è®¡ç®—é‡ |
|---|---------------------|---------------------|----------|
| 100 | 9.5% | 63.4% | ä½ |
| 500 | 39.3% | 99.3% | ä¸­ |
| 1000 | 63.2% | 99.995% | ä¸­é«˜ |
| 5000 | 99.3% | ~100% | é«˜ |

**å®è·µå»ºè®®**ï¼š
- ä¸€èˆ¬åœºæ™¯ï¼šK=1000ï¼ˆæ€§ä»·æ¯”æœ€é«˜ï¼‰
- é«˜å¬å›è¦æ±‚ï¼šK=5000
- ä½å»¶è¿Ÿè¦æ±‚ï¼šK=100-500

**è‡ªé€‚åº”ç­–ç•¥**ï¼šæ ¹æ®queryéš¾åº¦åŠ¨æ€è°ƒæ•´Kï¼š
- ç®€å•queryï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰ï¼šå°K
- å›°éš¾queryï¼ˆä½ç½®ä¿¡åº¦ï¼‰ï¼šå¤§K

---

### â“ Q5: å¦‚ä½•åˆ¤æ–­æˆ‘çš„æ•°æ®é›†æ˜¯å¦æœ‰ä¸¥é‡çš„å¯¹æŠ—æ ·æœ¬ï¼Ÿ

**A**: å¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‡æ ‡è¯„ä¼°ï¼š

**1. å­—é¢é‡å åº¦ç»Ÿè®¡**ï¼š

è®¡ç®—è´Ÿæ ·æœ¬çš„å¹³å‡tokené‡å åº¦ï¼š
```python
def compute_overlap(text1, text2):
    tokens1 = set(tokenize(text1))
    tokens2 = set(tokenize(text2))
    return len(tokens1 & tokens2) / len(tokens1 | tokens2)

negative_overlaps = [
    compute_overlap(t1, t2)
    for t1, t2, label in dataset if label == 0
]
avg_overlap = np.mean(negative_overlaps)

# avg_overlap > 0.5 â†’ å¯¹æŠ—æ€§å¼º
# avg_overlap < 0.3 â†’ å¯¹æŠ—æ€§å¼±
```

**2. æ— ç›‘ç£æ–¹æ³•çš„è¡¨ç°**ï¼š

åœ¨ä½ çš„æ•°æ®é›†ä¸Šæµ‹è¯•SimCSE/Sentence-BERTï¼ˆæ— ç›‘ç£ç‰ˆï¼‰ï¼š
- æ•ˆæœ > 70%ï¼šå¯¹æŠ—æ€§å¼±
- æ•ˆæœ < 60%ï¼šå¯¹æŠ—æ€§å¼º

**3. äººå·¥æŠ½æ ·æ£€æŸ¥**ï¼š

éšæœºæŠ½å–100å¯¹è´Ÿæ ·æœ¬ï¼Œäººå·¥åˆ¤æ–­ï¼š
- è¯­ä¹‰æ˜æ˜¾ä¸åŒä½†å­—é¢ç›¸ä¼¼çš„æ¯”ä¾‹ > 30%ï¼šå¯¹æŠ—æ€§å¼º

---

### â“ Q6: Powellä¼˜åŒ–æœ‰æ—¶ä¸æ”¶æ•›æ€ä¹ˆåŠï¼Ÿ

**A**: Powellä¸æ”¶æ•›é€šå¸¸æœ‰ä»¥ä¸‹åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š

**åŸå› 1ï¼šåˆå§‹å€¼ä¸å¥½**
```python
# ä¸å¥½çš„åˆå§‹åŒ–
t0 = np.random.rand(num_classes)  # éšæœºåˆå§‹åŒ–

# å¥½çš„åˆå§‹åŒ–
t0 = np.zeros(num_classes)  # å¯¹åº”ç­‰æƒé‡
```

**åŸå› 2ï¼šå®¹å¿åº¦è®¾ç½®ä¸å½“**
```python
# å¤ªä¸¥æ ¼ï¼ˆå¯èƒ½æ°¸è¿œä¸æ”¶æ•›ï¼‰
options = {'xtol': 1e-15, 'ftol': 1e-15}

# åˆç†è®¾ç½®
options = {'xtol': 1e-8, 'ftol': 1e-8}
```

**åŸå› 3ï¼šç›®æ ‡å‡½æ•°å™ªå£°å¤ªå¤§**

å¦‚æœéªŒè¯é›†å¤ªå°ï¼ˆ<1000æ ·æœ¬ï¼‰ï¼Œå»ºè®®ï¼š
- å¢å¤§éªŒè¯é›†
- æˆ–ä½¿ç”¨K-foldäº¤å‰éªŒè¯
- æˆ–ç®€åŒ–ä¸ºäºŒåˆ†ç±»ï¼ˆå‡å°‘å‚æ•°ï¼‰

**æ›¿ä»£æ–¹æ¡ˆ**ï¼š
- ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰ï¼šç®€å•ä½†æ…¢
- è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰ï¼šé€‚åˆé«˜å™ªå£°åœºæ™¯

</div>

---

## å…«ã€æ€»ç»“ä¸å±•æœ›

<div class="note-box">

### æ ¸å¿ƒç»“è®º

**å®éªŒç»“æœ**ï¼š
1. âœ… åœ¨ATECå’ŒBQä¸Šï¼ŒCoSENTä¸äº¤äº’å¼**æ— æ˜¾è‘—å·®å¼‚**
2. âœ… åœ¨LCQMCä¸Šï¼Œå·®è·çº¦1%ï¼ˆå¯æ¥å—ï¼‰
3. âš ï¸ åœ¨PAWSXä¸Šï¼Œå·®è·5-7%ï¼ˆå¯¹æŠ—æ ·æœ¬å¤šï¼‰

**ç†è®ºæ´å¯Ÿ**ï¼š
1. âœ… ç†è®ºä¸Šï¼Œç‰¹å¾å¼å¯ä»¥é€šè¿‡å†…ç§¯æ‹Ÿåˆä»»æ„ç›¸ä¼¼åº¦çŸ©é˜µ
2. âœ… 768ç»´å¥å‘é‡è¶³ä»¥è¡¨ç¤ºç™¾ä¸‡çº§æ ·æœ¬çš„ä¸¤ä¸¤ç›¸ä¼¼åº¦
3. âš ï¸ å®è·µä¸­çš„å·®è·æ¥è‡ª**è¿ç»­æ€§ vs å¯¹æŠ—æ€§**çš„çŸ›ç›¾

**æ–¹æ³•è´¡çŒ®**ï¼š
1. âœ… æå‡ºPowellä¼˜åŒ–æ–¹æ³•è‡ªåŠ¨æœç´¢å¤šåˆ†ç±»é˜ˆå€¼
2. âœ… ç³»ç»Ÿå¯¹æ¯”äº†ç‰¹å¾å¼ä¸äº¤äº’å¼çš„å·®è·
3. âœ… ç»™å‡ºäº†ç†è®ºåˆ†æå’Œå®ç”¨å»ºè®®

</div>

### æœªæ¥æ–¹å‘

<div class="intuition-box">

### ğŸ”¬ ç ”ç©¶å±•æœ›

**1. ç¼©å°PAWSXå·®è·**

å¯èƒ½çš„æ–¹å‘ï¼š
- å¯¹æŠ—è®­ç»ƒï¼šæ˜¾å¼åŠ å…¥å¯¹æŠ—æ ·æœ¬
- ç¡¬è´Ÿä¾‹æŒ–æ˜ï¼šé‡ç‚¹è®­ç»ƒå›°éš¾æ ·æœ¬
- å¤šä»»åŠ¡å­¦ä¹ ï¼šè”åˆè®­ç»ƒäº¤äº’å¼å’Œç‰¹å¾å¼

å…·ä½“å®ç°æ€è·¯ï¼š
```python
# ç¡¬è´Ÿä¾‹æŒ–æ˜ç¤ºä¾‹
def mine_hard_negatives(model, query, candidates, top_k=100):
    """æŒ–æ˜ä¸queryç›¸ä¼¼ä½†æ ‡ç­¾ä¸ºè´Ÿçš„æ ·æœ¬"""
    similarities = model.compute_similarity(query, candidates)
    # é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜ä½†æ ‡ç­¾ä¸ºè´Ÿçš„æ ·æœ¬
    hard_negatives = []
    for i in similarities.argsort()[::-1]:
        if candidates[i].label == 0:  # è´Ÿæ ·æœ¬
            hard_negatives.append(candidates[i])
            if len(hard_negatives) >= top_k:
                break
    return hard_negatives
```

**2. æ··åˆæ¶æ„**

ç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼š
- å‰kå±‚ï¼šåˆ†åˆ«ç¼–ç ï¼ˆç‰¹å¾å¼ï¼‰
- åmå±‚ï¼šè”åˆç¼–ç ï¼ˆäº¤äº’å¼ï¼‰
- å¯è°ƒèŠ‚k/mä»¥å¹³è¡¡æ•ˆç‡å’Œæ•ˆæœ

**Poly-encoderæ¶æ„**ï¼ˆFacebook Researchï¼‰æ˜¯è¿™ä¸ªæ€è·¯çš„æˆåŠŸæ¡ˆä¾‹ï¼š
- ä½¿ç”¨mä¸ªå…¨å±€å‘é‡è¡¨ç¤ºcontext
- Queryä¸è¿™mä¸ªå‘é‡äº¤äº’
- å¤æ‚åº¦ï¼š$O(m)$ vs çº¯äº¤äº’çš„$O(n)$

**3. åŠ¨æ€é€‰æ‹©**

æ ¹æ®queryè‡ªé€‚åº”é€‰æ‹©ï¼š
- ç®€å•queryï¼šç‰¹å¾å¼ï¼ˆå¿«ï¼‰
- å›°éš¾queryï¼šäº¤äº’å¼ï¼ˆå‡†ï¼‰
- ä½¿ç”¨å…ƒæ¨¡å‹åˆ¤æ–­éš¾åº¦

**éš¾åº¦åˆ¤æ–­æŒ‡æ ‡**ï¼š
$$
\text{Difficulty}(q) = 1 - \max_i \text{sim}(q, c_i)
$$
- æœ€é«˜ç›¸ä¼¼åº¦å¾ˆé«˜ â†’ ç®€å•ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
- æœ€é«˜ç›¸ä¼¼åº¦ä¸€èˆ¬ â†’ å›°éš¾ï¼ˆéœ€è¦ç²¾æ’ï¼‰

**4. çŸ¥è¯†è’¸é¦**

ç”¨äº¤äº’å¼æŒ‡å¯¼ç‰¹å¾å¼ï¼š
- Teacherï¼šäº¤äº’å¼æ¨¡å‹
- Studentï¼šç‰¹å¾å¼æ¨¡å‹
- è’¸é¦å¯¹è±¡ï¼šç›¸ä¼¼åº¦åˆ†å¸ƒ

**è’¸é¦æŸå¤±**ï¼š
$$
\mathcal{L}_{\text{distill}} = \text{KL}\left(P_{\text{teacher}}(s | q, c) \parallel P_{\text{student}}(s | q, c)\right)
$$

å®éªŒè¡¨æ˜ï¼Œè’¸é¦åçš„ç‰¹å¾å¼æ¨¡å‹å¯ä»¥ç¼©å°ä¸äº¤äº’å¼çš„å·®è·2-3ä¸ªç™¾åˆ†ç‚¹ã€‚

**5. è·¨è¯­è¨€è¿ç§»**

å½“å‰å®éªŒä¸»è¦åœ¨ä¸­æ–‡ï¼Œæœªæ¥å¯ä»¥æ¢ç´¢ï¼š
- å¤šè¯­è¨€è”åˆè®­ç»ƒ
- è·¨è¯­è¨€zero-shotè¿ç§»
- ä½èµ„æºè¯­è¨€çš„è¡¨ç°

</div>

---

## ä¹ã€å®è·µæ£€æŸ¥æ¸…å•

<div class="note-box">

### âœ… ä½¿ç”¨CoSENTå‰çš„æ£€æŸ¥æ¸…å•

**æ•°æ®å‡†å¤‡**ï¼š
- [ ] ç¡®è®¤æœ‰è¶³å¤Ÿçš„æ ‡æ³¨æ•°æ®ï¼ˆ>10kå¯¹ï¼‰
- [ ] æ£€æŸ¥æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼ˆå»ºè®®1:1åˆ°1:3ï¼‰
- [ ] è¯„ä¼°å¯¹æŠ—æ ·æœ¬çš„æ¯”ä¾‹ï¼ˆå‚è€ƒQ5ï¼‰
- [ ] åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼ˆå»ºè®®7:1.5:1.5ï¼‰

**æ¨¡å‹é€‰æ‹©**ï¼š
- [ ] æ ¹æ®æ•°æ®é›†å¤§å°é€‰æ‹©base/largeæ¨¡å‹
- [ ] è€ƒè™‘æ˜¯å¦éœ€è¦é¢†åŸŸé¢„è®­ç»ƒæ¨¡å‹
- [ ] ç¡®å®šæ˜¯å¦éœ€è¦æ··åˆæ–¹æ¡ˆï¼ˆæ•°æ®é‡>100ä¸‡ï¼‰

**è®­ç»ƒé…ç½®**ï¼š
- [ ] è®¾ç½®åˆé€‚çš„å­¦ä¹ ç‡ï¼ˆæ¨è2e-5ï¼‰
- [ ] è®¾ç½®åˆé€‚çš„batch sizeï¼ˆæ¨è32-64ï¼‰
- [ ] å¯ç”¨æ—©åœï¼ˆpatience=3-5ï¼‰
- [ ] ä¿å­˜æœ€ä½³checkpoint

**è¯„ä¼°æ–¹æ³•**ï¼š
- [ ] ä½¿ç”¨Spearmanç³»æ•°ï¼ˆå›å½’è§†è§’ï¼‰
- [ ] ä½¿ç”¨Accuracy+é˜ˆå€¼æœç´¢ï¼ˆåˆ†ç±»è§†è§’ï¼‰
- [ ] ä¸baselineå¯¹æ¯”ï¼ˆSentence-BERT/äº¤äº’å¼ï¼‰
- [ ] åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯æ³›åŒ–æ€§

**éƒ¨ç½²è€ƒè™‘**ï¼š
- [ ] æµ‹è¯•æ¨ç†å»¶è¿Ÿï¼ˆç¦»çº¿ç¼–ç +åœ¨çº¿æ£€ç´¢ï¼‰
- [ ] è¯„ä¼°å†…å­˜å ç”¨ï¼ˆNÃ—768Ã—4 bytesï¼‰
- [ ] è€ƒè™‘é‡åŒ–/å‰ªæï¼ˆå¦‚é™ç»´åˆ°256ï¼‰
- [ ] å‡†å¤‡é™çº§æ–¹æ¡ˆï¼ˆäº¤äº’å¼ç²¾æ’ï¼‰

**æŒç»­ä¼˜åŒ–**ï¼š
- [ ] å®šæœŸæ›´æ–°æ¨¡å‹ï¼ˆæ–°æ•°æ®ï¼‰
- [ ] ç›‘æ§çº¿ä¸Šæ•ˆæœï¼ˆA/Bæµ‹è¯•ï¼‰
- [ ] æ”¶é›†badcaseï¼ˆå¯¹æŠ—æ ·æœ¬ï¼‰
- [ ] è¿­ä»£æ”¹è¿›ï¼ˆç¡¬è´Ÿä¾‹æŒ–æ˜ï¼‰

</div>

---

## åã€å‚è€ƒæ–‡çŒ®

<div class="note-box">

### ä¸»è¦å‚è€ƒæ–‡çŒ®

1. **Reimers, N., & Gurevych, I.** (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. *EMNLP 2019*.
   - æå‡ºSentence-BERTï¼Œå¼€åˆ›ç‰¹å¾å¼åŒ¹é…çš„æ–°èŒƒå¼

2. **Gao, T., Yao, X., & Chen, D.** (2021). SimCSE: Simple Contrastive Learning of Sentence Embeddings. *EMNLP 2021*.
   - æ— ç›‘ç£å¯¹æ¯”å­¦ä¹ çš„ä»£è¡¨æ€§å·¥ä½œ

3. **Su, J.** (2022). CoSENTï¼ˆä¸€ï¼‰ï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ. *https://spaces.ac.cn/archives/8847*
   - CoSENTåŸå§‹è®ºæ–‡ï¼Œæœ¬æ–‡çš„å‰ç½®å·¥ä½œ

4. **Dasgupta, S., & Gupta, A.** (2003). An elementary proof of a theorem of Johnson and Lindenstrauss. *Random Structures & Algorithms, 22(1)*, 60-65.
   - JLå¼•ç†çš„ç®€åŒ–è¯æ˜

5. **Powell, M. J. D.** (1964). An efficient method for finding the minimum of a function of several variables without calculating derivatives. *The Computer Journal, 7(2)*, 155-162.
   - Powellä¼˜åŒ–æ–¹æ³•çš„åŸå§‹è®ºæ–‡

6. **Humeau, S., Shuster, K., Lachaux, M. A., & Weston, J.** (2019). Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring. *ICLR 2020*.
   - Poly-encoderï¼šæ··åˆæ¶æ„çš„æˆåŠŸæ¡ˆä¾‹

7. **Yang, Y., Cer, D., Ahmad, A., Guo, M., Law, J., Constant, N., ... & Kurzweil, R.** (2020). Multilingual Universal Sentence Encoder for Semantic Retrieval. *ACL 2020*.
   - å¤šè¯­è¨€å¥å‘é‡çš„å·¥ä¸šçº§åº”ç”¨

### ç›¸å…³èµ„æº

**è‹å‰‘æ—çš„åšå®¢**ï¼š
- [è®©äººæƒŠå¹çš„Johnson-Lindenstrausså¼•ç†ï¼šç†è®ºç¯‡](https://spaces.ac.cn/archives/8679)
- [è®©äººæƒŠå¹çš„Johnson-Lindenstrausså¼•ç†ï¼šåº”ç”¨ç¯‡](https://spaces.ac.cn/archives/8706)
- [æ— ç›‘ç£è¯­ä¹‰ç›¸ä¼¼åº¦å“ªå®¶å¼ºï¼Ÿæˆ‘ä»¬åšäº†ä¸ªæ¯”è¾ƒå…¨é¢çš„è¯„æµ‹](https://spaces.ac.cn/archives/8321)
- [ä¸­æ–‡ä»»åŠ¡è¿˜æ˜¯SOTAå—ï¼Ÿæˆ‘ä»¬åšäº†ä¸€ä¸ªä¸­æ–‡NLUåŸºå‡†æµ‹è¯•](https://spaces.ac.cn/archives/7975)

**ä»£ç å®ç°**ï¼š
- Sentence-Transformersåº“ï¼šhttps://github.com/UKPLab/sentence-transformers
- CoSENTå®ç°ï¼šhttps://github.com/bojone/CoSENT
- SimCSEå®ç°ï¼šhttps://github.com/princeton-nlp/SimCSE

</div>

---

**ç›¸å…³æ–‡ç« **ï¼š
- [CoSENTï¼ˆä¸€ï¼‰ï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ](https://spaces.ac.cn/archives/8847)
- [è®©äººæƒŠå¹çš„Johnson-Lindenstrausså¼•ç†ï¼šç†è®ºç¯‡](https://spaces.ac.cn/archives/8679)
- [è®©äººæƒŠå¹çš„Johnson-Lindenstrausså¼•ç†ï¼šåº”ç”¨ç¯‡](https://spaces.ac.cn/archives/8706)
- [æ— ç›‘ç£è¯­ä¹‰ç›¸ä¼¼åº¦å“ªå®¶å¼ºï¼Ÿæˆ‘ä»¬åšäº†ä¸ªæ¯”è¾ƒå…¨é¢çš„è¯„æµ‹](https://spaces.ac.cn/archives/8321)

---

_**è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š**<https://spaces.ac.cn/archives/8860>_

_**æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š**_[ã€Šç§‘å­¦ç©ºé—´FAQã€‹](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "ã€Šç§‘å­¦ç©ºé—´FAQã€‹")

**å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š**

è‹å‰‘æ—. (Jan. 12, 2022). ã€ŠCoSENTï¼ˆäºŒï¼‰ï¼šç‰¹å¾å¼åŒ¹é…ä¸äº¤äº’å¼åŒ¹é…æœ‰å¤šå¤§å·®è·ï¼Ÿ ã€‹[Blog post]. Retrieved from <https://spaces.ac.cn/archives/8860>

@online{kexuefm-8860,
title={CoSENTï¼ˆäºŒï¼‰ï¼šç‰¹å¾å¼åŒ¹é…ä¸äº¤äº’å¼åŒ¹é…æœ‰å¤šå¤§å·®è·ï¼Ÿ},
author={è‹å‰‘æ—},
year={2022},
month={Jan},
url={\url{https://spaces.ac.cn/archives/8860}},
}
