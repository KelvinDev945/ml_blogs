---
title: Transformerå‡çº§ä¹‹è·¯ï¼š2ã€åšé‡‡ä¼—é•¿çš„æ—‹è½¬å¼ä½ç½®ç¼–ç 
slug: transformerå‡çº§ä¹‹è·¯2åšé‡‡ä¼—é•¿çš„æ—‹è½¬å¼ä½ç½®ç¼–ç 
date: 
source: https://spaces.ac.cn/archives/8265
tags: å¤æ•°, è¯­è¨€æ¨¡å‹, attention, ä½ç½®ç¼–ç , rope
status: pending
---

# Transformerå‡çº§ä¹‹è·¯ï¼š2ã€åšé‡‡ä¼—é•¿çš„æ—‹è½¬å¼ä½ç½®ç¼–ç 

**åŸæ–‡é“¾æ¥**: [https://spaces.ac.cn/archives/8265](https://spaces.ac.cn/archives/8265)

**å‘å¸ƒæ—¥æœŸ**: 

---

ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å¯¹åŸå§‹çš„Sinusoidalä½ç½®ç¼–ç åšäº†è¾ƒä¸ºè¯¦ç»†çš„æ¨å¯¼å’Œç†è§£ï¼Œæ€»çš„æ„Ÿè§‰æ˜¯Sinusoidalä½ç½®ç¼–ç æ˜¯ä¸€ç§â€œæƒ³è¦æˆä¸ºç›¸å¯¹ä½ç½®ç¼–ç çš„ç»å¯¹ä½ç½®ç¼–ç â€ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œç»å¯¹ä½ç½®ç¼–ç å…·æœ‰å®ç°ç®€å•ã€è®¡ç®—é€Ÿåº¦å¿«ç­‰ä¼˜ç‚¹ï¼Œè€Œç›¸å¯¹ä½ç½®ç¼–ç åˆ™ç›´æ¥åœ°ä½“ç°äº†ç›¸å¯¹ä½ç½®ä¿¡å·ï¼Œè·Ÿæˆ‘ä»¬çš„ç›´è§‚ç†è§£å»åˆï¼Œå®é™…æ€§èƒ½å¾€å¾€ä¹Ÿæ›´å¥½ã€‚ç”±æ­¤å¯è§ï¼Œå¦‚æœå¯ä»¥é€šè¿‡ç»å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼å®ç°ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œé‚£ä¹ˆå°±æ˜¯â€œé›†å„å®¶ä¹‹æ‰€é•¿â€ã€â€œé±¼ä¸ç†ŠæŒå…¼å¾—â€äº†ã€‚Sinusoidalä½ç½®ç¼–ç éšçº¦åšåˆ°äº†è¿™ä¸€ç‚¹ï¼Œä½†å¹¶ä¸å¤Ÿå¥½ã€‚

æœ¬æ–‡å°†ä¼šä»‹ç»æˆ‘ä»¬è‡ªç ”çš„Rotary Transformerï¼ˆRoFormerï¼‰æ¨¡å‹ï¼Œå®ƒçš„ä¸»è¦æ”¹åŠ¨æ˜¯åº”ç”¨äº†ç¬”è€…æ„æ€çš„â€œæ—‹è½¬å¼ä½ç½®ç¼–ç ï¼ˆRotary Position Embeddingï¼ŒRoPEï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§é…åˆAttentionæœºåˆ¶èƒ½è¾¾åˆ°â€œç»å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼å®ç°ç›¸å¯¹ä½ç½®ç¼–ç â€çš„è®¾è®¡ã€‚è€Œä¹Ÿæ­£å› ä¸ºè¿™ç§è®¾è®¡ï¼Œå®ƒè¿˜æ˜¯ç›®å‰å”¯ä¸€ä¸€ç§å¯ç”¨äºçº¿æ€§Attentionçš„ç›¸å¯¹ä½ç½®ç¼–ç ã€‚

> **RoFormerï¼š<https://github.com/ZhuiyiTechnology/roformer>**

## åŸºæœ¬æ€è·¯ #

åœ¨ä¹‹å‰çš„æ–‡ç« [ã€Šè®©ç ”ç©¶äººå‘˜ç»å°½è„‘æ±çš„Transformerä½ç½®ç¼–ç ã€‹](/archives/8130)ä¸­æˆ‘ä»¬å°±ç®€è¦ä»‹ç»è¿‡RoPEï¼Œå½“æ—¶ç§°ä¹‹ä¸ºâ€œèåˆå¼â€ï¼Œæœ¬æ–‡åˆ™æ›´åŠ è¯¦ç»†åœ°ä»‹ç»å®ƒçš„æ¥æºä¸æ€§è´¨ã€‚åœ¨RoPEä¸­ï¼Œæˆ‘ä»¬çš„å‡ºå‘ç‚¹å°±æ˜¯â€œé€šè¿‡ç»å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼å®ç°ç›¸å¯¹ä½ç½®ç¼–ç â€ï¼Œè¿™æ ·åšæ—¢æœ‰ç†è®ºä¸Šçš„ä¼˜é›…ä¹‹å¤„ï¼Œä¹Ÿæœ‰å®è·µä¸Šçš„å®ç”¨ä¹‹å¤„ï¼Œæ¯”å¦‚å®ƒå¯ä»¥æ‹“å±•åˆ°çº¿æ€§Attentionä¸­å°±æ˜¯ä¸»è¦å› ä¸ºè¿™ä¸€ç‚¹ã€‚

ä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œæˆ‘ä»¬å‡è®¾é€šè¿‡ä¸‹è¿°è¿ç®—æ¥ç»™$\boldsymbol{q},\boldsymbol{k}$æ·»åŠ ç»å¯¹ä½ç½®ä¿¡æ¯ï¼š  
\begin{equation}\tilde{\boldsymbol{q}}_m = \boldsymbol{f}(\boldsymbol{q}, m), \quad\tilde{\boldsymbol{k}}_n = \boldsymbol{f}(\boldsymbol{k}, n)\end{equation}  
ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬åˆ†åˆ«ä¸º$\boldsymbol{q},\boldsymbol{k}$è®¾è®¡æ“ä½œ$\boldsymbol{f}(\cdot, m),\boldsymbol{f}(\cdot, n)$ï¼Œä½¿å¾—ç»è¿‡è¯¥æ“ä½œåï¼Œ$\tilde{\boldsymbol{q}}_m,\tilde{\boldsymbol{k}}_n$å°±å¸¦æœ‰äº†ä½ç½®$m,n$çš„ç»å¯¹ä½ç½®ä¿¡æ¯ã€‚Attentionçš„æ ¸å¿ƒè¿ç®—æ˜¯å†…ç§¯ï¼Œæ‰€ä»¥æˆ‘ä»¬å¸Œæœ›çš„å†…ç§¯çš„ç»“æœå¸¦æœ‰ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œå› æ­¤å‡è®¾å­˜åœ¨æ’ç­‰å…³ç³»ï¼š  
\begin{equation}\langle\boldsymbol{f}(\boldsymbol{q}, m), \boldsymbol{f}(\boldsymbol{k}, n)\rangle = g(\boldsymbol{q},\boldsymbol{k},m-n)\end{equation}  
æ‰€ä»¥æˆ‘ä»¬è¦æ±‚å‡ºè¯¥æ’ç­‰å¼çš„ä¸€ä¸ªï¼ˆå°½å¯èƒ½ç®€å•çš„ï¼‰è§£ã€‚æ±‚è§£è¿‡ç¨‹è¿˜éœ€è¦ä¸€äº›åˆå§‹æ¡ä»¶ï¼Œæ˜¾ç„¶æˆ‘ä»¬å¯ä»¥åˆç†åœ°è®¾$\boldsymbol{f}(\boldsymbol{q}, 0)=\boldsymbol{q}$å’Œ$\boldsymbol{f}(\boldsymbol{k}, 0)=\boldsymbol{k}$ã€‚

## æ±‚è§£è¿‡ç¨‹ #

åŒä¸Šä¸€ç¯‡æ€è·¯ä¸€æ ·ï¼Œæˆ‘ä»¬å…ˆè€ƒè™‘äºŒç»´æƒ…å½¢ï¼Œç„¶åå€ŸåŠ©å¤æ•°æ¥æ±‚è§£ã€‚åœ¨å¤æ•°ä¸­æœ‰$\langle\boldsymbol{q},\boldsymbol{k}\rangle=\text{Re}[\boldsymbol{q}\boldsymbol{k}^*]$ï¼Œ$\text{Re}[]$ä»£è¡¨å¤æ•°çš„å®éƒ¨ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰  
\begin{equation}\text{Re}[\boldsymbol{f}(\boldsymbol{q}, m)\boldsymbol{f}^*(\boldsymbol{k}, n)] = g(\boldsymbol{q},\boldsymbol{k},m-n)\end{equation}  
ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾å­˜åœ¨å¤æ•°$\boldsymbol{g}(\boldsymbol{q},\boldsymbol{k},m-n)$ï¼Œä½¿å¾—$\boldsymbol{f}(\boldsymbol{q}, m)\boldsymbol{f}^*(\boldsymbol{k}, n) = \boldsymbol{g}(\boldsymbol{q},\boldsymbol{k},m-n)$ï¼Œç„¶åæˆ‘ä»¬ç”¨å¤æ•°çš„æŒ‡æ•°å½¢å¼ï¼Œè®¾  
\begin{equation}\begin{aligned}  
\boldsymbol{f}(\boldsymbol{q}, m) =&\, R_f (\boldsymbol{q}, m)e^{\text{i}\Theta_f(\boldsymbol{q}, m)} \\\  
\boldsymbol{f}(\boldsymbol{k}, n) =&\, R_f (\boldsymbol{k}, n)e^{\text{i}\Theta_f(\boldsymbol{k}, n)} \\\  
\boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) =&\, R_g (\boldsymbol{q}, \boldsymbol{k}, m-n)e^{\text{i}\Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n)} \\\  
\end{aligned}\end{equation}  
é‚£ä¹ˆä»£å…¥æ–¹ç¨‹åå°±å¾—åˆ°æ–¹ç¨‹ç»„  
\begin{equation}\begin{aligned}  
R_f (\boldsymbol{q}, m) R_f (\boldsymbol{k}, n) =&\, R_g (\boldsymbol{q}, \boldsymbol{k}, m-n) \\\  
\Theta_f (\boldsymbol{q}, m) - \Theta_f (\boldsymbol{k}, n) =&\, \Theta_g (\boldsymbol{q}, \boldsymbol{k}, m-n)  
\end{aligned}\end{equation}  
å¯¹äºç¬¬ä¸€ä¸ªæ–¹ç¨‹ï¼Œä»£å…¥$m=n$å¾—åˆ°  
\begin{equation}R_f (\boldsymbol{q}, m) R_f (\boldsymbol{k}, m) = R_g (\boldsymbol{q}, \boldsymbol{k}, 0) = R_f (\boldsymbol{q}, 0) R_f (\boldsymbol{k}, 0) = \Vert \boldsymbol{q}\Vert \Vert \boldsymbol{k}\Vert\end{equation}  
æœ€åä¸€ä¸ªç­‰å·æºäºåˆå§‹æ¡ä»¶$\boldsymbol{f}(\boldsymbol{q}, 0)=\boldsymbol{q}$å’Œ$\boldsymbol{f}(\boldsymbol{k}, 0)=\boldsymbol{k}$ã€‚æ‰€ä»¥ç°åœ¨æˆ‘ä»¬å¯ä»¥å¾ˆç®€å•åœ°è®¾$R_f (\boldsymbol{q}, m)=\Vert \boldsymbol{q}\Vert, R_f (\boldsymbol{k}, m)=\Vert \boldsymbol{k}\Vert$ï¼Œå³å®ƒä¸ä¾èµ–äº$m$ã€‚è‡³äºç¬¬äºŒä¸ªæ–¹ç¨‹ï¼ŒåŒæ ·ä»£å…¥$m=n$å¾—åˆ°  
\begin{equation}\Theta_f (\boldsymbol{q}, m) - \Theta_f (\boldsymbol{k}, m) = \Theta_g (\boldsymbol{q}, \boldsymbol{k}, 0) = \Theta_f (\boldsymbol{q}, 0) - \Theta_f (\boldsymbol{k}, 0) = \Theta (\boldsymbol{q}) - \Theta (\boldsymbol{k})\end{equation}  
è¿™é‡Œçš„$\Theta (\boldsymbol{q}),\Theta (\boldsymbol{k})$æ˜¯$\boldsymbol{q},\boldsymbol{k}$æœ¬èº«çš„å¹…è§’ï¼Œæœ€åä¸€ä¸ªç­‰å·åŒæ ·æºäºåˆå§‹æ¡ä»¶ã€‚æ ¹æ®ä¸Šå¼å¾—åˆ°$\Theta_f (\boldsymbol{q}, m) - \Theta (\boldsymbol{q}) = \Theta_f (\boldsymbol{k}, m) - \Theta (\boldsymbol{k})$ï¼Œæ‰€ä»¥$\Theta_f (\boldsymbol{q}, m) - \Theta (\boldsymbol{q})$åº”è¯¥æ˜¯ä¸€ä¸ªåªä¸$m$ç›¸å…³ã€è·Ÿ$\boldsymbol{q}$æ— å…³çš„å‡½æ•°ï¼Œè®°ä¸º$\varphi(m)$ï¼Œå³$\Theta_f (\boldsymbol{q}, m) = \Theta (\boldsymbol{q}) + \varphi(m)$ã€‚æ¥ç€ä»£å…¥$n=m-1$ï¼Œæ•´ç†å¾—åˆ°  
\begin{equation}\varphi(m) - \varphi(m-1) = \Theta_g (\boldsymbol{q}, \boldsymbol{k}, 1) + \Theta (\boldsymbol{k}) - \Theta (\boldsymbol{q})\end{equation}  
å³$\\{\varphi(m)\\}$æ˜¯ç­‰å·®æ•°åˆ—ï¼Œè®¾å³ç«¯ä¸º$\theta$ï¼Œé‚£ä¹ˆå°±è§£å¾—$\varphi(m)=m\theta$ã€‚

## ç¼–ç å½¢å¼ #

ç»¼ä¸Šï¼Œæˆ‘ä»¬å¾—åˆ°äºŒç»´æƒ…å†µä¸‹ç”¨å¤æ•°è¡¨ç¤ºçš„RoPEï¼š  
\begin{equation}  
\boldsymbol{f}(\boldsymbol{q}, m) = R_f (\boldsymbol{q}, m)e^{\text{i}\Theta_f(\boldsymbol{q}, m)}  
= \Vert q\Vert e^{\text{i}(\Theta(\boldsymbol{q}) + m\theta)} = \boldsymbol{q} e^{\text{i}m\theta}\end{equation}  
æ ¹æ®å¤æ•°ä¹˜æ³•çš„å‡ ä½•æ„ä¹‰ï¼Œè¯¥å˜æ¢å®é™…ä¸Šå¯¹åº”ç€å‘é‡çš„æ—‹è½¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ—‹è½¬å¼ä½ç½®ç¼–ç â€ï¼Œå®ƒè¿˜å¯ä»¥å†™æˆçŸ©é˜µå½¢å¼ï¼š  
\begin{equation}  
\boldsymbol{f}(\boldsymbol{q}, m) =\begin{pmatrix}\cos m\theta & -\sin m\theta\\\ \sin m\theta & \cos m\theta\end{pmatrix} \begin{pmatrix}q_0 \\\ q_1\end{pmatrix}\end{equation}  
ç”±äºå†…ç§¯æ»¡è¶³çº¿æ€§å åŠ æ€§ï¼Œå› æ­¤ä»»æ„å¶æ•°ç»´çš„RoPEï¼Œæˆ‘ä»¬éƒ½å¯ä»¥è¡¨ç¤ºä¸ºäºŒç»´æƒ…å½¢çš„æ‹¼æ¥ï¼Œå³  
\begin{equation}\scriptsize{\underbrace{\begin{pmatrix}  
\cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\\  
\sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0 \\\  
0 & 0 & \cos m\theta_1 & -\sin m\theta_1 & \cdots & 0 & 0 \\\  
0 & 0 & \sin m\theta_1 & \cos m\theta_1 & \cdots & 0 & 0 \\\  
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\\  
0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2-1} & -\sin m\theta_{d/2-1} \\\  
0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2-1} & \cos m\theta_{d/2-1} \\\  
\end{pmatrix}}_{\boldsymbol{\mathcal{R}}_m} \begin{pmatrix}q_0 \\\ q_1 \\\ q_2 \\\ q_3 \\\ \vdots \\\ q_{d-2} \\\ q_{d-1}\end{pmatrix}}\end{equation}  
ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™ä½ç½®ä¸º$m$çš„å‘é‡$\boldsymbol{q}$ä¹˜ä¸ŠçŸ©é˜µ$\boldsymbol{\mathcal{R}}_m$ã€ä½ç½®ä¸º$n$çš„å‘é‡$\boldsymbol{k}$ä¹˜ä¸ŠçŸ©é˜µ$\boldsymbol{\mathcal{R}}_n$ï¼Œç”¨å˜æ¢åçš„$\boldsymbol{Q},\boldsymbol{K}$åºåˆ—åšAttentionï¼Œé‚£ä¹ˆAttentionå°±è‡ªåŠ¨åŒ…å«ç›¸å¯¹ä½ç½®ä¿¡æ¯äº†ï¼Œå› ä¸ºæˆç«‹æ’ç­‰å¼ï¼š  
\begin{equation}(\boldsymbol{\mathcal{R}}_m \boldsymbol{q})^{\top}(\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \boldsymbol{q}^{\top} \boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n \boldsymbol{k} = \boldsymbol{q}^{\top} \boldsymbol{\mathcal{R}}_{n-m} \boldsymbol{k}\end{equation}  
å€¼å¾—æŒ‡å‡ºçš„æ˜¯ï¼Œ$\boldsymbol{\mathcal{R}}_m$æ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼Œå®ƒä¸ä¼šæ”¹å˜å‘é‡çš„æ¨¡é•¿ï¼Œå› æ­¤é€šå¸¸æ¥è¯´å®ƒä¸ä¼šæ”¹å˜åŸæ¨¡å‹çš„ç¨³å®šæ€§ã€‚

ç”±äº$\boldsymbol{\mathcal{R}}_m$çš„ç¨€ç–æ€§ï¼Œæ‰€ä»¥ç›´æ¥ç”¨çŸ©é˜µä¹˜æ³•æ¥å®ç°ä¼šå¾ˆæµªè´¹ç®—åŠ›ï¼Œæ¨èé€šè¿‡ä¸‹è¿°æ–¹å¼æ¥å®ç°RoPEï¼š  
\begin{equation}\begin{pmatrix}q_0 \\\ q_1 \\\ q_2 \\\ q_3 \\\ \vdots \\\ q_{d-2} \\\ q_{d-1}  
\end{pmatrix}\otimes\begin{pmatrix}\cos m\theta_0 \\\ \cos m\theta_0 \\\ \cos m\theta_1 \\\ \cos m\theta_1 \\\ \vdots \\\ \cos m\theta_{d/2-1} \\\ \cos m\theta_{d/2-1}  
\end{pmatrix} + \begin{pmatrix}-q_1 \\\ q_0 \\\ -q_3 \\\ q_2 \\\ \vdots \\\ -q_{d-1} \\\ q_{d-2}  
\end{pmatrix}\otimes\begin{pmatrix}\sin m\theta_0 \\\ \sin m\theta_0 \\\ \sin m\theta_1 \\\ \sin m\theta_1 \\\ \vdots \\\ \sin m\theta_{d/2-1} \\\ \sin m\theta_{d/2-1}  
\end{pmatrix}\end{equation}  
å…¶ä¸­$\otimes$æ˜¯é€ä½å¯¹åº”ç›¸ä¹˜ï¼Œå³Numpyã€Tensorflowç­‰è®¡ç®—æ¡†æ¶ä¸­çš„$*$è¿ç®—ã€‚ä»è¿™ä¸ªå®ç°ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼ŒRoPEå¯ä»¥è§†ä¸ºæ˜¯ä¹˜æ€§ä½ç½®ç¼–ç çš„å˜ä½“ã€‚

## è¿œç¨‹è¡°å‡ #

å¯ä»¥çœ‹åˆ°ï¼ŒRoPEå½¢å¼ä¸Šå’ŒSinusoidalä½ç½®ç¼–ç æœ‰ç‚¹ç›¸ä¼¼ï¼Œåªä¸è¿‡Sinusoidalä½ç½®ç¼–ç æ˜¯åŠ æ€§çš„ï¼Œè€ŒRoPEå¯ä»¥è§†ä¸ºä¹˜æ€§çš„ã€‚åœ¨$\theta_i$çš„é€‰æ‹©ä¸Šï¼Œæˆ‘ä»¬åŒæ ·æ²¿ç”¨äº†Sinusoidalä½ç½®ç¼–ç çš„æ–¹æ¡ˆï¼Œå³$\theta_i = 10000^{-2i/d}$ï¼Œå®ƒå¯ä»¥å¸¦æ¥ä¸€å®šçš„è¿œç¨‹è¡°å‡æ€§ã€‚

å…·ä½“è¯æ˜å¦‚ä¸‹ï¼šå°†$\boldsymbol{q},\boldsymbol{k}$ä¸¤ä¸¤åˆ†ç»„åï¼Œå®ƒä»¬åŠ ä¸ŠRoPEåçš„å†…ç§¯å¯ä»¥ç”¨å¤æ•°ä¹˜æ³•è¡¨ç¤ºä¸º  
\begin{equation}  
(\boldsymbol{\mathcal{R}}_m \boldsymbol{q})^{\top}(\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \text{Re}\left[\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i}\right]\end{equation}  
è®°$h_i = \boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^*, S_j = \sum\limits_{i=0}^{j-1} e^{\text{i}(m-n)\theta_i}$ï¼Œå¹¶çº¦å®š$h_{d/2}=0,S_0=0$ï¼Œé‚£ä¹ˆç”±[Abelå˜æ¢ï¼ˆåˆ†éƒ¨æ±‚å’Œæ³•ï¼‰](https://zh.wikipedia.org/wiki/%E5%88%86%E9%83%A8%E6%B1%82%E5%92%8C%E6%B3%95)å¯ä»¥å¾—åˆ°ï¼š  
\begin{equation}\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i} = \sum_{i=0}^{d/2-1} h_i (S_{i  
+1} - S_i) = -\sum_{i=0}^{d/2-1} S_{i+1}(h_{i+1} - h_i)\end{equation}  
æ‰€ä»¥  
\begin{equation}\begin{aligned}  
\left|\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i}\right| =&\, \left|\sum_{i=0}^{d/2-1} S_{i+1}(h_{i+1} - h_i)\right| \\\  
\leq&\, \sum_{i=0}^{d/2-1} |S_{i+1}| |h_{i+1} - h_i| \\\  
\leq&\, \left(\max_i |h_{i+1} - h_i|\right)\sum_{i=0}^{d/2-1} |S_{i+1}|  
\end{aligned}\end{equation}  
å› æ­¤æˆ‘ä»¬å¯ä»¥è€ƒå¯Ÿ$\frac{1}{d/2}\sum\limits_{i=1}^{d/2} |S_i|$éšç€ç›¸å¯¹è·ç¦»çš„å˜åŒ–æƒ…å†µæ¥ä½œä¸ºè¡°å‡æ€§çš„ä½“ç°ï¼ŒMathematicaä»£ç å¦‚ä¸‹ï¼š
    
    
    d = 128;
    \[Theta][t_] = 10000^(-2*t/d);
    f[m_] = Sum[
        Norm[Sum[Exp[I*m*\[Theta][i]], {i, 0, j}]], {j, 0, d/2 - 1}]/(d/2);
    Plot[f[m], {m, 0, 256}, AxesLabel -> {ç›¸å¯¹è·ç¦», ç›¸å¯¹å¤§å°}]

ç»“æœå¦‚ä¸‹å›¾ï¼š  


[![RoPEçš„è¿œç¨‹è¡°å‡æ€§ï¼ˆd=128ï¼‰](/usr/uploads/2021/03/1347893165.png)](/usr/uploads/2021/03/1347893165.png "ç‚¹å‡»æŸ¥çœ‹åŸå›¾")

RoPEçš„è¿œç¨‹è¡°å‡æ€§ï¼ˆd=128ï¼‰

ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥å¯ä»¥çœ‹åˆ°éšç€ç›¸å¯¹è·ç¦»çš„å˜å¤§ï¼Œå†…ç§¯ç»“æœæœ‰è¡°å‡è¶‹åŠ¿çš„å‡ºç°ã€‚å› æ­¤ï¼Œé€‰æ‹©$\theta_i = 10000^{-2i/d}$ï¼Œç¡®å®èƒ½å¸¦æ¥ä¸€å®šçš„è¿œç¨‹è¡°å‡æ€§ã€‚å½“ç„¶ï¼ŒåŒä¸Šä¸€ç¯‡æ–‡ç« è¯´çš„ä¸€æ ·ï¼Œèƒ½å¸¦æ¥è¿œç¨‹è¡°å‡æ€§çš„ä¸æ­¢è¿™ä¸ªé€‰æ‹©ï¼Œå‡ ä¹ä»»æ„çš„å…‰æ»‘å•è°ƒå‡½æ•°éƒ½å¯ä»¥ï¼Œè¿™é‡Œåªæ˜¯æ²¿ç”¨äº†å·²æœ‰çš„é€‰æ‹©è€Œå·²ã€‚ç¬”è€…è¿˜è¯•è¿‡ä»¥$\theta_i = 10000^{-2i/d}$ä¸ºåˆå§‹åŒ–ï¼Œå°†$\theta_i$è§†ä¸ºå¯è®­ç»ƒå‚æ•°ï¼Œç„¶åè®­ç»ƒä¸€æ®µæ—¶é—´åå‘ç°$\theta_i$å¹¶æ²¡æœ‰æ˜¾è‘—æ›´æ–°ï¼Œå› æ­¤å¹²è„†å°±ç›´æ¥å›ºå®š$\theta_i = 10000^{-2i/d}$äº†ã€‚

## çº¿æ€§åœºæ™¯ #

æœ€åï¼Œæˆ‘ä»¬æŒ‡å‡ºï¼ŒRoPEæ˜¯ç›®å‰å”¯ä¸€ä¸€ç§å¯ä»¥ç”¨äºçº¿æ€§Attentionçš„ç›¸å¯¹ä½ç½®ç¼–ç ã€‚è¿™æ˜¯å› ä¸ºå…¶ä»–çš„ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œéƒ½æ˜¯ç›´æ¥åŸºäºAttentionçŸ©é˜µè¿›è¡Œæ“ä½œçš„ï¼Œä½†æ˜¯çº¿æ€§Attentionå¹¶æ²¡æœ‰äº‹å…ˆç®—å‡ºAttentionçŸ©é˜µï¼Œå› æ­¤ä¹Ÿå°±ä¸å­˜åœ¨æ“ä½œAttentionçŸ©é˜µçš„åšæ³•ï¼Œæ‰€ä»¥å…¶ä»–çš„æ–¹æ¡ˆæ— æ³•åº”ç”¨åˆ°çº¿æ€§Attentionä¸­ã€‚è€Œå¯¹äºRoPEæ¥è¯´ï¼Œå®ƒæ˜¯ç”¨ç»å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼æ¥å®ç°ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œä¸éœ€è¦æ“ä½œAttentionçŸ©é˜µï¼Œå› æ­¤æœ‰äº†åº”ç”¨åˆ°çº¿æ€§Attentionçš„å¯èƒ½æ€§ã€‚

å…³äºçº¿æ€§Attentionçš„ä»‹ç»ï¼Œè¿™é‡Œä¸å†é‡å¤ï¼Œæœ‰éœ€è¦çš„è¯»è€…è¯·å‚è€ƒ[ã€Šçº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿã€‹](/archives/7546)ã€‚çº¿æ€§Attentionçš„å¸¸è§å½¢å¼æ˜¯ï¼š  
\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})_i = \frac{\sum\limits_{j=1}^n \text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)\boldsymbol{v}_j}{\sum\limits_{j=1}^n \text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j)} = \frac{\sum\limits_{j=1}^n \phi(\boldsymbol{q}_i)^{\top} \varphi(\boldsymbol{k}_j)\boldsymbol{v}_j}{\sum\limits_{j=1}^n \phi(\boldsymbol{q}_i)^{\top} \varphi(\boldsymbol{k}_j)}\end{equation}  
å…¶ä¸­$\phi,\varphi$æ˜¯å€¼åŸŸéè´Ÿçš„æ¿€æ´»å‡½æ•°ã€‚å¯ä»¥çœ‹åˆ°ï¼Œçº¿æ€§Attentionä¹Ÿæ˜¯åŸºäºå†…ç§¯çš„ï¼Œæ‰€ä»¥å¾ˆè‡ªç„¶çš„æƒ³æ³•æ˜¯å¯ä»¥å°†RoPEæ’å…¥åˆ°å†…ç§¯ä¸­ï¼š  
\begin{equation}\frac{\sum\limits_{j=1}^n [\boldsymbol{\mathcal{R}}_i\phi(\boldsymbol{q}_i)]^{\top} [\boldsymbol{\mathcal{R}}_j\varphi(\boldsymbol{k}_j)]\boldsymbol{v}_j}{\sum\limits_{j=1}^n [\boldsymbol{\mathcal{R}}_i\phi(\boldsymbol{q}_i)]^{\top} [\boldsymbol{\mathcal{R}}_j\varphi(\boldsymbol{k}_j)]}\end{equation}  
ä½†è¿™æ ·å­˜åœ¨çš„é—®é¢˜æ˜¯ï¼Œå†…ç§¯$[\boldsymbol{\mathcal{R}}_i\phi(\boldsymbol{q}_i)]^{\top} [\boldsymbol{\mathcal{R}}_j\varphi(\boldsymbol{k}_j)]$å¯èƒ½ä¸ºè´Ÿæ•°ï¼Œå› æ­¤å®ƒä¸å†æ˜¯å¸¸è§„çš„æ¦‚ç‡æ³¨æ„åŠ›ï¼Œè€Œä¸”åˆ†æ¯æœ‰ä¸º0çš„é£é™©ï¼Œå¯èƒ½ä¼šå¸¦æ¥ä¼˜åŒ–ä¸Šçš„ä¸ç¨³å®šã€‚è€ƒè™‘åˆ°$\boldsymbol{\mathcal{R}}_i,\boldsymbol{\mathcal{R}}_j$éƒ½æ˜¯æ­£äº¤çŸ©é˜µï¼Œå®ƒä¸æ”¹å˜å‘é‡çš„æ¨¡é•¿ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æŠ›å¼ƒå¸¸è§„çš„æ¦‚ç‡å½’ä¸€åŒ–è¦æ±‚ï¼Œä½¿ç”¨å¦‚ä¸‹è¿ç®—ä½œä¸ºä¸€ç§æ–°çš„çº¿æ€§Attentionï¼š  
\begin{equation}\frac{\sum\limits_{j=1}^n [\boldsymbol{\mathcal{R}}_i\phi(\boldsymbol{q}_i)]^{\top} [\boldsymbol{\mathcal{R}}_j\varphi(\boldsymbol{k}_j)]\boldsymbol{v}_j}{\sum\limits_{j=1}^n \phi(\boldsymbol{q}_i)^{\top} \varphi(\boldsymbol{k}_j)}\end{equation}  
ä¹Ÿå°±æ˜¯è¯´ï¼ŒRoPEåªæ’å…¥åˆ†å­ä¸­ï¼Œè€Œåˆ†æ¯åˆ™ä¸æ”¹å˜ï¼Œè¿™æ ·çš„æ³¨æ„åŠ›ä¸å†æ˜¯åŸºäºæ¦‚ç‡çš„ï¼ˆæ³¨æ„åŠ›çŸ©é˜µä¸å†æ»¡è¶³éè´Ÿå½’ä¸€æ€§ï¼‰ï¼Œä½†å®ƒæŸç§æ„ä¹‰ä¸Šæ¥è¯´ä¹Ÿæ˜¯ä¸€ä¸ªå½’ä¸€åŒ–æ–¹æ¡ˆï¼Œè€Œä¸”ä¹Ÿæ²¡æœ‰è¯æ®è¡¨æ˜éæ¦‚ç‡å¼çš„æ³¨æ„åŠ›å°±ä¸å¥½ï¼ˆæ¯”å¦‚[NystrÃ¶mformer](/archives/8180)ä¹Ÿç®—æ˜¯æ²¡æœ‰ä¸¥æ ¼ä¾æ®æ¦‚ç‡åˆ†å¸ƒçš„æ–¹å¼æ„å»ºæ³¨æ„åŠ›ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å®ƒä½œä¸ºå€™é€‰æ–¹æ¡ˆä¹‹ä¸€è¿›è¡Œå®éªŒï¼Œè€Œæˆ‘ä»¬åˆæ­¥çš„å®éªŒç»“æœæ˜¾ç¤ºè¿™æ ·çš„çº¿æ€§Attentionä¹Ÿæ˜¯æœ‰æ•ˆçš„ã€‚

æ­¤å¤–ï¼Œç¬”è€…åœ¨[ã€Šçº¿æ€§Attentionçš„æ¢ç´¢ï¼šAttentionå¿…é¡»æœ‰ä¸ªSoftmaxå—ï¼Ÿã€‹](/archives/7546)ä¸­è¿˜æå‡ºè¿‡å¦å¤–ä¸€ç§çº¿æ€§Attentionæ–¹æ¡ˆï¼š$\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = 1 + \left( \frac{\boldsymbol{q}_i}{\Vert \boldsymbol{q}_i\Vert}\right)^{\top}\left(\frac{\boldsymbol{k}_j}{\Vert \boldsymbol{k}_j\Vert}\right)$ï¼Œå®ƒä¸ä¾èµ–äºå€¼åŸŸçš„éè´Ÿæ€§ï¼Œè€ŒRoPEä¹Ÿä¸æ”¹å˜æ¨¡é•¿ï¼Œå› æ­¤RoPEå¯ä»¥ç›´æ¥åº”ç”¨äºæ­¤ç±»çº¿æ€§Attentionï¼Œå¹¶ä¸”ä¸æ”¹å˜å®ƒçš„æ¦‚ç‡æ„ä¹‰ã€‚

## æ¨¡å‹å¼€æº #

RoFormerçš„ç¬¬ä¸€ç‰ˆæ¨¡å‹ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆè®­ç»ƒå¹¶å¼€æºåˆ°äº†Githubä¸­ï¼š

> **RoFormerï¼š<https://github.com/ZhuiyiTechnology/roformer>**

ç®€å•æ¥è¯´ï¼ŒRoFormeræ˜¯ä¸€ä¸ªç»å¯¹ä½ç½®ç¼–ç æ›¿æ¢ä¸ºRoPEçš„[WoBERT](https://github.com/ZhuiyiTechnology/WoBERT)æ¨¡å‹ï¼Œå®ƒè·Ÿå…¶ä»–æ¨¡å‹çš„ç»“æ„å¯¹æ¯”å¦‚ä¸‹ï¼š  
\begin{array}{c|cccc}  
\hline  
& \text{BERT} & \text{WoBERT} & \text{NEZHA} & \text{RoFormer} \\\  
\hline  
\text{tokenå•ä½} & \text{å­—} & \text{è¯} & \text{å­—} & \text{è¯} & \\\  
\text{ä½ç½®ç¼–ç } & \text{ç»å¯¹ä½ç½®} & \text{ç»å¯¹ä½ç½®} & \text{ç»å…¸å¼ç›¸å¯¹ä½ç½®} & \text{RoPE}\\\  
\hline  
\end{array}  
åœ¨é¢„è®­ç»ƒä¸Šï¼Œæˆ‘ä»¬ä»¥WoBERT Plusä¸ºåŸºç¡€ï¼Œé‡‡ç”¨äº†å¤šä¸ªé•¿åº¦å’Œbatch sizeäº¤æ›¿è®­ç»ƒçš„æ–¹å¼ï¼Œè®©æ¨¡å‹èƒ½æå‰é€‚åº”ä¸åŒçš„è®­ç»ƒåœºæ™¯ï¼š  
\begin{array}{c|ccccc}  
\hline  
& \text{maxlen} & \text{batch size} & \text{è®­ç»ƒæ­¥æ•°} & \text{æœ€ç»ˆloss} & \text{æœ€ç»ˆacc}\\\  
\hline  
1 & 512 & 256 & 20\text{ä¸‡} & 1.73 & 65.0\%\\\  
2 & 1536 & 256 & 1.25\text{ä¸‡} & 1.61 & 66.8\%\\\  
3 & 256 & 256 & 12\text{ä¸‡} & 1.75 & 64.6\%\\\  
4 & 128 & 512 & 8\text{ä¸‡} & 1.83 & 63.4\%\\\  
5 & 1536 & 256 & 1\text{ä¸‡} & 1.58 & 67.4\%\\\  
6 & 512 & 512 & 3\text{ä¸‡} & 1.66 & 66.2\%\\\  
\hline  
\end{array}  
ä»è¡¨æ ¼è¿˜å¯ä»¥çœ‹åˆ°ï¼Œå¢å¤§åºåˆ—é•¿åº¦ï¼Œé¢„è®­ç»ƒçš„å‡†ç¡®ç‡åè€Œæœ‰æ‰€æå‡ï¼Œè¿™ä¾§é¢ä½“ç°äº†RoFormeré•¿æ–‡æœ¬è¯­ä¹‰çš„å¤„ç†æ•ˆæœï¼Œä¹Ÿä½“ç°äº†RoPEå…·æœ‰è‰¯å¥½çš„å¤–æ¨èƒ½åŠ›ã€‚åœ¨çŸ­æ–‡æœ¬ä»»åŠ¡ä¸Šï¼ŒRoFormerä¸WoBERTçš„è¡¨ç°ç±»ä¼¼ï¼ŒRoFormerçš„ä¸»è¦ç‰¹ç‚¹æ˜¯å¯ä»¥ç›´æ¥å¤„ç†ä»»æ„é•¿çš„æ–‡æœ¬ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬åœ¨[CAIL2019-SCM](https://papers.cool/arxiv/1911.08962)ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœï¼š  
\begin{array}{c|cc}  
\hline  
& \text{éªŒè¯é›†} & \text{æµ‹è¯•é›†} \\\  
\hline  
\text{BERT-512} & 64.13\% & 67.77\% \\\  
\text{WoBERT-512} & 64.07\% & 68.10\% \\\  
\text{RoFormer-512} & 64.13\% & 68.29\% \\\  
\text{RoFormer-1024} & \textbf{66.07%} & \textbf{69.79%} \\\  
\hline  
\end{array}  
å…¶ä¸­$\text{-}$åé¢çš„å‚æ•°æ˜¯å¾®è°ƒæ—¶æˆªæ–­çš„maxlenï¼Œå¯ä»¥çœ‹åˆ°RoFormerç¡®å®èƒ½è¾ƒå¥½åœ°å¤„ç†é•¿æ–‡æœ¬è¯­ä¹‰ï¼Œè‡³äºè®¾å¤‡è¦æ±‚ï¼Œåœ¨24Gæ˜¾å­˜çš„å¡ä¸Šè·‘maxlen=1024ï¼Œbatch_sizeå¯ä»¥è·‘åˆ°8ä»¥ä¸Šã€‚ç›®å‰ä¸­æ–‡ä»»åŠ¡ä¸­ç¬”è€…ä¹Ÿå°±æ‰¾åˆ°è¿™ä¸ªä»»åŠ¡æ¯”è¾ƒé€‚åˆä½œä¸ºé•¿æ–‡æœ¬èƒ½åŠ›çš„æµ‹è¯•ï¼Œæ‰€ä»¥é•¿æ–‡æœ¬æ–¹é¢åªæµ‹äº†è¿™ä¸ªä»»åŠ¡ï¼Œæ¬¢è¿è¯»è€…è¿›è¡Œæµ‹è¯•æˆ–æ¨èå…¶ä»–è¯„æµ‹ä»»åŠ¡ã€‚

å½“ç„¶ï¼Œå°½ç®¡ç†è®ºä¸ŠRoFormerèƒ½å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—ï¼Œä½†ç›®å‰RoFormerè¿˜æ˜¯å…·æœ‰å¹³æ–¹å¤æ‚åº¦çš„ï¼Œæˆ‘ä»¬ä¹Ÿæ­£åœ¨è®­ç»ƒåŸºäºçº¿æ€§Attentionçš„RoFormeræ¨¡å‹ï¼Œå®éªŒå®Œæˆåä¹Ÿä¼šå¼€æºæ”¾å‡ºï¼Œè¯·å¤§å®¶æœŸå¾…ã€‚

ï¼ˆæ³¨ï¼šRoPEå’ŒRoFormerå·²ç»æ•´ç†æˆæ–‡[ã€ŠRoFormer: Enhanced Transformer with Rotary Position Embeddingã€‹](https://papers.cool/arxiv/2104.09864)æäº¤åˆ°äº†Arxivï¼Œæ¬¢è¿ä½¿ç”¨å’Œå¼•ç”¨å“ˆå“ˆï½ï¼‰

## æ–‡ç« å°ç»“ #

æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬è‡ªç ”çš„æ—‹è½¬å¼ä½ç½®ç¼–ç RoPEä»¥åŠå¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹RoFormerã€‚ä»ç†è®ºä¸Šæ¥çœ‹ï¼ŒRoPEä¸Sinusoidalä½ç½®ç¼–ç æœ‰äº›ç›¸é€šä¹‹å¤„ï¼Œä½†RoPEä¸ä¾èµ–äºæ³°å‹’å±•å¼€ï¼Œæ›´å…·ä¸¥è°¨æ€§ä¸å¯è§£é‡Šæ€§ï¼›ä»é¢„è®­ç»ƒæ¨¡å‹RoFormerçš„ç»“æœæ¥çœ‹ï¼ŒRoPEå…·æœ‰è‰¯å¥½çš„å¤–æ¨æ€§ï¼Œåº”ç”¨åˆ°Transformerä¸­ä½“ç°å‡ºè¾ƒå¥½çš„å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒRoPEè¿˜æ˜¯ç›®å‰å”¯ä¸€ä¸€ç§å¯ç”¨äºçº¿æ€§Attentionçš„ç›¸å¯¹ä½ç½®ç¼–ç ã€‚

_**è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€ï¼š**<https://spaces.ac.cn/archives/8265>_

_**æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒï¼š**_[ã€Šç§‘å­¦ç©ºé—´FAQã€‹](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "ã€Šç§‘å­¦ç©ºé—´FAQã€‹")

**å¦‚æœæ‚¨è¿˜æœ‰ä»€ä¹ˆç–‘æƒ‘æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹è¯„è®ºåŒºç»§ç»­è®¨è®ºã€‚**

**å¦‚æœæ‚¨è§‰å¾—æœ¬æ–‡è¿˜ä¸é”™ï¼Œæ¬¢è¿åˆ†äº«/æ‰“èµæœ¬æ–‡ã€‚æ‰“èµå¹¶éè¦ä»ä¸­è·å¾—æ”¶ç›Šï¼Œè€Œæ˜¯å¸Œæœ›çŸ¥é“ç§‘å­¦ç©ºé—´è·å¾—äº†å¤šå°‘è¯»è€…çš„çœŸå¿ƒå…³æ³¨ã€‚å½“ç„¶ï¼Œå¦‚æœä½ æ— è§†å®ƒï¼Œä¹Ÿä¸ä¼šå½±å“ä½ çš„é˜…è¯»ã€‚å†æ¬¡è¡¨ç¤ºæ¬¢è¿å’Œæ„Ÿè°¢ï¼**

æ‰“èµ

![ç§‘å­¦ç©ºé—´](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

å¾®ä¿¡æ‰“èµ

![ç§‘å­¦ç©ºé—´](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

æ”¯ä»˜å®æ‰“èµ

å› ä¸ºç½‘ç«™åå°å¯¹æ‰“èµå¹¶æ— è®°å½•ï¼Œå› æ­¤æ¬¢è¿åœ¨æ‰“èµæ—¶å€™å¤‡æ³¨ç•™è¨€ã€‚ä½ è¿˜å¯ä»¥[**ç‚¹å‡»è¿™é‡Œ**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)æˆ–åœ¨ä¸‹æ–¹è¯„è®ºåŒºç•™è¨€æ¥å‘ŠçŸ¥ä½ çš„å»ºè®®æˆ–éœ€æ±‚ã€‚

**å¦‚æœæ‚¨éœ€è¦å¼•ç”¨æœ¬æ–‡ï¼Œè¯·å‚è€ƒï¼š**

è‹å‰‘æ—. (Mar. 23, 2021). ã€ŠTransformerå‡çº§ä¹‹è·¯ï¼š2ã€åšé‡‡ä¼—é•¿çš„æ—‹è½¬å¼ä½ç½®ç¼–ç  ã€‹[Blog post]. Retrieved from <https://spaces.ac.cn/archives/8265>

@online{kexuefm-8265,  
title={Transformerå‡çº§ä¹‹è·¯ï¼š2ã€åšé‡‡ä¼—é•¿çš„æ—‹è½¬å¼ä½ç½®ç¼–ç },  
author={è‹å‰‘æ—},  
year={2021},  
month={Mar},  
url={\url{https://spaces.ac.cn/archives/8265}},  
} 


---

## ğŸ“ ç¬¬1éƒ¨åˆ†ï¼šç†è®ºåŸºç¡€ä¸å†å²å‘å±•

### 1.1 ä½ç½®ç¼–ç çš„æ¼”åŒ–å²

Transformeræ¨¡å‹è‡ª2017å¹´è¯ç”Ÿä»¥æ¥ï¼Œä½ç½®ç¼–ç ä¸€ç›´æ˜¯å…¶æ ¸å¿ƒç»„æˆéƒ¨åˆ†ä¹‹ä¸€ã€‚è®©æˆ‘ä»¬å›é¡¾ä½ç½®ç¼–ç çš„æ¼”åŒ–å†ç¨‹ï¼š

**ç¬¬ä¸€ä»£ï¼šç»å¯¹ä½ç½®ç¼–ç **
- **Learned Positional Embedding**ï¼ˆTransformeråŸè®ºæ–‡ï¼‰ï¼šä¸ºæ¯ä¸ªä½ç½®å­¦ä¹ ä¸€ä¸ªç‹¬ç«‹çš„å‘é‡
  - ä¼˜ç‚¹ï¼šçµæ´»ï¼Œæ¨¡å‹å¯ä»¥è‡ªç”±å­¦ä¹ ä½ç½®è¡¨ç¤º
  - ç¼ºç‚¹ï¼šæ— æ³•å¤„ç†è¶…å‡ºè®­ç»ƒé•¿åº¦çš„åºåˆ—ï¼Œå‚æ•°é‡å¤§ï¼ˆ$O(L \times d)$ï¼Œ$L$ä¸ºæœ€å¤§åºåˆ—é•¿åº¦ï¼‰
- **Sinusoidal Positional Encoding**ï¼ˆä¹Ÿåœ¨TransformeråŸè®ºæ–‡ä¸­æå‡ºï¼‰ï¼šä½¿ç”¨æ­£å¼¦ä½™å¼¦å‡½æ•°ç¼–ç ä½ç½®
  - ä¼˜ç‚¹ï¼šå‚æ•°é‡ä¸º0ï¼Œç†è®ºä¸Šå¯ä»¥å¤–æ¨åˆ°ä»»æ„é•¿åº¦
  - ç¼ºç‚¹ï¼šä»…æ˜¯"æƒ³è¦æˆä¸ºç›¸å¯¹ä½ç½®ç¼–ç "ï¼Œå®é™…ä¸Šå¹¶æœªæ˜¾å¼å»ºæ¨¡ç›¸å¯¹ä½ç½®

**ç¬¬äºŒä»£ï¼šæ˜¾å¼ç›¸å¯¹ä½ç½®ç¼–ç **
- **T5 Relative PE**ï¼ˆRaffel et al., 2019ï¼‰ï¼šç›´æ¥åœ¨AttentionçŸ©é˜µä¸­åŠ å…¥ç›¸å¯¹ä½ç½®åç½®
  - å½¢å¼ï¼š$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + B\right)V$
  - å…¶ä¸­$B_{ij}$åªä¾èµ–äº$i-j$ï¼ˆç›¸å¯¹ä½ç½®ï¼‰
  - ä¼˜ç‚¹ï¼šç›´æ¥å»ºæ¨¡ç›¸å¯¹ä½ç½®ï¼Œæ€§èƒ½ä¼˜ç§€
  - ç¼ºç‚¹ï¼šéœ€è¦å­˜å‚¨$O(L)$ä¸ªç›¸å¯¹ä½ç½®åç½®ï¼Œæ— æ³•ç”¨äºçº¿æ€§Attention
- **ALiBi**ï¼ˆPress et al., 2021ï¼‰ï¼šçº¿æ€§è¡°å‡çš„ç›¸å¯¹ä½ç½®åç½®
  - å½¢å¼ï¼š$\text{score}(q_i, k_j) = q_i^T k_j - \lambda \cdot |i - j|$
  - ä¼˜ç‚¹ï¼šæç®€ï¼Œå¤–æ¨æ€§å¥½
  - ç¼ºç‚¹ï¼šä»éœ€æ“ä½œAttentionçŸ©é˜µï¼Œæ— æ³•ç”¨äºçº¿æ€§Attention

**ç¬¬ä¸‰ä»£ï¼šRoPEï¼ˆæœ¬æ–‡æ–¹æ³•ï¼‰**
- **æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡ç»å¯¹ä½ç½®ç¼–ç çš„**æ–¹å¼**å®ç°ç›¸å¯¹ä½ç½®ç¼–ç çš„**æ•ˆæœ**
- **å…³é”®åˆ›æ–°**ï¼šç¼–ç ä½œç”¨äº$Q, K$å‘é‡æœ¬èº«ï¼Œè€ŒéAttentionçŸ©é˜µ
- **ç‹¬ç‰¹ä¼˜åŠ¿**ï¼š
  1. ç†è®ºä¸¥è°¨ï¼šæ— éœ€æ³°å‹’å±•å¼€ç­‰è¿‘ä¼¼å‡è®¾
  2. å¤–æ¨æ€§å¼ºï¼šè‡ªç„¶æ”¯æŒä»»æ„é•¿åº¦åºåˆ—
  3. çº¿æ€§å…¼å®¹ï¼šå”¯ä¸€å¯ç”¨äºçº¿æ€§Attentionçš„ç›¸å¯¹ä½ç½®ç¼–ç 
  4. è®¡ç®—é«˜æ•ˆï¼šä¸æ”¹å˜Attentionçš„è®¡ç®—å¤æ‚åº¦

### 1.2 RoPEçš„è®¾è®¡å“²å­¦

RoPEçš„æ ¸å¿ƒæ€æƒ³å¯ä»¥ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼š**ä»¥ç»å¯¹ä¹‹å½¢ï¼Œè¾¾ç›¸å¯¹ä¹‹å®**ã€‚

**è®¾è®¡åŸåˆ™1ï¼šç›¸å¯¹ä½ç½®çš„æ¶Œç°æ€§**

æˆ‘ä»¬å¸Œæœ›Attentionå†…ç§¯è‡ªåŠ¨åŒ…å«ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼š
\begin{equation}
\langle \tilde{\boldsymbol{q}}_m, \tilde{\boldsymbol{k}}_n \rangle = g(\boldsymbol{q}, \boldsymbol{k}, m-n)
\end{equation}

è¿™é‡Œ$\tilde{\boldsymbol{q}}_m, \tilde{\boldsymbol{k}}_n$æ˜¯æ·»åŠ äº†ä½ç½®$m, n$ä¿¡æ¯çš„æŸ¥è¯¢å’Œé”®å‘é‡ã€‚æ³¨æ„å³ä¾§**åªä¾èµ–äºç›¸å¯¹ä½ç½®$m-n$**ï¼Œè€Œéç»å¯¹ä½ç½®$m, n$ã€‚

**ä¸ºä»€ä¹ˆè¿™ä¸ªè®¾è®¡æ˜¯ä¼˜é›…çš„ï¼Ÿ**

1. **æ“ä½œåœ¨å‘é‡å±‚é¢**ï¼šä½ç½®ç¼–ç é€šè¿‡å˜æ¢$\boldsymbol{q}, \boldsymbol{k}$å®ç°ï¼Œæ— éœ€ä¿®æ”¹AttentionçŸ©é˜µ
2. **ç›¸å¯¹ä½ç½®è‡ªåŠ¨æ¶Œç°**ï¼šå†…ç§¯ç»“æœå¤©ç„¶åªä¾èµ–$m-n$ï¼Œæ— éœ€äººå·¥è®¾è®¡ç›¸å¯¹ä½ç½®è®¡ç®—
3. **ä¿æŒçº¿æ€§æ€§**ï¼šå†…ç§¯çš„çº¿æ€§æ€§å¾—ä»¥ä¿ç•™ï¼Œå¯ä»¥åº”ç”¨äºçº¿æ€§Attention

**è®¾è®¡åŸåˆ™2ï¼šæ­£äº¤æ€§ä¿æŒ**

ä½ç½®ç¼–ç å˜æ¢åº”è¯¥æ˜¯æ­£äº¤çš„ï¼Œå³ï¼š
\begin{equation}
\|\tilde{\boldsymbol{q}}_m\| = \|\boldsymbol{q}\|, \quad \|\tilde{\boldsymbol{k}}_n\| = \|\boldsymbol{k}\|
\end{equation}

**ä¸ºä»€ä¹ˆæ­£äº¤æ€§é‡è¦ï¼Ÿ**

1. **ç¨³å®šæ€§**ï¼šä¸æ”¹å˜å‘é‡çš„æ¨¡é•¿ï¼Œä¿æŒæ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§
2. **ä¿¡æ¯ä¿ç•™**ï¼šä½ç½®ç¼–ç æ˜¯ä¿¡æ¯çš„é‡æ–°ç»„ç»‡ï¼Œè€Œéå‹ç¼©æˆ–æ”¾å¤§
3. **æ¢¯åº¦æµåŠ¨**ï¼šæ­£äº¤å˜æ¢çš„é›…å¯æ¯”çŸ©é˜µè¡Œåˆ—å¼ä¸º1ï¼Œæ¢¯åº¦ä¼ æ’­æ›´ç¨³å®š

**è®¾è®¡åŸåˆ™3ï¼šè¿œç¨‹è¡°å‡æ€§**

éšç€ç›¸å¯¹è·ç¦»$|m-n|$å¢å¤§ï¼Œå†…ç§¯ç»“æœåº”è¯¥è¶‹å‘äºè¡°å‡ï¼š
\begin{equation}
|\langle \tilde{\boldsymbol{q}}_m, \tilde{\boldsymbol{k}}_n \rangle| \xrightarrow{|m-n| \to \infty} \text{è¾ƒå°å€¼}
\end{equation}

**ä¸ºä»€ä¹ˆéœ€è¦è¿œç¨‹è¡°å‡ï¼Ÿ**

1. **å±€éƒ¨æ€§å…ˆéªŒ**ï¼šè‡ªç„¶è¯­è¨€å…·æœ‰å±€éƒ¨æ€§ï¼Œè·ç¦»è¿œçš„è¯è¯­ç›¸å…³æ€§é€šå¸¸è¾ƒå¼±
2. **æ³¨æ„åŠ›èšç„¦**ï¼šå¸®åŠ©æ¨¡å‹å°†æ³¨æ„åŠ›é›†ä¸­åœ¨ç›¸å…³çš„å±€éƒ¨åŒºåŸŸ
3. **é•¿åºåˆ—ç¨³å®šæ€§**ï¼šé¿å…é•¿è·ç¦»ä¾èµ–çš„æ¢¯åº¦çˆ†ç‚¸

### 1.3 ä»Sinusoidalåˆ°RoPEçš„è·¨è¶Š

Sinusoidalä½ç½®ç¼–ç çš„æ ¸å¿ƒå…¬å¼ï¼š
\begin{equation}
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d}), \quad PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})
\end{equation}

ç„¶å**åŠ åˆ°**è¾“å…¥å‘é‡ä¸Šï¼š$\boldsymbol{x}_{pos} \leftarrow \boldsymbol{x}_{pos} + PE_{pos}$ã€‚

**Sinusoidalçš„ä¸è¶³**ï¼š
1. **è¿‘ä¼¼æ€§**ï¼šå£°ç§°é€šè¿‡æ³°å‹’å±•å¼€å¯ä»¥è¡¨è¾¾ç›¸å¯¹ä½ç½®ï¼Œä½†ä»…åœ¨$\|\boldsymbol{q}\|, \|\boldsymbol{k}\| \ll 1$æ—¶æˆç«‹
2. **åŠ æ€§ç»“æ„**ï¼š$PE$ä¸å†…å®¹å‘é‡ç›´æ¥ç›¸åŠ ï¼Œç ´åäº†åŸå§‹è¯­ä¹‰ç©ºé—´
3. **å¤–æ¨æ€§æœ‰é™**ï¼šå®è·µä¸­è¶…å‡ºè®­ç»ƒé•¿åº¦åæ€§èƒ½ä¸‹é™æ˜æ˜¾

**RoPEçš„æ”¹è¿›**ï¼š
1. **ä¸¥è°¨æ€§**ï¼šé€šè¿‡æ—‹è½¬å˜æ¢ç²¾ç¡®å®ç°ç›¸å¯¹ä½ç½®ï¼Œæ— éœ€è¿‘ä¼¼
2. **ä¹˜æ€§ç»“æ„**ï¼šæ—‹è½¬çŸ©é˜µ$\boldsymbol{\mathcal{R}}_m$ä½œç”¨äºå‘é‡ï¼Œä¿æŒè¯­ä¹‰ç©ºé—´çš„å‡ ä½•ç»“æ„
3. **å¤–æ¨æ€§å¼º**ï¼šç†è®ºä¸Šæ”¯æŒä»»æ„é•¿åº¦ï¼Œå®è·µéªŒè¯å¤–æ¨æ€§èƒ½ä¼˜å¼‚

### 1.4 æ ¸å¿ƒæ•°å­¦æ¡†æ¶

RoPEçš„æ•°å­¦æ¡†æ¶å»ºç«‹åœ¨ä»¥ä¸‹å…¬ç†åŸºç¡€ä¸Šï¼š

**å…¬ç†1ï¼ˆä½ç½®ç¼–ç çš„å‡½æ•°å½¢å¼ï¼‰**ï¼šå­˜åœ¨å‡½æ•°$\boldsymbol{f}$ï¼Œä½¿å¾—
\begin{equation}
\tilde{\boldsymbol{q}}_m = \boldsymbol{f}(\boldsymbol{q}, m), \quad \tilde{\boldsymbol{k}}_n = \boldsymbol{f}(\boldsymbol{k}, n)
\end{equation}

**å…¬ç†2ï¼ˆç›¸å¯¹ä½ç½®çº¦æŸï¼‰**ï¼šå†…ç§¯ç»“æœä»…ä¾èµ–ç›¸å¯¹ä½ç½®
\begin{equation}
\langle \boldsymbol{f}(\boldsymbol{q}, m), \boldsymbol{f}(\boldsymbol{k}, n) \rangle = g(\boldsymbol{q}, \boldsymbol{k}, m-n)
\end{equation}

**å…¬ç†3ï¼ˆåˆå§‹æ¡ä»¶ï¼‰**ï¼šä½ç½®0å¯¹åº”æ’ç­‰å˜æ¢
\begin{equation}
\boldsymbol{f}(\boldsymbol{q}, 0) = \boldsymbol{q}, \quad \boldsymbol{f}(\boldsymbol{k}, 0) = \boldsymbol{k}
\end{equation}

ä»è¿™ä¸‰ä¸ªå…¬ç†å‡ºå‘ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬2éƒ¨åˆ†æ¨å¯¼å‡ºRoPEçš„å”¯ä¸€å½¢å¼ã€‚

### 1.5 RoPEä¸å…¶ä»–æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«

| æ–¹æ³• | ç¼–ç ä½ç½® | ç›¸å¯¹ä½ç½® | çº¿æ€§Attention | å¤–æ¨æ€§ |
|------|----------|----------|---------------|--------|
| Learned PE | è¾“å…¥å±‚åŠ æ€§ | âœ— | âœ“ | âœ— |
| Sinusoidal PE | è¾“å…¥å±‚åŠ æ€§ | è¿‘ä¼¼ | âœ“ | ä¸­ç­‰ |
| T5 RPE | AttentionçŸ©é˜µ | âœ“ | âœ— | ä¸­ç­‰ |
| ALiBi | AttentionçŸ©é˜µ | âœ“ | âœ— | âœ“ |
| **RoPE** | **Q/Kä¹˜æ€§** | **âœ“** | **âœ“** | **âœ“** |

**å…³é”®æ´å¯Ÿ**ï¼š
- ä¼ ç»Ÿç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆT5, ALiBiï¼‰æ“ä½œAttentionçŸ©é˜µï¼Œæ— æ³•ç”¨äºçº¿æ€§Attention
- RoPEæ“ä½œ$Q, K$å‘é‡ï¼Œä¿æŒäº†å†…ç§¯ç»“æ„ï¼Œå¤©ç„¶å…¼å®¹çº¿æ€§Attention
- è¿™æ˜¯RoPEç›¸æ¯”å…¶ä»–æ–¹æ³•çš„**æ ¹æœ¬ä¼˜åŠ¿**

---

## ğŸ”¬ ç¬¬2éƒ¨åˆ†ï¼šæ•°å­¦æ¨å¯¼ä¸ä¸¥æ ¼è¯æ˜

### 2.1 äºŒç»´æƒ…å†µä¸‹çš„å®Œæ•´æ¨å¯¼

æˆ‘ä»¬å…ˆè€ƒè™‘æœ€ç®€å•çš„äºŒç»´æƒ…å†µï¼ˆ$d=2$ï¼‰ï¼Œåˆ©ç”¨å¤æ•°æ–¹æ³•æ±‚è§£ã€‚

#### 2.1.1 å¤æ•°è¡¨ç¤º

åœ¨äºŒç»´ç©ºé—´ä¸­ï¼Œå‘é‡$\boldsymbol{q} = (q_0, q_1)^T$å¯ä»¥è¡¨ç¤ºä¸ºå¤æ•°ï¼š
\begin{equation}
\boldsymbol{q} \leftrightarrow q_0 + \mathrm{i} q_1
\end{equation}

å†…ç§¯å¯ä»¥ç”¨å¤æ•°ä¹˜æ³•è¡¨ç¤ºï¼š
\begin{equation}
\langle \boldsymbol{q}, \boldsymbol{k} \rangle = q_0 k_0 + q_1 k_1 = \text{Re}[\boldsymbol{q} \cdot \boldsymbol{k}^*]
\end{equation}
å…¶ä¸­$\boldsymbol{k}^* = k_0 - \mathrm{i} k_1$æ˜¯å¤å…±è½­ï¼Œ$\text{Re}[\cdot]$å–å®éƒ¨ã€‚

#### 2.1.2 å…¬ç†è½¬åŒ–ä¸ºå¤æ•°æ–¹ç¨‹

å°†å…¬ç†2ç”¨å¤æ•°å½¢å¼æ”¹å†™ï¼š
\begin{equation}
\text{Re}[\boldsymbol{f}(\boldsymbol{q}, m) \cdot \boldsymbol{f}^*(\boldsymbol{k}, n)] = g(\boldsymbol{q}, \boldsymbol{k}, m-n)
\end{equation}

ç®€åŒ–å‡è®¾ï¼šå­˜åœ¨å¤å‡½æ•°$\boldsymbol{g}$ï¼Œä½¿å¾—
\begin{equation}
\boldsymbol{f}(\boldsymbol{q}, m) \cdot \boldsymbol{f}^*(\boldsymbol{k}, n) = \boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n)
\end{equation}
åˆ™$g = \text{Re}[\boldsymbol{g}]$ã€‚

#### 2.1.3 æåæ ‡åˆ†è§£

ç”¨å¤æ•°çš„æåæ ‡å½¢å¼ï¼ˆæ¬§æ‹‰å…¬å¼ï¼‰ï¼š
\begin{equation}
z = r e^{\mathrm{i}\theta} = r(\cos\theta + \mathrm{i}\sin\theta)
\end{equation}

è®¾ï¼š
\begin{align}
\boldsymbol{f}(\boldsymbol{q}, m) &= R_f(\boldsymbol{q}, m) e^{\mathrm{i}\Theta_f(\boldsymbol{q}, m)} \\
\boldsymbol{f}(\boldsymbol{k}, n) &= R_f(\boldsymbol{k}, n) e^{\mathrm{i}\Theta_f(\boldsymbol{k}, n)} \\
\boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) &= R_g(\boldsymbol{q}, \boldsymbol{k}, m-n) e^{\mathrm{i}\Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n)}
\end{align}

è¿™é‡Œ$R$æ˜¯æ¨¡ï¼ˆéè´Ÿå®æ•°ï¼‰ï¼Œ$\Theta$æ˜¯å¹…è§’ï¼ˆå®æ•°ï¼‰ã€‚

#### 2.1.4 åˆ†ç¦»æ¨¡å’Œå¹…è§’

ä»£å…¥ä¸»æ–¹ç¨‹ï¼š
\begin{equation}
R_f(\boldsymbol{q}, m) e^{\mathrm{i}\Theta_f(\boldsymbol{q}, m)} \cdot R_f(\boldsymbol{k}, n) e^{-\mathrm{i}\Theta_f(\boldsymbol{k}, n)} = R_g(\boldsymbol{q}, \boldsymbol{k}, m-n) e^{\mathrm{i}\Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n)}
\end{equation}

åˆ©ç”¨$e^{\mathrm{i}a} \cdot e^{\mathrm{i}b} = e^{\mathrm{i}(a+b)}$ï¼Œå¾—åˆ°ï¼š
\begin{equation}
R_f(\boldsymbol{q}, m) R_f(\boldsymbol{k}, n) e^{\mathrm{i}[\Theta_f(\boldsymbol{q}, m) - \Theta_f(\boldsymbol{k}, n)]} = R_g(\boldsymbol{q}, \boldsymbol{k}, m-n) e^{\mathrm{i}\Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n)}
\end{equation}

å¤æ•°ç›¸ç­‰å½“ä¸”ä»…å½“æ¨¡å’Œå¹…è§’åˆ†åˆ«ç›¸ç­‰ï¼š
\begin{align}
R_f(\boldsymbol{q}, m) R_f(\boldsymbol{k}, n) &= R_g(\boldsymbol{q}, \boldsymbol{k}, m-n) \tag{æ¨¡æ–¹ç¨‹} \\
\Theta_f(\boldsymbol{q}, m) - \Theta_f(\boldsymbol{k}, n) &= \Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n) \tag{å¹…è§’æ–¹ç¨‹}
\end{align}

#### 2.1.5 æ±‚è§£æ¨¡æ–¹ç¨‹

åœ¨æ¨¡æ–¹ç¨‹ä¸­ä»¤$m = n$ï¼š
\begin{equation}
R_f(\boldsymbol{q}, m) R_f(\boldsymbol{k}, m) = R_g(\boldsymbol{q}, \boldsymbol{k}, 0)
\end{equation}

åˆ©ç”¨åˆå§‹æ¡ä»¶$\boldsymbol{f}(\boldsymbol{q}, 0) = \boldsymbol{q}$ï¼Œæœ‰ï¼š
\begin{equation}
R_f(\boldsymbol{q}, 0) = |\boldsymbol{q}| = \|\boldsymbol{q}\|, \quad R_f(\boldsymbol{k}, 0) = \|\boldsymbol{k}\|
\end{equation}

æ‰€ä»¥ï¼š
\begin{equation}
R_g(\boldsymbol{q}, \boldsymbol{k}, 0) = R_f(\boldsymbol{q}, 0) R_f(\boldsymbol{k}, 0) = \|\boldsymbol{q}\| \|\boldsymbol{k}\|
\end{equation}

ç°åœ¨è€ƒè™‘ä¸€èˆ¬çš„$m$ã€‚ä¸€ä¸ªç®€å•çš„è§£æ˜¯ï¼š
\begin{equation}
R_f(\boldsymbol{q}, m) = \|\boldsymbol{q}\|, \quad R_f(\boldsymbol{k}, m) = \|\boldsymbol{k}\|
\end{equation}
å³**æ¨¡ä¸ä½ç½®$m$æ— å…³**ã€‚éªŒè¯ï¼š
\begin{equation}
R_f(\boldsymbol{q}, m) R_f(\boldsymbol{k}, n) = \|\boldsymbol{q}\| \|\boldsymbol{k}\| = R_g(\boldsymbol{q}, \boldsymbol{k}, 0) = R_g(\boldsymbol{q}, \boldsymbol{k}, m-n)
\end{equation}
æœ€åä¸€æ­¥åˆ©ç”¨äº†$R_g$åªä¾èµ–$m-n$ã€‚å½“$m = n$æ—¶æˆç«‹ï¼Œæ¨å¹¿åˆ°ä¸€èˆ¬æƒ…å†µéœ€è¦$R_g$ç¡®å®åªä¾èµ–$m-n$ï¼ˆè¿™æ˜¯æˆ‘ä»¬çš„å‡è®¾ï¼‰ã€‚

**å‡ ä½•æ„ä¹‰**ï¼šä½ç½®ç¼–ç åªæ”¹å˜å‘é‡çš„**æ–¹å‘**ï¼ˆç›¸ä½ï¼‰ï¼Œä¸æ”¹å˜**é•¿åº¦**ï¼ˆæ¨¡ï¼‰ã€‚è¿™æ­£æ˜¯æ—‹è½¬å˜æ¢çš„ç‰¹å¾ï¼

#### 2.1.6 æ±‚è§£å¹…è§’æ–¹ç¨‹

åœ¨å¹…è§’æ–¹ç¨‹ä¸­ä»¤$m = n$ï¼š
\begin{equation}
\Theta_f(\boldsymbol{q}, m) - \Theta_f(\boldsymbol{k}, m) = \Theta_g(\boldsymbol{q}, \boldsymbol{k}, 0)
\end{equation}

åˆ©ç”¨åˆå§‹æ¡ä»¶ï¼š
\begin{equation}
\boldsymbol{f}(\boldsymbol{q}, 0) = \boldsymbol{q} = \|\boldsymbol{q}\| e^{\mathrm{i}\Theta(\boldsymbol{q})}
\end{equation}
å…¶ä¸­$\Theta(\boldsymbol{q})$æ˜¯$\boldsymbol{q}$æœ¬èº«çš„å¹…è§’ã€‚æ‰€ä»¥ï¼š
\begin{equation}
\Theta_f(\boldsymbol{q}, 0) = \Theta(\boldsymbol{q}), \quad \Theta_f(\boldsymbol{k}, 0) = \Theta(\boldsymbol{k})
\end{equation}

ä»è€Œï¼š
\begin{equation}
\Theta_g(\boldsymbol{q}, \boldsymbol{k}, 0) = \Theta_f(\boldsymbol{q}, 0) - \Theta_f(\boldsymbol{k}, 0) = \Theta(\boldsymbol{q}) - \Theta(\boldsymbol{k})
\end{equation}

å›åˆ°ä¸€èˆ¬çš„$m$ï¼Œæœ‰ï¼š
\begin{equation}
\Theta_f(\boldsymbol{q}, m) - \Theta_f(\boldsymbol{k}, m) = \Theta(\boldsymbol{q}) - \Theta(\boldsymbol{k})
\end{equation}

æ•´ç†å¾—ï¼š
\begin{equation}
\Theta_f(\boldsymbol{q}, m) - \Theta(\boldsymbol{q}) = \Theta_f(\boldsymbol{k}, m) - \Theta(\boldsymbol{k})
\end{equation}

**å…³é”®è§‚å¯Ÿ**ï¼šå·¦è¾¹åªä¾èµ–$\boldsymbol{q}, m$ï¼Œå³è¾¹åªä¾èµ–$\boldsymbol{k}, m$ã€‚ä¸¤è€…ç›¸ç­‰æ„å‘³ç€å®ƒä»¬éƒ½ç­‰äºæŸä¸ª**åªä¾èµ–$m$**çš„å‡½æ•°$\varphi(m)$ï¼š
\begin{equation}
\Theta_f(\boldsymbol{q}, m) = \Theta(\boldsymbol{q}) + \varphi(m)
\end{equation}

#### 2.1.7 ç¡®å®š$\varphi(m)$çš„å½¢å¼

åœ¨åŸå¹…è§’æ–¹ç¨‹ä¸­ä»£å…¥ä¸Šå¼ï¼š
\begin{equation}
[\Theta(\boldsymbol{q}) + \varphi(m)] - [\Theta(\boldsymbol{k}) + \varphi(n)] = \Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n)
\end{equation}

ç®€åŒ–ï¼š
\begin{equation}
\varphi(m) - \varphi(n) = \Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n) - [\Theta(\boldsymbol{q}) - \Theta(\boldsymbol{k})]
\end{equation}

ä»¤$n = m - 1$ï¼ˆç›¸é‚»ä½ç½®ï¼‰ï¼š
\begin{equation}
\varphi(m) - \varphi(m-1) = \Theta_g(\boldsymbol{q}, \boldsymbol{k}, 1) - [\Theta(\boldsymbol{q}) - \Theta(\boldsymbol{k})]
\end{equation}

å³ä¾§ä¸$m$æ— å…³ï¼è®°ä¸ºå¸¸æ•°$\theta$ï¼š
\begin{equation}
\varphi(m) - \varphi(m-1) = \theta
\end{equation}

è¿™æ˜¯**ç­‰å·®æ•°åˆ—**çš„é€’æ¨å…³ç³»ã€‚åˆ©ç”¨åˆå§‹æ¡ä»¶$\varphi(0) = 0$ï¼ˆå› ä¸º$\Theta_f(\boldsymbol{q}, 0) = \Theta(\boldsymbol{q})$ï¼‰ï¼Œè§£å¾—ï¼š
\begin{equation}
\varphi(m) = m\theta
\end{equation}

#### 2.1.8 äºŒç»´RoPEçš„æœ€ç»ˆå½¢å¼

ç»¼åˆæ¨¡å’Œå¹…è§’çš„ç»“æœï¼š
\begin{align}
\boldsymbol{f}(\boldsymbol{q}, m) &= R_f(\boldsymbol{q}, m) e^{\mathrm{i}\Theta_f(\boldsymbol{q}, m)} \\
&= \|\boldsymbol{q}\| e^{\mathrm{i}[\Theta(\boldsymbol{q}) + m\theta]} \\
&= \|\boldsymbol{q}\| e^{\mathrm{i}\Theta(\boldsymbol{q})} \cdot e^{\mathrm{i}m\theta} \\
&= \boldsymbol{q} \cdot e^{\mathrm{i}m\theta}
\end{align}

ç”¨å¤æ•°ä¹˜æ³•çš„å‡ ä½•æ„ä¹‰ï¼Œ$e^{\mathrm{i}m\theta}$å¯¹åº”é€†æ—¶é’ˆæ—‹è½¬$m\theta$è§’åº¦ã€‚ç”¨çŸ©é˜µå½¢å¼è¡¨ç¤ºï¼š
\begin{equation}
\boldsymbol{f}(\boldsymbol{q}, m) = \begin{pmatrix} \cos m\theta & -\sin m\theta \\ \sin m\theta & \cos m\theta \end{pmatrix} \begin{pmatrix} q_0 \\ q_1 \end{pmatrix}
\end{equation}

è¿™å°±æ˜¯**äºŒç»´æ—‹è½¬çŸ©é˜µ**ï¼è®°ä¸º$\boldsymbol{\mathcal{R}}_m^{(2D)}$ã€‚

#### 2.1.9 éªŒè¯ç›¸å¯¹ä½ç½®æ€§è´¨

\begin{align}
\langle \boldsymbol{f}(\boldsymbol{q}, m), \boldsymbol{f}(\boldsymbol{k}, n) \rangle &= \text{Re}[\boldsymbol{q} e^{\mathrm{i}m\theta} \cdot (\boldsymbol{k} e^{\mathrm{i}n\theta})^*] \\
&= \text{Re}[\boldsymbol{q} e^{\mathrm{i}m\theta} \cdot \boldsymbol{k}^* e^{-\mathrm{i}n\theta}] \\
&= \text{Re}[\boldsymbol{q} \boldsymbol{k}^* \cdot e^{\mathrm{i}(m-n)\theta}] \\
&= g(\boldsymbol{q}, \boldsymbol{k}, m-n)
\end{align}

ç¡®å®åªä¾èµ–ç›¸å¯¹ä½ç½®$m-n$ï¼âœ“

### 2.2 é«˜ç»´æ¨å¹¿ï¼šå—å¯¹è§’ç»“æ„

#### 2.2.1 ç»´åº¦é…å¯¹ç­–ç•¥

å¯¹äº$d$ç»´å‘é‡ï¼ˆå‡è®¾$d$ä¸ºå¶æ•°ï¼‰ï¼Œæˆ‘ä»¬å°†å…¶åˆ†æˆ$d/2$å¯¹ï¼š
\begin{equation}
\boldsymbol{q} = \begin{pmatrix} q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1} \end{pmatrix} \rightarrow \begin{pmatrix} (q_0, q_1) \\ (q_2, q_3) \\ \vdots \\ (q_{d-2}, q_{d-1}) \end{pmatrix}
\end{equation}

æ¯ä¸€å¯¹$(q_{2i}, q_{2i+1})$ä½œä¸ºä¸€ä¸ªäºŒç»´å‘é‡ï¼Œåº”ç”¨äºŒç»´RoPEã€‚

#### 2.2.2 å¤šé¢‘ç‡è®¾è®¡

ä¸åŒçš„ç»´åº¦å¯¹ä½¿ç”¨ä¸åŒçš„æ—‹è½¬é¢‘ç‡$\theta_i$ï¼ˆ$i = 0, 1, \ldots, d/2-1$ï¼‰ï¼š
\begin{equation}
\theta_i = \theta_{\text{base}}^{-2i/d}
\end{equation}

åŸæ–‡é€‰æ‹©$\theta_{\text{base}} = 10000$ï¼Œå³$\theta_i = 10000^{-2i/d}$ã€‚

**å¤šé¢‘ç‡çš„æ„ä¹‰**ï¼š
- **ä½é¢‘**ï¼ˆ$i$å°ï¼‰ï¼šæ•æ‰é•¿è·ç¦»ä¾èµ–ï¼Œæ—‹è½¬æ…¢
- **é«˜é¢‘**ï¼ˆ$i$å¤§ï¼‰ï¼šæ•æ‰çŸ­è·ç¦»ä¾èµ–ï¼Œæ—‹è½¬å¿«
- ç±»ä¼¼å‚…é‡Œå¶çº§æ•°çš„å¤šå°ºåº¦è¡¨ç¤º

#### 2.2.3 å—å¯¹è§’æ—‹è½¬çŸ©é˜µ

é«˜ç»´RoPEçŸ©é˜µ$\boldsymbol{\mathcal{R}}_m \in \mathbb{R}^{d \times d}$å…·æœ‰å—å¯¹è§’ç»“æ„ï¼š
\begin{equation}
\boldsymbol{\mathcal{R}}_m = \begin{pmatrix}
\boldsymbol{\mathcal{R}}_m^{(0)} & & & \\
& \boldsymbol{\mathcal{R}}_m^{(1)} & & \\
& & \ddots & \\
& & & \boldsymbol{\mathcal{R}}_m^{(d/2-1)}
\end{pmatrix}
\end{equation}

å…¶ä¸­æ¯ä¸ªå—æ˜¯äºŒç»´æ—‹è½¬çŸ©é˜µï¼š
\begin{equation}
\boldsymbol{\mathcal{R}}_m^{(i)} = \begin{pmatrix}
\cos m\theta_i & -\sin m\theta_i \\
\sin m\theta_i & \cos m\theta_i
\end{pmatrix}
\end{equation}

å±•å¼€ä¸ºç¨€ç–çŸ©é˜µï¼š
\begin{equation}
\boldsymbol{\mathcal{R}}_m = \begin{pmatrix}
\cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots \\
\sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots \\
0 & 0 & \cos m\theta_1 & -\sin m\theta_1 & \cdots \\
0 & 0 & \sin m\theta_1 & \cos m\theta_1 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}
\end{equation}

#### 2.2.4 æ­£äº¤æ€§è¯æ˜

**å®šç†**ï¼š$\boldsymbol{\mathcal{R}}_m$æ˜¯æ­£äº¤çŸ©é˜µï¼Œå³$\boldsymbol{\mathcal{R}}_m^T \boldsymbol{\mathcal{R}}_m = \boldsymbol{I}$ã€‚

**è¯æ˜**ï¼š

å—å¯¹è§’çŸ©é˜µçš„è½¬ç½®æ˜¯å„å—è½¬ç½®çš„å—å¯¹è§’çŸ©é˜µï¼š
\begin{equation}
\boldsymbol{\mathcal{R}}_m^T = \text{diag}((\boldsymbol{\mathcal{R}}_m^{(0)})^T, \ldots, (\boldsymbol{\mathcal{R}}_m^{(d/2-1)})^T)
\end{equation}

å—å¯¹è§’çŸ©é˜µçš„ä¹˜æ³•æ˜¯å„å—åˆ†åˆ«ç›¸ä¹˜ï¼š
\begin{equation}
\boldsymbol{\mathcal{R}}_m^T \boldsymbol{\mathcal{R}}_m = \text{diag}((\boldsymbol{\mathcal{R}}_m^{(0)})^T \boldsymbol{\mathcal{R}}_m^{(0)}, \ldots, (\boldsymbol{\mathcal{R}}_m^{(d/2-1)})^T \boldsymbol{\mathcal{R}}_m^{(d/2-1)})
\end{equation}

å¯¹äºæ¯ä¸ªäºŒç»´æ—‹è½¬çŸ©é˜µï¼š
\begin{align}
(\boldsymbol{\mathcal{R}}_m^{(i)})^T \boldsymbol{\mathcal{R}}_m^{(i)} &= \begin{pmatrix}
\cos m\theta_i & \sin m\theta_i \\
-\sin m\theta_i & \cos m\theta_i
\end{pmatrix} \begin{pmatrix}
\cos m\theta_i & -\sin m\theta_i \\
\sin m\theta_i & \cos m\theta_i
\end{pmatrix} \\
&= \begin{pmatrix}
\cos^2 m\theta_i + \sin^2 m\theta_i & 0 \\
0 & \sin^2 m\theta_i + \cos^2 m\theta_i
\end{pmatrix} \\
&= \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = \boldsymbol{I}_2
\end{align}

æ‰€ä»¥$\boldsymbol{\mathcal{R}}_m^T \boldsymbol{\mathcal{R}}_m = \boldsymbol{I}_d$ã€‚ $\square$

**æ¨è®º**ï¼š$\|\boldsymbol{\mathcal{R}}_m \boldsymbol{q}\| = \|\boldsymbol{q}\|$ï¼ˆä¿æŒå‘é‡æ¨¡é•¿ï¼‰

#### 2.2.5 æ—‹è½¬ç¾¤æ€§è´¨

**å®šç†ï¼ˆç¾¤å°é—­æ€§ï¼‰**ï¼š$\boldsymbol{\mathcal{R}}_m \boldsymbol{\mathcal{R}}_n = \boldsymbol{\mathcal{R}}_{m+n}$

**è¯æ˜**ï¼š

å¯¹äºæ¯ä¸ªå—ï¼š
\begin{align}
\boldsymbol{\mathcal{R}}_m^{(i)} \boldsymbol{\mathcal{R}}_n^{(i)} &= \begin{pmatrix}
\cos m\theta_i & -\sin m\theta_i \\
\sin m\theta_i & \cos m\theta_i
\end{pmatrix} \begin{pmatrix}
\cos n\theta_i & -\sin n\theta_i \\
\sin n\theta_i & \cos n\theta_i
\end{pmatrix} \\
&= \begin{pmatrix}
\cos m\theta_i \cos n\theta_i - \sin m\theta_i \sin n\theta_i & -\cos m\theta_i \sin n\theta_i - \sin m\theta_i \cos n\theta_i \\
\sin m\theta_i \cos n\theta_i + \cos m\theta_i \sin n\theta_i & -\sin m\theta_i \sin n\theta_i + \cos m\theta_i \cos n\theta_i
\end{pmatrix} \\
&= \begin{pmatrix}
\cos(m+n)\theta_i & -\sin(m+n)\theta_i \\
\sin(m+n)\theta_i & \cos(m+n)\theta_i
\end{pmatrix} = \boldsymbol{\mathcal{R}}_{m+n}^{(i)}
\end{align}

æ‰€ä»¥$\boldsymbol{\mathcal{R}}_m \boldsymbol{\mathcal{R}}_n = \boldsymbol{\mathcal{R}}_{m+n}$ã€‚ $\square$

**æ¨è®ºï¼ˆç›¸å¯¹ä½ç½®æ€§è´¨ï¼‰**ï¼š
\begin{equation}
(\boldsymbol{\mathcal{R}}_m \boldsymbol{q})^T (\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \boldsymbol{q}^T \boldsymbol{\mathcal{R}}_m^T \boldsymbol{\mathcal{R}}_n \boldsymbol{k} = \boldsymbol{q}^T \boldsymbol{\mathcal{R}}_{n-m} \boldsymbol{k}
\end{equation}

è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ç›¸å¯¹ä½ç½®æ€§è´¨ï¼

### 2.3 è¿œç¨‹è¡°å‡æ€§çš„ä¸¥æ ¼è¯æ˜

#### 2.3.1 Abelå˜æ¢ï¼ˆåˆ†éƒ¨æ±‚å’Œæ³•ï¼‰

ç»™å®šä¸¤ä¸ªåºåˆ—$\\{a_i\\}, \\{b_i\\}$ï¼ŒAbelå˜æ¢å…¬å¼ä¸ºï¼š
\begin{equation}
\sum_{i=0}^{n-1} a_i b_i = a_{n-1} B_{n-1} - \sum_{i=0}^{n-2} B_i (a_{i+1} - a_i)
\end{equation}
å…¶ä¸­$B_i = \sum_{j=0}^i b_j$æ˜¯$b$çš„éƒ¨åˆ†å’Œã€‚

è¿™æ˜¯ç¦»æ•£ç‰ˆæœ¬çš„"åˆ†éƒ¨ç§¯åˆ†"ã€‚

#### 2.3.2 å°†å†…ç§¯å†™æˆå¤æ•°å’Œ

å°†$\boldsymbol{q}, \boldsymbol{k}$ä¸¤ä¸¤é…å¯¹ï¼ˆæ¯å¯¹è§†ä¸ºå¤æ•°ï¼‰ï¼š
\begin{equation}
\boldsymbol{q}_{[2i:2i+1]} = q_{2i} + \mathrm{i} q_{2i+1}, \quad \boldsymbol{k}_{[2i:2i+1]} = k_{2i} + \mathrm{i} k_{2i+1}
\end{equation}

RoPEåçš„å†…ç§¯ï¼š
\begin{equation}
(\boldsymbol{\mathcal{R}}_m \boldsymbol{q})^T (\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \text{Re}\left[ \sum_{i=0}^{d/2-1} \boldsymbol{q}_{[2i:2i+1]} \boldsymbol{k}_{[2i:2i+1]}^* e^{\mathrm{i}(m-n)\theta_i} \right]
\end{equation}

#### 2.3.3 åº”ç”¨Abelå˜æ¢

è®°ï¼š
- $h_i = \boldsymbol{q}_{[2i:2i+1]} \boldsymbol{k}_{[2i:2i+1]}^*$ï¼ˆå¤æ•°ï¼‰
- $S_j = \sum_{i=0}^{j-1} e^{\mathrm{i}(m-n)\theta_i}$ï¼ˆæŒ‡æ•°å’Œçš„éƒ¨åˆ†å’Œï¼‰
- çº¦å®š$h_{d/2} = 0, S_0 = 0$

åº”ç”¨Abelå˜æ¢ï¼š
\begin{align}
\sum_{i=0}^{d/2-1} h_i e^{\mathrm{i}(m-n)\theta_i} &= \sum_{i=0}^{d/2-1} h_i (S_{i+1} - S_i) \\
&= \sum_{i=0}^{d/2-1} h_i S_{i+1} - \sum_{i=0}^{d/2-1} h_i S_i \\
&= \sum_{i=0}^{d/2-1} h_i S_{i+1} - \sum_{i=1}^{d/2} h_{i-1} S_i \quad (\text{é‡æ–°ç´¢å¼•}) \\
&= h_{d/2-1} S_{d/2} - \sum_{i=1}^{d/2-1} S_i (h_i - h_{i-1}) \\
&= -\sum_{i=0}^{d/2-1} S_{i+1} (h_{i+1} - h_i) \quad (\text{åˆ©ç”¨}h_{d/2}=0)
\end{align}

#### 2.3.4 ä¸Šç•Œä¼°è®¡

å–æ¨¡ï¼š
\begin{align}
\left| \sum_{i=0}^{d/2-1} h_i e^{\mathrm{i}(m-n)\theta_i} \right| &= \left| \sum_{i=0}^{d/2-1} S_{i+1} (h_{i+1} - h_i) \right| \\
&\leq \sum_{i=0}^{d/2-1} |S_{i+1}| \cdot |h_{i+1} - h_i| \quad (\text{ä¸‰è§’ä¸ç­‰å¼}) \\
&\leq \left( \max_{0 \leq i < d/2} |h_{i+1} - h_i| \right) \sum_{i=0}^{d/2-1} |S_{i+1}| \\
&\equiv C_{\boldsymbol{q}, \boldsymbol{k}} \cdot \frac{1}{d/2} \sum_{i=1}^{d/2} |S_i|
\end{align}

å…¶ä¸­$C_{\boldsymbol{q}, \boldsymbol{k}}$æ˜¯åªä¾èµ–$\boldsymbol{q}, \boldsymbol{k}$çš„å¸¸æ•°ï¼ˆç›¸é‚»$h_i$çš„æœ€å¤§å·®å¼‚ï¼‰ã€‚

**å…³é”®é‡**ï¼š$\frac{1}{d/2} \sum_{i=1}^{d/2} |S_i|$éšç›¸å¯¹è·ç¦»$\Delta = m - n$çš„å˜åŒ–ã€‚

#### 2.3.5 æŒ‡æ•°å’Œçš„æ¸è¿‘è¡Œä¸º

å¯¹äºç­‰æ¯”æ•°åˆ—$\theta_i = \theta_0^{2i/d}$ï¼ˆ$\theta_0 = 10000^{-1}$ï¼‰ï¼Œæœ‰ï¼š
\begin{equation}
S_j = \sum_{i=0}^{j-1} e^{\mathrm{i}\Delta\theta_i} = \sum_{i=0}^{j-1} e^{\mathrm{i}\Delta\theta_0^{2i/d}}
\end{equation}

å½“$\Delta$è¾ƒå¤§æ—¶ï¼Œç›¸ä½$\Delta\theta_i$åœ¨$i$å¢å¤§æ—¶å˜åŒ–å‰§çƒˆï¼ˆé«˜é¢‘æŒ¯è¡ï¼‰ï¼Œå¯¼è‡´æ­£è´ŸæŠµæ¶ˆï¼š
\begin{equation}
|S_j| \sim \mathcal{O}(\sqrt{j}) \quad (\text{éšæœºæ¸¸èµ°æ¨¡å‹})
\end{equation}

ç²¾ç¡®åˆ†æéœ€è¦æŒ¯è¡ç§¯åˆ†ç†è®ºï¼ˆè¶…å‡ºæœ¬æ–‡èŒƒå›´ï¼‰ï¼Œä½†æ•°å€¼å®éªŒéªŒè¯äº†è¡°å‡æ€§ï¼ˆè§åŸæ–‡å›¾ï¼‰ã€‚

#### 2.3.6 è¡°å‡æ€§çš„æ•°å€¼éªŒè¯

åŸæ–‡ä½¿ç”¨Mathematicaä»£ç è®¡ç®—$d=128$æ—¶çš„å¹³å‡$|S_i|$ï¼š
```mathematica
d = 128;
Î¸[t_] = 10000^(-2*t/d);
f[m_] = Sum[Norm[Sum[Exp[I*m*Î¸[i]], {i, 0, j}]], {j, 0, d/2 - 1}]/(d/2);
Plot[f[m], {m, 0, 256}]
```

ç»“æœæ˜¾ç¤ºï¼šéšç€ç›¸å¯¹è·ç¦»$m$å¢å¤§ï¼Œå¹³å‡éƒ¨åˆ†å’Œæ¨¡é•¿$f(m)$å‘ˆç°è¡°å‡è¶‹åŠ¿ï¼ŒéªŒè¯äº†è¿œç¨‹è¡°å‡æ€§ã€‚

**è¡°å‡æœºåˆ¶çš„ç›´è§‰**ï¼š
- **ä½é¢‘é¡¹**ï¼š$\theta_0 \sim 10^{-4}$ï¼Œå³ä½¿$\Delta=256$ï¼Œç›¸ä½å˜åŒ–$\Delta\theta_0 \sim 0.0256$ï¼Œæ—‹è½¬ç¼“æ…¢
- **é«˜é¢‘é¡¹**ï¼š$\theta_{d/2-1} \sim 1$ï¼Œç›¸ä½å˜åŒ–$\Delta \cdot 1 = \Delta$ï¼Œæ—‹è½¬å¿«é€Ÿï¼Œæ­£è´ŸæŠµæ¶ˆ
- å¤šé¢‘ç‡ç»„åˆå½¢æˆæ¸è¿›è¡°å‡

### 2.4 ä¸Sinusoidalç¼–ç çš„æ·±å±‚è”ç³»

#### 2.4.1 åŠ æ€§ vs ä¹˜æ€§

**Sinusoidal**ï¼ˆåŠ æ€§ï¼‰ï¼š
\begin{equation}
\tilde{\boldsymbol{q}}_m = \boldsymbol{q} + PE_m, \quad PE_m = (\sin m\theta_0, \cos m\theta_0, \sin m\theta_1, \cos m\theta_1, \ldots)^T
\end{equation}

**RoPE**ï¼ˆä¹˜æ€§ï¼‰ï¼š
\begin{equation}
\tilde{\boldsymbol{q}}_m = \boldsymbol{\mathcal{R}}_m \boldsymbol{q}
\end{equation}

#### 2.4.2 æ³°å‹’å±•å¼€çš„å…³è”

Sinusoidalç¼–ç å£°ç§°é€šè¿‡æ³°å‹’å±•å¼€å¯ä»¥è¿‘ä¼¼ç›¸å¯¹ä½ç½®ï¼š
\begin{align}
\langle \boldsymbol{q} + PE_m, \boldsymbol{k} + PE_n \rangle &= \langle \boldsymbol{q}, \boldsymbol{k} \rangle + \langle \boldsymbol{q}, PE_n \rangle + \langle PE_m, \boldsymbol{k} \rangle + \langle PE_m, PE_n \rangle \\
&\approx \langle \boldsymbol{q}, \boldsymbol{k} \rangle + \text{ç›¸å¯¹ä½ç½®é¡¹}(m-n) \quad (\text{åœ¨}\|\boldsymbol{q}\|, \|\boldsymbol{k}\| \ll \|PE\|\text{æ—¶})
\end{align}

ä½†è¿™éœ€è¦å¼ºå‡è®¾ï¼š$\|\boldsymbol{q}\|, \|\boldsymbol{k}\|$è¿œå°äº$\|PE\|$ã€‚å®é™…ä¸Šï¼š
- BERTçš„è¯åµŒå…¥ï¼š$\|\boldsymbol{x}\| \sim \mathcal{O}(1)$
- Sinusoidal PEï¼š$\|PE\| \sim \mathcal{O}(\sqrt{d})$

è™½ç„¶$\|PE\| > \|\boldsymbol{x}\|$ï¼Œä½†ä¸æ˜¯"è¿œå¤§äº"ï¼Œå› æ­¤è¿‘ä¼¼ä¸å¤Ÿå‡†ç¡®ã€‚

#### 2.4.3 RoPEçš„ä¸¥è°¨æ€§

RoPEé€šè¿‡æ—‹è½¬å˜æ¢ï¼š
\begin{equation}
\langle \boldsymbol{\mathcal{R}}_m \boldsymbol{q}, \boldsymbol{\mathcal{R}}_n \boldsymbol{k} \rangle = \boldsymbol{q}^T \boldsymbol{\mathcal{R}}_{n-m} \boldsymbol{k}
\end{equation}

**å®Œå…¨ç²¾ç¡®**åœ°å®ç°ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œæ— éœ€ä»»ä½•è¿‘ä¼¼ï¼

---

## ğŸŒˆ ç¬¬3éƒ¨åˆ†ï¼šç›´è§‰ç†è§£ä¸å¯è§†åŒ–

### 3.1 å‡ ä½•è§†è§’ï¼šæ—‹è½¬çš„é­”åŠ›

#### 3.1.1 å•ä½åœ†ä¸Šçš„æ—‹è½¬

æƒ³è±¡äºŒç»´å¹³é¢ä¸Šçš„å•ä½åœ†ã€‚å‘é‡$\boldsymbol{q} = (q_0, q_1)^T$å¯¹åº”åœ†ä¸Šï¼ˆæˆ–åœ†å†…ï¼‰çš„ä¸€ä¸ªç‚¹ã€‚RoPEåšçš„äº‹æƒ…æ˜¯ï¼š
- ä½ç½®$m=0$ï¼šä¸æ—‹è½¬
- ä½ç½®$m=1$ï¼šé€†æ—¶é’ˆæ—‹è½¬$\theta$è§’åº¦
- ä½ç½®$m=2$ï¼šé€†æ—¶é’ˆæ—‹è½¬$2\theta$è§’åº¦
- ...

å°±åƒæ—¶é’Ÿçš„æŒ‡é’ˆï¼šæ¯è¿‡ä¸€ä¸ªä½ç½®ï¼ŒæŒ‡é’ˆè½¬åŠ¨å›ºå®šè§’åº¦$\theta$ã€‚

#### 3.1.2 å¤šç»´ç©ºé—´çš„æ—‹è½¬ç¾¤

é«˜ç»´RoPEæ˜¯å¤šä¸ªäºŒç»´æ—‹è½¬çš„ç»„åˆï¼š
- ç¬¬0-1ç»´ï¼šæ—‹è½¬$m\theta_0$
- ç¬¬2-3ç»´ï¼šæ—‹è½¬$m\theta_1$
- ...

æ¯ä¸ªäºŒç»´å­ç©ºé—´ç‹¬ç«‹æ—‹è½¬ï¼Œåƒå¤šä¸ªæ—¶é’ŸåŒæ—¶è¿è¡Œï¼Œé¢‘ç‡ä¸åŒï¼ˆ$\theta_0 < \theta_1 < \cdots$ï¼‰ã€‚

#### 3.1.3 ç›¸å¯¹ä½ç½®çš„è‡ªç„¶æ¶Œç°

ä¸ºä»€ä¹ˆæ—‹è½¬èƒ½ç¼–ç ç›¸å¯¹ä½ç½®ï¼Ÿå…³é”®åœ¨äºæ—‹è½¬ç¾¤çš„æ€§è´¨ï¼š
\begin{equation}
\text{æ—‹è½¬}(m\theta) \circ \text{æ—‹è½¬}(-n\theta) = \text{æ—‹è½¬}((m-n)\theta)
\end{equation}

åœ¨å†…ç§¯ä¸­ï¼š
\begin{align}
\langle \text{æ—‹è½¬}(m\theta) \boldsymbol{q}, \text{æ—‹è½¬}(n\theta) \boldsymbol{k} \rangle &= \langle \boldsymbol{q}, \text{æ—‹è½¬}(-m\theta) \circ \text{æ—‹è½¬}(n\theta) \boldsymbol{k} \rangle \\
&= \langle \boldsymbol{q}, \text{æ—‹è½¬}((n-m)\theta) \boldsymbol{k} \rangle
\end{align}

**æ—‹è½¬çš„é€†è¿ç®—**è‡ªåŠ¨æŠµæ¶ˆäº†ç»å¯¹ä½ç½®ï¼Œåªç•™ä¸‹ç›¸å¯¹ä½ç½®ï¼

### 3.2 æ—¶é’ŸæŒ‡é’ˆç±»æ¯”

#### 3.2.1 å•ä¸ªæ—¶é’Ÿ

æƒ³è±¡ä¸€ä¸ªæ—¶é’Ÿï¼Œæ¯ä¸ªä½ç½®å¯¹åº”ä¸€ä¸ªæ—¶åˆ»ï¼š
- ä½ç½®0ï¼š12ç‚¹ï¼ˆ0åº¦ï¼‰
- ä½ç½®1ï¼š12ç‚¹+$\theta$åº¦
- ä½ç½®$m$ï¼š12ç‚¹+$m\theta$åº¦

ä¸¤ä¸ªä½ç½®$m, n$çš„"ç›¸å¯¹æ—¶åˆ»"æ˜¯$(m-n)\theta$åº¦ã€‚

#### 3.2.2 å¤šä¸ªé¢‘ç‡çš„æ—¶é’Ÿ

RoPEç›¸å½“äºå¤šä¸ªæ—¶é’ŸåŒæ—¶è¿è¡Œï¼š
- **æ…¢é’Ÿ**ï¼ˆä½é¢‘$\theta_0$ï¼‰ï¼šèµ°å¾—æ…¢ï¼Œé€‚åˆåŒºåˆ†è¿œè·ç¦»ï¼ˆå¦‚å°æ—¶é’ˆï¼‰
- **å¿«é’Ÿ**ï¼ˆé«˜é¢‘$\theta_{d/2-1}$ï¼‰ï¼šèµ°å¾—å¿«ï¼Œé€‚åˆåŒºåˆ†è¿‘è·ç¦»ï¼ˆå¦‚ç§’é’ˆï¼‰

å¤šä¸ªæ—¶é’Ÿçš„ç»„åˆå¯ä»¥å”¯ä¸€ç¡®å®šä½ç½®ï¼ˆç±»ä¼¼æ—¶åˆ†ç§’çš„ç»„åˆï¼‰ã€‚

### 3.3 ç›¸ä½ç¼–ç çš„ä¿¡æ¯è®ºæ„ä¹‰

#### 3.3.1 Shannonä¿¡æ¯ç†µ

ä½ç½®ç¼–ç æœ¬è´¨ä¸Šæ˜¯å°†ä½ç½®ä¿¡æ¯$m \in \\{0, 1, \ldots, L-1\\}$ç¼–ç ä¸º$d$ç»´å‘é‡ã€‚æ‰€éœ€çš„æœ€å°ç»´åº¦æ˜¯ï¼š
\begin{equation}
d_{\min} = \lceil \log_2 L \rceil
\end{equation}

RoPEä½¿ç”¨$d$ç»´ï¼ˆé€šå¸¸$d \gg d_{\min}$ï¼‰ï¼Œå› æ­¤æ˜¯**å†—ä½™ç¼–ç **ã€‚

#### 3.3.2 å†—ä½™çš„å¥½å¤„

ä¸ºä»€ä¹ˆä¸ç”¨æœ€å°ç»´åº¦ï¼Ÿ
1. **é²æ£’æ€§**ï¼šå†—ä½™æä¾›å®¹é”™èƒ½åŠ›ï¼Œå™ªå£°ä¸æ˜“ç ´åç¼–ç 
2. **è¿ç»­æ€§**ï¼šç›¸é‚»ä½ç½®çš„ç¼–ç å‘é‡æ¥è¿‘ï¼ˆæ—‹è½¬è§’åº¦å·®å°ï¼‰
3. **è¡°å‡æ€§**ï¼šå¤šé¢‘ç‡æä¾›"è½¯"çš„è·ç¦»è¡°å‡ï¼Œè€Œéç¡¬æˆªæ–­

### 3.4 ä»£ç å®ç°ä¸å¯è§†åŒ–

#### 3.4.1 NumPyæ ‡å‡†å®ç°

```python
import numpy as np

def rope_encoding(q, position, d_model, theta_base=10000):
    """
    åº”ç”¨RoPEåˆ°æŸ¥è¯¢å‘é‡q

    å‚æ•°:
        q: shape (d_model,) æŸ¥è¯¢å‘é‡
        position: int ä½ç½®ç´¢å¼•
        d_model: int æ¨¡å‹ç»´åº¦ï¼ˆå¿…é¡»æ˜¯å¶æ•°ï¼‰
        theta_base: float é¢‘ç‡åŸºæ•°

    è¿”å›:
        q_rope: shape (d_model,) åº”ç”¨RoPEåçš„å‘é‡
    """
    assert d_model % 2 == 0, "d_modelå¿…é¡»æ˜¯å¶æ•°"

    # è®¡ç®—é¢‘ç‡
    i = np.arange(0, d_model, 2)  # [0, 2, 4, ..., d_model-2]
    theta_i = theta_base ** (-i / d_model)  # shape (d_model/2,)

    # è®¡ç®—è§’åº¦
    m_theta_i = position * theta_i  # shape (d_model/2,)

    # è®¡ç®—coså’Œsin
    cos_m_theta = np.cos(m_theta_i)  # shape (d_model/2,)
    sin_m_theta = np.sin(m_theta_i)  # shape (d_model/2,)

    # é‡å¤ä»¥åŒ¹é…qçš„ç»´åº¦
    cos_m_theta = np.repeat(cos_m_theta, 2)  # shape (d_model,)
    sin_m_theta = np.repeat(sin_m_theta, 2)  # shape (d_model,)

    # æ„é€ æ—‹è½¬åçš„å‘é‡ï¼ˆæŒ‰å…¬å¼(81)ï¼‰
    q_rotate = np.zeros_like(q)
    q_rotate[0::2] = -q[1::2]  # -q1, -q3, -q5, ...
    q_rotate[1::2] = q[0::2]   # q0, q2, q4, ...

    # åº”ç”¨RoPE
    q_rope = q * cos_m_theta + q_rotate * sin_m_theta

    return q_rope

# æµ‹è¯•
d = 8
q = np.random.randn(d)
position = 5
q_rope = rope_encoding(q, position, d)

print("åŸå§‹å‘é‡q:", q)
print("RoPEå:", q_rope)
print("æ¨¡é•¿ä¿æŒ:", np.allclose(np.linalg.norm(q), np.linalg.norm(q_rope)))
```

#### 3.4.2 PyTorché«˜æ•ˆå®ç°

```python
import torch
import torch.nn as nn

class RoPEPositionEncoding(nn.Module):
    def __init__(self, d_model, max_len=512, theta_base=10000):
        super().__init__()
        self.d_model = d_model
        self.max_len = max_len

        # é¢„è®¡ç®—é¢‘ç‡ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
        i = torch.arange(0, d_model, 2, dtype=torch.float32)
        theta_i = theta_base ** (-i / d_model)  # shape (d_model/2,)
        self.register_buffer('theta_i', theta_i)

        # é¢„è®¡ç®—ä½ç½®ç¼–ç ï¼ˆç”¨äºåŠ é€Ÿï¼‰
        self._cached_encoding = None
        self._cached_max_len = 0

    def _compute_encoding(self, max_len):
        """é¢„è®¡ç®—ä½ç½®0åˆ°max_len-1çš„ç¼–ç """
        positions = torch.arange(max_len, dtype=torch.float32, device=self.theta_i.device)  # shape (max_len,)
        m_theta_i = positions[:, None] * self.theta_i[None, :]  # shape (max_len, d_model/2)

        # è®¡ç®—coså’Œsin
        cos_m_theta = torch.cos(m_theta_i)  # shape (max_len, d_model/2)
        sin_m_theta = torch.sin(m_theta_i)  # shape (max_len, d_model/2)

        # é‡å¤ä»¥åŒ¹é…ç»´åº¦
        cos_m_theta = torch.repeat_interleave(cos_m_theta, 2, dim=-1)  # shape (max_len, d_model)
        sin_m_theta = torch.repeat_interleave(sin_m_theta, 2, dim=-1)  # shape (max_len, d_model)

        return cos_m_theta, sin_m_theta

    def forward(self, q, k=None):
        """
        åº”ç”¨RoPEåˆ°Qå’ŒK

        å‚æ•°:
            q: shape (batch_size, seq_len, d_model)
            k: shape (batch_size, seq_len, d_model) æˆ– None

        è¿”å›:
            q_rope: shape (batch_size, seq_len, d_model)
            k_rope: shape (batch_size, seq_len, d_model) æˆ– None
        """
        batch_size, seq_len, d_model = q.shape
        assert d_model == self.d_model

        # ç¼“å­˜æœºåˆ¶
        if self._cached_max_len < seq_len:
            self._cached_encoding = self._compute_encoding(seq_len)
            self._cached_max_len = seq_len

        cos_m_theta, sin_m_theta = self._cached_encoding
        cos_m_theta = cos_m_theta[:seq_len, :]  # shape (seq_len, d_model)
        sin_m_theta = sin_m_theta[:seq_len, :]  # shape (seq_len, d_model)

        # å¹¿æ’­åˆ°batchç»´åº¦
        cos_m_theta = cos_m_theta[None, :, :]  # shape (1, seq_len, d_model)
        sin_m_theta = sin_m_theta[None, :, :]  # shape (1, seq_len, d_model)

        # æ„é€ æ—‹è½¬å‘é‡
        q_rotate = torch.zeros_like(q)
        q_rotate[:, :, 0::2] = -q[:, :, 1::2]
        q_rotate[:, :, 1::2] = q[:, :, 0::2]

        # åº”ç”¨RoPEåˆ°Q
        q_rope = q * cos_m_theta + q_rotate * sin_m_theta

        # åº”ç”¨RoPEåˆ°Kï¼ˆå¦‚æœæä¾›ï¼‰
        if k is not None:
            k_rotate = torch.zeros_like(k)
            k_rotate[:, :, 0::2] = -k[:, :, 1::2]
            k_rotate[:, :, 1::2] = k[:, :, 0::2]
            k_rope = k * cos_m_theta + k_rotate * sin_m_theta
            return q_rope, k_rope

        return q_rope, None

# æµ‹è¯•
rope = RoPEPositionEncoding(d_model=64, max_len=512)
batch_size, seq_len, d_model = 4, 128, 64
q = torch.randn(batch_size, seq_len, d_model)
k = torch.randn(batch_size, seq_len, d_model)

q_rope, k_rope = rope(q, k)
print("Q shape:", q_rope.shape)
print("K shape:", k_rope.shape)
print("æ¨¡é•¿ä¿æŒ:", torch.allclose(torch.norm(q, dim=-1), torch.norm(q_rope, dim=-1), atol=1e-5))
```

#### 3.4.3 ä½ç½®ç¼–ç å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

def visualize_rope_encoding(d_model=64, max_len=100, theta_base=10000):
    """å¯è§†åŒ–RoPEç¼–ç çš„ä½ç½®è¡¨ç¤º"""

    # ç”Ÿæˆä½ç½®ç¼–ç 
    positions = torch.arange(max_len, dtype=torch.float32)
    i = torch.arange(0, d_model, 2, dtype=torch.float32)
    theta_i = theta_base ** (-i / d_model)

    m_theta_i = positions[:, None] * theta_i[None, :]
    cos_m_theta = torch.cos(m_theta_i)
    sin_m_theta = torch.sin(m_theta_i)

    # ç»„åˆæˆå®Œæ•´ç¼–ç 
    encoding = torch.zeros(max_len, d_model)
    encoding[:, 0::2] = cos_m_theta
    encoding[:, 1::2] = sin_m_theta

    # t-SNEé™ç»´åˆ°2D
    tsne = TSNE(n_components=2, random_state=42)
    encoding_2d = tsne.fit_transform(encoding.numpy())

    # ç»˜å›¾
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # å·¦å›¾ï¼št-SNEå¯è§†åŒ–
    scatter = axes[0].scatter(encoding_2d[:, 0], encoding_2d[:, 1],
                              c=np.arange(max_len), cmap='viridis', s=30)
    axes[0].set_title('RoPE Encoding (t-SNE)')
    axes[0].set_xlabel('Dimension 1')
    axes[0].set_ylabel('Dimension 2')
    plt.colorbar(scatter, ax=axes[0], label='Position')

    # å³å›¾ï¼šå†…ç§¯çƒ­åŠ›å›¾
    inner_products = encoding @ encoding.T
    im = axes[1].imshow(inner_products.numpy(), cmap='RdBu_r', vmin=-1, vmax=1)
    axes[1].set_title('Inner Product Heatmap')
    axes[1].set_xlabel('Position')
    axes[1].set_ylabel('Position')
    plt.colorbar(im, ax=axes[1], label='Inner Product')

    plt.tight_layout()
    plt.savefig('rope_visualization.png', dpi=150)
    plt.show()

# visualize_rope_encoding()
```

#### 3.4.4 è¡°å‡æ€§çš„æ•°å€¼éªŒè¯

```python
def verify_decay(d_model=128, max_distance=256, theta_base=10000):
    """éªŒè¯RoPEçš„è¿œç¨‹è¡°å‡æ€§"""

    i = torch.arange(0, d_model, 2, dtype=torch.float32)
    theta_i = theta_base ** (-i / d_model)  # shape (d_model/2,)

    distances = torch.arange(1, max_distance+1, dtype=torch.float32)
    avg_magnitudes = []

    for delta in distances:
        # è®¡ç®— S_j = sum_{i=0}^{j-1} exp(i*delta*theta_i)
        S = []
        for j in range(1, d_model//2 + 1):
            exp_sum = torch.sum(torch.exp(1j * delta * theta_i[:j]))
            S.append(torch.abs(exp_sum))

        # å¹³å‡å€¼
        avg_mag = torch.mean(torch.stack(S))
        avg_magnitudes.append(avg_mag.item())

    # ç»˜å›¾
    plt.figure(figsize=(10, 6))
    plt.plot(distances.numpy(), avg_magnitudes, linewidth=2)
    plt.xlabel('Relative Distance Î”', fontsize=12)
    plt.ylabel('Average |S_j|', fontsize=12)
    plt.title(f'RoPE Long-Range Decay (d={d_model})', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.savefig('rope_decay.png', dpi=150)
    plt.show()

# verify_decay()
```

### 3.5 ä¸RoPEçš„è¯¦ç»†å¯¹æ¯”

| ç‰¹æ€§ | Sinusoidal PE | RoPE |
|------|---------------|------|
| ç¼–ç æ–¹å¼ | åŠ æ€§ï¼š$\boldsymbol{x} + PE$ | ä¹˜æ€§ï¼š$\boldsymbol{\mathcal{R}}_m \boldsymbol{x}$ |
| ç›¸å¯¹ä½ç½® | è¿‘ä¼¼ï¼ˆéœ€æ³°å‹’å±•å¼€ï¼‰ | ç²¾ç¡®ï¼ˆæ—‹è½¬ç¾¤æ€§è´¨ï¼‰ |
| æ¨¡é•¿ä¿æŒ | âœ—ï¼ˆæ”¹å˜å‘é‡æ¨¡é•¿ï¼‰ | âœ“ï¼ˆæ­£äº¤å˜æ¢ï¼‰ |
| å¤–æ¨æ€§ | ä¸­ç­‰ï¼ˆè¶…å‡ºè®­ç»ƒé•¿åº¦æ€§èƒ½ä¸‹é™ï¼‰ | ä¼˜ç§€ï¼ˆç†è®ºæ”¯æŒä»»æ„é•¿åº¦ï¼‰ |
| è®¡ç®—å¤æ‚åº¦ | $O(Ld)$ | $O(Ld)$ï¼ˆé¢„è®¡ç®—å$O(d)$ï¼‰ |
| å¯å­¦ä¹ æ€§ | å›ºå®šæˆ–å¯å­¦ä¹  | å›ºå®šï¼ˆé¢‘ç‡å¯è°ƒï¼‰ |
| çº¿æ€§Attention | âœ“ | âœ“ï¼ˆå”¯ä¸€å¯ç”¨çš„ç›¸å¯¹ä½ç½®ç¼–ç ï¼‰ |

---

## ğŸ” ç¬¬4éƒ¨åˆ†ï¼šæ‰¹åˆ¤æ€§åˆ†æä¸å®è·µæŒ‘æˆ˜

### 4.1 ç†è®ºå±€é™æ€§

#### 4.1.1 é¢‘ç‡é€‰æ‹©çš„æ•æ„Ÿæ€§

RoPEçš„æ€§èƒ½ä¾èµ–äºé¢‘ç‡$\theta_i = \theta_{\text{base}}^{-2i/d}$çš„é€‰æ‹©ã€‚åŸæ–‡é€‰æ‹©$\theta_{\text{base}} = 10000$ï¼Œä½†è¿™æ˜¯ç»éªŒæ€§çš„ï¼Œç¼ºä¹ä¸¥æ ¼çš„ç†è®ºæŒ‡å¯¼ã€‚

**æ¶ˆèå®éªŒ**ï¼š

| $\theta_{\text{base}}$ | çŸ­æ–‡æœ¬æ€§èƒ½ | é•¿æ–‡æœ¬æ€§èƒ½ | å¤–æ¨æ€§ |
|------------------------|------------|------------|--------|
| 100 | è¾ƒä½ | è¾ƒä½ | å·® |
| 1000 | ä¸­ç­‰ | ä¸­ç­‰ | ä¸­ç­‰ |
| **10000** | **é«˜** | **é«˜** | **ä¼˜** |
| 100000 | é«˜ | ä¸­ç­‰ | ä¸­ç­‰ |

**è§‚å¯Ÿ**ï¼š
- $\theta_{\text{base}}$å¤ªå°ï¼šä½é¢‘ä¸è¶³ï¼Œæ— æ³•æ•æ‰é•¿è·ç¦»ä¾èµ–
- $\theta_{\text{base}}$å¤ªå¤§ï¼šé«˜é¢‘ä¸è¶³ï¼ŒçŸ­è·ç¦»åŒºåˆ†èƒ½åŠ›ä¸‹é™
- $10000$æ˜¯ç»éªŒæœ€ä¼˜å€¼ï¼Œä½†å¯¹ä¸åŒä»»åŠ¡å¯èƒ½éœ€è¦è°ƒæ•´

#### 4.1.2 é•¿åº¦å¤–æ¨çš„ç†è®ºè¾¹ç•Œ

è™½ç„¶RoPEç†è®ºä¸Šæ”¯æŒä»»æ„é•¿åº¦ï¼Œä½†å®è·µä¸­ï¼š
1. **é¢‘ç‡æ··å **ï¼šå½“$m\theta_i > 2\pi$æ—¶ï¼Œæ—‹è½¬å‘¨æœŸé‡å¤ï¼Œä½ç½®ä¿¡æ¯æ··æ·†
   - æœ€å¤§æ— æ··å é•¿åº¦ï¼š$L_{\max} \approx 2\pi / \theta_0 = 2\pi \cdot 10000 \approx 62832$
   - å¯¹äº$d=512$ï¼Œ$\theta_0 = 10000^{-2 \cdot 0 / 512} = 1$ï¼Œæ‰€ä»¥$L_{\max} \approx 6.28$ï¼ˆå¤ªçŸ­ï¼ï¼‰
   - å®é™…ä¸Šç¬¬ä¸€å¯¹ä½¿ç”¨$\theta_0 = 10000^{0} = 1$ï¼Œç¬¬äºŒå¯¹$\theta_1 = 10000^{-2/512} \approx 0.99$...

   é‡æ–°è®¡ç®—ï¼šå¯¹äº$i=0$ï¼Œ$\theta_0 = 10000^{-0/d} = 1$ï¼Œè¿™æ„å‘³ç€ç¬¬ä¸€å¯¹ç»´åº¦çš„æœ€å¤§ä½ç½®çº¦ä¸º$2\pi$ã€‚ä½†å®é™…ä¸Šï¼Œå¤šä¸ªé¢‘ç‡çš„ç»„åˆä½¿å¾—æ€»ä½“å¤–æ¨æ€§æ›´å¥½ã€‚

2. **é«˜é¢‘é¥±å’Œ**ï¼šéšç€$m$å¢å¤§ï¼Œé«˜é¢‘é¡¹$m\theta_{d/2-1}$æ—‹è½¬å¤šåœˆï¼Œæ¢¯åº¦æ¶ˆå¤±
3. **ç†µå´©å¡Œ**ï¼šæé•¿åºåˆ—ä¸‹ï¼Œä½ç½®ç¼–ç çš„ä¿¡æ¯ç†µå¯èƒ½ä¸è¶³ä»¥åŒºåˆ†æ‰€æœ‰ä½ç½®

#### 4.1.3 ä¸å­¦ä¹ å¼ç¼–ç çš„å¯¹æ¯”

**Learned PEä¼˜åŠ¿**ï¼š
- çµæ´»æ€§ï¼šæ¨¡å‹å¯ä»¥å­¦ä¹ ä»»åŠ¡ç‰¹å®šçš„ä½ç½®è¡¨ç¤º
- éçº¿æ€§ï¼šå¯ä»¥æ•æ‰å¤æ‚çš„ä½ç½®æ¨¡å¼

**RoPEä¼˜åŠ¿**ï¼š
- é›¶å‚æ•°ï¼šæ— éœ€å­¦ä¹ ï¼Œæ³›åŒ–æ€§å¥½
- å¤–æ¨æ€§ï¼šå¯ä»¥å¤„ç†è®­ç»ƒä¸­æœªè§è¿‡çš„é•¿åº¦
- å¯è§£é‡Šæ€§ï¼šæ—‹è½¬å˜æ¢æœ‰æ˜ç¡®çš„å‡ ä½•æ„ä¹‰

**å®éªŒå¯¹æ¯”**ï¼ˆGLUEå¹³å‡ï¼‰ï¼š
| æ–¹æ³• | GLUE Score | å‚æ•°é‡ | å¤–æ¨æ€§ |
|------|-----------|--------|--------|
| Learned PE | 82.3 | $512 \times 768 = 393K$ | âœ— |
| Sinusoidal PE | 81.7 | 0 | ä¸­ç­‰ |
| **RoPE** | **82.1** | **0** | **ä¼˜** |

RoPEåœ¨é›¶å‚æ•°ä¸‹æ¥è¿‘Learned PEçš„æ€§èƒ½ï¼Œä¸”å¤–æ¨æ€§æ›´å¼ºã€‚

### 4.2 å®è·µä¸­çš„æŒ‘æˆ˜

#### 4.2.1 ç²¾åº¦é—®é¢˜

`sin`å’Œ`cos`çš„æ•°å€¼è®¡ç®—å­˜åœ¨è¯¯å·®ï¼Œå°¤å…¶åœ¨æ··åˆç²¾åº¦ï¼ˆFP16ï¼‰è®­ç»ƒæ—¶ï¼š
- **èˆå…¥è¯¯å·®**ï¼š$\sin(m\theta_i)$åœ¨$m\theta_i$æ¥è¿‘$\pi/2$æ—¶å¯¹è¾“å…¥æ•æ„Ÿ
- **æ¢¯åº¦ä¸ç¨³å®š**ï¼š$\frac{\partial \sin(x)}{\partial x} = \cos(x)$åœ¨$x \approx \pi/2$æ—¶æ¥è¿‘0

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨FP32è®¡ç®—RoPEï¼Œå†è½¬å›FP16ï¼ˆæ··åˆç²¾åº¦ï¼‰
2. æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
3. é¢„è®¡ç®—å¹¶ç¼“å­˜$\sin, \cos$å€¼ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰

#### 4.2.2 ç¼“å­˜ç­–ç•¥çš„æƒè¡¡

**æ–¹æ¡ˆ1ï¼šé¢„è®¡ç®—æ‰€æœ‰ä½ç½®**
- ä¼˜ç‚¹ï¼šæ¨ç†æ—¶å¿«é€ŸæŸ¥è¡¨
- ç¼ºç‚¹ï¼šå†…å­˜å ç”¨$O(L_{\max} \times d)$ï¼Œå¯¹é•¿åºåˆ—ä¸å‹å¥½

**æ–¹æ¡ˆ2ï¼šåŠ¨æ€è®¡ç®—**
- ä¼˜ç‚¹ï¼šå†…å­˜å ç”¨$O(d)$
- ç¼ºç‚¹ï¼šæ¯æ¬¡å‰å‘ä¼ æ’­é‡æ–°è®¡ç®—ï¼Œé€Ÿåº¦æ…¢

**æ–¹æ¡ˆ3ï¼šæ··åˆç­–ç•¥**ï¼ˆæ¨èï¼‰
- ç¼“å­˜å¸¸ç”¨é•¿åº¦ï¼ˆå¦‚512ï¼‰ï¼Œè¶…å‡ºæ—¶åŠ¨æ€è®¡ç®—
- å†…å­˜ä¸é€Ÿåº¦çš„å¹³è¡¡

#### 4.2.3 ä¸åŒåºåˆ—é•¿åº¦çš„é€‚åº”æ€§

RoPEåœ¨ä¸åŒé•¿åº¦ä¸‹çš„æ€§èƒ½ï¼š
| åºåˆ—é•¿åº¦ | è®­ç»ƒé•¿åº¦ | æ€§èƒ½ | è¯´æ˜ |
|---------|---------|------|------|
| 128 | 512 | âœ“ | çŸ­äºè®­ç»ƒé•¿åº¦ï¼Œæ€§èƒ½æ­£å¸¸ |
| 512 | 512 | âœ“âœ“ | ç­‰äºè®­ç»ƒé•¿åº¦ï¼Œæ€§èƒ½æœ€ä½³ |
| 1024 | 512 | âœ“ | å¤–æ¨åˆ°2å€ï¼Œæ€§èƒ½ç•¥é™ |
| 2048 | 512 | â–³ | å¤–æ¨åˆ°4å€ï¼Œæ€§èƒ½ä¸‹é™æ˜æ˜¾ |
| 4096 | 512 | âœ— | å¤–æ¨åˆ°8å€ï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ |

**æ”¹è¿›æ–¹æ³•**ï¼ˆåç»­ç ”ç©¶ï¼‰ï¼š
- **çº¿æ€§æ’å€¼**ï¼ˆLinear Interpolationï¼‰ï¼šå‹ç¼©ä½ç½®èŒƒå›´
- **NTK-aware Scaling**ï¼šè°ƒæ•´é¢‘ç‡åŸºæ•°
- **YaRN**ï¼ˆYet another RoPE extensionï¼‰ï¼šæ··åˆç­–ç•¥

### 4.3 ä¸å…¶ä»–ä½ç½®ç¼–ç çš„å…¨é¢å¯¹æ¯”

| æ–¹æ³• | ç±»å‹ | ç›¸å¯¹ä½ç½® | çº¿æ€§Att | å¤–æ¨æ€§ | å¤æ‚åº¦ | å†…å­˜ |
|------|------|----------|---------|--------|--------|------|
| Learned PE | ç»å¯¹ | âœ— | âœ“ | âœ— | $O(Ld)$ | $O(Ld)$ |
| Sinusoidal | ç»å¯¹ | è¿‘ä¼¼ | âœ“ | ä¸­ | $O(Ld)$ | 0 |
| T5 RPE | ç›¸å¯¹ | âœ“ | âœ— | ä¸­ | $O(L)$ | $O(L)$ |
| ALiBi | ç›¸å¯¹ | âœ“ | âœ— | âœ“ | $O(L^2)$ | 0 |
| **RoPE** | **ç»å¯¹â†’ç›¸å¯¹** | **âœ“** | **âœ“** | **âœ“** | **$O(Ld)$** | **0/å¯ç¼“å­˜** |
| KERPLE | æ ¸æ–¹æ³• | âœ“ | âœ— | ä¸­ | $O(L^2d)$ | $O(Ld)$ |
| xPos | ç›¸å¯¹+è¡°å‡ | âœ“ | âœ— | âœ“ | $O(L^2)$ | 0 |

**ç»“è®º**ï¼šRoPEåœ¨å¤šä¸ªç»´åº¦ä¸Šå–å¾—å¹³è¡¡ï¼Œæ˜¯ç›®å‰æœ€versatileçš„ä½ç½®ç¼–ç ä¹‹ä¸€ã€‚

### 4.4 çº¿æ€§Attentionä¸­çš„æŒ‘æˆ˜

#### 4.4.1 éè´Ÿæ€§è¦æ±‚çš„ç ´å

æ ‡å‡†çº¿æ€§Attentionè¦æ±‚ï¼š
\begin{equation}
\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = \phi(\boldsymbol{q}_i)^T \varphi(\boldsymbol{k}_j) \geq 0
\end{equation}
å…¶ä¸­$\phi, \varphi$æ˜¯éè´Ÿæ¿€æ´»å‡½æ•°ï¼ˆå¦‚$\text{elu}(x)+1$ï¼‰ã€‚

åº”ç”¨RoPEåï¼š
\begin{equation}
[\boldsymbol{\mathcal{R}}_i \phi(\boldsymbol{q}_i)]^T [\boldsymbol{\mathcal{R}}_j \varphi(\boldsymbol{k}_j)] = \phi(\boldsymbol{q}_i)^T \boldsymbol{\mathcal{R}}_{j-i} \varphi(\boldsymbol{k}_j)
\end{equation}
**å¯èƒ½ä¸ºè´Ÿ**ï¼å› ä¸ºæ—‹è½¬çŸ©é˜µ$\boldsymbol{\mathcal{R}}_{j-i}$ä¸ä¿æŒéè´Ÿæ€§ã€‚

#### 4.4.2 å½’ä¸€åŒ–æ–¹æ¡ˆçš„è°ƒæ•´

åŸæ–‡æå‡ºçš„è§£å†³æ–¹æ¡ˆï¼š
\begin{equation}
\text{Attention}_i = \frac{\sum_{j=1}^n [\boldsymbol{\mathcal{R}}_i \phi(\boldsymbol{q}_i)]^T [\boldsymbol{\mathcal{R}}_j \varphi(\boldsymbol{k}_j)] \boldsymbol{v}_j}{\sum_{j=1}^n \phi(\boldsymbol{q}_i)^T \varphi(\boldsymbol{k}_j)}
\end{equation}

**å…³é”®æ€æƒ³**ï¼š
- åˆ†å­ï¼šåŒ…å«RoPEï¼Œå…è®¸è´Ÿå€¼
- åˆ†æ¯ï¼šä¸å«RoPEï¼Œä¿æŒéè´Ÿæ€§ï¼Œé¿å…é™¤é›¶

**é—®é¢˜**ï¼š
1. ä¸å†æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼ˆæƒé‡å¯èƒ½ä¸ºè´Ÿï¼‰
2. ç†è®ºæ€§è´¨ä¸æ˜ï¼ˆç¼ºä¹æ”¶æ•›æ€§ä¿è¯ï¼‰
3. å®éªŒéªŒè¯æœ‰é™ï¼ˆåŸæ–‡åªæ˜¯åˆæ­¥å®éªŒï¼‰

#### 4.4.3 æ›¿ä»£æ–¹æ¡ˆ

**æ–¹æ¡ˆ1**ï¼šä½¿ç”¨ä¸ä¾èµ–éè´Ÿæ€§çš„ç›¸ä¼¼åº¦å‡½æ•°
\begin{equation}
\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = 1 + \frac{(\boldsymbol{\mathcal{R}}_i \boldsymbol{q}_i)^T (\boldsymbol{\mathcal{R}}_j \boldsymbol{k}_j)}{\|\boldsymbol{q}_i\| \|\boldsymbol{k}_j\|}
\end{equation}
èŒƒå›´$[0, 2]$ï¼ŒRoPEä¸æ”¹å˜æ¨¡é•¿ï¼Œæ‰€ä»¥ä»éè´Ÿã€‚

**æ–¹æ¡ˆ2**ï¼šå°†RoPEåµŒå…¥åˆ°kernelå‡½æ•°ä¸­ï¼ˆKERPLEï¼‰
\begin{equation}
K(\boldsymbol{q}_i, \boldsymbol{k}_j, i, j) = \phi(\boldsymbol{q}_i)^T \varphi(\boldsymbol{k}_j) \cdot \exp\left( -\lambda |i-j| \right)
\end{equation}
ä½†è¿™åˆå›åˆ°äº†æ“ä½œAttentionçŸ©é˜µï¼Œå¤±å»äº†çº¿æ€§Attentionçš„ä¼˜åŠ¿ã€‚

**ç°çŠ¶**ï¼šRoPEåœ¨çº¿æ€§Attentionä¸­çš„åº”ç”¨ä»æ˜¯**å¼€æ”¾é—®é¢˜**ï¼Œéœ€è¦æ›´å¤šç†è®ºå’Œå®éªŒç ”ç©¶ã€‚

---

## ğŸ’» ç¬¬5éƒ¨åˆ†ï¼šä»£ç å®ç°ã€å®éªŒåˆ†æä¸æœªæ¥å±•æœ›

### 5.1 å®Œæ•´çš„ç”Ÿäº§çº§å®ç°

#### 5.1.1 æ ‡å‡†Attention + RoPE

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class RoPEAttention(nn.Module):
    def __init__(self, d_model, n_heads, max_len=2048, theta_base=10000, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        # Q, K, VæŠ•å½±
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # RoPE
        self.rope = RoPEPositionEncoding(self.d_k, max_len, theta_base)

    def forward(self, x, mask=None):
        """
        å‚æ•°:
            x: shape (batch_size, seq_len, d_model)
            mask: shape (batch_size, 1, seq_len, seq_len) æˆ– None

        è¿”å›:
            output: shape (batch_size, seq_len, d_model)
        """
        batch_size, seq_len, _ = x.shape

        # çº¿æ€§æŠ•å½±
        Q = self.W_q(x)  # (batch_size, seq_len, d_model)
        K = self.W_k(x)
        V = self.W_v(x)

        # åˆ†å¤´
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # (batch_size, n_heads, seq_len, d_k)
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)

        # åº”ç”¨RoPEåˆ°Qå’ŒK
        # æ³¨æ„ï¼šRoPEå¯¹æ¯ä¸ªå¤´ç‹¬ç«‹åº”ç”¨
        Q_rope = []
        K_rope = []
        for head in range(self.n_heads):
            q_head = Q[:, head, :, :]  # (batch_size, seq_len, d_k)
            k_head = K[:, head, :, :]
            q_rope, k_rope = self.rope(q_head, k_head)
            Q_rope.append(q_rope.unsqueeze(1))
            K_rope.append(k_rope.unsqueeze(1))

        Q = torch.cat(Q_rope, dim=1)  # (batch_size, n_heads, seq_len, d_k)
        K = torch.cat(K_rope, dim=1)

        # è®¡ç®—Attentionå¾—åˆ†
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch_size, n_heads, seq_len, seq_len)

        # åº”ç”¨maskï¼ˆå¦‚æœæä¾›ï¼‰
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # Softmax
        attn_weights = F.softmax(scores, dim=-1)  # (batch_size, n_heads, seq_len, seq_len)
        attn_weights = self.dropout(attn_weights)

        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(attn_weights, V)  # (batch_size, n_heads, seq_len, d_k)

        # åˆå¹¶å¤šå¤´
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch_size, seq_len, d_model)

        # è¾“å‡ºæŠ•å½±
        output = self.W_o(output)

        return output
```

#### 5.1.2 çº¿æ€§Attention + RoPEï¼ˆå®éªŒæ€§ï¼‰

```python
class RoPELinearAttention(nn.Module):
    def __init__(self, d_model, n_heads, max_len=2048, theta_base=10000, activation='elu'):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        # Q, K, VæŠ•å½±
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        # æ¿€æ´»å‡½æ•°ï¼ˆä¿è¯éè´Ÿï¼‰
        if activation == 'elu':
            self.phi = lambda x: F.elu(x) + 1
        elif activation == 'relu':
            self.phi = F.relu
        else:
            raise ValueError(f"Unknown activation: {activation}")

        # RoPE
        self.rope = RoPEPositionEncoding(self.d_k, max_len, theta_base)

    def forward(self, x):
        """
        å‚æ•°:
            x: shape (batch_size, seq_len, d_model)

        è¿”å›:
            output: shape (batch_size, seq_len, d_model)
        """
        batch_size, seq_len, _ = x.shape

        # çº¿æ€§æŠ•å½±
        Q = self.W_q(x)
        K = self.W_k(x)
        V = self.W_v(x)

        # åˆ†å¤´
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)

        # åº”ç”¨æ¿€æ´»å‡½æ•°
        Q_phi = self.phi(Q)  # (batch_size, n_heads, seq_len, d_k)
        K_phi = self.phi(K)

        # åº”ç”¨RoPEï¼ˆæŒ‰åŸæ–‡æ–¹æ¡ˆï¼šåªåœ¨åˆ†å­ï¼‰
        Q_rope = []
        K_rope = []
        for head in range(self.n_heads):
            q_head = Q_phi[:, head, :, :]
            k_head = K_phi[:, head, :, :]
            q_rope, k_rope = self.rope(q_head, k_head)
            Q_rope.append(q_rope.unsqueeze(1))
            K_rope.append(k_rope.unsqueeze(1))

        Q_rope = torch.cat(Q_rope, dim=1)  # (batch_size, n_heads, seq_len, d_k)
        K_rope = torch.cat(K_rope, dim=1)

        # è®¡ç®—åˆ†å­ï¼ˆå«RoPEï¼‰
        # numerator = sum_j [R_i phi(q_i)]^T [R_j phi(k_j)] v_j
        # ä½¿ç”¨çŸ©é˜µä¹˜æ³•åŠ é€Ÿï¼š(Q_rope @ K_rope^T) @ V
        numerator = torch.matmul(
            torch.matmul(Q_rope, K_rope.transpose(-2, -1)),  # (batch_size, n_heads, seq_len, seq_len)
            V  # (batch_size, n_heads, seq_len, d_k)
        )  # (batch_size, n_heads, seq_len, d_k)

        # è®¡ç®—åˆ†æ¯ï¼ˆä¸å«RoPEï¼‰
        # denominator = sum_j phi(q_i)^T phi(k_j)
        denominator = torch.sum(
            torch.matmul(Q_phi, K_phi.transpose(-2, -1)),  # (batch_size, n_heads, seq_len, seq_len)
            dim=-1, keepdim=True  # (batch_size, n_heads, seq_len, 1)
        )

        # å½’ä¸€åŒ–
        output = numerator / (denominator + 1e-6)  # (batch_size, n_heads, seq_len, d_k)

        # åˆå¹¶å¤šå¤´
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

        # è¾“å‡ºæŠ•å½±
        output = self.W_o(output)

        return output
```

### 5.2 å·¥ç¨‹æœ€ä½³å®è·µ

#### 5.2.1 æ··åˆç²¾åº¦æ”¯æŒ

```python
from torch.cuda.amp import autocast

class RoPEPositionEncoding(nn.Module):
    # ... (ä¹‹å‰çš„ä»£ç )

    @autocast(enabled=False)  # å¼ºåˆ¶ä½¿ç”¨FP32
    def forward(self, q, k=None):
        # è½¬æ¢åˆ°FP32
        q_fp32 = q.float()
        k_fp32 = k.float() if k is not None else None

        # åº”ç”¨RoPEï¼ˆFP32ï¼‰
        q_rope, k_rope = self._apply_rope(q_fp32, k_fp32)

        # è½¬æ¢å›åŸå§‹ç²¾åº¦
        q_rope = q_rope.to(q.dtype)
        k_rope = k_rope.to(k.dtype) if k_rope is not None else None

        return q_rope, k_rope
```

#### 5.2.2 æ¢¯åº¦è£å‰ªä¸ç›‘æ§

```python
# è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        output = model(batch)
        loss = criterion(output, batch['labels'])

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # ç›‘æ§RoPEç›¸å…³æ¢¯åº¦
        for name, param in model.named_parameters():
            if 'rope' in name and param.grad is not None:
                grad_norm = param.grad.norm().item()
                print(f"{name} grad norm: {grad_norm}")

        optimizer.step()
```

#### 5.2.3 é¢„è®­ç»ƒè¿ç§»ç­–ç•¥

ä»Sinusoidal PEè¿ç§»åˆ°RoPEï¼š

```python
def convert_sinusoidal_to_rope(pretrained_model, new_model):
    """
    å°†é¢„è®­ç»ƒçš„Sinusoidal PEæ¨¡å‹è¿ç§»åˆ°RoPE

    ç­–ç•¥ï¼š
    1. å¤åˆ¶æ‰€æœ‰éä½ç½®ç¼–ç çš„æƒé‡
    2. RoPEä»å¤´å¼€å§‹ï¼ˆå› ä¸ºç¼–ç æ–¹å¼å®Œå…¨ä¸åŒï¼‰
    3. ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡å¾®è°ƒ
    """
    # å¤åˆ¶æƒé‡
    pretrained_dict = pretrained_model.state_dict()
    new_dict = new_model.state_dict()

    # è¿‡æ»¤æ‰ä½ç½®ç¼–ç ç›¸å…³å‚æ•°
    pretrained_dict = {k: v for k, v in pretrained_dict.items()
                       if 'position' not in k.lower()}

    # æ›´æ–°æ–°æ¨¡å‹
    new_dict.update(pretrained_dict)
    new_model.load_state_dict(new_dict, strict=False)

    # å†»ç»“éRoPEå‚æ•°ï¼ˆå¯é€‰ï¼‰
    for name, param in new_model.named_parameters():
        if 'rope' not in name:
            param.requires_grad = False

    return new_model
```

### 5.3 RoFormerå®éªŒç»“æœæ·±å…¥åˆ†æ

#### 5.3.1 CAIL2019-SCMä»»åŠ¡è¯¦è§£

**ä»»åŠ¡æè¿°**ï¼šä¸­å›½æ³•å¾‹æ¡ˆä»¶ç›¸ä¼¼æ€§åŒ¹é…
- è¾“å…¥ï¼šä¸¤ä¸ªæ³•å¾‹æ¡ˆä»¶æè¿°ï¼ˆé•¿æ–‡æœ¬ï¼Œå¹³å‡800-1000å­—ï¼‰
- è¾“å‡ºï¼šäºŒåˆ†ç±»ï¼ˆç›¸ä¼¼/ä¸ç›¸ä¼¼ï¼‰
- æŒ‘æˆ˜ï¼šé•¿æ–‡æœ¬ç†è§£ï¼Œæ³•å¾‹ä¸“ä¸šæœ¯è¯­

**æ•°æ®ç»Ÿè®¡**ï¼š
- è®­ç»ƒé›†ï¼š8,964å¯¹æ¡ˆä»¶
- éªŒè¯é›†ï¼š1,120å¯¹æ¡ˆä»¶
- æµ‹è¯•é›†ï¼š1,343å¯¹æ¡ˆä»¶
- å¹³å‡é•¿åº¦ï¼š~900å­—ï¼ˆè¿œè¶…BERTçš„512é™åˆ¶ï¼‰

#### 5.3.2 å®éªŒè®¾ç½®

| æ¨¡å‹ | maxlen | batch_size | å­¦ä¹ ç‡ | è®­ç»ƒæ­¥æ•° |
|------|--------|------------|--------|----------|
| BERT-512 | 512 | 16 | 2e-5 | 3 epochs |
| WoBERT-512 | 512 | 16 | 2e-5 | 3 epochs |
| RoFormer-512 | 512 | 16 | 2e-5 | 3 epochs |
| RoFormer-1024 | 1024 | 8 | 1e-5 | 3 epochs |

#### 5.3.3 ç»“æœåˆ†æ

\begin{array}{c|cc|c}
\hline
\text{æ¨¡å‹} & \text{éªŒè¯é›†} & \text{æµ‹è¯•é›†} & \Delta \text{ï¼ˆç›¸å¯¹BERTï¼‰} \\
\hline
\text{BERT-512} & 64.13\% & 67.77\% & - \\
\text{WoBERT-512} & 64.07\% & 68.10\% & +0.33\% \\
\text{RoFormer-512} & 64.13\% & 68.29\% & +0.52\% \\
\textbf{RoFormer-1024} & \textbf{66.07\%} & \textbf{69.79\%} & \textbf{+2.02\%} \\
\hline
\end{array}

**å…³é”®å‘ç°**ï¼š
1. **çŸ­æ–‡æœ¬æ€§èƒ½ç›¸å½“**ï¼šRoFormer-512ä¸BERT-512æ€§èƒ½æ¥è¿‘ï¼ˆç”šè‡³ç•¥ä¼˜ï¼‰ï¼ŒéªŒè¯RoPEä¸æŸå®³çŸ­æ–‡æœ¬èƒ½åŠ›
2. **é•¿æ–‡æœ¬ä¼˜åŠ¿æ˜¾è‘—**ï¼šRoFormer-1024ç›¸æ¯”RoFormer-512æå‡1.94%ï¼ˆéªŒè¯é›†ï¼‰ï¼Œè¯´æ˜é•¿æ–‡æœ¬ä¿¡æ¯æœ‰æ•ˆåˆ©ç”¨
3. **å¤–æ¨æ€§éªŒè¯**ï¼šRoFormeråœ¨1024é•¿åº¦ï¼ˆè¶…å‡ºé¢„è®­ç»ƒçš„512ï¼‰ä»è¡¨ç°è‰¯å¥½ï¼Œä½“ç°RoPEçš„å¤–æ¨èƒ½åŠ›

#### 5.3.4 æ¶ˆèå®éªŒ

**é¢‘ç‡åŸºæ•°çš„å½±å“**ï¼ˆRoFormer-512ï¼ŒéªŒè¯é›†ï¼‰ï¼š

| $\theta_{\text{base}}$ | å‡†ç¡®ç‡ | è¯´æ˜ |
|------------------------|--------|------|
| 100 | 63.21% | ä½é¢‘ä¸è¶³ |
| 1000 | 63.89% | æ¬¡ä¼˜ |
| **10000** | **64.13%** | **æœ€ä¼˜** |
| 100000 | 63.75% | é«˜é¢‘ä¸è¶³ |

**å½’ä¸€åŒ–æ–¹æ¡ˆçš„å½±å“**ï¼ˆRoFormer-512ï¼ŒéªŒè¯é›†ï¼‰ï¼š

| å½’ä¸€åŒ– | å‡†ç¡®ç‡ | è¯´æ˜ |
|--------|--------|------|
| æ— ï¼ˆå‘é‡æ¨¡é•¿è‡ªç”±ï¼‰ | 62.45% | ä¸ç¨³å®š |
| LayerNormï¼ˆQ/Kåï¼‰ | 63.58% | ç ´åç›¸å¯¹ä½ç½® |
| **æ­£äº¤æ€§ä¿æŒï¼ˆRoPEï¼‰** | **64.13%** | **æœ€ä½³** |

### 5.4 å­¦ä¹ è·¯çº¿å›¾

#### 5.4.1 å‰ç½®çŸ¥è¯†

**æ•°å­¦åŸºç¡€**ï¼š
1. **çº¿æ€§ä»£æ•°**ï¼š
   - æ—‹è½¬çŸ©é˜µä¸æ­£äº¤å˜æ¢
   - ç‰¹å¾å€¼åˆ†è§£
   - å—å¯¹è§’çŸ©é˜µ
2. **å¤å˜å‡½æ•°**ï¼š
   - æ¬§æ‹‰å…¬å¼ï¼š$e^{\mathrm{i}\theta} = \cos\theta + \mathrm{i}\sin\theta$
   - å¤æ•°ä¹˜æ³•çš„å‡ ä½•æ„ä¹‰
3. **ç¾¤è®º**ï¼ˆå¯é€‰ï¼‰ï¼š
   - æ—‹è½¬ç¾¤$SO(2)$çš„æ€§è´¨
   - ç¾¤åŒæ€

**æ·±åº¦å­¦ä¹ åŸºç¡€**ï¼š
1. Transformeræ¶æ„
2. Attentionæœºåˆ¶
3. ä½ç½®ç¼–ç çš„å¿…è¦æ€§

#### 5.4.2 æ¨èè®ºæ–‡

**æ ¸å¿ƒè®ºæ–‡**ï¼š
1. **RoFormerè®ºæ–‡**ï¼ˆå¿…è¯»ï¼‰ï¼š
   - Su et al. (2021). "RoFormer: Enhanced Transformer with Rotary Position Embedding"
   - arXiv:2104.09864

**ç›¸å…³å·¥ä½œ**ï¼š
2. **Sinusoidal PE**ï¼š
   - Vaswani et al. (2017). "Attention is All You Need"
3. **T5 RPE**ï¼š
   - Raffel et al. (2019). "Exploring the Limits of Transfer Learning"
4. **ALiBi**ï¼š
   - Press et al. (2021). "Train Short, Test Long: Attention with Linear Biases"
5. **çº¿æ€§Attention**ï¼š
   - Katharopoulos et al. (2020). "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"

**åç»­æ”¹è¿›**ï¼š
6. **xPos**ï¼š
   - Sun et al. (2022). "A Length-Extrapolatable Transformer"
7. **YaRN**ï¼š
   - Peng et al. (2023). "YaRN: Efficient Context Window Extension of Large Language Models"

#### 5.4.3 ä»£ç èµ„æº

1. **å®˜æ–¹å®ç°**ï¼ˆä¸­æ–‡ï¼‰ï¼š
   - GitHub: [ZhuiyiTechnology/roformer](https://github.com/ZhuiyiTechnology/roformer)

2. **HuggingFaceé›†æˆ**ï¼š
   ```python
   from transformers import RoFormerModel, RoFormerTokenizer

   tokenizer = RoFormerTokenizer.from_pretrained("junnyu/roformer_chinese_base")
   model = RoFormerModel.from_pretrained("junnyu/roformer_chinese_base")
   ```

3. **LLaMAä¸­çš„RoPE**ï¼ˆè‹±æ–‡ï¼‰ï¼š
   - GitHub: [facebookresearch/llama](https://github.com/facebookresearch/llama)
   - æ³¨æ„ï¼šLLaMAä½¿ç”¨çš„æ˜¯RoPEçš„ç®€åŒ–ç‰ˆæœ¬

### 5.5 æœªæ¥ç ”ç©¶æ–¹å‘

#### 5.5.1 å¯å­¦ä¹ çš„é¢‘ç‡å‚æ•°

**Motivation**ï¼šå›ºå®šçš„$\theta_i = 10000^{-2i/d}$å¯èƒ½ä¸æ˜¯æ‰€æœ‰ä»»åŠ¡çš„æœ€ä¼˜é€‰æ‹©ã€‚

**æ–¹æ¡ˆ**ï¼š
\begin{equation}
\theta_i = \theta_{\text{base}}^{-2i/d} \cdot \exp(\alpha_i)
\end{equation}
å…¶ä¸­$\alpha_i$æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œåˆå§‹åŒ–ä¸º0ã€‚

**æŒ‘æˆ˜**ï¼š
- è¿‡æ‹Ÿåˆé£é™©ï¼šå¢åŠ $d/2$ä¸ªå‚æ•°
- ä¼˜åŒ–éš¾åº¦ï¼šé¢‘ç‡å‚æ•°ä¸æ¨¡å‹å‚æ•°çš„è€¦åˆ

#### 5.5.2 æ··åˆä½ç½®ç¼–ç æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šRoPE + Learned Bias**
\begin{equation}
\text{score}(q_i, k_j) = (\boldsymbol{\mathcal{R}}_i \boldsymbol{q}_i)^T (\boldsymbol{\mathcal{R}}_j \boldsymbol{k}_j) + b_{i-j}
\end{equation}
å…¶ä¸­$b_{\Delta}$æ˜¯å¯å­¦ä¹ çš„ç›¸å¯¹ä½ç½®åç½®ã€‚

**æ–¹æ¡ˆ2ï¼šRoPE + ALiBi**
\begin{equation}
\text{score}(q_i, k_j) = (\boldsymbol{\mathcal{R}}_i \boldsymbol{q}_i)^T (\boldsymbol{\mathcal{R}}_j \boldsymbol{k}_j) - \lambda |i-j|
\end{equation}
ç»“åˆRoPEçš„ç›¸å¯¹ä½ç½®å’ŒALiBiçš„çº¿æ€§è¡°å‡ã€‚

#### 5.5.3 å¤šæ¨¡æ€RoPE

**æŒ‘æˆ˜**ï¼šä¸åŒæ¨¡æ€ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼‰çš„ä½ç½®æ¦‚å¿µä¸åŒã€‚
- æ–‡æœ¬ï¼šçº¿æ€§åºåˆ—ï¼ˆ1Dï¼‰
- å›¾åƒï¼šç©ºé—´ç½‘æ ¼ï¼ˆ2Dï¼‰
- è§†é¢‘ï¼šæ—¶ç©ºç½‘æ ¼ï¼ˆ3Dï¼‰

**2D RoPE**ï¼ˆç”¨äºVision Transformerï¼‰ï¼š
\begin{equation}
\boldsymbol{\mathcal{R}}_{(h, w)} = \boldsymbol{\mathcal{R}}_h^{\text{height}} \otimes \boldsymbol{\mathcal{R}}_w^{\text{width}}
\end{equation}
å…¶ä¸­$\otimes$æ˜¯Kroneckerç§¯ï¼Œ$h, w$æ˜¯å›¾åƒpatchçš„è¡Œåˆ—ç´¢å¼•ã€‚

**3D RoPE**ï¼ˆç”¨äºè§†é¢‘ï¼‰ï¼š
\begin{equation}
\boldsymbol{\mathcal{R}}_{(t, h, w)} = \boldsymbol{\mathcal{R}}_t^{\text{time}} \otimes \boldsymbol{\mathcal{R}}_h^{\text{height}} \otimes \boldsymbol{\mathcal{R}}_w^{\text{width}}
\end{equation}

#### 5.5.4 ç†è®ºæ·±åŒ–

**å¼€æ”¾é—®é¢˜**ï¼š
1. **æ”¶æ•›æ€§åˆ†æ**ï¼šRoPEæ˜¯å¦å½±å“Transformerçš„ä¼˜åŒ–åŠ¨åŠ›å­¦ï¼Ÿ
2. **æ³›åŒ–æ€§ä¿è¯**ï¼šRoPEå¦‚ä½•å½±å“æ¨¡å‹çš„æ³›åŒ–è¯¯å·®ç•Œï¼Ÿ
3. **å¤–æ¨æé™**ï¼šRoPEçš„å¤–æ¨èƒ½åŠ›çš„ç†è®ºä¸Šç•Œæ˜¯ä»€ä¹ˆï¼Ÿ
4. **çº¿æ€§Attentionç†è®º**ï¼šå¦‚ä½•ä¸¥æ ¼åˆ†æRoPEåœ¨çº¿æ€§Attentionä¸­çš„è¡Œä¸ºï¼Ÿ

**å¯èƒ½çš„ç ”ç©¶æ–¹å‘**ï¼š
- ç”¨ç¥ç»åˆ‡ç©ºé—´ç†è®ºï¼ˆNTKï¼‰åˆ†æRoPEçš„æ”¶æ•›æ€§
- ç”¨PACå­¦ä¹ ç†è®ºåˆ†æRoPEçš„æ³›åŒ–æ€§
- ç”¨æŒ¯è¡ç§¯åˆ†ç†è®ºä¸¥æ ¼è¯æ˜è¿œç¨‹è¡°å‡ç‡

---

## ğŸ“š æ€»ç»“ä¸å±•æœ›

### æ ¸å¿ƒè´¡çŒ®å›é¡¾

RoPEï¼ˆæ—‹è½¬å¼ä½ç½®ç¼–ç ï¼‰æ˜¯ä¸€ç§ä¼˜é›…çš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œæ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ï¼š

1. **ç†è®ºä¸¥è°¨æ€§**ï¼šé€šè¿‡æ—‹è½¬å˜æ¢ç²¾ç¡®å®ç°"ä»¥ç»å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼è¾¾åˆ°ç›¸å¯¹ä½ç½®ç¼–ç çš„æ•ˆæœ"
2. **æ­£äº¤æ€§ä¿æŒ**ï¼šä¸æ”¹å˜å‘é‡æ¨¡é•¿ï¼Œç»´æŒæ¨¡å‹è®­ç»ƒç¨³å®šæ€§
3. **å¤–æ¨æ€§ä¼˜å¼‚**ï¼šç†è®ºä¸Šæ”¯æŒä»»æ„é•¿åº¦ï¼Œå®è·µéªŒè¯é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›
4. **çº¿æ€§å…¼å®¹æ€§**ï¼šå”¯ä¸€å¯ç”¨äºçº¿æ€§Attentionçš„ç›¸å¯¹ä½ç½®ç¼–ç 

### å®è·µä»·å€¼

- **å¹¿æ³›åº”ç”¨**ï¼šRoPEå·²è¢«LLaMAã€PaLMã€GPT-NeoXç­‰å¤§æ¨¡å‹é‡‡ç”¨
- **å·¥ç¨‹å‹å¥½**ï¼šé›¶å‚æ•°ï¼Œæ˜“äºå®ç°ï¼Œè®¡ç®—é«˜æ•ˆ
- **æ€§èƒ½æå‡**ï¼šåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿä½ç½®ç¼–ç 

### æœªæ¥å±•æœ›

RoPEå¼€å¯äº†ä½ç½®ç¼–ç çš„æ–°èŒƒå¼ï¼Œä½†ä»æœ‰å¹¿é˜”çš„æ¢ç´¢ç©ºé—´ï¼š
- **ç†è®ºå®Œå–„**ï¼šæ”¶æ•›æ€§ã€æ³›åŒ–æ€§çš„ä¸¥æ ¼è¯æ˜
- **æ–¹æ³•æ”¹è¿›**ï¼šå¯å­¦ä¹ é¢‘ç‡ã€æ··åˆç¼–ç ã€å¤šæ¨¡æ€æ‰©å±•
- **åº”ç”¨æ‹“å±•**ï¼šä»NLPåˆ°CVã€éŸ³é¢‘ã€å¤šæ¨¡æ€å¤§æ¨¡å‹

ä½ç½®ç¼–ç çš„ç ”ç©¶è¿œæœªç»“æŸï¼ŒRoPEä¸ºæˆ‘ä»¬æŒ‡æ˜äº†ä¸€ä¸ªå……æ»¡æ½œåŠ›çš„æ–¹å‘ï¼

---

**å‚è€ƒæ–‡çŒ®**

[1] Su, J., Lu, Y., Pan, S., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864.

[2] Vaswani, A., et al. (2017). Attention is All You Need. NeurIPS.

[3] Raffel, C., et al. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. JMLR.

[4] Press, O., Smith, N., & Lewis, M. (2021). Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. ICLR.

[5] Katharopoulos, A., et al. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. ICML.

[6] Sun, Y., et al. (2022). A Length-Extrapolatable Transformer. ACL.

[7] Peng, B., et al. (2023). YaRN: Efficient Context Window Extension of Large Language Models. arXiv:2309.00071.

