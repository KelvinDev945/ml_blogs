---
title: 当生成模型肆虐：互联网将有“疯牛病”之忧？
slug: 当生成模型肆虐互联网将有疯牛病之忧
date: 2023-07-14
tags: 生成模型, 生成模型, attention, 优化, 语言模型
status: pending
---

# 当生成模型肆虐：互联网将有“疯牛病”之忧？

**原文链接**: [https://spaces.ac.cn/archives/9687](https://spaces.ac.cn/archives/9687)

**发布日期**: 

---

众所周知，不管是文本还是视觉领域，各种生成模型正在以无法阻挡的势头“肆虐”互联网。虽然大家都明白，实现真正的通用人工智能（AGI）还有很长的路要走，但这并不妨碍人们越来越频繁地利用生成模型来创作和分享内容。君不见，很多网络文章已经配上了Stable Diffusion模型生成的插图；君不见，很多新闻风格已经越来越显现出ChatGPT的影子。看似无害的这种趋势，正悄然引发了一个问题：我们是否应该对互联网上充斥的生成模型数据保持警惕？

近期发表的论文[《Self-Consuming Generative Models Go MAD》](https://papers.cool/arxiv/2307.01850)揭示了一种令人担忧的可能性，那就是生成模型正在互联网上的无节制扩张，可能会导致一场数字版的“疯牛病”疫情。本文一起学习这篇论文，探讨其可能带来的影响。

## “食自己” #

一方面，人们使用生成模型的频率越来越高，将会导致互联网上由生成模型创作的内容越来越多；另一方面，生成模型也在更新迭代，其所用的数据也是从互联网爬取的，可以想像，后面的训练集中由生成模型创作的部分占比将会越来越高。换句话说，后面的每一代模型迭代时可能都没有足够多的新鲜数据，纯粹是用自己生产的数据来训，用广东话说就是“食自己”，这将导致模型的质量或者多样性越来越差，原论文称之为“模型自噬紊乱（Model Autophagy Disorder，MAD）”。

无独有偶，生物学上也曾出现了类似的例子。牛是草食动物，然而，一些畜牧业者为了增强其营养供应，将其他牛的残骸（包括大脑）粉碎并混入饲料中。这在当时看起来是一个机智的做法，但未曾想到最后导致了“[疯牛症](https://zh.wikipedia.org/zh-cn/%E7%89%9B%E8%85%A6%E6%B5%B7%E7%B6%BF%E7%8B%80%E7%97%85%E8%AE%8A)”的出现和大规模传播。这一事例说明，长期的“食自己”可能会导致有害因素累积在生物体内，一旦达到一定程度，甚至可能触发灾难性的疾病。

因此，我们同样需要反思生成模型的“肆虐”是否会在互联网上引发另一场“疯牛症”——这不仅可能导致信息的同质化，使得各种内容开始变得千篇一律，缺乏原创性和多样性，还有可能引发一系列无法预见的问题。

## 降多样性 #

可能有读者会产生疑问：生成模型不就是对真实数据分布的模拟吗？即便连续地使用生成模型的数据进行迭代训练，应该只是在重复呈现真实的数据分布，怎么会导致多样性的丧失呢？

这其中的原因是多方面的。首先，训练生成模型的数据往往并非直接取自真实分布，而是经过人为的加工处理，比如去噪、规范化和对齐。经过加工后，训练集就已经丧失了部分多样性。例如，我们之所以能观察到很多新闻报道或知乎回答都有一股ChatGPT的味道，并非是因为内容本身，而是因为它们的格式与ChatGPT的相似性，这就说明ChatGPT的训练数据和输出结果的风格都比较明显且局限。再比如，为了降低图像生成模型的训练难度，我们通常需要对图像进行对齐处理，如在训练人脸生成模型时，常常需要将所有人脸的眼睛对齐到同一位置，这些操作也导致了多样性的丧失。

此外，还有一个很关键的因素是，由于生成模型本身或者训练技巧等限制，每个生成模型都无法做到完美，此时我们通常会主动地引入一些牺牲多样性来提高生成质量的技巧。比如，对于GAN、[Flow](/archives/5807)等生成模型，我们会选择降低采样噪声的方差，以获得质量更高的生成结果，这就是所谓的截断技巧或退火技巧。另外，如[《生成扩散模型漫谈（九）：条件控制生成结果》](/archives/9257)所述，在扩散模型中我们通常引入条件信息以控制输出结果，不管是Classifier-Guidance还是Classifier-Free方案，额外条件的引入也会限制生成结果的多样性。总而言之，在生成模型不尽完美时，我们在平衡质量与多样性的过程中，就主动地放弃了部分多样性。

## 正态分布 #

为了更深刻地认识到这种现象，我们接下来将探讨一些具体的例子。作为开始，我们首先考虑的是正态分布，因为它足够简单，所以求解和分析都更加清晰。但后面我们可以观察到，结果已经足够有代表性了。

假设真实分布是多元正态分布$\mathcal{N}(\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0)$，我们用来建模的分布也是正态分布$\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$，那么训练模型的过程，就是从训练集里边估计均值向量$\boldsymbol{\mu}$和协方差矩阵$\boldsymbol{\Sigma}$。接下来我们假设每一代生成模型训练时，都只用到上一代生成模型创作的数据，这是比较极端的假设，但不可否认当生成模型进一步普及时，这个假设越来越接近成立。

在这些假设下，我们从$t-1$代生成模型$\mathcal{N}(\boldsymbol{\mu}_{t-1},\boldsymbol{\Sigma}_{t-1})$中采样$n$个样本$\boldsymbol{x}_{t-1}^{(1)},\boldsymbol{x}_{t-1}^{(2)},\cdots,\boldsymbol{x}_{t-1}^{(n)}$，来训练第$t$代的生成模型：  
\begin{equation}\boldsymbol{\mu}_t = \frac{1}{n}\sum_{i=1}^n \boldsymbol{x}_{t-1}^{(i)},\quad \boldsymbol{\Sigma}_t=\frac{1}{n-1} \sum_{i=1}^n \big(\boldsymbol{x}_{t-1}^{(i)} - \boldsymbol{\mu}_t\big)\big(\boldsymbol{x}_{t-1}^{(i)} - \boldsymbol{\mu}_t\big)^{\top}\end{equation}  
注意，如果加上截断技巧，那么第$t$代的生成模型就是$\mathcal{N}(\boldsymbol{\mu}_t,\lambda\boldsymbol{\Sigma}_t)$，其中$\lambda\in(0,1)$。于是可以想象，每一代的方差（多样性）都将以$\lambda$的比率衰减下去，最后变成零（完全丧失多样性）。如果不使用截断技巧（即$\lambda=1$）是不是就没事了？并不是。根据定义$\boldsymbol{\mu}_t = \frac{1}{n}\sum\limits_{i=1}^n \boldsymbol{x}_{t-1}^{(i)}$，由于$\boldsymbol{x}_{t-1}^{(i)}$都是随机采样得到的，所以$\boldsymbol{\mu}_t$也是一个随机变量，根据正态分布的叠加性，它实际上服从  
\begin{equation}\boldsymbol{\mu}_t \sim \mathcal{N}\left(\boldsymbol{\mu}_{t-1},\frac{1}{n}\boldsymbol{\Sigma}_{t-1}\right)\quad\Rightarrow\quad\boldsymbol{\mu}_t \sim \mathcal{N}\left(\boldsymbol{\mu}_0,\frac{t}{n}\boldsymbol{\Sigma}_0\right)\end{equation}  
可以预见，当$t$足够大时，$\boldsymbol{\mu}_t$本身就会明显偏离$\boldsymbol{\mu}_0$，这对应的是质量的崩溃，而不单单是多样性的降低。

总的来说，截断技巧的引入，会大大加速多样性的丧失速度，而即便没有截断技巧，在长期有限样本的迭代训练中，生成分布也有可能明显偏离原始的真实分布。注意，正态分布这个例子所做的假设已经比一般的生成模型要弱得多，至少它的拟合能力是保证足够的，但这依然不可避免多样性衰减或者质量崩溃，而对于真实世界的数据和能力有限的生成模型来说，理论上只会更加糟糕。

## 生成模型 #

对于实际的生成模型，理论分析难以进行，所以只能通过实验来探索结果了。原论文做了非常丰富的实验，结果基本上跟正态分布的结论一致，即如果加入截断技巧的话，多样性将会迅速丧失，即使没有截断技巧，经过反复迭代后的模型依然会不可避免地出现一些偏离。

这是带有截断技巧的一个例子：  


[![带截断技巧，第1代生成结果](/usr/uploads/2023/07/3150920671.jpg)](/usr/uploads/2023/07/3150920671.jpg "点击查看原图")

带截断技巧，第1代生成结果

  


[![带截断技巧，第5代生成结果](/usr/uploads/2023/07/1165517418.jpg)](/usr/uploads/2023/07/1165517418.jpg "点击查看原图")

带截断技巧，第5代生成结果

这是不带截断技巧的一个例子：  


[![不带截断技巧，第1代生成结果](/usr/uploads/2023/07/4113965011.jpg)](/usr/uploads/2023/07/4113965011.jpg "点击查看原图")

不带截断技巧，第1代生成结果

  


[![不带截断技巧，第7代生成结果](/usr/uploads/2023/07/47157620.jpg)](/usr/uploads/2023/07/47157620.jpg "点击查看原图")

不带截断技巧，第7代生成结果

当然，“每一轮的迭代只用上一轮的模型生成的数据”这个假设比较极端，原论文还分析了每一轮都包含一定数量的真实数据的情况，这个情况有包含两个子情况：1、真实数据的采样结果一开始就恒定不变；2、每次迭代都能采样到新鲜的真实数据。第1种方式比较容易实现，但原论文显示它只能减缓退化的速度，无法从根本上解决这个问题；第2种方式虽然可以解决退化问题，但在实际背景下，我们却很难有效筛选出真实数据和模型生成的数据。

## 文章小结 #

本文探讨了当各种生成模型大规模“肆虐”互联网时可能出现的后果，在生成模型反复用自己生成的数据进行更新迭代时，可能导致信息严重同质化、丧失多样性的问题，类似于曾经因“牛吃牛”而出现的“疯牛病”。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/9687>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Jul. 14, 2023). 《当生成模型肆虐：互联网将有“疯牛病”之忧？ 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/9687>

@online{kexuefm-9687,  
title={当生成模型肆虐：互联网将有“疯牛病”之忧？},  
author={苏剑林},  
year={2023},  
month={Jul},  
url={\url{https://spaces.ac.cn/archives/9687}},  
} 


---

## 公式推导与注释

TODO: 添加详细的数学公式推导和注释

