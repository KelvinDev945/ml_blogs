---
title: 随机矩阵的谱范数的快速估计
slug: 随机矩阵的谱范数的快速估计
date: 2025-10-12
source: https://spaces.ac.cn/archives/11335
tags: 机器学习
status: pending
---

# 随机矩阵的谱范数的快速估计

**原文链接**: [https://spaces.ac.cn/archives/11335](https://spaces.ac.cn/archives/11335)

**发布日期**: 2025-10-12

---

在[《高阶MuP：更简明但更高明的谱条件缩放》](https://kexue.fm/archives/10795)的“近似估计”一节中，我们曾“预支”了一个结论：“一个服从标准正态分布的$n\times m$大小的随机矩阵，它的谱范数大致是$\sqrt{n}+\sqrt{m}$。”

这篇文章我们来补充讨论这个结论，给出随机矩阵谱范数的快速估计方法。

## 随机矩阵论

设有随机矩阵$\boldsymbol{W}\in\mathbb{R}^{n\times m}$，每个元素都是从标准正态分布$\mathcal{N}(0,1)$中独立重复地采样出来的，要求估计$\boldsymbol{W}$的谱范数，也就是最大奇异值，我们以$\mathbb{E}[\Vert\boldsymbol{W}\Vert_2]$为最终的估计结果。

[[...]](https://spaces.ac.cn/archives/11335 "随机矩阵的谱范数的快速估计")


---

## 公式推导与注释

本文研究随机矩阵论中的一个重要结果：高斯随机矩阵的谱范数（最大奇异值）的期望估计。

### 1. 问题设定

设 $\boldsymbol{W} \in \mathbb{R}^{n \times m}$ 是一个随机矩阵，其每个元素独立同分布于标准正态分布：

$$W_{ij} \sim \mathcal{N}(0, 1), \quad i=1,\ldots,n, \quad j=1,\ldots,m$$

我们的目标是估计谱范数（最大奇异值）的期望：

$$\mathbb{E}[\|\boldsymbol{W}\|_2]$$

其中 $\|\boldsymbol{W}\|_2 = \sigma_{\max}(\boldsymbol{W})$ 是 $\boldsymbol{W}$ 的最大奇异值。

### 2. 谱范数与奇异值

**定义回顾**：

矩阵的谱范数定义为：

$$\|\boldsymbol{W}\|_2 = \sup_{\|\boldsymbol{x}\|_2=1} \|\boldsymbol{W}\boldsymbol{x}\|_2 = \sqrt{\lambda_{\max}(\boldsymbol{W}^T\boldsymbol{W})}$$

这等价于 $\boldsymbol{W}$ 的最大奇异值。

**关键观察**：

对于 $n \times m$ 矩阵，奇异值分解为：

$$\boldsymbol{W} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T$$

其中 $\boldsymbol{\Sigma}$ 的对角元素是奇异值 $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{\min(n,m)} \geq 0$。

### 3. Marchenko-Pastur 定理

**定理（渐近结果）**：

当 $n, m \to \infty$，且比率 $\gamma = m/n$ 保持固定时，随机矩阵 $\boldsymbol{W}/\sqrt{n}$ 的经验谱分布收敛到 Marchenko-Pastur 分布。

**推论**：

最大奇异值渐近地满足：

$$\sigma_{\max}(\boldsymbol{W}) \approx \sqrt{n}(1 + \sqrt{\gamma}) = \sqrt{n} + \sqrt{m}$$

当 $n \approx m$ 时。

### 4. 精确估计推导

**步骤 1：使用 Wishart 矩阵**

定义 Gram 矩阵：

$$\boldsymbol{G} = \boldsymbol{W}^T\boldsymbol{W} \in \mathbb{R}^{m \times m}$$

$\boldsymbol{G}$ 服从 Wishart 分布 $W_m(n, \boldsymbol{I}_m)$。

最大奇异值的平方：

$$\|\boldsymbol{W}\|_2^2 = \lambda_{\max}(\boldsymbol{G})$$

**步骤 2：Tracy-Widom 分布**

对于大的 $n, m$，最大特征值的波动遵循 Tracy-Widom 分布：

$$\frac{\lambda_{\max}(\boldsymbol{G}) - \mu_n}{\sigma_n} \xrightarrow{d} TW_1$$

其中：
- $\mu_n = (\sqrt{n} + \sqrt{m})^2$ 是中心位置
- $\sigma_n = (\sqrt{n} + \sqrt{m})(n^{-1/2} + m^{-1/2})^{1/3}$ 是尺度参数

**步骤 3：期望值估计**

因此，最大奇异值的期望近似为：

$$\mathbb{E}[\|\boldsymbol{W}\|_2] \approx \sqrt{\mu_n} = \sqrt{n} + \sqrt{m}$$

### 5. 直观理解

**为什么是 $\sqrt{n} + \sqrt{m}$？**

**几何视角**：

1. 考虑 $\boldsymbol{W}$ 作为从 $\mathbb{R}^m$ 到 $\mathbb{R}^n$ 的线性映射
2. $n$ 维输出空间贡献 $\sqrt{n}$ 的增长
3. $m$ 维输入空间贡献 $\sqrt{m}$ 的增长
4. 两者相加而非相乘，是因为它们是"串联"而非"并联"的效应

**概率视角**：

随机向量 $\boldsymbol{W}\boldsymbol{x}$ 的每个分量是 $m$ 个独立高斯变量的和：

$$(\boldsymbol{W}\boldsymbol{x})_i = \sum_{j=1}^m W_{ij}x_j$$

当 $\|\boldsymbol{x}\|_2 = 1$ 时，这是均值为 0、方差为 1 的高斯变量。

在 $n$ 个这样的变量中取最大值，期望约为 $\sqrt{2\log n}$（参见第一篇文章）。

但完整分析需要考虑所有方向，得到 $\sqrt{n} + \sqrt{m}$。

### 6. 更精确的估计

**修正项**：

更精确的估计包含对数修正项：

$$\mathbb{E}[\|\boldsymbol{W}\|_2] = \sqrt{n} + \sqrt{m} + O\left(\frac{(\log n)^{2/3}}{n^{1/6}}\right)$$

**数值验证**：

对于 $n = m = 1000$：
- 理论估计：$\sqrt{1000} + \sqrt{1000} \approx 63.2$
- 数值模拟：约 63.0

误差小于 0.5%。

### 7. 一般高斯矩阵

**非标准情况**：

如果 $W_{ij} \sim \mathcal{N}(0, \sigma^2)$，则：

$$\mathbb{E}[\|\boldsymbol{W}\|_2] \approx \sigma(\sqrt{n} + \sqrt{m})$$

**非方阵情况**：

对于 $n \ll m$ 或 $m \ll n$：

$$\mathbb{E}[\|\boldsymbol{W}\|_2] \approx \sqrt{\max(n, m)} + \sqrt{\min(n, m)}$$

主导项总是较大的维度。

### 8. 应用场景

**1. 神经网络初始化**

在深度学习中，权重矩阵的初始化需要控制谱范数：

- 如果谱范数太大，会导致梯度爆炸
- 如果谱范数太小，会导致梯度消失

常用初始化：

$$W_{ij} \sim \mathcal{N}\left(0, \frac{1}{n + m}\right)$$

使得 $\mathbb{E}[\|\boldsymbol{W}\|_2] \approx 1$。

**2. MaximalUpdate Parametrization (MuP)**

MuP 使用谱条件缩放，需要知道随机矩阵的谱范数：

- 学习率缩放：$\eta \propto \frac{1}{\sqrt{n} + \sqrt{m}}$
- 权重初始化：$\sigma \propto \frac{1}{\sqrt{n} + \sqrt{m}}$

**3. 压缩感知**

在矩阵补全和压缩感知中，需要估计测量矩阵的条件数，谱范数是关键参数。

**4. 随机投影**

Johnson-Lindenstrauss 引理使用随机投影降维，谱范数控制了保持距离的精度。

### 9. 与其他矩阵的比较

不同类型矩阵的谱范数增长：

| 矩阵类型 | 谱范数期望 | 增长速度 |
|---------|----------|---------|
| 高斯随机 $\mathcal{N}(0,1)$ | $\sqrt{n} + \sqrt{m}$ | $O(\sqrt{n})$ |
| Rademacher $\pm 1$ | $\approx \sqrt{n} + \sqrt{m}$ | $O(\sqrt{n})$ |
| 均匀 $U[-1,1]$ | $\approx \sqrt{n} + \sqrt{m}$ | $O(\sqrt{n})$ |
| 稀疏高斯（密度 $p$） | $\approx \sqrt{pn} + \sqrt{pm}$ | $O(\sqrt{pn})$ |

**结论**：谱范数主要由维度决定，对分布类型不太敏感（只要方差有限）。

### 10. 计算复杂度

**直接计算 SVD**：
- 时间复杂度：$O(nm\min(n,m))$
- 对于大矩阵不可行

**幂迭代法**：
- 时间复杂度：$O(k \cdot nm)$，其中 $k$ 是迭代次数
- 通常 $k \ll \min(n,m)$

**理论估计**：
- 时间复杂度：$O(1)$（仅计算 $\sqrt{n} + \sqrt{m}$）
- 误差：相对误差通常 < 5%

**权衡**：对于初始化和理论分析，使用 $\sqrt{n} + \sqrt{m}$ 估计足够准确且高效。

### 11. 推广：非高斯情况

**一般分布的结果**：

设矩阵元素 $W_{ij}$ 独立同分布，均值为 0，方差为 1，四阶矩有限。则当 $n, m \to \infty$ 时：

$$\mathbb{E}[\|\boldsymbol{W}\|_2] \to \sqrt{n} + \sqrt{m}$$

**普适性（Universality）**：

只要矩阵元素满足：
1. 独立同分布
2. 均值为 0
3. 方差为 1
4. 有限的高阶矩

谱范数的渐近行为就与高斯情况相同。这是随机矩阵论中的**普适性原理**。

