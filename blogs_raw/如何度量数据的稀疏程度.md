---
title: 如何度量数据的稀疏程度？
slug: 如何度量数据的稀疏程度
date: 2023-05-05
tags: 概率, 熵, 度量, 稀疏, 生成模型
status: pending
---

# 如何度量数据的稀疏程度？

**原文链接**: [https://spaces.ac.cn/archives/9595](https://spaces.ac.cn/archives/9595)

**发布日期**: 

---

在机器学习中，我们经常会谈到稀疏性，比如我们经常说注意力矩阵通常是很稀疏的。然而，不知道大家发现没有，我们似乎从没有给出过度量稀疏程度的标准方法。也就是说，以往我们关于稀疏性的讨论，仅仅是直观层面的感觉，并没有过定量分析。那么问题来了，稀疏性的度量有标准方法了吗？

经过搜索，笔者发现确实是有一些可用的指标，比如$l_1/l_2$、熵等，但由于关注视角的不同，在稀疏性度量方面并没有标准答案。本文简单记录一下笔者的结果。

## 基本结果 #

狭义上来讲，“稀疏”就是指数据中有大量的零，所以最简单的稀疏性指标就是统计零的比例。但如果仅仅是这样的话，注意力矩阵就谈不上稀疏了，因为softmax出来的结果一定是正数。所以，有必要推广稀疏的概念。一个朴素的想法是统计绝对值不超过$\epsilon$的元素比例，但这个$\epsilon$怎么确定呢？

1相比于0.0001很大，但是相比10000又很小，所以大和小的概念不是绝对的。直观来想，稀疏的向量存在很多接近于零的数，那么它的绝对值的平均肯定会比较小，又因为大和小的概念是相对的，我们不妨将这个平均值跟最大值做除法，以获得相对结果，所以一个看上去比较合理的指标是  
\begin{equation}S_0(\boldsymbol{x})=\frac{(|x_1|+|x_2|+\cdots+|x_n|)/n}{\max(|x_1|,|x_2|,\cdots,|x_n|)}\label{eq:start}\end{equation}  
其中$\boldsymbol{x}=[x_1,x_2,\cdots,x_n]\in\mathbb{R}^n$是需要评估稀疏性的向量（下面都假设$\boldsymbol{x}$并非全等向量，即至少有两个不同的元素），指标越小的向量越稀疏。不过，尽管这个指标有一定的合理性，但它却不够“光滑”，主要是$\max$这个操作极易受到异常值的影响，不能很好地反应数据统计特性。

## 光滑齐次 #

于是，我们按照[《寻求一个光滑的最大值函数》](/archives/3290)的思路，将$\max$换成它的光滑近似。$\max$标准的光滑近似是$\text{logsumexp}$：  
\begin{equation}\max(|x_1|,|x_2|,\cdots,|x_n|)\approx \frac{1}{k}\log\sum_{i=1}^n e^{k|x_i|}\end{equation}  
然而，如果这里用$\text{logsumexp}$替代$\max$，并不能起到较好的改进效果。一是因为有$e^{k|x_i|}$的放大作用，$\text{logsumexp}$同样容易受到异常值的影响，二是因为$\text{logsumexp}$没有正齐次性，反而不美（所有$x_i$乘以正数$\alpha$，稀疏性应该不改变）。在[《寻求一个光滑的最大值函数》](/archives/3290)中我们也给出了一个$\max$的具备正齐次性的光滑近似，它正好是$l_p$范数（$p > 1$）：  
\begin{equation}\max(|x_1|,|x_2|,\cdots,|x_n|)\approx \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}\triangleq l_p(\boldsymbol{x})\end{equation}  
再次观察式$\eqref{eq:start}$，我们可以发现它的分子正好是$l_1(\boldsymbol{x}) / n$，所以综合上面的结果，我们得到了度量稀疏性的一个指标为  
\begin{equation}S_{1,p}(\boldsymbol{x})=\frac{l_1(\boldsymbol{x})/n}{l_p(\boldsymbol{x})}\label{eq:s1p}\end{equation}  
如果只是固定维度的向量比较，那么$/n$可以略掉。常见的是取$p=2$，那么就得到稀疏性度量的$l_1/l_2$指标；如果取$p\to\infty$，那么其实就是指标$\eqref{eq:start}$。

## 理想性质 #

在[《“熵”不起：从熵、最大熵原理到最大熵模型（一）》](/archives/3534)介绍“熵”的概念时，我们发现可以通过几条熵应当具备的理想性质来把熵的数学形式确定下来。那么稀疏性可否效仿这一点呢？

论文[《Comparing Measures of Sparsity》](https://papers.cool/arxiv/0811.4706)做了这方面的尝试，它提出稀疏性度量$S(\boldsymbol{x})$应该具备以下几点理想性质（不失一般性，这里假设$\boldsymbol{x}$是非负向量，如果不是的话，逐项取绝对值即可）：

> **D1** 、$S([\cdots,x_i - \alpha,\cdots,x_j + \alpha,\cdots]) > S(\boldsymbol{x})$，其中$x_i > x_j$且$0 < \alpha < \frac{x_i - x_j}{2}$。这点性质说的是如果总和不变，那么越均匀越不稀疏。
> 
> **D2** 、$S(\alpha\boldsymbol{x}) = S(\boldsymbol{x})$，其中$\alpha > 0$。这点容易理解，就是指稀疏是一个相对的性质，所有元素乘以一个相同的倍数，不改变相对大小，也就不改变稀疏性。
> 
> **D3** 、$S(\alpha + \boldsymbol{x}) > S(\boldsymbol{x})$，其中$\alpha > 0$。这点也容易理解，全体元素加上一个正数，那么全体都更加远离了零，稀疏性自然要降低。
> 
> **D4** 、$S(\boldsymbol{x}) = S(\boldsymbol{x}\circ\boldsymbol{x}) = S(\boldsymbol{x}\circ\boldsymbol{x}\circ\boldsymbol{x}) = \cdots$，这里的$\circ$指两个向量拼接。这点也不难理解，就是单纯的数据复制不改变稀疏性。
> 
> **P1** 、任意给定$i\in\\{1,2,\cdots,n\\}$，存在$\beta_i > 0$，使得对于任意$\alpha > 0$，都有$S([\cdots,x_{i-1},x_i + \beta_i + \alpha,x_{i  
>  +1},\cdots]) < S([\cdots,x_{i-1},x_i + \beta_i,x_{i+1},\cdots])$。这条性质想表达的是当某个元素足够大的时候，整个向量的稀疏性就由它主导了。
> 
> **P2** 、$S(\boldsymbol{x}\circ[0]) < S(\boldsymbol{x})$。这个就很朴素了，给向量添加零，应该导致稀疏性增加。

原论文对稀疏性的各种常用指标做了推导，发现只有一个名为“Gini指数”的指标同时满足以上6点性质（但这个Gini指数比较复杂，这里就不介绍了）。不过要提醒读者的是，读原论文时需要仔细留意一下推导过程，因为笔者发现它证明$l_1/l_2$不满足**D3** 的证明是错的，事实上$l_1/l_2$是满足**D3** 的，至于其他推导笔者也没有细看，读者请在阅读过程中自行甄别正误。

## 参考证明 #

对于本文的两个指标，指标$\eqref{eq:start}$同时满足除**D1** 外的另外5点性质（如果D1的$ > $改为$\geq $，那么也满足），这些性质的判断是比较平凡的，这里不详细讨论了，请读者自行完成。

至于指标$\eqref{eq:s1p}$，可以证明它同时满足除**D4** 外的另外5点性质，其中**D3** 、**P1** 的证明稍微复杂一些，这里给出它们的参考证明。

> 为了证明**D3** ，只需要证明  
>  \begin{equation}\frac{\left(\sum\limits_{i=1}^n (x_i + \alpha)\right)^p}{\sum\limits_{i=1}^n (x_i + \alpha)^p}\end{equation}  
>  关于$\alpha > 0$是单调递增的。两边取对数，得到  
>  \begin{equation}p\log\sum\limits_{i=1}^n (x_i + \alpha) - \log\sum\limits_{i=1}^n (x_i + \alpha)^p\triangleq f(\alpha)\end{equation}  
>  我们只需要证明$f'(\alpha) > 0$。直接求导得到  
>  \begin{equation}f'(\alpha) = \frac{pn}{\sum\limits_{i=1}^n (x_i + \alpha)} - \frac{p\sum\limits_{i=1}^n (x_i + \alpha)^{p-1}}{\sum\limits_{i=1}^n (x_i + \alpha)^p}\end{equation}  
>  $f'(\alpha) > 0$等价于  
>  \begin{equation}\frac{1}{n}\sum\limits_{i=1}^n (x_i + \alpha)^p > \left(\frac{1}{n}\sum\limits_{i=1}^n (x_i + \alpha)\right)\left(\frac{1}{n}\sum\limits_{i=1}^n (x_i + \alpha)^{p-1}\right)\end{equation}  
>  这由[幂均值不等式](https://en.wikipedia.org/wiki/Generalized_mean)直接可得。

> **P1** 的证明思路是类似的，只需要证明当$x_i$足够大的时候，  
>  \begin{equation}\frac{\left(x_i + \alpha + \sum\limits_{j\neq i} x_j\right)^p}{(x_i + \alpha)^p + \sum\limits_{j\neq i} x_j^p}\end{equation}  
>  关于$\alpha > 0$是单调递减的。两边取对数，得到  
>  \begin{equation}p\log\left(x_i + \alpha + \sum\limits_{j\neq i} x_j\right) - \log\left((x_i + \alpha)^p + \sum\limits_{j\neq i} x_j^p\right)\triangleq g(\alpha)\end{equation}  
>  我们只需要证明$g'(\alpha) < 0$。直接求导得到  
>  \begin{equation}g'(\alpha) = \frac{p}{x_i + \alpha + \sum\limits_{j\neq i} x_j} - \frac{p (x_i + \alpha)^{p-1}}{(x_i + \alpha)^p + \sum\limits_{j\neq i} x_j^p}\end{equation}  
>  $g'(\alpha) < 0$等价于  
>  \begin{equation}p \sum\limits_{j\neq i} x_j^p < p(x_i+\alpha)^{p-1}\sum\limits_{j\neq i} x_j\end{equation}  
>  只要各$x_j$并非全零，那么当$x_i$足够大时，总可以使得上式对$\forall \alpha > 0$恒成立。

## 完美指标 #

尽管指标$\eqref{eq:s1p}$不满足**D4** ，但我们将它简单修改为  
\begin{equation}S_{1,p}^*(\boldsymbol{x})=\frac{l_1(\boldsymbol{x})/n}{l_p(\boldsymbol{x})/n^{1/p}}=n^{(1-p)/p}\frac{l_1(\boldsymbol{x})}{l_p(\boldsymbol{x})}\label{eq:s1p-plus}\end{equation}  
后，它是满足**D4** 的，并且也可以检验它也满足**P2** ，至于其他几点性质并不涉及到维度$n$的改变，因此跟指标$\eqref{eq:s1p}$一样也满足。也就是说，$S_{1,p}^*(\boldsymbol{x})$是同时满足6点性质的“**完美指标** ”！

由[幂均值不等式](https://en.wikipedia.org/wiki/Generalized_mean)可知$l_p(\boldsymbol{x})/n^{1/p} \geq l_1(\boldsymbol{x})/n$，所以$S_{1,p}^*(\boldsymbol{x})\leq 1$；此外，由于  
\begin{equation}\frac{l_p(\boldsymbol{x})}{l_1(\boldsymbol{x})}=\left(\left(\frac{x_1}{l_1(\boldsymbol{x})}\right)^p+\cdots+\left(\frac{x_n}{l_1(\boldsymbol{x})}\right)^p\right)^{1/p}\leq\left(\frac{x_1}{l_1(\boldsymbol{x})}+\cdots+\frac{x_n}{l_1(\boldsymbol{x})}\right)^{1/p}=1\end{equation}  
所以$S_{1,p}^*(\boldsymbol{x})\geq n^{(1-p)/p}$，综上即$S_{1,p}^*(\boldsymbol{x})\in[n^{(1-p)/p},1]$。此外，$S_{1,p}^*(\boldsymbol{x})$还可以更一般地推广为：  
\begin{equation}S_{q,p}^*(\boldsymbol{x})=\frac{l_q(\boldsymbol{x})/n^{1/q}}{l_p(\boldsymbol{x})/n^{1/p}}=n^{1/p-1/q}\frac{l_q(\boldsymbol{x})}{l_p(\boldsymbol{x})}\in\left[n^{1/p-1/q},1\right]\end{equation}  
只要$p > q > 0$，那么它也是符合6点性质的“完美指标”。

当$p=2,q=1$时，有  
\begin{equation}S_{1,2}^*(\boldsymbol{x})=\frac{1}{\sqrt{n}}\frac{l_1(\boldsymbol{x})}{l_2(\boldsymbol{x})}\in\left[\frac{1}{\sqrt{n}},1\right]\end{equation}  
这个结果可以回答关于稀疏化的一些问题。比如为什么L1正则有利于稀疏化？因为L1出现在上式的分子中，它越小越稀疏。为什么L2正则不利于稀疏化？因为L2出现在上式的分母中，它越小越不稀疏。如果要更准确地实现稀疏，应该以$S_{1,2}^*(\boldsymbol{x})$为正则项，它既最小化L1，又最大化L2，直接优化稀疏指标。

特别地，当$\boldsymbol{x}$全不为零时，我们还有关系式  
\begin{equation}S_{1,2}^*(\boldsymbol{x})=\cos(\text{sign}(\boldsymbol{x}),\boldsymbol{x})\end{equation}  
其中$\text{sign}$是对向量的每一个分量都取符号函数。这个式子很形象，$\text{sign}(\boldsymbol{x})$可以说是$\boldsymbol{x}$的最稠密的衍生向量，稀疏程度就是$\boldsymbol{x}$与最稠密那个衍生向量的相似度，相似度越小自然是代表越稀疏。

如果我们将$\boldsymbol{x}$的元素视为随机变量$x$的采样结果，那么还可以写出  
\begin{equation}S_{1,2}^*(\boldsymbol{x})=\frac{\mathbb{E}[|x|]}{\sqrt{\mathbb{E}[x^2]}}=\frac{\mathbb{E}[|x|]}{\sqrt{\mathbb{E}[|x|]^2 + \mathbb{V}ar[|x|]}} = \frac{1}{1 + \mathbb{V}ar[|x|] / \mathbb{E}[|x|]^2}\end{equation}  
单独拎出来，$\frac{\mathbb{E}[|x|]}{\sqrt{\mathbb{V}ar[|x|]}}$的平方正是$|x|$的“信噪比”（均值平方与方差之比），所以这告诉我们“$|x|$的信噪比越低，那么$x$就越稀疏”。

## 熵的联系 #

现在回到注意力矩阵上，它的特点是每一行对应一个概率分布，即自动满足$x_i \geq 0$且$x_1+\cdots+x_n=1$。最确定的概率分布是one hot分布，此时它也最稀疏；最不确定的分布是均匀分布，很显然此时它也最不稀疏。从这两个极端可以猜测，概率分布的稀疏性与不确定性有一定的关联。

我们知道，概率分布的不确定性一般用（香侬）熵来度量：  
\begin{equation}H(\boldsymbol{x}) = -\sum_{i=1}^n x_i \log x_i\in[0,\log n] \end{equation}  
而此时指标$\eqref{eq:s1p-plus}$则变为$\frac{n^{(1-p)/p}}{l_p(\boldsymbol{x})}$，它是$l_p$范数的衍生物。既然稀疏性与不确定性可能有一定的关联，那么是否意味着熵与$l_p$范数存在一定程度的相关性呢？

确实如此。事实上，基于$l_p$范数我们可以构造出[Rényi熵](https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy)：  
\begin{equation}H_p(\boldsymbol{x}) = -\frac{1}{p-1}\log \sum_{i=1}^n x_i^p \end{equation}  
可以证明$H_1(\boldsymbol{x}) = \lim\limits_{p\to 1} H_p(\boldsymbol{x}) = H(\boldsymbol{x})$，即$p\to 1$时正好对应于经典的香侬熵，而当$p \neq 1$时就是一般的Rényi熵（有些场景Rényi熵也特指$p=2$时的情形）。每种Rényi熵都可以作为不确定性的某种度量，它们的值域都是$[0,\log n]$，都是在one hot分布取的最小值、在均匀分布取得最大值。从这个意义上来说，所有的Rényi熵在一定程度上都是等价的，这就解释了熵与$l_p$范数的关联。

值得一提的是，$p \neq 1$时的Rényi熵往往对数值计算更加友好，这是因为$\sum\limits_{i=1}^n x_i^p$必然是一个正的、有界的结果，$\log$运算不用担心$\log 0$问题，而且只需要对最后的结果做一次$\log$；相反标准的香侬熵，需要对每个$x_i$都算$\log$，计算量增加而且还要特别$\text{clip}$一下以防$\log 0$的出现。

## 文章小结 #

本文系统整理了一下关于稀疏性的度量问题，并且讨论了它与L1、L2、熵等概念的联系。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/9595>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 05, 2023). 《如何度量数据的稀疏程度？ 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/9595>

@online{kexuefm-9595,  
title={如何度量数据的稀疏程度？},  
author={苏剑林},  
year={2023},  
month={May},  
url={\url{https://spaces.ac.cn/archives/9595}},  
} 


---

## 公式推导与注释

### 1. $l_p$范数的基础性质

**定义1.1 ($l_p$范数)**

对于向量$\boldsymbol{x} = [x_1, \ldots, x_n] \in \mathbb{R}^n$，其$l_p$范数（$p \geq 1$）定义为：
\begin{equation}
\|\boldsymbol{x}\|_p = \left(\sum_{i=1}^{n} |x_i|^p\right)^{1/p} \tag{1}
\end{equation}

**性质1.1 (正齐次性)**

对于任意标量$\alpha \in \mathbb{R}$：
\begin{equation}
\|\alpha \boldsymbol{x}\|_p = |\alpha| \|\boldsymbol{x}\|_p \tag{2}
\end{equation}

**证明**:
\begin{equation}
\|\alpha \boldsymbol{x}\|_p = \left(\sum |\alpha x_i|^p\right)^{1/p} = \left(|\alpha|^p \sum |x_i|^p\right)^{1/p} = |\alpha| \|\boldsymbol{x}\|_p
\end{equation}

**性质1.2 (极限性质)**

\begin{equation}
\lim_{p \to \infty} \|\boldsymbol{x}\|_p = \max_i |x_i| = \|\boldsymbol{x}\|_\infty \tag{3}
\end{equation}

**证明**: 设$M = \max_i |x_i|$。
\begin{equation}
M \leq \left(\sum |x_i|^p\right)^{1/p} \leq \left(n M^p\right)^{1/p} = n^{1/p} M
\end{equation}
当$p \to \infty$时，$n^{1/p} \to 1$，夹逼定理得证。

**性质1.3 (范数不等式)**

对于$1 \leq p < q$，有：
\begin{equation}
\|\boldsymbol{x}\|_q \leq \|\boldsymbol{x}\|_p \leq n^{1/p - 1/q} \|\boldsymbol{x}\|_q \tag{4}
\end{equation}

这说明不同范数是等价的，但数值上有差异。

### 2. 稀疏性度量的推导

**出发点**：稀疏意味着"很多元素接近0"，或者"能量集中在少数元素上"。

**朴素度量1 ($L_0$范数)**：
\begin{equation}
\|\boldsymbol{x}\|_0 = \sum_{i=1}^{n} \mathbb{I}(x_i \neq 0) \tag{5}
\end{equation}
缺点：不连续，对微小噪声敏感。

**朴素度量2 (最大值占比)**：
\begin{equation}
S_{\text{naive}}(\boldsymbol{x}) = \frac{\max_i |x_i|}{\sum_i |x_i|} = \frac{\|\boldsymbol{x}\|_\infty}{\|\boldsymbol{x}\|_1} \tag{6}
\end{equation}
值越大越稀疏（范围$[1/n, 1]$）。

**Hoyer's Measure (2004)**：
基于$L_1$和$L_2$的关系：
\begin{equation}
S_{\text{Hoyer}}(\boldsymbol{x}) = \frac{\sqrt{n} - \frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_2}}{\sqrt{n} - 1} \tag{7}
\end{equation}
范围$[0, 1]$，0最稠密（全相等），1最稀疏（one-hot）。

**本文提出的度量**：
\begin{equation}
S_{1,p}^*(\boldsymbol{x}) = n^{(1-p)/p} \frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_p} \tag{8}
\end{equation}
注意：这里采用了$S \in [n^{(1-p)/p}, 1]$的形式，值越小越稀疏（与Hoyer相反）。为了统一习惯，通常希望**值越大越稀疏**，可以取倒数或$1-S$。

若定义**稀疏度**为值越大越稀疏，则本文的$S_{1,p}^*$实际上是**稠密度**度量。

### 3. 理想稀疏性度量的六条公理（详细证明）

Hurley & Rickard (2009) 提出的公理体系：

**D1 (Robin Hood)**：从富人（大元素）拿走$\alpha$给穷人（小元素），稀疏性应降低。
设$x_i > x_j$，$\boldsymbol{y}$是将$\boldsymbol{x}$的$x_i \to x_i - \alpha, x_j \to x_j + \alpha$。
要求$S(\boldsymbol{y}) < S(\boldsymbol{x})$（稀疏性降低）。

**本文指标$S_{1,p}^*$验证（以稠密度理解，应增加）**：
$\|\boldsymbol{y}\|_1 = \|\boldsymbol{x}\|_1$（和不变）。
考察$\|\boldsymbol{y}\|_p^p - \|\boldsymbol{x}\|_p^p = (x_i-\alpha)^p + (x_j+\alpha)^p - x_i^p - x_j^p$。
设$f(t) = t^p$ ($p>1$)是严格凸函数。
根据琴生不等式或凸函数性质，更均匀的分布会使凸函数和更小。
即$\|\boldsymbol{y}\|_p < \|\boldsymbol{x}\|_p$。
因此$S_{1,p}^*(\boldsymbol{y}) = C \frac{\|\boldsymbol{y}\|_1}{\|\boldsymbol{y}\|_p} > C \frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_p} = S_{1,p}^*(\boldsymbol{x})$。
稠密度增加 $\Leftrightarrow$ 稀疏度降低。满足D1。

**D2 (Scaling)**：$S(\alpha \boldsymbol{x}) = S(\boldsymbol{x})$。
验证：
\begin{equation}
S_{1,p}^*(\alpha \boldsymbol{x}) = C \frac{\alpha \|\boldsymbol{x}\|_1}{\alpha \|\boldsymbol{x}\|_p} = S_{1,p}^*(\boldsymbol{x}) \tag{9}
\end{equation}
满足D2。

**D3 (Rising Tide)**：所有元素加上常数$\alpha > 0$，稀疏性应降低（稠密度应增加）。
需要证明$f(\alpha) = \frac{\sum (x_i+\alpha)}{(\sum (x_i+\alpha)^p)^{1/p}}$关于$\alpha$单调递增。

**证明细节**：
设$y_i(\alpha) = x_i + \alpha$。我们要证$\frac{d}{d\alpha} \ln f(\alpha) > 0$。
\begin{equation}
\ln f(\alpha) = \ln \sum y_i - \frac{1}{p} \ln \sum y_i^p \tag{10}
\end{equation}
\begin{equation}
\frac{d}{d\alpha} \ln f(\alpha) = \frac{n}{\sum y_i} - \frac{1}{p} \frac{p \sum y_i^{p-1}}{\sum y_i^p} = \frac{n}{\sum y_i} - \frac{\sum y_i^{p-1}}{\sum y_i^p} \tag{11}
\end{equation}
我们需要：
\begin{equation}
\frac{n}{\sum y_i} > \frac{\sum y_i^{p-1}}{\sum y_i^p} \iff n \sum y_i^p > (\sum y_i)(\sum y_i^{p-1}) \tag{12}
\end{equation}
这是一个切比雪夫总和不等式（Chebyshev's Sum Inequality）的反向应用？不，这是由幂均值不等式或Holder不等式推导的。
由Holder不等式：
\begin{equation}
\sum y_i \cdot \sum y_i^{p-1} \leq n \cdot \sum y_i^p \tag{13}
\end{equation}
这里有点问题。让我们检查原文的证明思路。
原文使用**幂均值不等式**：
\begin{equation}
\left(\frac{1}{n}\sum y_i^p\right)^{1/p} > \frac{1}{n}\sum y_i \tag{14}
\end{equation}
但这似乎不能直接推出(12)。

让我们重新审视(12)。
令$A = \frac{1}{n}\sum y_i^p$, $B = \frac{1}{n}\sum y_i$, $C = \frac{1}{n}\sum y_i^{p-1}$。
需证 $n^2 A > n^2 B C \implies A > B C$。
即 $\frac{1}{n}\sum y_i^p > (\frac{1}{n}\sum y_i)(\frac{1}{n}\sum y_i^{p-1})$。
这正是切比雪夫总和不等式：如果$a_i, b_i$同序单调，则$\frac{1}{n}\sum a_i b_i \geq (\frac{1}{n}\sum a_i)(\frac{1}{n}\sum b_i)$。
取$a_i = y_i, b_i = y_i^{p-1}$。因为$y_i$和$y_i^{p-1}$显然同序（$p>1$），所以不等式成立。
满足D3。

**D4 (Cloning)**：复制数据不改变稀疏性。
$S(\boldsymbol{x} \circ \boldsymbol{x})$。
$\|\boldsymbol{x} \circ \boldsymbol{x}\|_1 = 2\|\boldsymbol{x}\|_1$。
$\|\boldsymbol{x} \circ \boldsymbol{x}\|_p = (2 \sum |x_i|^p)^{1/p} = 2^{1/p} \|\boldsymbol{x}\|_p$。
新维度$2n$。
\begin{equation}
\begin{aligned}
S_{1,p}^*(\boldsymbol{x} \circ \boldsymbol{x}) &= (2n)^{(1-p)/p} \frac{2\|\boldsymbol{x}\|_1}{2^{1/p}\|\boldsymbol{x}\|_p} \\
&= 2^{(1-p)/p} n^{(1-p)/p} \cdot 2 \cdot 2^{-1/p} \frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_p} \\
&= 2^{1/p - 1 + 1 - 1/p} \cdot S_{1,p}^*(\boldsymbol{x}) \\
&= 2^0 \cdot S_{1,p}^*(\boldsymbol{x}) = S_{1,p}^*(\boldsymbol{x})
\end{aligned} \tag{15}
\end{equation}
完美满足D4。这就是为什么需要引入$n$的因子的原因。

**P1 (Bill Gates)**：一个元素无限大，稀疏性趋于最大。
当$x_i \to \infty$时，$\|\boldsymbol{x}\|_p \approx x_i$，$\|\boldsymbol{x}\|_1 \approx x_i$。
$S_{1,p}^* \to n^{(1-p)/p} \cdot 1 = n^{(1-p)/p}$。
这对应最稀疏的情况（one-hot）。（注意我们定义的S是稠密度，值越小越稀疏）。
满足P1。

**P2 (Babies)**：添加0元素，稀疏性应增加（稠密度S应减小）。
设$\boldsymbol{y} = \boldsymbol{x} \circ [0]$。
$\|\boldsymbol{y}\|_1 = \|\boldsymbol{x}\|_1$，$\|\boldsymbol{y}\|_p = \|\boldsymbol{x}\|_p$。
新维度$n+1$。
\begin{equation}
S_{1,p}^*(\boldsymbol{y}) = (n+1)^{(1-p)/p} \frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_p}
\end{equation}
由于$p>1 \implies (1-p)/p < 0$，函数$x \mapsto x^{(1-p)/p}$单调递减。
所以$(n+1)^{(1-p)/p} < n^{(1-p)/p}$。
稠密度减小 $\Leftrightarrow$ 稀疏度增加。
满足P2。

### 4. 几何解释与信噪比

**几何视角**：
考虑$p=2$。
\begin{equation}
S_{1,2}^*(\boldsymbol{x}) = \frac{1}{\sqrt{n}} \frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_2} \tag{16}
\end{equation}
令$\boldsymbol{u} = [1, 1, \ldots, 1]^T$（全1向量）。
$\|\boldsymbol{x}\|_1 = \boldsymbol{x}^T \text{sign}(\boldsymbol{x})$。若$\boldsymbol{x} \geq 0$，则$\|\boldsymbol{x}\|_1 = \boldsymbol{x}^T \boldsymbol{u}$。
\begin{equation}
\cos(\theta) = \frac{\boldsymbol{x}^T \boldsymbol{u}}{\|\boldsymbol{x}\|_2 \|\boldsymbol{u}\|_2} = \frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_2 \sqrt{n}} = S_{1,2}^*(\boldsymbol{x}) \tag{17}
\end{equation}
所以，$S_{1,2}^*$正是$\boldsymbol{x}$与"最稠密向量"（对角线向量）夹角的余弦值。
- 夹角越小（接近0），越稠密，$S \to 1$。
- 夹角越大（接近$90^\circ$），越稀疏（靠近坐标轴），$S \to 1/\sqrt{n}$。

**概率视角（信噪比）**：
设$\boldsymbol{x}$是随机变量$X$的$n$次观测。
$\mathbb{E}[|X|] \approx \frac{1}{n}\|\boldsymbol{x}\|_1$。
$\mathbb{E}[X^2] \approx \frac{1}{n}\|\boldsymbol{x}\|_2^2$。
\begin{equation}
S_{1,2}^*(\boldsymbol{x}) \approx \frac{\mathbb{E}[|X|]}{\sqrt{\mathbb{E}[X^2]}} \tag{18}
\end{equation}
利用方差公式 $\text{Var}(|X|) = \mathbb{E}[X^2] - (\mathbb{E}[|X|])^2$：
\begin{equation}
(S_{1,2}^*)^2 = \frac{(\mathbb{E}[|X|])^2}{(\mathbb{E}[|X|])^2 + \text{Var}(|X|)} = \frac{1}{1 + \frac{\text{Var}(|X|)}{(\mathbb{E}[|X|])^2}} \tag{19}
\end{equation}
分母中的项 $\frac{(\mathbb{E}[|X|])^2}{\text{Var}(|X|)}$ 正是**信噪比(SNR)**的定义（均值/标准差）的平方。
- 稀疏（很多0，偶尔大值）$\implies$ 方差大，均值小 $\implies$ SNR低 $\implies$ $S$小。
- 稠密（均匀分布）$\implies$ 方差小，均值大 $\implies$ SNR高 $\implies$ $S$大。

### 5. 与Gini系数的关系

Gini系数常用于衡量收入不平等，本质上也是稀疏性度量。
\begin{equation}
G(\boldsymbol{x}) = 1 - 2\sum_{i=1}^{n} \frac{x_{(i)}}{\|\boldsymbol{x}\|_1} \left(\frac{n-i+0.5}{n}\right) \tag{20}
\end{equation}
其中$x_{(i)}$是排序后的元素。
Gini系数满足所有6条公理。
本文的$S_{1,p}^*$虽然形式不同，但也满足所有公理，且计算更简单（不需要排序）。

### 6. 熵与$l_p$范数的深层联系

**Rényi熵**：
\begin{equation}
H_\alpha(\boldsymbol{p}) = \frac{1}{1-\alpha} \log \left(\sum_{i=1}^{n} p_i^\alpha\right) \tag{21}
\end{equation}
对于概率分布$\boldsymbol{p}$（$\|\boldsymbol{p}\|_1 = 1$）：
\begin{equation}
\sum p_i^\alpha = \|\boldsymbol{p}\|_\alpha^\alpha \tag{22}
\end{equation}
所以Rényi熵是$l_\alpha$范数的单调函数。
- $l_\alpha$范数越大 $\iff$ 越不均匀（稀疏） $\iff$ 熵越小。
- $l_\alpha$范数越小 $\iff$ 越均匀（稠密） $\iff$ 熵越大。

**Tsallis熵**：
\begin{equation}
T_q(\boldsymbol{p}) = \frac{1}{q-1} \left(1 - \sum_{i=1}^{n} p_i^q\right) \tag{23}
\end{equation}
同样由$l_q$范数决定。

### 7. 常用稀疏性度量对比表

| 指标 | 公式 | 范围 (稀疏 $\to$ 稠密) | 特点 |
|------|------|---------------------|------|
| $L_0$ | $\|\boldsymbol{x}\|_0$ | $1 \to n$ | 不连续，NP难 |
| 峭度 (Kurtosis) | $\frac{\mu_4}{\sigma^4} - 3$ | 大 $\to$ 小 | 对异常值敏感 |
| Hoyer | $\frac{\sqrt{n} - \|\boldsymbol{x}\|_1/\|\boldsymbol{x}\|_2}{\sqrt{n}-1}$ | $1 \to 0$ | 标准化良好，满足公理 |
| $L_1/L_2$ | $\frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_2}$ | $1 \to \sqrt{n}$ | 简单，未标准化 |
| **本文 $S_{1,2}^*$** | $\frac{1}{\sqrt{n}}\frac{\|\boldsymbol{x}\|_1}{\|\boldsymbol{x}\|_2}$ | $1/\sqrt{n} \to 1$ | 满足D4(复制不变性)，几何意义清晰 |
| 熵 | $-\sum p_i \log p_i$ | $0 \to \log n$ | 仅适用于概率分布 |

### 8. Python实现代码

```python
import numpy as np

def sparsity_measure(x, p=2):
    """
    计算基于L1/Lp的稀疏性度量 S_{1,p}^*
    返回稠密度 (1 = 最稠密/全相等, 接近0 = 最稀疏)
    """
    x = np.abs(x)
    n = len(x)
    
    # 避免除以0
    if np.sum(x) == 0:
        return 0.0
        
    l1 = np.sum(x)
    lp = np.sum(x**p) ** (1/p)
    
    # 归一化因子 n^{(1-p)/p}
    factor = n ** ((1-p)/p)
    
    s = factor * (l1 / lp)
    return s

def hoyer_sparsity(x):
    """
    Hoyer's measure (0 = 最稠密, 1 = 最稀疏)
    """
    x = np.abs(x)
    n = len(x)
    
    l1 = np.sum(x)
    l2 = np.sqrt(np.sum(x**2))
    
    return (np.sqrt(n) - l1/l2) / (np.sqrt(n) - 1)

# 测试
v_dense = np.array([1, 1, 1, 1])
v_sparse = np.array([1, 0, 0, 0])
v_mixed = np.array([1, 2, 0, 5])

print(f"Dense: S={sparsity_measure(v_dense):.4f}, Hoyer={hoyer_sparsity(v_dense):.4f}")
# Dense: S=1.0000, Hoyer=0.0000
print(f"Sparse: S={sparsity_measure(v_sparse):.4f}, Hoyer={hoyer_sparsity(v_sparse):.4f}")
# Sparse: S=0.5000 (1/sqrt(4)), Hoyer=1.0000
```

### 9. 总结

本文从公理化的角度探讨了稀疏性度量，证明了基于$l_1/l_p$范数比值的度量$S_{1,p}^*$满足所有理想性质（包括复制不变性）。该度量不仅计算简单，而且具有清晰的几何意义（余弦相似度）和统计学意义（信噪比），为深度学习中注意力稀疏性等问题的分析提供了有力的数学工具。



