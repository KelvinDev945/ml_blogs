---
title: T5 PEGASUS：开源一个中文生成式预训练模型
slug: t5-pegasus开源一个中文生成式预训练模型
date: 
source: https://spaces.ac.cn/archives/8209
tags: 语言模型, 文本生成, attention, 生成模型, attention
status: pending
---

# T5 PEGASUS：开源一个中文生成式预训练模型

**原文链接**: [https://spaces.ac.cn/archives/8209](https://spaces.ac.cn/archives/8209)

**发布日期**: 

---

去年在文章[《那个屠榜的T5模型，现在可以在中文上玩玩了》](/archives/7867)中我们介绍了Google的多国语言版T5模型（mT5），并给出了用mT5进行中文文本生成任务的例子。诚然，mT5做中文生成任务也是一个可用的方案，但缺乏完全由中文语料训练出来模型总感觉有点别扭，于是决心要搞一个出来。

经过反复斟酌测试，我们决定以mT5为基础架构和初始权重，先结合中文的特点完善Tokenizer，然后模仿[PEGASUS](https://papers.cool/arxiv/1912.08777)来构建预训练任务，从而训练一版新的T5模型，这就是本文所开源的T5 PEGASUS。

## Tokenizer #

首先，这里介绍我们对Tokenizer的完善工作。mT5使用的Tokenizer是[sentencepiece](https://github.com/google/sentencepiece)，这是一个C++所写的分词库，具有高效轻便的特点，但是很遗憾，对于中文来说它并不是特别友好，主要体现为：

> 1、sentencepiece会把某些全角符号强制转化为半角符号，这在某些情况下是难以接受的，而且还可能影响任务的评测结果；
> 
> 2、sentencepiece内置的算法虽然有能力分出中文词来，但对于中文分词来说其实还是不够智能的；
> 
> 3、sentencepiece用C++写的，虽然开源了，但对于用惯Python的人来说C++就相当于黑箱，难以阅读源码，改起来也不容易。

这些特点让我们决定将Tokenizer切换回BERT的Tokenizer。但直接替换原始版本的中文BERT的Tokenizer是不够的，一来是我们之前的工作[《提速不掉点：基于词颗粒度的中文WoBERT》](/archives/7758)已经表明以词为单位来做生成模型能获得更好的效果，二来哪怕只看字中文BERT的vocab.txt也是很不完善的，漏了一些常见的标点符号（如双引号）和中文字（比如“琊”等）。为此，我们选择给BERT的tokenizer加入分词功能，并进一步完善vocab.txt。

具体来说，我们往原始中文BERT的token_dict里边加入结巴分词的前20万个词，然后修改Tokenizer的逻辑，使得它能够切分出词来，这些改动都已经内置在bert4keras中了，直接调用就行。接着，我们用这个修改后的Tokenizer去遍历切分我们准备的预训练语料，统计各个token的频数，最后只保留最高频的5万个token，得到一个规模为5万的vocab.txt来构建我们最终的Tokenizer。

除了用这个新Tokenizer来训练T5 PEGASUS外，我们还用它来重新训练了一版WoBERT模型（WoBERT+），也欢迎读者尝试（[链接](https://github.com/ZhuiyiTechnology/WoBERT)）。

## 预训练任务 #

对于预训练任务，我们希望更加接近自然语言生成（而不是像T5那样的只预测挖空部分），并且尽可能具有实用价值。为此，我们关注到了PEGASUS，来自论文[《PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization》](https://papers.cool/arxiv/1912.08777)。PEGASUS在其论文称是专门为摘要定制的预训练模型，但在我们看来，它也可以作为通用的生成式预训练任务。PEGASUS的大体思路是通过最长公共子序列的方式该摘要类似的数据对，T5 PEGASUS并没有完全复现PEGASUS的做法，只是借鉴了PEGASUS的思路做语料构建。

[![T5 PEGASUS的训练数据示例](/usr/uploads/2021/03/2339430132.png)](/usr/uploads/2021/03/2339430132.png "点击查看原图")

T5 PEGASUS的训练数据示例

具体来说，假设一个文档有$n$个句子，我们从中挑出大约$n/4$个句子（可以不连续），使得这$n/4$个句子拼起来的文本，跟剩下的$3n/4$个句子拼起来的文本，最长公共子序列尽可能长，然后我们将$3n/4$个句子拼起来的文本视为原文，$n/4$个句子拼起来的文本视为摘要，这样就构成了一个“(原文, 摘要)”的伪摘要数据对了，就用这些数据对去训练Seq2Seq模型即可。注意，如果文档里没有重复句子的话，那么原文跟摘要的句子是不会有交集的，所以这样的生成任务并非是原文的简单复制，因此还是有一定难度的。

搜索算法则是通过如下的贪心算法逐步搜索至满足长度要求：

> 1、先找出1个句子，使得它跟生成的$n-1$个句子的最长公共子序列最长；
> 
> 2、假设已经找到了$k$个句子，那么继续找第$k+1$个句子，使得这$k+1$个句子拼起来的文本，跟剩下的$n-k-1$个句子拼起来的文本的最长公共子序列最长。

## 参数与配置 #

目前开源的T5 PEGASUS是base版，总参数量为2.75亿，训练时最大长度为512，batch_size为96，学习率为$10^{-4}$，使用6张3090训练了100万步，训练时间约13天，数据是30多G的精处理通用语料，训练acc约47%，训练loss约2.97。模型使用[bert4keras](bert4keras)进行编写、训练和测试。

> **Github地址：<https://github.com/ZhuiyiTechnology/t5-pegasus>**

## 实验与评测 #

在CSL和LCSTS两个文本生成任务上，T5 PEGASUS是我们已知的所有模型中的SOTA：  
\begin{array}{c}  
\text{CSL摘要生成实验结果}\\\  
{\begin{array}{c|c|cccc}  
\hline  
& \text{beam size} & \text{Rouge-L} & \text{Rouge-1} & \text{Rouge-2} & \text{BLEU} \\\  
\hline  
\text{BERT} & 1 & 63.81 & 65.45 & 54.91 & 45.52 \\\  
\text{WoBERT} & 1 & 66.38 & 68.22 & 57.83 & 47.76 \\\  
\text{mT5} & 1 & 66.96 & 69.00 & 58.74 & \textbf{49.79} \\\  
\text{T5 PEGASUS} & 1 & \textbf{67.68} & \textbf{69.87} & \textbf{59.8} & 49.37 \\\  
\hline  
\text{BERT} & 2 & 64.44 & 66.09 & 55.75 & 46.39 \\\  
\text{WoBERT} & 2 & 66.65 & 68.68 & 58.5 & 48.4 \\\  
\text{mT5} & 2 & 67.25 & 69.19 & 59.10 & \textbf{50.17} \\\  
\text{T5 PEGASUS} & 2 & \textbf{68.26} & \textbf{70.45} & \textbf{60.57} & 50.06 \\\  
\hline  
\text{BERT} & 3 & 64.75 & 66.34 & 56.06 & 46.7 \\\  
\text{WoBERT} & 3 & 66.83 & 68.81 & 58.67 & 48.6 \\\  
\text{mT5} & 3 & 67.17 & 69.11 & 59.05 & 50.13 \\\  
\text{T5 PEGASUS} & 3 & \textbf{68.39} & \textbf{70.54} & \textbf{60.69} & \textbf{50.19} \\\  
\hline  
\end{array}}\\\  
\\\  
\text{LCSTS摘要生成实验结果}\\\  
{\begin{array}{c|c|cccc}  
\hline  
& \text{beam size} & \text{Rouge-L} & \text{Rouge-1} & \text{Rouge-2} & \text{BLEU} \\\  
\hline  
\text{BERT} & 1 & 27.99 & 29.57 & 18.04 & 11.72 \\\  
\text{WoBERT} & 1 & \textbf{31.51} & 32.90 & 21.13 & 13.74 \\\  
\text{mT5} & 1 & 28.92 & 30.75 & 19.54 & 13.21 \\\  
\text{T5 PEGASUS} & 1 & 31.21 & \textbf{33.53} & \textbf{21.54} & \textbf{14.47} \\\  
\hline  
\text{BERT} & 2 & 29.20 & 30.70 & 19.17 & 12.64 \\\  
\text{WoBERT} & 2 & \textbf{31.91} & 33.35 & 21.55 & 14.13 \\\  
\text{mT5} & 2 & 29.96 & 31.67 & 20.40 & 13.84 \\\  
\text{T5 PEGASUS} & 2 & 31.47 & \textbf{34.00} & \textbf{21.98} & \textbf{14.75} \\\  
\hline  
\text{BERT} & 3 & 29.45 & 30.95 & 19.50 & 12.93 \\\  
\text{WoBERT} & 3 & \textbf{32.19} & 33.72 & 21.81 & 14.29 \\\  
\text{mT5} & 3 & 30.15 & 31.97 & 20.72 & 14.05 \\\  
\text{T5 PEGASUS} & 3 & 31.78 & \textbf{34.12} & \textbf{22.23} & \textbf{14.96} \\\  
\hline  
\end{array}}  
\end{array}

更重要的是，T5 PEGASUS有着非常出色的小样本学习能力：  
\begin{array}{c}  
\text{CSL摘要生成实验结果(小样本, beam size=1)}\\\  
{\begin{array}{c|c|cccc}  
\hline  
& \text{样本数} & \text{Rouge-L} & \text{Rouge-1} & \text{Rouge-2} & \text{BLEU} \\\  
\hline  
\text{WoBERT} & 10000 & 66.38 & 68.22 & 57.83 & 47.76 \\\  
\text{mT5} & 10000 & 66.96 & 69.00 & 58.74 & \textbf{49.79} \\\  
\text{T5 PEGASUS} & 10000 & \textbf{67.68} & \textbf{69.87} & \textbf{59.8} & 49.37 \\\  
\hline  
\text{WoBERT} & 1000 & 59.34 & 60.42 & 49.07 & 37.87 \\\  
\text{mT5} & 1000 & 59.91 & 61.52 & 50.38 & 40.87 \\\  
\text{T5 PEGASUS} & 1000 & \textbf{63.12} & \textbf{65.28} & \textbf{54.54} & \textbf{43.55} \\\  
\hline  
\text{WoBERT} & 100 & 55.68 & 55.33 & 43.10 & 31.55 \\\  
\text{mT5} & 100 & 55.33 & 54.62 & 42.78 & 32.50 \\\  
\text{T5 PEGASUS} & 100 & \textbf{60.87} & \textbf{62.78} & \textbf{52.30} & \textbf{41.40} \\\  
\hline  
\text{WoBERT} & 10 & 26.32 & 20.99 & 12.29 & 5.76 \\\  
\text{mT5} & 10 & 26.62 & 27.00 & 17.95 & 13.11 \\\  
\text{T5 PEGASUS} & 10 & \textbf{55.85} & \textbf{57.66} & \textbf{47.52} & \textbf{35.97} \\\  
\hline  
\end{array}}  
\end{array}

哪怕样本标注样本降低到10个，T5 PEGASUS依然可以微调出一个摘要（标题）生成模型出来，性能显著超过其他模型。在LCSTS上，T5 PEGASUS具有类似的小样本学习效果，只不过非T5 PEGASUS模型效果实在太差了，所以就没有把表格整理在此了。

## 小样本演示 #

下面是标注样本数为10个时训练出来的模型生成效果演示：

> **输入：** 针对以超立方体网络为蓝本的多处理机系统的可靠性和容错能力的精准度量问题,结合多处理机系统遭受计算机病毒攻击时常常发生结构性故障的特点,研究了n维超立方体网络的结构连通性和子结构连通性评价问题。首先,使 用构造n维超立方体网络的3路结构割的方法得到其3路结构连通度的一个上界;然后,使用构造n维超立方体网络的3路子结构集的等价变换或约简变换的方法,得到其3路结构子连通度的一个下界;最后,利用任意网络的3路结构连通度不小于3路子结构连通度的性质,证实了超立方体网络的3路结构连通度和子结构连通度均为该超立方体网络维数  
>  **标题：** 超立方体网络的3路结构连通度及子结构连通度  
>  **预测：** 基于n维超立方体网络结构连通性和子结构连通性评价研究
> 
> **输入：** 针对传统无线体域网(WBAN)预测模型对感知数据预测精度低、计算量大、能耗高的问题,提出一种基于惩罚误差矩阵的自适应三次指数平滑算法。首先在感知节点与路由节点之间建立轻量级预测模型,其次采用地毯式搜索方式 对预测模型进行参数优化处理,最后采用惩罚误差矩阵对预测模型参数作进一步的细粒化处理。实验结果表明,与Zig Bee协议相比,在1000时隙范围内,所提方法可节省12%左右的能量;而采用惩罚误差矩阵与地毯式搜索方式相比,预测精度提高了3. 306%。所提方法在有效降低计算复杂度的同时能进一步降低WBAN的能耗  
>  **标题：** 基于惩罚误差矩阵的同步预测无线体域网节能方法  
>  **预测：** 基于惩罚误差矩阵的自适应三次指数平滑算法
> 
> **输入：** 针对车联网(IoV)环境下消息传输效率低下、网络资源开销较大等诸多问题,提出一种适用于城市交通场景下基于车辆节点认知交互的路由算法。首先,依据信任理论提出节点认知交互度的概念,并在此基础上对车联网中的车辆 节点进行分类,赋予它们不同的认知交互度初值;同时还引入车辆节点交互时间、交互频率、车辆节点物理间隔距离、间隔跳数以及消息生存时间等影响因子,进而构建了车辆节点认知交互评估模型。基于该模型计算并更新节点的认知 交互度,并通过比较对应车辆节点间的认知交互度值来选取认知交互度相对较高的邻居节点作为中继节点进行消息转  
>  **标题：** 车联网环境下基于节点认知交互的路由算法  
>  **预测：** 基于车辆节点认知交互的路由算法
> 
> **输入：** 针对近场源波达方向(DOA)和距离的联合估计问题,提出一种近场迭代自适应算法(NF-IAA)。首先通过划分二维网格表示出近场区域内信源所有可能的位置,每个位置都看作存在一个潜在的信源入射到阵列上,表示出阵列输出的 数据模型;然后通过循环迭代利用上一次谱估计的结果构建信号的协方差矩阵,将协方差矩阵的逆作为加权矩阵估计出每个位置对应的潜在信源能量;最后绘制出三维能量谱图,由于只有真实存在的信源能量不为0,因此谱峰对应的位置即为真实存在信源的位置。仿真实验表明在10个快拍条件下,NF-IAA的DOA分辨概率达到了9  
>  **标题：** 基于迭代自适应方法的近场源二维参数联合估计  
>  **预测：** 基于nf-iaa的近场迭代自适应算法
> 
> **输入：** 针对现有的软件众包工人选择机制对工人间协同开发考虑不足的问题,在竞标模式的基础上提出一种基于活跃时间分组的软件众包工人选择机制。首先,基于活跃时间将众包工人划分为多个协同开发组;然后,根据组内工人开发 能力和协同因子计算协同工作组权重;最后,选定权重最大的协同工作组为最优工作组,并根据模块复杂度为每个任务模块从该组内选择最适合的工人。实验结果表明,该机制相比能力优先选择方法在工人平均能力上仅有0. 57%的差距, 同时因为保证了工人间的协同而使项目风险平均降低了32%,能有效指导需多人协同进行的众包软件任务的工  
>  **标题：** 基于活跃时间分组的软件众包工人选择机制  
>  **预测：** 基于活跃时间分组的软件众包工人选择机制

可以看到哪怕标注样本很少，但依然能够得到可读性较好的生成结果，这得益于PEGASUS式的伪摘要预训练与下游任务是很贴近的。

## 简单的总结 #

本文主要分享了我们的中文生成式预训练模型T5 PEGASUS，它以mT5为基础，在中文语料上使用PEGASUS式的伪摘要预训练，最终有着不错的文本生成表现，尤其是出色的小样本学习能力，欢迎有文本生成需求的读者使用。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/8209>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (Mar. 03, 2021). 《T5 PEGASUS：开源一个中文生成式预训练模型 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/8209>

@online{kexuefm-8209,  
title={T5 PEGASUS：开源一个中文生成式预训练模型},  
author={苏剑林},  
year={2021},  
month={Mar},  
url={\url{https://spaces.ac.cn/archives/8209}},  
} 


---

## 公式推导与注释

TODO: 添加详细的数学公式推导和注释

