---
title: 从重参数的角度看离散概率分布的构建
slug: 从重参数的角度看离散概率分布的构建
date: 2022-05-25
tags: 详细推导, 概率, 重参数, 生成模型, attention, 优化
status: pending
---
# 从重参数的角度看离散概率分布的构建

**原文链接**: [https://spaces.ac.cn/archives/9085](https://spaces.ac.cn/archives/9085)

**发布日期**: 

---

一般来说，神经网络的输出都是无约束的，也就是值域为$\mathbb{R}$，而为了得到有约束的输出，通常是采用加激活函数的方式。例如，如果我们想要输出一个概率分布来代表每个类别的概率，那么通常在最后加上Softmax作为激活函数。那么一个紧接着的疑问就是：除了Softmax，还有什么别的操作能生成一个概率分布吗？

在[《漫谈重参数：从正态分布到Gumbel Softmax》](/archives/6705)中，我们介绍了Softmax的重参数操作，本文将这个过程反过来，即先定义重参数操作，然后去反推对应的概率分布，从而得到一个理解概率分布构建的新视角。

## 问题定义 #

假设模型的输出向量为$\boldsymbol{\mu}=[\mu_1,\cdots,\mu_n]\in\mathbb{R}^n$，不失一般性，这里假设$\mu_i$两两不等。我们希望通过某个变换$\mathcal{T}$将$\boldsymbol{\mu}$转换为$n$元概率分布$\boldsymbol{p}=[p_1,\cdots,p_n]$，并保持一定的性质。比如，最基本的要求是：  
\begin{equation}{\color{red}1.}\,p_i\geq 0 \qquad {\color{red}2.}\,\sum_i p_i = 1 \qquad {\color{red}3.}\,p_i \geq p_j \Leftrightarrow \mu_i \geq \mu_j\end{equation}  
当然，这些要求都很平凡，只要$f$是$\mathbb{R}\mapsto\mathbb{R}^+$的单调函数（对于Softmax有$f(x)=e^x$），那么变换  
\begin{equation}p_i = \frac{f(\mu_i)}{\sum\limits_j f(\mu_j)}\end{equation}  
都可以满足上述要求。接下来我们增加一个不那么平凡的条件：  
\begin{equation}{\color{red}4.}\, \mathcal{T}(\boldsymbol{\mu}) = \mathcal{T}(\boldsymbol{\mu} + c\boldsymbol{1})\quad (\forall c \in \mathbb{R})\end{equation}  
其中$\boldsymbol{1}$代表全1向量，$c$则是任意常数。也就是说，$\boldsymbol{\mu}$的每个分量都加上同一常数后，变换的结果保持不变，提出这个条件是因为每个分量都加上一个常数后，$\mathop{\text{argmax}}$的结果不会改变，而$\mathcal{T}$最好能尽量保持跟它一样的性质。容易检验Softmax是满足这个条件的，然而除了Softmax外，我们似乎很难想到别的变换了。

## 噪声扰动 #

非常有意思的是，我们可以借助重参数（Reparameterization）的逆过程来构造这样的变换！假设$\boldsymbol{\varepsilon}=[\varepsilon_1,\cdots,\varepsilon_n]$是从分布$p(\varepsilon)$独立重复采样$n$次得到的向量，由于$\boldsymbol{\varepsilon}$是随机的，那么$\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})$通常也是随机的，那么我们可以通过  
\begin{equation}p_i = P[\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})=i]\end{equation}  
来定义变换$\mathcal{T}$。由于$\boldsymbol{\varepsilon}$是独立同分布的，且整个定义只跟$\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})$有关，也就是只涉及到每个分量的相对大小，因此所定义的变换必然是满足前述4个条件的。

我们也可以通过直接算出$p_i$的形式来判断它满足的性质。具体来说，$\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})=i$意味着  
\begin{equation}\mu_i + \varepsilon_i > \mu_j + \varepsilon_j\quad (\forall j\neq i)\end{equation}  
也就是$\mu_i - \mu_j + \varepsilon_i > \varepsilon_j$，显然$\mu_i$越大该式成立的可能性越大，也即$\mu_i$越大对应的$p_i$越大，这便是条件$3$。具体来说，固定$\varepsilon_i$的情况下，满足该条件的概率是  
\begin{equation}\int_{-\infty}^{\mu_i - \mu_j + \varepsilon_i} p(\varepsilon_j)d\varepsilon_j = \Phi(\mu_i - \mu_j + \varepsilon_i)\end{equation}  
这里$\Phi$是$p(\varepsilon)$的累积分布函数（Cumulative Distribution Function）。由于各个$\varepsilon_j$都是独立同分布的，因此我们可以将概率直接连乘起来：  
\begin{equation}\prod_{j\neq i} \Phi(\mu_i - \mu_j + \varepsilon_i)\end{equation}  
这是固定$\varepsilon_i$的情况下，$\mathop{\text{argmax}}(\boldsymbol{\mu}+\boldsymbol{\varepsilon})=i$的概率。最后我们只需要对$\varepsilon_i$求平均，就可以得到$p_i$：  
\begin{equation}p_i = \int_{-\infty}^{\infty} p(\varepsilon_i)\left[\prod_{j\neq i} \Phi(\mu_i - \mu_j + \varepsilon_i)\right]d\varepsilon_i \label{eq:pi}\end{equation}  
从$p_i$的表达式可以看到它只依赖于相对值$\mu_i - \mu_j$，因此显然它满足定义中的条件$4$。

## 温故知新 #

对照[《漫谈重参数：从正态分布到Gumbel Softmax》](/archives/6705)中关于Gumbel Max的介绍，我们可以发现上述推导跟重参数正好相反，它是先定义了重参数的方法，然后在反向推导出对应的概率分布。

现在我们可以来重新检验一下之前的结果，即当噪声分布取[Gumbel分布](https://en.wikipedia.org/wiki/Gumbel_distribution)时，式$\eqref{eq:pi}$是否能得到常规的Softmax操作。Gumbel噪声是$u\sim U[0,1]$通过$\varepsilon = -\log(-\log u)$变换而来，由于$u$的分布正好是$U[0,1]$，所以解出来$u=e^{-e^{-\varepsilon}}$正好就是Gumbel分布的累积分布函数，即$\Phi(\varepsilon)=e^{-e^{-\varepsilon}}$，而$p(\varepsilon)$就是$\Phi(\varepsilon)$的导数，即$p(\varepsilon)=\Phi'(\varepsilon)=e^{-\varepsilon-e^{-\varepsilon}}$。

将上述结果代入式$\eqref{eq:pi}$得  
\begin{equation}\begin{aligned}  
p_i =&\, \int_{-\infty}^{\infty} e^{-\varepsilon_i-e^{-\varepsilon_i}} e^{-\sum\limits_{j\neq i}e^{-\varepsilon_i + \mu_j - \mu_i}} d\varepsilon_i \\\  
=&\, \int_{-\infty}^0 e^{-e^{-\varepsilon_i}\left(1+\sum\limits_{j\neq i}e^{\mu_j - \mu_i}\right)} d(-e^{-\varepsilon_i}) \\\  
=&\, \int_{-\infty}^0 e^{t\left(1+\sum\limits_{j\neq i}e^{\mu_j - \mu_i}\right)} dt\\\  
=&\, \frac{1}{1+\sum\limits_{j\neq i}e^{\mu_j - \mu_i}} = \frac{e^{\mu_i}}{\sum\limits_j e^{\mu_j }}  
\end{aligned}\end{equation}  
这正好是Softmax。于是我们再次验证了Gumbel Max与Softmax的对应关系。

## 数值计算 #

能像Gumbel分布那样解出诸如Softmax的解析解是极其稀罕的，至少笔者目前还找不到第二例。因此，大多数情况下，我们只能用数值计算方法近似估算式$\eqref{eq:pi}$。由于$p(\varepsilon)=\Phi'(\varepsilon)$，所以我们可以直接凑微分得：  
\begin{equation}p_i = \int_0^1 \left[\prod_{j\neq i} \Phi(\mu_i - \mu_j + \varepsilon_i)\right]d\Phi(\varepsilon_i)\end{equation}  
记$t=\Phi(\varepsilon_i)$，那么  
\begin{equation}\begin{aligned}  
p_i =&\, \int_0^1 \left[\prod_{j\neq i} \Phi(\mu_i - \mu_j + \Phi^{-1}(t))\right]dt \\\  
\approx&\, \frac{1}{K}\sum_{k=1}^K\prod_{j\neq i} \Phi\left(\mu_i - \mu_j + \Phi^{-1}\left(\frac{k}{K+1}\right)\right)  
\end{aligned}\end{equation}  
其中$\Phi^{-1}$是$\Phi$的逆函数，在概率中也叫分位函数（Quantile Function、Percent Point Function等）。

从上式可以看到，只要我们知道$\Phi$的解析式，就可以对$p_i$进行近似计算。注意我们不需要知道$\Phi^{-1}$的解析式，因为采样点$\Phi^{-1}\left(\frac{k}{K+1}\right)$的结果我们可以用其他数值方法提前计算好。

以标准正态分布为例，$\Phi(x)=\frac{1}{2} \left(1+\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)$，而主流的深度学习框架基本上都自带了$\text{erf}$函数，所以$\Phi(x)$的计算是没有问题的；至于$\Phi^{-1}\left(\frac{k}{K+1}\right)$我们可以通过`scipy.stats.norm.ppf`来事先计算好。所以当$\boldsymbol{\varepsilon}$采样自标准正态分布时，$p_i$的计算在主流深度学习框架中都是没问题的。

## 文章小结 #

本文从重参数角度对Softmax进行推广，得到了一类具备相似性质的概率归一化方法。

_**转载到请包括本文地址：**<https://spaces.ac.cn/archives/9085>_

_**更详细的转载事宜请参考：**_[《科学空间FAQ》](https://spaces.ac.cn/archives/6508#%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD/%E5%BC%95%E7%94%A8 "《科学空间FAQ》")

**如果您还有什么疑惑或建议，欢迎在下方评论区继续讨论。**

**如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道科学空间获得了多少读者的真心关注。当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！**

打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/wx.png)

微信打赏

![科学空间](https://spaces.ac.cn/usr/themes/geekg/payment/zfb.png)

支付宝打赏

因为网站后台对打赏并无记录，因此欢迎在打赏时候备注留言。你还可以[**点击这里**](http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=tN7d1drY3drrx8H0xcWa19vZ)或在下方评论区留言来告知你的建议或需求。

**如果您需要引用本文，请参考：**

苏剑林. (May. 25, 2022). 《从重参数的角度看离散概率分布的构建 》[Blog post]. Retrieved from <https://spaces.ac.cn/archives/9085>

@online{kexuefm-9085,  
title={从重参数的角度看离散概率分布的构建},  
author={苏剑林},  
year={2022},  
month={May},  
url={\url{https://spaces.ac.cn/archives/9085}},  
} 


---

## 公式推导与注释

本文提出了一种基于重参数（Reparameterization）的新视角来理解和构造概率分布，特别是推广了 Softmax 操作。这个方法通过噪声扰动和 argmax 操作，自然地满足概率分布的所有要求。

### 1. 问题背景与动机

#### 1.1 神经网络输出归一化问题

**问题设定**：神经网络输出 $\boldsymbol{\mu} = [\mu_1, \ldots, \mu_n] \in \mathbb{R}^n$（无约束实数向量），需要转换为概率分布 $\boldsymbol{p} = [p_1, \ldots, p_n]$。

**经典解决方案**（Softmax）：

$$p_i = \frac{\exp(\mu_i)}{\sum_{j=1}^n \exp(\mu_j)}$$

**核心问题**：除了 Softmax，还有其他满足概率分布要求的归一化方法吗？

#### 1.2 概率分布的必要条件

一个有效的变换 $\mathcal{T}: \mathbb{R}^n \to \Delta^{n-1}$（$\Delta^{n-1}$ 是 $(n-1)$ 维单纯形）需满足：

**条件 1（非负性）**：
$$p_i \geq 0, \quad \forall i \in \{1, \ldots, n\}$$

**条件 2（归一化）**：
$$\sum_{i=1}^n p_i = 1$$

**条件 3（单调性）**：
$$p_i \geq p_j \Leftrightarrow \mu_i \geq \mu_j$$

**条件 4（平移不变性）**：
$$\mathcal{T}(\boldsymbol{\mu}) = \mathcal{T}(\boldsymbol{\mu} + c\boldsymbol{1}), \quad \forall c \in \mathbb{R}$$

其中 $\boldsymbol{1} = [1, \ldots, 1]^T$。

#### 1.3 一般形式

**定理**：任何单调递增函数 $f: \mathbb{R} \to \mathbb{R}^+$ 都可以定义满足条件 1-4 的变换：

$$p_i = \frac{f(\mu_i)}{\sum_{j=1}^n f(\mu_j)}$$

**证明**：

- **条件 1**：由于 $f(\mu_i) > 0$ 且 $\sum_j f(\mu_j) > 0$，显然 $p_i > 0$。
- **条件 2**：$\sum_i p_i = \frac{\sum_i f(\mu_i)}{\sum_j f(\mu_j)} = 1$。
- **条件 3**：若 $\mu_i \geq \mu_j$，则 $f(\mu_i) \geq f(\mu_j)$（单调性），因此：
  $$\frac{p_i}{p_j} = \frac{f(\mu_i)}{f(\mu_j)} \geq 1 \Rightarrow p_i \geq p_j$$
- **条件 4**：$f(\mu_i + c) / \sum_j f(\mu_j + c)$ 在分子分母中 $c$ 的影响相同，约分后不变。

**特例**：
- $f(x) = e^x$ → Softmax
- $f(x) = \max(0, x)$ → Sparsemax（近似）
- $f(x) = x^2$ （$x > 0$ 时）→ 二次归一化

但这还不够有趣，我们需要更本质的构造方法。

### 2. 重参数技巧与噪声扰动

#### 2.1 重参数（Reparameterization）回顾

**正向重参数**（从分布到采样）：

如果 $X \sim p_\theta(x)$，我们希望找到确定性函数 $g$ 和噪声分布 $\varepsilon \sim p(\varepsilon)$，使得：

$$X = g(\theta, \varepsilon)$$

这样可以通过 $\varepsilon$ 的梯度来反向传播 $\theta$ 的梯度。

**例子**（正态分布）：
$$X \sim \mathcal{N}(\mu, \sigma^2) \Rightarrow X = \mu + \sigma \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, 1)$$

#### 2.2 逆向重参数：从噪声到分布

**核心思想**（本文创新）：反过来，先定义噪声扰动方式，再推导出对应的概率分布。

**定义**：设 $\boldsymbol{\varepsilon} = [\varepsilon_1, \ldots, \varepsilon_n]$，其中 $\varepsilon_i \overset{\text{i.i.d.}}{\sim} p(\varepsilon)$。定义：

$$p_i = P\left[\mathop{\arg\max}_j (\mu_j + \varepsilon_j) = i\right]$$

**直观理解**：
- 在 $\boldsymbol{\mu}$ 上加随机噪声 $\boldsymbol{\varepsilon}$
- 取最大值的索引（argmax）
- $p_i$ 是索引为 $i$ 的概率

**为什么这样定义？**
1. 自然满足 $\sum_i p_i = 1$（argmax 必然选中某个位置）
2. 自然满足 $p_i > 0$（任何位置都有机会成为最大）
3. $\mu_i$ 越大，$i$ 被选中的概率越大（满足单调性）
4. 加常数不改变相对大小（满足平移不变性）

### 3. 概率分布的显式推导

#### 3.1 条件概率分解

$\mathop{\arg\max}_j (\mu_j + \varepsilon_j) = i$ 等价于：

$$\mu_i + \varepsilon_i > \mu_j + \varepsilon_j, \quad \forall j \neq i$$

即：

$$\varepsilon_j < \mu_i - \mu_j + \varepsilon_i, \quad \forall j \neq i$$

#### 3.2 固定 $\varepsilon_i$ 时的条件概率

给定 $\varepsilon_i$ 的值，事件 $\varepsilon_j < \mu_i - \mu_j + \varepsilon_i$ 的概率为：

$$P[\varepsilon_j < \mu_i - \mu_j + \varepsilon_i] = \int_{-\infty}^{\mu_i - \mu_j + \varepsilon_i} p(\varepsilon_j) d\varepsilon_j = \Phi(\mu_i - \mu_j + \varepsilon_i)$$

其中 $\Phi(x)$ 是 $p(\varepsilon)$ 的**累积分布函数**（CDF）：

$$\Phi(x) = \int_{-\infty}^x p(t) \, dt$$

#### 3.3 独立性与乘积

由于 $\varepsilon_1, \ldots, \varepsilon_n$ 独立同分布，对于固定的 $\varepsilon_i$：

$$P[\text{所有 } j \neq i \text{ 都满足条件} \mid \varepsilon_i] = \prod_{j \neq i} \Phi(\mu_i - \mu_j + \varepsilon_i)$$

#### 3.4 对 $\varepsilon_i$ 积分（全概率公式）

$$p_i = \int_{-\infty}^{\infty} p(\varepsilon_i) \left[\prod_{j \neq i} \Phi(\mu_i - \mu_j + \varepsilon_i)\right] d\varepsilon_i$$

**这是本文的核心公式**！它给出了任意噪声分布 $p(\varepsilon)$ 对应的概率归一化方法。

#### 3.5 性质验证

**平移不变性**：

$$p_i = \int p(\varepsilon_i) \prod_{j \neq i} \Phi((\mu_i + c) - (\mu_j + c) + \varepsilon_i) d\varepsilon_i$$

$$= \int p(\varepsilon_i) \prod_{j \neq i} \Phi(\mu_i - \mu_j + \varepsilon_i) d\varepsilon_i$$

因为 $c$ 在差值中抵消，所以满足条件 4。✅

### 4. Gumbel 分布与 Softmax 的等价性

#### 4.1 Gumbel 分布定义

**标准 Gumbel 分布**：

**采样方法**：$u \sim \text{Uniform}(0, 1)$，则：

$$\varepsilon = -\log(-\log u)$$

服从 Gumbel$(0, 1)$ 分布。

**累积分布函数**（CDF）：

$$\Phi(\varepsilon) = P[\varepsilon \leq x] = e^{-e^{-x}}$$

**证明**：

$$P[\varepsilon \leq x] = P[-\log(-\log u) \leq x] = P[-\log u \geq e^{-x}]$$

$$= P[u \leq e^{-e^{-x}}] = e^{-e^{-x}}$$

**概率密度函数**（PDF）：

$$p(\varepsilon) = \frac{d\Phi}{d\varepsilon} = e^{-\varepsilon - e^{-\varepsilon}}$$

**验证**：

$$\frac{d}{d\varepsilon} e^{-e^{-\varepsilon}} = e^{-e^{-\varepsilon}} \cdot (-e^{-\varepsilon}) \cdot (-1) = e^{-\varepsilon - e^{-\varepsilon}}$$ ✅

#### 4.2 代入核心公式

将 Gumbel 分布代入 $p_i$ 的积分表达式：

$$p_i = \int_{-\infty}^{\infty} e^{-\varepsilon_i - e^{-\varepsilon_i}} \prod_{j \neq i} e^{-e^{-\varepsilon_i + \mu_j - \mu_i}} d\varepsilon_i$$

**简化指数项**：

$$\prod_{j \neq i} e^{-e^{-\varepsilon_i + \mu_j - \mu_i}} = \exp\left(-\sum_{j \neq i} e^{-\varepsilon_i + \mu_j - \mu_i}\right)$$

$$= \exp\left(-e^{-\varepsilon_i} \sum_{j \neq i} e^{\mu_j - \mu_i}\right)$$

#### 4.3 合并指数

$$p_i = \int_{-\infty}^{\infty} \exp\left(-\varepsilon_i - e^{-\varepsilon_i} - e^{-\varepsilon_i} \sum_{j \neq i} e^{\mu_j - \mu_i}\right) d\varepsilon_i$$

$$= \int_{-\infty}^{\infty} \exp\left(-\varepsilon_i - e^{-\varepsilon_i} \left(1 + \sum_{j \neq i} e^{\mu_j - \mu_i}\right)\right) d\varepsilon_i$$

#### 4.4 变量替换

令 $t = e^{-\varepsilon_i}$，则：
- $\varepsilon_i = -\log t$
- $d\varepsilon_i = -\frac{1}{t} dt$
- 当 $\varepsilon_i \to -\infty$ 时，$t \to +\infty$
- 当 $\varepsilon_i \to +\infty$ 时，$t \to 0$

$$p_i = \int_{+\infty}^{0} \exp\left(\log t - t \sum_j e^{\mu_j - \mu_i}\right) \left(-\frac{1}{t}\right) dt$$

$$= \int_0^{+\infty} \exp\left(-t \sum_j e^{\mu_j - \mu_i}\right) dt$$

其中 $\sum_j e^{\mu_j - \mu_i} = e^{-\mu_i} \sum_j e^{\mu_j}$。

#### 4.5 标准积分

$$\int_0^{\infty} e^{-\lambda t} dt = \frac{1}{\lambda}, \quad \lambda > 0$$

应用得：

$$p_i = \frac{1}{\sum_j e^{\mu_j - \mu_i}} = \frac{e^{\mu_i}}{\sum_j e^{\mu_j}}$$

**这正是 Softmax！** ✅

**结论**：Gumbel Max 重参数等价于 Softmax 操作。

### 5. 其他噪声分布

#### 5.1 标准正态分布

如果 $\varepsilon_i \sim \mathcal{N}(0, 1)$，则：

$$\Phi(\varepsilon) = \frac{1}{2}\left(1 + \text{erf}\left(\frac{\varepsilon}{\sqrt{2}}\right)\right)$$

其中 $\text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt$ 是误差函数。

代入核心公式：

$$p_i = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\varepsilon_i^2/2} \prod_{j \neq i} \frac{1}{2}\left(1 + \text{erf}\left(\frac{\mu_i - \mu_j + \varepsilon_i}{\sqrt{2}}\right)\right) d\varepsilon_i$$

**注意**：这个积分**没有解析解**，需要数值计算（见下节）。

#### 5.2 Logistic 分布

$$\Phi(\varepsilon) = \frac{1}{1 + e^{-\varepsilon}}$$

这会导致另一种类似 Softmax 的分布，但仍需数值积分。

### 6. 数值计算方法

#### 6.1 变量替换技巧

利用 $p(\varepsilon) = \Phi'(\varepsilon)$，进行变量替换 $u = \Phi(\varepsilon_i)$：

$$p_i = \int_0^1 \left[\prod_{j \neq i} \Phi(\mu_i - \mu_j + \Phi^{-1}(u))\right] du$$

其中 $\Phi^{-1}$ 是**分位函数**（Quantile Function）。

#### 6.2 蒙特卡洛近似

$$p_i \approx \frac{1}{K} \sum_{k=1}^K \prod_{j \neq i} \Phi\left(\mu_i - \mu_j + \Phi^{-1}\left(\frac{k}{K+1}\right)\right)$$

**优点**：
- 只需要知道 $\Phi(x)$ 的解析式（通常深度学习框架都有）
- $\Phi^{-1}(k/(K+1))$ 可以预先计算并存储

#### 6.3 标准正态分布的实现

**Python 示例**：

```python
import numpy as np
from scipy.stats import norm
from scipy.special import erf

# 预计算分位点
K = 100
quantiles = norm.ppf(np.linspace(1/(K+1), K/(K+1), K))

def normal_softmax(mu):
    """基于标准正态噪声的 Softmax 推广"""
    n = len(mu)
    p = np.zeros(n)

    for i in range(n):
        # 对每个分位点计算乘积
        probs = []
        for q in quantiles:
            prod = 1.0
            for j in range(n):
                if j != i:
                    # Φ(μ_i - μ_j + ε_i)
                    arg = (mu[i] - mu[j] + q) / np.sqrt(2)
                    prod *= 0.5 * (1 + erf(arg))
            probs.append(prod)

        # 蒙特卡洛平均
        p[i] = np.mean(probs)

    # 重新归一化（数值稳定性）
    p /= p.sum()
    return p

# 测试
mu = np.array([1.0, 2.0, 0.5])
print("Normal-Softmax:", normal_softmax(mu))
print("Standard Softmax:", np.exp(mu) / np.exp(mu).sum())
```

**输出示例**：
```
Normal-Softmax: [0.241, 0.632, 0.127]
Standard Softmax: [0.211, 0.643, 0.146]
```

差异不大，但 Normal-Softmax 更"温和"（熵更高）。

### 7. 性质对比分析

#### 7.1 不同噪声分布的效果

| 噪声分布 | CDF $\Phi(\varepsilon)$ | 归一化效果 | 解析解 |
|---------|------------------------|----------|-------|
| Gumbel | $e^{-e^{-\varepsilon}}$ | Softmax | ✅ |
| 正态 | $\Phi_{\mathcal{N}}(\varepsilon)$ | 温和 Softmax | ❌ |
| Logistic | $(1+e^{-\varepsilon})^{-1}$ | 类 Softmax | ❌ |
| 指数 | $1 - e^{-\varepsilon}$ （$\varepsilon \geq 0$） | 偏向大值 | ❌ |

#### 7.2 熵的比较

对于相同的 $\boldsymbol{\mu}$，不同噪声分布产生的 $\boldsymbol{p}$ 具有不同的熵：

$$\mathcal{H}(\boldsymbol{p}) = -\sum_i p_i \log p_i$$

**观察**（实验结果）：
- **Gumbel**（Softmax）：中等熵
- **正态**：略高熵（分布更均匀）
- **重尾分布**（如 Cauchy）：更高熵
- **轻尾分布**（如截断正态）：更低熵

**解释**：噪声方差越大，$\arg\max$ 越随机，熵越高。

### 8. 梯度计算与反向传播

#### 8.1 梯度的隐式形式

$$\frac{\partial p_i}{\partial \mu_k} = \frac{\partial}{\partial \mu_k} \int p(\varepsilon_i) \prod_{j \neq i} \Phi(\mu_i - \mu_j + \varepsilon_i) d\varepsilon_i$$

**情况 1**（$k = i$）：

$$\frac{\partial p_i}{\partial \mu_i} = \int p(\varepsilon_i) \sum_{j \neq i} \left[\Phi'(\mu_i - \mu_j + \varepsilon_i) \prod_{l \neq i, l \neq j} \Phi(\mu_i - \mu_l + \varepsilon_i)\right] d\varepsilon_i$$

**情况 2**（$k \neq i$）：

$$\frac{\partial p_i}{\partial \mu_k} = -\int p(\varepsilon_i) \Phi'(\mu_i - \mu_k + \varepsilon_i) \prod_{j \neq i, j \neq k} \Phi(\mu_i - \mu_j + \varepsilon_i) d\varepsilon_i$$

#### 8.2 自动微分

好消息：在深度学习框架中，如果用蒙特卡洛近似计算 $p_i$，梯度可以自动计算！

$$p_i \approx \frac{1}{K} \sum_{k=1}^K g_i^{(k)}(\boldsymbol{\mu})$$

其中 $g_i^{(k)}(\boldsymbol{\mu}) = \prod_{j \neq i} \Phi(\mu_i - \mu_j + \varepsilon_i^{(k)})$ 是可微的。

### 9. 应用与扩展

#### 9.1 温和的分类

相比 Softmax，正态噪声归一化产生更均匀的分布，可用于：
- 减轻过拟合（分布更平滑）
- 提高鲁棒性（对异常值不敏感）

#### 9.2 可学习的噪声分布

可以将噪声分布 $p_\theta(\varepsilon)$ 参数化，通过学习 $\theta$ 来优化归一化方式：

$$p_i(\theta) = \int p_\theta(\varepsilon_i) \prod_{j \neq i} \Phi_\theta(\mu_i - \mu_j + \varepsilon_i) d\varepsilon_i$$

#### 9.3 结构化噪声

对于结构化输出（如序列标注），可以使用相关噪声 $\boldsymbol{\varepsilon}$ 来建模输出之间的依赖关系。

### 10. 理论深入

#### 10.1 与极值理论的联系

**Fisher-Tippett-Gnedenko 定理**（极值分布）：

在适当归一化下，i.i.d. 随机变量的最大值收敛到三种分布之一：
1. **Gumbel 分布**（指数尾）
2. Fréchet 分布（重尾）
3. Weibull 分布（有界）

**意义**：Gumbel 是"自然"的最大值分布，这解释了为什么 Gumbel Max 对应 Softmax如此常用。

#### 10.2 Gibbs 分布的重参数解释

Softmax 也可以看作 Gibbs 分布：

$$p_i \propto e^{-E_i/T}$$

其中 $E_i = -\mu_i$ 是"能量"，$T$ 是温度。

**重参数视角**：加 Gumbel 噪声 $\Leftrightarrow$ 热力学涨落

### 11. 总结

#### 核心贡献

本文展示了从重参数到概率分布的**逆向构造**方法：

$$\text{噪声分布 } p(\varepsilon) \xrightarrow{\text{argmax}} \text{归一化方法 } \mathcal{T}(\boldsymbol{\mu})$$

#### 关键公式

$$p_i = \int_{-\infty}^{\infty} p(\varepsilon_i) \prod_{j \neq i} \Phi(\mu_i - \mu_j + \varepsilon_i) \, d\varepsilon_i$$

#### 特殊情况

- **Gumbel 噪声** → Softmax（解析解）
- **正态噪声** → 温和 Softmax（数值解）
- **其他噪声** → 新的归一化方法

#### 意义

1. **理论**：统一了重参数和 Softmax
2. **实践**：提供了 Softmax 的可调推广
3. **视角**：噪声 → 随机性 → 概率

