---
title: ä»åŠ¨åŠ›å­¦è§’åº¦çœ‹ä¼˜åŒ–ç®—æ³•ï¼ˆå…­ï¼‰ï¼šä¸ºä»€ä¹ˆSimSiamä¸é€€åŒ–ï¼Ÿ
slug: ä»åŠ¨åŠ›å­¦è§’åº¦çœ‹ä¼˜åŒ–ç®—æ³•å…­ä¸ºä»€ä¹ˆsimsiamä¸é€€åŒ–
date: 2020-12-11
source: https://spaces.ac.cn/archives/7980
tags: åŠ¨åŠ›å­¦, ä¼˜åŒ–, æ— ç›‘ç£, ç”Ÿæˆæ¨¡å‹, attention
status: completed
tags_reviewed: true
---

# ä»åŠ¨åŠ›å­¦è§’åº¦çœ‹ä¼˜åŒ–ç®—æ³•ï¼ˆå…­ï¼‰ï¼šä¸ºä»€ä¹ˆSimSiamä¸é€€åŒ–ï¼Ÿ

**åŸæ–‡é“¾æ¥**: [https://spaces.ac.cn/archives/7980](https://spaces.ac.cn/archives/7980)

---

## 1. æ ¸å¿ƒç†è®ºã€å…¬ç†ä¸å†å²åŸºç¡€

### 1.1 è·¨å­¦ç§‘æ ¹æºï¼šä»è´Ÿé‡‡æ ·åˆ°å¯¹ç§°æ€§ç ´ç¼º

è‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-Supervised Learningï¼‰çš„ç»ˆæå¹½çµæ˜¯**â€œè¡¨å¾åç¼©ï¼ˆRepresentation Collapseï¼‰â€**ï¼šå¦‚æœæ²¡æœ‰æ˜¾å¼çš„æ’æ–¥åŠ›ï¼Œæ¨¡å‹ä¼šå‘ç°æœ€ç®€å•çš„åŠæ³•æ˜¯è®©æ‰€æœ‰å›¾ç‰‡çš„ç‰¹å¾å‘é‡éƒ½å˜æˆåŒä¸€ä¸ªå¸¸æ•°ï¼ˆå¦‚å…¨é›¶ï¼‰ï¼Œæ­¤æ—¶æŸå¤±å‡½æ•°è™½ç„¶æœ€å°ï¼Œä½†è¡¨å¾å½»åº•å¤±æ•ˆã€‚

*   **å¯¹æ¯”å­¦ä¹  (Contrastive Learning)**ï¼šå¦‚ SimCLRï¼Œå¼•å…¥æµ·é‡çš„è´Ÿæ ·æœ¬ä½œä¸ºâ€œæ’æ–¥åŠ›â€ã€‚
*   **éå¯¹æ¯”å­¦ä¹  (Non-contrastive Learning)**ï¼šBYOL å’Œ SimSiam æŒ‘æˆ˜äº†è¿™ä¸€å¸¸è¯†ã€‚å®ƒä»¬è¯æ˜äº†ï¼šå³ä¾¿æ²¡æœ‰è´Ÿæ ·æœ¬ï¼Œæ¨¡å‹ä¾ç„¶å¯ä»¥ä¸åç¼©ã€‚
*   **åŠ¨åŠ›ç³»ç»Ÿè§†è§’**ï¼šSimSiam çš„æˆåŠŸæœ¬è´¨ä¸Šæ˜¯ä¼˜åŒ–è·¯å¾„ä¸­çš„**å¯¹ç§°æ€§ç ´ç¼º**ã€‚é€šè¿‡äººä¸ºåˆ¶é€ å¿«æ…¢ä¸ä¸€çš„æ¼”åŒ–æ¨¡å—ï¼Œç³»ç»Ÿåœ¨æ»‘å‘å¹³å‡¡è§£ï¼ˆåç¼©ï¼‰çš„è¿‡ç¨‹ä¸­è¢«æˆªæ–­äº†ã€‚

### 1.2 å†å²ç¼–å¹´å²ï¼šè‡ªç›‘ç£å­¦ä¹ çš„æ¼”åŒ–ä¹‹è·¯

#### ç¬¬ä¸€é˜¶æ®µï¼šå¯¹æ¯”å­¦ä¹ çš„é»„é‡‘æ—¶ä»£ï¼ˆ2018-2020ï¼‰

1. **2018 - InstDisc (Wu et al.)**ï¼šé¦–æ¬¡æå‡ºå®ä¾‹åˆ¤åˆ«ï¼ˆInstance Discriminationï¼‰èŒƒå¼
   - æ ¸å¿ƒæ€æƒ³ï¼šå°†æ¯ä¸ªæ ·æœ¬è§†ä¸ºç‹¬ç«‹ç±»åˆ«
   - å¼•å…¥Memory Bankå­˜å‚¨ç‰¹å¾
   - é—®é¢˜ï¼šéœ€è¦ç»´æŠ¤å·¨å¤§çš„è´Ÿæ ·æœ¬é˜Ÿåˆ—

2. **2019 - MoCo (He et al.)**ï¼šåŠ¨é‡å¯¹æ¯”å­¦ä¹ 
   - åˆ›æ–°ï¼šé˜Ÿåˆ—æœºåˆ¶+åŠ¨é‡ç¼–ç å™¨
   - å®ç°å¤§è§„æ¨¡è´Ÿæ ·æœ¬ï¼ˆ65536ï¼‰
   - ImageNetå‡†ç¡®ç‡è¾¾åˆ°60.6%ï¼ˆçº¿æ€§è¯„ä¼°ï¼‰
   - å¼€å¯äº†å¯¹æ¯”å­¦ä¹ çš„å®ç”¨åŒ–

3. **2020 - SimCLR (Chen et al.)**ï¼šç®€åŒ–å¯¹æ¯”å­¦ä¹ 
   - æç®€è®¾è®¡ï¼šæ— é˜Ÿåˆ—ã€æ— Memory Bank
   - ä¾èµ–è¶…å¤§Batch Sizeï¼ˆ4096+ï¼‰
   - æ ¸å¿ƒå‘ç°ï¼šæ•°æ®å¢å¼º+æŠ•å½±å¤´çš„é‡è¦æ€§
   - å‡†ç¡®ç‡çªç ´69%
   - **å±€é™**ï¼šè®¡ç®—æˆæœ¬æé«˜ï¼ˆéœ€è¦TPU v3 Ã—128ï¼‰

#### ç¬¬äºŒé˜¶æ®µï¼šéå¯¹æ¯”é©å‘½ï¼ˆ2020-2021ï¼‰

4. **2020.06 - BYOL (Grill et al., DeepMind)**ï¼šæ‰“ç ´å¯¹æ¯”èŒƒå¼
   - éœ‡æ’¼å‘ç°ï¼š**æ— éœ€è´Ÿæ ·æœ¬å³å¯é˜²æ­¢åç¼©**
   - æœºåˆ¶ï¼šEMAï¼ˆExponential Moving Averageï¼‰ç¼–ç å™¨
   - ç†è®ºç–‘é—®ï¼šä¸ºä»€ä¹ˆä¸ä¼šåç¼©ï¼Ÿ
   - ç¤¾åŒºåå“ï¼šå¼•å‘æ¿€çƒˆäº‰è®ºï¼Œéƒ¨åˆ†å­¦è€…æ€€ç–‘æ˜¯BNçš„éšå¼ä½œç”¨

5. **2020.11 - SimSiam (Chen & He, CVPR 2021)**ï¼šæœ€å°åŒ–è®¾è®¡
   - æè‡´ç®€åŒ–ï¼šå»æ‰EMAï¼Œåªä¿ç•™Stop-gradient
   - æ ¸å¿ƒç»„ä»¶ä»…3ä¸ªï¼š
     * Siameseç½‘ç»œ
     * Predictor MLPï¼ˆ2å±‚ï¼‰
     * Stop-gradientç®—å­
   - ç†è®ºè´¡çŒ®ï¼šè¯æ˜"å¿«æ…¢åŠ¨åŠ›å­¦"æ˜¯å…³é”®
   - å‡†ç¡®ç‡ï¼š71.3%ï¼ˆResNet-50ï¼Œ200epochï¼‰
   - **å“²å­¦æ„ä¹‰**ï¼šLess is Moreçš„å…¸èŒƒ

6. **2021 - Barlow Twins (Zbontar et al.)**ï¼šä¿¡æ¯è®ºè§†è§’
   - åˆ›æ–°ï¼šäº’ä¿¡æ¯å†—ä½™åº¦çº¦æŸ
   - æŸå¤±å‡½æ•°ï¼šäº’åæ–¹å·®çŸ©é˜µâ†’å•ä½é˜µ
   - ä¼˜åŠ¿ï¼šæ— éœ€Predictorã€æ— éœ€Stop-grad
   - ç†è®ºæ¸…æ™°ï¼šç›´æ¥ä¼˜åŒ–ç‰¹å¾ç‹¬ç«‹æ€§

#### ç¬¬ä¸‰é˜¶æ®µï¼šç†è®ºç»Ÿä¸€ä¸æ‰©å±•ï¼ˆ2021-2024ï¼‰

7. **2021 - VICReg (Variance-Invariance-Covariance)**ï¼š
   - å°†BYOL/SimSiamçš„ä¸‰å¤§éšå¼çº¦æŸæ˜¾å¼åŒ–
   - æ–¹å·®æ­£åˆ™åŒ–ï¼šé˜²æ­¢åç¼©åˆ°é›¶ç‚¹
   - ä¸å˜æ€§çº¦æŸï¼šæ­£æ ·æœ¬å¯¹é½
   - åæ–¹å·®æ­£åˆ™åŒ–ï¼šå»ç›¸å…³

8. **2021 - DINO (Caron et al., ICCV)**ï¼š
   - å°†SimSiamæ€æƒ³è¿ç§»åˆ°Vision Transformer
   - æ›¿æ¢BNä¸ºCentering+Sharpening
   - å‘ç°ï¼šè‡ªç›‘ç£ViTæ¶Œç°å‡ºæ˜¾å¼çš„Attention Map
   - å½±å“ï¼šå¯å‘äº†DALL-E 2ã€Stable Diffusionçš„è®¾è®¡

9. **2022 - åŠ¨åŠ›å­¦ç†è®ºçš„å½¢å¼åŒ– (Tian et al.)**ï¼š
   - ç”¨å¾®åˆ†æ–¹ç¨‹ä¸¥æ ¼åˆ†æSimSiam
   - è¯æ˜ï¼šStop-grad = Asymmetric Loss Landscape
   - æ­ç¤ºï¼šPredictorå­¦ä¹ é€Ÿåº¦å¿…é¡» >> Encoder

10. **2023-2024 - å¤§æ¨¡å‹æ—¶ä»£çš„è‡ªç›‘ç£**ï¼š
    - MAEï¼ˆMasked Autoencoderï¼‰ï¼šå›å½’ç”Ÿæˆå¼è‡ªç›‘ç£
    - JEPAï¼ˆJoint-Embedding Predictive Architectureï¼‰ï¼šLeCunçš„ç»Ÿä¸€æ¡†æ¶
    - SimSiamåŸç†è¢«æ•´åˆè¿›å¤šæ¨¡æ€é¢„è®­ç»ƒï¼ˆCLIPå˜ä½“ï¼‰

### 1.3 ä¸¥è°¨å…¬ç†åŒ–

<div class="theorem-box">

### æ ¸å¿ƒå…¬ç†ä½“ç³»ï¼šSimSiam ä¸åç¼©ä¸‰è¦ç´ 

**å…¬ç† 1 (ä¸€è‡´æ€§çº¦æŸ)**ï¼šæ­£æ ·æœ¬å¯¹ $T_1(x), T_2(x)$ çš„è¡¨ç¤ºå¿…é¡»å°½å¯èƒ½é‡åˆã€‚
**å…¬ç† 2 (Predictor å¼•å…¥)**ï¼šæ”¯è·¯é—´å¿…é¡»å­˜åœ¨ä¸€ä¸ªéçº¿æ€§çš„é¢„æµ‹å™¨ $h$ï¼Œæ‰“ç ´æ’ç­‰æ˜ å°„ã€‚
**å…¬ç† 3 (åœæ­¢æ¢¯åº¦ç®—å­)**ï¼šæ¢¯åº¦çš„æµåŠ¨å¿…é¡»æ˜¯ä¸å¯¹ç§°çš„ã€‚
\begin{equation} \nabla_{\theta} \| h_{\boldsymbol{\varphi}}(z_1) - \text{stop\_grad}(z_2) \|^2 \tag{1} \end{equation}

</div>

### 1.4 è®¾è®¡å“²å­¦ï¼šå¿«ä¸æ…¢çš„åšå¼ˆ

SimSiam çš„è®¾è®¡å“²å­¦æ˜¯ï¼š**â€œè·‘å¾—æ¯”åç¼©å¿«ã€‚â€** 
åç¼©æ˜¯ä¸€ä¸ªé•¿æœŸçš„ã€ç»“æ„æ€§çš„è¶‹åŠ¿ã€‚å¦‚æœæ¨¡å‹ä¸­çš„æŸä¸ªéƒ¨åˆ†ï¼ˆPredictorï¼‰èƒ½å¤Ÿä»¥æå¿«çš„é€Ÿåº¦å®Œæˆå¯¹ç›®æ ‡ï¼ˆEncoder è¾“å‡ºï¼‰çš„å±€éƒ¨æ‹Ÿåˆï¼Œé‚£ä¹ˆæ¨åŠ¨ Encoder æ•´ä½“åç¼©çš„æ¢¯åº¦å‹åŠ›å°±ä¼šè¿…é€Ÿæ¶ˆæ•£ã€‚è¿™å°±åƒæ˜¯åœ¨æµæ²™æ²‰æ²¡ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆåœ¨è¡¨é¢é“ºå¥½äº†ä¸€å±‚è½»è´¨ç”²æ¿ã€‚

---

## 2. ä¸¥è°¨çš„æ ¸å¿ƒæ•°å­¦æ¨å¯¼

æœ¬èŠ‚å°†é€šè¿‡åŠ¨åŠ›å­¦æ–¹ç¨‹ç»„ï¼Œå®šé‡æ­ç¤º Stop-gradient å¦‚ä½•æ‹¦æˆªåç¼©è¿‡ç¨‹ã€‚

### 2.1 å»ºç«‹ Siamese åŠ¨åŠ›å­¦æ¨¡å‹

è®¾ç¼–ç å™¨å‚æ•°ä¸º $\boldsymbol{\theta}$ï¼Œé¢„æµ‹å™¨å‚æ•°ä¸º $\boldsymbol{\varphi}$ã€‚æŸå¤±å‡½æ•°ä¸ºï¼š
\begin{equation}
\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\varphi}) = \mathbb{E}_{x, \mathcal{T}_1, \mathcal{T}_2} \left[ \| h_{\boldsymbol{\varphi}}(f_{\boldsymbol{	heta}}(\mathcal{T}_1(x))) - f_{\boldsymbol{	heta}}(\mathcal{T}_2(x)) \|^2 \right] \tag{2}
\end{equation}

<div class="derivation-box">

### æ¨å¯¼ï¼šæœ‰æ—  Stop-gradient çš„æ¢¯åº¦æµå¯¹æ¯”

**æƒ…å½¢ Aï¼šæ—  Stop-gradientï¼ˆå¯¹ç§°æ›´æ–°ï¼‰**
å‚æ•° $\boldsymbol{\theta}$ çš„æ¼”åŒ–é€Ÿåº¦å–å†³äºä¸¤è¾¹çš„æ¢¯åº¦ï¼š
\begin{equation}
\dot{\boldsymbol{\theta}} = -\left( \underbrace{\frac{\partial \mathcal{L}}{\partial f_1} \frac{\partial f_1}{\partial \boldsymbol{\theta}}}_{\text{æ”¯è·¯1}} + \underbrace{\frac{\partial \mathcal{L}}{\partial f_2} \frac{\partial f_2}{\partial \boldsymbol{\theta}}}_{\text{æ”¯è·¯2}} \right) \tag{3}
\end{equation}
ç”±äºä¸¤è¾¹æ–¹å‘ä¸€è‡´ï¼Œ$\boldsymbol{\theta}$ ä¼šè·å¾—åŒå€çš„åŠ¨åŠ›å†²å‘å¸¸æ•°è§£ã€‚

**æƒ…å½¢ Bï¼šæœ‰ Stop-gradient (SimSiam)**
æ”¯è·¯ 2 çš„æ¢¯åº¦è¢«åˆ‡æ–­ï¼ŒåŠ¨åŠ›å­¦å˜ä¸ºï¼š
\begin{equation}
\dot{\boldsymbol{\theta}} = -\frac{\partial \mathcal{L}}{\partial f_1} \frac{\partial f_1}{\partial \boldsymbol{\theta}} \tag{4}
\end{equation}
åŒæ—¶ï¼Œé¢„æµ‹å™¨ $\boldsymbol{\varphi}$ çš„æ¼”åŒ–ä¸ºï¼š
\begin{equation}
\dot{\boldsymbol{\varphi}} = -\frac{\partial \mathcal{L}}{\partial h} \frac{\partial h}{\partial \boldsymbol{\varphi}} \tag{5}
\end{equation}

</div>

### 2.2 ç©å…·æ¨¡å‹åˆ†æï¼šæ ‡é‡æ¼”åŒ–æ¨¡æ‹Ÿ

ä¸ºäº†çœ‹æ¸…æœ¬è´¨ï¼Œæˆ‘ä»¬å‡è®¾ $f_{\theta}(x) = \theta x$ï¼ˆçº¿æ€§ç¼–ç ï¼‰ï¼Œ$h_{\varphi}(z) = \varphi z$ï¼ˆçº¿æ€§é¢„æµ‹ï¼‰ã€‚

<div class="derivation-box">

### æ¨å¯¼ï¼šåç¼©é€Ÿåº¦çš„å®šé‡è®¡ç®—

è®¾ç›®æ ‡æ˜¯æœ€å°åŒ– $\frac{1}{2}(\varphi \theta - \theta)^2$ã€‚

**æ²¡æœ‰ Stop-grad æ—¶**ï¼š
\begin{equation}
\dot{\theta} = -(\varphi \theta - \theta) \varphi = -\theta \varphi (\varphi - 1) \tag{6}
\end{equation}
å¦‚æœåˆå§‹æ—¶ $\varphi$ è¿˜æ²¡å­¦å¥½ï¼ˆä¾‹å¦‚ $\varphi < 1$ï¼‰ï¼Œé‚£ä¹ˆ $\dot{\theta}$ ä¼šè®© $\theta \to 0$ã€‚ä¸€æ—¦ $\theta=0$ï¼Œç‰¹å¾å…¨å¤±ï¼Œæ— æ³•æŒ½å›ã€‚

**æœ‰ Stop-grad æ—¶**ï¼š
ç”±äº Predictor $\varphi$ ä½äºè¾“å‡ºå±‚ï¼Œå…¶å­¦ä¹ è·¯å¾„æ›´çŸ­ï¼Œ**åŠ¨åŠ›å­¦æå¿«**ã€‚
\begin{equation}
\dot{\boldsymbol{\varphi}} = -(\varphi \theta - \theta) \theta = -\theta^2 (\varphi - 1) \tag{7}
\end{equation}
ç”±äº $\dot{\boldsymbol{\varphi}}$ çš„æ”¶æ•›å¸¸æ•°æ˜¯ $\theta^2$ï¼ˆé€šå¸¸å¤§äºé›¶ä¸”è¾ƒç¨³å®šï¼‰ï¼Œ$\varphi$ ä¼šä»¥æŒ‡æ•°çº§é€Ÿåº¦ $e^{-\theta^2 t}$ è¶‹å‘äº 1ã€‚
**å…³é”®ç‚¹**ï¼šå½“ $\varphi$ è¿…é€Ÿåˆ°è¾¾ 1 æ—¶ï¼Œ(6) å¼ä¸­çš„åŠ¨åŠ› $(\varphi - 1)$ å˜ä¸º 0ã€‚
è¿™æ„å‘³ç€ï¼š**Encoder è¿˜æ²¡æ¥å¾—åŠæ»‘åˆ° 0ï¼Œé©±åŠ¨å®ƒæ»‘åŠ¨çš„åŠ›å°±å·²ç»è¢« Predictor æŠµæ¶ˆäº†ã€‚**

</div>

### 2.3 æé›…æ™®è¯ºå¤«ç¨³å®šæ€§åˆ†æ

<div class="theorem-box">

### å®šç†2.1ï¼šSimSiamçš„æ¡ä»¶ç¨³å®šæ€§

**å‘½é¢˜**ï¼šè®¾ç¼–ç å™¨å’Œé¢„æµ‹å™¨çš„å‚æ•°åˆ†åˆ«ä¸º $\boldsymbol{\theta}$ å’Œ $\boldsymbol{\varphi}$ï¼ŒæŸå¤±å‡½æ•°ä¸ºï¼š
\begin{equation}
L(\boldsymbol{\theta}, \boldsymbol{\varphi}) = \mathbb{E}\left[ \| h_{\boldsymbol{\varphi}}(f_{\boldsymbol{\theta}}(x_1)) - f_{\boldsymbol{\theta}}(x_2) \|^2 \right] \tag{8}
\end{equation}

å…¶ä¸­ $x_1, x_2$ æ˜¯åŒä¸€å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾ã€‚

**ç¨³å®šå¹³è¡¡ç‚¹**ï¼šç³»ç»Ÿçš„éå¹³å‡¡ç¨³å®šç‚¹æ»¡è¶³ï¼š
\begin{align}
h_{\boldsymbol{\varphi}}(z) &= z, \quad \forall z \in \text{Range}(f_{\boldsymbol{\theta}}) \tag{9a}\\
\mathbb{E}[f_{\boldsymbol{\theta}}(x_1)] &= \mathbb{E}[f_{\boldsymbol{\theta}}(x_2)] = \mathbf{0} \tag{9b}\\
\text{Cov}(f_{\boldsymbol{\theta}}(x)) &\succ 0 \tag{9c}
\end{align}

**è¯æ˜**ï¼šæ„é€ æé›…æ™®è¯ºå¤«å‡½æ•°ï¼š
\begin{equation}
V(\boldsymbol{\theta}, \boldsymbol{\varphi}) = L(\boldsymbol{\theta}, \boldsymbol{\varphi}) + \lambda \| \text{Cov}(f_{\boldsymbol{\theta}}) - I \|_F^2 \tag{10}
\end{equation}

å…¶ä¸­ $\lambda > 0$ æ˜¯æ­£åˆ™åŒ–ç³»æ•°ï¼ˆéšå¼ç”±BNæä¾›ï¼‰ã€‚

**ç¨³å®šæ€§æ¡ä»¶**ï¼š
1. $\dot{V} < 0$ï¼ˆèƒ½é‡å•è°ƒé€’å‡ï¼‰
2. $\nabla_{\boldsymbol{\varphi}} L$ çš„æ”¶æ•›é€Ÿåº¦ >> $\nabla_{\boldsymbol{\theta}} L$

**å…³é”®å¼•ç†**ï¼šå½“ä½¿ç”¨Stop-gradientæ—¶ï¼Œ$\boldsymbol{\varphi}$ çš„æœ‰æ•ˆå­¦ä¹ ç‡è¢«æ”¾å¤§ $\mathcal{O}(d)$ å€ï¼ˆ$d$ æ˜¯ç‰¹å¾ç»´åº¦ï¼‰ã€‚

</div>

#### 2.3.1 çº¿æ€§åŒ–åˆ†æï¼šé›…å¯æ¯”çŸ©é˜µçš„è°±æ€§è´¨

<div class="derivation-box">

### æ¨å¯¼2.2ï¼šåç¼©è§£çš„å¤±ç¨³æ¡ä»¶

**è®¾å®š**ï¼šè€ƒè™‘åç¼©è§£ $f_{\boldsymbol{\theta}}(x) = \mathbf{c}$ï¼ˆå¸¸æ•°ï¼‰ï¼Œ$h_{\boldsymbol{\varphi}}(z) = \mathbf{c}$ã€‚

**æ‰°åŠ¨åˆ†æ**ï¼šè®¾ $f_{\boldsymbol{\theta}} = \mathbf{c} + \epsilon \mathbf{u}(x)$ï¼Œå…¶ä¸­ $\epsilon \ll 1$ã€‚

**æœ‰Stop-gradçš„æƒ…å†µ**ï¼š

æŸå¤±å‡½æ•°çš„çº¿æ€§åŒ–ï¼š
\begin{align}
L &= \mathbb{E}\left[ \| h_{\boldsymbol{\varphi}}(\mathbf{c} + \epsilon \mathbf{u}_1) - (\mathbf{c} + \epsilon \mathbf{u}_2) \|^2 \right] \tag{11a}\\
&\approx \mathbb{E}\left[ \| \mathbf{J}_{\boldsymbol{\varphi}} \epsilon \mathbf{u}_1 - \epsilon \mathbf{u}_2 \|^2 \right] \tag{11b}\\
&= \epsilon^2 \mathbb{E}\left[ \| (\mathbf{J}_{\boldsymbol{\varphi}} - I) \mathbf{u}_1 \|^2 \right] + \mathcal{O}(\epsilon^3) \tag{11c}
\end{align}

**å…³é”®è§‚å¯Ÿ**ï¼š
- å¦‚æœ $\mathbf{J}_{\boldsymbol{\varphi}} = I$ï¼ˆPredictorå®Œç¾æ‹Ÿåˆï¼‰ï¼Œåˆ™ $L = 0$
- Predictorçš„æ¢¯åº¦ï¼š$\nabla_{\boldsymbol{\varphi}} L \propto \epsilon^2$ ï¼ˆäºŒé˜¶å°é‡ï¼‰
- Encoderçš„æ¢¯åº¦ï¼š$\nabla_{\boldsymbol{\theta}} L \propto \epsilon$ ï¼ˆä¸€é˜¶å°é‡ï¼‰

**ç»“è®º**ï¼šç”±äº $\boldsymbol{\varphi}$ çš„æ¢¯åº¦æ›´å°ï¼Œå®ƒä¼š**å…ˆ**æ”¶æ•›åˆ°ä½¿ $\mathbf{J}_{\boldsymbol{\varphi}} \to I$ çš„é…ç½®ï¼Œä»è€Œ**æˆªæ–­** $\boldsymbol{\theta}$ ç»§ç»­åç¼©çš„åŠ¨åŠ›ã€‚

</div>

### 2.4 æ·±åº¦å±•å¼€åˆ†æï¼šéšå¼æ–¹å·®è¡¥å¿

å¦‚æœå°† SimSiam çœ‹ä½œä¸€ä¸ª EM ç®—æ³•ï¼ˆExpectation-Maximizationï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ›´æœ‰è¶£çš„å‘ç°ã€‚

<div class="formula-explanation">

### æŸå¤±å‡½æ•°çš„ä¸€é˜¶æ³°å‹’å±•å¼€

å‡è®¾æ•°æ®å¢å¼º $\mathcal{T}(x) = x + \Delta x$ï¼Œå…¶ä¸­ $\Delta x$ æ˜¯å°æ‰°åŠ¨ã€‚

<div class="formula-step">
<div class="step-label">1. ç›®æ ‡ä¸­å¿ƒåŒ–</div>
å¯¹äºç›®æ ‡é¡¹ $f_{\theta}(\mathcal{T}_2(x))$ï¼Œå…¶å¹³å‡å€¼ä¸º $\bar{z} = f_{\theta}(\bar{x})$ã€‚
</div>

<div class="formula-step">
<div class="step-label">2. å±•å¼€é¢„æµ‹è¯¯å·®</div>
\begin{equation}
\mathcal{L}(\theta) \approx \mathbb{E}_{x, \Delta x} \left[ \left\Vert \boldsymbol{J}_{\theta}(x) \Delta x \right\|^2 \right] \tag{12}
\end{equation}
å…¶ä¸­ $\boldsymbol{J}_{\theta}$ æ˜¯ç¼–ç å™¨çš„é›…å¯æ¯”çŸ©é˜µï¼ˆç‰¹å¾çµæ•åº¦ï¼‰ã€‚
</div>

<div class="formula-step">
<parameter name="step-label">3. å‡ ä½•æ„ä¹‰</div>
SimSiam å®é™…ä¸Šåœ¨å¯»æ‰¾ä¸€ä¸ªç‰¹å¾æ˜ å°„ï¼Œä½¿å¾—å®ƒå¯¹å¸¸è§çš„å›¾åƒå˜æ¢ï¼ˆæ•°æ®å¢å¼ºï¼‰å…·æœ‰ä½æ•æ„Ÿåº¦ï¼ŒåŒæ—¶é€šè¿‡ Predictor çš„è§£è€¦æ•ˆåº”ï¼Œåœ¨ä¸ç‰ºç‰²è¡¨ç¤ºç»´åº¦ï¼ˆå³ä¸åç¼©ï¼‰çš„å‰æä¸‹å®ç°è¿™ä¸€ç‚¹ã€‚
</div>

</div>

### 2.5 Batch Normalizationçš„éšå¼ä½œç”¨

<div class="critical-analysis">

**æ ¸å¿ƒç–‘é—®**ï¼šä¸ºä»€ä¹ˆSimSiamå¼ºçƒˆä¾èµ–BNï¼Ÿ

**ç­”æ¡ˆ**ï¼šBNæä¾›äº†ä¸‰é‡éšå¼çº¦æŸ

#### çº¦æŸ1ï¼šéšå¼å»ä¸­å¿ƒåŒ–ï¼ˆImplicit Centeringï¼‰

BNå±‚å¼ºåˆ¶æ¯ä¸ªæ‰¹æ¬¡çš„ç‰¹å¾å‡å€¼ä¸ºé›¶ï¼š
\begin{equation}
\mathbb{E}_{\text{batch}}[z_i] = 0 \tag{13}
\end{equation}

è¿™é˜²æ­¢äº†æ‰€æœ‰ç‰¹å¾åŒæ—¶æ¼‚ç§»åˆ°ç›¸åŒçš„éé›¶å¸¸æ•°ã€‚

#### çº¦æŸ2ï¼šéšå¼æ–¹å·®æ­£åˆ™åŒ–ï¼ˆImplicit Variance Regularizationï¼‰

BNæ ‡å‡†åŒ–æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ–¹å·®ä¸º1ï¼š
\begin{equation}
\text{Var}_{\text{batch}}(z_i) = 1 \tag{14}
\end{equation}

è¿™é˜²æ­¢äº†ç‰¹å¾åç¼©åˆ°é›¶ç‚¹ï¼ˆæ–¹å·®ä¸º0ï¼‰ã€‚

#### çº¦æŸ3ï¼šéšå¼Batchå†…å¯¹æ¯”ï¼ˆImplicit Batch-level Contrastï¼‰

**å®šç†2.2ï¼ˆRichemond et al. 2021ï¼‰**ï¼šBNåœ¨batchç»´åº¦å¼•å…¥çš„éšå¼å¯¹æ¯”æ•ˆåº”ç­‰ä»·äºï¼š
\begin{equation}
L_{\text{BN}} = L_{\text{SimSiam}} + \underbrace{\frac{\lambda}{B} \sum_{i \neq j} \langle z_i, z_j \rangle}_{\text{éšå¼è´Ÿæ ·æœ¬é¡¹}} \tag{15}
\end{equation}

å…¶ä¸­ $B$ æ˜¯batch sizeï¼Œ$\lambda$ æ˜¯éšå¼ç³»æ•°ã€‚

**å®éªŒéªŒè¯**ï¼š
- å»æ‰BNåï¼ŒSimSiamåœ¨100 epochå†…åç¼©ï¼ˆæ‰€æœ‰ç‰¹å¾ â†’ é›¶å‘é‡ï¼‰
- ä½¿ç”¨LayerNorm/GroupNormæ›¿ä»£BNï¼Œåç¼©é€Ÿåº¦å‡ç¼“ä½†ä»ç„¶å‘ç”Ÿ
- åªæœ‰ä¿ç•™Batchç»´åº¦ç»Ÿè®¡çš„å½’ä¸€åŒ–ï¼ˆå¦‚SyncBNï¼‰æ‰èƒ½å®Œå…¨é˜²æ­¢åç¼©

</div>

### 2.6 éçº¿æ€§åŠ¨åŠ›å­¦ï¼šå¿«å˜æµå½¢ç†è®º

<div class="advanced-theory">

#### å¿«æ…¢ç³»ç»Ÿåˆ†è§£ï¼ˆSlow-Fast Systems Decompositionï¼‰

å°†SimSiamå»ºæ¨¡ä¸ºå¥‡å¼‚æ‘„åŠ¨ç³»ç»Ÿï¼ˆSingular Perturbation Systemï¼‰ï¼š
\begin{align}
\dot{\boldsymbol{\theta}} &= -\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}, \boldsymbol{\varphi}) \tag{16a}\\
\epsilon \dot{\boldsymbol{\varphi}} &= -\nabla_{\boldsymbol{\varphi}} L(\boldsymbol{\theta}, \boldsymbol{\varphi}) \tag{16b}
\end{align}

å…¶ä¸­ $\epsilon \ll 1$ è¡¨ç¤ºPredictorçš„æ—¶é—´å°ºåº¦è¿œå°äºEncoderã€‚

**Tikhonovå®šç†åº”ç”¨**ï¼š
åœ¨ $\epsilon \to 0$ æé™ä¸‹ï¼Œç³»ç»Ÿæ¼”åŒ–åˆ†ä¸¤ä¸ªé˜¶æ®µï¼š

**å¿«é€Ÿé˜¶æ®µ**ï¼ˆFast Transientï¼Œ$t = \mathcal{O}(\epsilon)$ï¼‰ï¼š
- $\boldsymbol{\theta}$ å‡ ä¹ä¸åŠ¨
- $\boldsymbol{\varphi}$ å¿«é€Ÿæ”¶æ•›åˆ°å‡†å¹³è¡¡ç‚¹ï¼š
  \begin{equation}
  \nabla_{\boldsymbol{\varphi}} L(\boldsymbol{\theta}, \boldsymbol{\varphi}) = 0 \Rightarrow h_{\boldsymbol{\varphi}}(z) \approx z \tag{17}
  \end{equation}

**æ…¢é€Ÿé˜¶æ®µ**ï¼ˆSlow Manifoldï¼Œ$t = \mathcal{O}(1)$ï¼‰ï¼š
- $\boldsymbol{\varphi}$ å§‹ç»ˆä¿æŒåœ¨å‡†å¹³è¡¡æµå½¢ä¸Š
- $\boldsymbol{\theta}$ æ²¿ç€é™ç»´çš„æœ‰æ•ˆèƒ½é‡é¢æ¼”åŒ–ï¼š
  \begin{equation}
  \dot{\boldsymbol{\theta}} \approx -\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}, \boldsymbol{\varphi}^*(\boldsymbol{\theta})) \tag{18}
  \end{equation}

**å‡ ä½•è§£é‡Š**ï¼šPredictoråœ¨é«˜ç»´å‚æ•°ç©ºé—´ä¸­å¿«é€Ÿ"æ»‘è¡Œ"åˆ°ä¸€ä¸ªä½ç»´æµå½¢ï¼ˆæ…¢æµå½¢ï¼‰ï¼ŒEncoderåˆ™è¢«çº¦æŸåœ¨è¿™ä¸ªæµå½¢ä¸Šç¼“æ…¢ä¼˜åŒ–ã€‚è¿™ç§é™ç»´æ•ˆåº”å¤©ç„¶é˜²æ­¢äº†åç¼©ï¼Œå› ä¸ºæµå½¢çš„ç»´åº¦ç”±æ•°æ®å¢å¼ºçš„å¤šæ ·æ€§å†³å®šï¼Œè€Œéç½‘ç»œçš„è¿‡å‚æ•°åŒ–ã€‚

</div>

---

## 3. æ•°å­¦ç›´è§‰ã€å‡ ä½•è§†è§’ä¸å¤šç»´ç±»æ¯”

<div class="intuition-box">

### ğŸ§  ç›´è§‰ç†è§£ï¼šå½±å­çƒä¸å¿«é€Ÿæ•æ‰æ‰‹ ğŸ¾

æƒ³è±¡ä½ åœ¨å’Œä¸€ä¸ªå½±å­ï¼ˆPredictorï¼‰ç©æŠ›æ¥çƒã€‚

1.  **åç¼©ï¼ˆå…¨æ¢¯åº¦ï¼‰**ï¼šä½ å’Œå½±å­éƒ½åœ¨æ‹¼å‘½å¾€åœ°æ¿ï¼ˆé›¶ç‚¹ï¼‰ç¼©ã€‚å› ä¸ºä½ ä»¬åŠ¨ä½œä¸€è‡´ï¼Œæœ€åä½ ä»¬éƒ½ä¼šå˜æˆåœ°æ¿ä¸Šçš„ä¸€ä¸ªç‚¹ã€‚
2.  **SimSiam ä¸åç¼©**ï¼š
    *   ä½ ï¼ˆEncoderï¼‰åŠ¨å¾—å¾ˆæ…¢ã€‚
    *   å½±å­ï¼ˆPredictorï¼‰æ˜¯ä¸€ä¸ªèº«æ‰‹æå¿«çš„æ•æ‰æ‰‹ã€‚
    *   **Stop-gradient**ï¼šä½ æŠ›çƒæ—¶ï¼Œå½±å­å¿…é¡»åœä¸‹æ¥æ¥ï¼Œä¸èƒ½åè¿‡æ¥æ‹½ä½ ã€‚
    *   **ç»“æœ**ï¼šæ¯å½“ä½ ç¨å¾®åç¦»ä¸€ç‚¹æ–¹å‘ï¼Œå½±å­ç”±äºåŠ¨ä½œæå¿«ï¼Œä¼šåœ¨ä½ è¿˜æ²¡åŠ¨ä¸‹ä¸€è„šä¹‹å‰å°±ç«™åœ¨äº†çƒçš„è½ç‚¹ä¸Šã€‚æ—¢ç„¶å½±å­å·²ç»æ¥åˆ°äº†çƒï¼ˆLoss å˜å°ï¼‰ï¼Œä½ å°±æ²¡æœ‰åŠ¨åŠ›ç»§ç»­å¾€åœ°æ¿ç¼©äº†ã€‚ä½ åœåœ¨äº†åŠè·¯ï¼Œä¿ä½äº†ä½ çš„ä½ç½®ï¼ˆç‰¹å¾ï¼‰ã€‚

</div>

### 3.2 å‡ ä½•è§†è§’ï¼šèƒ½é‡ç›†åœ°çš„è„Šçº¿é©»ç•™

åœ¨ç‰¹å¾ç©ºé—´ä¸­ï¼Œåç¼©æ˜¯ä¸€ä¸ªæ·±ä¸è§åº•çš„ä¸­å¿ƒé»‘æ´ã€‚
- **å¯¹æ¯”å­¦ä¹ **ï¼šæ˜¯åœ¨é»‘æ´å‘¨å›´ä¿®äº†ä¸€åœˆæŒ¡æ¿ï¼ˆè´Ÿæ ·æœ¬ï¼‰ã€‚
- **SimSiam**ï¼šæ˜¯åˆ©ç”¨åŠ¨åŠ›å­¦åœ¨é»‘æ´è¾¹ç¼˜å»ºç«‹äº†ä¸€ä¸ªâ€œåŠ¨æ€å¹³è¡¡è½¨é“â€ã€‚é€šè¿‡åˆ‡æ–­æ¢¯åº¦ï¼Œæˆ‘ä»¬å°†åŸæœ¬å‚ç›´è½å…¥é»‘æ´çš„åŠ›ï¼Œè½¬åŒ–ä¸ºäº†åœ¨è½¨é“ä¸Šåˆ‡å‘è¿åŠ¨çš„åŠ›ã€‚è¿™ç§ç°è±¡åœ¨éçº¿æ€§ç‰©ç†ä¸­è¢«ç§°ä¸º**â€œå¸å¼•å­çš„æ‹“æ‰‘æ”¹å˜â€**ã€‚

---

## 4. æ–¹æ³•è®ºå˜ä½“ã€æ‰¹åˆ¤æ€§æ¯”è¾ƒä¸ä¼˜åŒ–

### 4.1 å…¨é‡å¯¹æ¯”è¡¨

| æ¨¡å‹ | é˜²åç¼©æœºåˆ¶ | æ ¸å¿ƒç»„ä»¶ | **è‡´å‘½ç¼ºé™·** |
| :--- | :--- | :--- | :--- |
| **SimCLR** | è´Ÿæ ·æœ¬å¯¹é½ | å¤§ Batch Size | âŒ è®¡ç®—å¼€é”€æå¤§ |
| **BYOL** | åŠ¨é‡é¢„æµ‹ | EMA ç¼–ç å™¨ | âŒ ç†è®ºè¯æ˜å¤æ‚ |
| **SimSiam** | **åŠ¨åŠ›å­¦è§£è€¦** | **Stop-grad + Predictor** | âŒ **å¯¹ BN æåº¦ä¾èµ–** |
| **VICReg** | åæ–¹å·®çº¦æŸ | Variance Regularization | âŒ å‚æ•°è°ƒä¼˜å›°éš¾ |

### 4.2 æ·±åº¦æ‰¹åˆ¤ï¼šSimSiam çš„â€œä¼ªç§‘å­¦â€é™·é˜±

è™½ç„¶å®éªŒç»“æœæƒŠè‰³ï¼Œä½† SimSiam çš„ç†è®ºåŸºç¡€å­˜åœ¨ä¸‰ä¸ªè„†å¼±ç‚¹ï¼š

1.  **è‡´å‘½ç¼ºé™· 1ï¼šBatch Normalization (BN) çš„éšå¼å¯¹æ¯”**
    *   **åˆ†æ**ï¼šå¦‚æœå»æ‰ BNï¼ŒSimSiam ä¼šç¬é—´åç¼©ã€‚
    *   **çœŸç›¸**ï¼šBN åœ¨ Batch ç»´åº¦ä¸Šçš„å‡å€¼å’Œæ–¹å·®è®¡ç®—ï¼Œå®é™…ä¸Šæä¾›äº†ä¸€ç§éšå¼çš„â€œè´Ÿæ ·æœ¬â€æ•ˆåº”ï¼Œå¼ºè¿«åŒä¸€ä¸ª Batch å†…çš„ç‰¹å¾ä¸èƒ½å…¨ç­‰ã€‚**SimSiam çš„æˆåŠŸæœ‰ä¸€åŠæ˜¯å±äº BN çš„ã€‚**
2.  **è‡´å‘½ç¼ºé™· 2ï¼šPredictor çš„æ¶æ„é»‘ç®±**
    *   **é—®é¢˜**ï¼šPredictor å¦‚æœå¤ªæ·±ï¼Œæ”¶æ•›ææ…¢ï¼›å¦‚æœå¤ªæµ…ï¼Œæ— æ³•æ‰“ç ´å¯¹ç§°æ€§ã€‚
    *   **å±€é™**ï¼šç›®å‰æ²¡æœ‰æ•°å­¦å…¬å¼èƒ½è®¡ç®—å‡ºé’ˆå¯¹ç‰¹å®šä¸»å¹²ç½‘ç»œçš„æœ€ä¼˜ Predictor æ·±åº¦ã€‚
3.  **è‡´å‘½ç¼ºé™· 3ï¼šç‰¹å¾å†—ä½™ (Redundancy)**
    *   ç”±äºæ²¡æœ‰å»ç›¸å…³çš„æ˜¾å¼çº¦æŸï¼ŒSimSiam å­¦åˆ°çš„ 2048 ç»´ç‰¹å¾ä¸­ï¼Œå¯èƒ½åªæœ‰æå°‘æ•°ç»´åº¦æ˜¯æœ‰ä¿¡æ¯çš„ï¼Œå…¶ä½™ç»´åº¦é«˜åº¦ç›¸å…³ã€‚

### 4.3 ä¼˜åŒ–æ¼”è¿›

*   **Barlow Twins**ï¼šé€šè¿‡è®©äº’åæ–¹å·®çŸ©é˜µé€¼è¿‘å•ä½é˜µï¼Œä»æ•°å­¦ä¸Šå½»åº•æ¶ˆé™¤äº†åç¼©çš„å¯èƒ½æ€§ï¼Œä¸å†ä¾èµ–åŠ¨åŠ›å­¦å·§åˆã€‚
*   **DINO**ï¼šå°† SimSiam çš„æ€æƒ³åº”ç”¨åˆ° Transformer ä¸­ï¼Œåˆ©ç”¨ä¸­å¿ƒåŒ–ï¼ˆCenteringï¼‰å’Œé”åŒ–ï¼ˆSharpeningï¼‰æ›¿ä»£ BNï¼Œå®ç°äº†æ›´é«˜è´¨é‡çš„æ— ç›‘ç£å­¦ä¹ ã€‚

---

## 5. å®Œæ•´æ•°å€¼å®éªŒï¼šä»ç©å…·æ¨¡å‹åˆ°çœŸå®è®­ç»ƒ

### 5.1 å®éªŒ1ï¼šç©å…·æ¨¡å‹å¯è§†åŒ–

<div class="code-box">

**ç›®æ ‡**ï¼šé€šè¿‡æ ‡é‡åŠ¨åŠ›å­¦ç›´è§‚å±•ç¤ºStop-gradientçš„ä½œç”¨ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt

# ç©å…·æ¨¡å‹ï¼šçº¿æ€§ç¼–ç å™¨å’Œé¢„æµ‹å™¨
def toy_dynamics(T=500, gamma_theta=0.01, gamma_phi=0.1, use_stopgrad=True):
    """
    æ¨¡æ‹Ÿæ ‡é‡SimSiamåŠ¨åŠ›å­¦

    å‚æ•°:
        T: è¿­ä»£æ­¥æ•°
        gamma_theta: Encoderå­¦ä¹ ç‡ï¼ˆæ…¢ï¼‰
        gamma_phi: Predictorå­¦ä¹ ç‡ï¼ˆå¿«ï¼‰
        use_stopgrad: æ˜¯å¦ä½¿ç”¨Stop-gradient
    """
    # åˆå§‹åŒ–
    theta = 1.0  # ç¼–ç å™¨å‚æ•°
    phi = 0.1    # é¢„æµ‹å™¨å‚æ•°ï¼ˆåˆå§‹æ—¶è¿œç¦»1ï¼‰

    # è®°å½•è½¨è¿¹
    theta_history = [theta]
    phi_history = [phi]
    loss_history = []

    for t in range(T):
        # è®¡ç®—æŸå¤±ï¼šL = 0.5 * (phi * theta - theta)^2
        loss = 0.5 * (phi * theta - theta)**2
        loss_history.append(loss)

        if use_stopgrad:
            # Stop-gradientï¼šåªæœ‰phiæ”¶åˆ°æ¢¯åº¦
            grad_phi = (phi * theta - theta) * theta  # âˆ‚L/âˆ‚phi
            grad_theta = 0  # è¢«stop_gradæˆªæ–­
        else:
            # æ— Stop-gradientï¼šåŒå‘æ¢¯åº¦
            grad_phi = (phi * theta - theta) * theta
            grad_theta = (phi * theta - theta) * (phi - 1)

        # æ›´æ–°å‚æ•°
        phi -= gamma_phi * grad_phi
        theta -= gamma_theta * grad_theta

        theta_history.append(theta)
        phi_history.append(phi)

    return np.array(theta_history), np.array(phi_history), np.array(loss_history)

# è¿è¡Œå®éªŒï¼šå¯¹æ¯”æœ‰æ— Stop-gradient
theta_sg, phi_sg, loss_sg = toy_dynamics(T=500, use_stopgrad=True)
theta_no, phi_no, loss_no = toy_dynamics(T=500, use_stopgrad=False)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# å­å›¾1ï¼šå‚æ•°è½¨è¿¹
ax1 = axes[0]
ax1.plot(theta_sg, label='Î¸ (w/ Stop-grad)', linewidth=2, color='C0')
ax1.plot(phi_sg, label='Ï† (w/ Stop-grad)', linewidth=2, color='C1', linestyle='--')
ax1.plot(theta_no, label='Î¸ (w/o Stop-grad)', linewidth=2, color='C2', alpha=0.7)
ax1.plot(phi_no, label='Ï† (w/o Stop-grad)', linewidth=2, color='C3', alpha=0.7, linestyle='--')
ax1.axhline(y=0, color='red', linestyle=':', linewidth=1.5, label='Collapse Point')
ax1.axhline(y=1, color='green', linestyle=':', linewidth=1.5, label='Target (Ï†=1)')
ax1.set_xlabel('Iteration', fontsize=12)
ax1.set_ylabel('Parameter Value', fontsize=12)
ax1.set_title('Parameter Trajectory Comparison', fontsize=14, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)

# å­å›¾2ï¼šç›¸ç©ºé—´ï¼ˆÎ¸-Ï†å¹³é¢ï¼‰
ax2 = axes[1]
ax2.plot(theta_sg, phi_sg, linewidth=2.5, color='C0', label='w/ Stop-grad')
ax2.plot(theta_no, phi_no, linewidth=2.5, color='C2', alpha=0.7, label='w/o Stop-grad')
ax2.plot(theta_sg[0], phi_sg[0], 'go', markersize=10, label='Start')
ax2.plot(theta_sg[-1], phi_sg[-1], 'r*', markersize=15, label='End (Stop-grad)')
ax2.plot(theta_no[-1], phi_no[-1], 'bx', markersize=12, label='End (No Stop-grad)')
ax2.axvline(x=0, color='red', linestyle=':', linewidth=1.5, alpha=0.5)
ax2.axhline(y=1, color='green', linestyle=':', linewidth=1.5, alpha=0.5)
ax2.set_xlabel('Encoder Î¸', fontsize=12)
ax2.set_ylabel('Predictor Ï†', fontsize=12)
ax2.set_title('Phase Space Trajectory', fontsize=14, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(True, alpha=0.3)

# å­å›¾3ï¼šæŸå¤±æ¼”åŒ–
ax3 = axes[2]
ax3.semilogy(loss_sg, linewidth=2.5, color='C0', label='w/ Stop-grad')
ax3.semilogy(loss_no, linewidth=2.5, color='C2', alpha=0.7, label='w/o Stop-grad')
ax3.set_xlabel('Iteration', fontsize=12)
ax3.set_ylabel('Loss (log scale)', fontsize=12)
ax3.set_title('Loss Evolution', fontsize=14, fontweight='bold')
ax3.legend(fontsize=10)
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('simsiam_toy_dynamics.png', dpi=150)
print("âœ“ å›¾åƒå·²ä¿å­˜è‡³ simsiam_toy_dynamics.png")

# æ‰“å°å…³é”®è§‚å¯Ÿ
print("\nå…³é”®è§‚å¯Ÿï¼š")
print(f"1. Stop-gradæƒ…å†µï¼š")
print(f"   - æœ€ç»ˆÎ¸ = {theta_sg[-1]:.4f} ï¼ˆä¿æŒéé›¶ï¼ï¼‰")
print(f"   - æœ€ç»ˆÏ† = {phi_sg[-1]:.4f} ï¼ˆæ¥è¿‘1ï¼‰")
print(f"   - æœ€ç»ˆLoss = {loss_sg[-1]:.6f}")
print(f"\n2. æ— Stop-gradæƒ…å†µï¼š")
print(f"   - æœ€ç»ˆÎ¸ = {theta_no[-1]:.4f} ï¼ˆåç¼©åˆ°é›¶ï¼ï¼‰")
print(f"   - æœ€ç»ˆÏ† = {phi_no[-1]:.4f}")
print(f"   - æœ€ç»ˆLoss = {loss_no[-1]:.6f}")
```

**è¾“å‡ºè§£é‡Š**ï¼š
- **æœ‰Stop-gradient**ï¼š$\theta$ ä¿æŒåœ¨éé›¶å€¼ï¼Œ$\varphi$ å¿«é€Ÿæ”¶æ•›åˆ°1ï¼Œç³»ç»Ÿç¨³å®š
- **æ— Stop-gradient**ï¼š$\theta$ è¿…é€Ÿåç¼©åˆ°0ï¼Œ$\varphi$ æ— æ³•è¡¥æ•‘ï¼Œç³»ç»Ÿå¤±è´¥

</div>

### 5.2 å®éªŒ2ï¼šå®Œæ•´SimSiamå®ç°ä¸è®­ç»ƒ

<div class="code-box">

**ç›®æ ‡**ï¼šåœ¨CIFAR-10ä¸Šå¤ç°SimSiamï¼ŒéªŒè¯BNä¾èµ–æ€§ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np

# SimSiamæ¶æ„
class SimSiam(nn.Module):
    def __init__(self, base_encoder, dim=2048, pred_dim=512):
        """
        å‚æ•°:
            base_encoder: éª¨å¹²ç½‘ç»œï¼ˆå¦‚ResNet-18ï¼‰
            dim: æŠ•å½±å¤´è¾“å‡ºç»´åº¦
            pred_dim: Predictoréšè—å±‚ç»´åº¦
        """
        super(SimSiam, self).__init__()

        # Encoder
        self.encoder = base_encoder
        # è·å–encoderè¾“å‡ºç»´åº¦
        self.encoder_dim = base_encoder.fc.in_features
        base_encoder.fc = nn.Identity()  # ç§»é™¤åˆ†ç±»å¤´

        # Projection Headï¼ˆ3å±‚MLPï¼‰
        self.projector = nn.Sequential(
            nn.Linear(self.encoder_dim, pred_dim, bias=False),
            nn.BatchNorm1d(pred_dim),  # å…³é”®ï¼šBNå±‚
            nn.ReLU(inplace=True),
            nn.Linear(pred_dim, pred_dim, bias=False),
            nn.BatchNorm1d(pred_dim),
            nn.ReLU(inplace=True),
            nn.Linear(pred_dim, dim, bias=False),
            nn.BatchNorm1d(dim, affine=False)  # è¾“å‡ºBNæ— å¯å­¦ä¹ å‚æ•°
        )

        # Predictorï¼ˆ2å±‚MLPï¼‰
        self.predictor = nn.Sequential(
            nn.Linear(dim, pred_dim, bias=False),
            nn.BatchNorm1d(pred_dim),
            nn.ReLU(inplace=True),
            nn.Linear(pred_dim, dim)  # æ— BN
        )

    def forward(self, x1, x2):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            x1, x2: ä¸¤ä¸ªaugmented views

        è¿”å›:
            p1, p2: Predictorè¾“å‡º
            z1, z2: Projectorè¾“å‡ºï¼ˆå°†è¢«detachï¼‰
        """
        # ç¼–ç +æŠ•å½±
        z1 = self.projector(self.encoder(x1))
        z2 = self.projector(self.encoder(x2))

        # é¢„æµ‹
        p1 = self.predictor(z1)
        p2 = self.predictor(z2)

        return p1, p2, z1.detach(), z2.detach()

# æŸå¤±å‡½æ•°
def simsiam_loss(p, z):
    """
    è´Ÿä½™å¼¦ç›¸ä¼¼åº¦

    å‚æ•°:
        p: Predictorè¾“å‡º
        z: Targetï¼ˆå·²detachï¼‰
    """
    # L2å½’ä¸€åŒ–
    p = F.normalize(p, dim=1)
    z = F.normalize(z, dim=1)

    # è´Ÿä½™å¼¦ç›¸ä¼¼åº¦ = 1 - cos(p, z)
    return -(p * z).sum(dim=1).mean()

# æ•°æ®å¢å¼º
def get_transforms():
    """SimSiamçš„æ•°æ®å¢å¼ºç­–ç•¥"""
    return transforms.Compose([
        transforms.RandomResizedCrop(32, scale=(0.2, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomApply([
            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
        ], p=0.8),
        transforms.RandomGrayscale(p=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.4914, 0.4822, 0.4465],
                           [0.2023, 0.1994, 0.2010])
    ])

# TwoCropsTransformï¼šç”Ÿæˆä¸¤ä¸ªaugmented views
class TwoCropsTransform:
    def __init__(self, base_transform):
        self.base_transform = base_transform

    def __call__(self, x):
        return [self.base_transform(x), self.base_transform(x)]

# è®­ç»ƒå‡½æ•°
def train_simsiam(model, train_loader, epochs=100, lr=0.05, device='cuda'):
    """è®­ç»ƒSimSiamæ¨¡å‹"""
    model = model.to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr,
                               momentum=0.9, weight_decay=1e-4)

    # Cosineå­¦ä¹ ç‡è°ƒåº¦
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=epochs
    )

    # è®°å½•ç»Ÿè®¡
    loss_history = []

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0

        for batch_idx, ([x1, x2], _) in enumerate(train_loader):
            x1, x2 = x1.to(device), x2.to(device)

            # å‰å‘ä¼ æ’­
            p1, p2, z1, z2 = model(x1, x2)

            # è®¡ç®—å¯¹ç§°æŸå¤±
            loss = 0.5 * simsiam_loss(p1, z2) + 0.5 * simsiam_loss(p2, z1)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        # è®°å½•
        avg_loss = epoch_loss / len(train_loader)
        loss_history.append(avg_loss)

        # å­¦ä¹ ç‡è¡°å‡
        scheduler.step()

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], '
                  f'Loss: {avg_loss:.4f}, '
                  f'LR: {scheduler.get_last_lr()[0]:.6f}')

    return loss_history

# ä¸»å®éªŒ
def run_cifar10_experiment():
    """CIFAR-10å®Œæ•´å®éªŒ"""
    # æ•°æ®åŠ è½½
    transform = TwoCropsTransform(get_transforms())
    train_dataset = datasets.CIFAR10(root='./data', train=True,
                                     download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=512,
                             shuffle=True, num_workers=4,
                             pin_memory=True, drop_last=True)

    # æ¨¡å‹åˆå§‹åŒ–
    from torchvision.models import resnet18
    base_encoder = resnet18()
    model = SimSiam(base_encoder, dim=2048, pred_dim=512)

    # è®­ç»ƒ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}\n")

    loss_history = train_simsiam(model, train_loader, epochs=100,
                                 lr=0.05, device=device)

    # å¯è§†åŒ–æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(loss_history, linewidth=2)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('SimSiam Loss', fontsize=12)
    plt.title('Training Loss Curve (CIFAR-10)', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.savefig('simsiam_cifar10_loss.png', dpi=150)
    print("âœ“ æŸå¤±æ›²çº¿å·²ä¿å­˜è‡³ simsiam_cifar10_loss.png")

    return model, loss_history

# è¿è¡Œå®éªŒ
model, loss_history = run_cifar10_experiment()
```

</div>

### 5.3 å®éªŒ3ï¼šBNä¾èµ–æ€§æ¶ˆèå®éªŒ

<div class="code-box">

**ç›®æ ‡**ï¼šéªŒè¯å»æ‰BNåSimSiamæ˜¯å¦åç¼©ã€‚

```python
def ablation_study_bn():
    """BNæ¶ˆèå®éªŒ"""

    # å®šä¹‰æ— BNçš„SimSiamï¼ˆç”¨LayerNormæ›¿ä»£ï¼‰
    class SimSiamNoBN(nn.Module):
        def __init__(self, base_encoder, dim=2048, pred_dim=512):
            super(SimSiamNoBN, self).__init__()
            self.encoder = base_encoder
            self.encoder_dim = base_encoder.fc.in_features
            base_encoder.fc = nn.Identity()

            # æŠ•å½±å¤´ï¼ˆä½¿ç”¨LayerNormï¼‰
            self.projector = nn.Sequential(
                nn.Linear(self.encoder_dim, pred_dim),
                nn.LayerNorm(pred_dim),  # æ›¿æ¢BN
                nn.ReLU(inplace=True),
                nn.Linear(pred_dim, pred_dim),
                nn.LayerNorm(pred_dim),
                nn.ReLU(inplace=True),
                nn.Linear(pred_dim, dim)
            )

            # é¢„æµ‹å™¨
            self.predictor = nn.Sequential(
                nn.Linear(dim, pred_dim),
                nn.LayerNorm(pred_dim),
                nn.ReLU(inplace=True),
                nn.Linear(pred_dim, dim)
            )

        def forward(self, x1, x2):
            z1 = self.projector(self.encoder(x1))
            z2 = self.projector(self.encoder(x2))
            p1 = self.predictor(z1)
            p2 = self.predictor(z2)
            return p1, p2, z1.detach(), z2.detach()

    # è®­ç»ƒä¸¤ä¸ªç‰ˆæœ¬å¹¶å¯¹æ¯”
    print("è®­ç»ƒæ ‡å‡†SimSiamï¼ˆå¸¦BNï¼‰...")
    model_bn = SimSiam(resnet18(), dim=2048)
    loss_bn = train_simsiam(model_bn, train_loader, epochs=50)

    print("\nè®­ç»ƒSimSiamï¼ˆæ— BNï¼Œç”¨LayerNormï¼‰...")
    model_ln = SimSiamNoBN(resnet18(), dim=2048)
    loss_ln = train_simsiam(model_ln, train_loader, epochs=50)

    # å¯è§†åŒ–å¯¹æ¯”
    plt.figure(figsize=(12, 6))
    plt.plot(loss_bn, label='With BatchNorm', linewidth=2.5, color='C0')
    plt.plot(loss_ln, label='With LayerNorm (No BN)', linewidth=2.5,
             color='C1', linestyle='--')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('BN Ablation Study', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.savefig('simsiam_bn_ablation.png', dpi=150)

    print(f"\næœ€ç»ˆæŸå¤±å¯¹æ¯”ï¼š")
    print(f"  BNç‰ˆæœ¬: {loss_bn[-1]:.4f}")
    print(f"  LayerNormç‰ˆæœ¬: {loss_ln[-1]:.4f}")
    print(f"  å·®å¼‚: {abs(loss_bn[-1] - loss_ln[-1]):.4f}")

    # æ£€æŸ¥åç¼©ï¼ˆç‰¹å¾æ ‡å‡†å·®ï¼‰
    model_bn.eval()
    model_ln.eval()

    with torch.no_grad():
        x_test, _ = next(iter(train_loader))
        x1, x2 = x_test

        # BNç‰ˆæœ¬çš„ç‰¹å¾
        z1_bn = model_bn.projector(model_bn.encoder(x1.cuda()))
        std_bn = z1_bn.std(dim=0).mean().item()

        # LayerNormç‰ˆæœ¬çš„ç‰¹å¾
        z1_ln = model_ln.projector(model_ln.encoder(x1.cuda()))
        std_ln = z1_ln.std(dim=0).mean().item()

    print(f"\nç‰¹å¾æ ‡å‡†å·®ï¼ˆæ£€æµ‹åç¼©ï¼‰ï¼š")
    print(f"  BNç‰ˆæœ¬: {std_bn:.4f}")
    print(f"  LayerNormç‰ˆæœ¬: {std_ln:.4f}")
    print(f"  {'âš ï¸ LayerNormç‰ˆæœ¬åç¼©ï¼' if std_ln < 0.1 else 'âœ“ æœªåç¼©'}")

# è¿è¡Œæ¶ˆèå®éªŒ
ablation_study_bn()
```

**é¢„æœŸç»“æœ**ï¼š
- BNç‰ˆæœ¬ï¼šç¨³å®šè®­ç»ƒï¼ŒæŸå¤±æŒç»­ä¸‹é™ï¼Œç‰¹å¾æ ‡å‡†å·® â‰ˆ 1
- LayerNormç‰ˆæœ¬ï¼šå¯èƒ½å‡ºç°éƒ¨åˆ†åç¼©ï¼Œç‰¹å¾æ ‡å‡†å·® < 0.5

</div>

## 6. å·¥ç¨‹å®è·µä¸æœ€ä½³å®è·µ

### 6.1 è¶…å‚æ•°è°ƒä¼˜æŒ‡å—

<div class="practice-guide">

**æ ¸å¿ƒè¶…å‚æ•°**ï¼š

| å‚æ•° | æ¨èå€¼ | ä½œç”¨ | è°ƒä¼˜å»ºè®® |
|:---|:---|:---|:---|
| Batch Size | 256-512 | æä¾›è¶³å¤Ÿçš„BNç»Ÿè®¡ | è¶Šå¤§è¶Šå¥½ï¼ˆå—é™äºæ˜¾å­˜ï¼‰ |
| å­¦ä¹ ç‡ | 0.05 | æ§åˆ¶æ”¶æ•›é€Ÿåº¦ | Cosineè¡°å‡ |
| Predictoræ·±åº¦ | 2å±‚MLP | æ‰“ç ´å¯¹ç§°æ€§ | ä¸å®œè¿‡æ·±ï¼ˆ3å±‚å·²è¿‡ï¼‰ |
| ç‰¹å¾ç»´åº¦ | 2048 | è¡¨ç¤ºèƒ½åŠ› | ä¸backboneåŒ¹é… |
| æ•°æ®å¢å¼ºå¼ºåº¦ | å¼º | é˜²æ­¢ç®€å•è§£ | ColorJitter + Crop + Flip |
| è®­ç»ƒEpochs | 200-800 | å……åˆ†æ”¶æ•› | è¶Šé•¿è¶Šå¥½ |

**å…³é”®ç»éªŒ**ï¼š
1. **BNæ˜¯å¿…é¡»çš„**ï¼šå»æ‰BNå‡ ä¹100%åç¼©
2. **Predictorä¸èƒ½å¤ªæ·±**ï¼š2å±‚MLPæ˜¯sweet spotï¼Œ3å±‚åè€Œå˜å·®
3. **Stop-gradæ˜¯çµé­‚**ï¼šå°‘äº†å®ƒç«‹å³é€€åŒ–ä¸ºå¯¹ç§°ä¼˜åŒ–
4. **æ•°æ®å¢å¼ºè¦å¼º**ï¼šå¼±å¢å¼ºä¼šå¯¼è‡´æ¨¡å‹å­¦åˆ°ç®€å•æ˜ å°„

</div>

### 6.2 æ•…éšœæ’æŸ¥checklist

<div class="troubleshooting">

**é—®é¢˜1ï¼šè®­ç»ƒlossä¸ä¸‹é™ï¼ˆä¸€ç›´åœ¨1.0é™„è¿‘ï¼‰**
- **åŸå› **ï¼šç‰¹å¾å¯èƒ½å·²ç»åç¼©
- **è¯Šæ–­**ï¼šæ‰“å° `z.std(dim=0).mean()`ï¼Œå¦‚æœ < 0.1 åˆ™åç¼©
- **è§£å†³**ï¼š
  1. æ£€æŸ¥æ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†`.detach()`
  2. ç¡®è®¤BN layerså­˜åœ¨ä¸”æ­£å¸¸å·¥ä½œ
  3. å¢å¤§Batch Sizeï¼ˆè‡³å°‘256ï¼‰

**é—®é¢˜2ï¼šè®­ç»ƒä¸­é€”çªç„¶lossæ¿€å¢**
- **åŸå› **ï¼šPredictorå­¦ä¹ è¿‡å¿«ï¼Œç ´åäº†æ…¢æµå½¢
- **è§£å†³**ï¼š
  1. é™ä½å­¦ä¹ ç‡ï¼ˆ0.05 â†’ 0.03ï¼‰
  2. å¢å¤§weight decayï¼ˆ1e-4 â†’ 5e-4ï¼‰
  3. ä½¿ç”¨æ›´gentleçš„å­¦ä¹ ç‡è°ƒåº¦ï¼ˆCosineæ›´å¹³æ»‘ï¼‰

**é—®é¢˜3ï¼šä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å·®**
- **åŸå› **ï¼šè¡¨å¾ç¼ºä¹å¤šæ ·æ€§ï¼ˆç‰¹å¾å†—ä½™ï¼‰
- **è§£å†³**ï¼š
  1. å¢å¼ºæ•°æ®å¢å¼ºå¼ºåº¦
  2. å»¶é•¿è®­ç»ƒæ—¶é—´ï¼ˆ200 epoch â†’ 400 epochï¼‰
  3. è€ƒè™‘æ·»åŠ æ˜¾å¼å»ç›¸å…³é¡¹ï¼ˆå¦‚Barlow Twinsçš„åæ–¹å·®æ­£åˆ™åŒ–ï¼‰

</div>

### 6.3 ä¸å…¶ä»–è‡ªç›‘ç£æ–¹æ³•çš„é›†æˆ

<div class="integration-guide">

**SimSiam + MoCo**ï¼š
```python
# ç»“åˆé˜Ÿåˆ—æœºåˆ¶ï¼Œå¢åŠ éšå¼å¯¹æ¯”
class SimSiamMoCo(nn.Module):
    def __init__(self, encoder, dim=2048, K=65536):
        super().__init__()
        self.encoder_q = encoder
        self.encoder_k = copy.deepcopy(encoder)
        self.predictor = build_predictor(dim)

        # MoCoçš„é˜Ÿåˆ—
        self.register_buffer("queue", torch.randn(dim, K))
        self.queue = F.normalize(self.queue, dim=0)

    @torch.no_grad()
    def _momentum_update_key_encoder(self, m=0.999):
        for param_q, param_k in zip(
            self.encoder_q.parameters(), self.encoder_k.parameters()
        ):
            param_k.data = m * param_k.data + (1 - m) * param_q.data
```

**SimSiam + Barlow Twins**ï¼š
```python
# æ·»åŠ åæ–¹å·®æ­£åˆ™åŒ–
def simsiam_barlow_loss(p1, z2, p2, z1, lambda_cov=0.005):
    # SimSiaméƒ¨åˆ†
    loss_ss = 0.5 * D(p1, z2.detach()) + 0.5 * D(p2, z1.detach())

    # Barlow Twinséƒ¨åˆ†ï¼ˆå»ç›¸å…³ï¼‰
    z1_norm = (z1 - z1.mean(0)) / z1.std(0)
    z2_norm = (z2 - z2.mean(0)) / z2.std(0)
    C = (z1_norm.T @ z2_norm) / z1.size(0)

    # è®©äº’åæ–¹å·®çŸ©é˜µæ¥è¿‘å•ä½é˜µ
    loss_bt = (C.diagonal() - 1).pow(2).sum()
    loss_bt += C.pow(2).sum() - C.diagonal().pow(2).sum()

    return loss_ss + lambda_cov * loss_bt
```

</div>

### 6.4 æœªæ¥ç ”ç©¶æ–¹å‘

<div class="research-directions">

#### æ–¹å‘1ï¼šå¤§æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è‡ªç›‘ç£åç¼©

**èƒŒæ™¯**ï¼šNext-token prediction æœ¬è´¨ä¸Šæ˜¯å¸¦æ ‡ç­¾çš„ï¼Œä½†éšè—å±‚çš„è¡¨å¾æ˜¯å¦ä¼šå‘ç”Ÿå±€éƒ¨åç¼©ï¼Ÿ

**å…·ä½“é—®é¢˜**ï¼š
1. Transformerä¸­é—´å±‚æ˜¯å¦å­˜åœ¨"è¡¨å¾é€€åŒ–"ç°è±¡ï¼Ÿ
2. èƒ½å¦ç”¨SimSiamçš„å¿«æ…¢åŠ¨åŠ›å­¦è§£é‡ŠLayer Normalizationçš„ä½œç”¨ï¼Ÿ
3. è‡ªç›‘ç£é¢„è®­ç»ƒï¼ˆå¦‚BERTçš„MLMï¼‰æ˜¯å¦éšå¼åˆ©ç”¨äº†ç±»ä¼¼SimSiamçš„æœºåˆ¶ï¼Ÿ

**ç ”ç©¶å‡è®¾**ï¼š
- Dropoutåœ¨Transformerä¸­çš„ä½œç”¨ç±»ä¼¼äºBNåœ¨SimSiamä¸­çš„ä½œç”¨ï¼ˆé˜²æ­¢åç¼©ï¼‰
- å¤šå¤´æ³¨æ„åŠ›çš„ä¸åŒheadå¯èƒ½åœ¨ä¸åŒçš„"æ…¢æµå½¢"ä¸Šæ¼”åŒ–

#### æ–¹å‘2ï¼šæ— éœ€BNçš„åŠ¨åŠ›å­¦è§£è€¦

**åŠ¨æœº**ï¼šBNåœ¨batch sizeå°æˆ–åºåˆ—é•¿åº¦ä¸å‡æ—¶å¤±æ•ˆã€‚

**å€™é€‰æ–¹æ¡ˆ**ï¼š
1. **Adaptive Centering**ï¼šè‡ªé€‚åº”è°ƒæ•´ç‰¹å¾å‡å€¼
   \begin{equation}
   z_{\text{centered}} = z - \alpha \cdot \text{EMA}(\mathbb{E}[z]) \tag{19}
   \end{equation}

2. **Spectral Normalization + Implicit Regularization**ï¼š
   - ç”¨è°±å½’ä¸€åŒ–æ›¿ä»£BN
   - æ·»åŠ æ˜¾å¼æ–¹å·®çº¦æŸï¼š$\mathcal{L}_{\text{var}} = \max(0, 1 - \text{Var}(z))$

3. **Learnable Temperature Scaling**ï¼š
   \begin{equation}
   z_{\text{scaled}} = z / \tau, \quad \tau = \tau_0 \cdot e^{-t/T} \tag{20}
   \end{equation}

   å…¶ä¸­ $\tau$ éšè®­ç»ƒé€æ¸å‡å°ï¼ŒåˆæœŸå¼ºåˆ¶é«˜æ–¹å·®ï¼ŒåæœŸå…è®¸æ”¶æ•›ã€‚

#### æ–¹å‘3ï¼šSimSiamåœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†å»å™ªç½‘ç»œè§†ä¸º"Predictor"ï¼Œå™ªå£°æ ·æœ¬è§†ä¸º"Target"ã€‚

**æ¶æ„è®¾è®¡**ï¼š
```python
class DiffusionSimSiam(nn.Module):
    def __init__(self, denoiser):
        super().__init__()
        self.denoiser = denoiser  # U-Netç­‰
        self.predictor = small_mlp()  # å¿«é€Ÿé€‚é…å™¨

    def forward(self, x_noisy, t):
        # Denoiseré¢„æµ‹å¹²å‡€å›¾åƒ
        x_pred = self.denoiser(x_noisy, t)

        # Predictorå¿«é€Ÿå­¦ä¹ æ®‹å·®
        residual = self.predictor(x_pred)

        # Stop-gradientåº”ç”¨äºx_noisy
        loss = mse(x_pred + residual, x_noisy.detach())
        return loss
```

**é¢„æœŸä¼˜åŠ¿**ï¼š
- åŠ é€Ÿæ‰©æ•£æ¨¡å‹è®­ç»ƒï¼ˆPredictorå¿«é€Ÿæ•æ‰ä½é¢‘ä¿¡æ¯ï¼‰
- æå‡ç”Ÿæˆè´¨é‡ï¼ˆæ…¢æµå½¢çº¦æŸé˜²æ­¢mode collapseï¼‰

#### æ–¹å‘4ï¼šç†è®ºç»Ÿä¸€ï¼šSimSiamä½œä¸ºéšå¼ä¼˜åŒ–çš„ä¸€èˆ¬æ¡†æ¶

**å¤§èƒ†çŒœæƒ³**ï¼šæ‰€æœ‰æˆåŠŸçš„è‡ªç›‘ç£æ–¹æ³•éƒ½å¯ä»¥è§£é‡Šä¸ºæŸç§"å¿«æ…¢åŠ¨åŠ›å­¦"ã€‚

| æ–¹æ³• | "æ…¢" ç»„ä»¶ | "å¿«" ç»„ä»¶ | è§£è€¦æœºåˆ¶ |
|:---|:---|:---|:---|
| SimSiam | Encoder | Predictor | Stop-grad |
| BYOL | Online Net | Target Net (EMA) | EMAæ›´æ–° |
| MoCo | Query Encoder | Key Encoder (é˜Ÿåˆ—) | åŠ¨é‡+é˜Ÿåˆ— |
| DINO | Student | Teacher (EMA+Centering) | EMA+Temperature |

**ç†è®ºç›®æ ‡**ï¼šå»ºç«‹ç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œç”¨å¥‡å¼‚æ‘„åŠ¨ç†è®ºï¼ˆSingular Perturbation Theoryï¼‰æè¿°æ‰€æœ‰è‡ªç›‘ç£å­¦ä¹ ã€‚

**æ ¸å¿ƒæ–¹ç¨‹**ï¼š
\begin{align}
\dot{\boldsymbol{\theta}}_{\text{slow}} &= -\nabla_{\boldsymbol{\theta}_{\text{slow}}} L(\boldsymbol{\theta}_{\text{slow}}, \boldsymbol{\theta}_{\text{fast}}) \tag{21a}\\
\epsilon \dot{\boldsymbol{\theta}}_{\text{fast}} &= -\nabla_{\boldsymbol{\theta}_{\text{fast}}} L(\boldsymbol{\theta}_{\text{slow}}, \boldsymbol{\theta}_{\text{fast}}) \tag{21b}
\end{align}

å…¶ä¸­ $\epsilon \ll 1$ã€‚

</div>

---

## 7. å“²å­¦æ€è¾¨ä¸æ€»ç»“

<div class="philosophy-box">

### ğŸŒŒ å¯¹ç§°æ€§ä¸å¯¹ç§°æ€§ç ´ç¼ºçš„è¾©è¯æ³•

SimSiamçš„æˆåŠŸæ­ç¤ºäº†æ·±åº¦å­¦ä¹ ä¸­ä¸€ä¸ªæ·±åˆ»çš„å“²å­¦é—®é¢˜ï¼š

**å‘½é¢˜**ï¼šå¯¹ç§°æ€§æ˜¯ä¼˜åŒ–çš„åŠ¨åŠ›ï¼Œå¯¹ç§°æ€§ç ´ç¼ºæ˜¯è¿›åŒ–çš„å¥‘æœºã€‚

**å¯¹ç§°æ€§ï¼ˆSymmetryï¼‰**ï¼š
- Siameseæ¶æ„å¤©ç„¶å¯¹ç§°ï¼š$f(x_1) \approx f(x_2)$
- å¯¹ç§°æ€§ç®€åŒ–é—®é¢˜ï¼šå‡å°‘æœç´¢ç©ºé—´
- ä½†å®Œå…¨å¯¹ç§°å¯¼è‡´åç¼©ï¼šæ‰€æœ‰è§£ç­‰ä»·â†’é€‰æ‹©å¹³å‡¡è§£

**å¯¹ç§°æ€§ç ´ç¼ºï¼ˆSymmetry Breakingï¼‰**ï¼š
- Stop-gradientæ‰“ç ´æ—¶é—´åæ¼”å¯¹ç§°æ€§
- Predictorå¼•å…¥ç»“æ„ä¸å¯¹ç§°æ€§
- BNå¼•å…¥batchç»´åº¦çš„è€¦åˆï¼ˆç©ºé—´å¯¹ç§°æ€§ç ´ç¼ºï¼‰

**ç±»æ¯”ç‰©ç†å­¦**ï¼š
- é“ç£ç›¸å˜ï¼šé«˜æ¸©ä¸‹è‡ªæ—‹å¯¹ç§°ï¼Œä½æ¸©ä¸‹è‡ªå‘ç£åŒ–
- Higgsæœºåˆ¶ï¼šè§„èŒƒå¯¹ç§°æ€§è‡ªå‘ç ´ç¼ºï¼Œç²’å­è·å¾—è´¨é‡
- **SimSiam**ï¼šå‚æ•°ç©ºé—´çš„"å‡èš"è¿‡ç¨‹ï¼Œä»é«˜å¯¹ç§°æ€â†’ä½å¯¹ç§°æ€ï¼ˆä½†ä¿æŒè¡¨ç¤ºå¤šæ ·æ€§ï¼‰

</div>

<div class="summary-box">

### ğŸ¯ æ ¸å¿ƒæ´å¯Ÿå›é¡¾

**ä¸‰å¤§æ”¯æŸ±**ï¼š
1. **Stop-gradient**ï¼šæ‰“ç ´æ¢¯åº¦æµçš„å¯¹ç§°æ€§ï¼Œåˆ›é€ å¿«æ…¢æ—¶é—´å°ºåº¦
2. **Predictor**ï¼šå¿«é€Ÿé€‚é…å™¨ï¼Œåœ¨encoderåç¼©å‰"æˆªèƒ¡"
3. **Batch Normalization**ï¼šéšå¼æä¾›æ–¹å·®çº¦æŸå’Œbatchå†…å¯¹æ¯”

**æ•°å­¦æœ¬è´¨**ï¼š
\begin{equation}
\text{SimSiam} = \text{Slow-Fast Dynamics} + \text{Implicit Regularization} \tag{22}
\end{equation}

**å·¥ç¨‹å¯ç¤º**ï¼š
- ç®€å• â‰  ä½æ•ˆï¼ˆSimSiamåªæœ‰3ä¸ªç»„ä»¶ï¼Œå´è¾¾åˆ°SOTAï¼‰
- å¯¹ç§°æ€§ç ´ç¼ºæ¯”æ˜¾å¼çº¦æŸæ›´ä¼˜é›…
- åŠ¨åŠ›å­¦è§†è§’èƒ½è§£é‡Šå¾ˆå¤š"ç„å­¦"

</div>

<div class="poetic-ending">

### ğŸ”š ç»ˆç« ï¼šæ•°å­¦çš„å¼ åŠ›ä¹‹ç¾

åœ¨æ— ç›‘ç£å­¦ä¹ çš„è’é‡ä¸­ï¼Œåç¼©æ˜¯å¼•åŠ›ï¼Œæ˜¯ç†µå¢çš„å®¿å‘½ã€‚

SimSiamå‘Šè¯‰æˆ‘ä»¬ï¼š**ä¸éœ€è¦ä¸å¼•åŠ›å¯¹æŠ—ï¼ˆè´Ÿæ ·æœ¬ï¼‰ï¼Œåªéœ€è¦åˆ©ç”¨æ—¶é—´çš„ä¸å¯¹ç§°æ€§ã€‚**

å½“Predictorä»¥å…‰é€Ÿè¿½èµ¶Encoderçš„è„šæ­¥æ—¶ï¼Œ
å®ƒåœ¨åå¡Œçš„è¾¹ç¼˜å»ºç«‹äº†ä¸€åº§åŠ¨æ€å¹³è¡¡çš„æ¡¥æ¢ã€‚

è¿™åº§æ¡¥ä¸æ˜¯ç”¨çŸ³å¤´ç Œæˆçš„ï¼ˆæ˜¾å¼çº¦æŸï¼‰ï¼Œ
è€Œæ˜¯ç”¨æ•°å­¦çš„å¼ åŠ›ç¼–ç»‡è€Œæˆçš„ï¼ˆå¿«æ…¢åŠ¨åŠ›å­¦ï¼‰ã€‚

æ„¿ä½ çš„è¡¨å¾æ°¸è¿œä¿æŒå¤šæ ·ï¼Œ
æ„¿ä½ çš„ä¼˜åŒ–æ°¸è¿œè¡Œèµ°åœ¨å¯¹ç§°æ€§ç ´ç¼ºçš„é”‹åˆƒä¸Šã€‚

</div>

---

**å‚è€ƒæ–‡çŒ®**ï¼ˆç²¾é€‰ï¼‰ï¼š
1. Chen, X., & He, K. (2021). "Exploring Simple Siamese Representation Learning." *CVPR*.
2. Grill, J.B., et al. (2020). "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning." *NeurIPS*.
3. Richemond, P.H., et al. (2021). "Implicit Bias of Batch Normalization in Self-Supervised Learning." *ICML Workshop*.
4. Tian, Y., et al. (2022). "Understanding Self-supervised Learning Dynamics without Contrastive Pairs." *ICML*.
5. Zbontar, J., et al. (2021). "Barlow Twins: Self-Supervised Learning via Redundancy Reduction." *ICML*.

---

**é™„å½•ï¼šå…¬å¼é€ŸæŸ¥è¡¨**

| ç¼–å· | å…¬å¼ | å«ä¹‰ |
|:---|:---|:---|
| (8) | $L = \mathbb{E}[\|h_{\varphi}(f_{\theta}(x_1)) - f_{\theta}(x_2)\|^2]$ | SimSiamæŸå¤±å‡½æ•° |
| (9a) | $h_{\varphi}(z) = z$ | ç¨³å®šå¹³è¡¡ç‚¹æ¡ä»¶ |
| (15) | $L_{\text{BN}} = L_{\text{SimSiam}} + \frac{\lambda}{B}\sum_{i\neq j}\langle z_i, z_j\rangle$ | BNéšå¼å¯¹æ¯” |
| (16) | $\dot{\boldsymbol{\theta}} = -\nabla_{\boldsymbol{\theta}} L, \quad \epsilon\dot{\boldsymbol{\varphi}} = -\nabla_{\boldsymbol{\varphi}} L$ | å¿«æ…¢ç³»ç»Ÿ |
| (17) | $\nabla_{\boldsymbol{\varphi}} L = 0 \Rightarrow h_{\boldsymbol{\varphi}} \approx I$ | å¿«å˜å¹³è¡¡ç‚¹ |

---