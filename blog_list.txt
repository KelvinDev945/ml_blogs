# Blog List - 博客列表
# Format: title | slug | path | status | date | tags
# Status: pending (未开始) | in_progress (进行中) | completed (已完成)

n个正态随机数的最大值的渐近估计 | n个正态随机数的最大值的渐近估计 | blogs_raw/n个正态随机数的最大值的渐近估计.md | pending | 2025-11-06 | 数学
流形上的最速下降：5. 对偶梯度下降 | 流形上的最速下降5-对偶梯度下降 | blogs_raw/流形上的最速下降5-对偶梯度下降.md | pending | 2025-11-03 | 机器学习
低精度Attention可能存在有偏的舍入误差 | 低精度attention可能存在有偏的舍入误差 | blogs_raw/低精度attention可能存在有偏的舍入误差.md | pending | 2025-10-27 | 机器学习
MuP之上：1. 好模型的三个特征 | mup之上1-好模型的三个特征 | blogs_raw/mup之上1-好模型的三个特征.md | pending | 2025-10-21 | 优化
随机矩阵的谱范数的快速估计 | 随机矩阵的谱范数的快速估计 | blogs_raw/随机矩阵的谱范数的快速估计.md | pending | 2025-10-12 | 机器学习
DiVeQ：一种非常简洁的VQ训练方案 | diveq一种非常简洁的vq训练方案 | blogs_raw/diveq一种非常简洁的vq训练方案.md | pending | 2025-10-08 | 机器学习
为什么线性注意力要加Short Conv？ | 为什么线性注意力要加short-conv | blogs_raw/为什么线性注意力要加short-conv.md | pending | 2025-10-05 | 机器学习
AdamW的Weight RMS的渐近估计 | adamw的weight-rms的渐近估计 | blogs_raw/adamw的weight-rms的渐近估计.md | pending | 2025-10-01 | 优化
重新思考学习率与Batch Size（四）：EMA | 重新思考学习率与batch-size四ema | blogs_raw/重新思考学习率与batch-size四ema.md | pending | 2025-09-22 | 优化
重新思考学习率与Batch Size（三）：Muon | 重新思考学习率与batch-size三muon | blogs_raw/重新思考学习率与batch-size三muon.md | pending | 2025-09-15 | 优化
n个正态随机数的最大值的渐近估计 | n个正态随机数的最大值的渐近估计-1 | blogs_raw/n个正态随机数的最大值的渐近估计-1.md | pending | 2025-11-06 | 数学
流形上的最速下降：5. 对偶梯度下降 | 流形上的最速下降5-对偶梯度下降-1 | blogs_raw/流形上的最速下降5-对偶梯度下降-1.md | pending | 2025-11-03 | 机器学习
低精度Attention可能存在有偏的舍入误差 | 低精度attention可能存在有偏的舍入误差-1 | blogs_raw/低精度attention可能存在有偏的舍入误差-1.md | pending | 2025-10-27 | 机器学习
MuP之上：1. 好模型的三个特征 | mup之上1-好模型的三个特征-1 | blogs_raw/mup之上1-好模型的三个特征-1.md | pending | 2025-10-21 | 优化
随机矩阵的谱范数的快速估计 | 随机矩阵的谱范数的快速估计-1 | blogs_raw/随机矩阵的谱范数的快速估计-1.md | pending | 2025-10-12 | 机器学习
DiVeQ：一种非常简洁的VQ训练方案 | diveq一种非常简洁的vq训练方案-1 | blogs_raw/diveq一种非常简洁的vq训练方案-1.md | pending | 2025-10-08 | 机器学习
为什么线性注意力要加Short Conv？ | 为什么线性注意力要加short-conv-1 | blogs_raw/为什么线性注意力要加short-conv-1.md | pending | 2025-10-05 | 机器学习
AdamW的Weight RMS的渐近估计 | adamw的weight-rms的渐近估计-1 | blogs_raw/adamw的weight-rms的渐近估计-1.md | pending | 2025-10-01 | 优化
重新思考学习率与Batch Size（四）：EMA | 重新思考学习率与batch-size四ema-1 | blogs_raw/重新思考学习率与batch-size四ema-1.md | pending | 2025-09-22 | 优化
重新思考学习率与Batch Size（三）：Muon | 重新思考学习率与batch-size三muon-1 | blogs_raw/重新思考学习率与batch-size三muon-1.md | pending | 2025-09-15 | 优化
低精度Attention可能存在有... | 低精度attention可能存在有 | blogs_raw/低精度attention可能存在有.md | pending |  | 近似,分析,优化,attention,生成模型
为什么线性注意力要加Short C... | 为什么线性注意力要加short-c | blogs_raw/为什么线性注意力要加short-c.md | pending |  | 线性,RNN,生成模型,attention,生成模型
AdamW的Weight RMS的... | adamw的weight-rms的 | blogs_raw/adamw的weight-rms的.md | pending |  | 估计,梯度,优化器,平均场,生成模型
重新思考学习率与Batch Siz... | 重新思考学习率与batch-siz | blogs_raw/重新思考学习率与batch-siz.md | pending |  | 学习率,优化器,尺度定律,平均场,生成模型
苏剑林: 我的pretrain的小模型，暂时没有链接。 | 苏剑林-我的pretrain的小模型暂时没有链接 | blogs_raw/苏剑林-我的pretrain的小模型暂时没有链接.md | pending |  | 语言模型,attention,位置编码,rope,生成模型
苏剑林: 就是反向构造出来的。 | 苏剑林-就是反向构造出来的 | blogs_raw/苏剑林-就是反向构造出来的.md | pending |  | SSM,SSM,SSM,线性,RNN
个性邮箱 | 个性邮箱 | blogs_raw/个性邮箱.md | pending |  | 邮箱,生成模型,attention,优化,语言模型
观测ISS | 观测iss | blogs_raw/观测iss.md | pending |  | 国际空间站,ISS,观测,生成模型,attention
重新思考学习率与Batch Size（二）：平均场 | 重新思考学习率与batch-size二平均场 | blogs_raw/重新思考学习率与batch-size二平均场.md | pending |  | 学习率,优化器,尺度定律,平均场,生成模型
为什么Adam的Update RMS是0.2？ | 为什么adam的update-rms是02 | blogs_raw/为什么adam的update-rms是02.md | pending |  | 分析,梯度,优化器,平均场,生成模型
重新思考学习率与Batch Size（一）：现状 | 重新思考学习率与batch-size一现状 | blogs_raw/重新思考学习率与batch-size一现状.md | pending |  | 梯度,学习率,优化器,尺度定律,生成模型
Cool Papers更新：简单适配Zotero Connector | cool-papers更新简单适配zotero-connector | blogs_raw/cool-papers更新简单适配zotero-connector.md | pending |  | 网站,论文,酷论文,生成模型,attention
流形上的最速下降：4. Muon + 谱球面 | 流形上的最速下降4-muon-谱球面 | blogs_raw/流形上的最速下降4-muon-谱球面.md | pending |  | 矩阵,优化器,muon,约束,最速下降
ReLU/GeLU/Swish的一个恒等式 | relugeluswish的一个恒等式 | blogs_raw/relugeluswish的一个恒等式.md | pending |  | 分析,神经网络,恒等式,生成模型,attention
流形上的最速下降：3. Muon + Stiefel | 流形上的最速下降3-muon-stiefel | blogs_raw/流形上的最速下降3-muon-stiefel.md | pending |  | 矩阵,优化器,muon,约束,最速下降
流形上的最速下降：2. Muon + 正交 | 流形上的最速下降2-muon-正交 | blogs_raw/流形上的最速下降2-muon-正交.md | pending |  | 矩阵,优化器,muon,约束,最速下降
基于树莓派Zero2W搭建一个随身旁路由 | 基于树莓派zero2w搭建一个随身旁路由 | blogs_raw/基于树莓派zero2w搭建一个随身旁路由.md | pending |  | linux,网络,路由器,智能家居,生成模型
流形上的最速下降：1.  SGD + 超球面 | 流形上的最速下降1-sgd-超球面 | blogs_raw/流形上的最速下降1-sgd-超球面.md | pending |  | 不等式,优化器,约束,最速下降,生成模型
矩阵r次方根和逆r次方根的高效计算 | 矩阵r次方根和逆r次方根的高效计算 | blogs_raw/矩阵r次方根和逆r次方根的高效计算.md | pending |  | 代数,迭代,矩阵,线性,生成模型
矩阵平方根和逆平方根的高效计算 | 矩阵平方根和逆平方根的高效计算 | blogs_raw/矩阵平方根和逆平方根的高效计算.md | pending |  | 代数,迭代,矩阵,线性,生成模型
QK-Clip：让Muon在Scaleup之路上更进一步 | qk-clip让muon在scaleup之路上更进一步 | blogs_raw/qk-clip让muon在scaleup之路上更进一步.md | pending |  | 优化,attention,优化器,muon,生成模型
Transformer升级之路：21、MLA好在哪里?（下） | transformer升级之路21mla好在哪里下 | blogs_raw/transformer升级之路21mla好在哪里下.md | pending |  | 优化,语言模型,生成模型,attention,生成模型
“对角+低秩”三角阵的高效求逆方法 | 对角低秩三角阵的高效求逆方法 | blogs_raw/对角低秩三角阵的高效求逆方法.md | pending |  | 计算,矩阵,RNN,attention,生成模型
通过msign来计算奇异值裁剪mclip（下） | 通过msign来计算奇异值裁剪mclip下 | blogs_raw/通过msign来计算奇异值裁剪mclip下.md | pending |  | 迭代,近似,矩阵,SVD,muon
矩阵符号函数mcsgn能计算什么？ | 矩阵符号函数mcsgn能计算什么 | blogs_raw/矩阵符号函数mcsgn能计算什么.md | pending |  | 代数,矩阵,线性,生成模型,attention
msign的导数 | msign的导数 | blogs_raw/msign的导数.md | pending |  | 微积分,矩阵,梯度,muon,生成模型
通过msign来计算奇异值裁剪mclip（上） | 通过msign来计算奇异值裁剪mclip上 | blogs_raw/通过msign来计算奇异值裁剪mclip上.md | pending |  | 迭代,近似,矩阵,SVD,muon
msign算子的Newton-Schulz迭代（下） | msign算子的newton-schulz迭代下 | blogs_raw/msign算子的newton-schulz迭代下.md | pending |  | 迭代,近似,优化器,muon,生成模型
等值振荡定理：最优多项式逼近的充要条件 | 等值振荡定理最优多项式逼近的充要条件 | blogs_raw/等值振荡定理最优多项式逼近的充要条件.md | pending |  | 导数,近似,最优,分析,生成模型
生成扩散模型漫谈（三十）：从瞬时速度到平均速度 | 生成扩散模型漫谈三十从瞬时速度到平均速度 | blogs_raw/生成扩散模型漫谈三十从瞬时速度到平均速度.md | pending |  | 微分方程,生成模型,采样,扩散,生成模型
MoE环游记：5、均匀分布的反思 | moe环游记5均匀分布的反思 | blogs_raw/moe环游记5均匀分布的反思.md | pending |  | 优化,稀疏,moe,生成模型,attention
msign算子的Newton-Schulz迭代（上） | msign算子的newton-schulz迭代上 | blogs_raw/msign算子的newton-schulz迭代上.md | pending |  | 迭代,近似,优化器,muon,生成模型
Transformer升级之路：20、MLA好在哪里?（上） | transformer升级之路20mla好在哪里上 | blogs_raw/transformer升级之路20mla好在哪里上.md | pending |  | 优化,语言模型,生成模型,attention,生成模型
一道概率不等式：盯着它到显然成立为止！ | 一道概率不等式盯着它到显然成立为止 | blogs_raw/一道概率不等式盯着它到显然成立为止.md | pending |  | 不等式,概率,显然成立,生成模型,attention
SVD的导数 | svd的导数 | blogs_raw/svd的导数.md | pending |  | 微积分,分析,矩阵,SVD,梯度
智能家居之手搓一套能接入米家的零冷水装置 | 智能家居之手搓一套能接入米家的零冷水装置 | blogs_raw/智能家居之手搓一套能接入米家的零冷水装置.md | pending |  | 生活,智能家居,生成模型,attention,优化
矩阵的有效秩（Effective Rank） | 矩阵的有效秩effective-rank | blogs_raw/矩阵的有效秩effective-rank.md | pending |  | 矩阵,熵,稀疏,低秩,生成模型
通过梯度近似寻找Normalization的替代品 | 通过梯度近似寻找normalization的替代品 | blogs_raw/通过梯度近似寻找normalization的替代品.md | pending |  | 函数,分析,梯度,光滑,生成模型
MoE环游记：4、难处应当多投入 | moe环游记4难处应当多投入 | blogs_raw/moe环游记4难处应当多投入.md | pending |  | 优化,梯度,moe,动态,生成模型
高阶MuP：更简明但更高明的谱条件缩放 | 高阶mup更简明但更高明的谱条件缩放 | blogs_raw/高阶mup更简明但更高明的谱条件缩放.md | pending |  | LoRA,梯度,优化器,尺度定律,谱范数
初探MuP：超参数的跨模型尺度迁移规律 | 初探mup超参数的跨模型尺度迁移规律 | blogs_raw/初探mup超参数的跨模型尺度迁移规律.md | pending |  | 梯度,学习率,优化器,尺度定律,生成模型
MoE环游记：3、换个思路来分配 | moe环游记3换个思路来分配 | blogs_raw/moe环游记3换个思路来分配.md | pending |  | 最优,损失函数,梯度,moe,生成模型
Muon续集：为什么我们选择尝试Muon？ | muon续集为什么我们选择尝试muon | blogs_raw/muon续集为什么我们选择尝试muon.md | pending |  | 矩阵,梯度,优化器,谱范数,muon
MoE环游记：2、不患寡而患不均 | moe环游记2不患寡而患不均 | blogs_raw/moe环游记2不患寡而患不均.md | pending |  | 损失函数,梯度,稀疏,moe,生成模型
生成扩散模型漫谈（二十九）：用DDPM来离散编码 | 生成扩散模型漫谈二十九用ddpm来离散编码 | blogs_raw/生成扩散模型漫谈二十九用ddpm来离散编码.md | pending |  | 生成模型,编码,DDPM,扩散,离散化
MoE环游记：1、从几何意义出发 | moe环游记1从几何意义出发 | blogs_raw/moe环游记1从几何意义出发.md | pending |  | 模型,几何,稀疏,moe,生成模型
三个球的交点坐标（三球交会定位） | 三个球的交点坐标三球交会定位 | blogs_raw/三个球的交点坐标三球交会定位.md | pending |  | 方程,几何,生成模型,attention,优化
细水长flow之TARFLOW：流模型满血归来？ | 细水长flow之tarflow流模型满血归来 | blogs_raw/细水长flow之tarflow流模型满血归来.md | pending |  | 流模型,flow,生成模型,attention,生成模型
低秩近似之路（五）：CUR | 低秩近似之路五cur | blogs_raw/低秩近似之路五cur.md | pending |  | 近似,最优,矩阵,低秩,生成模型
为什么梯度裁剪的默认模长是1？ | 为什么梯度裁剪的默认模长是1 | blogs_raw/为什么梯度裁剪的默认模长是1.md | pending |  | 优化,梯度,学习率,优化器,生成模型
从谱范数梯度到新式权重衰减的思考 | 从谱范数梯度到新式权重衰减的思考 | blogs_raw/从谱范数梯度到新式权重衰减的思考.md | pending |  | 矩阵,优化,梯度,优化器,谱范数
生成扩散模型漫谈（二十八）：分步理解一致性模型 | 生成扩散模型漫谈二十八分步理解一致性模型 | blogs_raw/生成扩散模型漫谈二十八分步理解一致性模型.md | pending |  | 微分方程,生成模型,采样,扩散,生成模型
生成扩散模型漫谈（二十七）：将步长作为条件输入 | 生成扩散模型漫谈二十七将步长作为条件输入 | blogs_raw/生成扩散模型漫谈二十七将步长作为条件输入.md | pending |  | 微分方程,生成模型,采样,扩散,生成模型
Muon优化器赏析：从向量到矩阵的本质跨越 | muon优化器赏析从向量到矩阵的本质跨越 | blogs_raw/muon优化器赏析从向量到矩阵的本质跨越.md | pending |  | 矩阵,梯度,优化器,谱范数,muon
从Hessian近似看自适应学习率优化器 | 从hessian近似看自适应学习率优化器 | blogs_raw/从hessian近似看自适应学习率优化器.md | pending |  | 优化,梯度,学习率,优化器,生成模型
生成扩散模型漫谈（二十六）：基于恒等式的蒸馏（下） | 生成扩散模型漫谈二十六基于恒等式的蒸馏下 | blogs_raw/生成扩散模型漫谈二十六基于恒等式的蒸馏下.md | pending |  | 生成模型,梯度,扩散,去噪,生成模型
Adam的epsilon如何影响学习率的Scaling Law？ | adam的epsilon如何影响学习率的scaling-law | blogs_raw/adam的epsilon如何影响学习率的scaling-law.md | pending |  | 梯度,学习率,优化器,尺度定律,生成模型
当Batch Size增大时，学习率该如何随之变化？ | 当batch-size增大时学习率该如何随之变化 | blogs_raw/当batch-size增大时学习率该如何随之变化.md | pending |  | 梯度,学习率,优化器,尺度定律,生成模型
VQ的又一技巧：给编码表加一个线性变换 | vq的又一技巧给编码表加一个线性变换 | blogs_raw/vq的又一技巧给编码表加一个线性变换.md | pending |  | 生成模型,编码,梯度,离散化,生成模型
低秩近似之路（四）：ID | 低秩近似之路四id | blogs_raw/低秩近似之路四id.md | pending |  | 近似,最优,矩阵,低秩,生成模型
VQ的旋转技巧：梯度直通估计的一般推广 | vq的旋转技巧梯度直通估计的一般推广 | blogs_raw/vq的旋转技巧梯度直通估计的一般推广.md | pending |  | 生成模型,编码,梯度,离散化,生成模型
Cool Papers浏览器扩展升级至v0.2.0 | cool-papers浏览器扩展升级至v020 | blogs_raw/cool-papers浏览器扩展升级至v020.md | pending |  | 网站,论文,酷论文,生成模型,attention
让MathJax的数学公式随窗口大小自动缩放 | 让mathjax的数学公式随窗口大小自动缩放 | blogs_raw/让mathjax的数学公式随窗口大小自动缩放.md | pending |  | 网站,latex,生成模型,attention,优化
低秩近似之路（三）：CR | 低秩近似之路三cr | blogs_raw/低秩近似之路三cr.md | pending |  | 近似,最优,矩阵,低秩,生成模型
低秩近似之路（二）：SVD | 低秩近似之路二svd | blogs_raw/低秩近似之路二svd.md | pending |  | 近似,最优,矩阵,SVD,低秩
利用“熄火保护 + 通断器”实现燃气灶智能关火 | 利用熄火保护-通断器实现燃气灶智能关火 | blogs_raw/利用熄火保护-通断器实现燃气灶智能关火.md | pending |  | 生活,智能家居,米家,生成模型,attention
Softmax后传：寻找Top-K的光滑近似 | softmax后传寻找top-k的光滑近似 | blogs_raw/softmax后传寻找top-k的光滑近似.md | pending |  | 概率,近似,梯度,光滑,生成模型
低秩近似之路（一）：伪逆 | 低秩近似之路一伪逆 | blogs_raw/低秩近似之路一伪逆.md | pending |  | 近似,矩阵,低秩,生成模型,attention
“闭门造车”之多模态思路浅谈（三）：位置编码 | 闭门造车之多模态思路浅谈三位置编码 | blogs_raw/闭门造车之多模态思路浅谈三位置编码.md | pending |  | attention,位置编码,多模态,生成模型,attention
Decoder-only的LLM为什么需要位置编码？ | decoder-only的llm为什么需要位置编码 | blogs_raw/decoder-only的llm为什么需要位置编码.md | pending |  | 语言模型,attention,位置编码,生成模型,attention
近乎完美地解决MathJax与Marked的冲突 | 近乎完美地解决mathjax与marked的冲突 | blogs_raw/近乎完美地解决mathjax与marked的冲突.md | pending |  | 网站,latex,论文,酷论文,生成模型
让MathJax更好地兼容谷歌翻译和延时加载 | 让mathjax更好地兼容谷歌翻译和延时加载 | blogs_raw/让mathjax更好地兼容谷歌翻译和延时加载.md | pending |  | 网站,latex,论文,酷论文,生成模型
“Cool Papers + 站内搜索”的一些新尝试 | cool-papers-站内搜索的一些新尝试 | blogs_raw/cool-papers-站内搜索的一些新尝试.md | pending |  | 网站,论文,酷论文,生成模型,attention
通向最优分布之路：概率空间的最小化 | 通向最优分布之路概率空间的最小化 | blogs_raw/通向最优分布之路概率空间的最小化.md | pending |  | 概率,优化,梯度,扩散,生成模型
对齐全量微调！这是我看过最精彩的LoRA改进（二） | 对齐全量微调这是我看过最精彩的lora改进二 | blogs_raw/对齐全量微调这是我看过最精彩的lora改进二.md | pending |  | 梯度,优化器,低秩,lora,生成模型
Monarch矩阵：计算高效的稀疏型矩阵分解 | monarch矩阵计算高效的稀疏型矩阵分解 | blogs_raw/monarch矩阵计算高效的稀疏型矩阵分解.md | pending |  | 矩阵,语言模型,稀疏,低秩,生成模型
【生活杂记】用电饭锅来煮米汤 | 生活杂记用电饭锅来煮米汤 | blogs_raw/生活杂记用电饭锅来煮米汤.md | pending |  | 生活,情感,怀念,生成模型,attention
对齐全量微调！这是我看过最精彩的LoRA改进（一） | 对齐全量微调这是我看过最精彩的lora改进一 | blogs_raw/对齐全量微调这是我看过最精彩的lora改进一.md | pending |  | 梯度,优化器,低秩,lora,生成模型
“闭门造车”之多模态思路浅谈（二）：自回归 | 闭门造车之多模态思路浅谈二自回归 | blogs_raw/闭门造车之多模态思路浅谈二自回归.md | pending |  | 生成模型,扩散,多模态,自回归,生成模型
重温SSM（四）：有理生成函数的新视角 | 重温ssm四有理生成函数的新视角 | blogs_raw/重温ssm四有理生成函数的新视角.md | pending |  | 生成函数,线性,RNN,ssm,生成模型
重温SSM（三）：HiPPO的高效计算（S4） | 重温ssm三hippo的高效计算s4 | blogs_raw/重温ssm三hippo的高效计算s4.md | pending |  | 矩阵,线性,RNN,ssm,生成模型
通向概率分布之路：盘点Softmax及其替代品 | 通向概率分布之路盘点softmax及其替代品 | blogs_raw/通向概率分布之路盘点softmax及其替代品.md | pending |  | 概率,分析,损失函数,梯度,生成模型
重温SSM（二）：HiPPO的一些遗留问题 | 重温ssm二hippo的一些遗留问题 | blogs_raw/重温ssm二hippo的一些遗留问题.md | pending |  | 线性,差分,RNN,梯度,ssm
Transformer升级之路：18、RoPE的底数选择原则 | transformer升级之路18rope的底数选择原则 | blogs_raw/transformer升级之路18rope的底数选择原则.md | pending |  | 不等式,attention,位置编码,rope,生成模型
重温SSM（一）：线性系统和HiPPO矩阵 | 重温ssm一线性系统和hippo矩阵 | blogs_raw/重温ssm一线性系统和hippo矩阵.md | pending |  | 微分方程,线性,RNN,ssm,生成模型
缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA | 缓存与效果的极限拉扯从mhamqagqa到mla | blogs_raw/缓存与效果的极限拉扯从mhamqagqa到mla.md | pending |  | 优化,语言模型,生成模型,attention,生成模型
Cool Papers更新：简单搭建了一个站内检索系统 | cool-papers更新简单搭建了一个站内检索系统 | blogs_raw/cool-papers更新简单搭建了一个站内检索系统.md | pending |  | 网站,论文,酷论文,生成模型,attention
生成扩散模型漫谈（二十五）：基于恒等式的蒸馏（上） | 生成扩散模型漫谈二十五基于恒等式的蒸馏上 | blogs_raw/生成扩散模型漫谈二十五基于恒等式的蒸馏上.md | pending |  | 生成模型,梯度,扩散,去噪,生成模型
生成扩散模型漫谈（二十四）：少走捷径，更快到达 | 生成扩散模型漫谈二十四少走捷径更快到达 | blogs_raw/生成扩散模型漫谈二十四少走捷径更快到达.md | pending |  | 微分方程,生成模型,扩散,生成模型,attention
生成扩散模型漫谈（二十三）：信噪比与大图生成（下） | 生成扩散模型漫谈二十三信噪比与大图生成下 | blogs_raw/生成扩散模型漫谈二十三信噪比与大图生成下.md | pending |  | 无监督,生成模型,扩散,信噪比,生成模型
生成扩散模型漫谈（二十二）：信噪比与大图生成（上） | 生成扩散模型漫谈二十二信噪比与大图生成上 | blogs_raw/生成扩散模型漫谈二十二信噪比与大图生成上.md | pending |  | 损失函数,生成模型,扩散,信噪比,生成模型
Transformer升级之路：17、多模态位置编码的简单思考 | transformer升级之路17多模态位置编码的简单思考 | blogs_raw/transformer升级之路17多模态位置编码的简单思考.md | pending |  | attention,位置编码,rope,多模态,生成模型
时空之章：将Attention视为平方复杂度的RNN | 时空之章将attention视为平方复杂度的rnn | blogs_raw/时空之章将attention视为平方复杂度的rnn.md | pending |  | 语言模型,RNN,attention,复杂度,生成模型
用傅里叶级数拟合一维概率密度函数 | 用傅里叶级数拟合一维概率密度函数 | blogs_raw/用傅里叶级数拟合一维概率密度函数.md | pending |  | 级数,概率,分析,逼近,生成模型
配置不同的学习率，LoRA还能再涨一点？ | 配置不同的学习率lora还能再涨一点 | blogs_raw/配置不同的学习率lora还能再涨一点.md | pending |  | 梯度,优化器,低秩,lora,生成模型
“闭门造车”之多模态思路浅谈（一）：无损输入 | 闭门造车之多模态思路浅谈一无损输入 | blogs_raw/闭门造车之多模态思路浅谈一无损输入.md | pending |  | VAE,GAN,Flow,Diffusion,生成模型
更便捷的Cool Papers打开方式：Chrome重定向扩展 | 更便捷的cool-papers打开方式chrome重定向扩展 | blogs_raw/更便捷的cool-papers打开方式chrome重定向扩展.md | pending |  | 网站,论文,酷论文,生成模型,attention
幂等生成网络IGN：试图将判别和生成合二为一的GAN | 幂等生成网络ign试图将判别和生成合二为一的gan | blogs_raw/幂等生成网络ign试图将判别和生成合二为一的gan.md | pending |  | GAN,GAN,生成模型,对抗,生成模型
Transformer升级之路：16、“复盘”长度外推技术 | transformer升级之路16复盘长度外推技术 | blogs_raw/transformer升级之路16复盘长度外推技术.md | pending |  | attention,位置编码,泛化,外推,rope
旁门左道之如何让Python的重试代码更加优雅 | 旁门左道之如何让python的重试代码更加优雅 | blogs_raw/旁门左道之如何让python的重试代码更加优雅.md | pending |  | 编程,代码,python,优化,生成模型
局部余弦相似度大，全局余弦相似度一定也大吗？ | 局部余弦相似度大全局余弦相似度一定也大吗 | blogs_raw/局部余弦相似度大全局余弦相似度一定也大吗.md | pending |  | 不等式,相似度,悖论,生成模型,attention
新年快乐！记录一下 Cool Papers 的开发体验 | 新年快乐记录一下-cool-papers-的开发体验 | blogs_raw/新年快乐记录一下-cool-papers-的开发体验.md | pending |  | 网站,论文,酷论文,生成模型,attention
写了个刷论文的辅助网站：Cool Papers | 写了个刷论文的辅助网站cool-papers | blogs_raw/写了个刷论文的辅助网站cool-papers.md | pending |  | 网站,论文,酷论文,生成模型,attention
让炼丹更科学一些（一）：SGD的平均损失收敛 | 让炼丹更科学一些一sgd的平均损失收敛 | blogs_raw/让炼丹更科学一些一sgd的平均损失收敛.md | pending |  | 优化器,不等式,优化器,sgd,炼丹
注意力机制真的可以“集中注意力”吗？ | 注意力机制真的可以集中注意力吗 | blogs_raw/注意力机制真的可以集中注意力吗.md | pending |  | 熵,稀疏,attention,秩,生成模型
生成扩散模型漫谈（二十一）：中值定理加速ODE采样 | 生成扩散模型漫谈二十一中值定理加速ode采样 | blogs_raw/生成扩散模型漫谈二十一中值定理加速ode采样.md | pending |  | 微分方程,生成模型,扩散,生成模型,attention
我在Performer中发现了Transformer-VQ的踪迹 | 我在performer中发现了transformer-vq的踪迹 | blogs_raw/我在performer中发现了transformer-vq的踪迹.md | pending |  | 量子化,语言模型,attention,生成模型,attention
Transformer升级之路：15、Key归一化助力长度外推 | transformer升级之路15key归一化助力长度外推 | blogs_raw/transformer升级之路15key归一化助力长度外推.md | pending |  | attention,位置编码,泛化,外推,生成模型
【生活杂记】炒锅的尽头是铁锅 | 生活杂记炒锅的尽头是铁锅 | blogs_raw/生活杂记炒锅的尽头是铁锅.md | pending |  | 生活,厨房,美食,生成模型,attention
VQ一下Key，Transformer的复杂度就变成线性了 | vq一下keytransformer的复杂度就变成线性了 | blogs_raw/vq一下keytransformer的复杂度就变成线性了.md | pending |  | 量子化,编码,梯度,attention,生成模型
简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE | 简单得令人尴尬的fsq四舍五入超越了vq-vae | blogs_raw/简单得令人尴尬的fsq四舍五入超越了vq-vae.md | pending |  | 生成模型,编码,梯度,离散化,生成模型
从梯度最大化看Attention的Scale操作 | 从梯度最大化看attention的scale操作 | blogs_raw/从梯度最大化看attention的scale操作.md | pending |  | 优化,梯度,attention,生成模型,attention
随机分词再探：从Viterbi Sampling到完美采样算法 | 随机分词再探从viterbi-sampling到完美采样算法 | blogs_raw/随机分词再探从viterbi-sampling到完美采样算法.md | pending |  | 概率,随机,优化,分词,采样
EMO：基于最优传输思想设计的分类损失函数 | emo基于最优传输思想设计的分类损失函数 | blogs_raw/emo基于最优传输思想设计的分类损失函数.md | pending |  | 概率,优化,损失函数,最优传输,生成模型
预训练一下，Transformer的长序列成绩还能涨不少！ | 预训练一下transformer的长序列成绩还能涨不少 | blogs_raw/预训练一下transformer的长序列成绩还能涨不少.md | pending |  | 语言模型,attention,生成模型,attention,优化
脑洞大开：非线性RNN居然也可以并行计算？ | 脑洞大开非线性rnn居然也可以并行计算 | blogs_raw/脑洞大开非线性rnn居然也可以并行计算.md | pending |  | 摄动,方程,迭代,语言模型,RNN
自然数集中 N = ab + c 时 a + b + c 的最小值 | 自然数集中-n-ab-c-时-a-b-c-的最小值 | blogs_raw/自然数集中-n-ab-c-时-a-b-c-的最小值.md | pending |  | 最优,问题,生成模型,attention,优化
随机分词浅探：从Viterbi Decoding到Viterbi Sampling | 随机分词浅探从viterbi-decoding到viterbi-sampling | blogs_raw/随机分词浅探从viterbi-decoding到viterbi-sampling.md | pending |  | 概率,随机,分词,新词发现,生成模型
大词表语言模型在续写任务上的一个问题及对策 | 大词表语言模型在续写任务上的一个问题及对策 | blogs_raw/大词表语言模型在续写任务上的一个问题及对策.md | pending |  | 概率,问题,语言模型,生成模型,attention
BytePiece：更纯粹、更高压缩率的Tokenizer | bytepiece更纯粹更高压缩率的tokenizer | blogs_raw/bytepiece更纯粹更高压缩率的tokenizer.md | pending |  | 最小熵,分词,无监督,新词发现,生成模型
Lion/Tiger优化器训练下的Embedding异常和对策 | liontiger优化器训练下的embedding异常和对策 | blogs_raw/liontiger优化器训练下的embedding异常和对策.md | pending |  | 问题,梯度,优化器,生成模型,attention
Transformer升级之路：14、当HWFA遇见ReRoPE | transformer升级之路14当hwfa遇见rerope | blogs_raw/transformer升级之路14当hwfa遇见rerope.md | pending |  | attention,位置编码,外推,rope,生成模型
Transformer升级之路：13、逆用Leaky ReRoPE | transformer升级之路13逆用leaky-rerope | blogs_raw/transformer升级之路13逆用leaky-rerope.md | pending |  | attention,位置编码,泛化,外推,rope
Transformer升级之路：12、无限外推的ReRoPE？ | transformer升级之路12无限外推的rerope | blogs_raw/transformer升级之路12无限外推的rerope.md | pending |  | attention,位置编码,泛化,外推,rope
Transformer升级之路：11、将β进制位置进行到底 | transformer升级之路11将β进制位置进行到底 | blogs_raw/transformer升级之路11将β进制位置进行到底.md | pending |  | attention,位置编码,泛化,外推,rope
语言模型输出端共享Embedding的重新探索 | 语言模型输出端共享embedding的重新探索 | blogs_raw/语言模型输出端共享embedding的重新探索.md | pending |  | 语言模型,初始化,生成模型,attention,优化
当生成模型肆虐：互联网将有“疯牛病”之忧？ | 当生成模型肆虐互联网将有疯牛病之忧 | blogs_raw/当生成模型肆虐互联网将有疯牛病之忧.md | pending |  | 生成模型,生成模型,attention,优化,语言模型
Transformer升级之路：10、RoPE是一种β进制编码 | transformer升级之路10rope是一种β进制编码 | blogs_raw/transformer升级之路10rope是一种β进制编码.md | pending |  | attention,位置编码,泛化,外推,rope
生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP | 生成扩散模型漫谈二十从reflow到wgan-gp | blogs_raw/生成扩散模型漫谈二十从reflow到wgan-gp.md | pending |  | 优化,GAN,梯度,扩散,生成模型
生成扩散模型漫谈（十九）：作为扩散ODE的GAN | 生成扩散模型漫谈十九作为扩散ode的gan | blogs_raw/生成扩散模型漫谈十九作为扩散ode的gan.md | pending |  | 优化,GAN,扩散,生成模型,attention
梯度流：探索通向最小值之路 | 梯度流探索通向最小值之路 | blogs_raw/梯度流探索通向最小值之路.md | pending |  | 泛函,动力学,优化,梯度,生成模型
Naive Bayes is all you need ? | naive-bayes-is-all-you-need | blogs_raw/naive-bayes-is-all-you-need.md | pending |  | 语言模型,attention,LLM,贝叶斯,生成模型
关于NBCE方法的一些补充说明和分析 | 关于nbce方法的一些补充说明和分析 | blogs_raw/关于nbce方法的一些补充说明和分析.md | pending |  | 语言模型,外推,LLM,贝叶斯,生成模型
NBCE：使用朴素贝叶斯扩展LLM的Context处理长度 | nbce使用朴素贝叶斯扩展llm的context处理长度 | blogs_raw/nbce使用朴素贝叶斯扩展llm的context处理长度.md | pending |  | 语言模型,外推,LLM,贝叶斯,生成模型
基于量子化假设推导模型的尺度定律（Scaling Law） | 基于量子化假设推导模型的尺度定律scaling-law | blogs_raw/基于量子化假设推导模型的尺度定律scaling-law.md | pending |  | 模型,分析,量子,尺度定律,生成模型
Transformer升级之路：9、一种全局长度外推的新思路 | transformer升级之路9一种全局长度外推的新思路 | blogs_raw/transformer升级之路9一种全局长度外推的新思路.md | pending |  | attention,泛化,外推,生成模型,attention
如何度量数据的稀疏程度？ | 如何度量数据的稀疏程度 | blogs_raw/如何度量数据的稀疏程度.md | pending |  | 概率,熵,度量,稀疏,生成模型
注意力和Softmax的两点有趣发现：鲁棒性和信息量 | 注意力和softmax的两点有趣发现鲁棒性和信息量 | blogs_raw/注意力和softmax的两点有趣发现鲁棒性和信息量.md | pending |  | 信息,熵,attention,生成模型,attention
梯度视角下的LoRA：简介、分析、猜测及推广 | 梯度视角下的lora简介分析猜测及推广 | blogs_raw/梯度视角下的lora简介分析猜测及推广.md | pending |  | 梯度,优化器,低秩,lora,生成模型
从JL引理看熵不变性Attention | 从jl引理看熵不变性attention | blogs_raw/从jl引理看熵不变性attention.md | pending |  | 熵,attention,生成模型,attention,优化
Bias项的神奇作用：RoPE + Bias = 更好的长度外推性 | bias项的神奇作用rope-bias-更好的长度外推性 | blogs_raw/bias项的神奇作用rope-bias-更好的长度外推性.md | pending |  | 语言模型,attention,位置编码,外推,rope
Google新作试图“复活”RNN：RNN能否再次辉煌？ | google新作试图复活rnnrnn能否再次辉煌 | blogs_raw/google新作试图复活rnnrnn能否再次辉煌.md | pending |  | 语言模型,RNN,生成模型,attention,生成模型
《为什么现在的LLM都是Decoder-only的架构？》FAQ | 为什么现在的llm都是decoder-only的架构faq | blogs_raw/为什么现在的llm都是decoder-only的架构faq.md | pending |  | 问答,语言模型,文本生成,attention,生成模型
为什么现在的LLM都是Decoder-only的架构？ | 为什么现在的llm都是decoder-only的架构 | blogs_raw/为什么现在的llm都是decoder-only的架构.md | pending |  | 分析,语言模型,文本生成,attention,生成模型
缓解交叉熵过度自信的一个简明方案 | 缓解交叉熵过度自信的一个简明方案 | blogs_raw/缓解交叉熵过度自信的一个简明方案.md | pending |  | 优化,损失函数,光滑,生成模型,attention
Tiger：一个“抠”到极致的优化器 | tiger一个抠到极致的优化器 | blogs_raw/tiger一个抠到极致的优化器.md | pending |  | 模型,优化,优化器,生成模型,attention
生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配 | 生成扩散模型漫谈十八得分匹配-条件得分匹配 | blogs_raw/生成扩散模型漫谈十八得分匹配-条件得分匹配.md | pending |  | 概率,分析,生成模型,扩散,生成模型
生成扩散模型漫谈（十七）：构建ODE的一般步骤（下） | 生成扩散模型漫谈十七构建ode的一般步骤下 | blogs_raw/生成扩散模型漫谈十七构建ode的一般步骤下.md | pending |  | 概率,微分方程,生成模型,扩散,生成模型
Google新搜出的优化器Lion：效率与效果兼得的“训练狮” | google新搜出的优化器lion效率与效果兼得的训练狮 | blogs_raw/google新搜出的优化器lion效率与效果兼得的训练狮.md | pending |  | 分析,优化,优化器,生成模型,attention
生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配 | 生成扩散模型漫谈十六w距离-得分匹配 | blogs_raw/生成扩散模型漫谈十六w距离-得分匹配.md | pending |  | 微分方程,GAN,生成模型,扩散,生成模型
测试函数法推导连续性方程和Fokker-Planck方程 | 测试函数法推导连续性方程和fokker-planck方程 | blogs_raw/测试函数法推导连续性方程和fokker-planck方程.md | pending |  | 概率,微分方程,随机,扩散,生成模型
Transformer升级之路：8、长度外推性与位置鲁棒性 | transformer升级之路8长度外推性与位置鲁棒性 | blogs_raw/transformer升级之路8长度外推性与位置鲁棒性.md | pending |  | 语言模型,attention,位置编码,外推,生成模型
Transformer升级之路：7、长度外推性与局部注意力 | transformer升级之路7长度外推性与局部注意力 | blogs_raw/transformer升级之路7长度外推性与局部注意力.md | pending |  | 语言模型,attention,位置编码,外推,生成模型
智能家居之热水器零冷水技术原理浅析 | 智能家居之热水器零冷水技术原理浅析 | blogs_raw/智能家居之热水器零冷水技术原理浅析.md | pending |  | 智能家居,生成模型,attention,优化,语言模型
Transformer升级之路：6、旋转位置编码的完备性分析 | transformer升级之路6旋转位置编码的完备性分析 | blogs_raw/transformer升级之路6旋转位置编码的完备性分析.md | pending |  | 矩阵,attention,位置编码,rope,生成模型
生成扩散模型漫谈（十五）：构建ODE的一般步骤（中） | 生成扩散模型漫谈十五构建ode的一般步骤中 | blogs_raw/生成扩散模型漫谈十五构建ode的一般步骤中.md | pending |  | 微分方程,生成模型,扩散,格林函数,生成模型
生成扩散模型漫谈（十四）：构建ODE的一般步骤（上） | 生成扩散模型漫谈十四构建ode的一般步骤上 | blogs_raw/生成扩散模型漫谈十四构建ode的一般步骤上.md | pending |  | 微分方程,生成模型,扩散,格林函数,生成模型
从局部到全局：语义相似度的测地线距离 | 从局部到全局语义相似度的测地线距离 | blogs_raw/从局部到全局语义相似度的测地线距离.md | pending |  | 黎曼几何,语义,语义相似度,生成模型,attention
智能家居之小爱同学控制极米投影仪的简单方案 | 智能家居之小爱同学控制极米投影仪的简单方案 | blogs_raw/智能家居之小爱同学控制极米投影仪的简单方案.md | pending |  | 生活,智能家居,米家,生成模型,attention
用热传导方程来指导自监督学习 | 用热传导方程来指导自监督学习 | blogs_raw/用热传导方程来指导自监督学习.md | pending |  | 物理,无监督,生成模型,attention,优化
基于Amos优化器思想推导出来的一些“炼丹策略” | 基于amos优化器思想推导出来的一些炼丹策略 | blogs_raw/基于amos优化器思想推导出来的一些炼丹策略.md | pending |  | 分析,优化,渐近,优化器,生成模型
CoSENT（三）：作为交互式相似度的损失函数 | cosent三作为交互式相似度的损失函数 | blogs_raw/cosent三作为交互式相似度的损失函数.md | pending |  | 语义,语义相似度,对比学习,生成模型,attention
利用CUR分解加速交互式相似度模型的检索 | 利用cur分解加速交互式相似度模型的检索 | blogs_raw/利用cur分解加速交互式相似度模型的检索.md | pending |  | 矩阵,语义,语义相似度,生成模型,attention
圆内随机n点在同一个圆心角为θ的扇形的概率 | 圆内随机n点在同一个圆心角为θ的扇形的概率 | blogs_raw/圆内随机n点在同一个圆心角为θ的扇形的概率.md | pending |  | 概率,竞赛,随机,生成模型,attention
生成扩散模型漫谈（十三）：从万有引力到扩散模型 | 生成扩散模型漫谈十三从万有引力到扩散模型 | blogs_raw/生成扩散模型漫谈十三从万有引力到扩散模型.md | pending |  | 引力,场论,生成模型,扩散,生成模型
“十字架”组合计数问题浅试 | 十字架组合计数问题浅试 | blogs_raw/十字架组合计数问题浅试.md | pending |  | 证明,数学,组合数学,生成模型,attention
生成扩散模型漫谈（十二）：“硬刚”扩散ODE | 生成扩散模型漫谈十二硬刚扩散ode | blogs_raw/生成扩散模型漫谈十二硬刚扩散ode.md | pending |  | 微分方程,生成模型,扩散,生成模型,attention
生成扩散模型漫谈（十一）：统一扩散模型（应用篇） | 生成扩散模型漫谈十一统一扩散模型应用篇 | blogs_raw/生成扩散模型漫谈十一统一扩散模型应用篇.md | pending |  | 统一,生成模型,DDPM,扩散,生成模型
生成扩散模型漫谈（十）：统一扩散模型（理论篇） | 生成扩散模型漫谈十统一扩散模型理论篇 | blogs_raw/生成扩散模型漫谈十统一扩散模型理论篇.md | pending |  | 统一,生成模型,DDPM,扩散,生成模型
生成扩散模型漫谈（九）：条件控制生成结果 | 生成扩散模型漫谈九条件控制生成结果 | blogs_raw/生成扩散模型漫谈九条件控制生成结果.md | pending |  | 概率,生成模型,DDPM,扩散,生成模型
生成扩散模型漫谈（八）：最优扩散方差估计（下） | 生成扩散模型漫谈八最优扩散方差估计下 | blogs_raw/生成扩散模型漫谈八最优扩散方差估计下.md | pending |  | 优化,生成模型,DDPM,扩散,生成模型
生成扩散模型漫谈（七）：最优扩散方差估计（上） | 生成扩散模型漫谈七最优扩散方差估计上 | blogs_raw/生成扩散模型漫谈七最优扩散方差估计上.md | pending |  | 优化,生成模型,DDPM,扩散,生成模型
生成扩散模型漫谈（六）：一般框架之ODE篇 | 生成扩散模型漫谈六一般框架之ode篇 | blogs_raw/生成扩散模型漫谈六一般框架之ode篇.md | pending |  | flow模型,微分方程,生成模型,DDPM,扩散
生成扩散模型漫谈（五）：一般框架之SDE篇 | 生成扩散模型漫谈五一般框架之sde篇 | blogs_raw/生成扩散模型漫谈五一般框架之sde篇.md | pending |  | 微分方程,生成模型,DDPM,扩散,生成模型
生成扩散模型漫谈（四）：DDIM = 高观点DDPM | 生成扩散模型漫谈四ddim-高观点ddpm | blogs_raw/生成扩散模型漫谈四ddim-高观点ddpm.md | pending |  | 微分方程,生成模型,DDPM,扩散,生成模型
生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪 | 生成扩散模型漫谈三ddpm-贝叶斯-去噪 | blogs_raw/生成扩散模型漫谈三ddpm-贝叶斯-去噪.md | pending |  | 概率,生成模型,DDPM,扩散,生成模型
不成功的尝试：将多标签交叉熵推广到“n个m分类”上去 | 不成功的尝试将多标签交叉熵推广到n个m分类上去 | blogs_raw/不成功的尝试将多标签交叉熵推广到n个m分类上去.md | pending |  | 优化,损失函数,生成模型,attention,优化
生成扩散模型漫谈（二）：DDPM = 自回归式VAE | 生成扩散模型漫谈二ddpm-自回归式vae | blogs_raw/生成扩散模型漫谈二ddpm-自回归式vae.md | pending |  | vae,生成模型,DDPM,扩散,生成模型
“维度灾难”之Hubness现象浅析 | 维度灾难之hubness现象浅析 | blogs_raw/维度灾难之hubness现象浅析.md | pending |  | 维度,GAN,生成模型,生成模型,attention
Ladder Side-Tuning：预训练模型的“过墙梯” | ladder-side-tuning预训练模型的过墙梯 | blogs_raw/ladder-side-tuning预训练模型的过墙梯.md | pending |  | 语言模型,预训练,生成模型,attention,优化
生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼 | 生成扩散模型漫谈一ddpm-拆楼-建楼 | blogs_raw/生成扩散模型漫谈一ddpm-拆楼-建楼.md | pending |  | VAE,GAN,flow模型,概率,生成模型
相对位置编码Transformer的一个理论缺陷与对策 | 相对位置编码transformer的一个理论缺陷与对策 | blogs_raw/相对位置编码transformer的一个理论缺陷与对策.md | pending |  | 语言模型,attention,位置编码,生成模型,attention
如何训练你的准确率？ | 如何训练你的准确率 | blogs_raw/如何训练你的准确率.md | pending |  | 概率,优化,损失函数,生成模型,attention
从重参数的角度看离散概率分布的构建 | 从重参数的角度看离散概率分布的构建 | blogs_raw/从重参数的角度看离散概率分布的构建.md | pending |  | 概率,重参数,生成模型,attention,优化
当BERT-whitening引入超参数：总有一款适合你 | 当bert-whitening引入超参数总有一款适合你 | blogs_raw/当bert-whitening引入超参数总有一款适合你.md | pending |  | 语言模型,语义,语义相似度,生成模型,attention
logsumexp运算的几个不等式 | logsumexp运算的几个不等式 | blogs_raw/logsumexp运算的几个不等式.md | pending |  | 不等式,函数,生成模型,attention,优化
多标签“Softmax+交叉熵”的软标签版本 | 多标签softmax交叉熵的软标签版本 | blogs_raw/多标签softmax交叉熵的软标签版本.md | pending |  | 优化,损失函数,光滑,生成模型,attention
在bert4keras中使用混合精度和XLA加速训练 | 在bert4keras中使用混合精度和xla加速训练 | blogs_raw/在bert4keras中使用混合精度和xla加速训练.md | pending |  | 模型,优化,梯度,生成模型,attention
GAU-α：尝鲜体验快好省的下一代Attention | gau-α尝鲜体验快好省的下一代attention | blogs_raw/gau-α尝鲜体验快好省的下一代attention.md | pending |  | 语言模型,attention,预训练,生成模型,attention
你的语言模型有没有“无法预测的词”？ | 你的语言模型有没有无法预测的词 | blogs_raw/你的语言模型有没有无法预测的词.md | pending |  | 语言模型,多任务,生成模型,attention,优化
GlobalPointer下的“KL散度”应该是怎样的？ | globalpointer下的kl散度应该是怎样的 | blogs_raw/globalpointer下的kl散度应该是怎样的.md | pending |  | 损失函数,对抗训练,NER,正则化,生成模型
熵不变性Softmax的一个快速推导 | 熵不变性softmax的一个快速推导 | blogs_raw/熵不变性softmax的一个快速推导.md | pending |  | 近似,熵,attention,生成模型,attention
听说Attention与Softmax更配哦～ | 听说attention与softmax更配哦 | blogs_raw/听说attention与softmax更配哦.md | pending |  | 熵,语言模型,attention,预训练,生成模型
为什么Pre Norm的效果不如Post Norm？ | 为什么pre-norm的效果不如post-norm | blogs_raw/为什么pre-norm的效果不如post-norm.md | pending |  | 优化,梯度,attention,生成模型,attention
RoFormerV2：自然语言理解的极限探索 | roformerv2自然语言理解的极限探索 | blogs_raw/roformerv2自然语言理解的极限探索.md | pending |  | 语言模型,预训练,生成模型,attention,优化
为什么需要残差？一个来自DeepNet的视角 | 为什么需要残差一个来自deepnet的视角 | blogs_raw/为什么需要残差一个来自deepnet的视角.md | pending |  | 模型,优化,深度学习,梯度,生成模型
门控注意力单元（GAU）还需要Warmup吗？ | 门控注意力单元gau还需要warmup吗 | blogs_raw/门控注意力单元gau还需要warmup吗.md | pending |  | 模型,优化,attention,生成模型,attention
训练1000层的Transformer究竟有什么困难？ | 训练1000层的transformer究竟有什么困难 | blogs_raw/训练1000层的transformer究竟有什么困难.md | pending |  | 优化,梯度,attention,生成模型,attention
指数梯度下降 + 元学习 = 自适应学习率 | 指数梯度下降-元学习-自适应学习率 | blogs_raw/指数梯度下降-元学习-自适应学习率.md | pending |  | 优化,梯度,优化器,生成模型,attention
FLASH：可能是近来最有意思的高效Transformer设计 | flash可能是近来最有意思的高效transformer设计 | blogs_raw/flash可能是近来最有意思的高效transformer设计.md | pending |  | 语言模型,生成模型,attention,生成模型,attention
GPLinker：基于GlobalPointer的事件联合抽取 | gplinker基于globalpointer的事件联合抽取 | blogs_raw/gplinker基于globalpointer的事件联合抽取.md | pending |  | NLP,信息抽取,NER,生成模型,attention
多任务学习漫谈（三）：分主次之序 | 多任务学习漫谈三分主次之序 | blogs_raw/多任务学习漫谈三分主次之序.md | pending |  | 深度学习,损失函数,梯度,多任务,生成模型
多任务学习漫谈（二）：行梯度之事 | 多任务学习漫谈二行梯度之事 | blogs_raw/多任务学习漫谈二行梯度之事.md | pending |  | 深度学习,损失函数,梯度,多任务,生成模型
GPLinker：基于GlobalPointer的实体关系联合抽取 | gplinker基于globalpointer的实体关系联合抽取 | blogs_raw/gplinker基于globalpointer的实体关系联合抽取.md | pending |  | NLP,信息抽取,NER,生成模型,attention
Efficient GlobalPointer：少点参数，多点效果 | efficient-globalpointer少点参数多点效果 | blogs_raw/efficient-globalpointer少点参数多点效果.md | pending |  | 模型,NLP,NER,生成模型,attention
多任务学习漫谈（一）：以损失之名 | 多任务学习漫谈一以损失之名 | blogs_raw/多任务学习漫谈一以损失之名.md | pending |  | 深度学习,损失函数,多任务,生成模型,attention
CoSENT（二）：特征式匹配与交互式匹配有多大差距？ | cosent二特征式匹配与交互式匹配有多大差距 | blogs_raw/cosent二特征式匹配与交互式匹配有多大差距.md | pending |  | 语义,语义相似度,对比学习,生成模型,attention
CoSENT（一）：比Sentence-BERT更有效的句向量方案 | cosent一比sentence-bert更有效的句向量方案 | blogs_raw/cosent一比sentence-bert更有效的句向量方案.md | pending |  | 语义,语义相似度,对比学习,生成模型,attention
SquarePlus：可能是运算最简单的ReLU光滑近似 | squareplus可能是运算最简单的relu光滑近似 | blogs_raw/squareplus可能是运算最简单的relu光滑近似.md | pending |  | 函数,近似,分析,生成模型,attention
概率分布的熵归一化（Entropy Normalization） | 概率分布的熵归一化entropy-normalization | blogs_raw/概率分布的熵归一化entropy-normalization.md | pending |  | 概率,熵,生成模型,attention,优化
从熵不变性看Attention的Scale操作 | 从熵不变性看attention的scale操作 | blogs_raw/从熵不变性看attention的scale操作.md | pending |  | 概率,熵,attention,生成模型,attention
Seq2Seq+前缀树：检索任务新范式（以KgCLUE为例） | seq2seq前缀树检索任务新范式以kgclue为例 | blogs_raw/seq2seq前缀树检索任务新范式以kgclue为例.md | pending |  | 代码,语义,keras,相似度,生成模型
输入梯度惩罚与参数梯度惩罚的一个不等式 | 输入梯度惩罚与参数梯度惩罚的一个不等式 | blogs_raw/输入梯度惩罚与参数梯度惩罚的一个不等式.md | pending |  | 不等式,优化,梯度,泛化,生成模型
变分自编码器（八）：估计样本概率密度 | 变分自编码器八估计样本概率密度 | blogs_raw/变分自编码器八估计样本概率密度.md | pending |  | 概率,变分,vae,生成模型,生成模型
开局一段扯，数据全靠编？真被一篇“神论文”气到了 | 开局一段扯数据全靠编真被一篇神论文气到了 | blogs_raw/开局一段扯数据全靠编真被一篇神论文气到了.md | pending |  | 情感,模型,生成模型,attention,优化
Dropout视角下的MLM和MAE：一些新的启发 | dropout视角下的mlm和mae一些新的启发 | blogs_raw/dropout视角下的mlm和mae一些新的启发.md | pending |  | 模型,概率,分析,优化,生成模型
ChildTuning：试试把Dropout加到梯度上去？ | childtuning试试把dropout加到梯度上去 | blogs_raw/childtuning试试把dropout加到梯度上去.md | pending |  | 模型,优化,梯度,生成模型,attention
WGAN新方案：通过梯度归一化来实现L约束 | wgan新方案通过梯度归一化来实现l约束 | blogs_raw/wgan新方案通过梯度归一化来实现l约束.md | pending |  | 无监督,GAN,生成模型,生成模型,attention
模型优化漫谈：BERT的初始标准差为什么是0.02？ | 模型优化漫谈bert的初始标准差为什么是002 | blogs_raw/模型优化漫谈bert的初始标准差为什么是002.md | pending |  | 模型,分析,优化,梯度,生成模型
bert4keras在手，baseline我有：CLUE基准代码 | bert4keras在手baseline我有clue基准代码 | blogs_raw/bert4keras在手baseline我有clue基准代码.md | pending |  | 模型,代码,keras,生成模型,attention
CAN：借助先验分布提升分类性能的简单后处理技巧 | can借助先验分布提升分类性能的简单后处理技巧 | blogs_raw/can借助先验分布提升分类性能的简单后处理技巧.md | pending |  | 模型,概率,分析,技巧,生成模型
初始化方法中非方阵的维度平均策略思考 | 初始化方法中非方阵的维度平均策略思考 | blogs_raw/初始化方法中非方阵的维度平均策略思考.md | pending |  | 模型,优化,梯度,生成模型,attention
用狄拉克函数来构造非光滑函数的光滑近似 | 用狄拉克函数来构造非光滑函数的光滑近似 | blogs_raw/用狄拉克函数来构造非光滑函数的光滑近似.md | pending |  | 函数,近似,分析,光滑,生成模型
关于WhiteningBERT原创性的疑问和沟通 | 关于whiteningbert原创性的疑问和沟通 | blogs_raw/关于whiteningbert原创性的疑问和沟通.md | pending |  | 情感,模型,工作,生成模型,attention
