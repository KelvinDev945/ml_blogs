2020年全年天象.md
adafactor优化器浅析附开源实现.md
adamw的weight-rms的渐近估计上.md
adax优化器浅析附开源实现.md
bert-of-theseus基于模块替换的模型压缩方法.md
bert4keras在手baseline我有百度lic2020.md
bert可以上几年级了seq2seq硬刚小学数学应用题.md
crf用过了不妨再了解下更快的memm.md
designing-gans又一个gan生产车间.md
eae自编码器-bn-最大熵-生成模型.md
expx在x0处的偶次泰勒展开式总是正的.md
flatnce小批次对比学习效果差的原因竟是浮点误差.md
gelu的两个初等函数近似是怎么来的.md
globalpointer用统一的方式处理嵌套和非嵌套ner.md
google新作synthesizer我们还不够了解自注意力.md
l2正则没有想象那么好可能是权重尺度偏移惹的祸.md
mitchell近似乘法变为加法误差不超过19.md
muon优化器指南快速上手与关键细节.md
nyströmformer基于矩阵分解的线性化attention方案.md
p-tuning自动构建模版释放语言模型潜能.md
performer用随机投影将attention的复杂度线性化.md
printemps-苏神怎么看log-linear-attention这种变长的h.md
realformer把残差转移到attention矩阵上面去.md
self-orthogonality-module一个即插即用的核正交化模块.md
seq2seq中exposure-bias现象的浅析与对策.md
seq2seq重复解码现象的理论分析尝试.md
simbertv2来了融合检索和生成的roformer-sim模型.md
spaces抽取-生成式长文本摘要法研杯总结.md
t5-pegasus开源一个中文生成式预训练模型.md
teaforn让teacher-forcing更有远见一些.md
transformer升级之路19第二类旋转位置编码.md
transformer升级之路1sinusoidal位置编码追根溯源.md
transformer升级之路2博采众长的旋转式位置编码.md
transformer升级之路3从performer到线性attention.md
transformer升级之路4二维位置的旋转式位置编码.md
transformer升级之路5作为无限维的线性attention.md
univae基于transformer的单模型多尺度的vae模型.md
wgan的成功可能跟wasserstein距离没啥关系.md
一个二值化词向量模型是怎么跟果蝇搭上关系的.md
万能的seq2seq基于seq2seq的阅读理解问答.md
两个多元正态分布的kl散度巴氏距离和w距离.md
中文任务还是sota吗我们给simcse补充了一些实验.md
为什么deltanet要加l2-n.md
为什么梯度裁剪能加速训练过程一个简明的分析.md
也来扯几句全国青少年科技创新大赛.md
也来盘点一些最近的非transformer工作.md
也来谈谈rnn的梯度消失爆炸问题.md
从emdwmd到wrd文本向量序列的相似度计算.md
从一个单位向量变换到另一个单位向量的正交矩阵.md
从三角不等式到margin-softmax.md
从动力学角度看优化算法七sgd-svm.md
从动力学角度看优化算法五为什么学习率不宜过小.md
从动力学角度看优化算法六为什么simsiam不退化.md
从采样看优化可导优化与不可导优化的统一视角.md
你可能不需要bert-flow一个线性变换媲美bert-flow.md
你的crf层的学习率可能不够大.md
修改transformer结构设计一个更快更好的mlm模型.md
关于维度公式n-833-log-n的可用性分析.md
再谈类别不平衡问题调节权重与魔改loss的对比联系.md
动手做个dialogpt基于lm的生成式多轮对话模型.md
又是dropout两次这次它做到了有监督任务的sota.md
变分自编码器七球面上的vaevmf-vae.md
变分自编码器五vae-bn-更好的vae.md
变分自编码器六从几何视角来理解vae的尝试.md
基于conditional-layer-normalization的条件文本生成.md
如何划分一个跟测试集更接近的验证集.md
如何应对seq2seq中的根本停不下来问题.md
学会提问的bert端到端地从篇章中构建问答对.md
对抗训练浅谈意义方法和思考附keras实现.md
对比学习可以使用梯度累积吗.md
将softmax交叉熵推广到多标签分类问题.md
层次分解位置编码让bert可以处理超长文本.md
强大的nvae以后再也不能说vae生成的图像模糊了.md
当gpt遇上中国象棋写过文章解过题要不再来下盘棋.md
必须要gpt3吗不bert的mlm模型也能小样本学习.md
我们可以无损放大一个transformer模型吗一.md
我们真的需要把训练集的损失降低到零吗.md
抛开约束增强模型一行代码提升albert表现.md
提速不掉点基于词颗粒度的中文wobert.md
搜出来的文本一从文本生成到搜索采样.md
搜出来的文本三基于bert的文本采样.md
搜出来的文本二从mcmc到模拟退火.md
搜出来的文本四通过增删改来用词造句.md
搜狐文本匹配基于条件layernorm的多任务baseline.md
无监督分词和句法分析原来bert还可以这样用.md
无监督语义相似度哪家强我们做了个比较全面的评测.md
日食记.md
曾被嫌弃的预训练任务nsp做出了优秀的zero-shot效果.md
最小熵原理六词向量的维度应该怎么选择.md
有限内存下全局打乱几百g文件python.md
概率视角下的线性模型逻辑回归有解析解吗.md
殊途同归的策略梯度与零阶优化.md
泛化性乱弹从随机噪声梯度惩罚到虚拟对抗训练.md
浅谈transformer的初始化参数化与标准化.md
滑动平均视角下的权重衰减和学习率.md
现在可以用keras玩中文gpt2了gpt2-ml.md
生成扩散模型漫谈三十一预测数.md
用albert和electra之前请确认你真的了解它们.md
用bert4keras做三元组抽取.md
用开源的人工标注数据来增强roformer-sim.md
短文本匹配baseline脱敏数据使用预训练模型的尝试.md
积分梯度一种新颖的神经网络可视化方法.md
突破瓶颈打造更强大的transformer.md
第1000篇文章.md
线性attention的探索attention必须有个softmax吗.md
线性transformer应该不是你要等的那个模型.md
线性注意力简史从模仿创新到反哺.md
节省显存的重计算技巧也有了keras版了.md
苏剑林-x与单位阵的平均平方误差mse作为它跟单位阵的差距有什.md
苏剑林-从公式12到公式15都在推导和解释你说的这个.md
苏剑林-是梯度均值为零的假设这个问题不是在数值模拟一节讨论过了吗.md
苏剑林-相对位置编码似乎没有太多选择了要不rope这种算是乘性了.md
让人惊叹的johnson-lindenstrauss引理应用篇.md
让人惊叹的johnson-lindenstrauss引理理论篇.md
让炼丹更科学一些三sgd的终.md
让炼丹更科学一些二将结论推广.md
让炼丹更科学一些四新恒等式.md
跟风玩玩目前最大的中文gpt2模型bert4keras.md
通过互信息思想来缓解类别不平衡问题.md
那个屠榜的t5模型现在可以在中文上玩玩了.md
隐藏在动量中的梯度累积少更新几步效果反而更好.md
非自回归也不差基于mlm的阅读理解问答.md
鱼与熊掌兼得融合检索和生成的simbert模型.md
龟鱼记全陶粒的同程底滤生态缸.md
